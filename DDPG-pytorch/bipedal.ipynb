{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "---\n",
    "In this notebook, we train DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "# env.seed(10)\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0], random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,) float32 (4,) float32 (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape, env.observation_space.dtype, env.action_space.shape, env.action_space.dtype, \n",
    "      env.reward_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: -116.49\n",
      "Episode 1\tAverage Score: -117.27\n",
      "Episode 2\tAverage Score: -119.42\n",
      "Episode 3\tAverage Score: -113.42\n",
      "Episode 4\tAverage Score: -112.77\n",
      "Episode 5\tAverage Score: -110.44\n",
      "Episode 6\tAverage Score: -108.88\n",
      "Episode 7\tAverage Score: -111.70\n",
      "Episode 8\tAverage Score: -110.22\n",
      "Episode 9\tAverage Score: -108.90\n",
      "Episode 10\tAverage Score: -107.85\n",
      "Episode 11\tAverage Score: -106.89\n",
      "Episode 12\tAverage Score: -105.94\n",
      "Episode 13\tAverage Score: -102.94\n",
      "Episode 14\tAverage Score: -102.54\n",
      "Episode 15\tAverage Score: -102.20\n",
      "Episode 16\tAverage Score: -101.88\n",
      "Episode 17\tAverage Score: -101.62\n",
      "Episode 18\tAverage Score: -101.38\n",
      "Episode 19\tAverage Score: -101.16\n",
      "Episode 20\tAverage Score: -100.97\n",
      "Episode 21\tAverage Score: -100.79\n",
      "Episode 22\tAverage Score: -100.63\n",
      "Episode 23\tAverage Score: -100.46\n",
      "Episode 24\tAverage Score: -100.33\n",
      "Episode 25\tAverage Score: -100.20\n",
      "Episode 26\tAverage Score: -100.07\n",
      "Episode 27\tAverage Score: -99.96\n",
      "Episode 28\tAverage Score: -99.86\n",
      "Episode 29\tAverage Score: -99.77\n",
      "Episode 30\tAverage Score: -99.67\n",
      "Episode 31\tAverage Score: -99.59\n",
      "Episode 32\tAverage Score: -99.51\n",
      "Episode 33\tAverage Score: -99.44\n",
      "Episode 34\tAverage Score: -99.37\n",
      "Episode 35\tAverage Score: -99.30\n",
      "Episode 36\tAverage Score: -99.25\n",
      "Episode 37\tAverage Score: -99.19\n",
      "Episode 38\tAverage Score: -99.14\n",
      "Episode 39\tAverage Score: -99.08\n",
      "Episode 40\tAverage Score: -99.03\n",
      "Episode 41\tAverage Score: -98.98\n",
      "Episode 42\tAverage Score: -98.94\n",
      "Episode 43\tAverage Score: -98.89\n",
      "Episode 44\tAverage Score: -98.11\n",
      "Episode 45\tAverage Score: -98.09\n",
      "Episode 46\tAverage Score: -98.06\n",
      "Episode 47\tAverage Score: -98.04\n",
      "Episode 48\tAverage Score: -98.02\n",
      "Episode 49\tAverage Score: -97.99\n",
      "Episode 50\tAverage Score: -97.97\n",
      "Episode 51\tAverage Score: -97.33\n",
      "Episode 52\tAverage Score: -97.33\n",
      "Episode 53\tAverage Score: -97.33\n",
      "Episode 54\tAverage Score: -98.27\n",
      "Episode 55\tAverage Score: -98.59\n",
      "Episode 56\tAverage Score: -98.90\n",
      "Episode 57\tAverage Score: -99.20\n",
      "Episode 58\tAverage Score: -99.50\n",
      "Episode 59\tAverage Score: -99.79\n",
      "Episode 60\tAverage Score: -100.05\n",
      "Episode 61\tAverage Score: -100.31\n",
      "Episode 62\tAverage Score: -100.56\n",
      "Episode 63\tAverage Score: -100.81\n",
      "Episode 64\tAverage Score: -101.05\n",
      "Episode 65\tAverage Score: -101.28\n",
      "Episode 66\tAverage Score: -101.50\n",
      "Episode 67\tAverage Score: -101.72\n",
      "Episode 68\tAverage Score: -101.94\n",
      "Episode 69\tAverage Score: -102.15\n",
      "Episode 70\tAverage Score: -102.35\n",
      "Episode 71\tAverage Score: -102.55\n",
      "Episode 72\tAverage Score: -102.73\n",
      "Episode 73\tAverage Score: -102.92\n",
      "Episode 74\tAverage Score: -103.10\n",
      "Episode 75\tAverage Score: -103.27\n",
      "Episode 76\tAverage Score: -103.45\n",
      "Episode 77\tAverage Score: -103.61\n",
      "Episode 78\tAverage Score: -103.78\n"
     ]
    }
   ],
   "source": [
    "# agent.actor_local.load_state_dict(torch.load('bipedal_checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('bipedal_checkpoint_critic.pth'))\n",
    "total_reward_deque = deque(maxlen=100)\n",
    "for ep in range(1111):\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break \n",
    "    total_reward_deque.append(total_reward)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(total_reward_deque)))   \n",
    "    #if np.mean(total_reward_deque) <= -130:\n",
    "    if np.mean(total_reward_deque) >= 300:\n",
    "        torch.save(agent.actor_local.state_dict(), 'bipedal_checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'bipedal_checkpoint_critic.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.actor_local.load_state_dict(torch.load('bipedal_checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('bipedal_checkpoint_critic.pth'))\n",
    "# # 1 episode\n",
    "# state = env.reset()\n",
    "# agent.reset()\n",
    "# total_reward = 0\n",
    "# while True:\n",
    "#     env.render()\n",
    "#     action = agent.act(state)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     total_reward += reward\n",
    "#     if done:\n",
    "#         break\n",
    "        \n",
    "# print('total_reward:{}'.format(total_reward))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "- You may also like to implement prioritized experience replay, to see if it speeds learning.  \n",
    "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!\n",
    "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
