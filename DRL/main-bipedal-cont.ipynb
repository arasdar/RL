{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "# Discrete/int or continuos/float\n",
    "env.action_space.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(24,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (24,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111, 24) (1111, 4) (1111, 24) (1111,)\n",
      "dtypes: float64 float32 float64 float64\n",
      "states: 0.99992806 -0.9998551\n",
      "actions: 2.445514678955078 -2.266592502593994\n",
      "rewards: 2.445514678955078 -2.266592502593994\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.74703396e-03,  3.15252488e-06, -4.11065966e-04,\n",
       "        -1.60000205e-02,  9.22507718e-02,  9.54264484e-04,\n",
       "         8.60050470e-01,  6.44485272e-04,  1.00000000e+00,\n",
       "         3.26296426e-02,  9.54228162e-04,  8.53657931e-01,\n",
       "        -6.49294195e-04,  1.00000000e+00,  4.40813839e-01,\n",
       "         4.45819944e-01,  4.61422592e-01,  4.89549994e-01,\n",
       "         5.34102559e-01,  6.02460802e-01,  7.09148586e-01,\n",
       "         8.85931492e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 2.46125250e-03, -6.90751910e-03,  5.26649058e-03,\n",
       "         1.96621442e-02, -2.90470362e-01, -7.09619224e-01,\n",
       "         1.47205219e+00,  9.93585507e-01,  1.00000000e+00,\n",
       "         3.00898015e-01, -1.58208907e-02,  1.63350701e-01,\n",
       "         3.32505027e-01,  1.00000000e+00,  4.52898622e-01,\n",
       "         4.58041966e-01,  4.74072367e-01,  5.02970874e-01,\n",
       "         5.48744857e-01,  6.18977070e-01,  7.28589714e-01,\n",
       "         9.10219014e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 2.56516994e-03,  2.57482082e-03,  7.42817447e-03,\n",
       "         6.84252322e-03, -1.35565288e-02, -3.95231038e-01,\n",
       "         1.08347261e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         2.60842085e-01, -4.16364878e-01,  2.69976616e-01,\n",
       "         2.41971036e-01,  1.00000000e+00,  4.56012309e-01,\n",
       "         4.61191028e-01,  4.77331609e-01,  5.06428778e-01,\n",
       "         5.52517474e-01,  6.23232543e-01,  7.33598769e-01,\n",
       "         9.16476786e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-2.49468628e-02, -5.56278229e-02, -3.40317643e-02,\n",
       "        -1.13800514e-02,  4.47474122e-02,  9.91529047e-01,\n",
       "         9.97868437e-01, -6.24300480e-01,  1.00000000e+00,\n",
       "         3.42752606e-01,  9.34816599e-01,  2.12402284e-01,\n",
       "        -5.33377687e-01,  1.00000000e+00,  4.56193268e-01,\n",
       "         4.61374015e-01,  4.77521032e-01,  5.06629765e-01,\n",
       "         5.52736759e-01,  6.23479843e-01,  7.33889878e-01,\n",
       "         9.16840494e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-3.80703323e-02, -2.68175364e-02, -1.19944015e-02,\n",
       "        -3.09339575e-04,  1.67637676e-01,  1.97930768e-01,\n",
       "         4.35544670e-01,  5.46280295e-04,  1.00000000e+00,\n",
       "         3.60862792e-01,  4.28078115e-01,  1.38610840e-01,\n",
       "        -9.99812524e-01,  0.00000000e+00,  4.60175902e-01,\n",
       "         4.65401888e-01,  4.81689870e-01,  5.11052728e-01,\n",
       "         5.57562232e-01,  6.28922939e-01,  7.40296841e-01,\n",
       "         9.24844623e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-4.03079912e-02, -4.49754328e-03,  2.25704169e-02,\n",
       "         7.42304182e-02,  1.34206742e-01, -4.19146717e-01,\n",
       "         4.96328235e-01,  5.90760152e-01,  1.00000000e+00,\n",
       "         3.30577970e-01, -3.65413249e-01,  2.60667682e-01,\n",
       "         1.00000032e+00,  0.00000000e+00,  4.63641405e-01,\n",
       "         4.68906760e-01,  4.85317379e-01,  5.14901340e-01,\n",
       "         5.61761081e-01,  6.33659244e-01,  7.45871902e-01,\n",
       "         9.31809485e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-5.75147681e-02, -3.44387031e-02,  1.45856452e-02,\n",
       "         6.76961470e-02,  1.20861158e-01, -2.00155124e-01,\n",
       "         5.64341068e-01,  5.86016138e-01,  1.00000000e+00,\n",
       "         3.13870877e-01, -1.87838390e-01,  3.84789944e-01,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.66745257e-01,\n",
       "         4.72045839e-01,  4.88566339e-01,  5.18348396e-01,\n",
       "         5.65521836e-01,  6.37901306e-01,  7.50865161e-01,\n",
       "         9.38047469e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-4.73520234e-02,  2.02890229e-02,  5.20554042e-02,\n",
       "         8.52169895e-02,  4.59814928e-02, -9.23392236e-01,\n",
       "         6.85922951e-01,  9.97835318e-01,  1.00000000e+00,\n",
       "         2.33311281e-01, -1.00027120e+00,  5.06803274e-01,\n",
       "         9.99995311e-01,  0.00000000e+00,  4.70752120e-01,\n",
       "         4.76098210e-01,  4.92760539e-01,  5.22798240e-01,\n",
       "         5.70376635e-01,  6.43377483e-01,  7.57311106e-01,\n",
       "         9.46100354e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-3.24903429e-02,  2.96887922e-02,  4.41669488e-02,\n",
       "         1.06342182e-01,  2.57788226e-02, -2.41977870e-01,\n",
       "         6.18657351e-01, -5.74788173e-01,  1.00000000e+00,\n",
       "         1.52796865e-01, -9.97801781e-01,  6.27741098e-01,\n",
       "         9.86834685e-01,  1.00000000e+00,  4.75759804e-01,\n",
       "         4.81162757e-01,  4.98002350e-01,  5.28359532e-01,\n",
       "         5.76444089e-01,  6.50221467e-01,  7.65367091e-01,\n",
       "         9.56164598e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-1.63465012e-02,  3.22830248e-02,  5.29841959e-02,\n",
       "         9.19312191e-02, -5.31892292e-02, -9.81524229e-01,\n",
       "         7.29306459e-01,  9.08061107e-01,  0.00000000e+00,\n",
       "         7.22351819e-02, -9.99998569e-01,  7.49924600e-01,\n",
       "         1.00000262e+00,  1.00000000e+00,  4.80081737e-01,\n",
       "         4.85533774e-01,  5.02526343e-01,  5.33159316e-01,\n",
       "         5.81680655e-01,  6.56128287e-01,  7.72319913e-01,\n",
       "         9.64850664e-01,  1.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.46125250e-03, -6.90751910e-03,  5.26649058e-03,\n",
       "         1.96621442e-02, -2.90470362e-01, -7.09619224e-01,\n",
       "         1.47205219e+00,  9.93585507e-01,  1.00000000e+00,\n",
       "         3.00898015e-01, -1.58208907e-02,  1.63350701e-01,\n",
       "         3.32505027e-01,  1.00000000e+00,  4.52898622e-01,\n",
       "         4.58041966e-01,  4.74072367e-01,  5.02970874e-01,\n",
       "         5.48744857e-01,  6.18977070e-01,  7.28589714e-01,\n",
       "         9.10219014e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 2.56516994e-03,  2.57482082e-03,  7.42817447e-03,\n",
       "         6.84252322e-03, -1.35565288e-02, -3.95231038e-01,\n",
       "         1.08347261e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         2.60842085e-01, -4.16364878e-01,  2.69976616e-01,\n",
       "         2.41971036e-01,  1.00000000e+00,  4.56012309e-01,\n",
       "         4.61191028e-01,  4.77331609e-01,  5.06428778e-01,\n",
       "         5.52517474e-01,  6.23232543e-01,  7.33598769e-01,\n",
       "         9.16476786e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-2.49468628e-02, -5.56278229e-02, -3.40317643e-02,\n",
       "        -1.13800514e-02,  4.47474122e-02,  9.91529047e-01,\n",
       "         9.97868437e-01, -6.24300480e-01,  1.00000000e+00,\n",
       "         3.42752606e-01,  9.34816599e-01,  2.12402284e-01,\n",
       "        -5.33377687e-01,  1.00000000e+00,  4.56193268e-01,\n",
       "         4.61374015e-01,  4.77521032e-01,  5.06629765e-01,\n",
       "         5.52736759e-01,  6.23479843e-01,  7.33889878e-01,\n",
       "         9.16840494e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-3.80703323e-02, -2.68175364e-02, -1.19944015e-02,\n",
       "        -3.09339575e-04,  1.67637676e-01,  1.97930768e-01,\n",
       "         4.35544670e-01,  5.46280295e-04,  1.00000000e+00,\n",
       "         3.60862792e-01,  4.28078115e-01,  1.38610840e-01,\n",
       "        -9.99812524e-01,  0.00000000e+00,  4.60175902e-01,\n",
       "         4.65401888e-01,  4.81689870e-01,  5.11052728e-01,\n",
       "         5.57562232e-01,  6.28922939e-01,  7.40296841e-01,\n",
       "         9.24844623e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-4.03079912e-02, -4.49754328e-03,  2.25704169e-02,\n",
       "         7.42304182e-02,  1.34206742e-01, -4.19146717e-01,\n",
       "         4.96328235e-01,  5.90760152e-01,  1.00000000e+00,\n",
       "         3.30577970e-01, -3.65413249e-01,  2.60667682e-01,\n",
       "         1.00000032e+00,  0.00000000e+00,  4.63641405e-01,\n",
       "         4.68906760e-01,  4.85317379e-01,  5.14901340e-01,\n",
       "         5.61761081e-01,  6.33659244e-01,  7.45871902e-01,\n",
       "         9.31809485e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-5.75147681e-02, -3.44387031e-02,  1.45856452e-02,\n",
       "         6.76961470e-02,  1.20861158e-01, -2.00155124e-01,\n",
       "         5.64341068e-01,  5.86016138e-01,  1.00000000e+00,\n",
       "         3.13870877e-01, -1.87838390e-01,  3.84789944e-01,\n",
       "         1.00000000e+00,  0.00000000e+00,  4.66745257e-01,\n",
       "         4.72045839e-01,  4.88566339e-01,  5.18348396e-01,\n",
       "         5.65521836e-01,  6.37901306e-01,  7.50865161e-01,\n",
       "         9.38047469e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-4.73520234e-02,  2.02890229e-02,  5.20554042e-02,\n",
       "         8.52169895e-02,  4.59814928e-02, -9.23392236e-01,\n",
       "         6.85922951e-01,  9.97835318e-01,  1.00000000e+00,\n",
       "         2.33311281e-01, -1.00027120e+00,  5.06803274e-01,\n",
       "         9.99995311e-01,  0.00000000e+00,  4.70752120e-01,\n",
       "         4.76098210e-01,  4.92760539e-01,  5.22798240e-01,\n",
       "         5.70376635e-01,  6.43377483e-01,  7.57311106e-01,\n",
       "         9.46100354e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-3.24903429e-02,  2.96887922e-02,  4.41669488e-02,\n",
       "         1.06342182e-01,  2.57788226e-02, -2.41977870e-01,\n",
       "         6.18657351e-01, -5.74788173e-01,  1.00000000e+00,\n",
       "         1.52796865e-01, -9.97801781e-01,  6.27741098e-01,\n",
       "         9.86834685e-01,  1.00000000e+00,  4.75759804e-01,\n",
       "         4.81162757e-01,  4.98002350e-01,  5.28359532e-01,\n",
       "         5.76444089e-01,  6.50221467e-01,  7.65367091e-01,\n",
       "         9.56164598e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-1.63465012e-02,  3.22830248e-02,  5.29841959e-02,\n",
       "         9.19312191e-02, -5.31892292e-02, -9.81524229e-01,\n",
       "         7.29306459e-01,  9.08061107e-01,  0.00000000e+00,\n",
       "         7.22351819e-02, -9.99998569e-01,  7.49924600e-01,\n",
       "         1.00000262e+00,  1.00000000e+00,  4.80081737e-01,\n",
       "         4.85533774e-01,  5.02526343e-01,  5.33159316e-01,\n",
       "         5.81680655e-01,  6.56128287e-01,  7.72319913e-01,\n",
       "         9.64850664e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [-2.07865573e-02, -8.85009289e-03,  2.96799427e-02,\n",
       "         7.39010382e-02, -9.98542011e-02, -5.81232786e-01,\n",
       "         8.50447938e-01,  1.00000008e+00,  0.00000000e+00,\n",
       "         5.20804040e-02, -2.43193299e-01,  8.71739805e-01,\n",
       "         9.99999921e-01,  0.00000000e+00,  4.83457834e-01,\n",
       "         4.88948226e-01,  5.06060243e-01,  5.36908686e-01,\n",
       "         5.85771263e-01,  6.60742402e-01,  7.77751088e-01,\n",
       "         9.71635818e-01,  1.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9202328286156476, 0.09392780759037227)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.024455146789550783 -0.02266592502593994\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.33061767,  0.5409548 ,  0.83249   , -0.9815165 ], dtype=float32),\n",
       " array([-0.5404586 , -0.54526246,  0.5219986 ,  0.6396351 ], dtype=float32),\n",
       " array([0.7587739 , 1.179868  , 0.09640493, 2.6281323 ], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][1], batch[2][1], np.square(batch[1][1] - batch[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    # G\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    #actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    actions_labels = tf.nn.sigmoid(actions)\n",
    "    # neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "    #                                                                   labels=actions_labels)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=actions_labels)\n",
    "    #g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs) # error!\n",
    "    \n",
    "    # D\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * Qs)\n",
    "    return actions_logits, Qs, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 24) actions:(1000, 4)\n",
      "action size:371.38470458984375\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "# state_size = 37\n",
    "# state_size_ = (84, 84, 3)\n",
    "state_size = 24\n",
    "action_size = 4\n",
    "hidden_size = 24*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "gamma = 0.99                   # future reward discount\n",
    "memory_size = 1000            # memory capacity\n",
    "batch_size = 1000             # experience mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-114.8592 R:-114.85924937840878 gloss:-0.1710 dloss:10.1072 exploreP:0.9917\n",
      "Episode:1 meanR:-108.6199 R:-102.38057620347281 gloss:-168.8777 dloss:9.7398 exploreP:0.8466\n",
      "Episode:2 meanR:-106.4612 R:-102.14365668013754 gloss:-192.7809 dloss:0.2265 exploreP:0.8423\n",
      "Episode:3 meanR:-105.9194 R:-104.29425493636495 gloss:-322.3089 dloss:8.8371 exploreP:0.8379\n",
      "Episode:4 meanR:-104.6380 R:-99.5121267642888 gloss:-530.3848 dloss:16.1615 exploreP:0.8299\n",
      "Episode:5 meanR:-108.2965 R:-126.58929302986353 gloss:-898.6055 dloss:22.1505 exploreP:0.8124\n",
      "Episode:6 meanR:-107.4127 R:-102.10977879420327 gloss:-1227.1403 dloss:26.8314 exploreP:0.8049\n",
      "Episode:7 meanR:-106.5047 R:-100.14898194273127 gloss:-1560.2084 dloss:30.1961 exploreP:0.7985\n",
      "Episode:8 meanR:-105.8458 R:-100.57420357320645 gloss:-1874.5610 dloss:32.5871 exploreP:0.7924\n",
      "Episode:9 meanR:-106.1337 R:-108.72536417224816 gloss:-2695.7964 dloss:32.8745 exploreP:0.7785\n",
      "Episode:10 meanR:-113.9604 R:-192.22689733556513 gloss:-3952.4688 dloss:18.6976 exploreP:0.7033\n",
      "Episode:11 meanR:-114.2270 R:-117.16008308190388 gloss:-1968.3427 dloss:7.2425 exploreP:0.6996\n",
      "Episode:12 meanR:-114.5997 R:-119.07221587642346 gloss:-2734.5691 dloss:12.0343 exploreP:0.6885\n",
      "Episode:13 meanR:-114.8782 R:-118.49843502419256 gloss:-3424.2190 dloss:15.9564 exploreP:0.6854\n",
      "Episode:14 meanR:-114.3988 R:-107.68681001666545 gloss:-3963.8540 dloss:19.2271 exploreP:0.6817\n",
      "Episode:15 meanR:-114.0260 R:-108.43337575153603 gloss:-4575.9585 dloss:20.9060 exploreP:0.6780\n",
      "Episode:16 meanR:-114.3291 R:-119.17998656419044 gloss:-5046.5073 dloss:21.9864 exploreP:0.6748\n",
      "Episode:17 meanR:-114.3628 R:-114.93548029167826 gloss:-5421.7031 dloss:23.2636 exploreP:0.6704\n",
      "Episode:18 meanR:-114.0600 R:-108.61015374707182 gloss:-6004.0093 dloss:22.6231 exploreP:0.6669\n",
      "Episode:19 meanR:-114.4491 R:-121.84191677692533 gloss:-6419.7104 dloss:21.7572 exploreP:0.6610\n",
      "Episode:20 meanR:-114.9603 R:-125.18441958045959 gloss:-6723.7534 dloss:20.5566 exploreP:0.6561\n",
      "Episode:21 meanR:-115.0564 R:-117.07299182296995 gloss:-6873.9126 dloss:19.4890 exploreP:0.6516\n",
      "Episode:22 meanR:-115.3764 R:-122.4160407884065 gloss:-6913.4404 dloss:19.1559 exploreP:0.6457\n",
      "Episode:23 meanR:-121.1395 R:-253.69283072744292 gloss:-4747.5664 dloss:10.8639 exploreP:0.5550\n",
      "Episode:24 meanR:-120.3830 R:-102.22687934934969 gloss:-1993.7533 dloss:7.4920 exploreP:0.5524\n",
      "Episode:25 meanR:-119.6508 R:-101.34552620518456 gloss:-2617.3857 dloss:12.9919 exploreP:0.5498\n",
      "Episode:26 meanR:-119.0882 R:-104.4586020393856 gloss:-3411.1755 dloss:16.0741 exploreP:0.5461\n",
      "Episode:27 meanR:-118.3429 R:-98.21970136713733 gloss:-3975.6753 dloss:18.4967 exploreP:0.5416\n",
      "Episode:28 meanR:-117.7014 R:-99.7406155288144 gloss:-4347.8110 dloss:20.0876 exploreP:0.5384\n",
      "Episode:29 meanR:-117.7967 R:-120.56046126162597 gloss:-5414.9189 dloss:20.8228 exploreP:0.5326\n",
      "Episode:30 meanR:-117.2810 R:-101.80975101631631 gloss:-6565.5557 dloss:21.8462 exploreP:0.5290\n",
      "Episode:31 meanR:-116.8681 R:-104.0688191228596 gloss:-7247.1567 dloss:22.6903 exploreP:0.5264\n",
      "Episode:32 meanR:-117.2650 R:-129.96698960114583 gloss:-7996.4917 dloss:23.2732 exploreP:0.5197\n",
      "Episode:33 meanR:-116.8965 R:-104.73566383847843 gloss:-9026.0830 dloss:21.7854 exploreP:0.5163\n",
      "Episode:34 meanR:-116.5020 R:-103.08640986991301 gloss:-9425.0605 dloss:22.3180 exploreP:0.5124\n",
      "Episode:35 meanR:-116.1127 R:-102.48939361306218 gloss:-9822.3857 dloss:22.6903 exploreP:0.5090\n",
      "Episode:36 meanR:-115.8460 R:-106.24288805795027 gloss:-10004.8701 dloss:22.4885 exploreP:0.5050\n",
      "Episode:37 meanR:-115.4843 R:-102.10090337266338 gloss:-9687.6016 dloss:22.4472 exploreP:0.5008\n",
      "Episode:38 meanR:-115.2057 R:-104.62068394870931 gloss:-9236.7246 dloss:22.2601 exploreP:0.4979\n",
      "Episode:39 meanR:-114.9245 R:-103.95727641689095 gloss:-8935.9502 dloss:22.2660 exploreP:0.4955\n",
      "Episode:40 meanR:-114.8088 R:-110.17956579127691 gloss:-8333.5918 dloss:22.1806 exploreP:0.4893\n",
      "Episode:41 meanR:-115.0857 R:-126.43883094963492 gloss:-7518.5737 dloss:21.7480 exploreP:0.4836\n",
      "Episode:42 meanR:-114.7969 R:-102.67037795021943 gloss:-7355.8550 dloss:21.5280 exploreP:0.4805\n",
      "Episode:43 meanR:-114.5992 R:-106.09414892505916 gloss:-7309.2197 dloss:21.3048 exploreP:0.4756\n",
      "Episode:44 meanR:-114.3156 R:-101.83874717333633 gloss:-7262.8184 dloss:21.8654 exploreP:0.4717\n",
      "Episode:45 meanR:-114.1060 R:-104.67627221439034 gloss:-6962.1689 dloss:22.0321 exploreP:0.4688\n",
      "Episode:46 meanR:-113.9333 R:-105.98605031989608 gloss:-6852.6260 dloss:22.1840 exploreP:0.4662\n",
      "Episode:47 meanR:-113.7527 R:-105.26395684568521 gloss:-6680.3237 dloss:21.9962 exploreP:0.4624\n",
      "Episode:48 meanR:-113.5701 R:-104.80911493668115 gloss:-6499.7656 dloss:21.8968 exploreP:0.4586\n",
      "Episode:49 meanR:-113.4674 R:-108.43131248311326 gloss:-7033.4907 dloss:22.1109 exploreP:0.4545\n",
      "Episode:50 meanR:-114.1870 R:-150.16975260235742 gloss:-6901.3477 dloss:11.3276 exploreP:0.3888\n",
      "Episode:51 meanR:-114.8964 R:-151.07727064734857 gloss:-1493.2124 dloss:0.1885 exploreP:0.3328\n",
      "Episode:52 meanR:-115.6122 R:-152.83032537323513 gloss:-1471.8097 dloss:0.1049 exploreP:0.2851\n",
      "Episode:53 meanR:-116.0242 R:-137.86021133608682 gloss:-2037.4688 dloss:0.1195 exploreP:0.2444\n",
      "Episode:54 meanR:-116.6250 R:-149.07010702141505 gloss:-2427.4595 dloss:0.0823 exploreP:0.2097\n",
      "Episode:55 meanR:-117.2474 R:-151.4766749177717 gloss:-2594.6646 dloss:0.0801 exploreP:0.1802\n",
      "Episode:56 meanR:-117.9254 R:-155.8925990254225 gloss:-2499.9500 dloss:0.0858 exploreP:0.1550\n",
      "Episode:57 meanR:-118.6534 R:-160.15163720242498 gloss:-2444.9412 dloss:0.0791 exploreP:0.1336\n",
      "Episode:58 meanR:-119.3238 R:-158.21019703083516 gloss:-2454.5225 dloss:0.0668 exploreP:0.1153\n",
      "Episode:59 meanR:-120.0757 R:-164.4334842253057 gloss:-2258.8772 dloss:0.0750 exploreP:0.0997\n",
      "Episode:60 meanR:-122.0363 R:-239.67163743109032 gloss:-2011.0784 dloss:0.2077 exploreP:0.0885\n",
      "Episode:61 meanR:-121.9614 R:-117.39332395188883 gloss:-4188.8237 dloss:7.4453 exploreP:0.0882\n",
      "Episode:62 meanR:-121.8838 R:-117.07296807606394 gloss:-6806.6304 dloss:12.7183 exploreP:0.0871\n",
      "Episode:63 meanR:-121.8150 R:-117.47881151610551 gloss:-13244.4717 dloss:15.9868 exploreP:0.0861\n",
      "Episode:64 meanR:-121.7386 R:-116.85225664900956 gloss:-18319.1836 dloss:19.1490 exploreP:0.0850\n",
      "Episode:65 meanR:-121.6421 R:-115.36606169658528 gloss:-16949.4902 dloss:19.6827 exploreP:0.0840\n",
      "Episode:66 meanR:-121.5741 R:-117.09138616399775 gloss:-12648.2324 dloss:20.2085 exploreP:0.0830\n",
      "Episode:67 meanR:-121.5585 R:-120.51293204027476 gloss:-11212.2578 dloss:21.3301 exploreP:0.0827\n",
      "Episode:68 meanR:-121.4802 R:-116.15747674329953 gloss:-12587.9297 dloss:21.7183 exploreP:0.0822\n",
      "Episode:69 meanR:-121.4099 R:-116.5528376525895 gloss:-13232.2783 dloss:25.8934 exploreP:0.0816\n",
      "Episode:70 meanR:-121.3012 R:-113.69725918364712 gloss:-13597.2324 dloss:28.0370 exploreP:0.0809\n",
      "Episode:71 meanR:-121.1883 R:-113.16899201614729 gloss:-13163.2939 dloss:22.0930 exploreP:0.0802\n",
      "Episode:72 meanR:-121.1846 R:-120.91588872528519 gloss:-12927.6758 dloss:21.6284 exploreP:0.0791\n",
      "Episode:73 meanR:-121.2241 R:-124.10808759779607 gloss:-13607.9854 dloss:20.5176 exploreP:0.0788\n",
      "Episode:74 meanR:-121.1739 R:-117.46414252135341 gloss:-13174.2393 dloss:20.5453 exploreP:0.0778\n",
      "Episode:75 meanR:-121.1009 R:-115.6222407679842 gloss:-13456.1670 dloss:21.1799 exploreP:0.0768\n",
      "Episode:76 meanR:-121.1171 R:-122.35363741432553 gloss:-13737.7998 dloss:19.9760 exploreP:0.0755\n",
      "Episode:77 meanR:-121.0821 R:-118.38479244454639 gloss:-12550.1387 dloss:19.3345 exploreP:0.0745\n",
      "Episode:78 meanR:-121.0247 R:-116.54756700918576 gloss:-11880.1348 dloss:18.6135 exploreP:0.0737\n",
      "Episode:79 meanR:-120.9645 R:-116.20947760525968 gloss:-10635.6201 dloss:18.4372 exploreP:0.0729\n",
      "Episode:80 meanR:-120.9215 R:-117.47673571493725 gloss:-9703.9160 dloss:18.4764 exploreP:0.0721\n",
      "Episode:81 meanR:-120.8225 R:-112.8086651489387 gloss:-9561.0430 dloss:18.6414 exploreP:0.0716\n",
      "Episode:82 meanR:-120.7767 R:-117.02243156083176 gloss:-9793.3223 dloss:18.7496 exploreP:0.0707\n",
      "Episode:83 meanR:-120.7097 R:-115.14682049913581 gloss:-9858.2920 dloss:18.7768 exploreP:0.0700\n",
      "Episode:84 meanR:-120.6347 R:-114.33565163187187 gloss:-10644.8867 dloss:19.2162 exploreP:0.0693\n",
      "Episode:85 meanR:-120.5510 R:-113.43168408571248 gloss:-10063.7314 dloss:19.8505 exploreP:0.0687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:86 meanR:-120.4995 R:-116.07647661913839 gloss:-10345.1719 dloss:19.0645 exploreP:0.0679\n",
      "Episode:87 meanR:-120.4476 R:-115.92631605460558 gloss:-10892.2275 dloss:19.0780 exploreP:0.0672\n",
      "Episode:88 meanR:-120.3965 R:-115.89721876847497 gloss:-11938.5029 dloss:19.1622 exploreP:0.0666\n",
      "Episode:89 meanR:-120.3703 R:-118.04298236798371 gloss:-13254.1963 dloss:19.2267 exploreP:0.0663\n",
      "Episode:90 meanR:-120.3513 R:-118.6430174836839 gloss:-13442.5225 dloss:19.6240 exploreP:0.0656\n",
      "Episode:91 meanR:-120.3287 R:-118.26627160261074 gloss:-13646.7520 dloss:18.5089 exploreP:0.0649\n",
      "Episode:92 meanR:-120.3529 R:-122.58163401370558 gloss:-14017.4375 dloss:21.3557 exploreP:0.0647\n",
      "Episode:93 meanR:-120.3057 R:-115.91594035798437 gloss:-12668.5947 dloss:18.4383 exploreP:0.0640\n",
      "Episode:94 meanR:-120.2591 R:-115.87749728827708 gloss:-11431.5420 dloss:18.5395 exploreP:0.0632\n",
      "Episode:95 meanR:-120.3125 R:-125.39153566696639 gloss:-11428.4512 dloss:23.0321 exploreP:0.0619\n",
      "Episode:96 meanR:-120.2935 R:-118.4685278814702 gloss:-12776.5459 dloss:20.1797 exploreP:0.0612\n",
      "Episode:97 meanR:-120.5394 R:-144.39128602157174 gloss:-12746.5449 dloss:20.3892 exploreP:0.0597\n",
      "Episode:98 meanR:-120.5164 R:-118.25880645046941 gloss:-12866.3516 dloss:20.2576 exploreP:0.0590\n",
      "Episode:99 meanR:-120.4963 R:-118.50857419838073 gloss:-20686.7285 dloss:27.0388 exploreP:0.0588\n",
      "Episode:100 meanR:-120.5680 R:-122.0312969057335 gloss:-25934.8398 dloss:30.7580 exploreP:0.0579\n",
      "Episode:101 meanR:-120.7069 R:-116.2660811509428 gloss:-31651.4512 dloss:29.0273 exploreP:0.0572\n",
      "Episode:102 meanR:-120.8492 R:-116.37983553951607 gloss:-35429.5898 dloss:24.1941 exploreP:0.0566\n",
      "Episode:103 meanR:-121.0416 R:-123.526361768183 gloss:-42912.6016 dloss:33.0928 exploreP:0.0564\n",
      "Episode:104 meanR:-121.2827 R:-123.62925782292896 gloss:-47609.7188 dloss:38.0458 exploreP:0.0562\n",
      "Episode:105 meanR:-121.2523 R:-123.54828964108974 gloss:-52653.1016 dloss:39.8528 exploreP:0.0560\n",
      "Episode:106 meanR:-121.4651 R:-123.38497402890337 gloss:-59208.1445 dloss:45.8791 exploreP:0.0558\n",
      "Episode:107 meanR:-121.6978 R:-123.41827141295934 gloss:-65205.1328 dloss:40.0975 exploreP:0.0556\n",
      "Episode:108 meanR:-121.9226 R:-123.06133134017823 gloss:-67415.0938 dloss:33.2594 exploreP:0.0554\n",
      "Episode:109 meanR:-122.0737 R:-123.82790853342104 gloss:-70512.6406 dloss:31.5408 exploreP:0.0553\n",
      "Episode:110 meanR:-121.3909 R:-123.94877862941786 gloss:-72910.5000 dloss:38.2813 exploreP:0.0551\n",
      "Episode:111 meanR:-121.4583 R:-123.90056103869901 gloss:-76879.8438 dloss:28.7641 exploreP:0.0549\n",
      "Episode:112 meanR:-121.4803 R:-121.27119055441892 gloss:-79534.7969 dloss:26.7469 exploreP:0.0547\n",
      "Episode:113 meanR:-121.5290 R:-123.36648254153877 gloss:-81318.2812 dloss:26.8909 exploreP:0.0545\n",
      "Episode:114 meanR:-121.6873 R:-123.51970264505533 gloss:-81828.0000 dloss:28.9815 exploreP:0.0543\n",
      "Episode:115 meanR:-121.8393 R:-123.63246191872719 gloss:-82898.0547 dloss:26.4790 exploreP:0.0541\n",
      "Episode:116 meanR:-121.8833 R:-123.58109612939073 gloss:-80295.1641 dloss:24.3391 exploreP:0.0539\n",
      "Episode:117 meanR:-121.9681 R:-123.41713961128085 gloss:-76453.6484 dloss:28.2983 exploreP:0.0538\n",
      "Episode:118 meanR:-122.1176 R:-123.55874132434343 gloss:-74510.9609 dloss:29.0972 exploreP:0.0536\n",
      "Episode:119 meanR:-122.0851 R:-118.58984120600547 gloss:-68753.3281 dloss:23.5731 exploreP:0.0533\n",
      "Episode:120 meanR:-122.0680 R:-123.47364457432866 gloss:-56155.0898 dloss:25.1565 exploreP:0.0531\n",
      "Episode:121 meanR:-122.1311 R:-123.38843848865915 gloss:-48877.7656 dloss:22.5039 exploreP:0.0529\n",
      "Episode:122 meanR:-122.1361 R:-122.91812343986024 gloss:-40989.5352 dloss:21.1145 exploreP:0.0528\n",
      "Episode:123 meanR:-120.8347 R:-123.54533071517511 gloss:-32794.5664 dloss:19.9462 exploreP:0.0526\n",
      "Episode:124 meanR:-121.0844 R:-127.19665709318384 gloss:-14740.9756 dloss:16.9090 exploreP:0.0516\n",
      "Episode:125 meanR:-121.3066 R:-123.56900449597649 gloss:-12048.4590 dloss:15.0204 exploreP:0.0514\n",
      "Episode:126 meanR:-121.4974 R:-123.5395563013802 gloss:-11702.5273 dloss:14.6293 exploreP:0.0513\n",
      "Episode:127 meanR:-121.7492 R:-123.40032702505837 gloss:-11089.1826 dloss:14.2702 exploreP:0.0511\n",
      "Episode:128 meanR:-121.9648 R:-121.29644324752192 gloss:-10704.7393 dloss:13.4051 exploreP:0.0509\n",
      "Episode:129 meanR:-121.9913 R:-123.21692034169412 gloss:-10832.6025 dloss:13.2551 exploreP:0.0507\n",
      "Episode:130 meanR:-122.1772 R:-120.39385410237003 gloss:-11064.0908 dloss:13.2094 exploreP:0.0506\n",
      "Episode:131 meanR:-122.3249 R:-118.84218866900231 gloss:-11391.5938 dloss:13.6724 exploreP:0.0503\n",
      "Episode:132 meanR:-122.2582 R:-123.29954157922441 gloss:-11325.0820 dloss:13.4848 exploreP:0.0501\n",
      "Episode:133 meanR:-122.4459 R:-123.50730663738835 gloss:-11695.5605 dloss:13.2964 exploreP:0.0500\n",
      "Episode:134 meanR:-122.6489 R:-123.38638667337659 gloss:-11970.9307 dloss:13.1438 exploreP:0.0498\n",
      "Episode:135 meanR:-122.8577 R:-123.36783681216527 gloss:-12219.4951 dloss:13.1307 exploreP:0.0496\n",
      "Episode:136 meanR:-123.0314 R:-123.60768144559862 gloss:-12096.8242 dloss:16.9874 exploreP:0.0495\n",
      "Episode:137 meanR:-123.2261 R:-121.5758466064874 gloss:-11905.8887 dloss:13.9400 exploreP:0.0493\n",
      "Episode:138 meanR:-123.4145 R:-123.45529180021336 gloss:-11872.4893 dloss:12.5706 exploreP:0.0491\n",
      "Episode:139 meanR:-123.6102 R:-123.52532586787456 gloss:-11912.1895 dloss:11.5260 exploreP:0.0490\n",
      "Episode:140 meanR:-123.7498 R:-124.14776752696062 gloss:-11931.0654 dloss:11.1748 exploreP:0.0488\n",
      "Episode:141 meanR:-123.7206 R:-123.51780696529212 gloss:-11646.9775 dloss:10.9682 exploreP:0.0486\n",
      "Episode:142 meanR:-123.9303 R:-123.6338624939242 gloss:-11319.0693 dloss:10.8256 exploreP:0.0485\n",
      "Episode:143 meanR:-124.1049 R:-123.56022873111255 gloss:-10857.2227 dloss:10.4236 exploreP:0.0483\n",
      "Episode:144 meanR:-124.3209 R:-123.43901481368889 gloss:-10832.2266 dloss:10.0588 exploreP:0.0482\n",
      "Episode:145 meanR:-124.5105 R:-123.63372385972926 gloss:-11216.8311 dloss:9.8835 exploreP:0.0480\n",
      "Episode:146 meanR:-124.6863 R:-123.57028191192822 gloss:-11132.7695 dloss:9.6437 exploreP:0.0478\n",
      "Episode:147 meanR:-124.8621 R:-122.83756178286797 gloss:-10975.7637 dloss:8.7483 exploreP:0.0477\n",
      "Episode:148 meanR:-125.0486 R:-123.46379868024712 gloss:-11421.1309 dloss:8.4252 exploreP:0.0475\n",
      "Episode:149 meanR:-125.2035 R:-123.92315710807466 gloss:-11372.5186 dloss:7.6938 exploreP:0.0474\n",
      "Episode:150 meanR:-124.9385 R:-123.66451319735435 gloss:-11673.7764 dloss:7.5951 exploreP:0.0472\n",
      "Episode:151 meanR:-124.6590 R:-123.13250881794033 gloss:-11616.6787 dloss:7.6304 exploreP:0.0470\n",
      "Episode:152 meanR:-124.3682 R:-123.75032327747098 gloss:-12272.0117 dloss:7.4590 exploreP:0.0469\n",
      "Episode:153 meanR:-124.2287 R:-123.9023034704222 gloss:-12126.0830 dloss:7.5422 exploreP:0.0467\n",
      "Episode:154 meanR:-123.9730 R:-123.5052284654025 gloss:-11472.3848 dloss:6.6218 exploreP:0.0466\n",
      "Episode:155 meanR:-123.6921 R:-123.38634379256952 gloss:-10864.3701 dloss:6.0245 exploreP:0.0464\n",
      "Episode:156 meanR:-123.3677 R:-123.45424093513935 gloss:-10232.0020 dloss:5.4496 exploreP:0.0463\n",
      "Episode:157 meanR:-123.0025 R:-123.62743092551021 gloss:-10018.8652 dloss:5.2385 exploreP:0.0461\n",
      "Episode:158 meanR:-122.6514 R:-123.09705939677792 gloss:-10266.2920 dloss:5.4008 exploreP:0.0460\n",
      "Episode:159 meanR:-122.2364 R:-122.94115504903596 gloss:-10909.8838 dloss:5.1146 exploreP:0.0458\n",
      "Episode:160 meanR:-121.0739 R:-123.42167178555766 gloss:-11440.4570 dloss:5.3160 exploreP:0.0457\n",
      "Episode:161 meanR:-121.1355 R:-123.5532475354479 gloss:-11140.5645 dloss:4.9334 exploreP:0.0455\n",
      "Episode:162 meanR:-121.1993 R:-123.4538097456672 gloss:-11211.6182 dloss:5.1119 exploreP:0.0454\n",
      "Episode:163 meanR:-121.2592 R:-123.46224522584987 gloss:-11190.6729 dloss:5.2126 exploreP:0.0452\n",
      "Episode:164 meanR:-121.3253 R:-123.46661024281073 gloss:-11021.2012 dloss:4.7578 exploreP:0.0451\n",
      "Episode:165 meanR:-121.4001 R:-122.84512766218495 gloss:-11311.0801 dloss:4.7764 exploreP:0.0449\n",
      "Episode:166 meanR:-121.4647 R:-123.54544543661612 gloss:-11384.8740 dloss:5.1407 exploreP:0.0448\n",
      "Episode:167 meanR:-121.4940 R:-123.44863249287445 gloss:-11477.3574 dloss:4.6732 exploreP:0.0446\n",
      "Episode:168 meanR:-121.5662 R:-123.3775436794696 gloss:-11490.1787 dloss:4.9411 exploreP:0.0445\n",
      "Episode:169 meanR:-121.6344 R:-123.37013592852031 gloss:-11701.9229 dloss:4.8303 exploreP:0.0444\n",
      "Episode:170 meanR:-121.7296 R:-123.21550778454358 gloss:-12411.6191 dloss:4.8458 exploreP:0.0442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:171 meanR:-121.8335 R:-123.56139495399906 gloss:-11883.5381 dloss:5.0000 exploreP:0.0441\n",
      "Episode:172 meanR:-121.7854 R:-116.10234185767112 gloss:-12110.9902 dloss:5.8873 exploreP:0.0439\n",
      "Episode:173 meanR:-121.7805 R:-123.62642156710662 gloss:-11697.9814 dloss:10.8934 exploreP:0.0437\n",
      "Episode:174 meanR:-121.8421 R:-123.62452777752156 gloss:-11267.7158 dloss:8.0230 exploreP:0.0436\n",
      "Episode:175 meanR:-121.9222 R:-123.63047219251717 gloss:-10312.0703 dloss:7.7431 exploreP:0.0434\n",
      "Episode:176 meanR:-121.9337 R:-123.50465409247703 gloss:-9557.1895 dloss:7.5839 exploreP:0.0433\n",
      "Episode:177 meanR:-121.9862 R:-123.63631939356155 gloss:-8970.1553 dloss:7.4371 exploreP:0.0432\n",
      "Episode:178 meanR:-122.0563 R:-123.54884225122011 gloss:-8687.5605 dloss:7.2992 exploreP:0.0430\n",
      "Episode:179 meanR:-122.1283 R:-123.41297956774696 gloss:-8896.3486 dloss:7.2485 exploreP:0.0429\n",
      "Episode:180 meanR:-122.1889 R:-123.53984174150663 gloss:-9086.1426 dloss:7.1833 exploreP:0.0428\n",
      "Episode:181 meanR:-122.2889 R:-122.80642736524034 gloss:-9269.3828 dloss:7.1908 exploreP:0.0426\n",
      "Episode:182 meanR:-122.3530 R:-123.43584306583182 gloss:-8560.5801 dloss:7.5372 exploreP:0.0425\n",
      "Episode:183 meanR:-122.4318 R:-123.02080014842315 gloss:-8681.8115 dloss:6.8753 exploreP:0.0423\n",
      "Episode:184 meanR:-122.5244 R:-123.5961698459536 gloss:-8085.9800 dloss:7.2398 exploreP:0.0422\n",
      "Episode:185 meanR:-122.6158 R:-122.57808436576464 gloss:-8464.2900 dloss:6.8611 exploreP:0.0421\n",
      "Episode:186 meanR:-122.6898 R:-123.47152885402181 gloss:-8583.6201 dloss:7.9834 exploreP:0.0419\n",
      "Episode:187 meanR:-122.7562 R:-122.5689075769956 gloss:-8629.8643 dloss:7.0049 exploreP:0.0417\n",
      "Episode:188 meanR:-122.8189 R:-122.1620446433779 gloss:-8254.0000 dloss:7.9910 exploreP:0.0416\n",
      "Episode:189 meanR:-122.9123 R:-127.38316239807011 gloss:-8459.1504 dloss:8.2453 exploreP:0.0412\n",
      "Episode:190 meanR:-122.9481 R:-122.22159481387524 gloss:-8037.3184 dloss:9.2210 exploreP:0.0410\n",
      "Episode:191 meanR:-123.0036 R:-123.82140645508841 gloss:-7634.0044 dloss:9.8510 exploreP:0.0409\n",
      "Episode:192 meanR:-123.0160 R:-123.8183041708842 gloss:-7762.3828 dloss:9.7287 exploreP:0.0407\n",
      "Episode:193 meanR:-123.0868 R:-122.9978291856808 gloss:-7548.4004 dloss:8.8712 exploreP:0.0406\n",
      "Episode:194 meanR:-123.1661 R:-123.81215922808833 gloss:-7744.5938 dloss:9.9097 exploreP:0.0405\n",
      "Episode:195 meanR:-123.1172 R:-120.49938047805118 gloss:-8867.3281 dloss:8.3438 exploreP:0.0404\n",
      "Episode:196 meanR:-123.1512 R:-121.86307700088558 gloss:-9295.4893 dloss:8.3859 exploreP:0.0402\n",
      "Episode:197 meanR:-122.9422 R:-123.49126288542213 gloss:-9390.0205 dloss:8.3066 exploreP:0.0401\n",
      "Episode:198 meanR:-122.9844 R:-122.47756810317935 gloss:-9636.2158 dloss:8.7182 exploreP:0.0400\n",
      "Episode:199 meanR:-123.0292 R:-122.99698431815516 gloss:-9729.9912 dloss:8.4809 exploreP:0.0398\n",
      "Episode:200 meanR:-123.0490 R:-124.00894667845779 gloss:-9568.2080 dloss:8.8031 exploreP:0.0397\n",
      "Episode:201 meanR:-123.1218 R:-123.54634265686448 gloss:-9225.0508 dloss:9.1334 exploreP:0.0396\n",
      "Episode:202 meanR:-123.1937 R:-123.56576297189233 gloss:-8744.9424 dloss:11.3423 exploreP:0.0395\n",
      "Episode:203 meanR:-123.1918 R:-123.336832697869 gloss:-8617.8779 dloss:9.2572 exploreP:0.0393\n",
      "Episode:204 meanR:-123.1897 R:-123.41667183737519 gloss:-8205.2139 dloss:8.9412 exploreP:0.0392\n",
      "Episode:205 meanR:-123.1885 R:-123.43512988087537 gloss:-8482.9561 dloss:8.7805 exploreP:0.0391\n",
      "Episode:206 meanR:-123.1667 R:-121.20362709482885 gloss:-8239.7959 dloss:8.6397 exploreP:0.0390\n",
      "Episode:207 meanR:-123.1663 R:-123.38023378565785 gloss:-7922.4175 dloss:8.5702 exploreP:0.0388\n",
      "Episode:208 meanR:-123.1576 R:-122.18576869604675 gloss:-8000.8105 dloss:9.1072 exploreP:0.0387\n",
      "Episode:209 meanR:-123.1497 R:-123.03695804781157 gloss:-8083.7886 dloss:8.5591 exploreP:0.0386\n",
      "Episode:210 meanR:-123.1459 R:-123.57403452292706 gloss:-7860.7056 dloss:8.0995 exploreP:0.0385\n",
      "Episode:211 meanR:-123.1391 R:-123.22361983808318 gloss:-8029.0068 dloss:7.0990 exploreP:0.0384\n",
      "Episode:212 meanR:-123.1615 R:-123.5099512218771 gloss:-8299.0957 dloss:8.4872 exploreP:0.0382\n",
      "Episode:213 meanR:-123.1480 R:-122.01289696856155 gloss:-8592.3252 dloss:6.9340 exploreP:0.0381\n",
      "Episode:214 meanR:-123.1488 R:-123.5952266280614 gloss:-8866.7393 dloss:7.8804 exploreP:0.0380\n",
      "Episode:215 meanR:-123.0932 R:-118.08002550196952 gloss:-8781.7363 dloss:7.0710 exploreP:0.0376\n",
      "Episode:216 meanR:-123.0837 R:-122.62512866210194 gloss:-7521.1431 dloss:8.4713 exploreP:0.0375\n",
      "Episode:217 meanR:-123.0837 R:-123.42062532042837 gloss:-7607.3486 dloss:8.2948 exploreP:0.0373\n",
      "Episode:218 meanR:-123.0836 R:-123.5493168430999 gloss:-7509.5938 dloss:8.1838 exploreP:0.0372\n",
      "Episode:219 meanR:-123.1340 R:-123.63317559773537 gloss:-7225.3511 dloss:8.8735 exploreP:0.0371\n",
      "Episode:220 meanR:-123.1343 R:-123.50345440486262 gloss:-7278.2021 dloss:8.0739 exploreP:0.0370\n",
      "Episode:221 meanR:-123.1346 R:-123.41865075661585 gloss:-7436.3774 dloss:7.9797 exploreP:0.0369\n",
      "Episode:222 meanR:-123.1405 R:-123.5003798554099 gloss:-7572.5103 dloss:9.4012 exploreP:0.0368\n",
      "Episode:223 meanR:-123.1336 R:-122.86088540424839 gloss:-7989.6460 dloss:8.2404 exploreP:0.0367\n",
      "Episode:224 meanR:-123.0886 R:-122.69099341029116 gloss:-8243.8760 dloss:8.0054 exploreP:0.0365\n",
      "Episode:225 meanR:-123.0886 R:-123.56829824288499 gloss:-8063.2725 dloss:7.9309 exploreP:0.0364\n",
      "Episode:226 meanR:-123.0881 R:-123.49633531984065 gloss:-7629.5708 dloss:7.9758 exploreP:0.0363\n",
      "Episode:227 meanR:-123.0892 R:-123.50596717454742 gloss:-7162.9160 dloss:7.9524 exploreP:0.0362\n",
      "Episode:228 meanR:-123.1128 R:-123.65317984001214 gloss:-7757.2485 dloss:7.7118 exploreP:0.0361\n",
      "Episode:229 meanR:-123.1063 R:-122.56876597350279 gloss:-7496.6577 dloss:7.8099 exploreP:0.0360\n",
      "Episode:230 meanR:-123.1151 R:-121.27858065499117 gloss:-7875.7832 dloss:7.9765 exploreP:0.0359\n",
      "Episode:231 meanR:-123.1531 R:-122.64008212174849 gloss:-8183.5161 dloss:7.7338 exploreP:0.0358\n",
      "Episode:232 meanR:-123.1600 R:-123.98929754615452 gloss:-7790.5684 dloss:7.8530 exploreP:0.0357\n",
      "Episode:233 meanR:-123.1515 R:-122.65483734137007 gloss:-7346.8901 dloss:7.8534 exploreP:0.0355\n",
      "Episode:234 meanR:-123.1574 R:-123.98356121611596 gloss:-6840.6782 dloss:7.7335 exploreP:0.0354\n",
      "Episode:235 meanR:-123.1631 R:-123.93487764432717 gloss:-6839.0669 dloss:7.4823 exploreP:0.0353\n",
      "Episode:236 meanR:-123.1649 R:-123.78817919819741 gloss:-6685.4194 dloss:7.7558 exploreP:0.0352\n",
      "Episode:237 meanR:-123.1846 R:-123.54901065193862 gloss:-6342.4463 dloss:8.6097 exploreP:0.0351\n",
      "Episode:238 meanR:-123.1864 R:-123.63012036073457 gloss:-6155.1685 dloss:7.9713 exploreP:0.0350\n",
      "Episode:239 meanR:-123.1869 R:-123.57628808036262 gloss:-6124.7588 dloss:5.8731 exploreP:0.0349\n",
      "Episode:240 meanR:-123.1802 R:-123.48175673394155 gloss:-5925.2334 dloss:5.1799 exploreP:0.0348\n",
      "Episode:241 meanR:-123.1623 R:-121.7208851599712 gloss:-6062.5225 dloss:5.8182 exploreP:0.0347\n",
      "Episode:242 meanR:-123.1620 R:-123.60184153940033 gloss:-6208.0151 dloss:5.2013 exploreP:0.0346\n",
      "Episode:243 meanR:-123.1592 R:-123.28114477537139 gloss:-6602.4917 dloss:5.2298 exploreP:0.0345\n",
      "Episode:244 meanR:-123.1603 R:-123.55708574124053 gloss:-6772.0513 dloss:5.2518 exploreP:0.0344\n",
      "Episode:245 meanR:-123.1579 R:-123.38630813575165 gloss:-6666.1260 dloss:5.7690 exploreP:0.0343\n",
      "Episode:246 meanR:-123.1556 R:-123.34566423587316 gloss:-6692.5566 dloss:4.9904 exploreP:0.0342\n",
      "Episode:247 meanR:-123.1532 R:-122.5976480035546 gloss:-11064.0215 dloss:13.2065 exploreP:0.0341\n",
      "Episode:248 meanR:-123.0927 R:-117.40789984159483 gloss:-60866.0547 dloss:40.9331 exploreP:0.0339\n",
      "Episode:249 meanR:-123.0417 R:-118.8235808175852 gloss:-103944.8984 dloss:33.2067 exploreP:0.0337\n",
      "Episode:250 meanR:-122.9884 R:-118.33518918562464 gloss:-122411.4609 dloss:25.7914 exploreP:0.0336\n",
      "Episode:251 meanR:-122.8969 R:-113.98025436020457 gloss:-131045.5078 dloss:44.1269 exploreP:0.0334\n",
      "Episode:252 meanR:-122.8482 R:-118.88352599412451 gloss:-132442.3750 dloss:50.4126 exploreP:0.0333\n",
      "Episode:253 meanR:-122.7900 R:-118.08436308863014 gloss:-129065.0000 dloss:41.2002 exploreP:0.0331\n",
      "Episode:254 meanR:-122.7474 R:-119.24378514982388 gloss:-121622.4453 dloss:39.4431 exploreP:0.0329\n",
      "Episode:255 meanR:-122.7014 R:-118.78487338696358 gloss:-112331.6484 dloss:28.9517 exploreP:0.0327\n",
      "Episode:256 meanR:-122.6722 R:-120.53196653279042 gloss:-105715.8906 dloss:19.7285 exploreP:0.0326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:257 meanR:-123.2238 R:-178.7948789716949 gloss:-16615.2617 dloss:11.4533 exploreP:0.0292\n",
      "Episode:258 meanR:-123.7839 R:-179.10370512496573 gloss:-1005.1916 dloss:1.4612 exploreP:0.0264\n",
      "Episode:259 meanR:-124.3543 R:-179.98603745237307 gloss:-747.8523 dloss:0.4610 exploreP:0.0240\n",
      "Episode:260 meanR:-124.9155 R:-179.54149274663033 gloss:-753.5580 dloss:1.2297 exploreP:0.0219\n",
      "Episode:261 meanR:-125.4804 R:-180.04277638339596 gloss:-719.3569 dloss:0.5454 exploreP:0.0202\n",
      "Episode:262 meanR:-126.0517 R:-180.57724750432695 gloss:-512.6987 dloss:0.4104 exploreP:0.0186\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "                #action = np.argmax(action_logits) # For discrete action space\n",
    "#             print(action.shape)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(action.shape, next_state.shape, reward, done)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            #batch = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "#             print(actions.shape)\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "#             print(targetQs.shape)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) <= -150:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
