{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score and steps: 0.0 and 299\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "for steps in range(1111111):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([1.        , 0.        , 0.        , 0.        , 0.41103721,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.40960541,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.04057696,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.253993  ,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.30345654,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.19119175,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.3996276 ,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([1.        , 0.        , 0.        , 0.        , 0.41103721,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.40960541,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.04057696,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.253993  ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.30345654,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.19119175,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.3996276 ,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[1] for each in batch])\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "1.0 0.0\n",
      "10.889832496643066 -10.890602111816406\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, action_size, hidden_size):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 37) actions:(300,)\n",
      "action size:4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "action_size = 4\n",
    "state_size = 37\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.0 loss:0.4322\n",
      "Episode:1 meanR:0.0000 R:0.0 loss:15.9051\n",
      "Episode:2 meanR:0.6667 R:2.0 loss:41.9501\n",
      "Episode:3 meanR:-0.2500 R:-3.0 loss:26.5035\n",
      "Episode:4 meanR:-0.4000 R:-1.0 loss:16.7503\n",
      "Episode:5 meanR:-0.1667 R:1.0 loss:10.7855\n",
      "Episode:6 meanR:0.0000 R:1.0 loss:7.0130\n",
      "Episode:7 meanR:0.0000 R:0.0 loss:4.9258\n",
      "Episode:8 meanR:0.0000 R:0.0 loss:3.1157\n",
      "Episode:9 meanR:0.0000 R:0.0 loss:2.4247\n",
      "Episode:10 meanR:-0.0909 R:-1.0 loss:1.8304\n",
      "Episode:11 meanR:-0.0833 R:0.0 loss:1.7536\n",
      "Episode:12 meanR:-0.1538 R:-1.0 loss:1.2489\n",
      "Episode:13 meanR:-0.2143 R:-1.0 loss:0.9175\n",
      "Episode:14 meanR:-0.2667 R:-1.0 loss:0.7372\n",
      "Episode:15 meanR:-0.1875 R:1.0 loss:0.6136\n",
      "Episode:16 meanR:-0.1176 R:1.0 loss:0.5312\n",
      "Episode:17 meanR:-0.1667 R:-1.0 loss:0.4477\n",
      "Episode:18 meanR:-0.1579 R:0.0 loss:0.3462\n",
      "Episode:19 meanR:-0.1000 R:1.0 loss:0.3248\n",
      "Episode:20 meanR:-0.0952 R:0.0 loss:0.2886\n",
      "Episode:21 meanR:-0.1818 R:-2.0 loss:0.2591\n",
      "Episode:22 meanR:-0.1304 R:1.0 loss:0.2024\n",
      "Episode:23 meanR:0.0000 R:3.0 loss:0.1751\n",
      "Episode:24 meanR:0.0000 R:0.0 loss:0.1669\n",
      "Episode:25 meanR:0.0769 R:2.0 loss:0.1520\n",
      "Episode:26 meanR:0.1111 R:1.0 loss:0.1426\n",
      "Episode:27 meanR:0.1429 R:1.0 loss:0.1201\n",
      "Episode:28 meanR:0.1034 R:-1.0 loss:0.1360\n",
      "Episode:29 meanR:0.0000 R:-3.0 loss:0.1320\n",
      "Episode:30 meanR:-0.0645 R:-2.0 loss:0.0984\n",
      "Episode:31 meanR:-0.1250 R:-2.0 loss:0.0950\n",
      "Episode:32 meanR:-0.1818 R:-2.0 loss:0.0952\n",
      "Episode:33 meanR:-0.2353 R:-2.0 loss:0.0852\n",
      "Episode:34 meanR:-0.2286 R:0.0 loss:0.0825\n",
      "Episode:35 meanR:-0.2500 R:-1.0 loss:0.0754\n",
      "Episode:36 meanR:-0.2432 R:0.0 loss:0.0674\n",
      "Episode:37 meanR:-0.2368 R:0.0 loss:0.0625\n",
      "Episode:38 meanR:-0.2564 R:-1.0 loss:0.0569\n",
      "Episode:39 meanR:-0.2000 R:2.0 loss:0.0598\n",
      "Episode:40 meanR:-0.1707 R:1.0 loss:0.0524\n",
      "Episode:41 meanR:-0.0952 R:3.0 loss:0.0460\n",
      "Episode:42 meanR:0.0233 R:5.0 loss:0.0474\n",
      "Episode:43 meanR:0.0682 R:2.0 loss:0.0476\n",
      "Episode:44 meanR:0.0889 R:1.0 loss:0.0795\n",
      "Episode:45 meanR:0.1957 R:5.0 loss:0.0745\n",
      "Episode:46 meanR:0.2553 R:3.0 loss:0.0573\n",
      "Episode:47 meanR:0.2708 R:1.0 loss:0.0486\n",
      "Episode:48 meanR:0.2857 R:1.0 loss:0.0585\n",
      "Episode:49 meanR:0.3000 R:1.0 loss:0.0530\n",
      "Episode:50 meanR:0.3922 R:5.0 loss:0.0498\n",
      "Episode:51 meanR:0.4231 R:2.0 loss:0.0472\n",
      "Episode:52 meanR:0.4528 R:2.0 loss:0.0569\n",
      "Episode:53 meanR:0.5000 R:3.0 loss:0.0587\n",
      "Episode:54 meanR:0.5818 R:5.0 loss:0.0513\n",
      "Episode:55 meanR:0.6071 R:2.0 loss:0.0557\n",
      "Episode:56 meanR:0.7544 R:9.0 loss:0.0568\n",
      "Episode:57 meanR:0.7931 R:3.0 loss:0.0566\n",
      "Episode:58 meanR:0.8814 R:6.0 loss:0.0519\n",
      "Episode:59 meanR:0.9167 R:3.0 loss:0.0572\n",
      "Episode:60 meanR:0.9344 R:2.0 loss:0.0729\n",
      "Episode:61 meanR:1.0161 R:6.0 loss:0.0617\n",
      "Episode:62 meanR:1.0000 R:0.0 loss:0.0475\n",
      "Episode:63 meanR:1.0000 R:1.0 loss:0.0728\n",
      "Episode:64 meanR:1.0154 R:2.0 loss:0.0595\n",
      "Episode:65 meanR:1.1818 R:12.0 loss:0.0553\n",
      "Episode:66 meanR:1.3433 R:12.0 loss:0.0578\n",
      "Episode:67 meanR:1.3529 R:2.0 loss:0.0483\n",
      "Episode:68 meanR:1.3768 R:3.0 loss:0.0558\n",
      "Episode:69 meanR:1.4000 R:3.0 loss:0.0755\n",
      "Episode:70 meanR:1.5070 R:9.0 loss:0.0625\n",
      "Episode:71 meanR:1.5972 R:8.0 loss:0.0552\n",
      "Episode:72 meanR:1.6712 R:7.0 loss:0.0605\n",
      "Episode:73 meanR:1.7297 R:6.0 loss:0.0577\n",
      "Episode:74 meanR:1.7600 R:4.0 loss:0.0570\n",
      "Episode:75 meanR:1.7763 R:3.0 loss:0.0592\n",
      "Episode:76 meanR:1.8701 R:9.0 loss:0.0526\n",
      "Episode:77 meanR:1.9744 R:10.0 loss:0.0579\n",
      "Episode:78 meanR:2.0253 R:6.0 loss:0.0645\n",
      "Episode:79 meanR:2.1000 R:8.0 loss:0.0583\n",
      "Episode:80 meanR:2.1852 R:9.0 loss:0.0527\n",
      "Episode:81 meanR:2.2439 R:7.0 loss:0.0558\n",
      "Episode:82 meanR:2.2530 R:3.0 loss:0.0624\n",
      "Episode:83 meanR:2.3333 R:9.0 loss:0.0638\n",
      "Episode:84 meanR:2.3529 R:4.0 loss:0.0599\n",
      "Episode:85 meanR:2.4302 R:9.0 loss:0.0593\n",
      "Episode:86 meanR:2.5287 R:11.0 loss:0.0636\n",
      "Episode:87 meanR:2.5227 R:2.0 loss:0.0552\n",
      "Episode:88 meanR:2.5730 R:7.0 loss:0.0548\n",
      "Episode:89 meanR:2.6222 R:7.0 loss:0.0582\n",
      "Episode:90 meanR:2.6813 R:8.0 loss:0.0612\n",
      "Episode:91 meanR:2.6957 R:4.0 loss:0.0583\n",
      "Episode:92 meanR:2.7312 R:6.0 loss:0.0625\n",
      "Episode:93 meanR:2.8723 R:16.0 loss:0.0662\n",
      "Episode:94 meanR:2.8632 R:2.0 loss:0.0574\n",
      "Episode:95 meanR:2.9375 R:10.0 loss:0.0606\n",
      "Episode:96 meanR:3.0206 R:11.0 loss:0.0621\n",
      "Episode:97 meanR:3.1020 R:11.0 loss:0.0594\n",
      "Episode:98 meanR:3.2020 R:13.0 loss:0.0615\n",
      "Episode:99 meanR:3.2700 R:10.0 loss:0.0639\n",
      "Episode:100 meanR:3.3700 R:10.0 loss:0.0672\n",
      "Episode:101 meanR:3.4700 R:10.0 loss:0.0589\n",
      "Episode:102 meanR:3.5300 R:8.0 loss:0.0648\n",
      "Episode:103 meanR:3.7100 R:15.0 loss:0.0608\n",
      "Episode:104 meanR:3.8500 R:13.0 loss:0.0656\n",
      "Episode:105 meanR:3.8600 R:2.0 loss:0.0645\n",
      "Episode:106 meanR:3.9700 R:12.0 loss:0.0578\n",
      "Episode:107 meanR:4.0000 R:3.0 loss:0.0576\n",
      "Episode:108 meanR:4.0900 R:9.0 loss:0.0690\n",
      "Episode:109 meanR:4.2300 R:14.0 loss:0.0639\n",
      "Episode:110 meanR:4.2900 R:5.0 loss:0.0657\n",
      "Episode:111 meanR:4.4100 R:12.0 loss:0.0663\n",
      "Episode:112 meanR:4.5000 R:8.0 loss:0.0580\n",
      "Episode:113 meanR:4.5800 R:7.0 loss:0.0669\n",
      "Episode:114 meanR:4.7200 R:13.0 loss:0.0654\n",
      "Episode:115 meanR:4.8100 R:10.0 loss:0.0596\n",
      "Episode:116 meanR:4.9200 R:12.0 loss:0.0597\n",
      "Episode:117 meanR:4.9900 R:6.0 loss:0.0676\n",
      "Episode:118 meanR:5.1100 R:12.0 loss:0.0681\n",
      "Episode:119 meanR:5.2200 R:12.0 loss:0.0652\n",
      "Episode:120 meanR:5.3300 R:11.0 loss:0.0671\n",
      "Episode:121 meanR:5.4000 R:5.0 loss:0.0603\n",
      "Episode:122 meanR:5.5000 R:11.0 loss:0.0647\n",
      "Episode:123 meanR:5.5900 R:12.0 loss:0.0666\n",
      "Episode:124 meanR:5.7000 R:11.0 loss:0.0672\n",
      "Episode:125 meanR:5.7000 R:2.0 loss:0.0671\n",
      "Episode:126 meanR:5.8000 R:11.0 loss:0.0751\n",
      "Episode:127 meanR:5.9100 R:12.0 loss:0.0748\n",
      "Episode:128 meanR:6.0800 R:16.0 loss:0.0697\n",
      "Episode:129 meanR:6.1200 R:1.0 loss:0.0670\n",
      "Episode:130 meanR:6.2400 R:10.0 loss:0.0692\n",
      "Episode:131 meanR:6.3500 R:9.0 loss:0.0700\n",
      "Episode:132 meanR:6.4900 R:12.0 loss:0.0615\n",
      "Episode:133 meanR:6.5400 R:3.0 loss:0.0642\n",
      "Episode:134 meanR:6.6700 R:13.0 loss:0.0691\n",
      "Episode:135 meanR:6.7400 R:6.0 loss:0.0682\n",
      "Episode:136 meanR:6.8600 R:12.0 loss:0.0720\n",
      "Episode:137 meanR:6.9900 R:13.0 loss:0.0701\n",
      "Episode:138 meanR:7.0800 R:8.0 loss:0.0651\n",
      "Episode:139 meanR:7.1300 R:7.0 loss:0.0773\n",
      "Episode:140 meanR:7.2700 R:15.0 loss:0.0714\n",
      "Episode:141 meanR:7.3300 R:9.0 loss:0.0736\n",
      "Episode:142 meanR:7.3400 R:6.0 loss:0.0717\n",
      "Episode:143 meanR:7.4100 R:9.0 loss:0.0681\n",
      "Episode:144 meanR:7.4900 R:9.0 loss:0.0657\n",
      "Episode:145 meanR:7.5700 R:13.0 loss:0.0605\n",
      "Episode:146 meanR:7.6700 R:13.0 loss:0.0800\n",
      "Episode:147 meanR:7.7300 R:7.0 loss:0.0707\n",
      "Episode:148 meanR:7.8300 R:11.0 loss:0.0720\n",
      "Episode:149 meanR:7.8500 R:3.0 loss:0.0697\n",
      "Episode:150 meanR:7.9000 R:10.0 loss:0.0593\n",
      "Episode:151 meanR:8.0300 R:15.0 loss:0.0725\n",
      "Episode:152 meanR:8.1000 R:9.0 loss:0.0745\n",
      "Episode:153 meanR:8.1300 R:6.0 loss:0.0669\n",
      "Episode:154 meanR:8.1100 R:3.0 loss:0.0643\n",
      "Episode:155 meanR:8.2400 R:15.0 loss:0.0732\n",
      "Episode:156 meanR:8.2300 R:8.0 loss:0.0676\n",
      "Episode:157 meanR:8.2300 R:3.0 loss:0.0623\n",
      "Episode:158 meanR:8.2300 R:6.0 loss:0.0692\n",
      "Episode:159 meanR:8.3600 R:16.0 loss:0.0694\n",
      "Episode:160 meanR:8.3800 R:4.0 loss:0.0695\n",
      "Episode:161 meanR:8.4400 R:12.0 loss:0.0656\n",
      "Episode:162 meanR:8.5100 R:7.0 loss:0.0706\n",
      "Episode:163 meanR:8.5800 R:8.0 loss:0.0719\n",
      "Episode:164 meanR:8.6500 R:9.0 loss:0.0616\n",
      "Episode:165 meanR:8.6900 R:16.0 loss:0.0665\n",
      "Episode:166 meanR:8.6700 R:10.0 loss:0.0604\n",
      "Episode:167 meanR:8.7800 R:13.0 loss:0.0698\n",
      "Episode:168 meanR:8.8200 R:7.0 loss:0.0646\n",
      "Episode:169 meanR:8.9100 R:12.0 loss:0.0628\n",
      "Episode:170 meanR:8.9300 R:11.0 loss:0.0712\n",
      "Episode:171 meanR:9.0100 R:16.0 loss:0.0676\n",
      "Episode:172 meanR:9.0300 R:9.0 loss:0.0735\n",
      "Episode:173 meanR:9.1300 R:16.0 loss:0.0739\n",
      "Episode:174 meanR:9.1300 R:4.0 loss:0.0658\n",
      "Episode:175 meanR:9.2500 R:15.0 loss:0.0702\n",
      "Episode:176 meanR:9.2500 R:9.0 loss:0.0631\n",
      "Episode:177 meanR:9.3000 R:15.0 loss:0.0712\n",
      "Episode:178 meanR:9.3600 R:12.0 loss:0.0731\n",
      "Episode:179 meanR:9.4000 R:12.0 loss:0.0823\n",
      "Episode:180 meanR:9.4100 R:10.0 loss:0.0656\n",
      "Episode:181 meanR:9.5000 R:16.0 loss:0.0768\n",
      "Episode:182 meanR:9.6000 R:13.0 loss:0.0751\n",
      "Episode:183 meanR:9.5600 R:5.0 loss:0.0659\n",
      "Episode:184 meanR:9.6700 R:15.0 loss:0.0766\n",
      "Episode:185 meanR:9.7300 R:15.0 loss:0.0772\n",
      "Episode:186 meanR:9.6800 R:6.0 loss:0.0652\n",
      "Episode:187 meanR:9.7600 R:10.0 loss:0.0747\n",
      "Episode:188 meanR:9.7200 R:3.0 loss:0.0679\n",
      "Episode:189 meanR:9.6700 R:2.0 loss:0.0696\n",
      "Episode:190 meanR:9.6400 R:5.0 loss:0.0679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:191 meanR:9.7500 R:15.0 loss:0.0601\n",
      "Episode:192 meanR:9.8500 R:16.0 loss:0.0600\n",
      "Episode:193 meanR:9.7800 R:9.0 loss:0.0647\n",
      "Episode:194 meanR:9.8400 R:8.0 loss:0.0650\n",
      "Episode:195 meanR:9.8900 R:15.0 loss:0.0617\n",
      "Episode:196 meanR:9.8700 R:9.0 loss:0.0661\n",
      "Episode:197 meanR:9.8400 R:8.0 loss:0.0661\n",
      "Episode:198 meanR:9.7900 R:8.0 loss:0.0649\n",
      "Episode:199 meanR:9.7400 R:5.0 loss:0.0676\n",
      "Episode:200 meanR:9.7500 R:11.0 loss:0.0634\n",
      "Episode:201 meanR:9.8000 R:15.0 loss:0.0634\n",
      "Episode:202 meanR:9.8700 R:15.0 loss:0.0690\n",
      "Episode:203 meanR:9.8000 R:8.0 loss:0.0704\n",
      "Episode:204 meanR:9.7900 R:12.0 loss:0.0712\n",
      "Episode:205 meanR:9.8700 R:10.0 loss:0.0780\n",
      "Episode:206 meanR:9.9100 R:16.0 loss:0.0678\n",
      "Episode:207 meanR:10.0100 R:13.0 loss:0.0588\n",
      "Episode:208 meanR:10.0000 R:8.0 loss:0.0750\n",
      "Episode:209 meanR:10.0100 R:15.0 loss:0.0651\n",
      "Episode:210 meanR:10.1000 R:14.0 loss:0.0705\n",
      "Episode:211 meanR:10.0100 R:3.0 loss:0.0683\n",
      "Episode:212 meanR:10.0300 R:10.0 loss:0.0744\n",
      "Episode:213 meanR:10.1000 R:14.0 loss:0.0784\n",
      "Episode:214 meanR:10.1100 R:14.0 loss:0.0806\n",
      "Episode:215 meanR:10.0800 R:7.0 loss:0.0709\n",
      "Episode:216 meanR:10.0300 R:7.0 loss:0.0707\n",
      "Episode:217 meanR:10.0700 R:10.0 loss:0.0647\n",
      "Episode:218 meanR:9.9500 R:0.0 loss:0.0622\n",
      "Episode:219 meanR:9.9700 R:14.0 loss:0.0619\n",
      "Episode:220 meanR:9.9400 R:8.0 loss:0.0718\n",
      "Episode:221 meanR:10.0000 R:11.0 loss:0.0631\n",
      "Episode:222 meanR:10.0300 R:14.0 loss:0.0743\n",
      "Episode:223 meanR:9.9800 R:7.0 loss:0.0694\n",
      "Episode:224 meanR:9.9900 R:12.0 loss:0.0805\n",
      "Episode:225 meanR:10.0900 R:12.0 loss:0.0678\n",
      "Episode:226 meanR:10.1000 R:12.0 loss:0.0648\n",
      "Episode:227 meanR:10.0800 R:10.0 loss:0.0610\n",
      "Episode:228 meanR:10.0200 R:10.0 loss:0.0726\n",
      "Episode:229 meanR:10.1300 R:12.0 loss:0.0661\n",
      "Episode:230 meanR:10.1700 R:14.0 loss:0.0672\n",
      "Episode:231 meanR:10.1800 R:10.0 loss:0.0664\n",
      "Episode:232 meanR:10.1800 R:12.0 loss:0.0731\n",
      "Episode:233 meanR:10.2900 R:14.0 loss:0.0716\n",
      "Episode:234 meanR:10.2500 R:9.0 loss:0.0712\n",
      "Episode:235 meanR:10.2600 R:7.0 loss:0.0681\n",
      "Episode:236 meanR:10.2900 R:15.0 loss:0.0763\n",
      "Episode:237 meanR:10.2800 R:12.0 loss:0.0787\n",
      "Episode:238 meanR:10.3000 R:10.0 loss:0.0752\n",
      "Episode:239 meanR:10.3700 R:14.0 loss:0.0713\n",
      "Episode:240 meanR:10.4200 R:20.0 loss:0.0836\n",
      "Episode:241 meanR:10.4000 R:7.0 loss:0.0753\n",
      "Episode:242 meanR:10.3800 R:4.0 loss:0.0828\n",
      "Episode:243 meanR:10.4200 R:13.0 loss:0.0722\n",
      "Episode:244 meanR:10.4800 R:15.0 loss:0.0762\n",
      "Episode:245 meanR:10.4300 R:8.0 loss:0.0736\n",
      "Episode:246 meanR:10.4200 R:12.0 loss:0.0768\n",
      "Episode:247 meanR:10.4300 R:8.0 loss:0.0726\n",
      "Episode:248 meanR:10.4800 R:16.0 loss:0.0674\n",
      "Episode:249 meanR:10.5900 R:14.0 loss:0.0752\n",
      "Episode:250 meanR:10.6500 R:16.0 loss:0.0771\n",
      "Episode:251 meanR:10.5800 R:8.0 loss:0.0854\n",
      "Episode:252 meanR:10.5900 R:10.0 loss:0.0866\n",
      "Episode:253 meanR:10.6900 R:16.0 loss:0.0903\n",
      "Episode:254 meanR:10.7700 R:11.0 loss:0.0745\n",
      "Episode:255 meanR:10.6700 R:5.0 loss:0.0795\n",
      "Episode:256 meanR:10.7000 R:11.0 loss:0.0778\n",
      "Episode:257 meanR:10.7800 R:11.0 loss:0.0901\n",
      "Episode:258 meanR:10.8200 R:10.0 loss:0.0802\n",
      "Episode:259 meanR:10.7600 R:10.0 loss:0.0754\n",
      "Episode:260 meanR:10.8500 R:13.0 loss:0.0799\n",
      "Episode:261 meanR:10.8200 R:9.0 loss:0.0805\n",
      "Episode:262 meanR:10.8700 R:12.0 loss:0.0730\n",
      "Episode:263 meanR:10.8600 R:7.0 loss:0.0856\n",
      "Episode:264 meanR:10.8300 R:6.0 loss:0.0813\n",
      "Episode:265 meanR:10.7000 R:3.0 loss:0.0747\n",
      "Episode:266 meanR:10.7300 R:13.0 loss:0.0825\n",
      "Episode:267 meanR:10.8000 R:20.0 loss:0.0719\n",
      "Episode:268 meanR:10.8400 R:11.0 loss:0.0779\n",
      "Episode:269 meanR:10.8100 R:9.0 loss:0.0692\n",
      "Episode:270 meanR:10.7600 R:6.0 loss:0.0714\n",
      "Episode:271 meanR:10.7300 R:13.0 loss:0.0769\n",
      "Episode:272 meanR:10.6500 R:1.0 loss:0.0712\n",
      "Episode:273 meanR:10.6000 R:11.0 loss:0.0684\n",
      "Episode:274 meanR:10.6100 R:5.0 loss:0.0699\n",
      "Episode:275 meanR:10.5300 R:7.0 loss:0.0717\n",
      "Episode:276 meanR:10.5600 R:12.0 loss:0.0680\n",
      "Episode:277 meanR:10.4600 R:5.0 loss:0.0699\n",
      "Episode:278 meanR:10.4100 R:7.0 loss:0.0740\n",
      "Episode:279 meanR:10.3700 R:8.0 loss:0.0654\n",
      "Episode:280 meanR:10.3500 R:8.0 loss:0.0648\n",
      "Episode:281 meanR:10.2600 R:7.0 loss:0.0677\n",
      "Episode:282 meanR:10.1800 R:5.0 loss:0.0650\n",
      "Episode:283 meanR:10.1600 R:3.0 loss:0.0675\n",
      "Episode:284 meanR:10.1400 R:13.0 loss:0.0649\n",
      "Episode:285 meanR:10.2100 R:22.0 loss:0.0608\n",
      "Episode:286 meanR:10.3000 R:15.0 loss:0.0661\n",
      "Episode:287 meanR:10.2800 R:8.0 loss:0.0617\n",
      "Episode:288 meanR:10.3100 R:6.0 loss:0.0663\n",
      "Episode:289 meanR:10.3400 R:5.0 loss:0.0576\n",
      "Episode:290 meanR:10.4800 R:19.0 loss:0.0713\n",
      "Episode:291 meanR:10.4000 R:7.0 loss:0.0680\n",
      "Episode:292 meanR:10.4000 R:16.0 loss:0.0588\n",
      "Episode:293 meanR:10.4200 R:11.0 loss:0.0711\n",
      "Episode:294 meanR:10.4500 R:11.0 loss:0.0675\n",
      "Episode:295 meanR:10.3500 R:5.0 loss:0.0780\n",
      "Episode:296 meanR:10.3500 R:9.0 loss:0.0720\n",
      "Episode:297 meanR:10.3400 R:7.0 loss:0.0657\n",
      "Episode:298 meanR:10.3800 R:12.0 loss:0.0709\n",
      "Episode:299 meanR:10.3300 R:0.0 loss:0.0761\n",
      "Episode:300 meanR:10.2600 R:4.0 loss:0.0667\n",
      "Episode:301 meanR:10.2200 R:11.0 loss:0.0655\n",
      "Episode:302 meanR:10.2000 R:13.0 loss:0.0659\n",
      "Episode:303 meanR:10.2400 R:12.0 loss:0.0717\n",
      "Episode:304 meanR:10.2800 R:16.0 loss:0.0706\n",
      "Episode:305 meanR:10.3100 R:13.0 loss:0.0735\n",
      "Episode:306 meanR:10.3300 R:18.0 loss:0.0748\n",
      "Episode:307 meanR:10.2700 R:7.0 loss:0.0728\n",
      "Episode:308 meanR:10.2000 R:1.0 loss:0.0751\n",
      "Episode:309 meanR:10.1500 R:10.0 loss:0.0789\n",
      "Episode:310 meanR:10.1400 R:13.0 loss:0.0654\n",
      "Episode:311 meanR:10.2400 R:13.0 loss:0.0832\n",
      "Episode:312 meanR:10.2200 R:8.0 loss:0.0741\n",
      "Episode:313 meanR:10.2000 R:12.0 loss:0.0708\n",
      "Episode:314 meanR:10.1500 R:9.0 loss:0.0814\n",
      "Episode:315 meanR:10.1500 R:7.0 loss:0.0867\n",
      "Episode:316 meanR:10.1400 R:6.0 loss:0.0740\n",
      "Episode:317 meanR:10.0500 R:1.0 loss:0.0710\n",
      "Episode:318 meanR:10.1300 R:8.0 loss:0.0735\n",
      "Episode:319 meanR:10.1000 R:11.0 loss:0.0730\n",
      "Episode:320 meanR:10.1300 R:11.0 loss:0.0645\n",
      "Episode:321 meanR:10.1600 R:14.0 loss:0.0728\n",
      "Episode:322 meanR:10.1200 R:10.0 loss:0.0781\n",
      "Episode:323 meanR:10.0500 R:0.0 loss:0.0782\n",
      "Episode:324 meanR:9.9300 R:0.0 loss:0.0713\n",
      "Episode:325 meanR:9.9100 R:10.0 loss:0.0755\n",
      "Episode:326 meanR:9.8500 R:6.0 loss:0.0728\n",
      "Episode:327 meanR:9.8400 R:9.0 loss:0.0722\n",
      "Episode:328 meanR:9.8700 R:13.0 loss:0.0751\n",
      "Episode:329 meanR:9.8700 R:12.0 loss:0.0659\n",
      "Episode:330 meanR:9.8200 R:9.0 loss:0.0640\n",
      "Episode:331 meanR:9.7400 R:2.0 loss:0.0665\n",
      "Episode:332 meanR:9.8000 R:18.0 loss:0.0720\n",
      "Episode:333 meanR:9.7300 R:7.0 loss:0.0653\n",
      "Episode:334 meanR:9.7900 R:15.0 loss:0.0687\n",
      "Episode:335 meanR:9.7100 R:-1.0 loss:0.0704\n",
      "Episode:336 meanR:9.6800 R:12.0 loss:0.0694\n",
      "Episode:337 meanR:9.6100 R:5.0 loss:0.0671\n",
      "Episode:338 meanR:9.6000 R:9.0 loss:0.0665\n",
      "Episode:339 meanR:9.6400 R:18.0 loss:0.0677\n",
      "Episode:340 meanR:9.6000 R:16.0 loss:0.0644\n",
      "Episode:341 meanR:9.6800 R:15.0 loss:0.0705\n",
      "Episode:342 meanR:9.7700 R:13.0 loss:0.0628\n",
      "Episode:343 meanR:9.7300 R:9.0 loss:0.0609\n",
      "Episode:344 meanR:9.6200 R:4.0 loss:0.0672\n",
      "Episode:345 meanR:9.6000 R:6.0 loss:0.0654\n",
      "Episode:346 meanR:9.5600 R:8.0 loss:0.0612\n",
      "Episode:347 meanR:9.6200 R:14.0 loss:0.0609\n",
      "Episode:348 meanR:9.5700 R:11.0 loss:0.0564\n",
      "Episode:349 meanR:9.4900 R:6.0 loss:0.0621\n",
      "Episode:350 meanR:9.4400 R:11.0 loss:0.0634\n",
      "Episode:351 meanR:9.5400 R:18.0 loss:0.0694\n",
      "Episode:352 meanR:9.5700 R:13.0 loss:0.0626\n",
      "Episode:353 meanR:9.5800 R:17.0 loss:0.0683\n",
      "Episode:354 meanR:9.5200 R:5.0 loss:0.0664\n",
      "Episode:355 meanR:9.5400 R:7.0 loss:0.0639\n",
      "Episode:356 meanR:9.5400 R:11.0 loss:0.0622\n",
      "Episode:357 meanR:9.6000 R:17.0 loss:0.0643\n",
      "Episode:358 meanR:9.6500 R:15.0 loss:0.0731\n",
      "Episode:359 meanR:9.6900 R:14.0 loss:0.0727\n",
      "Episode:360 meanR:9.6700 R:11.0 loss:0.0726\n",
      "Episode:361 meanR:9.6200 R:4.0 loss:0.0723\n",
      "Episode:362 meanR:9.6100 R:11.0 loss:0.0729\n",
      "Episode:363 meanR:9.6100 R:7.0 loss:0.0728\n",
      "Episode:364 meanR:9.6400 R:9.0 loss:0.0779\n",
      "Episode:365 meanR:9.6900 R:8.0 loss:0.0756\n",
      "Episode:366 meanR:9.7500 R:19.0 loss:0.0829\n",
      "Episode:367 meanR:9.5800 R:3.0 loss:0.0768\n",
      "Episode:368 meanR:9.6600 R:19.0 loss:0.0695\n",
      "Episode:369 meanR:9.7000 R:13.0 loss:0.0801\n",
      "Episode:370 meanR:9.7600 R:12.0 loss:0.0685\n",
      "Episode:371 meanR:9.7300 R:10.0 loss:0.0733\n",
      "Episode:372 meanR:9.8100 R:9.0 loss:0.0790\n",
      "Episode:373 meanR:9.8200 R:12.0 loss:0.0679\n",
      "Episode:374 meanR:9.9200 R:15.0 loss:0.0708\n",
      "Episode:375 meanR:9.9600 R:11.0 loss:0.0735\n",
      "Episode:376 meanR:9.9100 R:7.0 loss:0.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:377 meanR:9.9300 R:7.0 loss:0.0726\n",
      "Episode:378 meanR:10.0300 R:17.0 loss:0.0805\n",
      "Episode:379 meanR:9.9900 R:4.0 loss:0.0713\n",
      "Episode:380 meanR:10.0200 R:11.0 loss:0.0637\n",
      "Episode:381 meanR:10.0000 R:5.0 loss:0.0764\n",
      "Episode:382 meanR:10.1100 R:16.0 loss:0.0713\n",
      "Episode:383 meanR:10.1700 R:9.0 loss:0.0784\n",
      "Episode:384 meanR:10.1600 R:12.0 loss:0.0706\n",
      "Episode:385 meanR:10.0700 R:13.0 loss:0.0756\n",
      "Episode:386 meanR:10.1200 R:20.0 loss:0.0750\n",
      "Episode:387 meanR:10.1100 R:7.0 loss:0.0698\n",
      "Episode:388 meanR:10.1800 R:13.0 loss:0.0715\n",
      "Episode:389 meanR:10.1800 R:5.0 loss:0.0821\n",
      "Episode:390 meanR:10.1100 R:12.0 loss:0.0717\n",
      "Episode:391 meanR:10.1300 R:9.0 loss:0.0738\n",
      "Episode:392 meanR:10.1200 R:15.0 loss:0.0735\n",
      "Episode:393 meanR:10.1600 R:15.0 loss:0.0731\n",
      "Episode:394 meanR:10.2400 R:19.0 loss:0.0799\n",
      "Episode:395 meanR:10.3500 R:16.0 loss:0.0789\n",
      "Episode:396 meanR:10.3700 R:11.0 loss:0.0881\n",
      "Episode:397 meanR:10.3900 R:9.0 loss:0.0730\n",
      "Episode:398 meanR:10.3800 R:11.0 loss:0.0858\n",
      "Episode:399 meanR:10.5400 R:16.0 loss:0.0799\n",
      "Episode:400 meanR:10.5700 R:7.0 loss:0.0816\n",
      "Episode:401 meanR:10.6200 R:16.0 loss:0.0801\n",
      "Episode:402 meanR:10.6600 R:17.0 loss:0.0807\n",
      "Episode:403 meanR:10.6600 R:12.0 loss:0.0856\n",
      "Episode:404 meanR:10.5300 R:3.0 loss:0.0712\n",
      "Episode:405 meanR:10.4900 R:9.0 loss:0.0724\n",
      "Episode:406 meanR:10.4200 R:11.0 loss:0.0758\n",
      "Episode:407 meanR:10.4900 R:14.0 loss:0.0711\n",
      "Episode:408 meanR:10.5500 R:7.0 loss:0.0640\n",
      "Episode:409 meanR:10.6200 R:17.0 loss:0.0738\n",
      "Episode:410 meanR:10.6300 R:14.0 loss:0.0871\n",
      "Episode:411 meanR:10.5800 R:8.0 loss:0.0873\n",
      "Episode:412 meanR:10.6400 R:14.0 loss:0.0745\n",
      "Episode:413 meanR:10.5400 R:2.0 loss:0.0697\n",
      "Episode:414 meanR:10.5700 R:12.0 loss:0.0731\n",
      "Episode:415 meanR:10.6000 R:10.0 loss:0.0870\n",
      "Episode:416 meanR:10.6900 R:15.0 loss:0.0724\n",
      "Episode:417 meanR:10.7600 R:8.0 loss:0.0799\n",
      "Episode:418 meanR:10.7900 R:11.0 loss:0.0806\n",
      "Episode:419 meanR:10.7700 R:9.0 loss:0.0691\n",
      "Episode:420 meanR:10.7500 R:9.0 loss:0.0824\n",
      "Episode:421 meanR:10.7400 R:13.0 loss:0.0713\n",
      "Episode:422 meanR:10.8100 R:17.0 loss:0.0874\n",
      "Episode:423 meanR:10.8700 R:6.0 loss:0.0812\n",
      "Episode:424 meanR:10.9800 R:11.0 loss:0.0865\n",
      "Episode:425 meanR:10.9900 R:11.0 loss:0.0773\n",
      "Episode:426 meanR:10.9900 R:6.0 loss:0.0722\n",
      "Episode:427 meanR:11.0000 R:10.0 loss:0.0748\n",
      "Episode:428 meanR:11.0100 R:14.0 loss:0.0742\n",
      "Episode:429 meanR:11.0300 R:14.0 loss:0.0750\n",
      "Episode:430 meanR:11.0400 R:10.0 loss:0.0743\n",
      "Episode:431 meanR:11.1700 R:15.0 loss:0.0712\n",
      "Episode:432 meanR:11.1000 R:11.0 loss:0.0792\n",
      "Episode:433 meanR:11.0900 R:6.0 loss:0.0702\n",
      "Episode:434 meanR:11.0600 R:12.0 loss:0.0709\n",
      "Episode:435 meanR:11.1400 R:7.0 loss:0.0725\n",
      "Episode:436 meanR:11.1000 R:8.0 loss:0.0748\n",
      "Episode:437 meanR:11.0700 R:2.0 loss:0.0756\n",
      "Episode:438 meanR:11.0600 R:8.0 loss:0.0657\n",
      "Episode:439 meanR:10.9500 R:7.0 loss:0.0691\n",
      "Episode:440 meanR:10.9000 R:11.0 loss:0.0758\n",
      "Episode:441 meanR:10.9200 R:17.0 loss:0.0711\n",
      "Episode:442 meanR:10.9600 R:17.0 loss:0.0747\n",
      "Episode:443 meanR:10.9900 R:12.0 loss:0.0701\n",
      "Episode:444 meanR:11.0300 R:8.0 loss:0.0672\n",
      "Episode:445 meanR:11.1400 R:17.0 loss:0.0784\n",
      "Episode:446 meanR:11.1500 R:9.0 loss:0.0705\n",
      "Episode:447 meanR:11.0800 R:7.0 loss:0.0708\n",
      "Episode:448 meanR:11.0900 R:12.0 loss:0.0713\n",
      "Episode:449 meanR:11.1400 R:11.0 loss:0.0647\n",
      "Episode:450 meanR:11.1200 R:9.0 loss:0.0671\n",
      "Episode:451 meanR:11.0700 R:13.0 loss:0.0682\n",
      "Episode:452 meanR:11.0500 R:11.0 loss:0.0711\n",
      "Episode:453 meanR:11.0300 R:15.0 loss:0.0708\n",
      "Episode:454 meanR:11.2000 R:22.0 loss:0.0720\n",
      "Episode:455 meanR:11.2300 R:10.0 loss:0.0833\n",
      "Episode:456 meanR:11.3200 R:20.0 loss:0.0800\n",
      "Episode:457 meanR:11.2200 R:7.0 loss:0.0762\n",
      "Episode:458 meanR:11.1300 R:6.0 loss:0.0801\n",
      "Episode:459 meanR:11.1000 R:11.0 loss:0.0676\n",
      "Episode:460 meanR:11.0800 R:9.0 loss:0.0729\n",
      "Episode:461 meanR:11.1100 R:7.0 loss:0.0761\n",
      "Episode:462 meanR:11.1800 R:18.0 loss:0.0788\n",
      "Episode:463 meanR:11.2400 R:13.0 loss:0.0763\n",
      "Episode:464 meanR:11.2600 R:11.0 loss:0.0813\n",
      "Episode:465 meanR:11.2100 R:3.0 loss:0.0783\n",
      "Episode:466 meanR:11.1000 R:8.0 loss:0.0700\n",
      "Episode:467 meanR:11.1700 R:10.0 loss:0.0705\n",
      "Episode:468 meanR:11.0900 R:11.0 loss:0.0780\n",
      "Episode:469 meanR:11.0600 R:10.0 loss:0.0725\n",
      "Episode:470 meanR:10.9800 R:4.0 loss:0.0708\n",
      "Episode:471 meanR:10.9600 R:8.0 loss:0.0814\n",
      "Episode:472 meanR:11.0000 R:13.0 loss:0.0645\n",
      "Episode:473 meanR:11.0100 R:13.0 loss:0.0801\n",
      "Episode:474 meanR:10.9800 R:12.0 loss:0.0774\n",
      "Episode:475 meanR:10.9400 R:7.0 loss:0.0878\n",
      "Episode:476 meanR:10.9600 R:9.0 loss:0.0817\n",
      "Episode:477 meanR:11.0200 R:13.0 loss:0.0776\n",
      "Episode:478 meanR:10.9000 R:5.0 loss:0.0739\n",
      "Episode:479 meanR:10.8700 R:1.0 loss:0.0704\n",
      "Episode:480 meanR:10.8800 R:12.0 loss:0.0750\n",
      "Episode:481 meanR:10.9200 R:9.0 loss:0.0722\n",
      "Episode:482 meanR:10.9000 R:14.0 loss:0.0792\n",
      "Episode:483 meanR:10.9400 R:13.0 loss:0.0714\n",
      "Episode:484 meanR:10.9100 R:9.0 loss:0.0731\n",
      "Episode:485 meanR:10.8700 R:9.0 loss:0.0763\n",
      "Episode:486 meanR:10.7600 R:9.0 loss:0.0816\n",
      "Episode:487 meanR:10.8100 R:12.0 loss:0.0760\n",
      "Episode:488 meanR:10.7400 R:6.0 loss:0.0662\n",
      "Episode:489 meanR:10.7800 R:9.0 loss:0.0737\n",
      "Episode:490 meanR:10.8100 R:15.0 loss:0.0687\n",
      "Episode:491 meanR:10.8300 R:11.0 loss:0.0683\n",
      "Episode:492 meanR:10.6800 R:0.0 loss:0.0613\n",
      "Episode:493 meanR:10.6100 R:8.0 loss:0.0604\n",
      "Episode:494 meanR:10.4900 R:7.0 loss:0.0617\n",
      "Episode:495 meanR:10.4600 R:13.0 loss:0.0653\n",
      "Episode:496 meanR:10.4800 R:13.0 loss:0.0573\n",
      "Episode:497 meanR:10.4200 R:3.0 loss:0.0645\n",
      "Episode:498 meanR:10.4500 R:14.0 loss:0.0563\n",
      "Episode:499 meanR:10.4400 R:15.0 loss:0.0603\n",
      "Episode:500 meanR:10.4200 R:5.0 loss:0.0608\n",
      "Episode:501 meanR:10.3700 R:11.0 loss:0.0537\n",
      "Episode:502 meanR:10.3700 R:17.0 loss:0.0609\n",
      "Episode:503 meanR:10.3600 R:11.0 loss:0.0571\n",
      "Episode:504 meanR:10.6000 R:27.0 loss:0.0606\n",
      "Episode:505 meanR:10.6900 R:18.0 loss:0.0673\n",
      "Episode:506 meanR:10.7400 R:16.0 loss:0.0688\n",
      "Episode:507 meanR:10.7600 R:16.0 loss:0.0704\n",
      "Episode:508 meanR:10.8500 R:16.0 loss:0.0673\n",
      "Episode:509 meanR:10.8500 R:17.0 loss:0.0723\n",
      "Episode:510 meanR:10.8000 R:9.0 loss:0.0724\n",
      "Episode:511 meanR:10.7800 R:6.0 loss:0.0641\n",
      "Episode:512 meanR:10.7300 R:9.0 loss:0.0703\n",
      "Episode:513 meanR:10.9200 R:21.0 loss:0.0771\n",
      "Episode:514 meanR:10.9600 R:16.0 loss:0.0731\n",
      "Episode:515 meanR:10.9900 R:13.0 loss:0.0754\n",
      "Episode:516 meanR:10.9500 R:11.0 loss:0.0717\n",
      "Episode:517 meanR:11.0300 R:16.0 loss:0.0744\n",
      "Episode:518 meanR:11.0800 R:16.0 loss:0.0771\n",
      "Episode:519 meanR:11.1800 R:19.0 loss:0.0728\n",
      "Episode:520 meanR:11.2100 R:12.0 loss:0.0889\n",
      "Episode:521 meanR:11.1800 R:10.0 loss:0.0797\n",
      "Episode:522 meanR:11.0900 R:8.0 loss:0.0834\n",
      "Episode:523 meanR:11.1300 R:10.0 loss:0.0847\n",
      "Episode:524 meanR:11.2000 R:18.0 loss:0.0826\n",
      "Episode:525 meanR:11.2600 R:17.0 loss:0.0838\n",
      "Episode:526 meanR:11.3700 R:17.0 loss:0.0842\n",
      "Episode:527 meanR:11.4500 R:18.0 loss:0.0853\n",
      "Episode:528 meanR:11.4200 R:11.0 loss:0.0950\n",
      "Episode:529 meanR:11.4500 R:17.0 loss:0.1006\n",
      "Episode:530 meanR:11.4300 R:8.0 loss:0.0955\n",
      "Episode:531 meanR:11.5000 R:22.0 loss:0.0777\n",
      "Episode:532 meanR:11.5900 R:20.0 loss:0.0950\n",
      "Episode:533 meanR:11.6000 R:7.0 loss:0.0912\n",
      "Episode:534 meanR:11.6100 R:13.0 loss:0.0955\n",
      "Episode:535 meanR:11.6200 R:8.0 loss:0.0894\n",
      "Episode:536 meanR:11.6700 R:13.0 loss:0.0874\n",
      "Episode:537 meanR:11.7400 R:9.0 loss:0.0947\n",
      "Episode:538 meanR:11.7900 R:13.0 loss:0.0954\n",
      "Episode:539 meanR:11.8200 R:10.0 loss:0.0935\n",
      "Episode:540 meanR:11.9100 R:20.0 loss:0.0853\n",
      "Episode:541 meanR:11.8100 R:7.0 loss:0.0973\n",
      "Episode:542 meanR:11.7700 R:13.0 loss:0.0910\n",
      "Episode:543 meanR:11.8200 R:17.0 loss:0.0843\n",
      "Episode:544 meanR:11.7800 R:4.0 loss:0.0892\n",
      "Episode:545 meanR:11.7500 R:14.0 loss:0.0970\n",
      "Episode:546 meanR:11.7200 R:6.0 loss:0.0871\n",
      "Episode:547 meanR:11.7500 R:10.0 loss:0.0831\n",
      "Episode:548 meanR:11.6600 R:3.0 loss:0.0888\n",
      "Episode:549 meanR:11.6700 R:12.0 loss:0.0819\n",
      "Episode:550 meanR:11.6700 R:9.0 loss:0.0826\n",
      "Episode:551 meanR:11.6000 R:6.0 loss:0.0915\n",
      "Episode:552 meanR:11.5600 R:7.0 loss:0.0805\n",
      "Episode:553 meanR:11.5800 R:17.0 loss:0.0830\n",
      "Episode:554 meanR:11.4200 R:6.0 loss:0.0828\n",
      "Episode:555 meanR:11.4200 R:10.0 loss:0.0786\n",
      "Episode:556 meanR:11.2600 R:4.0 loss:0.0810\n",
      "Episode:557 meanR:11.3500 R:16.0 loss:0.0731\n",
      "Episode:558 meanR:11.3900 R:10.0 loss:0.0768\n",
      "Episode:559 meanR:11.4000 R:12.0 loss:0.0760\n",
      "Episode:560 meanR:11.4500 R:14.0 loss:0.0753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:561 meanR:11.4400 R:6.0 loss:0.0725\n",
      "Episode:562 meanR:11.3200 R:6.0 loss:0.0777\n",
      "Episode:563 meanR:11.3700 R:18.0 loss:0.0709\n",
      "Episode:564 meanR:11.3500 R:9.0 loss:0.0820\n",
      "Episode:565 meanR:11.4700 R:15.0 loss:0.0703\n",
      "Episode:566 meanR:11.5600 R:17.0 loss:0.0832\n",
      "Episode:567 meanR:11.7000 R:24.0 loss:0.0714\n",
      "Episode:568 meanR:11.7600 R:17.0 loss:0.0712\n",
      "Episode:569 meanR:11.7900 R:13.0 loss:0.0712\n",
      "Episode:570 meanR:11.9200 R:17.0 loss:0.0827\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEKCAYAAAAxXHOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuUnHWd5/H3p6q6O91JIBdChCSQgPGCrhfMIsquqzJCQBd0RkdQl8jgZnTwiM4VdGbxumd05oiyqwgKIzgIMo5KVBQziDqzR5Eg95tpAUkTJAm5EGjS1+/+8fyqU+lUd1enn7q0/XmdU6fq+T2/p+pbz+nUN7/L83sUEZiZmeWl0OwAzMzs94sTi5mZ5cqJxczMcuXEYmZmuXJiMTOzXDmxmJlZrpxYzMwsV04sZmaWKycWMzPLVanZATTaIYccEsuXL292GGZm08Ztt922LSIW1Vp/xiWW5cuXs2HDhmaHYWY2bUj67WTquyvMzMxy5cRiZma5cmIxM7NcObGYmVmunFjMzCxXTixmZpYrJxYzM8uVE8sk7N69m6GhoWaHYWbW0pxYajQ4OMjmzZvZvHlzs0MxM2tpTiw1iggABgYGmhyJmVlrc2IxM7NcObGYmVmunFjMzCxXTixmZpYrJxYzM8uVE4uZmeXKicXMzHLlxDJJ5etZzMysOicWMzPLlROLmZnlqq6JRdIjku6WdIekDalsgaT1kjam5/mpXJIultQt6S5Jx1a8z5pUf6OkNRXlr0jv352OVR2/S73e2szs90ojWiyvi4iXRcSqtH0+cFNErARuStsApwAr02MtcAlkiQi4EHglcBxwYTkZpTprK45bXf+vY2Zm42lGV9jpwJXp9ZXAmyvKr4rML4B5kg4DTgbWR8T2iNgBrAdWp30HRcTPIxtRv6rivXLnQXszs9rUO7EE8CNJt0lam8oWR8TjAOn50FS+BNhUcWxPKhuvvKdKuZmZNVGpzu9/QkRslnQosF7SA+PUrTaIEQdQvv8bZ0ltLcARRxwxfsRmZjYldW2xRMTm9LwF+DbZGMkTqRuL9LwlVe8BllUcvhTYPEH50irl1eK4LCJWRcSqRYsWTfVrmZnZOOqWWCTNljS3/Bo4CbgHWAeUZ3atAa5Pr9cBZ6XZYccDu1JX2Y3ASZLmp0H7k4Ab077dko5Ps8HOqngvMzNrknp2hS0Gvp2m6ZaAr0fEDyXdClwn6RzgUeBtqf4NwKlAN9ALnA0QEdslfQK4NdX7eERsT6/fB3wV6AR+kB5mZtZEmmmznVatWhUbNmyY9HEDAwM89NBDACxcuJBDDjkk79DMzFqSpNsqLhmZkK+8PwBPPvlks0MwM2tZTiw1mmktOzOzA+XEYmZmuXJiMTOzXDmxmJlZrpxYzMwsV04sZmaWKycWMzPLlROLmZnlyonFzMxy5cRSI18gaWZWGycWMzPLlROLmZnlyonFzMxy5cRiZma5cmIxM7NcObGYmVmunFjMzCxXTixmZpYrJxYzM8uVE0uNfOW9mVltnFjMzCxXTixmZpYrJxYzM8uVE4uZmeXKicXMzHLlxHKAhoaGmh2CmVlLcmKpUeV047t7dvHYE082MRozs9ZV98QiqSjpdknfS9srJN0iaaOkb0hqT+Udabs77V9e8R4XpPIHJZ1cUb46lXVLOr/e3wVg29N9fP6mjXzmxgca8XFmZtNOI1os5wH3V2x/GrgoIlYCO4BzUvk5wI6IeC5wUaqHpGOAM4AXAauBL6ZkVQS+AJwCHAOcmerW1bbdfQDsfKa/3h9lZjYt1TWxSFoKvBH4StoW8Hrgm6nKlcCb0+vT0zZp/4mp/unAtRHRFxEPA93AcenRHREPRUQ/cG2qW1d9Q8MAzGov1vujzMympXq3WD4H/DUwnLYXAjsjYjBt9wBL0uslwCaAtH9Xqj9SPuqYscr3I2mtpA2SNmzdunVKX2h4ONLz8AQ1zcxmprolFklvArZExG2VxVWqxgT7Jlu+f2HEZRGxKiJWLVq0aJyoJ5YaLF47zMxsDKU6vvcJwGmSTgVmAQeRtWDmSSqlVslSYHOq3wMsA3oklYCDge0V5WWVx4xVXjdDkWWWoWEnFjOzaurWYomICyJiaUQsJxt8/3FEvBO4GXhrqrYGuD69Xpe2Sft/HFmzYB1wRpo1tgJYCfwSuBVYmWaZtafPWFev71O2t8VS708yM5ue6tliGcvfANdK+iRwO3B5Kr8c+JqkbrKWyhkAEXGvpOuA+4BB4NyIGAKQ9H7gRqAIXBER99Y7+JExFmcWM7OqGpJYIuInwE/S64fIZnSNrrMHeNsYx38K+FSV8huAG3IMdUJDKaH0DfrKezOzanzlfY3Kg/XlFktvvxOLmVk1TiyTNDiSWDzd2MysGieWSSqPrfT2DzQ5EjOz1uTEMknlFkvfYPhaFjOzKpxYJmm44vqVp/sGx6lpZjYzTZhYJH1G0kGS2iTdJGmbpHc1IrhWVJlYdva6O8zMbLRaWiwnRcRTwJvIroJ/HvBXdY2qhQ2FE4uZ2XhqSSxt6flU4JqI2F7HeFreUMWwys5nvXS+mdlotVwg+V1JDwDPAn8maRGwp75htZ7yQP1QWtNFuMViZlbNhC2WiDgfeBWwKiIGgF4acN+TVrVPV9izTixmZqPVMnjfBZwLXJKKDgdW1TOoVjY0DF0d2U2+fBdJM7P91TLG8k9AP/DqtN0DfLJuEbW44Rimo1igs63ALrdYzMz2U0tiOToiPgMMAETEs1S/ydaMMDQEhYJoL4r+IS/rYmY2Wi2JpV9SJ+nujJKOBvrqGlULG4qgKFEqFhhwYjEz208ts8IuBH4ILJN0NdmdId9dz6Ba2VAExaJoQ/QPekkXM7PRJkwsEbFe0q+A48m6wM6LiG11j6xFDQ9nLZa2QsFdYWZmVdQyK+wEYE9EfB+YB3xY0pF1j6xFDQ0HhYJoK4qBQScWM7PRahljuQTolfRSsqVcfgtcVdeoWtDIBZJpjKVT/R5jMTOropbEMhjZr+rpwMUR8Xlgbn3Dal1Dw0GxIEoFzwozM6umlsH73ZIuAN4FvEZSkb3rh804WWKBQoin3RVmZrafWlosbyebXnxORPwOWAL8Q12jamHZ4H3B043NzMZQU4sF+HxEDEl6HvAC4Jr6htW6hiJQqUgBGPB0YzOz/dTSYvkZ0CFpCXATcDbw1XoG1cqGhqFYKFIq4BaLmVkVtSQWRUQv8IfA/4mItwAvqm9YrWtoeBiV2rPBe4+xmJntp6bEIulVwDuB76eyYv1Cam3bCwdTKBY8K8zMbAy1JJYPAhcA346IeyUdBdxc37Ba1+BQUFI23dhdYWZm+6tlSZefAj+VNFfSnIh4CPhA/UNrLSMXSA4HpWKBEmJgyIP3Zmaj1bKky3+SdDtwD3CfpNskTTjGImmWpF9KulPSvZI+lspXSLpF0kZJ35DUnso70nZ32r+84r0uSOUPSjq5onx1KuuWdP7kv/7kDQ4NUyyIYqHAwOBQIz7SzGxaqaUr7FLgzyPiyIg4AvgL4Ms1HNcHvD4iXgq8DFgt6Xjg08BFEbES2AGck+qfA+yIiOcCF6V6SDoGOINswsBq4IuSiulCzS8ApwDHAGemunU1NBwUiwVKRdHnrjAzs/3UklhmR8TImEpE/ASYPdFBkXk6bbalRwCvB76Zyq8E3pxen562SftPlKRUfm1E9EXEw0A3cFx6dEfEQxHRD1yb6tbVwPDwyBhL/+DwSBeZmZllakksD0n6O0nL0+NvgYdrefPUsrgD2AKsB34D7IyIwVSlh+xKftLzJoC0fxewsLJ81DFjlddVdh1LgbaiEHicxcxslFoSy58Ai4BvAd9Or8+u5c0jYigiXgYsJWthvLBatfRc7XbHcQDl+5G0VtIGSRu2bt06ceDjWPuaoznuqAW0FbNT1+dxFjOzfdQyK2wHU5wFFhE7Jf2E7GZh8ySVUqtkKbA5VesBlgE9kkrAwcD2ivKyymPGKh/9+ZcBlwGsWrVqSk2M9/zXFfT29vLwpseBoG9weOYu9WxmVsWYiUXSdxmjBQAQEaeN98aSFgEDKal0An9ANiB/M/BWsjGRNcD16ZB1afvnaf+PIyIkrQO+LumzwOHASuCXZC2WlZJWAI+RDfC/Y8JvPEXZsA+0FQsI2DPgFouZWaXxWiz/OMX3Pgy4Ms3eKgDXRcT3JN0HXCvpk8DtwOWp/uXA1yR1k7VUzgBIF2VeB9wHDALnRsQQgKT3AzeSrQRwRUTcO8WYx1Q5SC+JtlKWYPq8rIuZ2T7GTCzpwsgDFhF3AS+vUv4Q2XjL6PI9wNvGeK9PAZ+qUn4DcMNU4jxQbYUCbQy5xWJmNkotg/c2Snt7O22lAoU0xmJmZns5sRyAUqnkMRYzszHUnFgkTXhR5EzSVvQYi5lZNbWsFfbqNOB+f9p+qaQv1j2yFiaJ9vJ1LG6xmJnto5YWy0XAycCTABFxJ/CaegY1HbSVCkhusZiZjVZTV1hEbBpVNOP/m55deR8eYzEzG2XCK++BTZJeDURa4v4DpG6xmUqSx1jMzMZQS4vlvcC5ZAs89pAtgX9uPYNqReULJMtX3rd7VpiZWVW1rBW2jex+95ZkLZby4L1bLGZmlSZMLJIurlK8C9gQEddX2TcjFAuiKLHHqxubme2jlq6wWWTdXxvT4yXAAuAcSZ+rY2wtb25piD39gxNXNDObQWoZvH8u2S2GBwEkXQL8CHgDcHcdY2tZ5XGWuaUh+p96ssnRmJm1llpaLEvY91bEs4HD0wrDfXWJappoK4qB/v5mh2Fm1lJqabF8Brgj3ahLZBdH/u+0xMu/1TG2ltdeLDAw5MF7M7NKtcwKu1zSDWRL3Qv4cESU79T4V/UMrlVV3uyrz/e8NzPbR62LUO4BHie7AddzJc3YJV3KSQWgvVSg37PCzMz2Uct04/cA55HdU/4OsvvW/xx4fX1Day2j7yAJ5cTirjAzs0q1tFjOA/4z8NuIeB3ZXSG31jWqaaKjWKDPYyxmZvuoJbHsSbcNRlJHRDwAPL++YU0PbaUC/b7y3sxsH7XMCuuRNA/4DrBe0g5g8wTHzAjtpQL9Hrw3M9tHLbPC3pJeflTSzcDBwA/rGtU00VEseBFKM7NRxk0skgrAXRHxYoCI+GlDopom2tuKHmMxMxtl3DGWiBgG7pR0RIPimVbai9mssOFhd4eZmZXVMsZyGHCvpF8Cz5QLI+K0ukU1TZTvydI3OExne7HZ4ZiZtYRaEsvH6h7FNDD6Rl8A7aXs9bMDQ04sZmZJLYP3P5V0JLAyIv5NUhfgX1GyWWEi6O0fZMHs9maHY2bWEia8jkXS/wS+CVyaipaQTT2e8braSxQJdj69p9mhmJm1jFoukDwXOAF4CiAiNgKHTnSQpGWSbpZ0v6R7JZ2XyhdIWi9pY3qen8ol6WJJ3ZLuknRsxXutSfU3SlpTUf4KSXenYy5WZT9VA3Sl7q8dT/c28mPNzFpaLYmlLyJGbjoiqQTUMg1qEPiLiHgh2fpi50o6BjgfuCkiVgI3pW2AU4CV6bEWuCR93gLgQuCVZCssX1hORqnO2orjVtcQV2662rOexN17fBdJM7OyWhLLTyV9GOiU9AbgX4DvTnRQRDweEb9Kr3cD95N1o50OXJmqXQm8Ob0+HbgqMr8A5kk6DDgZWB8R2yNiB7AeWJ32HRQRP49sZP2qivdqiI627PQ90+fEYmZWVktiOZ9s0cm7gT8FbgD+djIfImk52eKVtwCLI+JxyJIPe7vVlgCbKg7rSWXjlfdUKW+YtmJ2+rx0vpnZXrVMNy63JL58IB8gaQ7wr8AHI+KpcYZBqu2IAyivFsNasi4zjjgiv2s924pZCH1eOt/MbEQtLZbTgF9L+pqkN6YxlppIaiNLKldHxLdS8ROpG4v0vCWV9wDLKg5fSrbY5XjlS6uU7yciLouIVRGxatGiRbWGP6GRFovXCzMzGzFhYomIs4Hnko2tvAP4jaSvTHRcmqF1OXB/RHy2Ytc6oDyzaw1wfUX5WWl22PHArtRVdiNwkqT5adD+JODGtG+3pOPTZ51V8V65q3aBZFshO3197gozMxtRU+sjIgYk/YCsq6mTrHvsPRMcdgLwP4C7Jd2Ryj4M/D1wnaRzgEeBt6V9NwCnAt1AL3B2+uztkj4B3JrqfTwitqfX7wO+mmL6QXo0TLEACN9F0sysQi23Jl4NnAG8DvgJ8BXgjyc6LiL+g+rjIAAnVqkfZNfMVHuvK4ArqpRvAF48USz1IiktROkWi5lZWS0tlncD1wJ/GhF99Q1n+ukoFXim34nFzKyslrXCzqjclnQC8I6IqNq6mGm62os87QskzcxG1DTGIullZAP3fww8DHxr/CNmjq72Ert9gaSZ2YgxE4uk55GNrZwJPAl8A1BEvK5BsU0Lne1FtrvFYmY2YrwWywPAvwP/PSK6ASR9qCFRTSNd7UUe3TnQ7DDMzFrGeNex/BHwO+BmSV+WdCJjz/L6vVftOhaA2R3uCjMzqzRmYomIb0fE24EXkE0z/hCwWNIlkk5qUHwtr7MtG7wvJx4zs5mulivvn4mIqyPiTWTLptzB3qXuZ7zZ7SUGhofZM+CLJM3MoLa1wkakpesvjYjX1yug6aarI7vZ11N7PM5iZgaTTCy219FHHw3svYvkrmedWMzMwInlgJVK2YS6rvYSAp5yYjEzA5xYpqzcYnFXmJlZxollisr3vXdXmJlZxollirras1O4q9eJxcwMnFhqNtYFkp1tJebrWa9wbGaWOLFMUakoigXxjK++NzMDnFhy0VEq0OsWi5kZ4MQyJStWrACgo61Ab79bLGZm4MQyJeVrWXwXSTOzvZxYJmH0wH15e1ZbkV6PsZiZAU4suXCLxcxsLyeWHMwqFdnTt6fZYZiZtQQnlhqNd7+V9lKBwT1OLGZm4MSSi1ltRXeFmZklTixTUB68n9tZYkfvAMPDvoukmZkTyySMnhVWNr+znaEInnymv8ERmZm1HieWHJTvIukVjs3MnFhqNt7gfWdbkQjxtK9lMTOrX2KRdIWkLZLuqShbIGm9pI3peX4ql6SLJXVLukvSsRXHrEn1N0paU1H+Ckl3p2Mu1lj9VA0wq5S1WLwQpZlZfVssXwVWjyo7H7gpIlYCN6VtgFOAlemxFrgEskQEXAi8EjgOuLCcjFKdtRXHjf6shpnVXiSA3XucWMzM6pZYIuJnwPZRxacDV6bXVwJvrii/KjK/AOZJOgw4GVgfEdsjYgewHlid9h0UET+PrI/qqor3qpuxGkWdbVmLxV1hZmaNH2NZHBGPA6TnQ1P5EmBTRb2eVDZeeU+V8qokrZW0QdKGrVu3TvlLjDarLTuN7gozM2udwftqTYE4gPKqIuKyiFgVEasWLVp0QAGON3g/yy0WM7MRjU4sT6RuLNLzllTeAyyrqLcU2DxB+dIq5U3RVizQVvSsMDMzaHxiWQeUZ3atAa6vKD8rzQ47HtiVuspuBE6SND8N2p8E3Jj27ZZ0fJoNdlbFezXUvHnzAFjcBU/s8nphZmaler2xpGuA1wKHSOohm93198B1ks4BHgXelqrfAJwKdAO9wNkAEbFd0ieAW1O9j0dEeULA+8hmnnUCP0iPuqo2eD9//nx27tzJUQcX2fT47+odgplZy6tbYomIM8fYdWKVugGcO8b7XAFcUaV8A/DiqcSYh3KyOWhWG088sbvJ0ZiZNV+rDN63vPEG7wHmdBQ9xmJmhhNLbmZ3lOgdDPoHh5sdiplZUzmxTFG5K2xOR4lBCuzs9QrHZjazObFMwnjLkc3uyIardnqFYzOb4ZxYcjInJZYdvieLmc1wTiw1GmvwvtyKKbdYdvS6xWJmM5sTS07mzEpdYR5jMbMZzoklJwfNKiHgsZ3PNjsUM7OmcmKZhPEG79uKBVYs7OLOnl0NjMjMrPU4sUxRsVgceb14bjub3WIxsxnOiaVGE115D/CcuW08su1pevt9Bb6ZzVxOLDlYtmwZpVKJIxfOZmg4eHjbM80OycysaZxYctDV1cWCBQtYMLsdCDbv9PL5ZjZzObFMwniD95JYOKc9mxm2o7dxQZmZtRgnlpxIYm5HiVmlgqccm9mM5sRSo4kG7yUhicPnltiyu69BUZmZtR4nlpwt69jDlqecWMxs5nJiycnQ0BAAB3e2seUpd4WZ2czlxJKT4eHsBl8Hd5bY87SvvjezmcuJZRLGmxXW2dkJwLyudvr7+ti9x6scm9nM5MRSo4kG77u6uliyZAlL53fST5EHfre7QZGZmbUWJ5YczZkzhyMXzgbgnsfcHWZmM5MTS87mz+5g0Zw27ti0s9mhmJk1hRPLJIw3xlJWLBZ5weK5/OrRHQ2IyMys9Tix5KxQKPC8BUU2be9lqy+UNLMZyIklZ319fRy9aA5dDHC7Wy1mNgM5sdSolvuxlB25sIsFxT386lGPs5jZzDPtE4uk1ZIelNQt6fx6fU5EUChMfLqOOOII2ooFjlo4i3/fuJXh4doTkpnZ74NpnVgkFYEvAKcAxwBnSjqmHp81PDxcU2KZNWsWAP/t+Yfy0OZtXPH/Hq5HOGZmLavU7ACm6DigOyIeApB0LXA6cF/eH1RrYpHEypUrKRQK3LlpJ5//4T0sXzib173gUIqFiWeVmZlNd9M9sSwBNlVs9wCvrMcH1ZpYIJsZtnjxYta8up8nfvgAF/7zTXysWGBuR4n20t73KE9fHp1utLfC1AM3MwPmzGrnyg+c2pDPmu6Jpdov736DGpLWAmshGwM5EHPnzh1ZD6wWBx10EC895vlcfPhzuPm+zfzmid3s3jNA/1C2WGV5LsBIsKlg7/YBhWlmVlVXR1vDPmu6J5YeYFnF9lJg8+hKEXEZcBnAqlWrDugne/HixZM+plQqsfiQhZzxmoUH8pFmZtPStB68B24FVkpaIakdOANY1+SYzMxmtGndYomIQUnvB24EisAVEXFvk8MyM5vRpnViAYiIG4Abmh2HmZllpntXmJmZtRgnFjMzy5UTi5mZ5cqJxczMcuXEYmZmudJkloP/fSBpK/DbAzz8EGBbjuHU23SLFxxzI0y3eGH6xTzd4oXxYz4yIhbV+kYzLrFMhaQNEbGq2XHUarrFC465EaZbvDD9Yp5u8UK+MbsrzMzMcuXEYmZmuXJimZzLmh3AJE23eMExN8J0ixemX8zTLV7IMWaPsZiZWa7cYjEzs1w5sdRA0mpJD0rqlnR+s+Mpk7RM0s2S7pd0r6TzUvkCSeslbUzP81O5JF2cvsddko5tUtxFSbdL+l7aXiHplhTvN9ItEJDUkba70/7lTYp3nqRvSnognetXtfI5lvSh9Pdwj6RrJM1qtXMs6QpJWyTdU1E26XMqaU2qv1HSmibE/A/p7+IuSd+WNK9i3wUp5gclnVxR3rDfk2oxV+z7S0kh6ZC0nd95jgg/xnmQLcf/G+AooB24Ezim2XGl2A4Djk2v5wK/Bo4BPgOcn8rPBz6dXp8K/IDszpvHA7c0Ke4/B74OfC9tXweckV5/CXhfev1nwJfS6zOAbzQp3iuB96TX7cC8Vj3HZLfrfhjorDi37261cwy8BjgWuKeibFLnFFgAPJSe56fX8xsc80lAKb3+dEXMx6Tfig5gRfoNKTb696RazKl8GdntRn4LHJL3eW7YH/x0fQCvAm6s2L4AuKDZcY0R6/XAG4AHgcNS2WHAg+n1pcCZFfVH6jUwxqXATcDrge+lP+JtFf84R853+sN/VXpdSvXU4HgPSj/UGlXekueYLLFsSj8CpXSOT27FcwwsH/UjPalzCpwJXFpRvk+9RsQ8at9bgKvT631+J8rnuRm/J9ViBr4JvBR4hL2JJbfz7K6wiZX/oZb1pLKWkrowXg7cAiyOiMcB0vOhqVorfJfPAX8NDKfthcDOiBisEtNIvGn/rlS/kY4CtgL/lLrvviJpNi16jiPiMeAfgUeBx8nO2W209jkum+w5bYW/50p/QvY/fmjhmCWdBjwWEXeO2pVbzE4sE1OVspaaSidpDvCvwAcj4qnxqlYpa9h3kfQmYEtE3FZZXKVq1LCvUUpkXQmXRMTLgWfIumnG0uxzPB84naz75XBgNnDKODG1wjmeyFgxtkzskj4CDAJXl4uqVGt6zJK6gI8A/6va7iplBxSzE8vEesj6I8uWApubFMt+JLWRJZWrI+JbqfgJSYel/YcBW1J5s7/LCcBpkh4BriXrDvscME9S+W6mlTGNxJv2Hwxsb2C85Rh6IuKWtP1NskTTquf4D4CHI2JrRAwA3wJeTWuf47LJntNmn2sgG9gG3gS8M1Jf0TixNTvmo8n+03Fn+ne4FPiVpOeME9ukY3ZimditwMo0q6adbIBzXZNjArJZHMDlwP0R8dmKXeuA8syNNWRjL+Xys9Lsj+OBXeWuh0aIiAsiYmlELCc7jz+OiHcCNwNvHSPe8vd4a6rf0P+RRsTvgE2Snp+KTgTuo0XPMVkX2PGSutLfRznelj3HFSZ7Tm8ETpI0P7XUTkplDSNpNfA3wGkR0Vuxax1wRpp1twJYCfySJv+eRMTdEXFoRCxP/w57yCYA/Y48z3M9B41+Xx5ksyV+TTab4yPNjqcirv9C1iS9C7gjPU4l6yO/CdiYnhek+gK+kL7H3cCqJsb+WvbOCjuK7B9dN/AvQEcqn5W2u9P+o5oU68uADek8f4dsZkzLnmPgY8ADwD3A18hmJrXUOQauIRsDGkg/buccyDklG9foTo+zmxBzN9n4Q/nf35cq6n8kxfwgcEpFecN+T6rFPGr/I+wdvM/tPPvKezMzy5W7wszMLFdOLGZmlisnFjMzy5UTi5mZ5cqJxczMcuXEYjYFkoYk3VHxGHe1WknvlXRWDp/7SHlVWrNW4+nGZlMg6emImNOEz32E7DqDbY3+bLOJuMViVgepRfFpSb9Mj+em8o9K+sv0+gOS7kv3vrg2lS2Q9J1U9gtJL0nlCyX9KC2EeSkV6zdJelf6jDskXSqp2ISvbDbCicVsajpHdYW9vWLfUxFxHPB/ydZEG+184OUR8RLgvansY8DtqezDwFWp/ELgPyJbCHMdcASApBcCbwdOiIiXAUPAO/P9imaTU5q4ipmN49lyTF9OAAABLElEQVT0g17NNRXPF1XZfxdwtaTvkC0VA9kyPX8EEBE/Ti2Vg8lu2PSHqfz7knak+icCrwBuzZYGo5O9izeaNYUTi1n9xBivy95IljBOA/5O0osYf4nyau8h4MqIuGAqgZrlyV1hZvXz9ornn1fukFQAlkXEzWQ3PpsHzAF+RurKkvRaYFtk99ipLD+FbCFMyBZrfKukQ9O+BZKOrON3MpuQWyxmU9Mp6Y6K7R9GRHnKcYekW8j+A3fmqOOKwD+nbi4BF0XETkkfJbtb5V1AL3uXkf8YcI2kXwE/JVsen4i4T9LfAj9KyWoAOJfsXuZmTeHpxmZ14OnANpO5K8zMzHLlFouZmeXKLRYzM8uVE4uZmeXKicXMzHLlxGJmZrlyYjEzs1w5sZiZWa7+PyNjYttb/ukbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
