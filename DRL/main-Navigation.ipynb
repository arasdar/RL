{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.55250424,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.35652775,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.40146229,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.39749894,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.24460451,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.15436889,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.55250424,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.35652775,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.40146229,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.39749894,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.24460451,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.15436889,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "1.0 0.0\n",
      "10.537452697753906 -10.25605297088623\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return actions, states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(tf.multiply(actions_logits[:-1], actions_onehot[1:]), axis=1)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs, labels=tf.nn.sigmoid(targetQs[1:])))    \n",
    "    #loss = tf.reduce_mean(tf.square(Qs - targetQs[1:]))\n",
    "    #loss = tf.reduce_mean(tf.square(Qs[:-1] - targetQs[2:]))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 37) actions:(300,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print(np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# train_episodes = 1             # max number of episodes to learn from\n",
    "# max_steps = 3000000000         # max steps in an episode\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37                # number of units for the input state/observation -- simulation\n",
    "action_size = 4                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 1000              # number of samples in the memory/ experience as mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = \n",
    "env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "#while True: # infinite number of steps\n",
    "for _ in range(batch_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([action, state, done])\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        #env_info = \n",
    "        env.reset(train_mode=True)[brain_name] # reset the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 1.0 Average loss: 0.082005568 Explore P: 0.970741078\n",
      "Episode: 1 Total reward: 0.0 Average loss: 0.003912160 Explore P: 0.942346888\n",
      "Episode: 2 Total reward: 1.0 Average loss: 0.003585745 Explore P: 0.914791873\n",
      "Episode: 3 Total reward: 0.0 Average loss: 0.002743948 Explore P: 0.888051232\n",
      "Episode: 4 Total reward: 2.0 Average loss: 0.003112026 Explore P: 0.862100897\n",
      "Episode: 5 Total reward: 1.0 Average loss: 0.002880048 Explore P: 0.836917509\n",
      "Episode: 6 Total reward: 0.0 Average loss: 0.002880277 Explore P: 0.812478404\n",
      "Episode: 7 Total reward: 1.0 Average loss: 0.003482430 Explore P: 0.788761582\n",
      "Episode: 8 Total reward: -1.0 Average loss: 0.003046001 Explore P: 0.765745699\n",
      "Episode: 9 Total reward: 2.0 Average loss: 0.003337937 Explore P: 0.743410038\n",
      "Episode: 10 Total reward: 1.0 Average loss: 0.002923023 Explore P: 0.721734496\n",
      "Episode: 11 Total reward: 1.0 Average loss: 0.002601919 Explore P: 0.700699563\n",
      "Episode: 12 Total reward: 1.0 Average loss: 0.002873727 Explore P: 0.680286306\n",
      "Episode: 13 Total reward: 1.0 Average loss: 0.002513531 Explore P: 0.660476352\n",
      "Episode: 14 Total reward: 0.0 Average loss: 0.002606565 Explore P: 0.641251870\n",
      "Episode: 15 Total reward: 0.0 Average loss: 0.002769894 Explore P: 0.622595558\n",
      "Episode: 16 Total reward: 0.0 Average loss: 0.002700510 Explore P: 0.604490623\n",
      "Episode: 17 Total reward: 0.0 Average loss: 0.002663618 Explore P: 0.586920770\n",
      "Episode: 18 Total reward: 1.0 Average loss: 0.002834385 Explore P: 0.569870184\n",
      "Episode: 19 Total reward: 0.0 Average loss: 0.002503546 Explore P: 0.553323520\n",
      "Episode: 20 Total reward: 0.0 Average loss: 0.002930406 Explore P: 0.537265883\n",
      "Episode: 21 Total reward: -1.0 Average loss: 0.003139349 Explore P: 0.521682821\n",
      "Episode: 22 Total reward: -2.0 Average loss: 0.003134625 Explore P: 0.506560308\n",
      "Episode: 23 Total reward: 1.0 Average loss: 0.003137299 Explore P: 0.491884733\n",
      "Episode: 24 Total reward: 1.0 Average loss: 0.002658984 Explore P: 0.477642887\n",
      "Episode: 25 Total reward: 0.0 Average loss: 0.002468127 Explore P: 0.463821951\n",
      "Episode: 26 Total reward: 0.0 Average loss: 0.002488579 Explore P: 0.450409486\n",
      "Episode: 27 Total reward: 2.0 Average loss: 0.002753119 Explore P: 0.437393418\n",
      "Episode: 28 Total reward: 0.0 Average loss: 0.002412634 Explore P: 0.424762034\n",
      "Episode: 29 Total reward: 0.0 Average loss: 0.002665431 Explore P: 0.412503963\n",
      "Episode: 30 Total reward: 1.0 Average loss: 0.002757207 Explore P: 0.400608173\n",
      "Episode: 31 Total reward: 0.0 Average loss: 0.002461098 Explore P: 0.389063957\n",
      "Episode: 32 Total reward: 1.0 Average loss: 0.002666798 Explore P: 0.377860924\n",
      "Episode: 33 Total reward: 0.0 Average loss: 0.002529200 Explore P: 0.366988991\n",
      "Episode: 34 Total reward: 0.0 Average loss: 0.002747646 Explore P: 0.356438372\n",
      "Episode: 35 Total reward: 1.0 Average loss: 0.002545577 Explore P: 0.346199570\n",
      "Episode: 36 Total reward: 0.0 Average loss: 0.002708559 Explore P: 0.336263371\n",
      "Episode: 37 Total reward: -2.0 Average loss: 0.002575570 Explore P: 0.326620832\n",
      "Episode: 38 Total reward: 2.0 Average loss: 0.002639219 Explore P: 0.317263272\n",
      "Episode: 39 Total reward: 2.0 Average loss: 0.002677385 Explore P: 0.308182270\n",
      "Episode: 40 Total reward: 0.0 Average loss: 0.002603540 Explore P: 0.299369652\n",
      "Episode: 41 Total reward: 0.0 Average loss: 0.003543827 Explore P: 0.290817486\n",
      "Episode: 42 Total reward: -1.0 Average loss: 0.003040639 Explore P: 0.282518075\n",
      "Episode: 43 Total reward: 0.0 Average loss: 0.002865287 Explore P: 0.274463949\n",
      "Episode: 44 Total reward: -3.0 Average loss: 0.003059791 Explore P: 0.266647858\n",
      "Episode: 45 Total reward: -2.0 Average loss: 0.002956758 Explore P: 0.259062768\n",
      "Episode: 46 Total reward: 0.0 Average loss: 0.003247115 Explore P: 0.251701850\n",
      "Episode: 47 Total reward: 0.0 Average loss: 0.002967841 Explore P: 0.244558481\n",
      "Episode: 48 Total reward: 0.0 Average loss: 0.003240322 Explore P: 0.237626230\n",
      "Episode: 49 Total reward: 0.0 Average loss: 0.002781078 Explore P: 0.230898859\n",
      "Episode: 50 Total reward: 1.0 Average loss: 0.002504930 Explore P: 0.224370311\n",
      "Episode: 51 Total reward: 0.0 Average loss: 0.004628593 Explore P: 0.218034710\n",
      "Episode: 52 Total reward: -1.0 Average loss: 0.004747058 Explore P: 0.211886356\n",
      "Episode: 53 Total reward: 1.0 Average loss: 0.004391396 Explore P: 0.205919712\n",
      "Episode: 54 Total reward: 2.0 Average loss: 0.004327580 Explore P: 0.200129410\n",
      "Episode: 55 Total reward: 0.0 Average loss: 0.004144605 Explore P: 0.194510236\n",
      "Episode: 56 Total reward: -1.0 Average loss: 0.004334911 Explore P: 0.189057135\n",
      "Episode: 57 Total reward: 0.0 Average loss: 0.003693340 Explore P: 0.183765197\n",
      "Episode: 58 Total reward: -1.0 Average loss: 0.003981762 Explore P: 0.178629659\n",
      "Episode: 59 Total reward: 0.0 Average loss: 0.004151638 Explore P: 0.173645899\n",
      "Episode: 60 Total reward: -1.0 Average loss: 0.004924477 Explore P: 0.168809432\n",
      "Episode: 61 Total reward: 1.0 Average loss: 0.006206034 Explore P: 0.164115904\n",
      "Episode: 62 Total reward: -1.0 Average loss: 0.005277463 Explore P: 0.159561091\n",
      "Episode: 63 Total reward: 0.0 Average loss: 0.004544978 Explore P: 0.155140893\n",
      "Episode: 64 Total reward: 0.0 Average loss: 0.003442168 Explore P: 0.150851331\n",
      "Episode: 65 Total reward: 1.0 Average loss: 0.002829875 Explore P: 0.146688545\n",
      "Episode: 66 Total reward: 0.0 Average loss: 0.002499954 Explore P: 0.142648788\n",
      "Episode: 67 Total reward: -1.0 Average loss: 0.002589411 Explore P: 0.138728424\n",
      "Episode: 68 Total reward: -2.0 Average loss: 0.002597308 Explore P: 0.134923924\n",
      "Episode: 69 Total reward: -1.0 Average loss: 0.002873499 Explore P: 0.131231864\n",
      "Episode: 70 Total reward: 0.0 Average loss: 0.003608255 Explore P: 0.127648921\n",
      "Episode: 71 Total reward: 3.0 Average loss: 0.004126960 Explore P: 0.124171870\n",
      "Episode: 72 Total reward: 2.0 Average loss: 0.006004871 Explore P: 0.120797581\n",
      "Episode: 73 Total reward: 1.0 Average loss: 0.005844619 Explore P: 0.117523018\n",
      "Episode: 74 Total reward: 0.0 Average loss: 0.004261399 Explore P: 0.114345232\n",
      "Episode: 75 Total reward: -1.0 Average loss: 0.003109497 Explore P: 0.111261365\n",
      "Episode: 76 Total reward: 0.0 Average loss: 0.003529662 Explore P: 0.108268639\n",
      "Episode: 77 Total reward: -1.0 Average loss: 0.003474583 Explore P: 0.105364362\n",
      "Episode: 78 Total reward: 0.0 Average loss: 0.005137061 Explore P: 0.102545919\n",
      "Episode: 79 Total reward: 2.0 Average loss: 0.004485804 Explore P: 0.099810774\n",
      "Episode: 80 Total reward: -1.0 Average loss: 0.004880172 Explore P: 0.097156464\n",
      "Episode: 81 Total reward: 0.0 Average loss: 0.003891107 Explore P: 0.094580601\n",
      "Episode: 82 Total reward: 0.0 Average loss: 0.003424865 Explore P: 0.092080867\n",
      "Episode: 83 Total reward: 0.0 Average loss: 0.005746504 Explore P: 0.089655011\n",
      "Episode: 84 Total reward: 0.0 Average loss: 0.005874516 Explore P: 0.087300849\n",
      "Episode: 85 Total reward: -1.0 Average loss: 0.007565784 Explore P: 0.085016264\n",
      "Episode: 86 Total reward: -1.0 Average loss: 0.006015776 Explore P: 0.082799198\n",
      "Episode: 87 Total reward: 0.0 Average loss: 0.004667985 Explore P: 0.080647657\n",
      "Episode: 88 Total reward: 2.0 Average loss: 0.003249010 Explore P: 0.078559703\n",
      "Episode: 89 Total reward: 0.0 Average loss: 0.002717294 Explore P: 0.076533458\n",
      "Episode: 90 Total reward: -1.0 Average loss: 0.003900502 Explore P: 0.074567097\n",
      "Episode: 91 Total reward: 0.0 Average loss: 0.004322168 Explore P: 0.072658851\n",
      "Episode: 92 Total reward: -1.0 Average loss: 0.006021553 Explore P: 0.070807002\n",
      "Episode: 93 Total reward: 0.0 Average loss: 0.005417445 Explore P: 0.069009883\n",
      "Episode: 94 Total reward: 0.0 Average loss: 0.006021785 Explore P: 0.067265878\n",
      "Episode: 95 Total reward: 0.0 Average loss: 0.004990530 Explore P: 0.065573415\n",
      "Episode: 96 Total reward: 1.0 Average loss: 0.004289933 Explore P: 0.063930973\n",
      "Episode: 97 Total reward: 0.0 Average loss: 0.004824028 Explore P: 0.062337071\n",
      "Episode: 98 Total reward: -1.0 Average loss: 0.005906396 Explore P: 0.060790277\n",
      "Episode: 99 Total reward: 0.0 Average loss: 0.006033983 Explore P: 0.059289198\n",
      "Episode: 100 Total reward: 0.0 Average loss: 0.005824081 Explore P: 0.057832482\n",
      "Episode: 101 Total reward: -1.0 Average loss: 0.004748365 Explore P: 0.056418818\n",
      "Episode: 102 Total reward: 0.0 Average loss: 0.005408546 Explore P: 0.055046935\n",
      "Episode: 103 Total reward: 1.0 Average loss: 0.004262164 Explore P: 0.053715597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 104 Total reward: 0.0 Average loss: 0.005418715 Explore P: 0.052423606\n",
      "Episode: 105 Total reward: 1.0 Average loss: 0.007170116 Explore P: 0.051169799\n",
      "Episode: 106 Total reward: 0.0 Average loss: 0.006779387 Explore P: 0.049953047\n",
      "Episode: 107 Total reward: 0.0 Average loss: 0.005487750 Explore P: 0.048772256\n",
      "Episode: 108 Total reward: 1.0 Average loss: 0.004277959 Explore P: 0.047626363\n",
      "Episode: 109 Total reward: 1.0 Average loss: 0.003376219 Explore P: 0.046514336\n",
      "Episode: 110 Total reward: -2.0 Average loss: 0.003212612 Explore P: 0.045435174\n",
      "Episode: 111 Total reward: 0.0 Average loss: 0.005165759 Explore P: 0.044387906\n",
      "Episode: 112 Total reward: 0.0 Average loss: 0.005001122 Explore P: 0.043371590\n",
      "Episode: 113 Total reward: 0.0 Average loss: 0.005007234 Explore P: 0.042385311\n",
      "Episode: 114 Total reward: -1.0 Average loss: 0.004890116 Explore P: 0.041428180\n",
      "Episode: 115 Total reward: 0.0 Average loss: 0.004275976 Explore P: 0.040499337\n",
      "Episode: 116 Total reward: 0.0 Average loss: 0.004282502 Explore P: 0.039597945\n",
      "Episode: 117 Total reward: 0.0 Average loss: 0.005375517 Explore P: 0.038723194\n",
      "Episode: 118 Total reward: 1.0 Average loss: 0.004718073 Explore P: 0.037874295\n",
      "Episode: 119 Total reward: 0.0 Average loss: 0.005299907 Explore P: 0.037050485\n",
      "Episode: 120 Total reward: 0.0 Average loss: 0.005849760 Explore P: 0.036251023\n",
      "Episode: 121 Total reward: 0.0 Average loss: 0.005432043 Explore P: 0.035475188\n",
      "Episode: 122 Total reward: 0.0 Average loss: 0.006750902 Explore P: 0.034722282\n",
      "Episode: 123 Total reward: -1.0 Average loss: 0.006739685 Explore P: 0.033991628\n",
      "Episode: 124 Total reward: -4.0 Average loss: 0.005137920 Explore P: 0.033282568\n",
      "Episode: 125 Total reward: 0.0 Average loss: 0.004407925 Explore P: 0.032594465\n",
      "Episode: 126 Total reward: -3.0 Average loss: 0.003253370 Explore P: 0.031926697\n",
      "Episode: 127 Total reward: -2.0 Average loss: 0.005768771 Explore P: 0.031278665\n",
      "Episode: 128 Total reward: -1.0 Average loss: 0.005206133 Explore P: 0.030649786\n",
      "Episode: 129 Total reward: -1.0 Average loss: 0.006822678 Explore P: 0.030039492\n",
      "Episode: 130 Total reward: 1.0 Average loss: 0.006244394 Explore P: 0.029447236\n",
      "Episode: 131 Total reward: -1.0 Average loss: 0.006333250 Explore P: 0.028872483\n",
      "Episode: 132 Total reward: 0.0 Average loss: 0.005045236 Explore P: 0.028314717\n",
      "Episode: 133 Total reward: 0.0 Average loss: 0.003788435 Explore P: 0.027773435\n",
      "Episode: 134 Total reward: -1.0 Average loss: 0.005612467 Explore P: 0.027248151\n",
      "Episode: 135 Total reward: 0.0 Average loss: 0.004868502 Explore P: 0.026738391\n",
      "Episode: 136 Total reward: 0.0 Average loss: 0.004969096 Explore P: 0.026243697\n",
      "Episode: 137 Total reward: 0.0 Average loss: 0.004286998 Explore P: 0.025763623\n",
      "Episode: 138 Total reward: 0.0 Average loss: 0.005699010 Explore P: 0.025297738\n",
      "Episode: 139 Total reward: -1.0 Average loss: 0.005125059 Explore P: 0.024845621\n",
      "Episode: 140 Total reward: 0.0 Average loss: 0.004685130 Explore P: 0.024406867\n",
      "Episode: 141 Total reward: -1.0 Average loss: 0.005204173 Explore P: 0.023981079\n",
      "Episode: 142 Total reward: 0.0 Average loss: 0.005186404 Explore P: 0.023567876\n",
      "Episode: 143 Total reward: 1.0 Average loss: 0.004783600 Explore P: 0.023166885\n",
      "Episode: 144 Total reward: 1.0 Average loss: 0.003420629 Explore P: 0.022777744\n",
      "Episode: 145 Total reward: 0.0 Average loss: 0.002625310 Explore P: 0.022400105\n",
      "Episode: 146 Total reward: -1.0 Average loss: 0.005951585 Explore P: 0.022033627\n",
      "Episode: 147 Total reward: -3.0 Average loss: 0.006521649 Explore P: 0.021677979\n",
      "Episode: 148 Total reward: -2.0 Average loss: 0.006585642 Explore P: 0.021332843\n",
      "Episode: 149 Total reward: -2.0 Average loss: 0.005233785 Explore P: 0.020997907\n",
      "Episode: 150 Total reward: -5.0 Average loss: 0.003744473 Explore P: 0.020672869\n",
      "Episode: 151 Total reward: 0.0 Average loss: 0.003252757 Explore P: 0.020357438\n",
      "Episode: 152 Total reward: 0.0 Average loss: 0.004609816 Explore P: 0.020051330\n",
      "Episode: 153 Total reward: 1.0 Average loss: 0.005627180 Explore P: 0.019754268\n",
      "Episode: 154 Total reward: 0.0 Average loss: 0.005881685 Explore P: 0.019465986\n",
      "Episode: 155 Total reward: -1.0 Average loss: 0.005520456 Explore P: 0.019186224\n",
      "Episode: 156 Total reward: -1.0 Average loss: 0.004968558 Explore P: 0.018914730\n",
      "Episode: 157 Total reward: -1.0 Average loss: 0.004407228 Explore P: 0.018651260\n",
      "Episode: 158 Total reward: 1.0 Average loss: 0.004452222 Explore P: 0.018395576\n",
      "Episode: 159 Total reward: -2.0 Average loss: 0.004071272 Explore P: 0.018147450\n",
      "Episode: 160 Total reward: 0.0 Average loss: 0.006128359 Explore P: 0.017906656\n",
      "Episode: 161 Total reward: 3.0 Average loss: 0.005887409 Explore P: 0.017672979\n",
      "Episode: 162 Total reward: 0.0 Average loss: 0.005043645 Explore P: 0.017446208\n",
      "Episode: 163 Total reward: 0.0 Average loss: 0.003416260 Explore P: 0.017226140\n",
      "Episode: 164 Total reward: 0.0 Average loss: 0.002739055 Explore P: 0.017012575\n",
      "Episode: 165 Total reward: 1.0 Average loss: 0.002911045 Explore P: 0.016805322\n",
      "Episode: 166 Total reward: -4.0 Average loss: 0.004385653 Explore P: 0.016604194\n",
      "Episode: 167 Total reward: -1.0 Average loss: 0.004491512 Explore P: 0.016409011\n",
      "Episode: 168 Total reward: 3.0 Average loss: 0.006173135 Explore P: 0.016219596\n",
      "Episode: 169 Total reward: 0.0 Average loss: 0.004911358 Explore P: 0.016035779\n",
      "Episode: 170 Total reward: 4.0 Average loss: 0.007979169 Explore P: 0.015857395\n",
      "Episode: 171 Total reward: -1.0 Average loss: 0.005102936 Explore P: 0.015684283\n",
      "Episode: 172 Total reward: 0.0 Average loss: 0.004634001 Explore P: 0.015516287\n",
      "Episode: 173 Total reward: 0.0 Average loss: 0.004007842 Explore P: 0.015353256\n",
      "Episode: 174 Total reward: 1.0 Average loss: 0.004508865 Explore P: 0.015195043\n",
      "Episode: 175 Total reward: 2.0 Average loss: 0.004461912 Explore P: 0.015041506\n",
      "Episode: 176 Total reward: 3.0 Average loss: 0.003841141 Explore P: 0.014892507\n",
      "Episode: 177 Total reward: -1.0 Average loss: 0.003249321 Explore P: 0.014747912\n",
      "Episode: 178 Total reward: 0.0 Average loss: 0.004681301 Explore P: 0.014607590\n",
      "Episode: 179 Total reward: -1.0 Average loss: 0.006028979 Explore P: 0.014471415\n",
      "Episode: 180 Total reward: -1.0 Average loss: 0.005411692 Explore P: 0.014339265\n",
      "Episode: 181 Total reward: 0.0 Average loss: 0.003917470 Explore P: 0.014211020\n",
      "Episode: 182 Total reward: 1.0 Average loss: 0.005765107 Explore P: 0.014086566\n",
      "Episode: 183 Total reward: 1.0 Average loss: 0.005939447 Explore P: 0.013965789\n",
      "Episode: 184 Total reward: 0.0 Average loss: 0.008790650 Explore P: 0.013848583\n",
      "Episode: 185 Total reward: 2.0 Average loss: 0.008668276 Explore P: 0.013734840\n",
      "Episode: 186 Total reward: 3.0 Average loss: 0.009166650 Explore P: 0.013624459\n",
      "Episode: 187 Total reward: 6.0 Average loss: 0.007855226 Explore P: 0.013517340\n",
      "Episode: 188 Total reward: 5.0 Average loss: 0.004935375 Explore P: 0.013413387\n",
      "Episode: 189 Total reward: -1.0 Average loss: 0.003476351 Explore P: 0.013312506\n",
      "Episode: 190 Total reward: 4.0 Average loss: 0.004603850 Explore P: 0.013214606\n",
      "Episode: 191 Total reward: 4.0 Average loss: 0.004813894 Explore P: 0.013119600\n",
      "Episode: 192 Total reward: -1.0 Average loss: 0.005438043 Explore P: 0.013027402\n",
      "Episode: 193 Total reward: -2.0 Average loss: 0.004982466 Explore P: 0.012937929\n",
      "Episode: 194 Total reward: 1.0 Average loss: 0.003732836 Explore P: 0.012851100\n",
      "Episode: 195 Total reward: -2.0 Average loss: 0.003264952 Explore P: 0.012766837\n",
      "Episode: 196 Total reward: 1.0 Average loss: 0.002548331 Explore P: 0.012685065\n",
      "Episode: 197 Total reward: -2.0 Average loss: 0.003782056 Explore P: 0.012605709\n",
      "Episode: 198 Total reward: 2.0 Average loss: 0.005312357 Explore P: 0.012528699\n",
      "Episode: 199 Total reward: 1.0 Average loss: 0.005251162 Explore P: 0.012453965\n",
      "Episode: 200 Total reward: 0.0 Average loss: 0.005808524 Explore P: 0.012381439\n",
      "Episode: 201 Total reward: 1.0 Average loss: 0.005266709 Explore P: 0.012311057\n",
      "Episode: 202 Total reward: 1.0 Average loss: 0.004699847 Explore P: 0.012242755\n",
      "Episode: 203 Total reward: 1.0 Average loss: 0.007259951 Explore P: 0.012176471\n",
      "Episode: 204 Total reward: 3.0 Average loss: 0.008264457 Explore P: 0.012112147\n",
      "Episode: 205 Total reward: -3.0 Average loss: 0.010802820 Explore P: 0.012049724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 206 Total reward: 1.0 Average loss: 0.008575886 Explore P: 0.011989145\n",
      "Episode: 207 Total reward: 0.0 Average loss: 0.009621646 Explore P: 0.011930357\n",
      "Episode: 208 Total reward: -2.0 Average loss: 0.005930646 Explore P: 0.011873306\n",
      "Episode: 209 Total reward: 6.0 Average loss: 0.007593409 Explore P: 0.011817942\n",
      "Episode: 210 Total reward: 0.0 Average loss: 0.005337570 Explore P: 0.011764213\n",
      "Episode: 211 Total reward: -2.0 Average loss: 0.005819669 Explore P: 0.011712073\n",
      "Episode: 212 Total reward: 6.0 Average loss: 0.005537412 Explore P: 0.011661474\n",
      "Episode: 213 Total reward: -2.0 Average loss: 0.004943839 Explore P: 0.011612370\n",
      "Episode: 214 Total reward: 3.0 Average loss: 0.003031864 Explore P: 0.011564717\n",
      "Episode: 215 Total reward: 0.0 Average loss: 0.002636309 Explore P: 0.011518473\n",
      "Episode: 216 Total reward: -3.0 Average loss: 0.005697502 Explore P: 0.011473595\n",
      "Episode: 217 Total reward: 0.0 Average loss: 0.006106289 Explore P: 0.011430044\n",
      "Episode: 218 Total reward: 0.0 Average loss: 0.008476168 Explore P: 0.011387779\n",
      "Episode: 219 Total reward: 2.0 Average loss: 0.006125709 Explore P: 0.011346764\n",
      "Episode: 220 Total reward: 4.0 Average loss: 0.004512311 Explore P: 0.011306961\n",
      "Episode: 221 Total reward: -2.0 Average loss: 0.003803562 Explore P: 0.011268335\n",
      "Episode: 222 Total reward: 2.0 Average loss: 0.002707098 Explore P: 0.011230850\n",
      "Episode: 223 Total reward: 1.0 Average loss: 0.004553075 Explore P: 0.011194473\n",
      "Episode: 224 Total reward: 0.0 Average loss: 0.006882129 Explore P: 0.011159171\n",
      "Episode: 225 Total reward: 0.0 Average loss: 0.008237043 Explore P: 0.011124912\n",
      "Episode: 226 Total reward: 0.0 Average loss: 0.009534270 Explore P: 0.011091666\n",
      "Episode: 227 Total reward: -1.0 Average loss: 0.009304307 Explore P: 0.011059402\n",
      "Episode: 228 Total reward: 1.0 Average loss: 0.008324733 Explore P: 0.011028092\n",
      "Episode: 229 Total reward: 0.0 Average loss: 0.009389593 Explore P: 0.010997708\n",
      "Episode: 230 Total reward: 0.0 Average loss: 0.012008941 Explore P: 0.010968221\n",
      "Episode: 231 Total reward: 0.0 Average loss: 0.011592588 Explore P: 0.010939606\n",
      "Episode: 232 Total reward: 1.0 Average loss: 0.010891829 Explore P: 0.010911836\n",
      "Episode: 233 Total reward: 2.0 Average loss: 0.006688941 Explore P: 0.010884887\n",
      "Episode: 234 Total reward: 3.0 Average loss: 0.006008259 Explore P: 0.010858735\n",
      "Episode: 235 Total reward: -1.0 Average loss: 0.005063720 Explore P: 0.010833355\n",
      "Episode: 236 Total reward: -3.0 Average loss: 0.008367667 Explore P: 0.010808726\n",
      "Episode: 237 Total reward: 0.0 Average loss: 0.005916278 Explore P: 0.010784825\n",
      "Episode: 238 Total reward: -2.0 Average loss: 0.007653067 Explore P: 0.010761630\n",
      "Episode: 239 Total reward: 2.0 Average loss: 0.007377847 Explore P: 0.010739120\n",
      "Episode: 240 Total reward: -2.0 Average loss: 0.007146888 Explore P: 0.010717276\n",
      "Episode: 241 Total reward: -1.0 Average loss: 0.006407387 Explore P: 0.010696077\n",
      "Episode: 242 Total reward: 0.0 Average loss: 0.004327727 Explore P: 0.010675505\n",
      "Episode: 243 Total reward: 0.0 Average loss: 0.003238910 Explore P: 0.010655541\n",
      "Episode: 244 Total reward: 3.0 Average loss: 0.002716215 Explore P: 0.010636166\n",
      "Episode: 245 Total reward: 0.0 Average loss: 0.003386705 Explore P: 0.010617365\n",
      "Episode: 246 Total reward: -1.0 Average loss: 0.004627313 Explore P: 0.010599119\n",
      "Episode: 247 Total reward: 0.0 Average loss: 0.006432633 Explore P: 0.010581412\n",
      "Episode: 248 Total reward: 0.0 Average loss: 0.012603429 Explore P: 0.010564229\n",
      "Episode: 249 Total reward: -1.0 Average loss: 0.016263183 Explore P: 0.010547554\n",
      "Episode: 250 Total reward: 0.0 Average loss: 0.020866226 Explore P: 0.010531371\n",
      "Episode: 251 Total reward: 0.0 Average loss: 0.010798740 Explore P: 0.010515666\n",
      "Episode: 252 Total reward: 1.0 Average loss: 0.011799238 Explore P: 0.010500426\n",
      "Episode: 253 Total reward: -1.0 Average loss: 0.011951963 Explore P: 0.010485636\n",
      "Episode: 254 Total reward: 0.0 Average loss: 0.012002444 Explore P: 0.010471284\n",
      "Episode: 255 Total reward: 0.0 Average loss: 0.011224867 Explore P: 0.010457355\n",
      "Episode: 256 Total reward: 0.0 Average loss: 0.008920783 Explore P: 0.010443838\n",
      "Episode: 257 Total reward: 1.0 Average loss: 0.007792559 Explore P: 0.010430721\n",
      "Episode: 258 Total reward: 0.0 Average loss: 0.009658744 Explore P: 0.010417991\n",
      "Episode: 259 Total reward: 0.0 Average loss: 0.011878074 Explore P: 0.010405638\n",
      "Episode: 260 Total reward: 0.0 Average loss: 0.011633493 Explore P: 0.010393649\n",
      "Episode: 261 Total reward: 1.0 Average loss: 0.008203125 Explore P: 0.010382015\n",
      "Episode: 262 Total reward: 2.0 Average loss: 0.004579664 Explore P: 0.010370725\n",
      "Episode: 263 Total reward: 0.0 Average loss: 0.003150973 Explore P: 0.010359768\n",
      "Episode: 264 Total reward: 0.0 Average loss: 0.005458117 Explore P: 0.010349136\n",
      "Episode: 265 Total reward: -1.0 Average loss: 0.006043562 Explore P: 0.010338817\n",
      "Episode: 266 Total reward: 2.0 Average loss: 0.007703786 Explore P: 0.010328803\n",
      "Episode: 267 Total reward: -1.0 Average loss: 0.008656228 Explore P: 0.010319086\n",
      "Episode: 268 Total reward: 0.0 Average loss: 0.009190936 Explore P: 0.010309655\n",
      "Episode: 269 Total reward: 0.0 Average loss: 0.010709907 Explore P: 0.010300504\n",
      "Episode: 270 Total reward: 0.0 Average loss: 0.183731794 Explore P: 0.010291623\n",
      "Episode: 271 Total reward: 2.0 Average loss: 0.184510708 Explore P: 0.010283004\n",
      "Episode: 272 Total reward: 0.0 Average loss: 0.151670218 Explore P: 0.010274640\n",
      "Episode: 273 Total reward: -7.0 Average loss: 0.016889352 Explore P: 0.010266523\n",
      "Episode: 274 Total reward: 0.0 Average loss: 0.005091922 Explore P: 0.010258646\n",
      "Episode: 275 Total reward: -2.0 Average loss: 0.007300213 Explore P: 0.010251002\n",
      "Episode: 276 Total reward: 1.0 Average loss: 0.006062294 Explore P: 0.010243584\n",
      "Episode: 277 Total reward: -5.0 Average loss: 0.006220691 Explore P: 0.010236385\n",
      "Episode: 278 Total reward: 0.0 Average loss: 0.003554072 Explore P: 0.010229398\n",
      "Episode: 279 Total reward: 1.0 Average loss: 0.003680162 Explore P: 0.010222619\n",
      "Episode: 280 Total reward: 1.0 Average loss: 0.002779303 Explore P: 0.010216039\n",
      "Episode: 281 Total reward: 0.0 Average loss: 0.002996330 Explore P: 0.010209654\n",
      "Episode: 282 Total reward: 0.0 Average loss: 0.003104041 Explore P: 0.010203458\n",
      "Episode: 283 Total reward: 2.0 Average loss: 0.003360017 Explore P: 0.010197445\n",
      "Episode: 284 Total reward: 3.0 Average loss: 0.003396023 Explore P: 0.010191610\n",
      "Episode: 285 Total reward: -6.0 Average loss: 0.003056977 Explore P: 0.010185947\n",
      "Episode: 286 Total reward: -2.0 Average loss: 0.002740356 Explore P: 0.010180451\n",
      "Episode: 287 Total reward: -1.0 Average loss: 0.003131870 Explore P: 0.010175118\n",
      "Episode: 288 Total reward: 1.0 Average loss: 0.004782960 Explore P: 0.010169943\n",
      "Episode: 289 Total reward: -2.0 Average loss: 0.005728564 Explore P: 0.010164920\n",
      "Episode: 290 Total reward: -1.0 Average loss: 0.004836075 Explore P: 0.010160046\n",
      "Episode: 291 Total reward: 0.0 Average loss: 0.005904961 Explore P: 0.010155316\n",
      "Episode: 292 Total reward: -1.0 Average loss: 0.006629173 Explore P: 0.010150725\n",
      "Episode: 293 Total reward: -1.0 Average loss: 0.006581738 Explore P: 0.010146271\n",
      "Episode: 294 Total reward: -1.0 Average loss: 0.007144792 Explore P: 0.010141948\n",
      "Episode: 295 Total reward: -1.0 Average loss: 0.007388277 Explore P: 0.010137753\n",
      "Episode: 296 Total reward: 0.0 Average loss: 0.009328175 Explore P: 0.010133682\n",
      "Episode: 297 Total reward: -1.0 Average loss: 0.007658769 Explore P: 0.010129731\n",
      "Episode: 298 Total reward: 0.0 Average loss: 0.006817346 Explore P: 0.010125897\n",
      "Episode: 299 Total reward: 4.0 Average loss: 0.008598288 Explore P: 0.010122176\n",
      "Episode: 300 Total reward: -1.0 Average loss: 0.009119725 Explore P: 0.010118565\n",
      "Episode: 301 Total reward: 0.0 Average loss: 0.008400985 Explore P: 0.010115061\n",
      "Episode: 302 Total reward: 0.0 Average loss: 0.004224770 Explore P: 0.010111660\n",
      "Episode: 303 Total reward: 1.0 Average loss: 0.006641141 Explore P: 0.010108360\n",
      "Episode: 304 Total reward: 0.0 Average loss: 0.005968618 Explore P: 0.010105158\n",
      "Episode: 305 Total reward: 1.0 Average loss: 0.006287705 Explore P: 0.010102050\n",
      "Episode: 306 Total reward: -1.0 Average loss: 0.006682206 Explore P: 0.010099034\n",
      "Episode: 307 Total reward: 1.0 Average loss: 0.005934427 Explore P: 0.010096107\n",
      "Episode: 308 Total reward: 1.0 Average loss: 0.006302092 Explore P: 0.010093266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 309 Total reward: -1.0 Average loss: 0.005787408 Explore P: 0.010090510\n",
      "Episode: 310 Total reward: -3.0 Average loss: 0.004573601 Explore P: 0.010087835\n",
      "Episode: 311 Total reward: -1.0 Average loss: 0.004479950 Explore P: 0.010085239\n",
      "Episode: 312 Total reward: -1.0 Average loss: 0.003793813 Explore P: 0.010082720\n",
      "Episode: 313 Total reward: 0.0 Average loss: 0.003223396 Explore P: 0.010080275\n",
      "Episode: 314 Total reward: -6.0 Average loss: 0.003158770 Explore P: 0.010077903\n",
      "Episode: 315 Total reward: 0.0 Average loss: 0.002608769 Explore P: 0.010075600\n",
      "Episode: 316 Total reward: 0.0 Average loss: 0.002569395 Explore P: 0.010073366\n",
      "Episode: 317 Total reward: -3.0 Average loss: 0.002497981 Explore P: 0.010071198\n",
      "Episode: 318 Total reward: -1.0 Average loss: 0.002480387 Explore P: 0.010069093\n",
      "Episode: 319 Total reward: -6.0 Average loss: 0.004417809 Explore P: 0.010067051\n",
      "Episode: 320 Total reward: -1.0 Average loss: 0.004574806 Explore P: 0.010065070\n",
      "Episode: 321 Total reward: -1.0 Average loss: 0.004646257 Explore P: 0.010063147\n",
      "Episode: 322 Total reward: -2.0 Average loss: 0.006196711 Explore P: 0.010061280\n",
      "Episode: 323 Total reward: -5.0 Average loss: 0.005406819 Explore P: 0.010059469\n",
      "Episode: 324 Total reward: -1.0 Average loss: 0.005144354 Explore P: 0.010057712\n",
      "Episode: 325 Total reward: -3.0 Average loss: 0.003702836 Explore P: 0.010056006\n",
      "Episode: 326 Total reward: 0.0 Average loss: 0.003226850 Explore P: 0.010054351\n",
      "Episode: 327 Total reward: 1.0 Average loss: 0.003869273 Explore P: 0.010052745\n",
      "Episode: 328 Total reward: 1.0 Average loss: 0.006173167 Explore P: 0.010051186\n",
      "Episode: 329 Total reward: -1.0 Average loss: 0.007124214 Explore P: 0.010049673\n",
      "Episode: 330 Total reward: 0.0 Average loss: 0.009207118 Explore P: 0.010048205\n",
      "Episode: 331 Total reward: -2.0 Average loss: 0.009275605 Explore P: 0.010046780\n",
      "Episode: 332 Total reward: -2.0 Average loss: 0.007964623 Explore P: 0.010045398\n",
      "Episode: 333 Total reward: 3.0 Average loss: 0.012418937 Explore P: 0.010044056\n",
      "Episode: 334 Total reward: -2.0 Average loss: 0.007757529 Explore P: 0.010042754\n",
      "Episode: 335 Total reward: -2.0 Average loss: 0.005822388 Explore P: 0.010041490\n",
      "Episode: 336 Total reward: 0.0 Average loss: 0.003713547 Explore P: 0.010040264\n",
      "Episode: 337 Total reward: -3.0 Average loss: 0.004960204 Explore P: 0.010039074\n",
      "Episode: 338 Total reward: 0.0 Average loss: 0.004960126 Explore P: 0.010037919\n",
      "Episode: 339 Total reward: -1.0 Average loss: 0.005120707 Explore P: 0.010036799\n",
      "Episode: 340 Total reward: -4.0 Average loss: 0.003205756 Explore P: 0.010035711\n",
      "Episode: 341 Total reward: -1.0 Average loss: 0.002862374 Explore P: 0.010034656\n",
      "Episode: 342 Total reward: -1.0 Average loss: 0.008856657 Explore P: 0.010033631\n",
      "Episode: 343 Total reward: -2.0 Average loss: 0.009756968 Explore P: 0.010032637\n",
      "Episode: 344 Total reward: -3.0 Average loss: 0.008539896 Explore P: 0.010031673\n",
      "Episode: 345 Total reward: 0.0 Average loss: 0.003615145 Explore P: 0.010030737\n",
      "Episode: 346 Total reward: 0.0 Average loss: 0.004991052 Explore P: 0.010029828\n",
      "Episode: 347 Total reward: -1.0 Average loss: 0.004903047 Explore P: 0.010028947\n",
      "Episode: 348 Total reward: -1.0 Average loss: 0.005182926 Explore P: 0.010028091\n",
      "Episode: 349 Total reward: -1.0 Average loss: 0.003187255 Explore P: 0.010027261\n",
      "Episode: 350 Total reward: -2.0 Average loss: 0.002927548 Explore P: 0.010026455\n",
      "Episode: 351 Total reward: 0.0 Average loss: 0.002552610 Explore P: 0.010025674\n",
      "Episode: 352 Total reward: -2.0 Average loss: 0.003190895 Explore P: 0.010024915\n",
      "Episode: 353 Total reward: -1.0 Average loss: 0.003188598 Explore P: 0.010024178\n",
      "Episode: 354 Total reward: -4.0 Average loss: 0.003003716 Explore P: 0.010023464\n",
      "Episode: 355 Total reward: -1.0 Average loss: 0.002746837 Explore P: 0.010022770\n",
      "Episode: 356 Total reward: -3.0 Average loss: 0.003208373 Explore P: 0.010022097\n",
      "Episode: 357 Total reward: -2.0 Average loss: 0.002863814 Explore P: 0.010021444\n",
      "Episode: 358 Total reward: 2.0 Average loss: 0.002895825 Explore P: 0.010020811\n",
      "Episode: 359 Total reward: -2.0 Average loss: 0.002948965 Explore P: 0.010020196\n",
      "Episode: 360 Total reward: -2.0 Average loss: 0.003226171 Explore P: 0.010019599\n",
      "Episode: 361 Total reward: 0.0 Average loss: 0.003028961 Explore P: 0.010019019\n",
      "Episode: 362 Total reward: -2.0 Average loss: 0.003087542 Explore P: 0.010018457\n",
      "Episode: 363 Total reward: 1.0 Average loss: 0.008906754 Explore P: 0.010017912\n",
      "Episode: 364 Total reward: 1.0 Average loss: 0.010474099 Explore P: 0.010017382\n",
      "Episode: 365 Total reward: -1.0 Average loss: 0.010924007 Explore P: 0.010016869\n",
      "Episode: 366 Total reward: -1.0 Average loss: 0.009132704 Explore P: 0.010016370\n",
      "Episode: 367 Total reward: 0.0 Average loss: 0.010504048 Explore P: 0.010015886\n",
      "Episode: 368 Total reward: -1.0 Average loss: 0.011181527 Explore P: 0.010015417\n",
      "Episode: 369 Total reward: -2.0 Average loss: 0.012091420 Explore P: 0.010014961\n",
      "Episode: 370 Total reward: 2.0 Average loss: 0.005916556 Explore P: 0.010014519\n",
      "Episode: 371 Total reward: 1.0 Average loss: 0.004942838 Explore P: 0.010014090\n",
      "Episode: 372 Total reward: 0.0 Average loss: 0.005875472 Explore P: 0.010013674\n",
      "Episode: 373 Total reward: -4.0 Average loss: 0.006770873 Explore P: 0.010013269\n",
      "Episode: 374 Total reward: 1.0 Average loss: 0.006878089 Explore P: 0.010012877\n",
      "Episode: 375 Total reward: 1.0 Average loss: 0.004834188 Explore P: 0.010012497\n",
      "Episode: 376 Total reward: 3.0 Average loss: 0.003363658 Explore P: 0.010012127\n",
      "Episode: 377 Total reward: 0.0 Average loss: 0.002784410 Explore P: 0.010011769\n",
      "Episode: 378 Total reward: 0.0 Average loss: 0.003085665 Explore P: 0.010011421\n",
      "Episode: 379 Total reward: -1.0 Average loss: 0.004976981 Explore P: 0.010011084\n",
      "Episode: 380 Total reward: -4.0 Average loss: 0.006993867 Explore P: 0.010010756\n",
      "Episode: 381 Total reward: 0.0 Average loss: 0.006942526 Explore P: 0.010010438\n",
      "Episode: 382 Total reward: 0.0 Average loss: 0.006896961 Explore P: 0.010010130\n",
      "Episode: 383 Total reward: 1.0 Average loss: 0.005198889 Explore P: 0.010009830\n",
      "Episode: 384 Total reward: -1.0 Average loss: 0.004546652 Explore P: 0.010009540\n",
      "Episode: 385 Total reward: 0.0 Average loss: 0.005237080 Explore P: 0.010009258\n",
      "Episode: 386 Total reward: 3.0 Average loss: 0.008977337 Explore P: 0.010008984\n",
      "Episode: 387 Total reward: 1.0 Average loss: 0.011401122 Explore P: 0.010008719\n",
      "Episode: 388 Total reward: -4.0 Average loss: 0.013837531 Explore P: 0.010008461\n",
      "Episode: 389 Total reward: -1.0 Average loss: 0.009484518 Explore P: 0.010008211\n",
      "Episode: 390 Total reward: 1.0 Average loss: 0.020665413 Explore P: 0.010007968\n",
      "Episode: 391 Total reward: 2.0 Average loss: 0.012502299 Explore P: 0.010007733\n",
      "Episode: 392 Total reward: 2.0 Average loss: 0.007422771 Explore P: 0.010007504\n",
      "Episode: 393 Total reward: 1.0 Average loss: 0.003766072 Explore P: 0.010007282\n",
      "Episode: 394 Total reward: -1.0 Average loss: 0.003977484 Explore P: 0.010007067\n",
      "Episode: 395 Total reward: 2.0 Average loss: 0.006365345 Explore P: 0.010006858\n",
      "Episode: 396 Total reward: 2.0 Average loss: 0.006677553 Explore P: 0.010006656\n",
      "Episode: 397 Total reward: 1.0 Average loss: 0.007718825 Explore P: 0.010006459\n",
      "Episode: 398 Total reward: 1.0 Average loss: 0.005020070 Explore P: 0.010006268\n",
      "Episode: 399 Total reward: 0.0 Average loss: 0.005099434 Explore P: 0.010006083\n",
      "Episode: 400 Total reward: 3.0 Average loss: 0.004106097 Explore P: 0.010005903\n",
      "Episode: 401 Total reward: 0.0 Average loss: 0.004170381 Explore P: 0.010005729\n",
      "Episode: 402 Total reward: 0.0 Average loss: 0.010201849 Explore P: 0.010005559\n",
      "Episode: 403 Total reward: 0.0 Average loss: 0.008691640 Explore P: 0.010005395\n",
      "Episode: 404 Total reward: 0.0 Average loss: 0.015611221 Explore P: 0.010005235\n",
      "Episode: 405 Total reward: 0.0 Average loss: 0.011986929 Explore P: 0.010005081\n",
      "Episode: 406 Total reward: 0.0 Average loss: 0.016743025 Explore P: 0.010004931\n",
      "Episode: 407 Total reward: 0.0 Average loss: 0.010415511 Explore P: 0.010004785\n",
      "Episode: 408 Total reward: 0.0 Average loss: 0.006823683 Explore P: 0.010004643\n",
      "Episode: 409 Total reward: 0.0 Average loss: 0.017469119 Explore P: 0.010004506\n",
      "Episode: 410 Total reward: -1.0 Average loss: 0.050508812 Explore P: 0.010004373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 411 Total reward: 0.0 Average loss: 0.034249675 Explore P: 0.010004244\n",
      "Episode: 412 Total reward: -2.0 Average loss: 0.009585474 Explore P: 0.010004118\n",
      "Episode: 413 Total reward: 3.0 Average loss: 0.014787750 Explore P: 0.010003997\n",
      "Episode: 414 Total reward: -2.0 Average loss: 0.008645126 Explore P: 0.010003879\n",
      "Episode: 415 Total reward: 0.0 Average loss: 0.008324224 Explore P: 0.010003764\n",
      "Episode: 416 Total reward: 0.0 Average loss: 0.005916650 Explore P: 0.010003653\n",
      "Episode: 417 Total reward: 0.0 Average loss: 0.005802000 Explore P: 0.010003545\n",
      "Episode: 418 Total reward: 0.0 Average loss: 0.005027888 Explore P: 0.010003440\n",
      "Episode: 419 Total reward: 0.0 Average loss: 0.005935641 Explore P: 0.010003338\n",
      "Episode: 420 Total reward: 0.0 Average loss: 0.006872763 Explore P: 0.010003240\n",
      "Episode: 421 Total reward: 1.0 Average loss: 0.009153476 Explore P: 0.010003144\n",
      "Episode: 422 Total reward: -1.0 Average loss: 0.007292240 Explore P: 0.010003051\n",
      "Episode: 423 Total reward: 0.0 Average loss: 0.004927691 Explore P: 0.010002961\n",
      "Episode: 424 Total reward: -1.0 Average loss: 0.005190458 Explore P: 0.010002873\n",
      "Episode: 425 Total reward: 1.0 Average loss: 0.006994062 Explore P: 0.010002788\n",
      "Episode: 426 Total reward: 1.0 Average loss: 0.009239014 Explore P: 0.010002706\n",
      "Episode: 427 Total reward: 0.0 Average loss: 0.091849186 Explore P: 0.010002626\n",
      "Episode: 428 Total reward: 0.0 Average loss: 0.129741713 Explore P: 0.010002548\n",
      "Episode: 429 Total reward: 1.0 Average loss: 0.115657806 Explore P: 0.010002473\n",
      "Episode: 430 Total reward: 4.0 Average loss: 0.050398458 Explore P: 0.010002400\n",
      "Episode: 431 Total reward: 2.0 Average loss: 0.033867415 Explore P: 0.010002329\n",
      "Episode: 432 Total reward: 0.0 Average loss: 0.007104955 Explore P: 0.010002260\n",
      "Episode: 433 Total reward: 1.0 Average loss: 0.006449221 Explore P: 0.010002193\n",
      "Episode: 434 Total reward: 0.0 Average loss: 0.005619694 Explore P: 0.010002129\n",
      "Episode: 435 Total reward: 0.0 Average loss: 0.005514363 Explore P: 0.010002066\n",
      "Episode: 436 Total reward: 1.0 Average loss: 0.005036445 Explore P: 0.010002005\n",
      "Episode: 437 Total reward: -1.0 Average loss: 0.004670327 Explore P: 0.010001945\n",
      "Episode: 438 Total reward: 0.0 Average loss: 0.003698161 Explore P: 0.010001888\n",
      "Episode: 439 Total reward: 0.0 Average loss: 0.003280852 Explore P: 0.010001832\n",
      "Episode: 440 Total reward: 0.0 Average loss: 0.004427146 Explore P: 0.010001778\n",
      "Episode: 441 Total reward: 2.0 Average loss: 0.006671339 Explore P: 0.010001725\n",
      "Episode: 442 Total reward: 0.0 Average loss: 0.006508050 Explore P: 0.010001674\n",
      "Episode: 443 Total reward: 0.0 Average loss: 0.008164837 Explore P: 0.010001625\n",
      "Episode: 444 Total reward: 0.0 Average loss: 0.006194920 Explore P: 0.010001577\n",
      "Episode: 445 Total reward: 1.0 Average loss: 0.007125193 Explore P: 0.010001530\n",
      "Episode: 446 Total reward: 0.0 Average loss: 0.004326357 Explore P: 0.010001485\n",
      "Episode: 447 Total reward: 2.0 Average loss: 0.005918662 Explore P: 0.010001441\n",
      "Episode: 448 Total reward: 0.0 Average loss: 0.013759193 Explore P: 0.010001399\n",
      "Episode: 449 Total reward: 3.0 Average loss: 0.015162398 Explore P: 0.010001357\n",
      "Episode: 450 Total reward: 1.0 Average loss: 0.004569510 Explore P: 0.010001317\n",
      "Episode: 451 Total reward: 0.0 Average loss: 0.003004765 Explore P: 0.010001278\n",
      "Episode: 452 Total reward: 1.0 Average loss: 0.003945730 Explore P: 0.010001240\n",
      "Episode: 453 Total reward: 1.0 Average loss: 0.003690165 Explore P: 0.010001204\n",
      "Episode: 454 Total reward: 0.0 Average loss: 0.003429621 Explore P: 0.010001168\n",
      "Episode: 455 Total reward: 1.0 Average loss: 0.002982845 Explore P: 0.010001134\n",
      "Episode: 456 Total reward: 1.0 Average loss: 0.005094660 Explore P: 0.010001100\n",
      "Episode: 457 Total reward: 1.0 Average loss: 0.006583822 Explore P: 0.010001068\n",
      "Episode: 458 Total reward: 1.0 Average loss: 0.006590276 Explore P: 0.010001036\n",
      "Episode: 459 Total reward: 1.0 Average loss: 0.007177788 Explore P: 0.010001005\n",
      "Episode: 460 Total reward: 1.0 Average loss: 0.007741353 Explore P: 0.010000976\n",
      "Episode: 461 Total reward: 2.0 Average loss: 0.006944690 Explore P: 0.010000947\n",
      "Episode: 462 Total reward: 2.0 Average loss: 0.007211340 Explore P: 0.010000919\n",
      "Episode: 463 Total reward: 1.0 Average loss: 0.006087618 Explore P: 0.010000892\n",
      "Episode: 464 Total reward: 3.0 Average loss: 0.007513355 Explore P: 0.010000865\n",
      "Episode: 465 Total reward: 1.0 Average loss: 0.006327828 Explore P: 0.010000840\n",
      "Episode: 466 Total reward: 1.0 Average loss: 0.007090683 Explore P: 0.010000815\n",
      "Episode: 467 Total reward: -1.0 Average loss: 0.006021192 Explore P: 0.010000791\n",
      "Episode: 468 Total reward: 4.0 Average loss: 0.004122876 Explore P: 0.010000768\n",
      "Episode: 469 Total reward: -1.0 Average loss: 0.003997574 Explore P: 0.010000745\n",
      "Episode: 470 Total reward: 4.0 Average loss: 0.004058324 Explore P: 0.010000723\n",
      "Episode: 471 Total reward: 0.0 Average loss: 0.003756480 Explore P: 0.010000701\n",
      "Episode: 472 Total reward: -1.0 Average loss: 0.003531906 Explore P: 0.010000681\n",
      "Episode: 473 Total reward: 1.0 Average loss: 0.004744322 Explore P: 0.010000661\n",
      "Episode: 474 Total reward: -2.0 Average loss: 0.006420775 Explore P: 0.010000641\n",
      "Episode: 475 Total reward: 2.0 Average loss: 0.007855444 Explore P: 0.010000622\n",
      "Episode: 476 Total reward: 1.0 Average loss: 0.006291304 Explore P: 0.010000604\n",
      "Episode: 477 Total reward: 0.0 Average loss: 0.004726280 Explore P: 0.010000586\n",
      "Episode: 478 Total reward: -3.0 Average loss: 0.003471234 Explore P: 0.010000569\n",
      "Episode: 479 Total reward: 0.0 Average loss: 0.002775076 Explore P: 0.010000552\n",
      "Episode: 480 Total reward: -1.0 Average loss: 0.003223633 Explore P: 0.010000536\n",
      "Episode: 481 Total reward: 0.0 Average loss: 0.003353844 Explore P: 0.010000520\n",
      "Episode: 482 Total reward: 1.0 Average loss: 0.005248868 Explore P: 0.010000504\n",
      "Episode: 483 Total reward: -2.0 Average loss: 0.004807119 Explore P: 0.010000489\n",
      "Episode: 484 Total reward: -2.0 Average loss: 0.005239054 Explore P: 0.010000475\n",
      "Episode: 485 Total reward: -2.0 Average loss: 0.003203312 Explore P: 0.010000461\n",
      "Episode: 486 Total reward: -2.0 Average loss: 0.003176603 Explore P: 0.010000447\n",
      "Episode: 487 Total reward: 0.0 Average loss: 0.003394170 Explore P: 0.010000434\n",
      "Episode: 488 Total reward: 0.0 Average loss: 0.003705311 Explore P: 0.010000421\n",
      "Episode: 489 Total reward: 0.0 Average loss: 0.004877151 Explore P: 0.010000409\n",
      "Episode: 490 Total reward: 1.0 Average loss: 0.007178375 Explore P: 0.010000397\n",
      "Episode: 491 Total reward: 3.0 Average loss: 0.007204521 Explore P: 0.010000385\n",
      "Episode: 492 Total reward: -1.0 Average loss: 0.007909123 Explore P: 0.010000374\n",
      "Episode: 493 Total reward: -3.0 Average loss: 0.008404666 Explore P: 0.010000363\n",
      "Episode: 494 Total reward: -4.0 Average loss: 0.014254233 Explore P: 0.010000352\n",
      "Episode: 495 Total reward: 1.0 Average loss: 0.011280975 Explore P: 0.010000341\n",
      "Episode: 496 Total reward: -3.0 Average loss: 0.006428014 Explore P: 0.010000331\n",
      "Episode: 497 Total reward: 2.0 Average loss: 0.003128053 Explore P: 0.010000322\n",
      "Episode: 498 Total reward: -1.0 Average loss: 0.002625989 Explore P: 0.010000312\n",
      "Episode: 499 Total reward: -3.0 Average loss: 0.002985117 Explore P: 0.010000303\n",
      "Episode: 500 Total reward: -1.0 Average loss: 0.005282689 Explore P: 0.010000294\n",
      "Episode: 501 Total reward: -2.0 Average loss: 0.006863864 Explore P: 0.010000285\n",
      "Episode: 502 Total reward: -3.0 Average loss: 0.008942056 Explore P: 0.010000277\n",
      "Episode: 503 Total reward: -4.0 Average loss: 0.007140380 Explore P: 0.010000269\n",
      "Episode: 504 Total reward: -4.0 Average loss: 0.005235329 Explore P: 0.010000261\n",
      "Episode: 505 Total reward: -4.0 Average loss: 0.003007553 Explore P: 0.010000253\n",
      "Episode: 506 Total reward: -2.0 Average loss: 0.002550215 Explore P: 0.010000245\n",
      "Episode: 507 Total reward: 0.0 Average loss: 0.002962033 Explore P: 0.010000238\n",
      "Episode: 508 Total reward: 1.0 Average loss: 0.002905355 Explore P: 0.010000231\n",
      "Episode: 509 Total reward: -4.0 Average loss: 0.005290217 Explore P: 0.010000224\n",
      "Episode: 510 Total reward: -1.0 Average loss: 0.006012009 Explore P: 0.010000218\n",
      "Episode: 511 Total reward: 1.0 Average loss: 0.009046438 Explore P: 0.010000211\n",
      "Episode: 512 Total reward: 0.0 Average loss: 0.009445286 Explore P: 0.010000205\n",
      "Episode: 513 Total reward: 1.0 Average loss: 0.008462872 Explore P: 0.010000199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 514 Total reward: 0.0 Average loss: 0.005987152 Explore P: 0.010000193\n",
      "Episode: 515 Total reward: 0.0 Average loss: 0.003318171 Explore P: 0.010000187\n",
      "Episode: 516 Total reward: 1.0 Average loss: 0.003932383 Explore P: 0.010000182\n",
      "Episode: 517 Total reward: 2.0 Average loss: 0.004500999 Explore P: 0.010000176\n",
      "Episode: 518 Total reward: -2.0 Average loss: 0.004799596 Explore P: 0.010000171\n",
      "Episode: 519 Total reward: 1.0 Average loss: 0.008410360 Explore P: 0.010000166\n",
      "Episode: 520 Total reward: 1.0 Average loss: 0.008890964 Explore P: 0.010000161\n",
      "Episode: 521 Total reward: -1.0 Average loss: 0.010463728 Explore P: 0.010000157\n",
      "Episode: 522 Total reward: -2.0 Average loss: 0.008030135 Explore P: 0.010000152\n",
      "Episode: 523 Total reward: -3.0 Average loss: 0.005640310 Explore P: 0.010000147\n",
      "Episode: 524 Total reward: -3.0 Average loss: 0.003547465 Explore P: 0.010000143\n",
      "Episode: 525 Total reward: -2.0 Average loss: 0.003014635 Explore P: 0.010000139\n",
      "Episode: 526 Total reward: -2.0 Average loss: 0.002691525 Explore P: 0.010000135\n",
      "Episode: 527 Total reward: 0.0 Average loss: 0.003249117 Explore P: 0.010000131\n",
      "Episode: 528 Total reward: 0.0 Average loss: 0.002782822 Explore P: 0.010000127\n",
      "Episode: 529 Total reward: -1.0 Average loss: 0.002574883 Explore P: 0.010000123\n",
      "Episode: 530 Total reward: 3.0 Average loss: 0.003093157 Explore P: 0.010000119\n",
      "Episode: 531 Total reward: -3.0 Average loss: 0.003781518 Explore P: 0.010000116\n",
      "Episode: 532 Total reward: 0.0 Average loss: 0.006398735 Explore P: 0.010000113\n",
      "Episode: 533 Total reward: 0.0 Average loss: 0.006316967 Explore P: 0.010000109\n",
      "Episode: 534 Total reward: 0.0 Average loss: 0.005218143 Explore P: 0.010000106\n",
      "Episode: 535 Total reward: -2.0 Average loss: 0.005465839 Explore P: 0.010000103\n",
      "Episode: 536 Total reward: -1.0 Average loss: 0.005345469 Explore P: 0.010000100\n",
      "Episode: 537 Total reward: -1.0 Average loss: 0.006785705 Explore P: 0.010000097\n",
      "Episode: 538 Total reward: -3.0 Average loss: 0.007755551 Explore P: 0.010000094\n",
      "Episode: 539 Total reward: -1.0 Average loss: 0.007155520 Explore P: 0.010000091\n",
      "Episode: 540 Total reward: -2.0 Average loss: 0.005720297 Explore P: 0.010000089\n",
      "Episode: 541 Total reward: -2.0 Average loss: 0.003665413 Explore P: 0.010000086\n",
      "Episode: 542 Total reward: -1.0 Average loss: 0.003287575 Explore P: 0.010000083\n",
      "Episode: 543 Total reward: -5.0 Average loss: 0.003805645 Explore P: 0.010000081\n",
      "Episode: 544 Total reward: -2.0 Average loss: 0.003538882 Explore P: 0.010000079\n",
      "Episode: 545 Total reward: 0.0 Average loss: 0.006207125 Explore P: 0.010000076\n",
      "Episode: 546 Total reward: -1.0 Average loss: 0.007809970 Explore P: 0.010000074\n",
      "Episode: 547 Total reward: 2.0 Average loss: 0.009098029 Explore P: 0.010000072\n",
      "Episode: 548 Total reward: 3.0 Average loss: 0.007282745 Explore P: 0.010000070\n",
      "Episode: 549 Total reward: 2.0 Average loss: 0.007057460 Explore P: 0.010000068\n",
      "Episode: 550 Total reward: 4.0 Average loss: 0.005214020 Explore P: 0.010000066\n",
      "Episode: 551 Total reward: -3.0 Average loss: 0.004766422 Explore P: 0.010000064\n",
      "Episode: 552 Total reward: 2.0 Average loss: 0.003045230 Explore P: 0.010000062\n",
      "Episode: 553 Total reward: 5.0 Average loss: 0.002722783 Explore P: 0.010000060\n",
      "Episode: 554 Total reward: 1.0 Average loss: 0.002839297 Explore P: 0.010000058\n",
      "Episode: 555 Total reward: 1.0 Average loss: 0.002563003 Explore P: 0.010000056\n",
      "Episode: 556 Total reward: -1.0 Average loss: 0.002947437 Explore P: 0.010000055\n",
      "Episode: 557 Total reward: 3.0 Average loss: 0.002964040 Explore P: 0.010000053\n",
      "Episode: 558 Total reward: -1.0 Average loss: 0.002718269 Explore P: 0.010000052\n",
      "Episode: 559 Total reward: 1.0 Average loss: 0.005454344 Explore P: 0.010000050\n",
      "Episode: 560 Total reward: -2.0 Average loss: 0.006191352 Explore P: 0.010000049\n",
      "Episode: 561 Total reward: -2.0 Average loss: 0.006974133 Explore P: 0.010000047\n",
      "Episode: 562 Total reward: 2.0 Average loss: 0.008670454 Explore P: 0.010000046\n",
      "Episode: 563 Total reward: 5.0 Average loss: 0.006810046 Explore P: 0.010000044\n",
      "Episode: 564 Total reward: 3.0 Average loss: 0.006279890 Explore P: 0.010000043\n",
      "Episode: 565 Total reward: -1.0 Average loss: 0.003421128 Explore P: 0.010000042\n",
      "Episode: 566 Total reward: 1.0 Average loss: 0.003931622 Explore P: 0.010000041\n",
      "Episode: 567 Total reward: -2.0 Average loss: 0.003996407 Explore P: 0.010000039\n",
      "Episode: 568 Total reward: 0.0 Average loss: 0.003978091 Explore P: 0.010000038\n",
      "Episode: 569 Total reward: -2.0 Average loss: 0.003174300 Explore P: 0.010000037\n",
      "Episode: 570 Total reward: 0.0 Average loss: 0.002605591 Explore P: 0.010000036\n",
      "Episode: 571 Total reward: 0.0 Average loss: 0.002542300 Explore P: 0.010000035\n",
      "Episode: 572 Total reward: -1.0 Average loss: 0.005053005 Explore P: 0.010000034\n",
      "Episode: 573 Total reward: -1.0 Average loss: 0.005151920 Explore P: 0.010000033\n",
      "Episode: 574 Total reward: 1.0 Average loss: 0.005620194 Explore P: 0.010000032\n",
      "Episode: 575 Total reward: 2.0 Average loss: 0.003524551 Explore P: 0.010000031\n",
      "Episode: 576 Total reward: 0.0 Average loss: 0.005317600 Explore P: 0.010000030\n",
      "Episode: 577 Total reward: 1.0 Average loss: 0.005548864 Explore P: 0.010000029\n",
      "Episode: 578 Total reward: 0.0 Average loss: 0.005664512 Explore P: 0.010000028\n",
      "Episode: 579 Total reward: -2.0 Average loss: 0.005493260 Explore P: 0.010000027\n",
      "Episode: 580 Total reward: -2.0 Average loss: 0.004759528 Explore P: 0.010000027\n",
      "Episode: 581 Total reward: 0.0 Average loss: 0.005610031 Explore P: 0.010000026\n",
      "Episode: 582 Total reward: 2.0 Average loss: 0.006327311 Explore P: 0.010000025\n",
      "Episode: 583 Total reward: -1.0 Average loss: 0.008143448 Explore P: 0.010000024\n",
      "Episode: 584 Total reward: -4.0 Average loss: 0.008685759 Explore P: 0.010000024\n",
      "Episode: 585 Total reward: 1.0 Average loss: 0.008236662 Explore P: 0.010000023\n",
      "Episode: 586 Total reward: -3.0 Average loss: 0.006472727 Explore P: 0.010000022\n",
      "Episode: 587 Total reward: -4.0 Average loss: 0.004836788 Explore P: 0.010000022\n",
      "Episode: 588 Total reward: -1.0 Average loss: 0.003881139 Explore P: 0.010000021\n",
      "Episode: 589 Total reward: 0.0 Average loss: 0.004600200 Explore P: 0.010000020\n",
      "Episode: 590 Total reward: -2.0 Average loss: 0.004719688 Explore P: 0.010000020\n",
      "Episode: 591 Total reward: -1.0 Average loss: 0.004394691 Explore P: 0.010000019\n",
      "Episode: 592 Total reward: -1.0 Average loss: 0.003793833 Explore P: 0.010000019\n",
      "Episode: 593 Total reward: 0.0 Average loss: 0.002821711 Explore P: 0.010000018\n",
      "Episode: 594 Total reward: -5.0 Average loss: 0.004451135 Explore P: 0.010000018\n",
      "Episode: 595 Total reward: -2.0 Average loss: 0.005601143 Explore P: 0.010000017\n",
      "Episode: 596 Total reward: 0.0 Average loss: 0.005847344 Explore P: 0.010000016\n",
      "Episode: 597 Total reward: 0.0 Average loss: 0.006545057 Explore P: 0.010000016\n",
      "Episode: 598 Total reward: -1.0 Average loss: 0.007752928 Explore P: 0.010000016\n",
      "Episode: 599 Total reward: 0.0 Average loss: 0.009692552 Explore P: 0.010000015\n",
      "Episode: 600 Total reward: -3.0 Average loss: 0.007950299 Explore P: 0.010000015\n",
      "Episode: 601 Total reward: -2.0 Average loss: 0.006338170 Explore P: 0.010000014\n",
      "Episode: 602 Total reward: -5.0 Average loss: 0.004025718 Explore P: 0.010000014\n",
      "Episode: 603 Total reward: -2.0 Average loss: 0.003595872 Explore P: 0.010000013\n",
      "Episode: 604 Total reward: -3.0 Average loss: 0.005893294 Explore P: 0.010000013\n",
      "Episode: 605 Total reward: -3.0 Average loss: 0.004600008 Explore P: 0.010000013\n",
      "Episode: 606 Total reward: -1.0 Average loss: 0.004808615 Explore P: 0.010000012\n",
      "Episode: 607 Total reward: -5.0 Average loss: 0.003215032 Explore P: 0.010000012\n",
      "Episode: 608 Total reward: -1.0 Average loss: 0.002658007 Explore P: 0.010000012\n",
      "Episode: 609 Total reward: -2.0 Average loss: 0.004020254 Explore P: 0.010000011\n",
      "Episode: 610 Total reward: -2.0 Average loss: 0.006511353 Explore P: 0.010000011\n",
      "Episode: 611 Total reward: 0.0 Average loss: 0.006616377 Explore P: 0.010000011\n",
      "Episode: 612 Total reward: 0.0 Average loss: 0.008052877 Explore P: 0.010000010\n",
      "Episode: 613 Total reward: -1.0 Average loss: 0.006398654 Explore P: 0.010000010\n",
      "Episode: 614 Total reward: 0.0 Average loss: 0.008273317 Explore P: 0.010000010\n",
      "Episode: 615 Total reward: 0.0 Average loss: 0.005419193 Explore P: 0.010000009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 616 Total reward: -5.0 Average loss: 0.004478508 Explore P: 0.010000009\n",
      "Episode: 617 Total reward: -1.0 Average loss: 0.003064415 Explore P: 0.010000009\n",
      "Episode: 618 Total reward: -1.0 Average loss: 0.002570888 Explore P: 0.010000009\n",
      "Episode: 619 Total reward: 0.0 Average loss: 0.008953921 Explore P: 0.010000008\n",
      "Episode: 620 Total reward: 0.0 Average loss: 0.007972565 Explore P: 0.010000008\n",
      "Episode: 621 Total reward: 0.0 Average loss: 0.006180406 Explore P: 0.010000008\n",
      "Episode: 622 Total reward: 0.0 Average loss: 0.004817788 Explore P: 0.010000008\n",
      "Episode: 623 Total reward: 1.0 Average loss: 0.005043079 Explore P: 0.010000007\n",
      "Episode: 624 Total reward: 0.0 Average loss: 0.004668725 Explore P: 0.010000007\n",
      "Episode: 625 Total reward: 2.0 Average loss: 0.005888027 Explore P: 0.010000007\n",
      "Episode: 626 Total reward: -1.0 Average loss: 0.007546290 Explore P: 0.010000007\n",
      "Episode: 627 Total reward: 1.0 Average loss: 0.010127221 Explore P: 0.010000007\n",
      "Episode: 628 Total reward: -1.0 Average loss: 0.008538033 Explore P: 0.010000006\n",
      "Episode: 629 Total reward: 1.0 Average loss: 0.005652357 Explore P: 0.010000006\n",
      "Episode: 630 Total reward: 0.0 Average loss: 0.005115949 Explore P: 0.010000006\n",
      "Episode: 631 Total reward: 1.0 Average loss: 0.004867509 Explore P: 0.010000006\n",
      "Episode: 632 Total reward: 0.0 Average loss: 0.006286024 Explore P: 0.010000006\n",
      "Episode: 633 Total reward: 1.0 Average loss: 0.004919237 Explore P: 0.010000005\n",
      "Episode: 634 Total reward: -1.0 Average loss: 0.005507478 Explore P: 0.010000005\n",
      "Episode: 635 Total reward: -2.0 Average loss: 0.005923197 Explore P: 0.010000005\n",
      "Episode: 636 Total reward: 0.0 Average loss: 0.005549264 Explore P: 0.010000005\n",
      "Episode: 637 Total reward: 1.0 Average loss: 0.007471770 Explore P: 0.010000005\n",
      "Episode: 638 Total reward: 1.0 Average loss: 0.005957020 Explore P: 0.010000005\n",
      "Episode: 639 Total reward: 2.0 Average loss: 0.005186494 Explore P: 0.010000005\n",
      "Episode: 640 Total reward: 0.0 Average loss: 0.003770359 Explore P: 0.010000004\n",
      "Episode: 641 Total reward: 2.0 Average loss: 0.005874269 Explore P: 0.010000004\n",
      "Episode: 642 Total reward: 0.0 Average loss: 0.005967411 Explore P: 0.010000004\n",
      "Episode: 643 Total reward: 0.0 Average loss: 0.007368128 Explore P: 0.010000004\n",
      "Episode: 644 Total reward: 0.0 Average loss: 0.007817219 Explore P: 0.010000004\n",
      "Episode: 645 Total reward: 3.0 Average loss: 0.007324238 Explore P: 0.010000004\n",
      "Episode: 646 Total reward: 0.0 Average loss: 0.008731069 Explore P: 0.010000004\n",
      "Episode: 647 Total reward: 0.0 Average loss: 0.004050435 Explore P: 0.010000004\n",
      "Episode: 648 Total reward: 2.0 Average loss: 0.004652749 Explore P: 0.010000003\n",
      "Episode: 649 Total reward: -1.0 Average loss: 0.005536132 Explore P: 0.010000003\n",
      "Episode: 650 Total reward: 1.0 Average loss: 0.005211167 Explore P: 0.010000003\n",
      "Episode: 651 Total reward: 0.0 Average loss: 0.004123508 Explore P: 0.010000003\n",
      "Episode: 652 Total reward: 0.0 Average loss: 0.002805372 Explore P: 0.010000003\n",
      "Episode: 653 Total reward: 1.0 Average loss: 0.005305840 Explore P: 0.010000003\n",
      "Episode: 654 Total reward: 0.0 Average loss: 0.006393714 Explore P: 0.010000003\n",
      "Episode: 655 Total reward: 1.0 Average loss: 0.006423941 Explore P: 0.010000003\n",
      "Episode: 656 Total reward: 3.0 Average loss: 0.004660830 Explore P: 0.010000003\n",
      "Episode: 657 Total reward: 3.0 Average loss: 0.003399960 Explore P: 0.010000003\n",
      "Episode: 658 Total reward: -1.0 Average loss: 0.003322981 Explore P: 0.010000003\n",
      "Episode: 659 Total reward: -1.0 Average loss: 0.003276231 Explore P: 0.010000002\n",
      "Episode: 660 Total reward: 0.0 Average loss: 0.003076490 Explore P: 0.010000002\n",
      "Episode: 661 Total reward: -1.0 Average loss: 0.004203058 Explore P: 0.010000002\n",
      "Episode: 662 Total reward: 1.0 Average loss: 0.004365532 Explore P: 0.010000002\n",
      "Episode: 663 Total reward: -2.0 Average loss: 0.006118900 Explore P: 0.010000002\n",
      "Episode: 664 Total reward: 0.0 Average loss: 0.004138329 Explore P: 0.010000002\n",
      "Episode: 665 Total reward: 0.0 Average loss: 0.006527293 Explore P: 0.010000002\n",
      "Episode: 666 Total reward: 1.0 Average loss: 0.006927659 Explore P: 0.010000002\n",
      "Episode: 667 Total reward: 2.0 Average loss: 0.006495583 Explore P: 0.010000002\n",
      "Episode: 668 Total reward: 1.0 Average loss: 0.004680968 Explore P: 0.010000002\n",
      "Episode: 669 Total reward: 1.0 Average loss: 0.004367006 Explore P: 0.010000002\n",
      "Episode: 670 Total reward: 4.0 Average loss: 0.003626604 Explore P: 0.010000002\n",
      "Episode: 671 Total reward: 0.0 Average loss: 0.003495965 Explore P: 0.010000002\n",
      "Episode: 672 Total reward: -1.0 Average loss: 0.003875951 Explore P: 0.010000002\n",
      "Episode: 673 Total reward: 1.0 Average loss: 0.005106139 Explore P: 0.010000002\n",
      "Episode: 674 Total reward: 1.0 Average loss: 0.004925995 Explore P: 0.010000002\n",
      "Episode: 675 Total reward: -1.0 Average loss: 0.006425919 Explore P: 0.010000002\n",
      "Episode: 676 Total reward: 3.0 Average loss: 0.009426376 Explore P: 0.010000001\n",
      "Episode: 677 Total reward: 1.0 Average loss: 0.009923248 Explore P: 0.010000001\n",
      "Episode: 678 Total reward: 3.0 Average loss: 0.008025413 Explore P: 0.010000001\n",
      "Episode: 679 Total reward: 1.0 Average loss: 0.005719388 Explore P: 0.010000001\n",
      "Episode: 680 Total reward: 3.0 Average loss: 0.005349694 Explore P: 0.010000001\n",
      "Episode: 681 Total reward: -1.0 Average loss: 0.006225748 Explore P: 0.010000001\n",
      "Episode: 682 Total reward: 0.0 Average loss: 0.006816891 Explore P: 0.010000001\n",
      "Episode: 683 Total reward: 0.0 Average loss: 0.006347008 Explore P: 0.010000001\n",
      "Episode: 684 Total reward: -6.0 Average loss: 0.007004827 Explore P: 0.010000001\n",
      "Episode: 685 Total reward: -3.0 Average loss: 0.005298027 Explore P: 0.010000001\n",
      "Episode: 686 Total reward: -11.0 Average loss: 0.004736749 Explore P: 0.010000001\n",
      "Episode: 687 Total reward: -6.0 Average loss: 0.003141959 Explore P: 0.010000001\n",
      "Episode: 688 Total reward: 1.0 Average loss: 0.002541002 Explore P: 0.010000001\n",
      "Episode: 689 Total reward: -4.0 Average loss: 0.004506137 Explore P: 0.010000001\n",
      "Episode: 690 Total reward: 4.0 Average loss: 0.005403822 Explore P: 0.010000001\n",
      "Episode: 691 Total reward: 0.0 Average loss: 0.005476262 Explore P: 0.010000001\n",
      "Episode: 692 Total reward: 0.0 Average loss: 0.004133353 Explore P: 0.010000001\n",
      "Episode: 693 Total reward: 3.0 Average loss: 0.003929264 Explore P: 0.010000001\n",
      "Episode: 694 Total reward: 4.0 Average loss: 0.004419622 Explore P: 0.010000001\n",
      "Episode: 695 Total reward: 1.0 Average loss: 0.005154894 Explore P: 0.010000001\n",
      "Episode: 696 Total reward: 0.0 Average loss: 0.005405881 Explore P: 0.010000001\n",
      "Episode: 697 Total reward: -2.0 Average loss: 0.004925344 Explore P: 0.010000001\n",
      "Episode: 698 Total reward: -4.0 Average loss: 0.004520708 Explore P: 0.010000001\n",
      "Episode: 699 Total reward: 1.0 Average loss: 0.003267537 Explore P: 0.010000001\n",
      "Episode: 700 Total reward: 0.0 Average loss: 0.004049857 Explore P: 0.010000001\n",
      "Episode: 701 Total reward: -5.0 Average loss: 0.006599729 Explore P: 0.010000001\n",
      "Episode: 702 Total reward: -1.0 Average loss: 0.006743202 Explore P: 0.010000001\n",
      "Episode: 703 Total reward: -5.0 Average loss: 0.007363013 Explore P: 0.010000001\n",
      "Episode: 704 Total reward: -1.0 Average loss: 0.003874155 Explore P: 0.010000001\n",
      "Episode: 705 Total reward: -3.0 Average loss: 0.003383087 Explore P: 0.010000001\n",
      "Episode: 706 Total reward: -4.0 Average loss: 0.005811647 Explore P: 0.010000001\n",
      "Episode: 707 Total reward: -2.0 Average loss: 0.009035612 Explore P: 0.010000001\n",
      "Episode: 708 Total reward: 0.0 Average loss: 0.010717219 Explore P: 0.010000001\n",
      "Episode: 709 Total reward: -1.0 Average loss: 0.012767416 Explore P: 0.010000001\n",
      "Episode: 710 Total reward: -1.0 Average loss: 0.020907497 Explore P: 0.010000001\n",
      "Episode: 711 Total reward: 1.0 Average loss: 0.011171015 Explore P: 0.010000001\n",
      "Episode: 712 Total reward: 0.0 Average loss: 0.007540206 Explore P: 0.010000001\n",
      "Episode: 713 Total reward: -1.0 Average loss: 0.005405267 Explore P: 0.010000000\n",
      "Episode: 714 Total reward: 1.0 Average loss: 0.004597182 Explore P: 0.010000000\n",
      "Episode: 715 Total reward: 0.0 Average loss: 0.005675654 Explore P: 0.010000000\n",
      "Episode: 716 Total reward: 0.0 Average loss: 0.007010090 Explore P: 0.010000000\n",
      "Episode: 717 Total reward: 0.0 Average loss: 0.007661013 Explore P: 0.010000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 718 Total reward: -2.0 Average loss: 0.007147713 Explore P: 0.010000000\n",
      "Episode: 719 Total reward: -1.0 Average loss: 0.005910043 Explore P: 0.010000000\n",
      "Episode: 720 Total reward: 1.0 Average loss: 0.005114684 Explore P: 0.010000000\n",
      "Episode: 721 Total reward: 0.0 Average loss: 0.003974101 Explore P: 0.010000000\n",
      "Episode: 722 Total reward: 1.0 Average loss: 0.006211379 Explore P: 0.010000000\n",
      "Episode: 723 Total reward: 0.0 Average loss: 0.006440932 Explore P: 0.010000000\n",
      "Episode: 724 Total reward: -1.0 Average loss: 0.007618483 Explore P: 0.010000000\n",
      "Episode: 725 Total reward: 1.0 Average loss: 0.005816022 Explore P: 0.010000000\n",
      "Episode: 726 Total reward: -1.0 Average loss: 0.007553733 Explore P: 0.010000000\n",
      "Episode: 727 Total reward: -4.0 Average loss: 0.006241217 Explore P: 0.010000000\n",
      "Episode: 728 Total reward: 2.0 Average loss: 0.006847227 Explore P: 0.010000000\n",
      "Episode: 729 Total reward: 0.0 Average loss: 0.006807211 Explore P: 0.010000000\n",
      "Episode: 730 Total reward: -2.0 Average loss: 0.008915230 Explore P: 0.010000000\n",
      "Episode: 731 Total reward: -3.0 Average loss: 0.007523004 Explore P: 0.010000000\n",
      "Episode: 732 Total reward: -1.0 Average loss: 0.008905861 Explore P: 0.010000000\n",
      "Episode: 733 Total reward: 1.0 Average loss: 0.010132039 Explore P: 0.010000000\n",
      "Episode: 734 Total reward: -2.0 Average loss: 0.009350025 Explore P: 0.010000000\n",
      "Episode: 735 Total reward: 2.0 Average loss: 0.009883084 Explore P: 0.010000000\n",
      "Episode: 736 Total reward: 0.0 Average loss: 0.008476728 Explore P: 0.010000000\n",
      "Episode: 737 Total reward: -1.0 Average loss: 0.006853089 Explore P: 0.010000000\n",
      "Episode: 738 Total reward: -3.0 Average loss: 0.005310291 Explore P: 0.010000000\n",
      "Episode: 739 Total reward: 0.0 Average loss: 0.003413429 Explore P: 0.010000000\n",
      "Episode: 740 Total reward: 0.0 Average loss: 0.002769880 Explore P: 0.010000000\n",
      "Episode: 741 Total reward: -1.0 Average loss: 0.002647178 Explore P: 0.010000000\n",
      "Episode: 742 Total reward: -1.0 Average loss: 0.003096864 Explore P: 0.010000000\n",
      "Episode: 743 Total reward: -1.0 Average loss: 0.003359691 Explore P: 0.010000000\n",
      "Episode: 744 Total reward: 1.0 Average loss: 0.005000766 Explore P: 0.010000000\n",
      "Episode: 745 Total reward: -2.0 Average loss: 0.005240851 Explore P: 0.010000000\n",
      "Episode: 746 Total reward: 2.0 Average loss: 0.006046395 Explore P: 0.010000000\n",
      "Episode: 747 Total reward: 0.0 Average loss: 0.004267646 Explore P: 0.010000000\n",
      "Episode: 748 Total reward: 0.0 Average loss: 0.004166455 Explore P: 0.010000000\n",
      "Episode: 749 Total reward: -1.0 Average loss: 0.005658405 Explore P: 0.010000000\n",
      "Episode: 750 Total reward: 0.0 Average loss: 0.007322475 Explore P: 0.010000000\n",
      "Episode: 751 Total reward: 0.0 Average loss: 0.013251660 Explore P: 0.010000000\n",
      "Episode: 752 Total reward: 0.0 Average loss: 0.009531018 Explore P: 0.010000000\n",
      "Episode: 753 Total reward: 2.0 Average loss: 0.020684706 Explore P: 0.010000000\n",
      "Episode: 754 Total reward: 0.0 Average loss: 0.016739972 Explore P: 0.010000000\n",
      "Episode: 755 Total reward: -1.0 Average loss: 0.008176990 Explore P: 0.010000000\n",
      "Episode: 756 Total reward: -2.0 Average loss: 0.004029260 Explore P: 0.010000000\n",
      "Episode: 757 Total reward: -4.0 Average loss: 0.002654456 Explore P: 0.010000000\n",
      "Episode: 758 Total reward: -2.0 Average loss: 0.002734119 Explore P: 0.010000000\n",
      "Episode: 759 Total reward: -3.0 Average loss: 0.002682756 Explore P: 0.010000000\n",
      "Episode: 760 Total reward: -2.0 Average loss: 0.002735603 Explore P: 0.010000000\n",
      "Episode: 761 Total reward: 0.0 Average loss: 0.003554815 Explore P: 0.010000000\n",
      "Episode: 762 Total reward: 0.0 Average loss: 0.003435747 Explore P: 0.010000000\n",
      "Episode: 763 Total reward: 0.0 Average loss: 0.003908806 Explore P: 0.010000000\n",
      "Episode: 764 Total reward: -1.0 Average loss: 0.003339087 Explore P: 0.010000000\n",
      "Episode: 765 Total reward: 1.0 Average loss: 0.002497777 Explore P: 0.010000000\n",
      "Episode: 766 Total reward: -2.0 Average loss: 0.011170682 Explore P: 0.010000000\n",
      "Episode: 767 Total reward: -1.0 Average loss: 0.011821750 Explore P: 0.010000000\n",
      "Episode: 768 Total reward: 0.0 Average loss: 0.009426879 Explore P: 0.010000000\n",
      "Episode: 769 Total reward: -1.0 Average loss: 0.003942917 Explore P: 0.010000000\n",
      "Episode: 770 Total reward: -4.0 Average loss: 0.003231823 Explore P: 0.010000000\n",
      "Episode: 771 Total reward: -1.0 Average loss: 0.002983883 Explore P: 0.010000000\n",
      "Episode: 772 Total reward: 1.0 Average loss: 0.002732157 Explore P: 0.010000000\n",
      "Episode: 773 Total reward: -1.0 Average loss: 0.002561323 Explore P: 0.010000000\n",
      "Episode: 774 Total reward: -1.0 Average loss: 0.002651947 Explore P: 0.010000000\n",
      "Episode: 775 Total reward: -2.0 Average loss: 0.003057533 Explore P: 0.010000000\n",
      "Episode: 776 Total reward: -1.0 Average loss: 0.002642335 Explore P: 0.010000000\n",
      "Episode: 777 Total reward: -1.0 Average loss: 0.004919886 Explore P: 0.010000000\n",
      "Episode: 778 Total reward: -1.0 Average loss: 0.006619883 Explore P: 0.010000000\n",
      "Episode: 779 Total reward: 0.0 Average loss: 0.008832767 Explore P: 0.010000000\n",
      "Episode: 780 Total reward: 1.0 Average loss: 0.009987517 Explore P: 0.010000000\n",
      "Episode: 781 Total reward: 0.0 Average loss: 0.007614605 Explore P: 0.010000000\n",
      "Episode: 782 Total reward: -1.0 Average loss: 0.005333547 Explore P: 0.010000000\n",
      "Episode: 783 Total reward: -2.0 Average loss: 0.003328022 Explore P: 0.010000000\n",
      "Episode: 784 Total reward: 0.0 Average loss: 0.003756484 Explore P: 0.010000000\n",
      "Episode: 785 Total reward: 0.0 Average loss: 0.003694458 Explore P: 0.010000000\n",
      "Episode: 786 Total reward: -1.0 Average loss: 0.006233764 Explore P: 0.010000000\n",
      "Episode: 787 Total reward: 2.0 Average loss: 0.007451908 Explore P: 0.010000000\n",
      "Episode: 788 Total reward: -1.0 Average loss: 0.007561641 Explore P: 0.010000000\n",
      "Episode: 789 Total reward: -1.0 Average loss: 0.006921194 Explore P: 0.010000000\n",
      "Episode: 790 Total reward: -2.0 Average loss: 0.006618576 Explore P: 0.010000000\n",
      "Episode: 791 Total reward: 0.0 Average loss: 0.005897377 Explore P: 0.010000000\n",
      "Episode: 792 Total reward: -2.0 Average loss: 0.005565424 Explore P: 0.010000000\n",
      "Episode: 793 Total reward: -2.0 Average loss: 0.005189752 Explore P: 0.010000000\n",
      "Episode: 794 Total reward: 0.0 Average loss: 0.005237742 Explore P: 0.010000000\n",
      "Episode: 795 Total reward: 1.0 Average loss: 0.004360228 Explore P: 0.010000000\n",
      "Episode: 796 Total reward: -1.0 Average loss: 0.006589913 Explore P: 0.010000000\n",
      "Episode: 797 Total reward: 0.0 Average loss: 0.007434246 Explore P: 0.010000000\n",
      "Episode: 798 Total reward: -2.0 Average loss: 0.008409861 Explore P: 0.010000000\n",
      "Episode: 799 Total reward: 0.0 Average loss: 0.005412738 Explore P: 0.010000000\n",
      "Episode: 800 Total reward: -2.0 Average loss: 0.007273575 Explore P: 0.010000000\n",
      "Episode: 801 Total reward: -1.0 Average loss: 0.007894619 Explore P: 0.010000000\n",
      "Episode: 802 Total reward: 0.0 Average loss: 0.006695884 Explore P: 0.010000000\n",
      "Episode: 803 Total reward: 1.0 Average loss: 0.005930289 Explore P: 0.010000000\n",
      "Episode: 804 Total reward: 0.0 Average loss: 0.004287731 Explore P: 0.010000000\n",
      "Episode: 805 Total reward: 1.0 Average loss: 0.004518856 Explore P: 0.010000000\n",
      "Episode: 806 Total reward: 0.0 Average loss: 0.007952077 Explore P: 0.010000000\n",
      "Episode: 807 Total reward: 1.0 Average loss: 0.009563219 Explore P: 0.010000000\n",
      "Episode: 808 Total reward: 1.0 Average loss: 0.011374969 Explore P: 0.010000000\n",
      "Episode: 809 Total reward: 2.0 Average loss: 0.008912597 Explore P: 0.010000000\n",
      "Episode: 810 Total reward: 2.0 Average loss: 0.006055431 Explore P: 0.010000000\n",
      "Episode: 811 Total reward: -2.0 Average loss: 0.005022408 Explore P: 0.010000000\n",
      "Episode: 812 Total reward: 0.0 Average loss: 0.004639499 Explore P: 0.010000000\n",
      "Episode: 813 Total reward: 0.0 Average loss: 0.006413793 Explore P: 0.010000000\n",
      "Episode: 814 Total reward: 0.0 Average loss: 0.006660391 Explore P: 0.010000000\n",
      "Episode: 815 Total reward: -1.0 Average loss: 0.007217284 Explore P: 0.010000000\n",
      "Episode: 816 Total reward: 1.0 Average loss: 0.008128927 Explore P: 0.010000000\n",
      "Episode: 817 Total reward: 0.0 Average loss: 0.007867117 Explore P: 0.010000000\n",
      "Episode: 818 Total reward: 0.0 Average loss: 0.011206169 Explore P: 0.010000000\n",
      "Episode: 819 Total reward: 1.0 Average loss: 0.007971481 Explore P: 0.010000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 820 Total reward: 1.0 Average loss: 0.006590344 Explore P: 0.010000000\n",
      "Episode: 821 Total reward: 1.0 Average loss: 0.007240262 Explore P: 0.010000000\n",
      "Episode: 822 Total reward: 0.0 Average loss: 0.006044451 Explore P: 0.010000000\n",
      "Episode: 823 Total reward: 0.0 Average loss: 0.006027645 Explore P: 0.010000000\n",
      "Episode: 824 Total reward: 2.0 Average loss: 0.006875703 Explore P: 0.010000000\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = [] # accuracy\n",
    "loss_list = [] # loss\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize/restore variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Explore or exploit parameter\n",
    "    total_step = 0\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(2000):\n",
    "        \n",
    "        # Start new episode\n",
    "        #env.reset()\n",
    "        #env_info = \n",
    "        env.reset(train_mode=True)[brain_name] # reset the environment        \n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "\n",
    "        # Training steps/batches\n",
    "        #for _ in range(max_steps): # start=0, step=1, stop=max_steps/done/reward\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                # Get action from model\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([action, state, done])\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Batch from NEW memory\n",
    "            batch = memory.buffer\n",
    "            actions = np.array([each[0] for each in batch])\n",
    "            states = np.array([each[1] for each in batch])\n",
    "            dones = np.array([each[2] for each in batch])\n",
    "            \n",
    "            # Calculate targetQs/nextQs\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict={model.states: states})\n",
    "            Qs = np.max(actions_logits, axis=1)\n",
    "            targetQs = Qs * (1 - dones.astype(float))\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.actions: actions,\n",
    "                                                                     model.states: states,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # At the end of each episode/epoch\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'Total reward: {}'.format(total_reward),\n",
    "              'Average loss: {:.9f}'.format(np.mean(loss_batch)),\n",
    "              'Explore P: {:.9f}'.format(explore_p))\n",
    "        # Plotting loss and reward/acc\n",
    "        rewards_list.append((ep, total_reward))\n",
    "        loss_list.append((ep, np.mean(loss_batch)))\n",
    "        \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import gym\n",
    "# # # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('CartPole-v1')\n",
    "# # # env = gym.make('Acrobot-v1')\n",
    "# # # env = gym.make('MountainCar-v0')\n",
    "# # # env = gym.make('Pendulum-v0')\n",
    "# # # env = gym.make('Blackjack-v0')\n",
    "# # # env = gym.make('FrozenLake-v0')\n",
    "# # # env = gym.make('AirRaid-ram-v0')\n",
    "# # # env = gym.make('AirRaid-v0')\n",
    "# # # env = gym.make('BipedalWalker-v2')\n",
    "# # # env = gym.make('Copy-v0')\n",
    "# # # env = gym.make('CarRacing-v0')\n",
    "# # # env = gym.make('Ant-v2') #mujoco\n",
    "# # # env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     #sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "#     #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "#     # Episodes/epochs\n",
    "#     for _ in range(1):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "\n",
    "#         # Steps/batches\n",
    "#         #for _ in range(111111111111111111):\n",
    "#         while True:\n",
    "#             env.render()\n",
    "#             action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "#             action = np.argmax(action_logits)\n",
    "#             state, reward, done, _ = env.step(action)\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#         # Closing the env\n",
    "#         print('total_reward: {:.2f}'.format(total_reward))\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
