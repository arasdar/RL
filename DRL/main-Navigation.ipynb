{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([1.        , 0.        , 0.        , 0.        , 0.41103721,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.40960541,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.04057696,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.253993  ,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.30345654,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.19119175,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.3996276 ,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([1.        , 0.        , 0.        , 0.        , 0.41103721,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.40960541,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.04057696,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.253993  ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.30345654,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.19119175,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.3996276 ,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "0.0 0.0\n",
      "10.478745460510254 -9.995721817016602\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    rate = tf.placeholder(tf.float32, [], name='rate')\n",
    "    return states, actions, targetQs, rewards, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rewards, rate):\n",
    "    # G\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    Qs_labels = rewards[:-1] + (0.99 * targetQs[1:])\n",
    "    #g_loss = tf.reduce_mean(neg_log_prob_actions[:-1] * targetQs[1:])\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions[:-1] * Qs_labels)\n",
    "    \n",
    "    # D\n",
    "    Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    d_lossR = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs_logits,\n",
    "                                                                     labels=rate * tf.ones_like(Qs_logits)))\n",
    "    d_lossQ = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Qs_logits[:-1], shape=[-1]),\n",
    "                                                                     labels=tf.nn.sigmoid(Qs_labels)))\n",
    "    d_loss = d_lossR + d_lossQ\n",
    "\n",
    "    return actions_logits, Qs_logits, g_loss, d_loss, d_lossR, d_lossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rewards, self.rate = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.d_lossR, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, # model input\n",
    "            targetQs=self.targetQs, rewards=self.rewards, rate=self.rate) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 37) actions:(300,)\n",
      "action size:4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 37              # number of units for the input state/observation -- simulation\n",
    "action_size = 4              # number of units for the output actions -- simulation\n",
    "hidden_size = 37*16          # number of units in each Q-network hidden layer -- simulation\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "while True: # infinite number of steps\n",
    "#for _ in range(batch_size):\n",
    "    state = env_info.vector_observations[0]   # get the next state\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    #memory.buffer.append([action, state, done])\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-1.0000 rate:-0.0769 rateProb:0.4615 gloss:-0.1109 dloss:1.3839 dlossR:0.6913 dlossQ:0.6926\n",
      "Episode:1 meanR:-1.5000 rate:-0.1538 rateProb:0.4231 gloss:-0.7434 dloss:1.3464 dlossR:0.7266 dlossQ:0.6198\n",
      "Episode:2 meanR:-1.0000 rate:0.0000 rateProb:0.5000 gloss:-0.7682 dloss:1.5477 dlossR:0.8315 dlossQ:0.7162\n",
      "Episode:3 meanR:-0.7500 rate:0.0000 rateProb:0.5000 gloss:0.0120 dloss:1.3880 dlossR:0.6959 dlossQ:0.6920\n",
      "Episode:4 meanR:-0.6000 rate:0.0000 rateProb:0.5000 gloss:0.1979 dloss:1.3897 dlossR:0.6989 dlossQ:0.6908\n",
      "Episode:5 meanR:-0.6667 rate:-0.0769 rateProb:0.4615 gloss:0.3223 dloss:1.4023 dlossR:0.7228 dlossQ:0.6794\n",
      "Episode:6 meanR:-0.5714 rate:0.0000 rateProb:0.5000 gloss:0.2551 dloss:1.3922 dlossR:0.7089 dlossQ:0.6833\n",
      "Episode:7 meanR:-0.5000 rate:0.0000 rateProb:0.5000 gloss:0.1615 dloss:1.3874 dlossR:0.7044 dlossQ:0.6830\n",
      "Episode:8 meanR:-0.4444 rate:0.0000 rateProb:0.5000 gloss:0.0291 dloss:1.3876 dlossR:0.6954 dlossQ:0.6922\n",
      "Episode:9 meanR:-0.4000 rate:0.0000 rateProb:0.5000 gloss:-0.0247 dloss:1.3896 dlossR:0.6953 dlossQ:0.6943\n",
      "Episode:10 meanR:-0.3636 rate:0.0000 rateProb:0.5000 gloss:-0.0428 dloss:1.3876 dlossR:0.6973 dlossQ:0.6904\n",
      "Episode:11 meanR:-0.3333 rate:0.0000 rateProb:0.5000 gloss:-0.0391 dloss:1.3879 dlossR:0.6978 dlossQ:0.6901\n",
      "Episode:12 meanR:-0.3077 rate:0.0000 rateProb:0.5000 gloss:-0.0285 dloss:1.3889 dlossR:0.6972 dlossQ:0.6917\n",
      "Episode:13 meanR:-0.2857 rate:0.0000 rateProb:0.5000 gloss:-0.0198 dloss:1.3868 dlossR:0.6960 dlossQ:0.6909\n",
      "Episode:14 meanR:-0.2667 rate:0.0000 rateProb:0.5000 gloss:-0.0027 dloss:1.3870 dlossR:0.6940 dlossQ:0.6930\n",
      "Episode:15 meanR:-0.2500 rate:0.0000 rateProb:0.5000 gloss:0.0196 dloss:1.3866 dlossR:0.6958 dlossQ:0.6908\n",
      "Episode:16 meanR:-0.2353 rate:0.0000 rateProb:0.5000 gloss:0.0213 dloss:1.3872 dlossR:0.6963 dlossQ:0.6909\n",
      "Episode:17 meanR:-0.2222 rate:0.0000 rateProb:0.5000 gloss:0.0242 dloss:1.3869 dlossR:0.6985 dlossQ:0.6885\n",
      "Episode:18 meanR:-0.2105 rate:0.0000 rateProb:0.5000 gloss:0.0125 dloss:1.3867 dlossR:0.6957 dlossQ:0.6910\n",
      "Episode:19 meanR:-0.2000 rate:0.0000 rateProb:0.5000 gloss:0.0008 dloss:1.3867 dlossR:0.6934 dlossQ:0.6933\n",
      "Episode:20 meanR:-0.1905 rate:0.0000 rateProb:0.5000 gloss:-0.0070 dloss:1.3866 dlossR:0.6943 dlossQ:0.6923\n",
      "Episode:21 meanR:-0.1818 rate:0.0000 rateProb:0.5000 gloss:-0.0100 dloss:1.3873 dlossR:0.6958 dlossQ:0.6916\n",
      "Episode:22 meanR:-0.1739 rate:0.0000 rateProb:0.5000 gloss:-0.0057 dloss:1.3868 dlossR:0.6949 dlossQ:0.6919\n",
      "Episode:23 meanR:-0.1667 rate:0.0000 rateProb:0.5000 gloss:-0.0038 dloss:1.3866 dlossR:0.6944 dlossQ:0.6922\n",
      "Episode:24 meanR:-0.1600 rate:0.0000 rateProb:0.5000 gloss:-0.0006 dloss:1.3866 dlossR:0.6933 dlossQ:0.6934\n",
      "Episode:25 meanR:-0.2308 rate:-0.1538 rateProb:0.4231 gloss:0.0030 dloss:1.3904 dlossR:0.6972 dlossQ:0.6932\n",
      "Episode:26 meanR:-0.1852 rate:0.0769 rateProb:0.5385 gloss:-0.0003 dloss:1.3868 dlossR:0.6935 dlossQ:0.6933\n",
      "Episode:27 meanR:-0.1786 rate:0.0000 rateProb:0.5000 gloss:-0.0003 dloss:1.3866 dlossR:0.6936 dlossQ:0.6930\n",
      "Episode:28 meanR:-0.1724 rate:0.0000 rateProb:0.5000 gloss:-0.0024 dloss:1.3872 dlossR:0.6940 dlossQ:0.6932\n",
      "Episode:29 meanR:-0.1667 rate:0.0000 rateProb:0.5000 gloss:-0.0002 dloss:1.3873 dlossR:0.6937 dlossQ:0.6936\n",
      "Episode:30 meanR:-0.1613 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3867 dlossR:0.6936 dlossQ:0.6931\n",
      "Episode:31 meanR:-0.1875 rate:-0.0769 rateProb:0.4615 gloss:0.0014 dloss:1.3883 dlossR:0.6953 dlossQ:0.6929\n",
      "Episode:32 meanR:-0.1818 rate:0.0000 rateProb:0.5000 gloss:-0.0015 dloss:1.3865 dlossR:0.6935 dlossQ:0.6930\n",
      "Episode:33 meanR:-0.1765 rate:0.0000 rateProb:0.5000 gloss:-0.0057 dloss:1.3867 dlossR:0.6949 dlossQ:0.6918\n",
      "Episode:34 meanR:-0.1714 rate:0.0000 rateProb:0.5000 gloss:-0.0048 dloss:1.3869 dlossR:0.6953 dlossQ:0.6916\n",
      "Episode:35 meanR:-0.1667 rate:0.0000 rateProb:0.5000 gloss:-0.0037 dloss:1.3869 dlossR:0.6946 dlossQ:0.6922\n",
      "Episode:36 meanR:-0.1622 rate:0.0000 rateProb:0.5000 gloss:-0.0017 dloss:1.3866 dlossR:0.6935 dlossQ:0.6931\n",
      "Episode:37 meanR:-0.1579 rate:0.0000 rateProb:0.5000 gloss:-0.0003 dloss:1.3866 dlossR:0.6934 dlossQ:0.6932\n",
      "Episode:38 meanR:-0.1538 rate:0.0000 rateProb:0.5000 gloss:0.0017 dloss:1.3867 dlossR:0.6936 dlossQ:0.6931\n",
      "Episode:39 meanR:-0.1500 rate:0.0000 rateProb:0.5000 gloss:0.0036 dloss:1.3866 dlossR:0.6945 dlossQ:0.6921\n",
      "Episode:40 meanR:-0.1463 rate:0.0000 rateProb:0.5000 gloss:0.0029 dloss:1.3866 dlossR:0.6941 dlossQ:0.6925\n",
      "Episode:41 meanR:-0.1429 rate:0.0000 rateProb:0.5000 gloss:0.0034 dloss:1.3866 dlossR:0.6937 dlossQ:0.6929\n",
      "Episode:42 meanR:-0.1395 rate:0.0000 rateProb:0.5000 gloss:0.0015 dloss:1.3868 dlossR:0.6934 dlossQ:0.6934\n",
      "Episode:43 meanR:-0.1136 rate:0.0769 rateProb:0.5385 gloss:-0.0005 dloss:1.3871 dlossR:0.6940 dlossQ:0.6931\n",
      "Episode:44 meanR:-0.1111 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3864 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:45 meanR:-0.1087 rate:0.0000 rateProb:0.5000 gloss:0.0004 dloss:1.3866 dlossR:0.6934 dlossQ:0.6933\n",
      "Episode:46 meanR:-0.1064 rate:0.0000 rateProb:0.5000 gloss:0.0003 dloss:1.3865 dlossR:0.6934 dlossQ:0.6931\n",
      "Episode:47 meanR:-0.0833 rate:0.0769 rateProb:0.5385 gloss:0.0010 dloss:1.3864 dlossR:0.6931 dlossQ:0.6933\n",
      "Episode:48 meanR:-0.0816 rate:0.0000 rateProb:0.5000 gloss:0.0023 dloss:1.3866 dlossR:0.6937 dlossQ:0.6929\n",
      "Episode:49 meanR:-0.0800 rate:0.0000 rateProb:0.5000 gloss:0.0033 dloss:1.3866 dlossR:0.6939 dlossQ:0.6927\n",
      "Episode:50 meanR:-0.0784 rate:0.0000 rateProb:0.5000 gloss:0.0035 dloss:1.3866 dlossR:0.6942 dlossQ:0.6924\n",
      "Episode:51 meanR:-0.0962 rate:-0.0769 rateProb:0.4615 gloss:0.0049 dloss:1.3900 dlossR:0.6975 dlossQ:0.6925\n",
      "Episode:52 meanR:-0.0943 rate:0.0000 rateProb:0.5000 gloss:0.0002 dloss:1.3864 dlossR:0.6933 dlossQ:0.6932\n",
      "Episode:53 meanR:-0.0926 rate:0.0000 rateProb:0.5000 gloss:-0.0025 dloss:1.3870 dlossR:0.6938 dlossQ:0.6932\n",
      "Episode:54 meanR:-0.1091 rate:-0.0769 rateProb:0.4615 gloss:-0.0032 dloss:1.3830 dlossR:0.6909 dlossQ:0.6921\n",
      "Episode:55 meanR:-0.1071 rate:0.0000 rateProb:0.5000 gloss:-0.0045 dloss:1.3869 dlossR:0.6959 dlossQ:0.6910\n",
      "Episode:56 meanR:-0.1053 rate:0.0000 rateProb:0.5000 gloss:-0.0057 dloss:1.3868 dlossR:0.6973 dlossQ:0.6895\n",
      "Episode:57 meanR:-0.1034 rate:0.0000 rateProb:0.5000 gloss:-0.0032 dloss:1.3870 dlossR:0.6944 dlossQ:0.6926\n",
      "Episode:58 meanR:-0.1017 rate:0.0000 rateProb:0.5000 gloss:0.0002 dloss:1.3866 dlossR:0.6934 dlossQ:0.6932\n",
      "Episode:59 meanR:-0.1000 rate:0.0000 rateProb:0.5000 gloss:0.0019 dloss:1.3865 dlossR:0.6935 dlossQ:0.6930\n",
      "Episode:60 meanR:-0.0984 rate:0.0000 rateProb:0.5000 gloss:0.0019 dloss:1.3865 dlossR:0.6936 dlossQ:0.6929\n",
      "Episode:61 meanR:-0.0968 rate:0.0000 rateProb:0.5000 gloss:0.0033 dloss:1.3866 dlossR:0.6947 dlossQ:0.6919\n",
      "Episode:62 meanR:-0.0952 rate:0.0000 rateProb:0.5000 gloss:0.0018 dloss:1.3868 dlossR:0.6939 dlossQ:0.6929\n",
      "Episode:63 meanR:-0.0938 rate:0.0000 rateProb:0.5000 gloss:0.0023 dloss:1.3867 dlossR:0.6939 dlossQ:0.6929\n",
      "Episode:64 meanR:-0.0923 rate:0.0000 rateProb:0.5000 gloss:-0.0023 dloss:1.3864 dlossR:0.6935 dlossQ:0.6930\n",
      "Episode:65 meanR:-0.0909 rate:0.0000 rateProb:0.5000 gloss:0.0004 dloss:1.3865 dlossR:0.6933 dlossQ:0.6932\n",
      "Episode:66 meanR:-0.0746 rate:0.0769 rateProb:0.5385 gloss:-0.0002 dloss:1.3868 dlossR:0.6936 dlossQ:0.6932\n",
      "Episode:67 meanR:-0.0735 rate:0.0000 rateProb:0.5000 gloss:0.0013 dloss:1.3865 dlossR:0.6936 dlossQ:0.6929\n",
      "Episode:68 meanR:-0.0725 rate:0.0000 rateProb:0.5000 gloss:0.0014 dloss:1.3864 dlossR:0.6936 dlossQ:0.6929\n",
      "Episode:69 meanR:-0.0714 rate:0.0000 rateProb:0.5000 gloss:0.0007 dloss:1.3865 dlossR:0.6933 dlossQ:0.6932\n",
      "Episode:70 meanR:-0.0704 rate:0.0000 rateProb:0.5000 gloss:0.0009 dloss:1.3866 dlossR:0.6934 dlossQ:0.6933\n",
      "Episode:71 meanR:-0.0694 rate:0.0000 rateProb:0.5000 gloss:0.0007 dloss:1.3864 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:72 meanR:-0.0685 rate:0.0000 rateProb:0.5000 gloss:-0.0009 dloss:1.3864 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:73 meanR:-0.0541 rate:0.0769 rateProb:0.5385 gloss:-0.0003 dloss:1.3869 dlossR:0.6936 dlossQ:0.6932\n",
      "Episode:74 meanR:-0.0533 rate:0.0000 rateProb:0.5000 gloss:0.0007 dloss:1.3866 dlossR:0.6933 dlossQ:0.6933\n",
      "Episode:75 meanR:-0.0526 rate:0.0000 rateProb:0.5000 gloss:0.0006 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:76 meanR:-0.0519 rate:0.0000 rateProb:0.5000 gloss:0.0014 dloss:1.3864 dlossR:0.6935 dlossQ:0.6928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:77 meanR:-0.0385 rate:0.0769 rateProb:0.5385 gloss:0.0018 dloss:1.3846 dlossR:0.6917 dlossQ:0.6930\n",
      "Episode:78 meanR:-0.0380 rate:0.0000 rateProb:0.5000 gloss:0.0027 dloss:1.3864 dlossR:0.6939 dlossQ:0.6925\n",
      "Episode:79 meanR:-0.0375 rate:0.0000 rateProb:0.5000 gloss:0.0028 dloss:1.3863 dlossR:0.6937 dlossQ:0.6926\n",
      "Episode:80 meanR:-0.0370 rate:0.0000 rateProb:0.5000 gloss:0.0013 dloss:1.3865 dlossR:0.6935 dlossQ:0.6930\n",
      "Episode:81 meanR:-0.0366 rate:0.0000 rateProb:0.5000 gloss:0.0010 dloss:1.3864 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:82 meanR:-0.0361 rate:0.0000 rateProb:0.5000 gloss:0.0008 dloss:1.3864 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:83 meanR:-0.0476 rate:-0.0769 rateProb:0.4615 gloss:-0.0006 dloss:1.3856 dlossR:0.6926 dlossQ:0.6930\n",
      "Episode:84 meanR:-0.0471 rate:0.0000 rateProb:0.5000 gloss:-0.0021 dloss:1.3864 dlossR:0.6945 dlossQ:0.6920\n",
      "Episode:85 meanR:-0.0465 rate:0.0000 rateProb:0.5000 gloss:-0.0034 dloss:1.3865 dlossR:0.6953 dlossQ:0.6912\n",
      "Episode:86 meanR:-0.0460 rate:0.0000 rateProb:0.5000 gloss:-0.0018 dloss:1.3864 dlossR:0.6941 dlossQ:0.6922\n",
      "Episode:87 meanR:-0.0455 rate:0.0000 rateProb:0.5000 gloss:-0.0013 dloss:1.3864 dlossR:0.6938 dlossQ:0.6926\n",
      "Episode:88 meanR:-0.0449 rate:0.0000 rateProb:0.5000 gloss:-0.0011 dloss:1.3863 dlossR:0.6934 dlossQ:0.6929\n",
      "Episode:89 meanR:-0.0444 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3864 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:90 meanR:-0.0440 rate:0.0000 rateProb:0.5000 gloss:0.0017 dloss:1.3864 dlossR:0.6936 dlossQ:0.6928\n",
      "Episode:91 meanR:-0.0435 rate:0.0000 rateProb:0.5000 gloss:0.0019 dloss:1.3864 dlossR:0.6939 dlossQ:0.6925\n",
      "Episode:92 meanR:-0.0430 rate:0.0000 rateProb:0.5000 gloss:0.0024 dloss:1.3865 dlossR:0.6941 dlossQ:0.6924\n",
      "Episode:93 meanR:-0.0426 rate:0.0000 rateProb:0.5000 gloss:0.0013 dloss:1.3864 dlossR:0.6936 dlossQ:0.6928\n",
      "Episode:94 meanR:-0.0421 rate:0.0000 rateProb:0.5000 gloss:0.0007 dloss:1.3864 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:95 meanR:-0.0417 rate:0.0000 rateProb:0.5000 gloss:-0.0001 dloss:1.3864 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:96 meanR:-0.0412 rate:0.0000 rateProb:0.5000 gloss:-0.0008 dloss:1.3865 dlossR:0.6933 dlossQ:0.6932\n",
      "Episode:97 meanR:-0.0408 rate:0.0000 rateProb:0.5000 gloss:-0.0011 dloss:1.3864 dlossR:0.6937 dlossQ:0.6927\n",
      "Episode:98 meanR:-0.0404 rate:0.0000 rateProb:0.5000 gloss:-0.0016 dloss:1.3865 dlossR:0.6937 dlossQ:0.6928\n",
      "Episode:99 meanR:-0.0400 rate:0.0000 rateProb:0.5000 gloss:-0.0013 dloss:1.3864 dlossR:0.6935 dlossQ:0.6929\n",
      "Episode:100 meanR:-0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3863 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:101 meanR:-0.0100 rate:0.0000 rateProb:0.5000 gloss:-0.0004 dloss:1.3864 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:102 meanR:-0.0100 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3864 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:103 meanR:-0.0100 rate:0.0000 rateProb:0.5000 gloss:0.0009 dloss:1.3863 dlossR:0.6934 dlossQ:0.6929\n",
      "Episode:104 meanR:-0.0100 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3864 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:105 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3864 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:106 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0004 dloss:1.3864 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:107 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:108 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:109 meanR:-0.0100 rate:-0.0769 rateProb:0.4615 gloss:-0.0002 dloss:1.3863 dlossR:0.6931 dlossQ:0.6932\n",
      "Episode:110 meanR:0.0000 rate:0.0769 rateProb:0.5385 gloss:-0.0013 dloss:1.3885 dlossR:0.6956 dlossQ:0.6929\n",
      "Episode:111 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3863 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:112 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0004 dloss:1.3863 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:113 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0000 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:114 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:115 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0010 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:116 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0008 dloss:1.3864 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:117 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0013 dloss:1.3864 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:118 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0006 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:119 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:120 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:0.0002 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:121 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:122 meanR:0.0000 rate:0.0000 rateProb:0.5000 gloss:-0.0004 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:123 meanR:0.0100 rate:0.0769 rateProb:0.5385 gloss:-0.0008 dloss:1.3878 dlossR:0.6948 dlossQ:0.6930\n",
      "Episode:124 meanR:0.0100 rate:0.0000 rateProb:0.5000 gloss:0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:125 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:0.0008 dloss:1.3864 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:126 meanR:0.0200 rate:0.0000 rateProb:0.5000 gloss:0.0017 dloss:1.3863 dlossR:0.6937 dlossQ:0.6927\n",
      "Episode:127 meanR:0.0200 rate:0.0000 rateProb:0.5000 gloss:0.0016 dloss:1.3863 dlossR:0.6936 dlossQ:0.6927\n",
      "Episode:128 meanR:0.0200 rate:0.0000 rateProb:0.5000 gloss:0.0011 dloss:1.3864 dlossR:0.6937 dlossQ:0.6927\n",
      "Episode:129 meanR:0.0200 rate:0.0000 rateProb:0.5000 gloss:0.0009 dloss:1.3863 dlossR:0.6935 dlossQ:0.6928\n",
      "Episode:130 meanR:0.0200 rate:0.0000 rateProb:0.5000 gloss:0.0003 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:131 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0003 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:132 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3864 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:133 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0007 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:134 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0010 dloss:1.3864 dlossR:0.6936 dlossQ:0.6928\n",
      "Episode:135 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0013 dloss:1.3864 dlossR:0.6935 dlossQ:0.6929\n",
      "Episode:136 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0004 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:137 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:138 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:139 meanR:0.0400 rate:0.0769 rateProb:0.5385 gloss:0.0008 dloss:1.3845 dlossR:0.6917 dlossQ:0.6929\n",
      "Episode:140 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0011 dloss:1.3863 dlossR:0.6942 dlossQ:0.6922\n",
      "Episode:141 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0014 dloss:1.3864 dlossR:0.6943 dlossQ:0.6921\n",
      "Episode:142 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0013 dloss:1.3864 dlossR:0.6941 dlossQ:0.6923\n",
      "Episode:143 meanR:0.0400 rate:0.0769 rateProb:0.5385 gloss:0.0010 dloss:1.3843 dlossR:0.6915 dlossQ:0.6928\n",
      "Episode:144 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0011 dloss:1.3865 dlossR:0.6936 dlossQ:0.6928\n",
      "Episode:145 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0007 dloss:1.3863 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:146 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:-0.0000 dloss:1.3864 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:147 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0005 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:148 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0009 dloss:1.3863 dlossR:0.6935 dlossQ:0.6929\n",
      "Episode:149 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0004 dloss:1.3863 dlossR:0.6933 dlossQ:0.6931\n",
      "Episode:150 meanR:0.0300 rate:0.0000 rateProb:0.5000 gloss:-0.0006 dloss:1.3864 dlossR:0.6934 dlossQ:0.6930\n",
      "Episode:151 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:-0.0003 dloss:1.3864 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:152 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:-0.0000 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:153 meanR:0.0400 rate:0.0000 rateProb:0.5000 gloss:0.0002 dloss:1.3864 dlossR:0.6932 dlossQ:0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:154 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0004 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:155 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0005 dloss:1.3863 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:156 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0002 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n",
      "Episode:157 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0004 dloss:1.3864 dlossR:0.6933 dlossQ:0.6930\n",
      "Episode:158 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0001 dloss:1.3863 dlossR:0.6932 dlossQ:0.6932\n",
      "Episode:159 meanR:0.0500 rate:0.0000 rateProb:0.5000 gloss:0.0000 dloss:1.3863 dlossR:0.6932 dlossQ:0.6931\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episodes_total_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(111111):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # env first state\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            action_logits, Q_logits = sess.run(fetches=[model.actions_logits, model.Qs_logits], \n",
    "                                               feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            batch.append([state, action, Q_logits, reward])\n",
    "            total_reward += reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            if done is True: # episode ended success/failure\n",
    "                episodes_total_reward.append(total_reward) # stopping criteria\n",
    "                #rate = total_reward/ 500 # success is 500 points, rate is between 0 and +1 ~ sigmoid\n",
    "                rate = total_reward/ +13 # success is +13; rate is between -1 and +1 ~ tanh\n",
    "                if rate >= +1: rate = +1\n",
    "                if rate <= -1: rate = -1\n",
    "                # min -13, max +13\n",
    "                # min -1, max +1\n",
    "                #reward = x-(-13)/ 26 # 0-1\n",
    "                #prob_rate = (rate - (-1))/ (1-(-1))\n",
    "                #prob_rate = (rate - rate_min)/ (rate_max-rate_min)\n",
    "                rate_prob = (rate + 1)/2 # success rate 0-1\n",
    "                break\n",
    "\n",
    "        # Training using batches\n",
    "        #batch = memory.buffer\n",
    "        states = np.array([each[0] for each in batch])\n",
    "        actions = np.array([each[1] for each in batch])\n",
    "        targetQs = np.array([each[2] for each in batch])\n",
    "        rewards = np.array([each[3] for each in batch])\n",
    "        g_loss, d_loss, d_lossR, d_lossQ, _, _ = sess.run([model.g_loss, model.d_loss, \n",
    "                                                           model.d_lossR, model.d_lossQ, \n",
    "                                                           model.g_opt, model.d_opt],\n",
    "                                                          feed_dict = {model.states: states, \n",
    "                                                                       model.actions: actions,\n",
    "                                                                       model.targetQs: targetQs.reshape([-1]),\n",
    "                                                                       model.rewards: rewards, \n",
    "                                                                       model.rate: rate_prob})\n",
    "        # Average 100 episode total reward\n",
    "        # Print out\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episodes_total_reward)),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'rateProb:{:.4f}'.format(rate_prob),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss),\n",
    "              'dlossR:{:.4f}'.format(d_lossR),\n",
    "              'dlossQ:{:.4f}'.format(d_lossQ))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episodes_total_reward)])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episodes_total_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import gym\n",
    "# # # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('CartPole-v1')\n",
    "# # # env = gym.make('Acrobot-v1')\n",
    "# # # env = gym.make('MountainCar-v0')\n",
    "# # # env = gym.make('Pendulum-v0')\n",
    "# # # env = gym.make('Blackjack-v0')\n",
    "# # # env = gym.make('FrozenLake-v0')\n",
    "# # # env = gym.make('AirRaid-ram-v0')\n",
    "# # # env = gym.make('AirRaid-v0')\n",
    "# # # env = gym.make('BipedalWalker-v2')\n",
    "# # # env = gym.make('Copy-v0')\n",
    "# # # env = gym.make('CarRacing-v0')\n",
    "# # # env = gym.make('Ant-v2') #mujoco\n",
    "# # # env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     #sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "#     #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "#     # Episodes/epochs\n",
    "#     for _ in range(1):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "\n",
    "#         # Steps/batches\n",
    "#         #for _ in range(111111111111111111):\n",
    "#         while True:\n",
    "#             env.render()\n",
    "#             action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "#             action = np.argmax(action_logits)\n",
    "#             state, reward, done, _ = env.step(action)\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#         # Closing the env\n",
    "#         print('total_reward: {:.2f}'.format(total_reward))\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
