{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, array([1.        , 0.        , 0.        , 0.        , 0.20790455,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.01327663,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.1713129 ,\n",
       "         0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.01027161,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.02920904,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.00962341,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, array([1.        , 0.        , 0.        , 0.        , 0.20790455,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.01327663,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.1713129 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.01027161,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.02920904,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.00962341,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "0.0 -1.0\n",
      "10.882210731506348 -10.516666412353516\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return actions, states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 37) actions:(300,)\n",
      "action size:4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 4\n",
    "state_size = 37\n",
    "hidden_size = 37*4             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:2.0000 R:2.0 loss:12.5469 exploreP:0.9707\n",
      "Episode:1 meanR:1.5000 R:1.0 loss:1114.4866 exploreP:0.9423\n",
      "Episode:2 meanR:1.3333 R:1.0 loss:13103.1416 exploreP:0.9148\n",
      "Episode:3 meanR:0.5000 R:-2.0 loss:34559.1445 exploreP:0.8881\n",
      "Episode:4 meanR:0.8000 R:2.0 loss:54251.0859 exploreP:0.8621\n",
      "Episode:5 meanR:0.6667 R:0.0 loss:42695.1484 exploreP:0.8369\n",
      "Episode:6 meanR:0.7143 R:1.0 loss:40275.0859 exploreP:0.8125\n",
      "Episode:7 meanR:0.8750 R:2.0 loss:32052.2734 exploreP:0.7888\n",
      "Episode:8 meanR:0.6667 R:-1.0 loss:32957.8984 exploreP:0.7657\n",
      "Episode:9 meanR:0.6000 R:0.0 loss:48621.6133 exploreP:0.7434\n",
      "Episode:10 meanR:0.5455 R:0.0 loss:40616.1523 exploreP:0.7217\n",
      "Episode:11 meanR:0.7500 R:3.0 loss:36226.4844 exploreP:0.7007\n",
      "Episode:12 meanR:0.5385 R:-2.0 loss:41154.7656 exploreP:0.6803\n",
      "Episode:13 meanR:0.4286 R:-1.0 loss:39051.3672 exploreP:0.6605\n",
      "Episode:14 meanR:0.4000 R:0.0 loss:32967.0469 exploreP:0.6413\n",
      "Episode:15 meanR:0.3125 R:-1.0 loss:33499.6797 exploreP:0.6226\n",
      "Episode:16 meanR:0.1176 R:-3.0 loss:28556.2539 exploreP:0.6045\n",
      "Episode:17 meanR:0.1111 R:0.0 loss:28574.7324 exploreP:0.5869\n",
      "Episode:18 meanR:0.2105 R:2.0 loss:23098.8867 exploreP:0.5699\n",
      "Episode:19 meanR:0.1000 R:-2.0 loss:16279.9932 exploreP:0.5533\n",
      "Episode:20 meanR:0.1429 R:1.0 loss:22378.3848 exploreP:0.5373\n",
      "Episode:21 meanR:0.0455 R:-2.0 loss:20119.0762 exploreP:0.5217\n",
      "Episode:22 meanR:0.0000 R:-1.0 loss:25415.7266 exploreP:0.5066\n",
      "Episode:23 meanR:0.0833 R:2.0 loss:15469.0801 exploreP:0.4919\n",
      "Episode:24 meanR:0.1200 R:1.0 loss:14067.5098 exploreP:0.4776\n",
      "Episode:25 meanR:0.1154 R:0.0 loss:21206.0391 exploreP:0.4638\n",
      "Episode:26 meanR:0.1111 R:0.0 loss:18844.7500 exploreP:0.4504\n",
      "Episode:27 meanR:0.1786 R:2.0 loss:9327.1602 exploreP:0.4374\n",
      "Episode:28 meanR:0.0690 R:-3.0 loss:14573.7812 exploreP:0.4248\n",
      "Episode:29 meanR:0.0000 R:-2.0 loss:10268.5605 exploreP:0.4125\n",
      "Episode:30 meanR:-0.0968 R:-3.0 loss:12669.8701 exploreP:0.4006\n",
      "Episode:31 meanR:-0.1562 R:-2.0 loss:6090.5015 exploreP:0.3891\n",
      "Episode:32 meanR:-0.0909 R:2.0 loss:9018.3135 exploreP:0.3779\n",
      "Episode:33 meanR:-0.0882 R:0.0 loss:5473.2700 exploreP:0.3670\n",
      "Episode:34 meanR:-0.0286 R:2.0 loss:7944.3081 exploreP:0.3564\n",
      "Episode:35 meanR:-0.0278 R:0.0 loss:8141.2300 exploreP:0.3462\n",
      "Episode:36 meanR:-0.0270 R:0.0 loss:8837.3945 exploreP:0.3363\n",
      "Episode:37 meanR:0.0000 R:1.0 loss:6151.3027 exploreP:0.3266\n",
      "Episode:38 meanR:0.0000 R:0.0 loss:5566.8999 exploreP:0.3173\n",
      "Episode:39 meanR:0.0500 R:2.0 loss:3160.6313 exploreP:0.3082\n",
      "Episode:40 meanR:0.0488 R:0.0 loss:3896.2754 exploreP:0.2994\n",
      "Episode:41 meanR:0.0238 R:-1.0 loss:4016.4238 exploreP:0.2908\n",
      "Episode:42 meanR:0.0233 R:0.0 loss:3843.9675 exploreP:0.2825\n",
      "Episode:43 meanR:-0.0227 R:-2.0 loss:2947.2241 exploreP:0.2745\n",
      "Episode:44 meanR:-0.0444 R:-1.0 loss:3443.4551 exploreP:0.2666\n",
      "Episode:45 meanR:-0.0870 R:-2.0 loss:2293.2986 exploreP:0.2591\n",
      "Episode:46 meanR:-0.0851 R:0.0 loss:1175.8275 exploreP:0.2517\n",
      "Episode:47 meanR:-0.0833 R:0.0 loss:1168.3605 exploreP:0.2446\n",
      "Episode:48 meanR:-0.1429 R:-3.0 loss:1541.4121 exploreP:0.2376\n",
      "Episode:49 meanR:-0.1600 R:-1.0 loss:713.5526 exploreP:0.2309\n",
      "Episode:50 meanR:-0.1373 R:1.0 loss:1117.9915 exploreP:0.2244\n",
      "Episode:51 meanR:-0.1346 R:0.0 loss:749.3079 exploreP:0.2180\n",
      "Episode:52 meanR:-0.1132 R:1.0 loss:624.7783 exploreP:0.2119\n",
      "Episode:53 meanR:-0.1481 R:-2.0 loss:558.3905 exploreP:0.2059\n",
      "Episode:54 meanR:-0.1636 R:-1.0 loss:537.8912 exploreP:0.2001\n",
      "Episode:55 meanR:-0.1250 R:2.0 loss:281.7664 exploreP:0.1945\n",
      "Episode:56 meanR:-0.1053 R:1.0 loss:267.3318 exploreP:0.1891\n",
      "Episode:57 meanR:-0.0517 R:3.0 loss:176.2363 exploreP:0.1838\n",
      "Episode:58 meanR:-0.1186 R:-4.0 loss:237.2696 exploreP:0.1786\n",
      "Episode:59 meanR:-0.1167 R:0.0 loss:109.0021 exploreP:0.1736\n",
      "Episode:60 meanR:-0.0656 R:3.0 loss:90.7999 exploreP:0.1688\n",
      "Episode:61 meanR:-0.0645 R:0.0 loss:71.8098 exploreP:0.1641\n",
      "Episode:62 meanR:-0.0635 R:0.0 loss:78.5762 exploreP:0.1596\n",
      "Episode:63 meanR:-0.0469 R:1.0 loss:41.5660 exploreP:0.1551\n",
      "Episode:64 meanR:-0.0154 R:2.0 loss:31.5233 exploreP:0.1509\n",
      "Episode:65 meanR:0.0000 R:1.0 loss:28.5913 exploreP:0.1467\n",
      "Episode:66 meanR:0.0149 R:1.0 loss:20.6584 exploreP:0.1426\n",
      "Episode:67 meanR:0.0147 R:0.0 loss:23.3333 exploreP:0.1387\n",
      "Episode:68 meanR:0.0580 R:3.0 loss:9.5230 exploreP:0.1349\n",
      "Episode:69 meanR:0.0571 R:0.0 loss:17.8247 exploreP:0.1312\n",
      "Episode:70 meanR:0.0563 R:0.0 loss:8.9110 exploreP:0.1276\n",
      "Episode:71 meanR:0.0556 R:0.0 loss:6.4498 exploreP:0.1242\n",
      "Episode:72 meanR:0.0411 R:-1.0 loss:6.0653 exploreP:0.1208\n",
      "Episode:73 meanR:0.0541 R:1.0 loss:5.5437 exploreP:0.1175\n",
      "Episode:74 meanR:0.0533 R:0.0 loss:4.3671 exploreP:0.1143\n",
      "Episode:75 meanR:0.0526 R:0.0 loss:2.4258 exploreP:0.1113\n",
      "Episode:76 meanR:0.0779 R:2.0 loss:2.1266 exploreP:0.1083\n",
      "Episode:77 meanR:0.0769 R:0.0 loss:2.0268 exploreP:0.1054\n",
      "Episode:78 meanR:0.0759 R:0.0 loss:1.8938 exploreP:0.1025\n",
      "Episode:79 meanR:0.0625 R:-1.0 loss:1.2906 exploreP:0.0998\n",
      "Episode:80 meanR:0.0370 R:-2.0 loss:1.0511 exploreP:0.0972\n",
      "Episode:81 meanR:0.0366 R:0.0 loss:0.8550 exploreP:0.0946\n",
      "Episode:82 meanR:0.0361 R:0.0 loss:0.9167 exploreP:0.0921\n",
      "Episode:83 meanR:0.0714 R:3.0 loss:0.9905 exploreP:0.0897\n",
      "Episode:84 meanR:0.0941 R:2.0 loss:0.5832 exploreP:0.0873\n",
      "Episode:85 meanR:0.1163 R:2.0 loss:0.6976 exploreP:0.0850\n",
      "Episode:86 meanR:0.1609 R:4.0 loss:0.4062 exploreP:0.0828\n",
      "Episode:87 meanR:0.1705 R:1.0 loss:0.5898 exploreP:0.0806\n",
      "Episode:88 meanR:0.1798 R:1.0 loss:0.4502 exploreP:0.0786\n",
      "Episode:89 meanR:0.1667 R:-1.0 loss:0.7986 exploreP:0.0765\n",
      "Episode:90 meanR:0.1758 R:1.0 loss:0.7050 exploreP:0.0746\n",
      "Episode:91 meanR:0.1739 R:0.0 loss:0.6006 exploreP:0.0727\n",
      "Episode:92 meanR:0.1828 R:1.0 loss:0.5907 exploreP:0.0708\n",
      "Episode:93 meanR:0.1809 R:0.0 loss:0.3504 exploreP:0.0690\n",
      "Episode:94 meanR:0.1789 R:0.0 loss:0.5228 exploreP:0.0673\n",
      "Episode:95 meanR:0.2188 R:4.0 loss:0.3951 exploreP:0.0656\n",
      "Episode:96 meanR:0.2371 R:2.0 loss:0.4160 exploreP:0.0639\n",
      "Episode:97 meanR:0.2449 R:1.0 loss:0.3889 exploreP:0.0623\n",
      "Episode:98 meanR:0.2323 R:-1.0 loss:0.2203 exploreP:0.0608\n",
      "Episode:99 meanR:0.2400 R:1.0 loss:0.2459 exploreP:0.0593\n",
      "Episode:100 meanR:0.2600 R:4.0 loss:0.2619 exploreP:0.0578\n",
      "Episode:101 meanR:0.2600 R:1.0 loss:0.4952 exploreP:0.0564\n",
      "Episode:102 meanR:0.2500 R:0.0 loss:0.2640 exploreP:0.0550\n",
      "Episode:103 meanR:0.2800 R:1.0 loss:0.5180 exploreP:0.0537\n",
      "Episode:104 meanR:0.2700 R:1.0 loss:0.3151 exploreP:0.0524\n",
      "Episode:105 meanR:0.2900 R:2.0 loss:0.2849 exploreP:0.0512\n",
      "Episode:106 meanR:0.2600 R:-2.0 loss:0.3537 exploreP:0.0500\n",
      "Episode:107 meanR:0.2400 R:0.0 loss:0.3992 exploreP:0.0488\n",
      "Episode:108 meanR:0.2600 R:1.0 loss:0.5235 exploreP:0.0476\n",
      "Episode:109 meanR:0.2800 R:2.0 loss:0.6087 exploreP:0.0465\n",
      "Episode:110 meanR:0.2900 R:1.0 loss:0.4684 exploreP:0.0454\n",
      "Episode:111 meanR:0.2500 R:-1.0 loss:0.2409 exploreP:0.0444\n",
      "Episode:112 meanR:0.2600 R:-1.0 loss:0.4984 exploreP:0.0434\n",
      "Episode:113 meanR:0.2800 R:1.0 loss:0.9604 exploreP:0.0424\n",
      "Episode:114 meanR:0.2700 R:-1.0 loss:0.5037 exploreP:0.0414\n",
      "Episode:115 meanR:0.2900 R:1.0 loss:0.2976 exploreP:0.0405\n",
      "Episode:116 meanR:0.3200 R:0.0 loss:0.2849 exploreP:0.0396\n",
      "Episode:117 meanR:0.3300 R:1.0 loss:0.3064 exploreP:0.0387\n",
      "Episode:118 meanR:0.3200 R:1.0 loss:0.2907 exploreP:0.0379\n",
      "Episode:119 meanR:0.3500 R:1.0 loss:1.3310 exploreP:0.0371\n",
      "Episode:120 meanR:0.3500 R:1.0 loss:1.6491 exploreP:0.0363\n",
      "Episode:121 meanR:0.3600 R:-1.0 loss:0.3183 exploreP:0.0355\n",
      "Episode:122 meanR:0.3900 R:2.0 loss:0.1917 exploreP:0.0347\n",
      "Episode:123 meanR:0.3700 R:0.0 loss:0.1717 exploreP:0.0340\n",
      "Episode:124 meanR:0.3700 R:1.0 loss:0.2231 exploreP:0.0333\n",
      "Episode:125 meanR:0.3900 R:2.0 loss:0.3003 exploreP:0.0326\n",
      "Episode:126 meanR:0.4200 R:3.0 loss:1.3787 exploreP:0.0319\n",
      "Episode:127 meanR:0.3900 R:-1.0 loss:0.6503 exploreP:0.0313\n",
      "Episode:128 meanR:0.4300 R:1.0 loss:0.2809 exploreP:0.0306\n",
      "Episode:129 meanR:0.4300 R:-2.0 loss:0.1885 exploreP:0.0300\n",
      "Episode:130 meanR:0.4600 R:0.0 loss:0.3073 exploreP:0.0294\n",
      "Episode:131 meanR:0.4600 R:-2.0 loss:0.3893 exploreP:0.0289\n",
      "Episode:132 meanR:0.4100 R:-3.0 loss:0.2719 exploreP:0.0283\n",
      "Episode:133 meanR:0.4200 R:1.0 loss:0.4469 exploreP:0.0278\n",
      "Episode:134 meanR:0.3900 R:-1.0 loss:1.0098 exploreP:0.0272\n",
      "Episode:135 meanR:0.3900 R:0.0 loss:2.6918 exploreP:0.0267\n",
      "Episode:136 meanR:0.4100 R:2.0 loss:0.6958 exploreP:0.0262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:137 meanR:0.3900 R:-1.0 loss:0.3117 exploreP:0.0258\n",
      "Episode:138 meanR:0.4000 R:1.0 loss:0.5381 exploreP:0.0253\n",
      "Episode:139 meanR:0.3900 R:1.0 loss:0.4454 exploreP:0.0248\n",
      "Episode:140 meanR:0.4200 R:3.0 loss:0.7132 exploreP:0.0244\n",
      "Episode:141 meanR:0.4300 R:0.0 loss:0.2417 exploreP:0.0240\n",
      "Episode:142 meanR:0.4300 R:0.0 loss:0.2694 exploreP:0.0236\n",
      "Episode:143 meanR:0.4200 R:-3.0 loss:0.3638 exploreP:0.0232\n",
      "Episode:144 meanR:0.4600 R:3.0 loss:0.8998 exploreP:0.0228\n",
      "Episode:145 meanR:0.4800 R:0.0 loss:0.9593 exploreP:0.0224\n",
      "Episode:146 meanR:0.4600 R:-2.0 loss:0.6534 exploreP:0.0220\n",
      "Episode:147 meanR:0.4500 R:-1.0 loss:0.2267 exploreP:0.0217\n",
      "Episode:148 meanR:0.4800 R:0.0 loss:0.1228 exploreP:0.0213\n",
      "Episode:149 meanR:0.5000 R:1.0 loss:0.2629 exploreP:0.0210\n",
      "Episode:150 meanR:0.4800 R:-1.0 loss:0.3350 exploreP:0.0207\n",
      "Episode:151 meanR:0.4600 R:-2.0 loss:0.4525 exploreP:0.0204\n",
      "Episode:152 meanR:0.4500 R:0.0 loss:0.2404 exploreP:0.0201\n",
      "Episode:153 meanR:0.4700 R:0.0 loss:0.1788 exploreP:0.0198\n",
      "Episode:154 meanR:0.4700 R:-1.0 loss:0.2085 exploreP:0.0195\n",
      "Episode:155 meanR:0.4600 R:1.0 loss:0.4096 exploreP:0.0192\n",
      "Episode:156 meanR:0.4300 R:-2.0 loss:0.6278 exploreP:0.0189\n",
      "Episode:157 meanR:0.3700 R:-3.0 loss:0.4013 exploreP:0.0187\n",
      "Episode:158 meanR:0.4200 R:1.0 loss:1.1924 exploreP:0.0184\n",
      "Episode:159 meanR:0.4000 R:-2.0 loss:3.7169 exploreP:0.0181\n",
      "Episode:160 meanR:0.3700 R:0.0 loss:0.5717 exploreP:0.0179\n",
      "Episode:161 meanR:0.3700 R:0.0 loss:0.2224 exploreP:0.0177\n",
      "Episode:162 meanR:0.3800 R:1.0 loss:0.1226 exploreP:0.0174\n",
      "Episode:163 meanR:0.3800 R:1.0 loss:0.1651 exploreP:0.0172\n",
      "Episode:164 meanR:0.3500 R:-1.0 loss:0.1073 exploreP:0.0170\n",
      "Episode:165 meanR:0.3400 R:0.0 loss:0.0925 exploreP:0.0168\n",
      "Episode:166 meanR:0.3400 R:1.0 loss:0.1230 exploreP:0.0166\n",
      "Episode:167 meanR:0.3500 R:1.0 loss:0.1464 exploreP:0.0164\n",
      "Episode:168 meanR:0.3200 R:0.0 loss:0.2880 exploreP:0.0162\n",
      "Episode:169 meanR:0.3300 R:1.0 loss:0.6600 exploreP:0.0160\n",
      "Episode:170 meanR:0.3200 R:-1.0 loss:0.4359 exploreP:0.0159\n",
      "Episode:171 meanR:0.3200 R:0.0 loss:0.1603 exploreP:0.0157\n",
      "Episode:172 meanR:0.3000 R:-3.0 loss:0.3459 exploreP:0.0155\n",
      "Episode:173 meanR:0.2800 R:-1.0 loss:0.1271 exploreP:0.0154\n",
      "Episode:174 meanR:0.2700 R:-1.0 loss:0.1295 exploreP:0.0152\n",
      "Episode:175 meanR:0.2900 R:2.0 loss:0.5245 exploreP:0.0150\n",
      "Episode:176 meanR:0.2800 R:1.0 loss:0.1444 exploreP:0.0149\n",
      "Episode:177 meanR:0.2900 R:1.0 loss:0.1546 exploreP:0.0147\n",
      "Episode:178 meanR:0.2800 R:-1.0 loss:0.3779 exploreP:0.0146\n",
      "Episode:179 meanR:0.2700 R:-2.0 loss:0.1552 exploreP:0.0145\n",
      "Episode:180 meanR:0.2800 R:-1.0 loss:0.2524 exploreP:0.0143\n",
      "Episode:181 meanR:0.2800 R:0.0 loss:0.1389 exploreP:0.0142\n",
      "Episode:182 meanR:0.3000 R:2.0 loss:0.3751 exploreP:0.0141\n",
      "Episode:183 meanR:0.2700 R:0.0 loss:1.2294 exploreP:0.0140\n",
      "Episode:184 meanR:0.2600 R:1.0 loss:0.3637 exploreP:0.0138\n",
      "Episode:185 meanR:0.2400 R:0.0 loss:0.1480 exploreP:0.0137\n",
      "Episode:186 meanR:0.2000 R:0.0 loss:0.0846 exploreP:0.0136\n",
      "Episode:187 meanR:0.1900 R:0.0 loss:0.0822 exploreP:0.0135\n",
      "Episode:188 meanR:0.2100 R:3.0 loss:0.1417 exploreP:0.0134\n",
      "Episode:189 meanR:0.2200 R:0.0 loss:0.1950 exploreP:0.0133\n",
      "Episode:190 meanR:0.2300 R:2.0 loss:0.6868 exploreP:0.0132\n",
      "Episode:191 meanR:0.2100 R:-2.0 loss:1.2771 exploreP:0.0131\n",
      "Episode:192 meanR:0.2000 R:0.0 loss:0.1385 exploreP:0.0130\n",
      "Episode:193 meanR:0.2100 R:1.0 loss:0.0899 exploreP:0.0129\n",
      "Episode:194 meanR:0.2200 R:1.0 loss:0.0903 exploreP:0.0129\n",
      "Episode:195 meanR:0.1900 R:1.0 loss:0.0998 exploreP:0.0128\n",
      "Episode:196 meanR:0.2000 R:3.0 loss:0.1161 exploreP:0.0127\n",
      "Episode:197 meanR:0.2200 R:3.0 loss:0.1235 exploreP:0.0126\n",
      "Episode:198 meanR:0.2100 R:-2.0 loss:0.1211 exploreP:0.0125\n",
      "Episode:199 meanR:0.2100 R:1.0 loss:0.2258 exploreP:0.0125\n",
      "Episode:200 meanR:0.1700 R:0.0 loss:0.4255 exploreP:0.0124\n",
      "Episode:201 meanR:0.1300 R:-3.0 loss:0.2127 exploreP:0.0123\n",
      "Episode:202 meanR:0.1400 R:1.0 loss:0.3544 exploreP:0.0122\n",
      "Episode:203 meanR:0.1200 R:-1.0 loss:0.4635 exploreP:0.0122\n",
      "Episode:204 meanR:0.1100 R:0.0 loss:0.2833 exploreP:0.0121\n",
      "Episode:205 meanR:0.0700 R:-2.0 loss:0.1021 exploreP:0.0120\n",
      "Episode:206 meanR:0.1300 R:4.0 loss:0.1009 exploreP:0.0120\n",
      "Episode:207 meanR:0.1100 R:-2.0 loss:0.1755 exploreP:0.0119\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
