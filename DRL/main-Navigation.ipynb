{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([0.        , 0.        , 1.        , 0.        , 0.42380726,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.07645891,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.59556293,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.76522499,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.81929988,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.06445977,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.48016095,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([0.        , 0.        , 1.        , 0.        , 0.42380726,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.07645891,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.59556293,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.76522499,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.81929988,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.06445977,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.48016095,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "1.0 -1.0\n",
      "10.510990142822266 -10.793877601623535\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size) # hidden size\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, \n",
    "                                              cell=cell, initial_state=g_initial_state, lstm_size=hidden_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs_logits, d_final_state = discriminator(states=states, actions=actions_logits, \n",
    "                                             cell=cell, initial_state=d_initial_state, lstm_size=hidden_size)\n",
    "    d_loss = tf.reduce_mean(tf.square(tf.reshape(Qs_logits, [-1]) - targetQs))\n",
    "\n",
    "    return actions_logits, Qs_logits, g_final_state, d_final_state, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(64, 37) actions:(64,)\n",
      "action size:1\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 37                # number of units for the input state/observation -- simulation\n",
    "action_size = 4                # number of units for the output actions -- simulation\n",
    "hidden_size = 37*4             # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 64                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37) (?, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(?, 148)\n",
      "(?, 4)\n",
      "(?, 37) (?, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(?, 148)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(batch_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 rate:0.0000 gloss:-0.0000 dloss:0.0001\n",
      "Episode:1 meanR:0.5000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:2 meanR:0.3333 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:3 meanR:0.2500 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:4 meanR:0.2000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:5 meanR:0.1667 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:6 meanR:0.0000 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:7 meanR:-0.1250 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:8 meanR:-0.2222 rate:-1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:9 meanR:-0.2000 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:10 meanR:-0.0909 rate:1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:11 meanR:-0.1667 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:12 meanR:-0.3846 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:13 meanR:-0.4286 rate:-1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:14 meanR:-0.4667 rate:-1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:15 meanR:-0.2500 rate:3.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:16 meanR:-0.1765 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:17 meanR:-0.1667 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:18 meanR:-0.1053 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:19 meanR:-0.1000 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:20 meanR:-0.0952 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:21 meanR:-0.1364 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:22 meanR:-0.1304 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:23 meanR:-0.1667 rate:-1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:24 meanR:-0.1200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:25 meanR:-0.1154 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:26 meanR:-0.1111 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:27 meanR:-0.1429 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:28 meanR:-0.1379 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:29 meanR:-0.1667 rate:-1.0000 gloss:-0.0000 dloss:0.0001\n",
      "Episode:30 meanR:-0.2258 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:31 meanR:-0.1875 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:32 meanR:-0.1818 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:33 meanR:-0.2059 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:34 meanR:-0.2000 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:35 meanR:-0.1944 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:36 meanR:-0.1892 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:37 meanR:-0.1842 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:38 meanR:-0.2051 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:39 meanR:-0.1750 rate:1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:40 meanR:-0.1707 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:41 meanR:-0.1667 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:42 meanR:-0.1395 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:43 meanR:-0.1364 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:44 meanR:-0.1333 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:45 meanR:-0.1304 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:46 meanR:-0.1277 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:47 meanR:-0.1042 rate:1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:48 meanR:-0.1224 rate:-1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:49 meanR:-0.1000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:50 meanR:-0.0980 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:51 meanR:-0.0962 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:52 meanR:-0.1132 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:53 meanR:-0.0926 rate:1.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:54 meanR:-0.0545 rate:2.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:55 meanR:-0.0536 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:56 meanR:-0.0526 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:57 meanR:-0.0517 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:58 meanR:-0.0508 rate:0.0000 gloss:-0.0000 dloss:0.0000\n",
      "Episode:59 meanR:-0.0333 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:60 meanR:-0.0328 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:61 meanR:-0.0484 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:62 meanR:-0.0635 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:63 meanR:-0.1094 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:64 meanR:-0.0923 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:65 meanR:-0.0909 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:66 meanR:-0.1045 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:67 meanR:-0.1029 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:68 meanR:-0.1159 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:69 meanR:-0.1000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:70 meanR:-0.1127 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:71 meanR:-0.1111 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:72 meanR:-0.1096 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:73 meanR:-0.1351 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:74 meanR:-0.1333 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:75 meanR:-0.1447 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:76 meanR:-0.1299 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:77 meanR:-0.1410 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:78 meanR:-0.1772 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:79 meanR:-0.1500 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:80 meanR:-0.1605 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:81 meanR:-0.1463 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:82 meanR:-0.1446 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:83 meanR:-0.1429 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:84 meanR:-0.1294 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:85 meanR:-0.1279 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:86 meanR:-0.1264 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:87 meanR:-0.1250 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:88 meanR:-0.1236 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:89 meanR:-0.1444 rate:-2.0000 gloss:0.0000 dloss:0.0003\n",
      "Episode:90 meanR:-0.1429 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:91 meanR:-0.1304 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:92 meanR:-0.1183 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:93 meanR:-0.1064 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:94 meanR:-0.1053 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:95 meanR:-0.1042 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:96 meanR:-0.1134 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:97 meanR:-0.1224 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:98 meanR:-0.1212 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:99 meanR:-0.1300 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:100 meanR:-0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:101 meanR:-0.1200 rate:2.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:102 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:103 meanR:-0.1100 rate:1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:104 meanR:-0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:105 meanR:-0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:106 meanR:-0.0900 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:107 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:108 meanR:-0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:109 meanR:-0.0600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:110 meanR:-0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:111 meanR:-0.0400 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:112 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:113 meanR:-0.0100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:114 meanR:-0.0200 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:115 meanR:-0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:116 meanR:-0.0800 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:117 meanR:-0.1000 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:118 meanR:-0.1200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:119 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:120 meanR:-0.1300 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:121 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:122 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:123 meanR:-0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:124 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:125 meanR:-0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:126 meanR:-0.1000 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:127 meanR:-0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:128 meanR:-0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:129 meanR:-0.0700 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:130 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:131 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:132 meanR:-0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:133 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:134 meanR:-0.0300 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:135 meanR:-0.0200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:136 meanR:0.0000 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:137 meanR:0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:138 meanR:0.0300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:139 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:140 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:141 meanR:0.0300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:142 meanR:0.0000 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:143 meanR:-0.0200 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:144 meanR:-0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:145 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:146 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:147 meanR:-0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:148 meanR:0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:149 meanR:0.0200 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:150 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:151 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:152 meanR:0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:153 meanR:0.0300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:154 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:155 meanR:0.0200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:156 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:157 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:158 meanR:0.0300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:159 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:160 meanR:0.0100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:161 meanR:0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:162 meanR:0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:163 meanR:0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:164 meanR:0.0600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:165 meanR:0.0500 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:166 meanR:0.0700 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:167 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:168 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:169 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:170 meanR:0.0900 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:171 meanR:0.0800 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:172 meanR:0.0700 rate:-1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:173 meanR:0.1100 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:174 meanR:0.1200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:175 meanR:0.1400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:176 meanR:0.1500 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:177 meanR:0.1700 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:178 meanR:0.2000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:179 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:180 meanR:0.1900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:181 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:182 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:183 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:184 meanR:0.1800 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:185 meanR:0.1900 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:186 meanR:0.1900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:187 meanR:0.1900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:188 meanR:0.2100 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:189 meanR:0.2200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:190 meanR:0.2200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:191 meanR:0.2100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:192 meanR:0.1800 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:193 meanR:0.1700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:194 meanR:0.1700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:195 meanR:0.1700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:196 meanR:0.1700 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:197 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:198 meanR:0.1600 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:199 meanR:0.1900 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:200 meanR:0.2000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:201 meanR:0.1800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:202 meanR:0.2000 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:203 meanR:0.1900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:204 meanR:0.1700 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:205 meanR:0.1600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:206 meanR:0.1600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:207 meanR:0.1600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:208 meanR:0.1600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:209 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:210 meanR:0.1400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:211 meanR:0.1500 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:212 meanR:0.1400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:213 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:214 meanR:0.1900 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:215 meanR:0.2000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:216 meanR:0.2100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:217 meanR:0.2300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:218 meanR:0.2300 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:219 meanR:0.2300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:220 meanR:0.2700 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:221 meanR:0.2700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:222 meanR:0.2800 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:223 meanR:0.2800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:224 meanR:0.2700 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:225 meanR:0.2800 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:226 meanR:0.2600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:227 meanR:0.2600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:228 meanR:0.2300 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:229 meanR:0.2200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:230 meanR:0.2200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:231 meanR:0.2100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:232 meanR:0.2100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:233 meanR:0.2000 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:234 meanR:0.1600 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:235 meanR:0.1400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:236 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:237 meanR:0.1200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:238 meanR:0.1200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:239 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:240 meanR:0.1300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:241 meanR:0.1000 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:242 meanR:0.1500 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:243 meanR:0.1700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:244 meanR:0.1600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:245 meanR:0.1500 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:246 meanR:0.1300 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:247 meanR:0.1000 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:248 meanR:0.1100 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:249 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:250 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:251 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:252 meanR:0.0900 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:253 meanR:0.0800 rate:-1.0000 gloss:0.8178 dloss:0.0787\n",
      "Episode:254 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0027\n",
      "Episode:255 meanR:0.0800 rate:1.0000 gloss:0.0000 dloss:0.0015\n",
      "Episode:256 meanR:0.0800 rate:0.0000 gloss:-0.0000 dloss:0.0019\n",
      "Episode:257 meanR:0.0900 rate:1.0000 gloss:0.0000 dloss:0.0010\n",
      "Episode:258 meanR:0.1000 rate:2.0000 gloss:0.0000 dloss:0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:259 meanR:0.1100 rate:1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:260 meanR:0.1000 rate:-2.0000 gloss:0.0000 dloss:0.0012\n",
      "Episode:261 meanR:0.1100 rate:1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:262 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:263 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:264 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:265 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:266 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:267 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:268 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:269 meanR:0.0900 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:270 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:271 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:272 meanR:0.0800 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:273 meanR:0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:274 meanR:0.0400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:275 meanR:0.0500 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:276 meanR:0.0200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:277 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:278 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:279 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:280 meanR:0.0000 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:281 meanR:0.0000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:282 meanR:-0.0200 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:283 meanR:0.0100 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:284 meanR:-0.0100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:285 meanR:-0.0300 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:286 meanR:-0.0400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:287 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:288 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:289 meanR:-0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:290 meanR:-0.0500 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:291 meanR:-0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:292 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:293 meanR:-0.0600 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:294 meanR:-0.0900 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:295 meanR:-0.0700 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:296 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:297 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:298 meanR:-0.0400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:299 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:300 meanR:-0.0700 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:301 meanR:-0.0600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:302 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:303 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:304 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:305 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:306 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:307 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:308 meanR:-0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:309 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:310 meanR:-0.0300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:311 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:312 meanR:-0.0300 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:313 meanR:-0.0300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:314 meanR:-0.0700 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:315 meanR:-0.0800 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:316 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:317 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:318 meanR:-0.0600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:319 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:320 meanR:-0.0900 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:321 meanR:-0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:322 meanR:-0.0900 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:323 meanR:-0.0600 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:324 meanR:-0.0800 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:325 meanR:-0.0600 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:326 meanR:-0.0800 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:327 meanR:-0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:328 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:329 meanR:-0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:330 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:331 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:332 meanR:-0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:333 meanR:-0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:334 meanR:-0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:335 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:336 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:337 meanR:-0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:338 meanR:-0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:339 meanR:0.0000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:340 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:341 meanR:0.0200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:342 meanR:-0.0200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:343 meanR:-0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:344 meanR:-0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:345 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:346 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:347 meanR:0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:348 meanR:0.0100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:349 meanR:0.0200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:350 meanR:-0.0100 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:351 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:352 meanR:-0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:353 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:354 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:355 meanR:-0.0200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:356 meanR:-0.0100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:357 meanR:-0.0400 rate:-2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:358 meanR:-0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:359 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:360 meanR:-0.0300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:361 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:362 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:363 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:364 meanR:-0.0500 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:365 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:366 meanR:-0.0400 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:367 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:368 meanR:-0.0500 rate:-1.0000 gloss:-1.8999 dloss:0.5672\n",
      "Episode:369 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0035\n",
      "Episode:370 meanR:-0.0500 rate:-1.0000 gloss:0.0000 dloss:0.0058\n",
      "Episode:371 meanR:-0.0500 rate:0.0000 gloss:-0.0001 dloss:0.0701\n",
      "Episode:372 meanR:-0.0400 rate:-1.0000 gloss:-0.0000 dloss:0.0008\n",
      "Episode:373 meanR:-0.0500 rate:-1.0000 gloss:0.0000 dloss:0.0028\n",
      "Episode:374 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0004\n",
      "Episode:375 meanR:-0.0600 rate:0.0000 gloss:0.0000 dloss:0.0002\n",
      "Episode:376 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:377 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:378 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:379 meanR:-0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:380 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:381 meanR:-0.0300 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:382 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:383 meanR:-0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:384 meanR:-0.0400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:385 meanR:-0.0200 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:386 meanR:0.0000 rate:1.0000 gloss:0.0000 dloss:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:387 meanR:-0.0100 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:388 meanR:0.0000 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:389 meanR:-0.0200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:390 meanR:-0.0200 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:391 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0004\n",
      "Episode:392 meanR:-0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:393 meanR:0.0100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:394 meanR:0.0400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:395 meanR:0.0400 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:396 meanR:0.0300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:397 meanR:0.0600 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:398 meanR:0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:399 meanR:0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:400 meanR:0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:401 meanR:0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:402 meanR:0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:403 meanR:0.0600 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:404 meanR:0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:405 meanR:0.0700 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:406 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:407 meanR:0.0800 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:408 meanR:0.0800 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:409 meanR:0.0700 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:410 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:411 meanR:0.0600 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:412 meanR:0.0500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:413 meanR:0.0400 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:414 meanR:0.0900 rate:3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:415 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:416 meanR:0.1200 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:417 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:418 meanR:0.1300 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:419 meanR:0.0900 rate:-3.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:420 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:421 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:422 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:423 meanR:0.0600 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:424 meanR:0.0800 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:425 meanR:0.0500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:426 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:427 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:428 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:429 meanR:0.1000 rate:2.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:430 meanR:0.1100 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:431 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:432 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:433 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0008\n",
      "Episode:434 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:435 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:436 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:437 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:438 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:439 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:440 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:441 meanR:0.0700 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:442 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:443 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:444 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:445 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:446 meanR:0.0900 rate:1.0000 gloss:0.0000 dloss:0.0002\n",
      "Episode:447 meanR:0.0800 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:448 meanR:0.1000 rate:1.0000 gloss:0.0000 dloss:0.0001\n",
      "Episode:449 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:450 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:451 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:452 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:453 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:454 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:455 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:456 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:457 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:458 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:459 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:460 meanR:0.1000 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:461 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:462 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:463 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:464 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:465 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:466 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:467 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:468 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:469 meanR:0.1100 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:470 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:471 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:472 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:473 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:474 meanR:0.1500 rate:1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:475 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:476 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:477 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:478 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:479 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:480 meanR:0.1500 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:481 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:482 meanR:0.1300 rate:-1.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:483 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:484 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:485 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:486 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:487 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:488 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:489 meanR:0.1300 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:490 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:491 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:492 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:493 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:494 meanR:0.1400 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:495 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:496 meanR:0.1200 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:497 meanR:0.0900 rate:0.0000 gloss:0.0000 dloss:0.0000\n",
      "Episode:498 meanR:0.1000 rate:0.0000 gloss:0.0000 dloss:0.0000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # env first state\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            g_loss, d_loss, _, _ = sess.run(fetches=[model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs,\n",
    "                                    model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                                    model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        # Episode total reward and success rate/prob\n",
    "        episode_reward.append(total_reward) # stopping criteria\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'rate:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import gym\n",
    "# # # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('CartPole-v1')\n",
    "# # # env = gym.make('Acrobot-v1')\n",
    "# # # env = gym.make('MountainCar-v0')\n",
    "# # # env = gym.make('Pendulum-v0')\n",
    "# # # env = gym.make('Blackjack-v0')\n",
    "# # # env = gym.make('FrozenLake-v0')\n",
    "# # # env = gym.make('AirRaid-ram-v0')\n",
    "# # # env = gym.make('AirRaid-v0')\n",
    "# # # env = gym.make('BipedalWalker-v2')\n",
    "# # # env = gym.make('Copy-v0')\n",
    "# # # env = gym.make('CarRacing-v0')\n",
    "# # # env = gym.make('Ant-v2') #mujoco\n",
    "# # # env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     #sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "#     #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "#     # Episodes/epochs\n",
    "#     for _ in range(1):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "\n",
    "#         # Steps/batches\n",
    "#         #for _ in range(111111111111111111):\n",
    "#         while True:\n",
    "#             env.render()\n",
    "#             action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "#             action = np.argmax(action_logits)\n",
    "#             state, reward, done, _ = env.step(action)\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#         # Closing the env\n",
    "#         print('total_reward: {:.2f}'.format(total_reward))\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
