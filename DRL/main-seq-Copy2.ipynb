{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.208870795028335 -2.5781655146810603\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 0.7310585786300049)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size) # hidden size\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Generator/Controller: Generating/prediting the actions\n",
    "# def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('generator', reuse=reuse):\n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return actions logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "# def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('discriminator', reuse=reuse):\n",
    "#         # Fusion/merge states and actions/ SA/ SM\n",
    "#         x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return rewards logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, \n",
    "                                              cell=cell, initial_state=g_initial_state, lstm_size=hidden_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs_logits, d_final_state = discriminator(states=states, actions=actions_logits, \n",
    "                                             cell=cell, initial_state=d_initial_state, lstm_size=hidden_size)\n",
    "    d_lossQ = tf.reduce_mean(tf.square(tf.reshape(Qs_logits, [-1]) - targetQs))\n",
    "    d_lossQ_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Qs_logits, [-1]),\n",
    "                                                                          labels=tf.nn.sigmoid(targetQs)))\n",
    "    d_loss = d_lossQ_sigm + d_lossQ\n",
    "\n",
    "    return actions_logits, Qs_logits, g_final_state, d_final_state, g_loss, d_loss, d_lossQ, d_lossQ_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss, self.d_lossQ, self.d_lossQ_sigm = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1111, 4) actions:(1111,)\n",
      "action size:2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 32                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n",
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01069676, -0.00322705,  0.03619071, -0.03418689]),\n",
       " 0,\n",
       " array([-0.0107613 , -0.19884879,  0.03550697,  0.26969134]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:48.0000 rate:0.0960 gloss:0.5152 dloss:1.4261 dlossQ:0.9713 dlossQsigm:0.4548\n",
      "Episode:1 meanR:54.0000 rate:0.1200 gloss:0.1059 dloss:3.5117 dlossQ:3.4335 dlossQsigm:0.0782\n",
      "Episode:2 meanR:59.6667 rate:0.1420 gloss:0.0066 dloss:8.2521 dlossQ:8.1162 dlossQsigm:0.1359\n",
      "Episode:3 meanR:60.7500 rate:0.1280 gloss:0.0627 dloss:9.2201 dlossQ:9.0784 dlossQsigm:0.1416\n",
      "Episode:4 meanR:56.0000 rate:0.0740 gloss:0.2940 dloss:3.9147 dlossQ:3.8604 dlossQsigm:0.0543\n",
      "Episode:5 meanR:62.0000 rate:0.1840 gloss:0.0487 dloss:18.0800 dlossQ:17.8812 dlossQsigm:0.1988\n",
      "Episode:6 meanR:60.4286 rate:0.1020 gloss:0.1382 dloss:9.5126 dlossQ:9.3767 dlossQsigm:0.1359\n",
      "Episode:7 meanR:59.5000 rate:0.1060 gloss:0.1514 dloss:5.9219 dlossQ:5.7190 dlossQsigm:0.2029\n",
      "Episode:8 meanR:57.3333 rate:0.0800 gloss:-14.5985 dloss:17.7668 dlossQ:17.2592 dlossQsigm:0.5076\n",
      "Episode:9 meanR:57.4000 rate:0.1160 gloss:0.0268 dloss:11.9522 dlossQ:11.5416 dlossQsigm:0.4106\n",
      "Episode:10 meanR:71.7273 rate:0.4300 gloss:0.0039 dloss:47.6382 dlossQ:47.3087 dlossQsigm:0.3295\n",
      "Episode:11 meanR:70.5833 rate:0.1160 gloss:0.0640 dloss:15.7309 dlossQ:15.3773 dlossQsigm:0.3536\n",
      "Episode:12 meanR:69.3077 rate:0.1080 gloss:0.1098 dloss:73.7408 dlossQ:72.7988 dlossQsigm:0.9420\n",
      "Episode:13 meanR:69.0714 rate:0.1320 gloss:0.1113 dloss:52.3481 dlossQ:52.0015 dlossQsigm:0.3467\n",
      "Episode:14 meanR:70.3333 rate:0.1760 gloss:0.0120 dloss:11.7836 dlossQ:11.2108 dlossQsigm:0.5729\n",
      "Episode:15 meanR:69.9375 rate:0.1280 gloss:0.0369 dloss:53.1389 dlossQ:52.7949 dlossQsigm:0.3440\n",
      "Episode:16 meanR:68.2353 rate:0.0820 gloss:-1.1005 dloss:17.1769 dlossQ:16.7815 dlossQsigm:0.3955\n",
      "Episode:17 meanR:69.1111 rate:0.1680 gloss:0.1171 dloss:49.4445 dlossQ:49.1094 dlossQsigm:0.3352\n",
      "Episode:18 meanR:69.5789 rate:0.1560 gloss:-0.1745 dloss:22.8685 dlossQ:22.1556 dlossQsigm:0.7129\n",
      "Episode:19 meanR:68.6000 rate:0.1000 gloss:-37.7021 dloss:94.5953 dlossQ:92.3605 dlossQsigm:2.2348\n",
      "Episode:20 meanR:67.7619 rate:0.1020 gloss:0.0500 dloss:9.8628 dlossQ:9.7653 dlossQsigm:0.0975\n",
      "Episode:21 meanR:67.3182 rate:0.1160 gloss:-0.0238 dloss:8.6620 dlossQ:8.3088 dlossQsigm:0.3532\n",
      "Episode:22 meanR:66.8261 rate:0.1120 gloss:0.0120 dloss:16.1033 dlossQ:15.7463 dlossQsigm:0.3570\n",
      "Episode:23 meanR:66.6250 rate:0.1240 gloss:0.2294 dloss:15.0166 dlossQ:14.5983 dlossQsigm:0.4183\n",
      "Episode:24 meanR:65.5600 rate:0.0800 gloss:0.0444 dloss:12.3634 dlossQ:12.0544 dlossQsigm:0.3090\n",
      "Episode:25 meanR:65.1923 rate:0.1120 gloss:0.0468 dloss:13.2457 dlossQ:12.7040 dlossQsigm:0.5417\n",
      "Episode:26 meanR:64.7407 rate:0.1060 gloss:-44.2712 dloss:5.3796 dlossQ:4.9944 dlossQsigm:0.3852\n",
      "Episode:27 meanR:68.3214 rate:0.3300 gloss:0.3380 dloss:38.2311 dlossQ:37.9358 dlossQsigm:0.2953\n",
      "Episode:28 meanR:68.7931 rate:0.1640 gloss:-3.4035 dloss:5.5626 dlossQ:4.8535 dlossQsigm:0.7090\n",
      "Episode:29 meanR:82.0667 rate:0.9340 gloss:0.2232 dloss:48.9962 dlossQ:48.6605 dlossQsigm:0.3357\n",
      "Episode:30 meanR:84.1290 rate:0.2920 gloss:0.4041 dloss:8.1097 dlossQ:8.0121 dlossQsigm:0.0976\n",
      "Episode:31 meanR:88.0312 rate:0.4180 gloss:0.0159 dloss:52.9081 dlossQ:52.5923 dlossQsigm:0.3158\n",
      "Episode:32 meanR:88.8485 rate:0.2300 gloss:0.1077 dloss:10.4209 dlossQ:10.2706 dlossQsigm:0.1502\n",
      "Episode:33 meanR:89.7353 rate:0.2380 gloss:0.0538 dloss:13.1383 dlossQ:12.9743 dlossQsigm:0.1640\n",
      "Episode:34 meanR:101.1143 rate:0.9760 gloss:0.1341 dloss:46.0797 dlossQ:45.7558 dlossQsigm:0.3239\n",
      "Episode:35 meanR:102.1389 rate:0.2760 gloss:0.0088 dloss:4.2251 dlossQ:4.1633 dlossQsigm:0.0618\n",
      "Episode:36 meanR:106.9730 rate:0.5620 gloss:0.5341 dloss:8.8160 dlossQ:8.6941 dlossQsigm:0.1218\n",
      "Episode:37 meanR:107.0526 rate:0.2200 gloss:1.6446 dloss:1.6181 dlossQ:1.5697 dlossQsigm:0.0485\n",
      "Episode:38 meanR:107.7436 rate:0.2680 gloss:0.4402 dloss:66.2720 dlossQ:65.8917 dlossQsigm:0.3802\n",
      "Episode:39 meanR:117.2250 rate:0.9740 gloss:0.4977 dloss:61.7099 dlossQ:61.3348 dlossQsigm:0.3751\n",
      "Episode:40 meanR:118.0976 rate:0.3060 gloss:0.0356 dloss:60.8288 dlossQ:60.4550 dlossQsigm:0.3738\n",
      "Episode:41 meanR:119.0000 rate:0.3120 gloss:0.0695 dloss:37.8357 dlossQ:37.5370 dlossQsigm:0.2987\n",
      "Episode:42 meanR:119.7442 rate:0.3020 gloss:0.3496 dloss:11.0146 dlossQ:10.8856 dlossQsigm:0.1290\n",
      "Episode:43 meanR:121.0682 rate:0.3560 gloss:0.2643 dloss:39.4115 dlossQ:39.1071 dlossQsigm:0.3044\n",
      "Episode:44 meanR:124.8889 rate:0.5860 gloss:0.0511 dloss:44.2958 dlossQ:43.9764 dlossQsigm:0.3194\n",
      "Episode:45 meanR:124.5217 rate:0.2160 gloss:0.1870 dloss:6.5273 dlossQ:6.4833 dlossQsigm:0.0439\n",
      "Episode:46 meanR:125.2979 rate:0.3220 gloss:3.9017 dloss:14.1092 dlossQ:13.9304 dlossQsigm:0.1787\n",
      "Episode:47 meanR:126.2292 rate:0.3400 gloss:2.0070 dloss:32.5569 dlossQ:32.3013 dlossQsigm:0.2556\n",
      "Episode:48 meanR:129.0000 rate:0.5240 gloss:0.4119 dloss:49.5292 dlossQ:49.1966 dlossQsigm:0.3326\n",
      "Episode:49 meanR:131.1400 rate:0.4720 gloss:1.4000 dloss:27.6792 dlossQ:27.4289 dlossQsigm:0.2503\n",
      "Episode:50 meanR:131.7451 rate:0.3240 gloss:0.0704 dloss:32.6052 dlossQ:32.3492 dlossQsigm:0.2560\n",
      "Episode:51 meanR:132.0962 rate:0.3000 gloss:0.2360 dloss:20.9637 dlossQ:20.7462 dlossQsigm:0.2176\n",
      "Episode:52 meanR:133.0189 rate:0.3620 gloss:0.1843 dloss:16.0396 dlossQ:15.8487 dlossQsigm:0.1909\n",
      "Episode:53 meanR:133.3148 rate:0.2980 gloss:0.0127 dloss:11.4789 dlossQ:11.3228 dlossQsigm:0.1561\n",
      "Episode:54 meanR:133.4545 rate:0.2820 gloss:0.0534 dloss:6.5903 dlossQ:6.4872 dlossQsigm:0.1031\n",
      "Episode:55 meanR:133.3571 rate:0.2560 gloss:0.0406 dloss:3.4211 dlossQ:3.3860 dlossQsigm:0.0351\n",
      "Episode:56 meanR:132.6667 rate:0.1880 gloss:0.0376 dloss:7.6739 dlossQ:7.5743 dlossQsigm:0.0996\n",
      "Episode:57 meanR:132.5517 rate:0.2520 gloss:0.0192 dloss:3.2053 dlossQ:3.0945 dlossQsigm:0.1108\n",
      "Episode:58 meanR:132.1356 rate:0.2160 gloss:0.0326 dloss:4.4805 dlossQ:4.4409 dlossQsigm:0.0396\n",
      "Episode:59 meanR:131.3833 rate:0.1740 gloss:0.0401 dloss:4.7695 dlossQ:4.6754 dlossQsigm:0.0941\n",
      "Episode:60 meanR:130.8525 rate:0.1980 gloss:0.0091 dloss:2.6078 dlossQ:2.5677 dlossQsigm:0.0401\n",
      "Episode:61 meanR:131.1452 rate:0.2980 gloss:0.0159 dloss:3.7294 dlossQ:3.6640 dlossQsigm:0.0654\n",
      "Episode:62 meanR:130.3175 rate:0.1580 gloss:0.0254 dloss:4.0574 dlossQ:3.9950 dlossQsigm:0.0624\n",
      "Episode:63 meanR:129.6875 rate:0.1800 gloss:0.0084 dloss:4.0601 dlossQ:4.0001 dlossQsigm:0.0599\n",
      "Episode:64 meanR:129.6769 rate:0.2580 gloss:0.0121 dloss:5.8820 dlossQ:5.7905 dlossQsigm:0.0915\n",
      "Episode:65 meanR:129.9394 rate:0.2940 gloss:0.0447 dloss:9.9630 dlossQ:9.8219 dlossQsigm:0.1411\n",
      "Episode:66 meanR:129.8955 rate:0.2540 gloss:0.0720 dloss:27.5306 dlossQ:27.2868 dlossQsigm:0.2438\n",
      "Episode:67 meanR:130.5882 rate:0.3540 gloss:0.0968 dloss:13.9122 dlossQ:13.7390 dlossQsigm:0.1733\n",
      "Episode:68 meanR:131.0435 rate:0.3240 gloss:0.5668 dloss:15.4796 dlossQ:15.3033 dlossQsigm:0.1763\n",
      "Episode:69 meanR:132.8714 rate:0.5180 gloss:0.0334 dloss:72.3839 dlossQ:72.0216 dlossQsigm:0.3623\n",
      "Episode:70 meanR:134.0563 rate:0.4340 gloss:0.0016 dloss:64.8821 dlossQ:64.4986 dlossQsigm:0.3835\n",
      "Episode:71 meanR:134.9722 rate:0.4000 gloss:0.0267 dloss:17.8711 dlossQ:17.6982 dlossQsigm:0.1729\n",
      "Episode:72 meanR:134.9178 rate:0.2620 gloss:0.0733 dloss:19.9786 dlossQ:19.7988 dlossQsigm:0.1798\n",
      "Episode:73 meanR:135.5541 rate:0.3640 gloss:0.0991 dloss:21.2710 dlossQ:21.0683 dlossQsigm:0.2027\n",
      "Episode:74 meanR:136.1200 rate:0.3560 gloss:0.0290 dloss:6.0033 dlossQ:5.9622 dlossQsigm:0.0411\n",
      "Episode:75 meanR:137.6974 rate:0.5120 gloss:0.0399 dloss:124.8375 dlossQ:124.3308 dlossQsigm:0.5068\n",
      "Episode:76 meanR:139.4156 rate:0.5400 gloss:0.1091 dloss:22.0246 dlossQ:21.8188 dlossQsigm:0.2058\n",
      "Episode:77 meanR:140.9231 rate:0.5140 gloss:0.1192 dloss:8.5705 dlossQ:8.4611 dlossQsigm:0.1093\n",
      "Episode:78 meanR:141.7722 rate:0.4160 gloss:0.0961 dloss:8.2773 dlossQ:8.1541 dlossQsigm:0.1232\n",
      "Episode:79 meanR:142.5125 rate:0.4020 gloss:0.2331 dloss:8.4647 dlossQ:8.3748 dlossQsigm:0.0899\n",
      "Episode:80 meanR:143.0123 rate:0.3660 gloss:0.0143 dloss:5.1819 dlossQ:5.0967 dlossQsigm:0.0851\n",
      "Episode:81 meanR:143.2317 rate:0.3220 gloss:0.0461 dloss:2.9928 dlossQ:2.9501 dlossQsigm:0.0427\n",
      "Episode:82 meanR:143.6747 rate:0.3600 gloss:0.0104 dloss:22.9800 dlossQ:22.7701 dlossQsigm:0.2099\n",
      "Episode:83 meanR:144.1667 rate:0.3700 gloss:0.0112 dloss:59.9324 dlossQ:59.5748 dlossQsigm:0.3576\n",
      "Episode:84 meanR:147.9529 rate:0.9320 gloss:0.1179 dloss:95.2329 dlossQ:94.7694 dlossQsigm:0.4635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:85 meanR:148.3256 rate:0.3600 gloss:0.0047 dloss:38.6580 dlossQ:38.3657 dlossQsigm:0.2922\n",
      "Episode:86 meanR:152.3678 rate:1.0000 gloss:1.2108 dloss:256.4392 dlossQ:255.6707 dlossQsigm:0.7685\n",
      "Episode:87 meanR:156.3182 rate:1.0000 gloss:0.4264 dloss:252.4097 dlossQ:251.6474 dlossQsigm:0.7624\n",
      "Episode:88 meanR:157.3708 rate:0.5000 gloss:1.0918 dloss:138.2171 dlossQ:137.6527 dlossQsigm:0.5644\n",
      "Episode:89 meanR:161.1778 rate:1.0000 gloss:0.0297 dloss:254.4239 dlossQ:253.6584 dlossQsigm:0.7654\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "rates_list, d_lossQ_list, d_lossQsigm_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        state = env.reset() # env first state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            g_loss, d_loss, d_lossQ, d_lossQsigm, _, _ = sess.run(\n",
    "                fetches=[model.g_loss, model.d_loss, \n",
    "                         model.d_lossQ, model.d_lossQ_sigm,\n",
    "                         model.g_opt, model.d_opt], \n",
    "                feed_dict = {model.states: states, model.actions: actions,\n",
    "                             model.targetQs: targetQs,\n",
    "                             model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                             model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        # Episode total reward and success rate/prob\n",
    "        episode_reward.append(total_reward) # stopping criteria\n",
    "        rate = total_reward/ 500 # success is 500 points: 0-1\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss),\n",
    "              'dlossQ:{:.4f}'.format(d_lossQ),\n",
    "              'dlossQsigm:{:.4f}'.format(d_lossQsigm))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rates_list.append([ep, rate])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        d_lossQ_list.append([ep, d_lossQ])\n",
    "        d_lossQsigm_list.append([ep, d_lossQsigm])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossR_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(100):\n",
    "    #while True:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        #for _ in range(111111111111111111):\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Print and break condition\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "        # if total_reward == 500:\n",
    "        #     break\n",
    "                \n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
