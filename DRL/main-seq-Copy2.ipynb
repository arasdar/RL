{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.7905941157111283 -2.711930050330779\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 0.7310585786300049)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size) # hidden size\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Generator/Controller: Generating/prediting the actions\n",
    "# def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('generator', reuse=reuse):\n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return actions logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "# def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('discriminator', reuse=reuse):\n",
    "#         # Fusion/merge states and actions/ SA/ SM\n",
    "#         x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return rewards logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, \n",
    "                                              cell=cell, initial_state=g_initial_state, lstm_size=hidden_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs_logits, d_final_state = discriminator(states=states, actions=actions_logits, \n",
    "                                             cell=cell, initial_state=d_initial_state, lstm_size=hidden_size)\n",
    "    d_lossQ = tf.reduce_mean(tf.square(tf.reshape(Qs_logits, [-1]) - targetQs))\n",
    "    d_lossQ_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Qs_logits, [-1]),\n",
    "                                                                          labels=tf.nn.sigmoid(targetQs)))\n",
    "    d_loss = d_lossQ_sigm + d_lossQ\n",
    "\n",
    "    return actions_logits, Qs_logits, g_final_state, d_final_state, g_loss, d_loss, d_lossQ, d_lossQ_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss, self.d_lossQ, self.d_lossQ_sigm = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1111, 4) actions:(1111,)\n",
      "action size:2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 64                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n",
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.04807594,  0.02345137,  0.02045159,  0.03768093]),\n",
       " 0,\n",
       " array([-0.04760692, -0.1719578 ,  0.0212052 ,  0.33674573]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:80.0000 rate:0.1600 gloss:-3.2833 dloss:2.2360 dlossQ:1.5143 dlossQsigm:0.7217\n",
      "Episode:1 meanR:64.0000 rate:0.0960 gloss:1.9968 dloss:1.2606 dlossQ:0.9850 dlossQsigm:0.2756\n",
      "Episode:2 meanR:53.3333 rate:0.0640 gloss:1.7915 dloss:1.1929 dlossQ:1.0990 dlossQsigm:0.0938\n",
      "Episode:3 meanR:50.0000 rate:0.0800 gloss:1.0570 dloss:1.8423 dlossQ:1.7622 dlossQsigm:0.0802\n",
      "Episode:4 meanR:47.4000 rate:0.0740 gloss:0.3914 dloss:2.0815 dlossQ:2.0090 dlossQsigm:0.0725\n",
      "Episode:5 meanR:46.1667 rate:0.0800 gloss:0.4965 dloss:2.8467 dlossQ:2.7713 dlossQsigm:0.0754\n",
      "Episode:6 meanR:43.8571 rate:0.0600 gloss:0.6110 dloss:3.7688 dlossQ:3.6806 dlossQsigm:0.0881\n",
      "Episode:7 meanR:42.6250 rate:0.0680 gloss:0.8966 dloss:4.2057 dlossQ:4.0988 dlossQsigm:0.1069\n",
      "Episode:8 meanR:43.0000 rate:0.0920 gloss:3.2130 dloss:4.9532 dlossQ:4.8503 dlossQsigm:0.1029\n",
      "Episode:9 meanR:43.0000 rate:0.0860 gloss:0.4772 dloss:5.9415 dlossQ:5.8268 dlossQsigm:0.1147\n",
      "Episode:10 meanR:41.8182 rate:0.0600 gloss:0.7667 dloss:5.6821 dlossQ:5.5709 dlossQsigm:0.1112\n",
      "Episode:11 meanR:42.0000 rate:0.0880 gloss:0.5511 dloss:6.1364 dlossQ:6.0215 dlossQsigm:0.1149\n",
      "Episode:12 meanR:41.5385 rate:0.0720 gloss:0.2154 dloss:5.7965 dlossQ:5.6863 dlossQsigm:0.1103\n",
      "Episode:13 meanR:41.1429 rate:0.0720 gloss:0.6114 dloss:7.3158 dlossQ:7.1944 dlossQsigm:0.1214\n",
      "Episode:14 meanR:40.6667 rate:0.0680 gloss:0.4501 dloss:10.2502 dlossQ:10.0989 dlossQsigm:0.1514\n",
      "Episode:15 meanR:40.3750 rate:0.0720 gloss:0.4485 dloss:10.2384 dlossQ:10.0864 dlossQsigm:0.1520\n",
      "Episode:16 meanR:40.5294 rate:0.0860 gloss:0.2438 dloss:10.9443 dlossQ:10.7860 dlossQsigm:0.1583\n",
      "Episode:17 meanR:40.8333 rate:0.0920 gloss:0.3209 dloss:7.0678 dlossQ:6.9447 dlossQsigm:0.1231\n",
      "Episode:18 meanR:40.7895 rate:0.0800 gloss:0.0886 dloss:9.0559 dlossQ:8.9210 dlossQsigm:0.1348\n",
      "Episode:19 meanR:40.4000 rate:0.0660 gloss:0.1147 dloss:6.8680 dlossQ:6.7495 dlossQsigm:0.1185\n",
      "Episode:20 meanR:40.0952 rate:0.0680 gloss:0.1060 dloss:6.3724 dlossQ:6.2287 dlossQsigm:0.1436\n",
      "Episode:21 meanR:39.8182 rate:0.0680 gloss:0.2308 dloss:6.3533 dlossQ:6.2674 dlossQsigm:0.0859\n",
      "Episode:22 meanR:40.2174 rate:0.0980 gloss:0.0933 dloss:5.3322 dlossQ:5.2599 dlossQsigm:0.0722\n",
      "Episode:23 meanR:39.7500 rate:0.0580 gloss:0.1762 dloss:3.6452 dlossQ:3.5715 dlossQsigm:0.0737\n",
      "Episode:24 meanR:39.9600 rate:0.0900 gloss:0.4178 dloss:8.2640 dlossQ:8.1473 dlossQsigm:0.1167\n",
      "Episode:25 meanR:39.7308 rate:0.0680 gloss:0.3385 dloss:7.6000 dlossQ:7.4792 dlossQsigm:0.1209\n",
      "Episode:26 meanR:39.7037 rate:0.0780 gloss:0.1044 dloss:1.9542 dlossQ:1.9142 dlossQsigm:0.0400\n",
      "Episode:27 meanR:39.7143 rate:0.0800 gloss:0.0453 dloss:2.0846 dlossQ:2.0399 dlossQsigm:0.0446\n",
      "Episode:28 meanR:40.1034 rate:0.1020 gloss:0.2393 dloss:3.5161 dlossQ:3.4505 dlossQsigm:0.0656\n",
      "Episode:29 meanR:39.7000 rate:0.0560 gloss:0.1590 dloss:3.6301 dlossQ:3.5472 dlossQsigm:0.0829\n",
      "Episode:30 meanR:39.4194 rate:0.0620 gloss:0.0400 dloss:4.1269 dlossQ:4.0434 dlossQsigm:0.0835\n",
      "Episode:31 meanR:39.0938 rate:0.0580 gloss:0.0308 dloss:3.0102 dlossQ:2.9414 dlossQsigm:0.0688\n",
      "Episode:32 meanR:38.9697 rate:0.0700 gloss:0.1379 dloss:3.0817 dlossQ:3.0337 dlossQsigm:0.0480\n",
      "Episode:33 meanR:38.7353 rate:0.0620 gloss:0.0422 dloss:4.1108 dlossQ:4.0481 dlossQsigm:0.0627\n",
      "Episode:34 meanR:40.3143 rate:0.1880 gloss:0.2950 dloss:4.1294 dlossQ:4.0747 dlossQsigm:0.0547\n",
      "Episode:35 meanR:40.8333 rate:0.1180 gloss:0.0473 dloss:3.7390 dlossQ:3.6684 dlossQsigm:0.0706\n",
      "Episode:36 meanR:41.0811 rate:0.1000 gloss:0.2073 dloss:4.0305 dlossQ:3.9511 dlossQsigm:0.0794\n",
      "Episode:37 meanR:41.1842 rate:0.0900 gloss:0.0957 dloss:3.5766 dlossQ:3.5173 dlossQsigm:0.0593\n",
      "Episode:38 meanR:41.1538 rate:0.0800 gloss:0.0393 dloss:2.2761 dlossQ:2.2206 dlossQsigm:0.0555\n",
      "Episode:39 meanR:41.0000 rate:0.0700 gloss:0.0523 dloss:2.2537 dlossQ:2.2064 dlossQsigm:0.0473\n",
      "Episode:40 meanR:41.9024 rate:0.1560 gloss:0.0984 dloss:2.2959 dlossQ:2.2561 dlossQsigm:0.0399\n",
      "Episode:41 meanR:42.4762 rate:0.1320 gloss:0.1476 dloss:3.0950 dlossQ:3.0582 dlossQsigm:0.0368\n",
      "Episode:42 meanR:42.2326 rate:0.0640 gloss:0.0462 dloss:2.9259 dlossQ:2.8706 dlossQsigm:0.0553\n",
      "Episode:43 meanR:42.2955 rate:0.0900 gloss:0.0263 dloss:2.6288 dlossQ:2.5776 dlossQsigm:0.0511\n",
      "Episode:44 meanR:42.2444 rate:0.0800 gloss:0.0188 dloss:5.3545 dlossQ:4.9975 dlossQsigm:0.3571\n",
      "Episode:45 meanR:42.5000 rate:0.1080 gloss:0.0800 dloss:3.0278 dlossQ:2.8765 dlossQsigm:0.1513\n",
      "Episode:46 meanR:42.5745 rate:0.0920 gloss:0.1228 dloss:3.2947 dlossQ:3.2573 dlossQsigm:0.0373\n",
      "Episode:47 meanR:44.0417 rate:0.2260 gloss:0.0676 dloss:3.0411 dlossQ:3.0043 dlossQsigm:0.0369\n",
      "Episode:48 meanR:43.9796 rate:0.0820 gloss:0.0262 dloss:4.0093 dlossQ:3.9413 dlossQsigm:0.0680\n",
      "Episode:49 meanR:44.4600 rate:0.1360 gloss:0.2014 dloss:1.7388 dlossQ:1.7206 dlossQsigm:0.0181\n",
      "Episode:50 meanR:44.8235 rate:0.1260 gloss:0.1275 dloss:2.4717 dlossQ:2.4311 dlossQsigm:0.0406\n",
      "Episode:51 meanR:44.6923 rate:0.0760 gloss:0.0932 dloss:5.1462 dlossQ:5.0948 dlossQsigm:0.0514\n",
      "Episode:52 meanR:45.1321 rate:0.1360 gloss:0.1236 dloss:2.8273 dlossQ:2.8017 dlossQsigm:0.0256\n",
      "Episode:53 meanR:44.9444 rate:0.0700 gloss:0.1135 dloss:2.8120 dlossQ:2.7680 dlossQsigm:0.0440\n",
      "Episode:54 meanR:44.9091 rate:0.0860 gloss:0.0792 dloss:2.1943 dlossQ:2.1506 dlossQsigm:0.0436\n",
      "Episode:55 meanR:44.6786 rate:0.0640 gloss:0.1726 dloss:3.0577 dlossQ:3.0088 dlossQsigm:0.0488\n",
      "Episode:56 meanR:44.7719 rate:0.1000 gloss:0.0071 dloss:2.4467 dlossQ:2.4001 dlossQsigm:0.0465\n",
      "Episode:57 meanR:44.6724 rate:0.0780 gloss:0.0949 dloss:2.8787 dlossQ:2.8342 dlossQsigm:0.0445\n",
      "Episode:58 meanR:44.7966 rate:0.1040 gloss:0.0139 dloss:4.0301 dlossQ:3.9794 dlossQsigm:0.0507\n",
      "Episode:59 meanR:45.2333 rate:0.1420 gloss:0.0901 dloss:3.5238 dlossQ:3.4838 dlossQsigm:0.0401\n",
      "Episode:60 meanR:45.2295 rate:0.0900 gloss:0.0307 dloss:6.4381 dlossQ:6.3552 dlossQsigm:0.0829\n",
      "Episode:61 meanR:45.1774 rate:0.0840 gloss:0.0189 dloss:5.6923 dlossQ:5.6258 dlossQsigm:0.0665\n",
      "Episode:62 meanR:45.4921 rate:0.1300 gloss:0.1577 dloss:2.9987 dlossQ:2.9712 dlossQsigm:0.0275\n",
      "Episode:63 meanR:45.6719 rate:0.1140 gloss:0.1121 dloss:3.1149 dlossQ:3.0635 dlossQsigm:0.0514\n",
      "Episode:64 meanR:45.8769 rate:0.1180 gloss:0.0321 dloss:2.1233 dlossQ:2.0759 dlossQsigm:0.0474\n",
      "Episode:65 meanR:46.1364 rate:0.1260 gloss:0.0125 dloss:1.5178 dlossQ:1.4750 dlossQsigm:0.0427\n",
      "Episode:66 meanR:45.9701 rate:0.0700 gloss:0.0120 dloss:1.6218 dlossQ:1.5814 dlossQsigm:0.0403\n",
      "Episode:67 meanR:46.1765 rate:0.1200 gloss:0.0159 dloss:1.2221 dlossQ:1.1812 dlossQsigm:0.0410\n",
      "Episode:68 meanR:46.0000 rate:0.0680 gloss:0.0106 dloss:1.5845 dlossQ:1.5408 dlossQsigm:0.0437\n",
      "Episode:69 meanR:46.3571 rate:0.1420 gloss:0.0874 dloss:1.5049 dlossQ:1.4851 dlossQsigm:0.0199\n",
      "Episode:70 meanR:46.3662 rate:0.0940 gloss:0.0072 dloss:1.6678 dlossQ:1.6233 dlossQsigm:0.0445\n",
      "Episode:71 meanR:46.2361 rate:0.0740 gloss:0.0125 dloss:2.1168 dlossQ:2.0546 dlossQsigm:0.0622\n",
      "Episode:72 meanR:47.0548 rate:0.2120 gloss:0.0858 dloss:2.7010 dlossQ:2.6545 dlossQsigm:0.0465\n",
      "Episode:73 meanR:47.5270 rate:0.1640 gloss:0.2367 dloss:1.9495 dlossQ:1.9242 dlossQsigm:0.0253\n",
      "Episode:74 meanR:47.4267 rate:0.0800 gloss:0.0302 dloss:5.6523 dlossQ:5.5940 dlossQsigm:0.0583\n",
      "Episode:75 meanR:47.2500 rate:0.0680 gloss:0.0191 dloss:3.6192 dlossQ:3.5708 dlossQsigm:0.0484\n",
      "Episode:76 meanR:47.1688 rate:0.0820 gloss:0.0570 dloss:2.7862 dlossQ:2.7428 dlossQsigm:0.0434\n",
      "Episode:77 meanR:47.6282 rate:0.1660 gloss:0.0048 dloss:2.4618 dlossQ:2.4273 dlossQsigm:0.0345\n",
      "Episode:78 meanR:47.6203 rate:0.0940 gloss:0.0581 dloss:4.2299 dlossQ:4.1760 dlossQsigm:0.0539\n",
      "Episode:79 meanR:47.4250 rate:0.0640 gloss:0.0114 dloss:3.6502 dlossQ:3.5887 dlossQsigm:0.0616\n",
      "Episode:80 meanR:47.4815 rate:0.1040 gloss:0.0069 dloss:4.5064 dlossQ:4.4476 dlossQsigm:0.0588\n",
      "Episode:81 meanR:47.5488 rate:0.1060 gloss:0.0037 dloss:2.7668 dlossQ:2.7255 dlossQsigm:0.0412\n",
      "Episode:82 meanR:47.5181 rate:0.0900 gloss:0.0068 dloss:2.9995 dlossQ:2.9462 dlossQsigm:0.0533\n",
      "Episode:83 meanR:47.5714 rate:0.1040 gloss:0.0066 dloss:2.5703 dlossQ:2.5166 dlossQsigm:0.0537\n",
      "Episode:84 meanR:47.5176 rate:0.0860 gloss:0.0131 dloss:2.9344 dlossQ:2.8832 dlossQsigm:0.0512\n",
      "Episode:85 meanR:47.8256 rate:0.1480 gloss:0.2460 dloss:2.6395 dlossQ:2.6140 dlossQsigm:0.0256\n",
      "Episode:86 meanR:47.9885 rate:0.1240 gloss:0.0730 dloss:2.2997 dlossQ:2.2544 dlossQsigm:0.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:87 meanR:48.3182 rate:0.1540 gloss:0.1018 dloss:3.7135 dlossQ:3.6678 dlossQsigm:0.0458\n",
      "Episode:88 meanR:48.7079 rate:0.1660 gloss:0.0276 dloss:1.9742 dlossQ:1.9505 dlossQsigm:0.0237\n",
      "Episode:89 meanR:49.0000 rate:0.1500 gloss:0.0106 dloss:1.7205 dlossQ:1.7010 dlossQsigm:0.0195\n",
      "Episode:90 meanR:48.7912 rate:0.0600 gloss:0.0775 dloss:3.7846 dlossQ:3.7236 dlossQsigm:0.0610\n",
      "Episode:91 meanR:48.6848 rate:0.0780 gloss:0.0097 dloss:3.2909 dlossQ:3.2385 dlossQsigm:0.0523\n",
      "Episode:92 meanR:48.7527 rate:0.1100 gloss:0.0107 dloss:3.8214 dlossQ:3.7607 dlossQsigm:0.0606\n",
      "Episode:93 meanR:48.6915 rate:0.0860 gloss:0.0043 dloss:2.3672 dlossQ:2.3159 dlossQsigm:0.0513\n",
      "Episode:94 meanR:48.5579 rate:0.0720 gloss:0.0805 dloss:2.3420 dlossQ:2.2828 dlossQsigm:0.0591\n",
      "Episode:95 meanR:48.8438 rate:0.1520 gloss:0.0213 dloss:2.8622 dlossQ:2.8253 dlossQsigm:0.0369\n",
      "Episode:96 meanR:49.7216 rate:0.2680 gloss:0.0061 dloss:2.4017 dlossQ:2.3680 dlossQsigm:0.0336\n",
      "Episode:97 meanR:49.7857 rate:0.1120 gloss:0.0189 dloss:2.8913 dlossQ:2.8330 dlossQsigm:0.0583\n",
      "Episode:98 meanR:49.7576 rate:0.0940 gloss:0.0204 dloss:4.9007 dlossQ:4.8502 dlossQsigm:0.0505\n",
      "Episode:99 meanR:49.8100 rate:0.1100 gloss:0.0250 dloss:4.6471 dlossQ:4.5755 dlossQsigm:0.0716\n",
      "Episode:100 meanR:49.6000 rate:0.1180 gloss:0.0097 dloss:5.2181 dlossQ:5.1581 dlossQsigm:0.0600\n",
      "Episode:101 meanR:49.6700 rate:0.1100 gloss:0.0205 dloss:4.2052 dlossQ:4.1523 dlossQsigm:0.0529\n",
      "Episode:102 meanR:49.7400 rate:0.0780 gloss:0.0495 dloss:4.4830 dlossQ:4.4141 dlossQsigm:0.0688\n",
      "Episode:103 meanR:49.7400 rate:0.0800 gloss:0.0481 dloss:6.7548 dlossQ:6.6648 dlossQsigm:0.0900\n",
      "Episode:104 meanR:50.3800 rate:0.2020 gloss:0.0084 dloss:3.3859 dlossQ:3.3497 dlossQsigm:0.0363\n",
      "Episode:105 meanR:50.3600 rate:0.0760 gloss:0.0729 dloss:5.4540 dlossQ:5.3837 dlossQsigm:0.0702\n",
      "Episode:106 meanR:50.6300 rate:0.1140 gloss:0.0192 dloss:4.1578 dlossQ:4.1023 dlossQsigm:0.0555\n",
      "Episode:107 meanR:50.8600 rate:0.1140 gloss:0.0163 dloss:2.6836 dlossQ:2.6336 dlossQsigm:0.0500\n",
      "Episode:108 meanR:51.0200 rate:0.1240 gloss:0.0050 dloss:4.8657 dlossQ:4.8043 dlossQsigm:0.0614\n",
      "Episode:109 meanR:51.2100 rate:0.1240 gloss:0.0170 dloss:6.5077 dlossQ:6.4469 dlossQsigm:0.0608\n",
      "Episode:110 meanR:51.3400 rate:0.0860 gloss:0.0425 dloss:2.6960 dlossQ:2.6541 dlossQsigm:0.0418\n",
      "Episode:111 meanR:51.3100 rate:0.0820 gloss:0.0271 dloss:2.5211 dlossQ:2.4670 dlossQsigm:0.0541\n",
      "Episode:112 meanR:51.6900 rate:0.1480 gloss:0.0204 dloss:4.9696 dlossQ:4.9473 dlossQsigm:0.0223\n",
      "Episode:113 meanR:51.9700 rate:0.1280 gloss:0.0553 dloss:6.4162 dlossQ:6.3934 dlossQsigm:0.0228\n",
      "Episode:114 meanR:51.9900 rate:0.0720 gloss:0.0047 dloss:2.5775 dlossQ:2.5216 dlossQsigm:0.0559\n",
      "Episode:115 meanR:52.1700 rate:0.1080 gloss:0.0610 dloss:2.6981 dlossQ:2.6611 dlossQsigm:0.0370\n",
      "Episode:116 meanR:52.1600 rate:0.0840 gloss:0.0138 dloss:2.5053 dlossQ:2.4357 dlossQsigm:0.0696\n",
      "Episode:117 meanR:52.4300 rate:0.1460 gloss:0.1474 dloss:4.8225 dlossQ:4.7869 dlossQsigm:0.0356\n",
      "Episode:118 meanR:52.5400 rate:0.1020 gloss:0.0292 dloss:4.3452 dlossQ:4.2872 dlossQsigm:0.0579\n",
      "Episode:119 meanR:52.8700 rate:0.1320 gloss:0.0771 dloss:4.0861 dlossQ:4.0364 dlossQsigm:0.0498\n",
      "Episode:120 meanR:53.2900 rate:0.1520 gloss:0.0341 dloss:3.6053 dlossQ:3.5604 dlossQsigm:0.0449\n",
      "Episode:121 meanR:53.3900 rate:0.0880 gloss:0.0237 dloss:3.5164 dlossQ:3.4519 dlossQsigm:0.0644\n",
      "Episode:122 meanR:53.3400 rate:0.0880 gloss:0.1826 dloss:5.5554 dlossQ:5.4979 dlossQsigm:0.0576\n",
      "Episode:123 meanR:53.4200 rate:0.0740 gloss:0.1002 dloss:3.4998 dlossQ:3.4445 dlossQsigm:0.0552\n",
      "Episode:124 meanR:53.2900 rate:0.0640 gloss:0.0642 dloss:5.6559 dlossQ:5.5951 dlossQsigm:0.0609\n",
      "Episode:125 meanR:53.6400 rate:0.1380 gloss:0.0590 dloss:3.8080 dlossQ:3.7632 dlossQsigm:0.0447\n",
      "Episode:126 meanR:53.5700 rate:0.0640 gloss:0.0653 dloss:5.1605 dlossQ:5.0821 dlossQsigm:0.0784\n",
      "Episode:127 meanR:53.5700 rate:0.0800 gloss:0.0431 dloss:3.5183 dlossQ:3.4104 dlossQsigm:0.1079\n",
      "Episode:128 meanR:53.4700 rate:0.0820 gloss:0.0449 dloss:4.7044 dlossQ:4.6277 dlossQsigm:0.0767\n",
      "Episode:129 meanR:53.6600 rate:0.0940 gloss:0.0134 dloss:3.2882 dlossQ:3.2425 dlossQsigm:0.0457\n",
      "Episode:130 meanR:53.9900 rate:0.1280 gloss:0.1342 dloss:5.8265 dlossQ:5.8045 dlossQsigm:0.0220\n",
      "Episode:131 meanR:54.3600 rate:0.1320 gloss:0.1310 dloss:3.7656 dlossQ:3.7217 dlossQsigm:0.0439\n",
      "Episode:132 meanR:54.4500 rate:0.0880 gloss:0.0281 dloss:3.9388 dlossQ:3.8622 dlossQsigm:0.0767\n",
      "Episode:133 meanR:54.9100 rate:0.1540 gloss:0.1458 dloss:4.8333 dlossQ:4.7925 dlossQsigm:0.0409\n",
      "Episode:134 meanR:54.6800 rate:0.1420 gloss:0.0211 dloss:2.0049 dlossQ:1.9814 dlossQsigm:0.0235\n",
      "Episode:135 meanR:54.5700 rate:0.0960 gloss:0.0582 dloss:1.8914 dlossQ:1.8433 dlossQsigm:0.0481\n",
      "Episode:136 meanR:54.4200 rate:0.0700 gloss:0.0156 dloss:3.5814 dlossQ:3.5237 dlossQsigm:0.0577\n",
      "Episode:137 meanR:54.4700 rate:0.1000 gloss:0.0144 dloss:0.8752 dlossQ:0.8325 dlossQsigm:0.0427\n",
      "Episode:138 meanR:54.4600 rate:0.0780 gloss:0.0570 dloss:7.1380 dlossQ:7.0980 dlossQsigm:0.0401\n",
      "Episode:139 meanR:54.6500 rate:0.1080 gloss:0.0550 dloss:5.0185 dlossQ:4.9696 dlossQsigm:0.0489\n",
      "Episode:140 meanR:54.4000 rate:0.1060 gloss:0.0431 dloss:2.3706 dlossQ:2.3183 dlossQsigm:0.0523\n",
      "Episode:141 meanR:54.2900 rate:0.1100 gloss:0.0731 dloss:4.2816 dlossQ:4.2196 dlossQsigm:0.0620\n",
      "Episode:142 meanR:54.5400 rate:0.1140 gloss:0.0197 dloss:4.9995 dlossQ:4.9495 dlossQsigm:0.0500\n",
      "Episode:143 meanR:54.5100 rate:0.0840 gloss:0.0203 dloss:2.5850 dlossQ:2.5347 dlossQsigm:0.0503\n",
      "Episode:144 meanR:54.7800 rate:0.1340 gloss:0.0361 dloss:2.8363 dlossQ:2.8117 dlossQsigm:0.0245\n",
      "Episode:145 meanR:55.1100 rate:0.1740 gloss:1.3114 dloss:1.5925 dlossQ:1.5690 dlossQsigm:0.0235\n",
      "Episode:146 meanR:55.0300 rate:0.0760 gloss:0.0221 dloss:1.8526 dlossQ:1.8060 dlossQsigm:0.0465\n",
      "Episode:147 meanR:54.3200 rate:0.0840 gloss:0.0562 dloss:2.4582 dlossQ:2.4129 dlossQsigm:0.0453\n",
      "Episode:148 meanR:54.7300 rate:0.1640 gloss:0.0633 dloss:3.2224 dlossQ:3.1838 dlossQsigm:0.0386\n",
      "Episode:149 meanR:55.1400 rate:0.2180 gloss:0.0243 dloss:2.8372 dlossQ:2.7929 dlossQsigm:0.0442\n",
      "Episode:150 meanR:55.3100 rate:0.1600 gloss:0.0767 dloss:3.6422 dlossQ:3.6014 dlossQsigm:0.0408\n",
      "Episode:151 meanR:55.4100 rate:0.0960 gloss:0.0845 dloss:4.2843 dlossQ:4.2114 dlossQsigm:0.0729\n",
      "Episode:152 meanR:55.4600 rate:0.1460 gloss:0.0691 dloss:2.0290 dlossQ:2.0047 dlossQsigm:0.0243\n",
      "Episode:153 meanR:55.4700 rate:0.0720 gloss:0.0144 dloss:2.0627 dlossQ:2.0154 dlossQsigm:0.0473\n",
      "Episode:154 meanR:55.8700 rate:0.1660 gloss:0.0247 dloss:1.3610 dlossQ:1.3358 dlossQsigm:0.0252\n",
      "Episode:155 meanR:55.9800 rate:0.0860 gloss:0.0224 dloss:1.9439 dlossQ:1.8975 dlossQsigm:0.0464\n",
      "Episode:156 meanR:55.9400 rate:0.0920 gloss:0.0312 dloss:6.0426 dlossQ:5.9962 dlossQsigm:0.0464\n",
      "Episode:157 meanR:55.8900 rate:0.0680 gloss:0.0168 dloss:5.6913 dlossQ:5.6332 dlossQsigm:0.0581\n",
      "Episode:158 meanR:55.7500 rate:0.0760 gloss:0.0350 dloss:8.8830 dlossQ:8.8230 dlossQsigm:0.0600\n",
      "Episode:159 meanR:55.6800 rate:0.1280 gloss:0.1138 dloss:4.3715 dlossQ:4.3243 dlossQsigm:0.0472\n",
      "Episode:160 meanR:55.6500 rate:0.0840 gloss:0.0460 dloss:2.0962 dlossQ:2.0507 dlossQsigm:0.0454\n",
      "Episode:161 meanR:55.6500 rate:0.0840 gloss:0.0083 dloss:2.0858 dlossQ:2.0449 dlossQsigm:0.0408\n",
      "Episode:162 meanR:55.5100 rate:0.1020 gloss:0.0154 dloss:1.4883 dlossQ:1.4421 dlossQsigm:0.0462\n",
      "Episode:163 meanR:55.5300 rate:0.1180 gloss:0.0046 dloss:2.5296 dlossQ:2.4702 dlossQsigm:0.0594\n",
      "Episode:164 meanR:55.3000 rate:0.0720 gloss:0.0242 dloss:4.6774 dlossQ:4.5934 dlossQsigm:0.0840\n",
      "Episode:165 meanR:55.2500 rate:0.1160 gloss:0.0933 dloss:5.2910 dlossQ:5.2139 dlossQsigm:0.0771\n",
      "Episode:166 meanR:55.8000 rate:0.1800 gloss:0.0264 dloss:1.9543 dlossQ:1.9172 dlossQsigm:0.0370\n",
      "Episode:167 meanR:55.6000 rate:0.0800 gloss:0.0066 dloss:2.1505 dlossQ:2.0952 dlossQsigm:0.0553\n",
      "Episode:168 meanR:56.1100 rate:0.1700 gloss:0.0194 dloss:2.5272 dlossQ:2.5030 dlossQsigm:0.0242\n",
      "Episode:169 meanR:55.8400 rate:0.0880 gloss:0.0193 dloss:2.6559 dlossQ:2.6130 dlossQsigm:0.0429\n",
      "Episode:170 meanR:55.8200 rate:0.0900 gloss:0.0037 dloss:2.3893 dlossQ:2.3471 dlossQsigm:0.0422\n",
      "Episode:171 meanR:56.2100 rate:0.1520 gloss:0.0422 dloss:1.5063 dlossQ:1.4810 dlossQsigm:0.0252\n",
      "Episode:172 meanR:55.7800 rate:0.1260 gloss:0.1406 dloss:4.8209 dlossQ:4.7744 dlossQsigm:0.0465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:173 meanR:55.9000 rate:0.1880 gloss:0.0102 dloss:5.3963 dlossQ:5.3713 dlossQsigm:0.0250\n",
      "Episode:174 meanR:56.1100 rate:0.1220 gloss:0.0023 dloss:4.6028 dlossQ:4.5428 dlossQsigm:0.0600\n",
      "Episode:175 meanR:56.4500 rate:0.1360 gloss:0.0101 dloss:2.8544 dlossQ:2.8289 dlossQsigm:0.0255\n",
      "Episode:176 meanR:56.4400 rate:0.0800 gloss:0.0692 dloss:3.1025 dlossQ:3.0540 dlossQsigm:0.0485\n",
      "Episode:177 meanR:56.0000 rate:0.0780 gloss:0.0235 dloss:3.9003 dlossQ:3.8463 dlossQsigm:0.0540\n",
      "Episode:178 meanR:55.9200 rate:0.0780 gloss:0.0471 dloss:3.1817 dlossQ:3.1335 dlossQsigm:0.0482\n",
      "Episode:179 meanR:56.1700 rate:0.1140 gloss:0.0272 dloss:2.3139 dlossQ:2.2635 dlossQsigm:0.0504\n",
      "Episode:180 meanR:56.5600 rate:0.1820 gloss:0.0115 dloss:3.0103 dlossQ:2.9726 dlossQsigm:0.0377\n",
      "Episode:181 meanR:56.5600 rate:0.1060 gloss:0.0200 dloss:4.4657 dlossQ:4.3849 dlossQsigm:0.0808\n",
      "Episode:182 meanR:56.4800 rate:0.0740 gloss:0.0188 dloss:3.2673 dlossQ:3.2206 dlossQsigm:0.0468\n",
      "Episode:183 meanR:56.4000 rate:0.0880 gloss:0.0396 dloss:1.7272 dlossQ:1.6854 dlossQsigm:0.0418\n",
      "Episode:184 meanR:56.3100 rate:0.0680 gloss:0.0298 dloss:3.5279 dlossQ:3.4704 dlossQsigm:0.0576\n",
      "Episode:185 meanR:56.1800 rate:0.1220 gloss:0.0344 dloss:4.0015 dlossQ:3.9573 dlossQsigm:0.0442\n",
      "Episode:186 meanR:55.9500 rate:0.0780 gloss:0.0205 dloss:4.6804 dlossQ:4.6348 dlossQsigm:0.0456\n",
      "Episode:187 meanR:55.9500 rate:0.1540 gloss:0.2211 dloss:3.5238 dlossQ:3.4648 dlossQsigm:0.0590\n",
      "Episode:188 meanR:55.5800 rate:0.0920 gloss:0.0092 dloss:5.7902 dlossQ:5.7142 dlossQsigm:0.0759\n",
      "Episode:189 meanR:55.1500 rate:0.0640 gloss:0.0115 dloss:3.3138 dlossQ:3.2748 dlossQsigm:0.0391\n",
      "Episode:190 meanR:55.2600 rate:0.0820 gloss:0.0174 dloss:2.2261 dlossQ:2.1754 dlossQsigm:0.0507\n",
      "Episode:191 meanR:55.4000 rate:0.1060 gloss:0.0220 dloss:3.6454 dlossQ:3.5690 dlossQsigm:0.0765\n",
      "Episode:192 meanR:55.2700 rate:0.0840 gloss:0.0048 dloss:6.1329 dlossQ:6.0201 dlossQsigm:0.1129\n",
      "Episode:193 meanR:55.5200 rate:0.1360 gloss:0.0109 dloss:4.0145 dlossQ:3.9629 dlossQsigm:0.0516\n",
      "Episode:194 meanR:55.6300 rate:0.0940 gloss:0.0136 dloss:5.7869 dlossQ:5.6932 dlossQsigm:0.0937\n",
      "Episode:195 meanR:55.3700 rate:0.1000 gloss:0.0423 dloss:5.9761 dlossQ:5.9238 dlossQsigm:0.0523\n",
      "Episode:196 meanR:54.5500 rate:0.1040 gloss:0.2573 dloss:2.9483 dlossQ:2.8943 dlossQsigm:0.0541\n",
      "Episode:197 meanR:54.4300 rate:0.0880 gloss:0.0110 dloss:2.5248 dlossQ:2.4768 dlossQsigm:0.0480\n",
      "Episode:198 meanR:54.4500 rate:0.0980 gloss:0.0103 dloss:4.8151 dlossQ:4.7315 dlossQsigm:0.0836\n",
      "Episode:199 meanR:54.2900 rate:0.0780 gloss:0.0465 dloss:4.5871 dlossQ:4.5077 dlossQsigm:0.0795\n",
      "Episode:200 meanR:54.0800 rate:0.0760 gloss:0.0114 dloss:2.7081 dlossQ:2.6614 dlossQsigm:0.0467\n",
      "Episode:201 meanR:53.9100 rate:0.0760 gloss:0.1018 dloss:3.0760 dlossQ:3.0328 dlossQsigm:0.0431\n",
      "Episode:202 meanR:54.1500 rate:0.1260 gloss:0.0288 dloss:4.7685 dlossQ:4.7106 dlossQsigm:0.0579\n",
      "Episode:203 meanR:54.1100 rate:0.0720 gloss:0.0114 dloss:3.7192 dlossQ:3.6197 dlossQsigm:0.0996\n",
      "Episode:204 meanR:53.4800 rate:0.0760 gloss:0.6581 dloss:1.5277 dlossQ:1.4321 dlossQsigm:0.0957\n",
      "Episode:205 meanR:53.6400 rate:0.1080 gloss:0.0187 dloss:3.5649 dlossQ:3.5048 dlossQsigm:0.0601\n",
      "Episode:206 meanR:53.6400 rate:0.1140 gloss:0.0092 dloss:4.4503 dlossQ:4.4012 dlossQsigm:0.0492\n",
      "Episode:207 meanR:53.5100 rate:0.0880 gloss:0.0908 dloss:3.5369 dlossQ:3.4909 dlossQsigm:0.0461\n",
      "Episode:208 meanR:53.2900 rate:0.0800 gloss:0.0222 dloss:2.6700 dlossQ:2.6174 dlossQsigm:0.0526\n",
      "Episode:209 meanR:53.0400 rate:0.0740 gloss:0.0070 dloss:2.0391 dlossQ:1.9751 dlossQsigm:0.0640\n",
      "Episode:210 meanR:53.0300 rate:0.0840 gloss:0.0298 dloss:1.4127 dlossQ:1.3754 dlossQsigm:0.0373\n",
      "Episode:211 meanR:53.0000 rate:0.0760 gloss:0.0063 dloss:1.8462 dlossQ:1.8054 dlossQsigm:0.0408\n",
      "Episode:212 meanR:52.7800 rate:0.1040 gloss:0.0325 dloss:6.8069 dlossQ:6.7668 dlossQsigm:0.0401\n",
      "Episode:213 meanR:52.4600 rate:0.0640 gloss:0.0293 dloss:2.8419 dlossQ:2.7917 dlossQsigm:0.0502\n",
      "Episode:214 meanR:52.5200 rate:0.0840 gloss:0.0096 dloss:2.8708 dlossQ:2.8121 dlossQsigm:0.0587\n",
      "Episode:215 meanR:52.3700 rate:0.0780 gloss:0.0019 dloss:3.2900 dlossQ:3.2393 dlossQsigm:0.0506\n",
      "Episode:216 meanR:52.2900 rate:0.0680 gloss:0.0097 dloss:2.2106 dlossQ:2.1704 dlossQsigm:0.0402\n",
      "Episode:217 meanR:52.0000 rate:0.0880 gloss:0.0149 dloss:3.9855 dlossQ:3.9304 dlossQsigm:0.0551\n",
      "Episode:218 meanR:52.0200 rate:0.1060 gloss:0.0316 dloss:3.2268 dlossQ:3.1775 dlossQsigm:0.0494\n",
      "Episode:219 meanR:51.8500 rate:0.0980 gloss:0.0096 dloss:2.5046 dlossQ:2.4626 dlossQsigm:0.0420\n",
      "Episode:220 meanR:51.7400 rate:0.1300 gloss:0.0683 dloss:2.5586 dlossQ:2.5341 dlossQsigm:0.0246\n",
      "Episode:221 meanR:51.7800 rate:0.0960 gloss:0.0143 dloss:2.2291 dlossQ:2.1818 dlossQsigm:0.0473\n",
      "Episode:222 meanR:51.6500 rate:0.0620 gloss:0.0052 dloss:1.2642 dlossQ:1.2225 dlossQsigm:0.0417\n",
      "Episode:223 meanR:51.9400 rate:0.1320 gloss:0.0269 dloss:2.9331 dlossQ:2.9136 dlossQsigm:0.0195\n",
      "Episode:224 meanR:52.2000 rate:0.1160 gloss:0.0111 dloss:1.6089 dlossQ:1.5623 dlossQsigm:0.0466\n",
      "Episode:225 meanR:51.9200 rate:0.0820 gloss:0.0242 dloss:2.5796 dlossQ:2.5298 dlossQsigm:0.0498\n",
      "Episode:226 meanR:52.1500 rate:0.1100 gloss:0.0039 dloss:2.3094 dlossQ:2.2507 dlossQsigm:0.0587\n",
      "Episode:227 meanR:53.3600 rate:0.3220 gloss:0.0327 dloss:3.0114 dlossQ:2.9897 dlossQsigm:0.0218\n",
      "Episode:228 meanR:53.4400 rate:0.0980 gloss:0.0122 dloss:3.8064 dlossQ:3.7373 dlossQsigm:0.0691\n",
      "Episode:229 meanR:53.3500 rate:0.0760 gloss:0.0064 dloss:4.7525 dlossQ:4.6952 dlossQsigm:0.0573\n",
      "Episode:230 meanR:53.1500 rate:0.0880 gloss:0.0007 dloss:3.9978 dlossQ:3.9494 dlossQsigm:0.0483\n",
      "Episode:231 meanR:53.1000 rate:0.1220 gloss:0.0014 dloss:3.3140 dlossQ:3.2625 dlossQsigm:0.0515\n",
      "Episode:232 meanR:53.3800 rate:0.1440 gloss:0.0505 dloss:1.8911 dlossQ:1.8640 dlossQsigm:0.0271\n",
      "Episode:233 meanR:53.2200 rate:0.1220 gloss:0.0124 dloss:2.8158 dlossQ:2.7694 dlossQsigm:0.0464\n",
      "Episode:234 meanR:53.3200 rate:0.1620 gloss:0.0235 dloss:3.1595 dlossQ:3.1381 dlossQsigm:0.0215\n",
      "Episode:235 meanR:53.1700 rate:0.0660 gloss:0.0029 dloss:2.6827 dlossQ:2.6337 dlossQsigm:0.0489\n",
      "Episode:236 meanR:53.2800 rate:0.0920 gloss:0.0038 dloss:3.0007 dlossQ:2.9107 dlossQsigm:0.0900\n",
      "Episode:237 meanR:53.4500 rate:0.1340 gloss:0.0005 dloss:1.3459 dlossQ:1.3108 dlossQsigm:0.0351\n",
      "Episode:238 meanR:53.5400 rate:0.0960 gloss:0.0071 dloss:2.8343 dlossQ:2.7744 dlossQsigm:0.0598\n",
      "Episode:239 meanR:53.7300 rate:0.1460 gloss:0.0066 dloss:2.5577 dlossQ:2.5140 dlossQsigm:0.0437\n",
      "Episode:240 meanR:53.7000 rate:0.1000 gloss:0.0087 dloss:3.0186 dlossQ:2.9597 dlossQsigm:0.0589\n",
      "Episode:241 meanR:53.6800 rate:0.1060 gloss:0.0039 dloss:2.7363 dlossQ:2.6938 dlossQsigm:0.0426\n",
      "Episode:242 meanR:53.5600 rate:0.0900 gloss:0.0068 dloss:3.0519 dlossQ:3.0045 dlossQsigm:0.0474\n",
      "Episode:243 meanR:53.8200 rate:0.1360 gloss:0.0002 dloss:1.1582 dlossQ:1.1362 dlossQsigm:0.0220\n",
      "Episode:244 meanR:54.0900 rate:0.1880 gloss:0.0129 dloss:4.5185 dlossQ:4.4890 dlossQsigm:0.0295\n",
      "Episode:245 meanR:53.7100 rate:0.0980 gloss:0.0017 dloss:3.8371 dlossQ:3.7706 dlossQsigm:0.0665\n",
      "Episode:246 meanR:53.7100 rate:0.0760 gloss:0.0068 dloss:2.0370 dlossQ:1.9913 dlossQsigm:0.0457\n",
      "Episode:247 meanR:53.8600 rate:0.1140 gloss:0.0029 dloss:1.9770 dlossQ:1.9299 dlossQsigm:0.0471\n",
      "Episode:248 meanR:53.5200 rate:0.0960 gloss:0.0055 dloss:1.8077 dlossQ:1.7527 dlossQsigm:0.0550\n",
      "Episode:249 meanR:52.8100 rate:0.0760 gloss:0.0015 dloss:1.1838 dlossQ:1.1425 dlossQsigm:0.0412\n",
      "Episode:250 meanR:52.5600 rate:0.1100 gloss:0.0027 dloss:1.7946 dlossQ:1.7437 dlossQsigm:0.0509\n",
      "Episode:251 meanR:52.4100 rate:0.0660 gloss:0.0048 dloss:2.7970 dlossQ:2.7307 dlossQsigm:0.0664\n",
      "Episode:252 meanR:52.1500 rate:0.0940 gloss:0.0067 dloss:1.4947 dlossQ:1.4482 dlossQsigm:0.0466\n",
      "Episode:253 meanR:52.4700 rate:0.1360 gloss:0.0044 dloss:2.4124 dlossQ:2.3721 dlossQsigm:0.0403\n",
      "Episode:254 meanR:52.2900 rate:0.1300 gloss:0.0021 dloss:4.1673 dlossQ:4.1324 dlossQsigm:0.0349\n",
      "Episode:255 meanR:52.2500 rate:0.0780 gloss:0.0080 dloss:2.7099 dlossQ:2.6442 dlossQsigm:0.0657\n",
      "Episode:256 meanR:52.1800 rate:0.0780 gloss:0.0051 dloss:1.7899 dlossQ:1.7482 dlossQsigm:0.0417\n",
      "Episode:257 meanR:52.4100 rate:0.1140 gloss:0.0024 dloss:2.5641 dlossQ:2.5099 dlossQsigm:0.0542\n",
      "Episode:258 meanR:52.4300 rate:0.0800 gloss:0.0072 dloss:4.4339 dlossQ:4.3825 dlossQsigm:0.0513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:259 meanR:52.3200 rate:0.1060 gloss:0.0153 dloss:1.8199 dlossQ:1.7814 dlossQsigm:0.0385\n",
      "Episode:260 meanR:52.3900 rate:0.0980 gloss:0.6484 dloss:1.3950 dlossQ:1.3513 dlossQsigm:0.0437\n",
      "Episode:261 meanR:52.5100 rate:0.1080 gloss:0.0698 dloss:5.2804 dlossQ:5.2062 dlossQsigm:0.0743\n",
      "Episode:262 meanR:52.7200 rate:0.1440 gloss:0.0061 dloss:2.5068 dlossQ:2.4778 dlossQsigm:0.0290\n",
      "Episode:263 meanR:52.7700 rate:0.1280 gloss:0.0063 dloss:8.0492 dlossQ:8.0172 dlossQsigm:0.0321\n",
      "Episode:264 meanR:52.9100 rate:0.1000 gloss:0.0061 dloss:3.4525 dlossQ:3.3719 dlossQsigm:0.0806\n",
      "Episode:265 meanR:52.8500 rate:0.1040 gloss:0.0036 dloss:2.2825 dlossQ:2.2313 dlossQsigm:0.0511\n",
      "Episode:266 meanR:52.3600 rate:0.0820 gloss:0.0161 dloss:2.2125 dlossQ:2.1583 dlossQsigm:0.0542\n",
      "Episode:267 meanR:52.5400 rate:0.1160 gloss:0.0243 dloss:3.2469 dlossQ:3.1790 dlossQsigm:0.0678\n",
      "Episode:268 meanR:52.1300 rate:0.0880 gloss:0.0073 dloss:3.8881 dlossQ:3.7983 dlossQsigm:0.0898\n",
      "Episode:269 meanR:52.2600 rate:0.1140 gloss:0.0219 dloss:2.1917 dlossQ:2.1281 dlossQsigm:0.0636\n",
      "Episode:270 meanR:52.3000 rate:0.0980 gloss:0.0121 dloss:2.6715 dlossQ:2.5994 dlossQsigm:0.0720\n",
      "Episode:271 meanR:52.0800 rate:0.1080 gloss:0.0309 dloss:2.3545 dlossQ:2.2887 dlossQsigm:0.0657\n",
      "Episode:272 meanR:51.9400 rate:0.0980 gloss:0.0110 dloss:2.2037 dlossQ:2.1452 dlossQsigm:0.0585\n",
      "Episode:273 meanR:51.4700 rate:0.0940 gloss:0.0069 dloss:2.7600 dlossQ:2.7052 dlossQsigm:0.0549\n",
      "Episode:274 meanR:51.5000 rate:0.1280 gloss:0.0050 dloss:8.0231 dlossQ:7.9915 dlossQsigm:0.0316\n",
      "Episode:275 meanR:51.2400 rate:0.0840 gloss:0.0184 dloss:3.7956 dlossQ:3.7337 dlossQsigm:0.0620\n",
      "Episode:276 meanR:51.4400 rate:0.1200 gloss:0.0129 dloss:3.1318 dlossQ:3.0767 dlossQsigm:0.0551\n",
      "Episode:277 meanR:51.6000 rate:0.1100 gloss:0.0039 dloss:3.4987 dlossQ:3.4496 dlossQsigm:0.0491\n",
      "Episode:278 meanR:51.7800 rate:0.1140 gloss:0.0011 dloss:3.9533 dlossQ:3.9059 dlossQsigm:0.0474\n",
      "Episode:279 meanR:51.8900 rate:0.1360 gloss:0.0105 dloss:2.2091 dlossQ:2.1677 dlossQsigm:0.0414\n",
      "Episode:280 meanR:51.4800 rate:0.1000 gloss:0.0027 dloss:2.6676 dlossQ:2.5974 dlossQsigm:0.0702\n",
      "Episode:281 meanR:51.5000 rate:0.1100 gloss:0.0021 dloss:10.0657 dlossQ:9.9785 dlossQsigm:0.0871\n",
      "Episode:282 meanR:51.7800 rate:0.1300 gloss:0.0035 dloss:7.9892 dlossQ:7.9545 dlossQsigm:0.0347\n",
      "Episode:283 meanR:52.0000 rate:0.1320 gloss:0.0027 dloss:4.7074 dlossQ:4.6749 dlossQsigm:0.0324\n",
      "Episode:284 meanR:52.1100 rate:0.0900 gloss:0.0214 dloss:3.0110 dlossQ:2.9414 dlossQsigm:0.0696\n",
      "Episode:285 meanR:51.9400 rate:0.0880 gloss:0.1045 dloss:3.0633 dlossQ:2.9878 dlossQsigm:0.0755\n",
      "Episode:286 meanR:52.3100 rate:0.1520 gloss:0.0024 dloss:2.2376 dlossQ:2.1966 dlossQsigm:0.0410\n",
      "Episode:287 meanR:52.1100 rate:0.1140 gloss:0.0018 dloss:3.1279 dlossQ:3.0561 dlossQsigm:0.0718\n",
      "Episode:288 meanR:52.4100 rate:0.1520 gloss:0.0007 dloss:2.1905 dlossQ:2.1482 dlossQsigm:0.0423\n",
      "Episode:289 meanR:52.9000 rate:0.1620 gloss:0.0004 dloss:2.0094 dlossQ:1.9748 dlossQsigm:0.0346\n",
      "Episode:290 meanR:52.9700 rate:0.0960 gloss:0.0062 dloss:3.6284 dlossQ:3.5515 dlossQsigm:0.0769\n",
      "Episode:291 meanR:53.1000 rate:0.1320 gloss:0.0033 dloss:3.4474 dlossQ:3.4196 dlossQsigm:0.0277\n",
      "Episode:292 meanR:53.2900 rate:0.1220 gloss:0.0021 dloss:2.0993 dlossQ:2.0398 dlossQsigm:0.0595\n",
      "Episode:293 meanR:53.2300 rate:0.1240 gloss:0.0467 dloss:3.1791 dlossQ:3.1185 dlossQsigm:0.0606\n",
      "Episode:294 meanR:53.4100 rate:0.1300 gloss:0.0080 dloss:7.0843 dlossQ:7.0625 dlossQsigm:0.0218\n",
      "Episode:295 meanR:53.4500 rate:0.1080 gloss:0.0019 dloss:1.9040 dlossQ:1.8583 dlossQsigm:0.0458\n",
      "Episode:296 meanR:53.4100 rate:0.0960 gloss:0.0187 dloss:3.2897 dlossQ:3.2208 dlossQsigm:0.0689\n",
      "Episode:297 meanR:54.2900 rate:0.2640 gloss:0.0211 dloss:3.3009 dlossQ:3.2620 dlossQsigm:0.0390\n",
      "Episode:298 meanR:54.3100 rate:0.1020 gloss:0.0235 dloss:9.3384 dlossQ:9.2572 dlossQsigm:0.0813\n",
      "Episode:299 meanR:54.3800 rate:0.0920 gloss:0.0242 dloss:4.8901 dlossQ:4.8274 dlossQsigm:0.0627\n",
      "Episode:300 meanR:55.1300 rate:0.2260 gloss:0.0348 dloss:7.3516 dlossQ:7.3083 dlossQsigm:0.0433\n",
      "Episode:301 meanR:55.5100 rate:0.1520 gloss:0.0017 dloss:2.2963 dlossQ:2.2601 dlossQsigm:0.0363\n",
      "Episode:302 meanR:55.5300 rate:0.1300 gloss:0.0083 dloss:4.6084 dlossQ:4.5766 dlossQsigm:0.0318\n",
      "Episode:303 meanR:55.8200 rate:0.1300 gloss:0.0065 dloss:2.8740 dlossQ:2.8403 dlossQsigm:0.0337\n",
      "Episode:304 meanR:56.2700 rate:0.1660 gloss:0.0111 dloss:2.2658 dlossQ:2.2289 dlossQsigm:0.0369\n",
      "Episode:305 meanR:56.1300 rate:0.0800 gloss:0.0069 dloss:4.8575 dlossQ:4.7823 dlossQsigm:0.0751\n",
      "Episode:306 meanR:56.3700 rate:0.1620 gloss:0.0086 dloss:3.8151 dlossQ:3.7679 dlossQsigm:0.0472\n",
      "Episode:307 meanR:56.7100 rate:0.1560 gloss:0.0064 dloss:2.8332 dlossQ:2.7839 dlossQsigm:0.0493\n",
      "Episode:308 meanR:56.9700 rate:0.1320 gloss:0.0081 dloss:6.3651 dlossQ:6.3367 dlossQsigm:0.0284\n",
      "Episode:309 meanR:57.1900 rate:0.1180 gloss:0.0367 dloss:4.5899 dlossQ:4.5272 dlossQsigm:0.0627\n",
      "Episode:310 meanR:57.3100 rate:0.1080 gloss:0.0048 dloss:3.3930 dlossQ:3.3306 dlossQsigm:0.0623\n",
      "Episode:311 meanR:57.6700 rate:0.1480 gloss:0.0008 dloss:16.8535 dlossQ:16.7857 dlossQsigm:0.0679\n",
      "Episode:312 meanR:57.8800 rate:0.1460 gloss:0.0140 dloss:3.9453 dlossQ:3.8938 dlossQsigm:0.0515\n",
      "Episode:313 meanR:58.1400 rate:0.1160 gloss:0.0039 dloss:4.9520 dlossQ:4.8563 dlossQsigm:0.0957\n",
      "Episode:314 meanR:58.4200 rate:0.1400 gloss:0.0003 dloss:1.9553 dlossQ:1.9189 dlossQsigm:0.0364\n",
      "Episode:315 meanR:59.6600 rate:0.3260 gloss:0.0015 dloss:4.0302 dlossQ:3.9873 dlossQsigm:0.0429\n",
      "Episode:316 meanR:60.0200 rate:0.1400 gloss:0.0096 dloss:4.5802 dlossQ:4.5386 dlossQsigm:0.0416\n",
      "Episode:317 meanR:60.1300 rate:0.1100 gloss:0.0008 dloss:3.2356 dlossQ:3.1806 dlossQsigm:0.0550\n",
      "Episode:318 meanR:60.4000 rate:0.1600 gloss:0.0010 dloss:2.1440 dlossQ:2.1249 dlossQsigm:0.0191\n",
      "Episode:319 meanR:60.3300 rate:0.0840 gloss:0.0086 dloss:5.6540 dlossQ:5.6033 dlossQsigm:0.0507\n",
      "Episode:320 meanR:60.4400 rate:0.1520 gloss:0.0060 dloss:4.4322 dlossQ:4.4063 dlossQsigm:0.0260\n",
      "Episode:321 meanR:60.4100 rate:0.0900 gloss:0.0063 dloss:4.3929 dlossQ:4.3527 dlossQsigm:0.0402\n",
      "Episode:322 meanR:60.9400 rate:0.1680 gloss:0.0145 dloss:2.0966 dlossQ:2.0705 dlossQsigm:0.0261\n",
      "Episode:323 meanR:60.9900 rate:0.1420 gloss:0.0238 dloss:1.8568 dlossQ:1.8282 dlossQsigm:0.0285\n",
      "Episode:324 meanR:61.3300 rate:0.1840 gloss:0.0088 dloss:3.6573 dlossQ:3.6333 dlossQsigm:0.0240\n",
      "Episode:325 meanR:61.3900 rate:0.0940 gloss:0.0045 dloss:3.7431 dlossQ:3.6833 dlossQsigm:0.0598\n",
      "Episode:326 meanR:61.3000 rate:0.0920 gloss:0.0070 dloss:3.0692 dlossQ:3.0157 dlossQsigm:0.0535\n",
      "Episode:327 meanR:60.3000 rate:0.1220 gloss:0.0278 dloss:3.6301 dlossQ:3.5693 dlossQsigm:0.0607\n",
      "Episode:328 meanR:60.4500 rate:0.1280 gloss:0.0050 dloss:11.7859 dlossQ:11.7541 dlossQsigm:0.0317\n",
      "Episode:329 meanR:60.5500 rate:0.0960 gloss:0.0018 dloss:4.4819 dlossQ:4.4219 dlossQsigm:0.0599\n",
      "Episode:330 meanR:60.5300 rate:0.0840 gloss:0.0098 dloss:6.1498 dlossQ:6.0998 dlossQsigm:0.0500\n",
      "Episode:331 meanR:60.5600 rate:0.1280 gloss:0.1111 dloss:6.3587 dlossQ:6.3274 dlossQsigm:0.0314\n",
      "Episode:332 meanR:60.3200 rate:0.0960 gloss:0.0002 dloss:2.5727 dlossQ:2.5126 dlossQsigm:0.0601\n",
      "Episode:333 meanR:60.7200 rate:0.2020 gloss:0.0011 dloss:5.1175 dlossQ:5.0949 dlossQsigm:0.0226\n",
      "Episode:334 meanR:62.6600 rate:0.5500 gloss:0.0211 dloss:3.4680 dlossQ:3.4310 dlossQsigm:0.0370\n",
      "Episode:335 meanR:62.9000 rate:0.1140 gloss:0.0041 dloss:8.9118 dlossQ:8.8067 dlossQsigm:0.1051\n",
      "Episode:336 meanR:63.1200 rate:0.1360 gloss:0.0053 dloss:21.6953 dlossQ:21.5807 dlossQsigm:0.1146\n",
      "Episode:337 meanR:63.0700 rate:0.1240 gloss:0.0039 dloss:3.4210 dlossQ:3.3531 dlossQsigm:0.0679\n",
      "Episode:338 meanR:62.9800 rate:0.0780 gloss:1.0902 dloss:4.3676 dlossQ:4.2986 dlossQsigm:0.0690\n",
      "Episode:339 meanR:62.8700 rate:0.1240 gloss:0.0164 dloss:7.3541 dlossQ:7.2814 dlossQsigm:0.0727\n",
      "Episode:340 meanR:63.0100 rate:0.1280 gloss:0.0955 dloss:8.6044 dlossQ:8.5652 dlossQsigm:0.0392\n",
      "Episode:341 meanR:63.3800 rate:0.1800 gloss:0.0033 dloss:3.8952 dlossQ:3.8664 dlossQsigm:0.0288\n",
      "Episode:342 meanR:63.7900 rate:0.1720 gloss:0.0252 dloss:3.1204 dlossQ:3.0883 dlossQsigm:0.0321\n",
      "Episode:343 meanR:63.7800 rate:0.1340 gloss:0.0040 dloss:3.4942 dlossQ:3.4440 dlossQsigm:0.0502\n",
      "Episode:344 meanR:63.5600 rate:0.1440 gloss:0.0080 dloss:4.0272 dlossQ:3.9704 dlossQsigm:0.0568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:345 meanR:63.6000 rate:0.1060 gloss:0.0913 dloss:7.5862 dlossQ:7.4782 dlossQsigm:0.1080\n",
      "Episode:346 meanR:63.8400 rate:0.1240 gloss:0.0471 dloss:4.8284 dlossQ:4.7749 dlossQsigm:0.0535\n",
      "Episode:347 meanR:63.9900 rate:0.1440 gloss:0.0005 dloss:4.8157 dlossQ:4.7648 dlossQsigm:0.0509\n",
      "Episode:348 meanR:63.9700 rate:0.0920 gloss:0.0132 dloss:8.8462 dlossQ:8.7287 dlossQsigm:0.1175\n",
      "Episode:349 meanR:64.0300 rate:0.0880 gloss:0.0187 dloss:4.4033 dlossQ:4.2610 dlossQsigm:0.1423\n",
      "Episode:350 meanR:63.8800 rate:0.0800 gloss:0.0046 dloss:7.6487 dlossQ:7.5628 dlossQsigm:0.0859\n",
      "Episode:351 meanR:64.4000 rate:0.1700 gloss:0.0102 dloss:4.2764 dlossQ:4.2218 dlossQsigm:0.0545\n",
      "Episode:352 meanR:64.5600 rate:0.1260 gloss:0.0125 dloss:6.6457 dlossQ:6.5559 dlossQsigm:0.0898\n",
      "Episode:353 meanR:64.5200 rate:0.1280 gloss:0.0171 dloss:10.8579 dlossQ:10.8140 dlossQsigm:0.0439\n",
      "Episode:354 meanR:64.4900 rate:0.1240 gloss:0.0105 dloss:9.8006 dlossQ:9.7120 dlossQsigm:0.0886\n",
      "Episode:355 meanR:64.4900 rate:0.0780 gloss:0.0036 dloss:5.9396 dlossQ:5.8685 dlossQsigm:0.0711\n",
      "Episode:356 meanR:64.7300 rate:0.1260 gloss:0.0032 dloss:5.0048 dlossQ:4.9512 dlossQsigm:0.0536\n",
      "Episode:357 meanR:65.1200 rate:0.1920 gloss:0.0024 dloss:5.0566 dlossQ:5.0096 dlossQsigm:0.0469\n",
      "Episode:358 meanR:65.2400 rate:0.1040 gloss:0.0052 dloss:5.2015 dlossQ:5.1346 dlossQsigm:0.0669\n",
      "Episode:359 meanR:65.1900 rate:0.0960 gloss:0.0036 dloss:4.6128 dlossQ:4.5389 dlossQsigm:0.0738\n",
      "Episode:360 meanR:65.0900 rate:0.0780 gloss:0.0046 dloss:5.2864 dlossQ:5.2015 dlossQsigm:0.0849\n",
      "Episode:361 meanR:65.0500 rate:0.1000 gloss:0.0061 dloss:2.8889 dlossQ:2.8150 dlossQsigm:0.0739\n",
      "Episode:362 meanR:65.0500 rate:0.1440 gloss:0.0227 dloss:2.6634 dlossQ:2.6342 dlossQsigm:0.0292\n",
      "Episode:363 meanR:64.9300 rate:0.1040 gloss:0.0034 dloss:2.8350 dlossQ:2.6594 dlossQsigm:0.1756\n",
      "Episode:364 meanR:65.0400 rate:0.1220 gloss:0.0027 dloss:3.2130 dlossQ:3.1227 dlossQsigm:0.0903\n",
      "Episode:365 meanR:65.3000 rate:0.1560 gloss:0.0014 dloss:1.0989 dlossQ:1.0104 dlossQsigm:0.0885\n",
      "Episode:366 meanR:65.5600 rate:0.1340 gloss:0.0017 dloss:2.0777 dlossQ:2.0441 dlossQsigm:0.0336\n",
      "Episode:367 meanR:65.7900 rate:0.1620 gloss:0.0066 dloss:1.0797 dlossQ:1.0476 dlossQsigm:0.0321\n",
      "Episode:368 meanR:66.2300 rate:0.1760 gloss:0.0015 dloss:2.5846 dlossQ:2.5408 dlossQsigm:0.0439\n",
      "Episode:369 meanR:66.1300 rate:0.0940 gloss:0.0035 dloss:3.1113 dlossQ:3.0345 dlossQsigm:0.0768\n",
      "Episode:370 meanR:66.5400 rate:0.1800 gloss:0.0016 dloss:2.8997 dlossQ:2.8538 dlossQsigm:0.0459\n",
      "Episode:371 meanR:66.8900 rate:0.1780 gloss:0.0006 dloss:3.2248 dlossQ:3.1760 dlossQsigm:0.0488\n",
      "Episode:372 meanR:66.8100 rate:0.0820 gloss:0.0018 dloss:3.4657 dlossQ:3.3981 dlossQsigm:0.0676\n",
      "Episode:373 meanR:66.9500 rate:0.1220 gloss:0.0061 dloss:2.8231 dlossQ:2.7614 dlossQsigm:0.0617\n",
      "Episode:374 meanR:66.7100 rate:0.0800 gloss:0.0035 dloss:3.8197 dlossQ:3.7348 dlossQsigm:0.0848\n",
      "Episode:375 meanR:67.0500 rate:0.1520 gloss:0.0007 dloss:3.6619 dlossQ:3.6126 dlossQsigm:0.0493\n",
      "Episode:376 meanR:67.0900 rate:0.1280 gloss:0.0746 dloss:12.3719 dlossQ:12.3201 dlossQsigm:0.0518\n",
      "Episode:377 meanR:67.7100 rate:0.2340 gloss:0.0032 dloss:2.6340 dlossQ:2.6142 dlossQsigm:0.0199\n",
      "Episode:378 meanR:67.5700 rate:0.0860 gloss:0.0031 dloss:2.8903 dlossQ:2.8348 dlossQsigm:0.0554\n",
      "Episode:379 meanR:67.5100 rate:0.1240 gloss:0.0007 dloss:4.1935 dlossQ:4.1322 dlossQsigm:0.0613\n",
      "Episode:380 meanR:68.2100 rate:0.2400 gloss:0.0003 dloss:6.0344 dlossQ:6.0125 dlossQsigm:0.0219\n",
      "Episode:381 meanR:68.1700 rate:0.1020 gloss:0.0025 dloss:4.2900 dlossQ:4.2314 dlossQsigm:0.0586\n",
      "Episode:382 meanR:68.0700 rate:0.1100 gloss:0.0009 dloss:2.9945 dlossQ:2.9327 dlossQsigm:0.0618\n",
      "Episode:383 meanR:68.0200 rate:0.1220 gloss:0.0095 dloss:4.1440 dlossQ:4.0825 dlossQsigm:0.0615\n",
      "Episode:384 meanR:68.1000 rate:0.1060 gloss:0.0030 dloss:4.9978 dlossQ:4.9165 dlossQsigm:0.0812\n",
      "Episode:385 meanR:68.5000 rate:0.1680 gloss:0.0692 dloss:4.6606 dlossQ:4.6286 dlossQsigm:0.0319\n",
      "Episode:386 meanR:68.2100 rate:0.0940 gloss:0.0000 dloss:4.4063 dlossQ:4.3215 dlossQsigm:0.0848\n",
      "Episode:387 meanR:68.1600 rate:0.1040 gloss:0.0007 dloss:4.7089 dlossQ:4.6144 dlossQsigm:0.0945\n",
      "Episode:388 meanR:67.8500 rate:0.0900 gloss:0.0037 dloss:8.1425 dlossQ:8.0733 dlossQsigm:0.0692\n",
      "Episode:389 meanR:67.4700 rate:0.0860 gloss:0.0384 dloss:7.8886 dlossQ:7.6740 dlossQsigm:0.2146\n",
      "Episode:390 meanR:67.6600 rate:0.1340 gloss:0.0023 dloss:2.2890 dlossQ:2.2685 dlossQsigm:0.0205\n",
      "Episode:391 meanR:67.4500 rate:0.0900 gloss:0.0017 dloss:6.2917 dlossQ:6.2067 dlossQsigm:0.0850\n",
      "Episode:392 meanR:67.2400 rate:0.0800 gloss:0.0014 dloss:5.4985 dlossQ:5.4319 dlossQsigm:0.0666\n",
      "Episode:393 meanR:68.0400 rate:0.2840 gloss:0.0024 dloss:3.1726 dlossQ:3.1459 dlossQsigm:0.0267\n",
      "Episode:394 meanR:69.9700 rate:0.5160 gloss:0.0101 dloss:1.9529 dlossQ:1.9231 dlossQsigm:0.0298\n",
      "Episode:395 meanR:70.9700 rate:0.3080 gloss:0.0095 dloss:6.4442 dlossQ:6.3675 dlossQsigm:0.0767\n",
      "Episode:396 meanR:71.1600 rate:0.1340 gloss:0.0062 dloss:5.7986 dlossQ:5.7500 dlossQsigm:0.0486\n",
      "Episode:397 meanR:71.4700 rate:0.3260 gloss:0.0004 dloss:6.7172 dlossQ:6.6523 dlossQsigm:0.0649\n",
      "Episode:398 meanR:72.4600 rate:0.3000 gloss:0.0006 dloss:3.8301 dlossQ:3.7902 dlossQsigm:0.0398\n",
      "Episode:399 meanR:72.8300 rate:0.1660 gloss:0.0010 dloss:3.7759 dlossQ:3.7503 dlossQsigm:0.0256\n",
      "Episode:400 meanR:72.4000 rate:0.1400 gloss:0.0018 dloss:4.5614 dlossQ:4.5158 dlossQsigm:0.0456\n",
      "Episode:401 meanR:72.3000 rate:0.1320 gloss:0.0023 dloss:5.8990 dlossQ:5.8808 dlossQsigm:0.0182\n",
      "Episode:402 meanR:72.2500 rate:0.1200 gloss:0.0040 dloss:3.3846 dlossQ:3.3499 dlossQsigm:0.0347\n",
      "Episode:403 meanR:72.2200 rate:0.1240 gloss:0.0015 dloss:3.5672 dlossQ:3.5360 dlossQsigm:0.0311\n",
      "Episode:404 meanR:71.9900 rate:0.1200 gloss:0.0524 dloss:16.0729 dlossQ:15.9847 dlossQsigm:0.0882\n",
      "Episode:405 meanR:72.2400 rate:0.1300 gloss:0.0266 dloss:5.0977 dlossQ:5.0432 dlossQsigm:0.0545\n",
      "Episode:406 meanR:72.0100 rate:0.1160 gloss:0.0038 dloss:9.9129 dlossQ:9.8321 dlossQsigm:0.0809\n",
      "Episode:407 meanR:72.2800 rate:0.2100 gloss:0.0068 dloss:2.4840 dlossQ:2.4604 dlossQsigm:0.0236\n",
      "Episode:408 meanR:72.4000 rate:0.1560 gloss:0.0050 dloss:8.1060 dlossQ:8.0593 dlossQsigm:0.0467\n",
      "Episode:409 meanR:72.6300 rate:0.1640 gloss:0.0095 dloss:2.6641 dlossQ:2.6241 dlossQsigm:0.0400\n",
      "Episode:410 meanR:73.2000 rate:0.2220 gloss:0.0217 dloss:2.8304 dlossQ:2.7851 dlossQsigm:0.0454\n",
      "Episode:411 meanR:73.0400 rate:0.1160 gloss:0.0009 dloss:4.9244 dlossQ:4.8446 dlossQsigm:0.0798\n",
      "Episode:412 meanR:73.2000 rate:0.1780 gloss:0.0103 dloss:2.7200 dlossQ:2.6920 dlossQsigm:0.0280\n",
      "Episode:413 meanR:74.2200 rate:0.3200 gloss:0.0011 dloss:2.1824 dlossQ:2.1404 dlossQsigm:0.0420\n",
      "Episode:414 meanR:74.0200 rate:0.1000 gloss:0.0285 dloss:4.9085 dlossQ:4.8212 dlossQsigm:0.0873\n",
      "Episode:415 meanR:72.9600 rate:0.1140 gloss:0.0044 dloss:7.7350 dlossQ:7.6556 dlossQsigm:0.0794\n",
      "Episode:416 meanR:72.9600 rate:0.1400 gloss:0.0014 dloss:1.7334 dlossQ:1.7006 dlossQsigm:0.0328\n",
      "Episode:417 meanR:72.9800 rate:0.1140 gloss:0.0059 dloss:4.3322 dlossQ:4.2505 dlossQsigm:0.0817\n",
      "Episode:418 meanR:72.7900 rate:0.1220 gloss:0.0168 dloss:4.0600 dlossQ:3.9838 dlossQsigm:0.0762\n",
      "Episode:419 meanR:73.1700 rate:0.1600 gloss:0.0196 dloss:1.5654 dlossQ:1.5301 dlossQsigm:0.0353\n",
      "Episode:420 meanR:72.9500 rate:0.1080 gloss:0.0008 dloss:1.6672 dlossQ:1.6190 dlossQsigm:0.0482\n",
      "Episode:421 meanR:73.0500 rate:0.1100 gloss:0.0364 dloss:5.5268 dlossQ:5.4593 dlossQsigm:0.0675\n",
      "Episode:422 meanR:72.9000 rate:0.1380 gloss:0.0021 dloss:4.3055 dlossQ:4.2586 dlossQsigm:0.0469\n",
      "Episode:423 meanR:72.8900 rate:0.1400 gloss:0.0131 dloss:1.9241 dlossQ:1.9040 dlossQsigm:0.0201\n",
      "Episode:424 meanR:72.5500 rate:0.1160 gloss:0.0022 dloss:3.3396 dlossQ:3.2853 dlossQsigm:0.0543\n",
      "Episode:425 meanR:73.0100 rate:0.1860 gloss:0.0003 dloss:2.7627 dlossQ:2.7145 dlossQsigm:0.0481\n",
      "Episode:426 meanR:73.2200 rate:0.1340 gloss:0.0024 dloss:5.2108 dlossQ:5.1750 dlossQsigm:0.0359\n",
      "Episode:427 meanR:74.8700 rate:0.4520 gloss:0.0008 dloss:4.9604 dlossQ:4.8899 dlossQsigm:0.0704\n",
      "Episode:428 meanR:74.8700 rate:0.1280 gloss:0.0142 dloss:16.3695 dlossQ:16.3254 dlossQsigm:0.0441\n",
      "Episode:429 meanR:75.1200 rate:0.1460 gloss:0.0127 dloss:4.3619 dlossQ:4.3400 dlossQsigm:0.0219\n",
      "Episode:430 meanR:76.7800 rate:0.4160 gloss:0.0042 dloss:9.1227 dlossQ:9.0228 dlossQsigm:0.0999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:431 meanR:77.0300 rate:0.1780 gloss:0.0896 dloss:5.2641 dlossQ:5.2149 dlossQsigm:0.0492\n",
      "Episode:432 meanR:78.5600 rate:0.4020 gloss:0.0094 dloss:14.6531 dlossQ:14.5360 dlossQsigm:0.1171\n",
      "Episode:433 meanR:78.3000 rate:0.1500 gloss:0.0008 dloss:7.3760 dlossQ:7.3146 dlossQsigm:0.0614\n",
      "Episode:434 meanR:76.1900 rate:0.1280 gloss:0.0336 dloss:16.9764 dlossQ:16.9179 dlossQsigm:0.0585\n",
      "Episode:435 meanR:76.6900 rate:0.2140 gloss:0.0033 dloss:7.3125 dlossQ:7.2594 dlossQsigm:0.0531\n",
      "Episode:436 meanR:76.9000 rate:0.1780 gloss:0.0052 dloss:7.1757 dlossQ:7.0913 dlossQsigm:0.0845\n",
      "Episode:437 meanR:77.5400 rate:0.2520 gloss:0.0157 dloss:3.3632 dlossQ:3.3223 dlossQsigm:0.0409\n",
      "Episode:438 meanR:78.2400 rate:0.2180 gloss:0.0002 dloss:5.9969 dlossQ:5.9337 dlossQsigm:0.0633\n",
      "Episode:439 meanR:78.5900 rate:0.1940 gloss:0.0005 dloss:4.3817 dlossQ:4.3242 dlossQsigm:0.0576\n",
      "Episode:440 meanR:78.5600 rate:0.1220 gloss:0.0028 dloss:6.1899 dlossQ:6.0790 dlossQsigm:0.1109\n",
      "Episode:441 meanR:79.5200 rate:0.3720 gloss:0.0029 dloss:3.7430 dlossQ:3.6848 dlossQsigm:0.0582\n",
      "Episode:442 meanR:79.4100 rate:0.1500 gloss:0.0015 dloss:3.7670 dlossQ:3.7345 dlossQsigm:0.0325\n",
      "Episode:443 meanR:79.5500 rate:0.1620 gloss:0.0018 dloss:6.6991 dlossQ:6.6778 dlossQsigm:0.0213\n",
      "Episode:444 meanR:79.5100 rate:0.1360 gloss:0.0110 dloss:5.1382 dlossQ:5.1061 dlossQsigm:0.0321\n",
      "Episode:445 meanR:79.7300 rate:0.1500 gloss:0.0069 dloss:10.0695 dlossQ:9.9712 dlossQsigm:0.0983\n",
      "Episode:446 meanR:80.0700 rate:0.1920 gloss:0.0451 dloss:10.4237 dlossQ:10.3283 dlossQsigm:0.0954\n",
      "Episode:447 meanR:80.4500 rate:0.2200 gloss:0.0055 dloss:2.2591 dlossQ:2.2212 dlossQsigm:0.0379\n",
      "Episode:448 meanR:81.2000 rate:0.2420 gloss:0.0010 dloss:5.3680 dlossQ:5.2924 dlossQsigm:0.0756\n",
      "Episode:449 meanR:81.5600 rate:0.1600 gloss:0.0022 dloss:4.6315 dlossQ:4.5833 dlossQsigm:0.0483\n",
      "Episode:450 meanR:81.8600 rate:0.1400 gloss:0.0023 dloss:2.9781 dlossQ:2.9244 dlossQsigm:0.0537\n",
      "Episode:451 meanR:81.8600 rate:0.1700 gloss:0.0042 dloss:8.6406 dlossQ:8.5671 dlossQsigm:0.0735\n",
      "Episode:452 meanR:82.1600 rate:0.1860 gloss:0.0013 dloss:2.0356 dlossQ:2.0006 dlossQsigm:0.0351\n",
      "Episode:453 meanR:82.3700 rate:0.1700 gloss:0.0089 dloss:2.6565 dlossQ:2.6076 dlossQsigm:0.0488\n",
      "Episode:454 meanR:82.7900 rate:0.2080 gloss:0.0045 dloss:2.5512 dlossQ:2.5014 dlossQsigm:0.0498\n",
      "Episode:455 meanR:83.2400 rate:0.1680 gloss:0.0001 dloss:3.3205 dlossQ:3.2662 dlossQsigm:0.0544\n",
      "Episode:456 meanR:83.2600 rate:0.1300 gloss:0.0122 dloss:2.7241 dlossQ:2.6874 dlossQsigm:0.0367\n",
      "Episode:457 meanR:82.9000 rate:0.1200 gloss:0.0002 dloss:2.2652 dlossQ:2.1972 dlossQsigm:0.0680\n",
      "Episode:458 meanR:82.9500 rate:0.1140 gloss:0.0005 dloss:7.3543 dlossQ:7.2845 dlossQsigm:0.0698\n",
      "Episode:459 meanR:83.3800 rate:0.1820 gloss:0.0010 dloss:3.0367 dlossQ:2.9819 dlossQsigm:0.0548\n",
      "Episode:460 meanR:83.8100 rate:0.1640 gloss:0.0004 dloss:2.0296 dlossQ:1.9852 dlossQsigm:0.0444\n",
      "Episode:461 meanR:84.0900 rate:0.1560 gloss:0.0005 dloss:1.6071 dlossQ:1.5701 dlossQsigm:0.0370\n",
      "Episode:462 meanR:84.2500 rate:0.1760 gloss:0.0001 dloss:1.5559 dlossQ:1.5173 dlossQsigm:0.0386\n",
      "Episode:463 meanR:84.9500 rate:0.2440 gloss:0.0011 dloss:3.7947 dlossQ:3.7352 dlossQsigm:0.0595\n",
      "Episode:464 meanR:84.9900 rate:0.1300 gloss:0.0006 dloss:6.8323 dlossQ:6.7879 dlossQsigm:0.0444\n",
      "Episode:465 meanR:84.8000 rate:0.1180 gloss:0.0027 dloss:7.3894 dlossQ:7.2918 dlossQsigm:0.0976\n",
      "Episode:466 meanR:85.8700 rate:0.3480 gloss:0.0110 dloss:4.4532 dlossQ:4.4020 dlossQsigm:0.0512\n",
      "Episode:467 meanR:86.5300 rate:0.2940 gloss:0.0146 dloss:4.7467 dlossQ:4.6999 dlossQsigm:0.0469\n",
      "Episode:468 meanR:86.6500 rate:0.2000 gloss:0.0007 dloss:7.9483 dlossQ:7.9193 dlossQsigm:0.0290\n",
      "Episode:469 meanR:86.9300 rate:0.1500 gloss:0.0001 dloss:3.3779 dlossQ:3.3488 dlossQsigm:0.0290\n",
      "Episode:470 meanR:87.1000 rate:0.2140 gloss:0.0006 dloss:6.0657 dlossQ:6.0383 dlossQsigm:0.0274\n",
      "Episode:471 meanR:87.7500 rate:0.3080 gloss:0.0009 dloss:4.2012 dlossQ:4.1359 dlossQsigm:0.0653\n",
      "Episode:472 meanR:88.1300 rate:0.1580 gloss:0.0006 dloss:2.5924 dlossQ:2.5432 dlossQsigm:0.0492\n",
      "Episode:473 meanR:88.3900 rate:0.1740 gloss:0.0000 dloss:2.5788 dlossQ:2.5293 dlossQsigm:0.0495\n",
      "Episode:474 meanR:88.7300 rate:0.1480 gloss:0.0001 dloss:2.1705 dlossQ:2.1276 dlossQsigm:0.0429\n",
      "Episode:475 meanR:88.5900 rate:0.1240 gloss:-0.1720 dloss:4.5834 dlossQ:4.3772 dlossQsigm:0.2062\n",
      "Episode:476 meanR:89.3600 rate:0.2820 gloss:0.0002 dloss:3.9789 dlossQ:3.9324 dlossQsigm:0.0465\n",
      "Episode:477 meanR:89.0500 rate:0.1720 gloss:0.0234 dloss:5.6230 dlossQ:5.5484 dlossQsigm:0.0746\n",
      "Episode:478 meanR:89.6100 rate:0.1980 gloss:0.0024 dloss:8.5471 dlossQ:8.4514 dlossQsigm:0.0957\n",
      "Episode:479 meanR:90.0400 rate:0.2100 gloss:0.0044 dloss:6.5364 dlossQ:6.4786 dlossQsigm:0.0578\n",
      "Episode:480 meanR:89.6900 rate:0.1700 gloss:0.0016 dloss:6.6824 dlossQ:6.5994 dlossQsigm:0.0830\n",
      "Episode:481 meanR:89.8300 rate:0.1300 gloss:0.0154 dloss:4.0425 dlossQ:3.9919 dlossQsigm:0.0506\n",
      "Episode:482 meanR:90.0300 rate:0.1500 gloss:0.0043 dloss:1.6269 dlossQ:1.6022 dlossQsigm:0.0247\n",
      "Episode:483 meanR:89.9900 rate:0.1140 gloss:0.0048 dloss:4.5619 dlossQ:4.4808 dlossQsigm:0.0811\n",
      "Episode:484 meanR:90.3700 rate:0.1820 gloss:0.0020 dloss:5.4023 dlossQ:5.3494 dlossQsigm:0.0528\n",
      "Episode:485 meanR:90.2100 rate:0.1360 gloss:0.0064 dloss:2.9659 dlossQ:2.9312 dlossQsigm:0.0347\n",
      "Episode:486 meanR:90.6300 rate:0.1780 gloss:0.0065 dloss:2.6720 dlossQ:2.6299 dlossQsigm:0.0422\n",
      "Episode:487 meanR:90.8400 rate:0.1460 gloss:0.0007 dloss:8.2675 dlossQ:8.1754 dlossQsigm:0.0921\n",
      "Episode:488 meanR:91.2800 rate:0.1780 gloss:0.0027 dloss:6.1874 dlossQ:6.1103 dlossQsigm:0.0772\n",
      "Episode:489 meanR:91.5700 rate:0.1440 gloss:0.0005 dloss:8.1742 dlossQ:8.0856 dlossQsigm:0.0885\n",
      "Episode:490 meanR:91.6800 rate:0.1560 gloss:0.0017 dloss:9.3512 dlossQ:9.2572 dlossQsigm:0.0940\n",
      "Episode:491 meanR:92.1300 rate:0.1800 gloss:0.0286 dloss:8.3284 dlossQ:8.2377 dlossQsigm:0.0907\n",
      "Episode:492 meanR:92.6600 rate:0.1860 gloss:0.0093 dloss:3.8282 dlossQ:3.7857 dlossQsigm:0.0425\n",
      "Episode:493 meanR:92.3100 rate:0.2140 gloss:0.0157 dloss:2.2564 dlossQ:2.2284 dlossQsigm:0.0279\n",
      "Episode:494 meanR:91.0300 rate:0.2600 gloss:0.0098 dloss:11.4172 dlossQ:11.3081 dlossQsigm:0.1091\n",
      "Episode:495 meanR:90.4700 rate:0.1960 gloss:0.0021 dloss:8.5872 dlossQ:8.4939 dlossQsigm:0.0933\n",
      "Episode:496 meanR:90.4300 rate:0.1260 gloss:-0.0007 dloss:12.6776 dlossQ:12.3640 dlossQsigm:0.3136\n",
      "Episode:497 meanR:89.9800 rate:0.2360 gloss:0.0007 dloss:7.7148 dlossQ:7.6234 dlossQsigm:0.0913\n",
      "Episode:498 meanR:89.5800 rate:0.2200 gloss:0.0002 dloss:6.8089 dlossQ:6.7249 dlossQsigm:0.0840\n",
      "Episode:499 meanR:89.3900 rate:0.1280 gloss:0.1229 dloss:5.9578 dlossQ:5.7525 dlossQsigm:0.2053\n",
      "Episode:500 meanR:89.4100 rate:0.1440 gloss:0.0022 dloss:5.8640 dlossQ:5.8017 dlossQsigm:0.0624\n",
      "Episode:501 meanR:89.9500 rate:0.2400 gloss:0.0029 dloss:4.7609 dlossQ:4.6889 dlossQsigm:0.0721\n",
      "Episode:502 meanR:90.1800 rate:0.1660 gloss:0.0012 dloss:12.6342 dlossQ:12.5951 dlossQsigm:0.0391\n",
      "Episode:503 meanR:90.4300 rate:0.1740 gloss:0.0083 dloss:5.1570 dlossQ:5.0843 dlossQsigm:0.0726\n",
      "Episode:504 meanR:90.4100 rate:0.1160 gloss:0.0023 dloss:8.4419 dlossQ:8.3382 dlossQsigm:0.1037\n",
      "Episode:505 meanR:91.6200 rate:0.3720 gloss:0.0256 dloss:4.6023 dlossQ:4.5560 dlossQsigm:0.0464\n",
      "Episode:506 meanR:92.2900 rate:0.2500 gloss:0.0400 dloss:4.3679 dlossQ:4.3213 dlossQsigm:0.0466\n",
      "Episode:507 meanR:92.7000 rate:0.2920 gloss:0.0002 dloss:7.1054 dlossQ:7.0233 dlossQsigm:0.0821\n",
      "Episode:508 meanR:93.5200 rate:0.3200 gloss:0.0011 dloss:8.7957 dlossQ:8.7022 dlossQsigm:0.0935\n",
      "Episode:509 meanR:93.8000 rate:0.2200 gloss:0.0019 dloss:6.4374 dlossQ:6.3550 dlossQsigm:0.0824\n",
      "Episode:510 meanR:93.7400 rate:0.2100 gloss:0.0001 dloss:6.7266 dlossQ:6.6451 dlossQsigm:0.0814\n",
      "Episode:511 meanR:94.0000 rate:0.1680 gloss:0.0018 dloss:6.4549 dlossQ:6.4273 dlossQsigm:0.0276\n",
      "Episode:512 meanR:94.9400 rate:0.3660 gloss:0.0010 dloss:3.2654 dlossQ:3.2148 dlossQsigm:0.0506\n",
      "Episode:513 meanR:94.6700 rate:0.2660 gloss:0.0001 dloss:5.7180 dlossQ:5.6672 dlossQsigm:0.0508\n",
      "Episode:514 meanR:95.0000 rate:0.1660 gloss:0.0005 dloss:3.8593 dlossQ:3.8018 dlossQsigm:0.0575\n",
      "Episode:515 meanR:95.9500 rate:0.3040 gloss:0.0013 dloss:8.5323 dlossQ:8.4378 dlossQsigm:0.0945\n",
      "Episode:516 meanR:95.7800 rate:0.1060 gloss:0.0031 dloss:12.9866 dlossQ:12.8733 dlossQsigm:0.1133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:517 meanR:96.2300 rate:0.2040 gloss:0.0177 dloss:8.3284 dlossQ:8.2379 dlossQsigm:0.0906\n",
      "Episode:518 meanR:96.2200 rate:0.1200 gloss:0.0005 dloss:15.1030 dlossQ:14.9930 dlossQsigm:0.1101\n",
      "Episode:519 meanR:96.0600 rate:0.1280 gloss:0.0010 dloss:7.2652 dlossQ:7.2379 dlossQsigm:0.0272\n",
      "Episode:520 meanR:96.5600 rate:0.2080 gloss:0.0094 dloss:6.3734 dlossQ:6.3181 dlossQsigm:0.0552\n",
      "Episode:521 meanR:97.7100 rate:0.3400 gloss:0.0155 dloss:7.7956 dlossQ:7.7134 dlossQsigm:0.0821\n",
      "Episode:522 meanR:97.8600 rate:0.1680 gloss:0.0016 dloss:6.7089 dlossQ:6.6504 dlossQsigm:0.0585\n",
      "Episode:523 meanR:97.8000 rate:0.1280 gloss:0.0007 dloss:15.6870 dlossQ:15.6352 dlossQsigm:0.0517\n",
      "Episode:524 meanR:98.2800 rate:0.2120 gloss:0.0003 dloss:5.7385 dlossQ:5.6673 dlossQsigm:0.0713\n",
      "Episode:525 meanR:98.1000 rate:0.1500 gloss:0.0000 dloss:2.3382 dlossQ:2.3085 dlossQsigm:0.0297\n",
      "Episode:526 meanR:99.4300 rate:0.4000 gloss:0.0008 dloss:11.5360 dlossQ:11.4547 dlossQsigm:0.0813\n",
      "Episode:527 meanR:98.0300 rate:0.1720 gloss:0.0020 dloss:9.3572 dlossQ:9.3201 dlossQsigm:0.0371\n",
      "Episode:528 meanR:99.6300 rate:0.4480 gloss:0.0001 dloss:5.6969 dlossQ:5.6366 dlossQsigm:0.0603\n",
      "Episode:529 meanR:99.4900 rate:0.1180 gloss:0.0055 dloss:19.9868 dlossQ:19.8810 dlossQsigm:0.1058\n",
      "Episode:530 meanR:98.2200 rate:0.1620 gloss:0.0008 dloss:10.8513 dlossQ:10.8252 dlossQsigm:0.0261\n",
      "Episode:531 meanR:98.3600 rate:0.2060 gloss:0.0001 dloss:8.6049 dlossQ:8.5245 dlossQsigm:0.0804\n",
      "Episode:532 meanR:97.0200 rate:0.1340 gloss:0.0070 dloss:13.2485 dlossQ:13.2210 dlossQsigm:0.0275\n",
      "Episode:533 meanR:97.0300 rate:0.1520 gloss:0.0006 dloss:10.1422 dlossQ:10.0622 dlossQsigm:0.0800\n",
      "Episode:534 meanR:97.1800 rate:0.1580 gloss:0.0005 dloss:10.9772 dlossQ:10.9024 dlossQsigm:0.0749\n",
      "Episode:535 meanR:96.7300 rate:0.1240 gloss:0.0056 dloss:13.2055 dlossQ:13.0879 dlossQsigm:0.1176\n",
      "Episode:536 meanR:96.9800 rate:0.2280 gloss:0.0004 dloss:9.1657 dlossQ:9.0775 dlossQsigm:0.0882\n",
      "Episode:537 meanR:96.9200 rate:0.2400 gloss:0.0001 dloss:5.5071 dlossQ:5.4510 dlossQsigm:0.0561\n",
      "Episode:538 meanR:96.5300 rate:0.1400 gloss:0.0060 dloss:9.7986 dlossQ:9.7702 dlossQsigm:0.0285\n",
      "Episode:539 meanR:96.2700 rate:0.1420 gloss:0.0000 dloss:6.1888 dlossQ:6.1204 dlossQsigm:0.0684\n",
      "Episode:540 meanR:96.6100 rate:0.1900 gloss:0.0000 dloss:8.4057 dlossQ:8.3178 dlossQsigm:0.0879\n",
      "Episode:541 meanR:95.4900 rate:0.1480 gloss:0.0115 dloss:6.8257 dlossQ:6.7720 dlossQsigm:0.0537\n",
      "Episode:542 meanR:95.3600 rate:0.1240 gloss:0.0004 dloss:8.9121 dlossQ:8.8199 dlossQsigm:0.0922\n",
      "Episode:543 meanR:95.5900 rate:0.2080 gloss:0.0057 dloss:10.3205 dlossQ:10.2570 dlossQsigm:0.0635\n",
      "Episode:544 meanR:95.4900 rate:0.1160 gloss:0.0005 dloss:9.5274 dlossQ:9.3932 dlossQsigm:0.1342\n",
      "Episode:545 meanR:95.7800 rate:0.2080 gloss:0.0148 dloss:8.0017 dlossQ:7.9398 dlossQsigm:0.0620\n",
      "Episode:546 meanR:95.3800 rate:0.1120 gloss:0.0071 dloss:9.2884 dlossQ:9.1736 dlossQsigm:0.1147\n",
      "Episode:547 meanR:95.1400 rate:0.1720 gloss:0.0264 dloss:5.2453 dlossQ:5.2035 dlossQsigm:0.0418\n",
      "Episode:548 meanR:94.7300 rate:0.1600 gloss:0.0060 dloss:5.3111 dlossQ:5.2600 dlossQsigm:0.0511\n",
      "Episode:549 meanR:96.8500 rate:0.5840 gloss:0.0007 dloss:10.7867 dlossQ:10.7202 dlossQsigm:0.0665\n",
      "Episode:550 meanR:96.9700 rate:0.1640 gloss:0.0014 dloss:5.3282 dlossQ:5.2785 dlossQsigm:0.0497\n",
      "Episode:551 meanR:96.7700 rate:0.1300 gloss:0.0000 dloss:11.1033 dlossQ:11.0307 dlossQsigm:0.0727\n",
      "Episode:552 meanR:96.5500 rate:0.1420 gloss:0.0002 dloss:7.5733 dlossQ:7.5082 dlossQsigm:0.0651\n",
      "Episode:553 meanR:96.4400 rate:0.1480 gloss:0.0005 dloss:8.0402 dlossQ:7.9741 dlossQsigm:0.0661\n",
      "Episode:554 meanR:96.3100 rate:0.1820 gloss:0.0001 dloss:9.1518 dlossQ:9.0660 dlossQsigm:0.0859\n",
      "Episode:555 meanR:96.5500 rate:0.2160 gloss:0.0007 dloss:6.4105 dlossQ:6.3458 dlossQsigm:0.0647\n",
      "Episode:556 meanR:96.5600 rate:0.1320 gloss:0.0078 dloss:8.8723 dlossQ:8.8332 dlossQsigm:0.0391\n",
      "Episode:557 meanR:96.5600 rate:0.1200 gloss:0.0084 dloss:15.5302 dlossQ:15.3852 dlossQsigm:0.1450\n",
      "Episode:558 meanR:98.1000 rate:0.4220 gloss:0.0001 dloss:9.0604 dlossQ:8.9790 dlossQsigm:0.0814\n",
      "Episode:559 meanR:98.1700 rate:0.1960 gloss:0.0001 dloss:5.8924 dlossQ:5.8240 dlossQsigm:0.0684\n",
      "Episode:560 meanR:98.2500 rate:0.1800 gloss:0.0000 dloss:10.2905 dlossQ:10.1933 dlossQsigm:0.0973\n",
      "Episode:561 meanR:98.1700 rate:0.1400 gloss:0.0001 dloss:7.0488 dlossQ:6.9834 dlossQsigm:0.0654\n",
      "Episode:562 meanR:98.1100 rate:0.1640 gloss:0.0000 dloss:8.4797 dlossQ:8.3974 dlossQsigm:0.0823\n",
      "Episode:563 meanR:97.6200 rate:0.1460 gloss:0.0018 dloss:6.6071 dlossQ:6.5447 dlossQsigm:0.0623\n",
      "Episode:564 meanR:98.3700 rate:0.2800 gloss:0.0008 dloss:7.0210 dlossQ:6.9611 dlossQsigm:0.0598\n",
      "Episode:565 meanR:98.4900 rate:0.1420 gloss:0.0001 dloss:13.3762 dlossQ:13.3078 dlossQsigm:0.0685\n",
      "Episode:566 meanR:97.5000 rate:0.1500 gloss:0.0001 dloss:9.2034 dlossQ:9.1214 dlossQsigm:0.0819\n",
      "Episode:567 meanR:97.7700 rate:0.3480 gloss:0.0000 dloss:5.3865 dlossQ:5.3224 dlossQsigm:0.0641\n",
      "Episode:568 meanR:97.6100 rate:0.1680 gloss:0.0000 dloss:9.9550 dlossQ:9.8610 dlossQsigm:0.0940\n",
      "Episode:569 meanR:98.9500 rate:0.4180 gloss:0.0000 dloss:9.4132 dlossQ:9.3568 dlossQsigm:0.0564\n",
      "Episode:570 meanR:99.4700 rate:0.3180 gloss:0.0037 dloss:11.3202 dlossQ:11.2193 dlossQsigm:0.1009\n",
      "Episode:571 meanR:98.9400 rate:0.2020 gloss:0.0008 dloss:3.5466 dlossQ:3.5008 dlossQsigm:0.0458\n",
      "Episode:572 meanR:99.0600 rate:0.1820 gloss:0.0016 dloss:8.6576 dlossQ:8.5733 dlossQsigm:0.0843\n",
      "Episode:573 meanR:98.7900 rate:0.1200 gloss:0.0006 dloss:13.1177 dlossQ:12.9714 dlossQsigm:0.1463\n",
      "Episode:574 meanR:99.2000 rate:0.2300 gloss:0.0002 dloss:8.7217 dlossQ:8.6360 dlossQsigm:0.0857\n",
      "Episode:575 meanR:99.3800 rate:0.1600 gloss:0.0042 dloss:9.0683 dlossQ:8.9888 dlossQsigm:0.0794\n",
      "Episode:576 meanR:99.0200 rate:0.2100 gloss:0.0002 dloss:6.3634 dlossQ:6.2880 dlossQsigm:0.0753\n",
      "Episode:577 meanR:98.8500 rate:0.1380 gloss:0.0148 dloss:7.4794 dlossQ:7.4064 dlossQsigm:0.0730\n",
      "Episode:578 meanR:99.4900 rate:0.3260 gloss:0.0016 dloss:15.6106 dlossQ:15.5072 dlossQsigm:0.1034\n",
      "Episode:579 meanR:99.0700 rate:0.1260 gloss:0.0002 dloss:32.3542 dlossQ:32.1163 dlossQsigm:0.2380\n",
      "Episode:580 meanR:99.0500 rate:0.1660 gloss:0.0003 dloss:13.5356 dlossQ:13.4219 dlossQsigm:0.1136\n",
      "Episode:581 meanR:99.9300 rate:0.3060 gloss:0.0012 dloss:11.4041 dlossQ:11.3052 dlossQsigm:0.0989\n",
      "Episode:582 meanR:99.9800 rate:0.1600 gloss:0.0014 dloss:10.7079 dlossQ:10.6523 dlossQsigm:0.0556\n",
      "Episode:583 meanR:100.0900 rate:0.1360 gloss:0.0004 dloss:5.8181 dlossQ:5.7731 dlossQsigm:0.0450\n",
      "Episode:584 meanR:99.9600 rate:0.1560 gloss:0.0010 dloss:7.0391 dlossQ:6.9691 dlossQsigm:0.0701\n",
      "Episode:585 meanR:100.2600 rate:0.1960 gloss:0.0002 dloss:9.2473 dlossQ:9.1563 dlossQsigm:0.0911\n",
      "Episode:586 meanR:100.3600 rate:0.1980 gloss:0.0003 dloss:6.8426 dlossQ:6.7997 dlossQsigm:0.0429\n",
      "Episode:587 meanR:100.2900 rate:0.1320 gloss:0.0247 dloss:19.3681 dlossQ:19.2518 dlossQsigm:0.1163\n",
      "Episode:588 meanR:100.9200 rate:0.3040 gloss:0.0003 dloss:12.4423 dlossQ:12.3338 dlossQsigm:0.1085\n",
      "Episode:589 meanR:101.0100 rate:0.1620 gloss:0.0006 dloss:12.7018 dlossQ:12.6038 dlossQsigm:0.0981\n",
      "Episode:590 meanR:100.7700 rate:0.1080 gloss:0.0214 dloss:15.4003 dlossQ:15.2711 dlossQsigm:0.1292\n",
      "Episode:591 meanR:100.9600 rate:0.2180 gloss:0.0004 dloss:6.9470 dlossQ:6.8750 dlossQsigm:0.0719\n",
      "Episode:592 meanR:100.7700 rate:0.1480 gloss:0.0010 dloss:6.8648 dlossQ:6.7930 dlossQsigm:0.0717\n",
      "Episode:593 meanR:100.5700 rate:0.1740 gloss:0.0002 dloss:5.5943 dlossQ:5.5336 dlossQsigm:0.0606\n",
      "Episode:594 meanR:100.1400 rate:0.1740 gloss:0.0165 dloss:6.6473 dlossQ:6.5848 dlossQsigm:0.0625\n",
      "Episode:595 meanR:100.7600 rate:0.3200 gloss:0.0001 dloss:6.2551 dlossQ:6.1758 dlossQsigm:0.0793\n",
      "Episode:596 meanR:100.6200 rate:0.0980 gloss:0.0016 dloss:17.7550 dlossQ:17.5906 dlossQsigm:0.1644\n",
      "Episode:597 meanR:100.1000 rate:0.1320 gloss:0.0002 dloss:8.2707 dlossQ:8.1797 dlossQsigm:0.0910\n",
      "Episode:598 meanR:99.7000 rate:0.1400 gloss:0.0009 dloss:6.2198 dlossQ:6.1477 dlossQsigm:0.0721\n",
      "Episode:599 meanR:99.7700 rate:0.1420 gloss:0.0010 dloss:8.7907 dlossQ:8.7227 dlossQsigm:0.0680\n",
      "Episode:600 meanR:99.9400 rate:0.1780 gloss:0.0000 dloss:5.0871 dlossQ:5.0357 dlossQsigm:0.0514\n",
      "Episode:601 meanR:99.6000 rate:0.1720 gloss:0.0010 dloss:9.8426 dlossQ:9.7485 dlossQsigm:0.0941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:602 meanR:99.5500 rate:0.1560 gloss:0.0000 dloss:8.5958 dlossQ:8.5019 dlossQsigm:0.0940\n",
      "Episode:603 meanR:99.2000 rate:0.1040 gloss:0.0019 dloss:10.8313 dlossQ:10.6979 dlossQsigm:0.1334\n",
      "Episode:604 meanR:99.9000 rate:0.2560 gloss:0.0016 dloss:6.5268 dlossQ:6.4679 dlossQsigm:0.0589\n",
      "Episode:605 meanR:98.7200 rate:0.1360 gloss:0.0001 dloss:5.6171 dlossQ:5.5698 dlossQsigm:0.0472\n",
      "Episode:606 meanR:98.4800 rate:0.2020 gloss:0.0001 dloss:12.9708 dlossQ:12.8798 dlossQsigm:0.0909\n",
      "Episode:607 meanR:97.7200 rate:0.1400 gloss:0.0006 dloss:8.4768 dlossQ:8.3967 dlossQsigm:0.0801\n",
      "Episode:608 meanR:96.9200 rate:0.1600 gloss:0.0000 dloss:6.4930 dlossQ:6.4178 dlossQsigm:0.0752\n",
      "Episode:609 meanR:96.6600 rate:0.1680 gloss:0.0063 dloss:11.9371 dlossQ:11.8650 dlossQsigm:0.0721\n",
      "Episode:610 meanR:96.3000 rate:0.1380 gloss:0.0009 dloss:6.2889 dlossQ:6.2538 dlossQsigm:0.0351\n",
      "Episode:611 meanR:96.3400 rate:0.1760 gloss:0.0004 dloss:6.8410 dlossQ:6.7724 dlossQsigm:0.0687\n",
      "Episode:612 meanR:94.9900 rate:0.0960 gloss:0.0033 dloss:14.9816 dlossQ:14.8253 dlossQsigm:0.1563\n",
      "Episode:613 meanR:95.0500 rate:0.2780 gloss:0.0000 dloss:8.5673 dlossQ:8.4900 dlossQsigm:0.0773\n",
      "Episode:614 meanR:95.0300 rate:0.1620 gloss:0.0010 dloss:8.4357 dlossQ:8.3503 dlossQsigm:0.0854\n",
      "Episode:615 meanR:94.4300 rate:0.1840 gloss:0.0000 dloss:9.9587 dlossQ:9.8663 dlossQsigm:0.0924\n",
      "Episode:616 meanR:94.5600 rate:0.1320 gloss:0.0000 dloss:9.0358 dlossQ:8.9546 dlossQsigm:0.0812\n",
      "Episode:617 meanR:94.3200 rate:0.1560 gloss:0.0001 dloss:9.6145 dlossQ:9.5158 dlossQsigm:0.0986\n",
      "Episode:618 meanR:94.5600 rate:0.1680 gloss:0.0000 dloss:5.8089 dlossQ:5.7367 dlossQsigm:0.0722\n",
      "Episode:619 meanR:94.5600 rate:0.1280 gloss:0.0000 dloss:12.1889 dlossQ:12.1154 dlossQsigm:0.0735\n",
      "Episode:620 meanR:94.2600 rate:0.1480 gloss:0.0006 dloss:6.2177 dlossQ:6.1396 dlossQsigm:0.0781\n",
      "Episode:621 meanR:93.2800 rate:0.1440 gloss:0.0006 dloss:8.8140 dlossQ:8.7686 dlossQsigm:0.0454\n",
      "Episode:622 meanR:93.3600 rate:0.1840 gloss:0.0007 dloss:10.8234 dlossQ:10.7716 dlossQsigm:0.0518\n",
      "Episode:623 meanR:93.5000 rate:0.1560 gloss:0.0006 dloss:12.4546 dlossQ:12.3963 dlossQsigm:0.0583\n",
      "Episode:624 meanR:93.3500 rate:0.1820 gloss:0.0000 dloss:8.9852 dlossQ:8.9054 dlossQsigm:0.0798\n",
      "Episode:625 meanR:93.2400 rate:0.1280 gloss:0.0005 dloss:19.1327 dlossQ:19.0577 dlossQsigm:0.0750\n",
      "Episode:626 meanR:92.0400 rate:0.1600 gloss:0.0003 dloss:11.4549 dlossQ:11.3488 dlossQsigm:0.1061\n",
      "Episode:627 meanR:92.1100 rate:0.1860 gloss:0.0001 dloss:10.0827 dlossQ:9.9853 dlossQsigm:0.0975\n",
      "Episode:628 meanR:90.4100 rate:0.1080 gloss:0.0010 dloss:14.0829 dlossQ:13.9598 dlossQsigm:0.1231\n",
      "Episode:629 meanR:90.3900 rate:0.1140 gloss:0.0056 dloss:8.7998 dlossQ:8.7035 dlossQsigm:0.0964\n",
      "Episode:630 meanR:90.2400 rate:0.1320 gloss:0.0060 dloss:6.4174 dlossQ:6.3911 dlossQsigm:0.0263\n",
      "Episode:631 meanR:90.2000 rate:0.1980 gloss:0.0001 dloss:5.4757 dlossQ:5.4053 dlossQsigm:0.0703\n",
      "Episode:632 meanR:90.0800 rate:0.1100 gloss:0.0000 dloss:11.1587 dlossQ:11.0662 dlossQsigm:0.0926\n",
      "Episode:633 meanR:90.0700 rate:0.1500 gloss:0.0003 dloss:6.0864 dlossQ:6.0685 dlossQsigm:0.0180\n",
      "Episode:634 meanR:89.8400 rate:0.1120 gloss:0.0004 dloss:9.3437 dlossQ:9.3054 dlossQsigm:0.0383\n",
      "Episode:635 meanR:90.0300 rate:0.1620 gloss:0.0001 dloss:12.6765 dlossQ:12.5691 dlossQsigm:0.1074\n",
      "Episode:636 meanR:90.2800 rate:0.2780 gloss:0.0000 dloss:10.9332 dlossQ:10.8286 dlossQsigm:0.1047\n",
      "Episode:637 meanR:90.2300 rate:0.2300 gloss:0.0004 dloss:5.8329 dlossQ:5.7840 dlossQsigm:0.0489\n",
      "Episode:638 meanR:90.1400 rate:0.1220 gloss:0.0000 dloss:17.8367 dlossQ:17.6295 dlossQsigm:0.2072\n",
      "Episode:639 meanR:90.1900 rate:0.1520 gloss:0.0015 dloss:4.2147 dlossQ:4.1746 dlossQsigm:0.0401\n",
      "Episode:640 meanR:90.1600 rate:0.1840 gloss:0.0000 dloss:8.4533 dlossQ:8.3590 dlossQsigm:0.0943\n",
      "Episode:641 meanR:91.9000 rate:0.4960 gloss:0.0001 dloss:9.8669 dlossQ:9.7715 dlossQsigm:0.0953\n",
      "Episode:642 meanR:91.8600 rate:0.1160 gloss:0.0000 dloss:12.9041 dlossQ:12.7442 dlossQsigm:0.1599\n",
      "Episode:643 meanR:91.6200 rate:0.1600 gloss:0.0151 dloss:14.5015 dlossQ:14.4075 dlossQsigm:0.0940\n",
      "Episode:644 meanR:92.2700 rate:0.2460 gloss:0.0006 dloss:16.4378 dlossQ:16.3453 dlossQsigm:0.0924\n",
      "Episode:645 meanR:92.1600 rate:0.1860 gloss:0.0004 dloss:11.8841 dlossQ:11.7838 dlossQsigm:0.1003\n",
      "Episode:646 meanR:92.2400 rate:0.1280 gloss:0.0010 dloss:14.4583 dlossQ:14.3558 dlossQsigm:0.1025\n",
      "Episode:647 meanR:92.2400 rate:0.1720 gloss:0.0064 dloss:10.6232 dlossQ:10.5234 dlossQsigm:0.0998\n",
      "Episode:648 meanR:92.0900 rate:0.1300 gloss:0.0221 dloss:10.4246 dlossQ:10.3287 dlossQsigm:0.0960\n",
      "Episode:649 meanR:90.2300 rate:0.2120 gloss:0.0001 dloss:6.6422 dlossQ:6.5889 dlossQsigm:0.0533\n",
      "Episode:650 meanR:90.8100 rate:0.2800 gloss:0.0000 dloss:18.4291 dlossQ:18.3006 dlossQsigm:0.1285\n",
      "Episode:651 meanR:90.9500 rate:0.1580 gloss:0.0075 dloss:16.3774 dlossQ:16.2537 dlossQsigm:0.1237\n",
      "Episode:652 meanR:91.3300 rate:0.2180 gloss:0.0007 dloss:15.7773 dlossQ:15.6507 dlossQsigm:0.1266\n",
      "Episode:653 meanR:91.5900 rate:0.2000 gloss:0.0023 dloss:15.1962 dlossQ:15.0792 dlossQsigm:0.1170\n",
      "Episode:654 meanR:91.5000 rate:0.1640 gloss:0.0005 dloss:10.7570 dlossQ:10.6549 dlossQsigm:0.1021\n",
      "Episode:655 meanR:91.6300 rate:0.2420 gloss:0.0007 dloss:19.0336 dlossQ:18.8921 dlossQsigm:0.1415\n",
      "Episode:656 meanR:91.7800 rate:0.1620 gloss:0.0175 dloss:16.4467 dlossQ:16.3169 dlossQsigm:0.1298\n",
      "Episode:657 meanR:92.8400 rate:0.3320 gloss:0.0003 dloss:33.2151 dlossQ:33.0330 dlossQsigm:0.1822\n",
      "Episode:658 meanR:91.4900 rate:0.1520 gloss:0.0020 dloss:11.4619 dlossQ:11.3737 dlossQsigm:0.0882\n",
      "Episode:659 meanR:91.5100 rate:0.2000 gloss:0.0000 dloss:6.6672 dlossQ:6.6141 dlossQsigm:0.0531\n",
      "Episode:660 meanR:91.7000 rate:0.2180 gloss:0.0006 dloss:11.1873 dlossQ:11.1109 dlossQsigm:0.0764\n",
      "Episode:661 meanR:92.1500 rate:0.2300 gloss:0.0001 dloss:33.8205 dlossQ:33.7254 dlossQsigm:0.0952\n",
      "Episode:662 meanR:92.5800 rate:0.2500 gloss:0.0029 dloss:10.0219 dlossQ:9.9236 dlossQsigm:0.0983\n",
      "Episode:663 meanR:92.5000 rate:0.1300 gloss:0.0006 dloss:7.2918 dlossQ:7.2185 dlossQsigm:0.0733\n",
      "Episode:664 meanR:92.5200 rate:0.2840 gloss:0.0076 dloss:18.7186 dlossQ:18.5865 dlossQsigm:0.1321\n",
      "Episode:665 meanR:94.1200 rate:0.4620 gloss:0.0010 dloss:11.8665 dlossQ:11.7616 dlossQsigm:0.1049\n",
      "Episode:666 meanR:94.2700 rate:0.1800 gloss:0.0008 dloss:14.8222 dlossQ:14.6986 dlossQsigm:0.1237\n",
      "Episode:667 meanR:93.3000 rate:0.1540 gloss:0.0004 dloss:9.4287 dlossQ:9.3837 dlossQsigm:0.0450\n",
      "Episode:668 meanR:94.4100 rate:0.3900 gloss:0.0009 dloss:21.4524 dlossQ:21.3053 dlossQsigm:0.1471\n",
      "Episode:669 meanR:92.9000 rate:0.1160 gloss:0.0007 dloss:44.0709 dlossQ:43.8181 dlossQsigm:0.2527\n",
      "Episode:670 meanR:92.5200 rate:0.2420 gloss:0.0012 dloss:6.4760 dlossQ:6.4029 dlossQsigm:0.0731\n",
      "Episode:671 meanR:93.2400 rate:0.3460 gloss:0.0001 dloss:11.7979 dlossQ:11.6959 dlossQsigm:0.1020\n",
      "Episode:672 meanR:93.1900 rate:0.1720 gloss:0.0013 dloss:9.3411 dlossQ:9.2710 dlossQsigm:0.0700\n",
      "Episode:673 meanR:94.0000 rate:0.2820 gloss:0.0086 dloss:11.2364 dlossQ:11.1370 dlossQsigm:0.0994\n",
      "Episode:674 meanR:94.5500 rate:0.3400 gloss:0.0002 dloss:17.2395 dlossQ:17.1245 dlossQsigm:0.1150\n",
      "Episode:675 meanR:94.5100 rate:0.1520 gloss:0.0004 dloss:14.8790 dlossQ:14.7858 dlossQsigm:0.0932\n",
      "Episode:676 meanR:94.2100 rate:0.1500 gloss:0.0023 dloss:16.2360 dlossQ:16.1479 dlossQsigm:0.0881\n",
      "Episode:677 meanR:94.2600 rate:0.1480 gloss:0.0049 dloss:20.9445 dlossQ:20.8148 dlossQsigm:0.1297\n",
      "Episode:678 meanR:95.1300 rate:0.5000 gloss:0.0003 dloss:20.1187 dlossQ:19.9791 dlossQsigm:0.1396\n",
      "Episode:679 meanR:97.2800 rate:0.5560 gloss:0.0002 dloss:10.8211 dlossQ:10.7505 dlossQsigm:0.0706\n",
      "Episode:680 meanR:97.4300 rate:0.1960 gloss:0.0001 dloss:10.6166 dlossQ:10.5210 dlossQsigm:0.0956\n",
      "Episode:681 meanR:96.5400 rate:0.1280 gloss:0.0061 dloss:14.7740 dlossQ:14.6908 dlossQsigm:0.0832\n",
      "Episode:682 meanR:96.3500 rate:0.1220 gloss:0.0006 dloss:14.0578 dlossQ:13.8946 dlossQsigm:0.1632\n",
      "Episode:683 meanR:96.3000 rate:0.1260 gloss:0.0046 dloss:12.6750 dlossQ:12.5071 dlossQsigm:0.1679\n",
      "Episode:684 meanR:96.2800 rate:0.1520 gloss:0.0005 dloss:8.2945 dlossQ:8.2008 dlossQsigm:0.0937\n",
      "Episode:685 meanR:96.3400 rate:0.2080 gloss:0.0003 dloss:6.0421 dlossQ:5.9798 dlossQsigm:0.0623\n",
      "Episode:686 meanR:97.0000 rate:0.3300 gloss:0.0001 dloss:12.1961 dlossQ:12.0854 dlossQsigm:0.1107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:687 meanR:97.3100 rate:0.1940 gloss:0.0128 dloss:18.5430 dlossQ:18.4079 dlossQsigm:0.1351\n",
      "Episode:688 meanR:96.8300 rate:0.2080 gloss:0.0003 dloss:9.6652 dlossQ:9.5730 dlossQsigm:0.0922\n",
      "Episode:689 meanR:96.7100 rate:0.1380 gloss:0.0022 dloss:17.3375 dlossQ:17.2099 dlossQsigm:0.1276\n",
      "Episode:690 meanR:96.8700 rate:0.1400 gloss:0.0007 dloss:18.8161 dlossQ:18.6793 dlossQsigm:0.1368\n",
      "Episode:691 meanR:97.5500 rate:0.3540 gloss:0.0000 dloss:12.2507 dlossQ:12.1437 dlossQsigm:0.1070\n",
      "Episode:692 meanR:100.7600 rate:0.7900 gloss:0.0001 dloss:11.5909 dlossQ:11.4807 dlossQsigm:0.1102\n",
      "Episode:693 meanR:100.8900 rate:0.2000 gloss:0.0074 dloss:17.5782 dlossQ:17.4534 dlossQsigm:0.1248\n",
      "Episode:694 meanR:101.5000 rate:0.2960 gloss:0.0000 dloss:19.9587 dlossQ:19.8100 dlossQsigm:0.1487\n",
      "Episode:695 meanR:100.5900 rate:0.1380 gloss:0.0006 dloss:16.2150 dlossQ:16.1324 dlossQsigm:0.0826\n",
      "Episode:696 meanR:101.0400 rate:0.1880 gloss:0.0005 dloss:14.5490 dlossQ:14.4627 dlossQsigm:0.0863\n",
      "Episode:697 meanR:101.0500 rate:0.1340 gloss:0.0015 dloss:12.4418 dlossQ:12.3643 dlossQsigm:0.0775\n",
      "Episode:698 meanR:101.4300 rate:0.2160 gloss:0.0000 dloss:24.3990 dlossQ:24.2713 dlossQsigm:0.1277\n",
      "Episode:699 meanR:102.3700 rate:0.3300 gloss:0.0000 dloss:17.3600 dlossQ:17.2370 dlossQsigm:0.1229\n",
      "Episode:700 meanR:103.0100 rate:0.3060 gloss:0.0000 dloss:11.2703 dlossQ:11.1728 dlossQsigm:0.0976\n",
      "Episode:701 meanR:104.0400 rate:0.3780 gloss:0.0093 dloss:17.5689 dlossQ:17.4568 dlossQsigm:0.1121\n",
      "Episode:702 meanR:105.1700 rate:0.3820 gloss:0.0001 dloss:8.0354 dlossQ:7.9619 dlossQsigm:0.0736\n",
      "Episode:703 meanR:105.6400 rate:0.1980 gloss:0.0009 dloss:7.2747 dlossQ:7.2209 dlossQsigm:0.0538\n",
      "Episode:704 meanR:105.3100 rate:0.1900 gloss:0.0007 dloss:4.8605 dlossQ:4.8430 dlossQsigm:0.0176\n",
      "Episode:705 meanR:106.7800 rate:0.4300 gloss:0.0000 dloss:5.3744 dlossQ:5.3339 dlossQsigm:0.0405\n",
      "Episode:706 meanR:106.6700 rate:0.1800 gloss:0.0024 dloss:13.9102 dlossQ:13.8085 dlossQsigm:0.1017\n",
      "Episode:707 meanR:107.1000 rate:0.2260 gloss:0.0001 dloss:9.0339 dlossQ:8.9789 dlossQsigm:0.0550\n",
      "Episode:708 meanR:107.5200 rate:0.2440 gloss:0.0011 dloss:26.2331 dlossQ:26.0773 dlossQsigm:0.1559\n",
      "Episode:709 meanR:107.6700 rate:0.1980 gloss:0.0024 dloss:17.7290 dlossQ:17.6028 dlossQsigm:0.1262\n",
      "Episode:710 meanR:108.9200 rate:0.3880 gloss:0.0004 dloss:21.8101 dlossQ:21.6668 dlossQsigm:0.1433\n",
      "Episode:711 meanR:109.0700 rate:0.2060 gloss:0.0013 dloss:15.3185 dlossQ:15.2275 dlossQsigm:0.0910\n",
      "Episode:712 meanR:110.5700 rate:0.3960 gloss:0.0000 dloss:10.9339 dlossQ:10.8502 dlossQsigm:0.0837\n",
      "Episode:713 meanR:109.8200 rate:0.1280 gloss:0.4703 dloss:29.2071 dlossQ:29.1091 dlossQsigm:0.0980\n",
      "Episode:714 meanR:109.8000 rate:0.1580 gloss:0.0010 dloss:13.9789 dlossQ:13.8749 dlossQsigm:0.1040\n",
      "Episode:715 meanR:109.6100 rate:0.1460 gloss:0.0084 dloss:13.6039 dlossQ:13.4961 dlossQsigm:0.1078\n",
      "Episode:716 meanR:112.3200 rate:0.6740 gloss:0.0011 dloss:18.8694 dlossQ:18.7523 dlossQsigm:0.1171\n",
      "Episode:717 meanR:112.2300 rate:0.1380 gloss:0.0004 dloss:10.7028 dlossQ:10.6075 dlossQsigm:0.0953\n",
      "Episode:718 meanR:112.0800 rate:0.1380 gloss:0.0041 dloss:10.1544 dlossQ:10.0700 dlossQsigm:0.0845\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "rates_list, d_lossQ_list, d_lossQsigm_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        state = env.reset() # env first state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            g_loss, d_loss, d_lossQ, d_lossQsigm, _, _ = sess.run(\n",
    "                fetches=[model.g_loss, model.d_loss, \n",
    "                         model.d_lossQ, model.d_lossQ_sigm,\n",
    "                         model.g_opt, model.d_opt], \n",
    "                feed_dict = {model.states: states, model.actions: actions,\n",
    "                             model.targetQs: targetQs,\n",
    "                             model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                             model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        # Episode total reward and success rate/prob\n",
    "        episode_reward.append(total_reward) # stopping criteria\n",
    "        rate = total_reward/ 500 # success is 500 points: 0-1\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss),\n",
    "              'dlossQ:{:.4f}'.format(d_lossQ),\n",
    "              'dlossQsigm:{:.4f}'.format(d_lossQsigm))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rates_list.append([ep, rate])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        d_lossQ_list.append([ep, d_lossQ])\n",
    "        d_lossQsigm_list.append([ep, d_lossQsigm])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-seq2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossR_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(100):\n",
    "    #while True:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        #for _ in range(111111111111111111):\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Print and break condition\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "        # if total_reward == 500:\n",
    "        #     break\n",
    "                \n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
