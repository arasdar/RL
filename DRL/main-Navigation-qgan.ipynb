{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# for steps in range(1111111):\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         print(state.shape)\n",
    "#         break\n",
    "    \n",
    "# print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     #print(state)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# batch = []\n",
    "# while True: # infinite number of steps\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     #print(state, action, reward, done)\n",
    "#     batch.append([action, state, reward, done])\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    # rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('actor', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size,\n",
    "               states, actions, next_states, dones, rates):\n",
    "    actions_logits = actor(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ###############################################\n",
    "    next_states_logits = generator(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                   action_size=action_size, state_size=state_size)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    aloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, \n",
    "                                                                    labels=next_states_labels))\n",
    "    ####################################################\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, action_size=action_size)\n",
    "    #rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    # dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "    #                                                                labels=rates)) # 0-1\n",
    "    dQs = tf.nn.tanh(tf.reshape(dQs, shape=[-1]))\n",
    "    dloss = tf.reduce_mean(tf.square(dQs-rates)) # [-1, +1]\n",
    "    ####################################################\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states, action_size=action_size, \n",
    "                        reuse=True)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs))) # 0-1\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.ones_like(gQs))) # 0-1\n",
    "    #####################################################\n",
    "    next_actions_logits = actor(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    gQs2 = discriminator(actions=next_actions_logits, hidden_size=hidden_size, states=next_states, \n",
    "                         action_size=action_size, reuse=True)\n",
    "    gQs2 = tf.reshape(gQs2, shape=[-1]) * (1-dones)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs2))) # 0-1\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                     labels=tf.ones_like(gQs2))) # 0-1\n",
    "    return actions_logits, aloss, dloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, d_loss, a_loss2, a_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('actor')]\n",
    "    #g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, d_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.dones, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.d_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.d_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        d_loss=self.d_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 'continuous', 4, 'discrete')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_observation_space_type, \\\n",
    "brain.vector_action_space_size, brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "# gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              a_learning_rate=a_learning_rate, \n",
    "              d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        # state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-2.0000 R:-2.0000 rate:-0.1538 aloss:2.2151 dloss:1.4167 aloss2:1.5538 exploreP:0.9707\n",
      "Episode:1 meanR:-0.5000 R:1.0000 rate:0.0769 aloss:2.1404 dloss:1.2774 aloss2:1.8209 exploreP:0.9423\n",
      "Episode:2 meanR:-1.0000 R:-2.0000 rate:-0.1538 aloss:2.1091 dloss:1.2442 aloss2:1.8236 exploreP:0.9148\n",
      "Episode:3 meanR:-1.7500 R:-4.0000 rate:-0.3077 aloss:2.1278 dloss:1.2428 aloss2:1.8251 exploreP:0.8881\n",
      "Episode:4 meanR:-1.2000 R:1.0000 rate:0.0769 aloss:2.1899 dloss:1.2582 aloss2:1.8251 exploreP:0.8621\n",
      "Episode:5 meanR:-1.3333 R:-2.0000 rate:-0.1538 aloss:2.1781 dloss:1.2575 aloss2:1.8259 exploreP:0.8369\n",
      "Episode:6 meanR:-1.0000 R:1.0000 rate:0.0769 aloss:2.1630 dloss:1.2529 aloss2:1.8229 exploreP:0.8125\n",
      "Episode:7 meanR:-0.8750 R:0.0000 rate:0.0000 aloss:2.1727 dloss:1.2443 aloss2:1.8380 exploreP:0.7888\n",
      "Episode:8 meanR:-0.8889 R:-1.0000 rate:-0.0769 aloss:2.1818 dloss:1.2490 aloss2:1.8364 exploreP:0.7657\n",
      "Episode:9 meanR:-0.7000 R:1.0000 rate:0.0769 aloss:2.1780 dloss:1.2486 aloss2:1.8335 exploreP:0.7434\n",
      "Episode:10 meanR:-0.6364 R:0.0000 rate:0.0000 aloss:2.1766 dloss:1.2322 aloss2:1.8531 exploreP:0.7217\n",
      "Episode:11 meanR:-0.3333 R:3.0000 rate:0.2308 aloss:2.1762 dloss:1.2314 aloss2:1.8518 exploreP:0.7007\n",
      "Episode:12 meanR:-0.3077 R:0.0000 rate:0.0000 aloss:2.1992 dloss:1.2599 aloss2:1.8279 exploreP:0.6803\n",
      "Episode:13 meanR:-0.2143 R:1.0000 rate:0.0769 aloss:2.1655 dloss:1.2311 aloss2:1.8467 exploreP:0.6605\n",
      "Episode:14 meanR:-0.2000 R:0.0000 rate:0.0000 aloss:2.1812 dloss:1.2173 aloss2:1.8655 exploreP:0.6413\n",
      "Episode:15 meanR:-0.1250 R:1.0000 rate:0.0769 aloss:2.1965 dloss:1.2447 aloss2:1.8440 exploreP:0.6226\n",
      "Episode:16 meanR:-0.1176 R:0.0000 rate:0.0000 aloss:2.1787 dloss:1.2294 aloss2:1.8548 exploreP:0.6045\n",
      "Episode:17 meanR:-0.0556 R:1.0000 rate:0.0769 aloss:2.1855 dloss:1.2101 aloss2:1.8873 exploreP:0.5869\n",
      "Episode:18 meanR:-0.0526 R:0.0000 rate:0.0000 aloss:2.1740 dloss:1.2285 aloss2:1.8569 exploreP:0.5699\n",
      "Episode:19 meanR:0.0500 R:2.0000 rate:0.1538 aloss:2.1697 dloss:1.2014 aloss2:1.8865 exploreP:0.5533\n",
      "Episode:20 meanR:0.0476 R:0.0000 rate:0.0000 aloss:2.1790 dloss:1.2127 aloss2:1.8760 exploreP:0.5373\n",
      "Episode:21 meanR:0.0909 R:1.0000 rate:0.0769 aloss:2.1808 dloss:1.2005 aloss2:1.8978 exploreP:0.5217\n",
      "Episode:22 meanR:0.1739 R:2.0000 rate:0.1538 aloss:2.1763 dloss:1.1987 aloss2:1.8872 exploreP:0.5066\n",
      "Episode:23 meanR:0.2500 R:2.0000 rate:0.1538 aloss:2.1730 dloss:1.1746 aloss2:1.9283 exploreP:0.4919\n",
      "Episode:24 meanR:0.2800 R:1.0000 rate:0.0769 aloss:2.1871 dloss:1.1905 aloss2:1.9087 exploreP:0.4776\n",
      "Episode:25 meanR:0.3077 R:1.0000 rate:0.0769 aloss:2.1849 dloss:1.1721 aloss2:1.9354 exploreP:0.4638\n",
      "Episode:26 meanR:0.3333 R:1.0000 rate:0.0769 aloss:2.1755 dloss:1.1860 aloss2:1.9115 exploreP:0.4504\n",
      "Episode:27 meanR:0.3571 R:1.0000 rate:0.0769 aloss:2.1533 dloss:1.1312 aloss2:1.9737 exploreP:0.4374\n",
      "Episode:28 meanR:0.3103 R:-1.0000 rate:-0.0769 aloss:2.1888 dloss:1.1516 aloss2:1.9654 exploreP:0.4248\n",
      "Episode:29 meanR:0.3000 R:0.0000 rate:0.0000 aloss:2.1757 dloss:1.1556 aloss2:1.9534 exploreP:0.4125\n",
      "Episode:30 meanR:0.2581 R:-1.0000 rate:-0.0769 aloss:2.1668 dloss:1.1250 aloss2:1.9953 exploreP:0.4006\n",
      "Episode:31 meanR:0.2500 R:0.0000 rate:0.0000 aloss:2.1573 dloss:1.1065 aloss2:2.0016 exploreP:0.3891\n",
      "Episode:32 meanR:0.3030 R:2.0000 rate:0.1538 aloss:2.1704 dloss:1.1161 aloss2:1.9964 exploreP:0.3779\n",
      "Episode:33 meanR:0.3235 R:1.0000 rate:0.0769 aloss:2.1836 dloss:1.1187 aloss2:1.9981 exploreP:0.3670\n",
      "Episode:34 meanR:0.3143 R:0.0000 rate:0.0000 aloss:2.1724 dloss:1.0940 aloss2:2.0324 exploreP:0.3564\n",
      "Episode:35 meanR:0.3056 R:0.0000 rate:0.0000 aloss:2.1965 dloss:1.1331 aloss2:1.9864 exploreP:0.3462\n",
      "Episode:36 meanR:0.3243 R:1.0000 rate:0.0769 aloss:2.1931 dloss:1.0924 aloss2:2.0393 exploreP:0.3363\n",
      "Episode:37 meanR:0.2368 R:-3.0000 rate:-0.2308 aloss:2.1833 dloss:1.0802 aloss2:2.1137 exploreP:0.3266\n",
      "Episode:38 meanR:0.2564 R:1.0000 rate:0.0769 aloss:2.1829 dloss:1.1068 aloss2:2.0417 exploreP:0.3173\n",
      "Episode:39 meanR:0.2250 R:-1.0000 rate:-0.0769 aloss:2.1730 dloss:1.0818 aloss2:2.0882 exploreP:0.3082\n",
      "Episode:40 meanR:0.3171 R:4.0000 rate:0.3077 aloss:2.1720 dloss:1.0695 aloss2:2.1231 exploreP:0.2994\n",
      "Episode:41 meanR:0.3333 R:1.0000 rate:0.0769 aloss:2.1626 dloss:1.0555 aloss2:2.1696 exploreP:0.2908\n",
      "Episode:42 meanR:0.3256 R:0.0000 rate:0.0000 aloss:2.1602 dloss:1.0504 aloss2:2.2014 exploreP:0.2825\n",
      "Episode:43 meanR:0.3182 R:0.0000 rate:0.0000 aloss:2.1622 dloss:1.0246 aloss2:2.2408 exploreP:0.2745\n",
      "Episode:44 meanR:0.2889 R:-1.0000 rate:-0.0769 aloss:2.1603 dloss:1.0297 aloss2:2.2523 exploreP:0.2666\n",
      "Episode:45 meanR:0.2826 R:0.0000 rate:0.0000 aloss:2.1731 dloss:1.0422 aloss2:2.2252 exploreP:0.2591\n",
      "Episode:46 meanR:0.1915 R:-4.0000 rate:-0.3077 aloss:2.1758 dloss:1.0123 aloss2:2.3163 exploreP:0.2517\n",
      "Episode:47 meanR:0.1875 R:0.0000 rate:0.0000 aloss:2.1739 dloss:0.9712 aloss2:2.4350 exploreP:0.2446\n",
      "Episode:48 meanR:0.1837 R:0.0000 rate:0.0000 aloss:2.1557 dloss:0.9499 aloss2:2.4844 exploreP:0.2376\n",
      "Episode:49 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.1632 dloss:0.8902 aloss2:2.6536 exploreP:0.2309\n",
      "Episode:50 meanR:0.2157 R:2.0000 rate:0.1538 aloss:2.1911 dloss:1.0119 aloss2:2.6388 exploreP:0.2244\n",
      "Episode:51 meanR:0.2115 R:0.0000 rate:0.0000 aloss:2.1836 dloss:0.9998 aloss2:2.5467 exploreP:0.2180\n",
      "Episode:52 meanR:0.2264 R:1.0000 rate:0.0769 aloss:2.1851 dloss:0.8983 aloss2:2.7539 exploreP:0.2119\n",
      "Episode:53 meanR:0.2037 R:-1.0000 rate:-0.0769 aloss:2.1708 dloss:0.9039 aloss2:2.8990 exploreP:0.2059\n",
      "Episode:54 meanR:0.2000 R:0.0000 rate:0.0000 aloss:2.1694 dloss:0.9766 aloss2:2.4016 exploreP:0.2001\n",
      "Episode:55 meanR:0.2143 R:1.0000 rate:0.0769 aloss:2.1705 dloss:0.9014 aloss2:2.5282 exploreP:0.1945\n",
      "Episode:56 meanR:0.2456 R:2.0000 rate:0.1538 aloss:2.1760 dloss:0.8751 aloss2:2.8925 exploreP:0.1891\n",
      "Episode:57 meanR:0.2586 R:1.0000 rate:0.0769 aloss:2.1860 dloss:0.8810 aloss2:2.8091 exploreP:0.1838\n",
      "Episode:58 meanR:0.2542 R:0.0000 rate:0.0000 aloss:2.1553 dloss:0.8317 aloss2:2.8733 exploreP:0.1786\n",
      "Episode:59 meanR:0.2500 R:0.0000 rate:0.0000 aloss:2.1676 dloss:0.7931 aloss2:3.0928 exploreP:0.1736\n",
      "Episode:60 meanR:0.2295 R:-1.0000 rate:-0.0769 aloss:2.1666 dloss:0.7640 aloss2:3.2874 exploreP:0.1688\n",
      "Episode:61 meanR:0.2419 R:1.0000 rate:0.0769 aloss:2.1564 dloss:0.7607 aloss2:3.6118 exploreP:0.1641\n",
      "Episode:62 meanR:0.2540 R:1.0000 rate:0.0769 aloss:2.1737 dloss:0.9537 aloss2:2.7745 exploreP:0.1596\n",
      "Episode:63 meanR:0.2656 R:1.0000 rate:0.0769 aloss:2.1585 dloss:0.8994 aloss2:2.8450 exploreP:0.1551\n",
      "Episode:64 meanR:0.2769 R:1.0000 rate:0.0769 aloss:2.1580 dloss:0.8420 aloss2:3.1327 exploreP:0.1509\n",
      "Episode:65 meanR:0.2727 R:0.0000 rate:0.0000 aloss:2.1424 dloss:0.8040 aloss2:3.5006 exploreP:0.1467\n",
      "Episode:66 meanR:0.2687 R:0.0000 rate:0.0000 aloss:2.1522 dloss:0.7348 aloss2:3.8173 exploreP:0.1426\n",
      "Episode:67 meanR:0.2500 R:-1.0000 rate:-0.0769 aloss:2.1859 dloss:1.0190 aloss2:3.4013 exploreP:0.1387\n",
      "Episode:68 meanR:0.2464 R:0.0000 rate:0.0000 aloss:2.2037 dloss:1.2000 aloss2:4.0010 exploreP:0.1349\n",
      "Episode:69 meanR:0.2286 R:-1.0000 rate:-0.0769 aloss:2.2052 dloss:1.2890 aloss2:6.5935 exploreP:0.1312\n",
      "Episode:70 meanR:0.2254 R:0.0000 rate:0.0000 aloss:2.1686 dloss:1.1988 aloss2:9.8836 exploreP:0.1276\n",
      "Episode:71 meanR:0.2222 R:0.0000 rate:0.0000 aloss:2.1724 dloss:1.1993 aloss2:11.6336 exploreP:0.1242\n",
      "Episode:72 meanR:0.2192 R:0.0000 rate:0.0000 aloss:2.1583 dloss:1.2027 aloss2:12.7460 exploreP:0.1208\n",
      "Episode:73 meanR:0.2162 R:0.0000 rate:0.0000 aloss:2.1373 dloss:1.1984 aloss2:13.5875 exploreP:0.1175\n",
      "Episode:74 meanR:0.2133 R:0.0000 rate:0.0000 aloss:2.1294 dloss:1.1924 aloss2:14.2975 exploreP:0.1143\n",
      "Episode:75 meanR:0.2105 R:0.0000 rate:0.0000 aloss:2.1204 dloss:1.2066 aloss2:14.8642 exploreP:0.1113\n",
      "Episode:76 meanR:0.2078 R:0.0000 rate:0.0000 aloss:2.1059 dloss:1.1897 aloss2:15.3715 exploreP:0.1083\n",
      "Episode:77 meanR:0.1923 R:-1.0000 rate:-0.0769 aloss:2.1202 dloss:1.1887 aloss2:15.8731 exploreP:0.1054\n",
      "Episode:78 meanR:0.1899 R:0.0000 rate:0.0000 aloss:2.1143 dloss:1.1955 aloss2:16.2797 exploreP:0.1025\n",
      "Episode:79 meanR:0.1875 R:0.0000 rate:0.0000 aloss:2.1040 dloss:1.1971 aloss2:16.7411 exploreP:0.0998\n",
      "Episode:80 meanR:0.1852 R:0.0000 rate:0.0000 aloss:2.1011 dloss:1.1888 aloss2:17.1902 exploreP:0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:81 meanR:0.1829 R:0.0000 rate:0.0000 aloss:2.0954 dloss:1.1966 aloss2:17.5732 exploreP:0.0946\n",
      "Episode:82 meanR:0.1687 R:-1.0000 rate:-0.0769 aloss:2.1020 dloss:1.2042 aloss2:18.0006 exploreP:0.0921\n",
      "Episode:83 meanR:0.1667 R:0.0000 rate:0.0000 aloss:2.0957 dloss:1.1912 aloss2:18.3607 exploreP:0.0897\n",
      "Episode:84 meanR:0.1529 R:-1.0000 rate:-0.0769 aloss:2.1035 dloss:1.1962 aloss2:18.6704 exploreP:0.0873\n",
      "Episode:85 meanR:0.1512 R:0.0000 rate:0.0000 aloss:2.0986 dloss:1.1820 aloss2:19.1553 exploreP:0.0850\n",
      "Episode:86 meanR:0.1494 R:0.0000 rate:0.0000 aloss:2.0924 dloss:1.1966 aloss2:19.4838 exploreP:0.0828\n",
      "Episode:87 meanR:0.1477 R:0.0000 rate:0.0000 aloss:2.0838 dloss:1.1781 aloss2:19.8512 exploreP:0.0806\n",
      "Episode:88 meanR:0.1348 R:-1.0000 rate:-0.0769 aloss:2.0863 dloss:1.1897 aloss2:20.2636 exploreP:0.0786\n",
      "Episode:89 meanR:0.1333 R:0.0000 rate:0.0000 aloss:2.0860 dloss:1.2070 aloss2:20.6202 exploreP:0.0765\n",
      "Episode:90 meanR:0.1319 R:0.0000 rate:0.0000 aloss:2.0845 dloss:1.2004 aloss2:21.0000 exploreP:0.0746\n",
      "Episode:91 meanR:0.1304 R:0.0000 rate:0.0000 aloss:2.0826 dloss:1.1719 aloss2:21.4043 exploreP:0.0727\n",
      "Episode:92 meanR:0.1290 R:0.0000 rate:0.0000 aloss:2.0814 dloss:1.1894 aloss2:21.7674 exploreP:0.0708\n",
      "Episode:93 meanR:0.1170 R:-1.0000 rate:-0.0769 aloss:2.0819 dloss:1.1946 aloss2:22.1761 exploreP:0.0690\n",
      "Episode:94 meanR:0.1158 R:0.0000 rate:0.0000 aloss:2.0839 dloss:1.1918 aloss2:22.5571 exploreP:0.0673\n",
      "Episode:95 meanR:0.1146 R:0.0000 rate:0.0000 aloss:2.0823 dloss:1.1996 aloss2:22.9837 exploreP:0.0656\n",
      "Episode:96 meanR:0.1237 R:1.0000 rate:0.0769 aloss:2.0831 dloss:1.1910 aloss2:23.3156 exploreP:0.0639\n",
      "Episode:97 meanR:0.1224 R:0.0000 rate:0.0000 aloss:2.0807 dloss:1.1625 aloss2:23.6813 exploreP:0.0623\n",
      "Episode:98 meanR:0.1111 R:-1.0000 rate:-0.0769 aloss:2.0824 dloss:1.1948 aloss2:24.0858 exploreP:0.0608\n",
      "Episode:99 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0758 dloss:1.1886 aloss2:24.4768 exploreP:0.0593\n",
      "Episode:100 meanR:0.1300 R:0.0000 rate:0.0000 aloss:2.0762 dloss:1.1857 aloss2:24.7500 exploreP:0.0578\n",
      "Episode:101 meanR:0.1200 R:0.0000 rate:0.0000 aloss:2.0774 dloss:1.1765 aloss2:25.2485 exploreP:0.0564\n",
      "Episode:102 meanR:0.1400 R:0.0000 rate:0.0000 aloss:2.0758 dloss:1.1807 aloss2:25.5927 exploreP:0.0550\n",
      "Episode:103 meanR:0.1900 R:1.0000 rate:0.0769 aloss:2.0762 dloss:1.1664 aloss2:25.9568 exploreP:0.0537\n",
      "Episode:104 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.0778 dloss:1.1691 aloss2:26.3949 exploreP:0.0524\n",
      "Episode:105 meanR:0.2000 R:0.0000 rate:0.0000 aloss:2.0754 dloss:1.1939 aloss2:26.7034 exploreP:0.0512\n",
      "Episode:106 meanR:0.1800 R:-1.0000 rate:-0.0769 aloss:2.0761 dloss:1.1846 aloss2:27.1830 exploreP:0.0500\n",
      "Episode:107 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.0729 dloss:1.1775 aloss2:27.4169 exploreP:0.0488\n",
      "Episode:108 meanR:0.2000 R:1.0000 rate:0.0769 aloss:2.0756 dloss:1.1957 aloss2:27.8680 exploreP:0.0476\n",
      "Episode:109 meanR:0.1900 R:0.0000 rate:0.0000 aloss:2.0750 dloss:1.1702 aloss2:28.1798 exploreP:0.0465\n",
      "Episode:110 meanR:0.2000 R:1.0000 rate:0.0769 aloss:2.0770 dloss:1.1925 aloss2:28.6150 exploreP:0.0454\n",
      "Episode:111 meanR:0.1700 R:0.0000 rate:0.0000 aloss:2.0698 dloss:1.1745 aloss2:28.9042 exploreP:0.0444\n",
      "Episode:112 meanR:0.1700 R:0.0000 rate:0.0000 aloss:2.0774 dloss:1.1805 aloss2:29.4199 exploreP:0.0434\n",
      "Episode:113 meanR:0.1600 R:0.0000 rate:0.0000 aloss:2.0773 dloss:1.1908 aloss2:29.6680 exploreP:0.0424\n",
      "Episode:114 meanR:0.1600 R:0.0000 rate:0.0000 aloss:2.0748 dloss:1.1958 aloss2:29.9976 exploreP:0.0414\n",
      "Episode:115 meanR:0.1400 R:-1.0000 rate:-0.0769 aloss:2.0732 dloss:1.1921 aloss2:30.4602 exploreP:0.0405\n",
      "Episode:116 meanR:0.1400 R:0.0000 rate:0.0000 aloss:2.0756 dloss:1.1994 aloss2:30.7933 exploreP:0.0396\n",
      "Episode:117 meanR:0.1300 R:0.0000 rate:0.0000 aloss:2.0772 dloss:1.1727 aloss2:31.2312 exploreP:0.0387\n",
      "Episode:118 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:2.0725 dloss:1.1847 aloss2:31.4956 exploreP:0.0379\n",
      "Episode:119 meanR:0.1100 R:1.0000 rate:0.0769 aloss:2.0780 dloss:1.1798 aloss2:31.7857 exploreP:0.0371\n",
      "Episode:120 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0768 dloss:1.1800 aloss2:32.2218 exploreP:0.0363\n",
      "Episode:121 meanR:0.0900 R:-1.0000 rate:-0.0769 aloss:2.0779 dloss:1.1835 aloss2:32.6222 exploreP:0.0355\n",
      "Episode:122 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0797 dloss:1.1839 aloss2:32.9700 exploreP:0.0347\n",
      "Episode:123 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:2.0779 dloss:1.1997 aloss2:33.2825 exploreP:0.0340\n",
      "Episode:124 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0775 dloss:1.1808 aloss2:33.6809 exploreP:0.0333\n",
      "Episode:125 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0779 dloss:1.1682 aloss2:34.0141 exploreP:0.0326\n",
      "Episode:126 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0790 dloss:1.1966 aloss2:34.4543 exploreP:0.0319\n",
      "Episode:127 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0792 dloss:1.1838 aloss2:34.7979 exploreP:0.0313\n",
      "Episode:128 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0803 dloss:1.1708 aloss2:35.1367 exploreP:0.0306\n",
      "Episode:129 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0798 dloss:1.1624 aloss2:35.5601 exploreP:0.0300\n",
      "Episode:130 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0813 dloss:1.1938 aloss2:35.9225 exploreP:0.0294\n",
      "Episode:131 meanR:0.0000 R:-2.0000 rate:-0.1538 aloss:2.0795 dloss:1.1758 aloss2:36.2484 exploreP:0.0289\n",
      "Episode:132 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0798 dloss:1.1843 aloss2:36.5944 exploreP:0.0283\n",
      "Episode:133 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0801 dloss:1.1827 aloss2:36.9002 exploreP:0.0278\n",
      "Episode:134 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0791 dloss:1.1534 aloss2:37.3193 exploreP:0.0272\n",
      "Episode:135 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0811 dloss:1.1686 aloss2:37.6402 exploreP:0.0267\n",
      "Episode:136 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:2.0817 dloss:1.1956 aloss2:37.9134 exploreP:0.0262\n",
      "Episode:137 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0810 dloss:1.1778 aloss2:38.3848 exploreP:0.0258\n",
      "Episode:138 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0779 dloss:1.1613 aloss2:38.6907 exploreP:0.0253\n",
      "Episode:139 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0804 dloss:1.1772 aloss2:39.0311 exploreP:0.0248\n",
      "Episode:140 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:2.0799 dloss:1.1552 aloss2:39.4278 exploreP:0.0244\n",
      "Episode:141 meanR:-0.0500 R:1.0000 rate:0.0769 aloss:2.0773 dloss:1.1610 aloss2:39.7278 exploreP:0.0240\n",
      "Episode:142 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:2.0790 dloss:1.1816 aloss2:40.0559 exploreP:0.0236\n",
      "Episode:143 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:2.0813 dloss:1.1457 aloss2:40.4345 exploreP:0.0232\n",
      "Episode:144 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:2.0823 dloss:1.1809 aloss2:40.7275 exploreP:0.0228\n",
      "Episode:145 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0794 dloss:1.1706 aloss2:41.0967 exploreP:0.0224\n",
      "Episode:146 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0746 dloss:1.1680 aloss2:41.4977 exploreP:0.0220\n",
      "Episode:147 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:2.0804 dloss:1.1646 aloss2:41.8345 exploreP:0.0217\n",
      "Episode:148 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0807 dloss:1.1548 aloss2:42.0923 exploreP:0.0213\n",
      "Episode:149 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0804 dloss:1.1586 aloss2:42.4816 exploreP:0.0210\n",
      "Episode:150 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0809 dloss:1.1596 aloss2:42.7701 exploreP:0.0207\n",
      "Episode:151 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0786 dloss:1.1566 aloss2:43.1065 exploreP:0.0204\n",
      "Episode:152 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:2.0789 dloss:1.1584 aloss2:43.3984 exploreP:0.0201\n",
      "Episode:153 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:2.0752 dloss:1.1739 aloss2:43.7975 exploreP:0.0198\n",
      "Episode:154 meanR:0.0100 R:2.0000 rate:0.1538 aloss:2.0779 dloss:1.1568 aloss2:44.1898 exploreP:0.0195\n",
      "Episode:155 meanR:0.0100 R:1.0000 rate:0.0769 aloss:2.0748 dloss:1.1567 aloss2:44.5232 exploreP:0.0192\n",
      "Episode:156 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0758 dloss:1.1614 aloss2:44.7910 exploreP:0.0189\n",
      "Episode:157 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0816 dloss:1.1718 aloss2:45.1164 exploreP:0.0187\n",
      "Episode:158 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0751 dloss:1.1458 aloss2:45.4284 exploreP:0.0184\n",
      "Episode:159 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:2.0808 dloss:1.1809 aloss2:45.8234 exploreP:0.0181\n",
      "Episode:160 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0755 dloss:1.1715 aloss2:46.0841 exploreP:0.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:161 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:2.0753 dloss:1.1754 aloss2:46.3478 exploreP:0.0177\n",
      "Episode:162 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0723 dloss:1.1547 aloss2:46.7277 exploreP:0.0174\n",
      "Episode:163 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:2.0739 dloss:1.1604 aloss2:46.9762 exploreP:0.0172\n",
      "Episode:164 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:2.0733 dloss:1.1644 aloss2:47.3104 exploreP:0.0170\n",
      "Episode:165 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:2.0682 dloss:1.1540 aloss2:47.5716 exploreP:0.0168\n",
      "Episode:166 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:2.0698 dloss:1.1635 aloss2:47.9356 exploreP:0.0166\n",
      "Episode:167 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:2.0679 dloss:1.1571 aloss2:48.0602 exploreP:0.0164\n",
      "Episode:168 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:2.0722 dloss:1.1538 aloss2:48.4956 exploreP:0.0162\n",
      "Episode:169 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0727 dloss:1.1563 aloss2:48.7045 exploreP:0.0160\n",
      "Episode:170 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0674 dloss:1.1652 aloss2:48.8842 exploreP:0.0159\n",
      "Episode:171 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:2.0694 dloss:1.1603 aloss2:49.2016 exploreP:0.0157\n",
      "Episode:172 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:2.0700 dloss:1.1591 aloss2:49.4662 exploreP:0.0155\n",
      "Episode:173 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0610 dloss:1.1719 aloss2:49.6360 exploreP:0.0154\n",
      "Episode:174 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0690 dloss:1.1645 aloss2:49.8818 exploreP:0.0152\n",
      "Episode:175 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0661 dloss:1.1581 aloss2:50.0748 exploreP:0.0150\n",
      "Episode:176 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0679 dloss:1.1605 aloss2:50.1997 exploreP:0.0149\n",
      "Episode:177 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0698 dloss:1.1456 aloss2:50.4929 exploreP:0.0147\n",
      "Episode:178 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0582 dloss:1.1575 aloss2:50.7337 exploreP:0.0146\n",
      "Episode:179 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0670 dloss:1.1533 aloss2:51.0005 exploreP:0.0145\n",
      "Episode:180 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0715 dloss:1.1606 aloss2:51.1530 exploreP:0.0143\n",
      "Episode:181 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.0626 dloss:1.1611 aloss2:51.2149 exploreP:0.0142\n",
      "Episode:182 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0638 dloss:1.1584 aloss2:51.5444 exploreP:0.0141\n",
      "Episode:183 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0540 dloss:1.1602 aloss2:51.6497 exploreP:0.0140\n",
      "Episode:184 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0644 dloss:1.1550 aloss2:51.8290 exploreP:0.0138\n",
      "Episode:185 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0530 dloss:1.1750 aloss2:51.9913 exploreP:0.0137\n",
      "Episode:186 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0525 dloss:1.1663 aloss2:52.0994 exploreP:0.0136\n",
      "Episode:187 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0584 dloss:1.1562 aloss2:52.4475 exploreP:0.0135\n",
      "Episode:188 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0522 dloss:1.1477 aloss2:52.5869 exploreP:0.0134\n",
      "Episode:189 meanR:0.0200 R:1.0000 rate:0.0769 aloss:2.0628 dloss:1.1709 aloss2:52.6281 exploreP:0.0133\n",
      "Episode:190 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0566 dloss:1.1608 aloss2:52.7398 exploreP:0.0132\n",
      "Episode:191 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0698 dloss:1.1556 aloss2:52.8614 exploreP:0.0131\n",
      "Episode:192 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0703 dloss:1.1480 aloss2:53.0141 exploreP:0.0130\n",
      "Episode:193 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0681 dloss:1.1724 aloss2:53.1038 exploreP:0.0129\n",
      "Episode:194 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0822 dloss:1.1565 aloss2:53.1102 exploreP:0.0129\n",
      "Episode:195 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0527 dloss:1.1541 aloss2:53.2929 exploreP:0.0128\n",
      "Episode:196 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1079 dloss:1.1721 aloss2:53.2900 exploreP:0.0127\n",
      "Episode:197 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1240 dloss:1.1605 aloss2:53.4970 exploreP:0.0126\n",
      "Episode:198 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.1084 dloss:1.1547 aloss2:53.5817 exploreP:0.0125\n",
      "Episode:199 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.1253 dloss:1.1793 aloss2:53.6602 exploreP:0.0125\n",
      "Episode:200 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.1096 dloss:1.1672 aloss2:53.6475 exploreP:0.0124\n",
      "Episode:201 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0598 dloss:1.1426 aloss2:53.9020 exploreP:0.0123\n",
      "Episode:202 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0829 dloss:1.1550 aloss2:53.9831 exploreP:0.0122\n",
      "Episode:203 meanR:0.0300 R:1.0000 rate:0.0769 aloss:2.0989 dloss:1.1557 aloss2:53.9493 exploreP:0.0122\n",
      "Episode:204 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0926 dloss:1.1435 aloss2:54.1349 exploreP:0.0121\n",
      "Episode:205 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0869 dloss:1.1584 aloss2:54.1312 exploreP:0.0120\n",
      "Episode:206 meanR:0.0500 R:1.0000 rate:0.0769 aloss:2.0743 dloss:1.1672 aloss2:54.3410 exploreP:0.0120\n",
      "Episode:207 meanR:0.0500 R:0.0000 rate:0.0000 aloss:2.0719 dloss:1.1425 aloss2:54.1911 exploreP:0.0119\n",
      "Episode:208 meanR:0.0400 R:0.0000 rate:0.0000 aloss:2.0547 dloss:1.1562 aloss2:54.4094 exploreP:0.0119\n",
      "Episode:209 meanR:0.0400 R:0.0000 rate:0.0000 aloss:2.0658 dloss:1.1789 aloss2:54.3684 exploreP:0.0118\n",
      "Episode:210 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0393 dloss:1.1497 aloss2:54.4375 exploreP:0.0118\n",
      "Episode:211 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0390 dloss:1.1393 aloss2:54.6900 exploreP:0.0117\n",
      "Episode:212 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0560 dloss:1.1472 aloss2:54.6922 exploreP:0.0117\n",
      "Episode:213 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:2.0908 dloss:1.1576 aloss2:54.5962 exploreP:0.0116\n",
      "Episode:214 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0597 dloss:1.1231 aloss2:54.7584 exploreP:0.0116\n",
      "Episode:215 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0344 dloss:1.1507 aloss2:54.8393 exploreP:0.0115\n",
      "Episode:216 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0387 dloss:1.1649 aloss2:54.9035 exploreP:0.0115\n",
      "Episode:217 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0594 dloss:1.1424 aloss2:54.8055 exploreP:0.0114\n",
      "Episode:218 meanR:0.0600 R:2.0000 rate:0.1538 aloss:2.0744 dloss:1.1326 aloss2:54.9843 exploreP:0.0114\n",
      "Episode:219 meanR:0.0600 R:1.0000 rate:0.0769 aloss:2.0533 dloss:1.1425 aloss2:54.8775 exploreP:0.0113\n",
      "Episode:220 meanR:0.0600 R:0.0000 rate:0.0000 aloss:2.0607 dloss:1.1237 aloss2:54.9430 exploreP:0.0113\n",
      "Episode:221 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0883 dloss:1.1280 aloss2:55.0558 exploreP:0.0113\n",
      "Episode:222 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0272 dloss:1.1446 aloss2:55.0966 exploreP:0.0112\n",
      "Episode:223 meanR:0.0800 R:0.0000 rate:0.0000 aloss:2.0557 dloss:1.1307 aloss2:55.0429 exploreP:0.0112\n",
      "Episode:224 meanR:0.0700 R:-1.0000 rate:-0.0769 aloss:2.0547 dloss:1.1255 aloss2:55.0693 exploreP:0.0112\n",
      "Episode:225 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0305 dloss:1.1348 aloss2:55.1914 exploreP:0.0111\n",
      "Episode:226 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0265 dloss:1.1454 aloss2:55.1781 exploreP:0.0111\n",
      "Episode:227 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0116 dloss:1.1489 aloss2:55.3732 exploreP:0.0111\n",
      "Episode:228 meanR:0.0700 R:0.0000 rate:0.0000 aloss:1.9971 dloss:1.1353 aloss2:55.3241 exploreP:0.0110\n",
      "Episode:229 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0337 dloss:1.1426 aloss2:55.3381 exploreP:0.0110\n",
      "Episode:230 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0237 dloss:1.1383 aloss2:55.4021 exploreP:0.0110\n",
      "Episode:231 meanR:0.1000 R:1.0000 rate:0.0769 aloss:2.0282 dloss:1.1500 aloss2:55.1788 exploreP:0.0109\n",
      "Episode:232 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0669 dloss:1.1449 aloss2:55.4692 exploreP:0.0109\n",
      "Episode:233 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0051 dloss:1.1486 aloss2:55.3724 exploreP:0.0109\n",
      "Episode:234 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0384 dloss:1.1368 aloss2:55.6292 exploreP:0.0109\n",
      "Episode:235 meanR:0.1100 R:1.0000 rate:0.0769 aloss:2.0556 dloss:1.1408 aloss2:55.5069 exploreP:0.0108\n",
      "Episode:236 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0405 dloss:1.1504 aloss2:55.6084 exploreP:0.0108\n",
      "Episode:237 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.9777 dloss:1.1290 aloss2:55.6504 exploreP:0.0108\n",
      "Episode:238 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0656 dloss:1.1655 aloss2:55.7573 exploreP:0.0108\n",
      "Episode:239 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.9966 dloss:1.1373 aloss2:55.6534 exploreP:0.0107\n",
      "Episode:240 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0229 dloss:1.1353 aloss2:55.8375 exploreP:0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:241 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0373 dloss:1.1233 aloss2:55.6546 exploreP:0.0107\n",
      "Episode:242 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0209 dloss:1.1392 aloss2:55.6685 exploreP:0.0107\n",
      "Episode:243 meanR:0.1000 R:1.0000 rate:0.0769 aloss:2.0027 dloss:1.1522 aloss2:55.6148 exploreP:0.0107\n",
      "Episode:244 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.9995 dloss:1.1327 aloss2:55.8490 exploreP:0.0106\n",
      "Episode:245 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.9598 dloss:1.1207 aloss2:55.8097 exploreP:0.0106\n",
      "Episode:246 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.9601 dloss:1.1445 aloss2:55.9311 exploreP:0.0106\n",
      "Episode:247 meanR:0.1000 R:0.0000 rate:0.0000 aloss:1.9863 dloss:1.1266 aloss2:55.6936 exploreP:0.0106\n",
      "Episode:248 meanR:0.1000 R:0.0000 rate:0.0000 aloss:2.0112 dloss:1.1450 aloss2:56.0562 exploreP:0.0106\n",
      "Episode:249 meanR:0.0900 R:-1.0000 rate:-0.0769 aloss:1.9816 dloss:1.1252 aloss2:55.9433 exploreP:0.0105\n",
      "Episode:250 meanR:0.0900 R:0.0000 rate:0.0000 aloss:2.0098 dloss:1.1354 aloss2:55.8881 exploreP:0.0105\n",
      "Episode:251 meanR:0.0900 R:0.0000 rate:0.0000 aloss:2.0210 dloss:1.1372 aloss2:55.8539 exploreP:0.0105\n",
      "Episode:252 meanR:0.0800 R:0.0000 rate:0.0000 aloss:1.9521 dloss:1.1276 aloss2:55.9406 exploreP:0.0105\n",
      "Episode:253 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.9578 dloss:1.1298 aloss2:56.1628 exploreP:0.0105\n",
      "Episode:254 meanR:0.0700 R:0.0000 rate:0.0000 aloss:1.9444 dloss:1.1267 aloss2:56.0104 exploreP:0.0105\n",
      "Episode:255 meanR:0.0600 R:0.0000 rate:0.0000 aloss:2.0237 dloss:1.1343 aloss2:56.0236 exploreP:0.0105\n",
      "Episode:256 meanR:0.0600 R:0.0000 rate:0.0000 aloss:2.0474 dloss:1.1519 aloss2:56.0776 exploreP:0.0104\n",
      "Episode:257 meanR:0.0500 R:-1.0000 rate:-0.0769 aloss:1.9469 dloss:1.1210 aloss2:55.9623 exploreP:0.0104\n",
      "Episode:258 meanR:0.0500 R:0.0000 rate:0.0000 aloss:2.0198 dloss:1.1363 aloss2:56.1394 exploreP:0.0104\n",
      "Episode:259 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.9426 dloss:1.1264 aloss2:56.1612 exploreP:0.0104\n",
      "Episode:260 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9169 dloss:1.1390 aloss2:56.1305 exploreP:0.0104\n",
      "Episode:261 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9545 dloss:1.1297 aloss2:56.2818 exploreP:0.0104\n",
      "Episode:262 meanR:0.0700 R:1.0000 rate:0.0769 aloss:1.9648 dloss:1.1316 aloss2:56.1694 exploreP:0.0104\n",
      "Episode:263 meanR:0.0600 R:-1.0000 rate:-0.0769 aloss:2.0131 dloss:1.1395 aloss2:56.2975 exploreP:0.0104\n",
      "Episode:264 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9915 dloss:1.1421 aloss2:56.3867 exploreP:0.0103\n",
      "Episode:265 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9123 dloss:1.1360 aloss2:56.2213 exploreP:0.0103\n",
      "Episode:266 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9012 dloss:1.1239 aloss2:56.2820 exploreP:0.0103\n",
      "Episode:267 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9500 dloss:1.1311 aloss2:56.2531 exploreP:0.0103\n",
      "Episode:268 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9641 dloss:1.1258 aloss2:56.2680 exploreP:0.0103\n",
      "Episode:269 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9836 dloss:1.1584 aloss2:56.2780 exploreP:0.0103\n",
      "Episode:270 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9469 dloss:1.1442 aloss2:56.4048 exploreP:0.0103\n",
      "Episode:271 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9845 dloss:1.1215 aloss2:56.4784 exploreP:0.0103\n",
      "Episode:272 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9596 dloss:1.1429 aloss2:56.5524 exploreP:0.0103\n",
      "Episode:273 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9100 dloss:1.1349 aloss2:56.4545 exploreP:0.0103\n",
      "Episode:274 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9062 dloss:1.1243 aloss2:56.5060 exploreP:0.0103\n",
      "Episode:275 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9350 dloss:1.1302 aloss2:56.4686 exploreP:0.0103\n",
      "Episode:276 meanR:0.0600 R:0.0000 rate:0.0000 aloss:1.9214 dloss:1.1135 aloss2:56.4363 exploreP:0.0102\n",
      "Episode:277 meanR:0.0500 R:-1.0000 rate:-0.0769 aloss:1.9580 dloss:1.1229 aloss2:56.5193 exploreP:0.0102\n",
      "Episode:278 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9196 dloss:1.1280 aloss2:56.6787 exploreP:0.0102\n",
      "Episode:279 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9589 dloss:1.1312 aloss2:56.5826 exploreP:0.0102\n",
      "Episode:280 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:1.9424 dloss:1.1188 aloss2:56.5851 exploreP:0.0102\n",
      "Episode:281 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.9577 dloss:1.1240 aloss2:56.7161 exploreP:0.0102\n",
      "Episode:282 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.9673 dloss:1.1145 aloss2:56.7567 exploreP:0.0102\n",
      "Episode:283 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.9252 dloss:1.1311 aloss2:56.7800 exploreP:0.0102\n",
      "Episode:284 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.9455 dloss:1.1163 aloss2:56.6409 exploreP:0.0102\n",
      "Episode:285 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9389 dloss:1.1249 aloss2:56.7537 exploreP:0.0102\n",
      "Episode:286 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.9163 dloss:1.1333 aloss2:56.6362 exploreP:0.0102\n",
      "Episode:287 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:1.9413 dloss:1.1166 aloss2:56.6559 exploreP:0.0102\n",
      "Episode:288 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.8984 dloss:1.1071 aloss2:56.6109 exploreP:0.0102\n",
      "Episode:289 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9365 dloss:1.1124 aloss2:56.6944 exploreP:0.0102\n",
      "Episode:290 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9052 dloss:1.1293 aloss2:56.4819 exploreP:0.0102\n",
      "Episode:291 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.8959 dloss:1.1163 aloss2:56.7888 exploreP:0.0102\n",
      "Episode:292 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9133 dloss:1.1131 aloss2:56.9049 exploreP:0.0102\n",
      "Episode:293 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9392 dloss:1.1227 aloss2:56.8412 exploreP:0.0101\n",
      "Episode:294 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9381 dloss:1.1078 aloss2:56.4323 exploreP:0.0101\n",
      "Episode:295 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9081 dloss:1.1032 aloss2:56.2376 exploreP:0.0101\n",
      "Episode:296 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9982 dloss:1.1238 aloss2:55.9305 exploreP:0.0101\n",
      "Episode:297 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.9767 dloss:1.1100 aloss2:56.2783 exploreP:0.0101\n",
      "Episode:298 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.9449 dloss:1.1089 aloss2:56.3748 exploreP:0.0101\n",
      "Episode:299 meanR:0.0400 R:0.0000 rate:0.0000 aloss:2.0194 dloss:1.1213 aloss2:56.4323 exploreP:0.0101\n",
      "Episode:300 meanR:0.0400 R:0.0000 rate:0.0000 aloss:2.0180 dloss:1.1130 aloss2:56.5948 exploreP:0.0101\n",
      "Episode:301 meanR:0.0400 R:0.0000 rate:0.0000 aloss:2.0009 dloss:1.1195 aloss2:56.6147 exploreP:0.0101\n",
      "Episode:302 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.9453 dloss:1.0991 aloss2:57.0299 exploreP:0.0101\n",
      "Episode:303 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0264 dloss:1.1203 aloss2:57.0594 exploreP:0.0101\n",
      "Episode:304 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0614 dloss:1.1159 aloss2:56.9594 exploreP:0.0101\n",
      "Episode:305 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0115 dloss:1.1168 aloss2:57.2530 exploreP:0.0101\n",
      "Episode:306 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.8960 dloss:1.1216 aloss2:57.4429 exploreP:0.0101\n",
      "Episode:307 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.9928 dloss:1.1092 aloss2:57.5633 exploreP:0.0101\n",
      "Episode:308 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0108 dloss:1.1115 aloss2:57.3204 exploreP:0.0101\n",
      "Episode:309 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0499 dloss:1.1113 aloss2:57.4575 exploreP:0.0101\n",
      "Episode:310 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.9798 dloss:1.1203 aloss2:57.6114 exploreP:0.0101\n",
      "Episode:311 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:2.0301 dloss:1.1187 aloss2:57.5342 exploreP:0.0101\n",
      "Episode:312 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0065 dloss:1.1076 aloss2:57.5968 exploreP:0.0101\n",
      "Episode:313 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.9954 dloss:1.1006 aloss2:57.6382 exploreP:0.0101\n",
      "Episode:314 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.9475 dloss:1.1162 aloss2:57.8290 exploreP:0.0101\n",
      "Episode:315 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.9050 dloss:1.1053 aloss2:54.9689 exploreP:0.0101\n",
      "Episode:316 meanR:0.0400 R:1.0000 rate:0.0769 aloss:2.2565 dloss:1.1153 aloss2:50.8832 exploreP:0.0101\n",
      "Episode:317 meanR:0.0500 R:1.0000 rate:0.0769 aloss:2.2615 dloss:1.1067 aloss2:51.9338 exploreP:0.0101\n",
      "Episode:318 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.2289 dloss:1.1001 aloss2:52.8023 exploreP:0.0101\n",
      "Episode:319 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.2111 dloss:1.1297 aloss2:53.5218 exploreP:0.0101\n",
      "Episode:320 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.2446 dloss:1.1195 aloss2:54.0778 exploreP:0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:321 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0450 dloss:1.1025 aloss2:54.6052 exploreP:0.0101\n",
      "Episode:322 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1331 dloss:1.1105 aloss2:54.9899 exploreP:0.0101\n",
      "Episode:323 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1174 dloss:1.1075 aloss2:55.1251 exploreP:0.0101\n",
      "Episode:324 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0482 dloss:1.1113 aloss2:55.3285 exploreP:0.0101\n",
      "Episode:325 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.1660 dloss:1.0936 aloss2:55.4355 exploreP:0.0101\n",
      "Episode:326 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.1490 dloss:1.1130 aloss2:55.6246 exploreP:0.0101\n",
      "Episode:327 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:2.1733 dloss:1.1147 aloss2:55.9986 exploreP:0.0101\n",
      "Episode:328 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1575 dloss:1.1055 aloss2:56.0981 exploreP:0.0101\n",
      "Episode:329 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.2160 dloss:1.1137 aloss2:56.2795 exploreP:0.0100\n",
      "Episode:330 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1162 dloss:1.1147 aloss2:56.4242 exploreP:0.0100\n",
      "Episode:331 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.1727 dloss:1.1034 aloss2:56.4579 exploreP:0.0100\n",
      "Episode:332 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0770 dloss:1.1036 aloss2:56.6034 exploreP:0.0100\n",
      "Episode:333 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:2.0972 dloss:1.1098 aloss2:56.7745 exploreP:0.0100\n",
      "Episode:334 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.1315 dloss:1.1086 aloss2:56.6849 exploreP:0.0100\n",
      "Episode:335 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0849 dloss:1.1058 aloss2:56.8904 exploreP:0.0100\n",
      "Episode:336 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.1081 dloss:1.1078 aloss2:57.0457 exploreP:0.0100\n",
      "Episode:337 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0813 dloss:1.1192 aloss2:57.1035 exploreP:0.0100\n",
      "Episode:338 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.0538 dloss:1.0946 aloss2:57.2290 exploreP:0.0100\n",
      "Episode:339 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.9449 dloss:1.0984 aloss2:57.3043 exploreP:0.0100\n",
      "Episode:340 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.9868 dloss:1.1154 aloss2:57.3885 exploreP:0.0100\n",
      "Episode:341 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.9738 dloss:1.1100 aloss2:57.2897 exploreP:0.0100\n",
      "Episode:342 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.9905 dloss:1.1027 aloss2:57.5998 exploreP:0.0100\n",
      "Episode:343 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.8796 dloss:1.1104 aloss2:57.5620 exploreP:0.0100\n",
      "Episode:344 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.9523 dloss:1.1026 aloss2:57.4821 exploreP:0.0100\n",
      "Episode:345 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.9006 dloss:1.0902 aloss2:57.6882 exploreP:0.0100\n",
      "Episode:346 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:2.0281 dloss:1.1072 aloss2:57.5249 exploreP:0.0100\n",
      "Episode:347 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:1.9138 dloss:1.0949 aloss2:57.7375 exploreP:0.0100\n",
      "Episode:348 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.8938 dloss:1.0888 aloss2:57.6636 exploreP:0.0100\n",
      "Episode:349 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.8213 dloss:1.0911 aloss2:57.7586 exploreP:0.0100\n",
      "Episode:350 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.8704 dloss:1.1012 aloss2:57.9252 exploreP:0.0100\n",
      "Episode:351 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8955 dloss:1.0911 aloss2:57.9622 exploreP:0.0100\n",
      "Episode:352 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8937 dloss:1.0869 aloss2:57.9699 exploreP:0.0100\n",
      "Episode:353 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.9012 dloss:1.1004 aloss2:58.0600 exploreP:0.0100\n",
      "Episode:354 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8496 dloss:1.0845 aloss2:58.0286 exploreP:0.0100\n",
      "Episode:355 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.0543 dloss:1.0806 aloss2:58.0191 exploreP:0.0100\n",
      "Episode:356 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.9329 dloss:1.0996 aloss2:58.1157 exploreP:0.0100\n",
      "Episode:357 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.8375 dloss:1.0931 aloss2:58.3560 exploreP:0.0100\n",
      "Episode:358 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.8341 dloss:1.0914 aloss2:58.3731 exploreP:0.0100\n",
      "Episode:359 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7761 dloss:1.0836 aloss2:58.3970 exploreP:0.0100\n",
      "Episode:360 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7920 dloss:1.0915 aloss2:58.5262 exploreP:0.0100\n",
      "Episode:361 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8070 dloss:1.0856 aloss2:58.2668 exploreP:0.0100\n",
      "Episode:362 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.8318 dloss:1.0827 aloss2:58.6331 exploreP:0.0100\n",
      "Episode:363 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8026 dloss:1.0786 aloss2:58.6337 exploreP:0.0100\n",
      "Episode:364 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7374 dloss:1.0787 aloss2:58.7124 exploreP:0.0100\n",
      "Episode:365 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8175 dloss:1.0732 aloss2:58.9860 exploreP:0.0100\n",
      "Episode:366 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.7337 dloss:1.0820 aloss2:58.8622 exploreP:0.0100\n",
      "Episode:367 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.7589 dloss:1.0789 aloss2:58.9966 exploreP:0.0100\n",
      "Episode:368 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.7723 dloss:1.0818 aloss2:58.9426 exploreP:0.0100\n",
      "Episode:369 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.7335 dloss:1.0869 aloss2:58.9355 exploreP:0.0100\n",
      "Episode:370 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.7151 dloss:1.0902 aloss2:59.0024 exploreP:0.0100\n",
      "Episode:371 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7647 dloss:1.0782 aloss2:58.8357 exploreP:0.0100\n",
      "Episode:372 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7533 dloss:1.0788 aloss2:59.1125 exploreP:0.0100\n",
      "Episode:373 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7337 dloss:1.0767 aloss2:59.0791 exploreP:0.0100\n",
      "Episode:374 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.8187 dloss:1.0696 aloss2:59.0165 exploreP:0.0100\n",
      "Episode:375 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7658 dloss:1.0600 aloss2:59.2170 exploreP:0.0100\n",
      "Episode:376 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.7887 dloss:1.0763 aloss2:59.1767 exploreP:0.0100\n",
      "Episode:377 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7627 dloss:1.0735 aloss2:59.2432 exploreP:0.0100\n",
      "Episode:378 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.7626 dloss:1.0698 aloss2:59.1396 exploreP:0.0100\n",
      "Episode:379 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.6897 dloss:1.0730 aloss2:59.1625 exploreP:0.0100\n",
      "Episode:380 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.6566 dloss:1.0694 aloss2:59.2791 exploreP:0.0100\n",
      "Episode:381 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.7798 dloss:1.0612 aloss2:59.1986 exploreP:0.0100\n",
      "Episode:382 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.7598 dloss:1.0735 aloss2:59.3440 exploreP:0.0100\n",
      "Episode:383 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.8869 dloss:1.0703 aloss2:59.2203 exploreP:0.0100\n",
      "Episode:384 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.7604 dloss:1.0737 aloss2:59.2473 exploreP:0.0100\n",
      "Episode:385 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.7649 dloss:1.0768 aloss2:59.2118 exploreP:0.0100\n",
      "Episode:386 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:1.6599 dloss:1.0689 aloss2:59.4186 exploreP:0.0100\n",
      "Episode:387 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.7074 dloss:1.0617 aloss2:59.5596 exploreP:0.0100\n",
      "Episode:388 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.6808 dloss:1.0625 aloss2:59.4942 exploreP:0.0100\n",
      "Episode:389 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.6814 dloss:1.0739 aloss2:59.5498 exploreP:0.0100\n",
      "Episode:390 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.6438 dloss:1.0666 aloss2:59.6937 exploreP:0.0100\n",
      "Episode:391 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:1.5693 dloss:1.0686 aloss2:59.5635 exploreP:0.0100\n",
      "Episode:392 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.6424 dloss:1.0621 aloss2:59.6987 exploreP:0.0100\n",
      "Episode:393 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.6630 dloss:1.0548 aloss2:59.6849 exploreP:0.0100\n",
      "Episode:394 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.7333 dloss:1.0553 aloss2:59.6285 exploreP:0.0100\n",
      "Episode:395 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.6224 dloss:1.0527 aloss2:59.7715 exploreP:0.0100\n",
      "Episode:396 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.5845 dloss:1.0618 aloss2:59.7159 exploreP:0.0100\n",
      "Episode:397 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.6189 dloss:1.0559 aloss2:59.7547 exploreP:0.0100\n",
      "Episode:398 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.6866 dloss:1.0614 aloss2:59.7466 exploreP:0.0100\n",
      "Episode:399 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.5934 dloss:1.0561 aloss2:59.8083 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:400 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.6100 dloss:1.0552 aloss2:59.7440 exploreP:0.0100\n",
      "Episode:401 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.5949 dloss:1.0680 aloss2:59.7586 exploreP:0.0100\n",
      "Episode:402 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.6176 dloss:1.0710 aloss2:59.8494 exploreP:0.0100\n",
      "Episode:403 meanR:-0.0600 R:-1.0000 rate:-0.0769 aloss:1.5625 dloss:1.0516 aloss2:59.9224 exploreP:0.0100\n",
      "Episode:404 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.6008 dloss:1.0597 aloss2:59.8645 exploreP:0.0100\n",
      "Episode:405 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5514 dloss:1.0615 aloss2:59.8793 exploreP:0.0100\n",
      "Episode:406 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5520 dloss:1.0546 aloss2:59.9476 exploreP:0.0100\n",
      "Episode:407 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.6958 dloss:1.0546 aloss2:60.0337 exploreP:0.0100\n",
      "Episode:408 meanR:-0.0700 R:-1.0000 rate:-0.0769 aloss:1.5739 dloss:1.0557 aloss2:59.9546 exploreP:0.0100\n",
      "Episode:409 meanR:-0.0700 R:0.0000 rate:0.0000 aloss:1.5794 dloss:1.0591 aloss2:59.9832 exploreP:0.0100\n",
      "Episode:410 meanR:-0.0700 R:0.0000 rate:0.0000 aloss:1.5335 dloss:1.0584 aloss2:59.9901 exploreP:0.0100\n",
      "Episode:411 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.6058 dloss:1.0630 aloss2:59.9748 exploreP:0.0100\n",
      "Episode:412 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5815 dloss:1.0516 aloss2:60.2327 exploreP:0.0100\n",
      "Episode:413 meanR:-0.0700 R:0.0000 rate:0.0000 aloss:1.5400 dloss:1.0503 aloss2:59.9386 exploreP:0.0100\n",
      "Episode:414 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:1.5868 dloss:1.0689 aloss2:59.9775 exploreP:0.0100\n",
      "Episode:415 meanR:-0.0700 R:0.0000 rate:0.0000 aloss:1.4504 dloss:1.0599 aloss2:60.0897 exploreP:0.0100\n",
      "Episode:416 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:1.5776 dloss:1.0615 aloss2:60.2656 exploreP:0.0100\n",
      "Episode:417 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:1.5363 dloss:1.0543 aloss2:60.2672 exploreP:0.0100\n",
      "Episode:418 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:1.5706 dloss:1.0627 aloss2:60.0592 exploreP:0.0100\n",
      "Episode:419 meanR:-0.0600 R:2.0000 rate:0.1538 aloss:1.5224 dloss:1.0587 aloss2:60.2074 exploreP:0.0100\n",
      "Episode:420 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5516 dloss:1.0631 aloss2:60.1468 exploreP:0.0100\n",
      "Episode:421 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5563 dloss:1.0709 aloss2:60.1661 exploreP:0.0100\n",
      "Episode:422 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:1.5027 dloss:1.0717 aloss2:60.2620 exploreP:0.0100\n",
      "Episode:423 meanR:-0.0500 R:1.0000 rate:0.0769 aloss:1.4896 dloss:1.0702 aloss2:60.1715 exploreP:0.0100\n",
      "Episode:424 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.5115 dloss:1.0678 aloss2:60.1008 exploreP:0.0100\n",
      "Episode:425 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.5657 dloss:1.0675 aloss2:60.3208 exploreP:0.0100\n",
      "Episode:426 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.5262 dloss:1.0678 aloss2:60.2356 exploreP:0.0100\n",
      "Episode:427 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4850 dloss:1.0671 aloss2:60.3917 exploreP:0.0100\n",
      "Episode:428 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.5139 dloss:1.0627 aloss2:60.3382 exploreP:0.0100\n",
      "Episode:429 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.5103 dloss:1.0652 aloss2:60.5147 exploreP:0.0100\n",
      "Episode:430 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:1.5296 dloss:1.0677 aloss2:60.4773 exploreP:0.0100\n",
      "Episode:431 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.4644 dloss:1.0754 aloss2:60.4571 exploreP:0.0100\n",
      "Episode:432 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.4885 dloss:1.0534 aloss2:60.5672 exploreP:0.0100\n",
      "Episode:433 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4321 dloss:1.0705 aloss2:60.4709 exploreP:0.0100\n",
      "Episode:434 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4342 dloss:1.0548 aloss2:60.3603 exploreP:0.0100\n",
      "Episode:435 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4446 dloss:1.0567 aloss2:60.4712 exploreP:0.0100\n",
      "Episode:436 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4179 dloss:1.0731 aloss2:60.4315 exploreP:0.0100\n",
      "Episode:437 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.3714 dloss:1.0667 aloss2:60.3920 exploreP:0.0100\n",
      "Episode:438 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.3885 dloss:1.0673 aloss2:60.3872 exploreP:0.0100\n",
      "Episode:439 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4601 dloss:1.0642 aloss2:60.4156 exploreP:0.0100\n",
      "Episode:440 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4260 dloss:1.0680 aloss2:60.6793 exploreP:0.0100\n",
      "Episode:441 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.3725 dloss:1.0574 aloss2:60.3683 exploreP:0.0100\n",
      "Episode:442 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3927 dloss:1.0799 aloss2:60.4585 exploreP:0.0100\n",
      "Episode:443 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3242 dloss:1.0631 aloss2:60.2932 exploreP:0.0100\n",
      "Episode:444 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3430 dloss:1.0627 aloss2:60.3855 exploreP:0.0100\n",
      "Episode:445 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.3530 dloss:1.0615 aloss2:60.4940 exploreP:0.0100\n",
      "Episode:446 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.3258 dloss:1.0659 aloss2:60.4699 exploreP:0.0100\n",
      "Episode:447 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.3269 dloss:1.0654 aloss2:60.4220 exploreP:0.0100\n",
      "Episode:448 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.3333 dloss:1.0795 aloss2:60.3151 exploreP:0.0100\n",
      "Episode:449 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.3121 dloss:1.0628 aloss2:60.5192 exploreP:0.0100\n",
      "Episode:450 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.3225 dloss:1.0577 aloss2:60.4569 exploreP:0.0100\n",
      "Episode:451 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:1.3036 dloss:1.0604 aloss2:60.4238 exploreP:0.0100\n",
      "Episode:452 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3065 dloss:1.0665 aloss2:60.5754 exploreP:0.0100\n",
      "Episode:453 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3048 dloss:1.0583 aloss2:60.4770 exploreP:0.0100\n",
      "Episode:454 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2971 dloss:1.0659 aloss2:60.4512 exploreP:0.0100\n",
      "Episode:455 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3035 dloss:1.0712 aloss2:60.5171 exploreP:0.0100\n",
      "Episode:456 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3021 dloss:1.0519 aloss2:60.6210 exploreP:0.0100\n",
      "Episode:457 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2853 dloss:1.0734 aloss2:60.6045 exploreP:0.0100\n",
      "Episode:458 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.2844 dloss:1.0715 aloss2:60.6061 exploreP:0.0100\n",
      "Episode:459 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.2812 dloss:1.0669 aloss2:60.6410 exploreP:0.0100\n",
      "Episode:460 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2955 dloss:1.0558 aloss2:60.7225 exploreP:0.0100\n",
      "Episode:461 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2842 dloss:1.0688 aloss2:60.6577 exploreP:0.0100\n",
      "Episode:462 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2721 dloss:1.0621 aloss2:60.8367 exploreP:0.0100\n",
      "Episode:463 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2481 dloss:1.0675 aloss2:60.7613 exploreP:0.0100\n",
      "Episode:464 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2492 dloss:1.0685 aloss2:60.7227 exploreP:0.0100\n",
      "Episode:465 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2428 dloss:1.0616 aloss2:60.7542 exploreP:0.0100\n",
      "Episode:466 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2491 dloss:1.0576 aloss2:60.9179 exploreP:0.0100\n",
      "Episode:467 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2408 dloss:1.0638 aloss2:60.8700 exploreP:0.0100\n",
      "Episode:468 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.2226 dloss:1.0581 aloss2:60.7768 exploreP:0.0100\n",
      "Episode:469 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2194 dloss:1.0666 aloss2:60.8681 exploreP:0.0100\n",
      "Episode:470 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2167 dloss:1.0528 aloss2:60.9035 exploreP:0.0100\n",
      "Episode:471 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2091 dloss:1.0808 aloss2:60.7480 exploreP:0.0100\n",
      "Episode:472 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2069 dloss:1.0773 aloss2:60.7704 exploreP:0.0100\n",
      "Episode:473 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.1977 dloss:1.0602 aloss2:60.8307 exploreP:0.0100\n",
      "Episode:474 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.1942 dloss:1.0608 aloss2:60.6832 exploreP:0.0100\n",
      "Episode:475 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.1882 dloss:1.0650 aloss2:60.8375 exploreP:0.0100\n",
      "Episode:476 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.1827 dloss:1.0651 aloss2:60.8982 exploreP:0.0100\n",
      "Episode:477 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.1317 dloss:1.0628 aloss2:61.0529 exploreP:0.0100\n",
      "Episode:478 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0729 dloss:1.0778 aloss2:60.9769 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:479 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0678 dloss:1.0645 aloss2:61.1130 exploreP:0.0100\n",
      "Episode:480 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.0720 dloss:1.0627 aloss2:61.1392 exploreP:0.0100\n",
      "Episode:481 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0669 dloss:1.0663 aloss2:61.0202 exploreP:0.0100\n",
      "Episode:482 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0519 dloss:1.0640 aloss2:61.1364 exploreP:0.0100\n",
      "Episode:483 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0732 dloss:1.0684 aloss2:61.2026 exploreP:0.0100\n",
      "Episode:484 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0664 dloss:1.0625 aloss2:61.1652 exploreP:0.0100\n",
      "Episode:485 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0580 dloss:1.0581 aloss2:61.1581 exploreP:0.0100\n",
      "Episode:486 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.0737 dloss:1.0649 aloss2:61.3138 exploreP:0.0100\n",
      "Episode:487 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.0686 dloss:1.0527 aloss2:61.3594 exploreP:0.0100\n",
      "Episode:488 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.0627 dloss:1.0573 aloss2:61.3797 exploreP:0.0100\n",
      "Episode:489 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0599 dloss:1.0653 aloss2:61.3722 exploreP:0.0100\n",
      "Episode:490 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.0564 dloss:1.0523 aloss2:61.4053 exploreP:0.0100\n",
      "Episode:491 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0551 dloss:1.0544 aloss2:61.4850 exploreP:0.0100\n",
      "Episode:492 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.0587 dloss:1.0662 aloss2:61.4601 exploreP:0.0100\n",
      "Episode:493 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0515 dloss:1.0622 aloss2:61.5281 exploreP:0.0100\n",
      "Episode:494 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0461 dloss:1.0473 aloss2:61.3649 exploreP:0.0100\n",
      "Episode:495 meanR:0.0200 R:1.0000 rate:0.0769 aloss:1.0527 dloss:1.0576 aloss2:61.5517 exploreP:0.0100\n",
      "Episode:496 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0511 dloss:1.0646 aloss2:61.3824 exploreP:0.0100\n",
      "Episode:497 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0407 dloss:1.0562 aloss2:61.5955 exploreP:0.0100\n",
      "Episode:498 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0417 dloss:1.0627 aloss2:61.6082 exploreP:0.0100\n",
      "Episode:499 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0420 dloss:1.0726 aloss2:61.6593 exploreP:0.0100\n",
      "Episode:500 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0393 dloss:1.0518 aloss2:61.7481 exploreP:0.0100\n",
      "Episode:501 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0342 dloss:1.0636 aloss2:61.5961 exploreP:0.0100\n",
      "Episode:502 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0444 dloss:1.0636 aloss2:61.6766 exploreP:0.0100\n",
      "Episode:503 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.0398 dloss:1.0644 aloss2:61.8342 exploreP:0.0100\n",
      "Episode:504 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0415 dloss:1.0674 aloss2:61.7158 exploreP:0.0100\n",
      "Episode:505 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0342 dloss:1.0616 aloss2:61.8832 exploreP:0.0100\n",
      "Episode:506 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0427 dloss:1.0612 aloss2:61.8042 exploreP:0.0100\n",
      "Episode:507 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0436 dloss:1.0633 aloss2:61.8509 exploreP:0.0100\n",
      "Episode:508 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.0375 dloss:1.0637 aloss2:61.8408 exploreP:0.0100\n",
      "Episode:509 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0405 dloss:1.0676 aloss2:61.9631 exploreP:0.0100\n",
      "Episode:510 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0516 dloss:1.0626 aloss2:61.9299 exploreP:0.0100\n",
      "Episode:511 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0458 dloss:1.0490 aloss2:61.8289 exploreP:0.0100\n",
      "Episode:512 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0461 dloss:1.0707 aloss2:61.7680 exploreP:0.0100\n",
      "Episode:513 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0488 dloss:1.0600 aloss2:61.6981 exploreP:0.0100\n",
      "Episode:514 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0457 dloss:1.0542 aloss2:61.7678 exploreP:0.0100\n",
      "Episode:515 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0357 dloss:1.0621 aloss2:61.7029 exploreP:0.0100\n",
      "Episode:516 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:1.0443 dloss:1.0682 aloss2:61.8661 exploreP:0.0100\n",
      "Episode:517 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0349 dloss:1.0554 aloss2:61.9014 exploreP:0.0100\n",
      "Episode:518 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0397 dloss:1.0743 aloss2:61.8449 exploreP:0.0100\n",
      "Episode:519 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0311 dloss:1.0748 aloss2:61.7630 exploreP:0.0100\n",
      "Episode:520 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0356 dloss:1.0641 aloss2:62.0135 exploreP:0.0100\n",
      "Episode:521 meanR:0.0200 R:1.0000 rate:0.0769 aloss:1.0296 dloss:1.0603 aloss2:61.9013 exploreP:0.0100\n",
      "Episode:522 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0260 dloss:1.0597 aloss2:62.1371 exploreP:0.0100\n",
      "Episode:523 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0313 dloss:1.0606 aloss2:62.1259 exploreP:0.0100\n",
      "Episode:524 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0343 dloss:1.0648 aloss2:61.9913 exploreP:0.0100\n",
      "Episode:525 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0326 dloss:1.0577 aloss2:62.0663 exploreP:0.0100\n",
      "Episode:526 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.0318 dloss:1.0504 aloss2:62.1682 exploreP:0.0100\n",
      "Episode:527 meanR:0.0100 R:1.0000 rate:0.0769 aloss:1.0161 dloss:1.0487 aloss2:62.1223 exploreP:0.0100\n",
      "Episode:528 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0321 dloss:1.0593 aloss2:62.3100 exploreP:0.0100\n",
      "Episode:529 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0250 dloss:1.0637 aloss2:62.2946 exploreP:0.0100\n",
      "Episode:530 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0308 dloss:1.0647 aloss2:62.1803 exploreP:0.0100\n",
      "Episode:531 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0211 dloss:1.0716 aloss2:62.0363 exploreP:0.0100\n",
      "Episode:532 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0305 dloss:1.0596 aloss2:62.2387 exploreP:0.0100\n",
      "Episode:533 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:1.0220 dloss:1.0614 aloss2:62.0711 exploreP:0.0100\n",
      "Episode:534 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.0170 dloss:1.0626 aloss2:62.2198 exploreP:0.0100\n",
      "Episode:535 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0388 dloss:1.0635 aloss2:62.3211 exploreP:0.0100\n",
      "Episode:536 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0250 dloss:1.0614 aloss2:62.3741 exploreP:0.0100\n",
      "Episode:537 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0304 dloss:1.0542 aloss2:62.4387 exploreP:0.0100\n",
      "Episode:538 meanR:0.0200 R:2.0000 rate:0.1538 aloss:1.0260 dloss:1.0591 aloss2:62.3102 exploreP:0.0100\n",
      "Episode:539 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0205 dloss:1.0571 aloss2:62.3339 exploreP:0.0100\n",
      "Episode:540 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0299 dloss:1.0636 aloss2:62.3484 exploreP:0.0100\n",
      "Episode:541 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0253 dloss:1.0671 aloss2:62.3041 exploreP:0.0100\n",
      "Episode:542 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0170 dloss:1.0628 aloss2:62.4401 exploreP:0.0100\n",
      "Episode:543 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.0175 dloss:1.0574 aloss2:62.4577 exploreP:0.0100\n",
      "Episode:544 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0258 dloss:1.0538 aloss2:62.4260 exploreP:0.0100\n",
      "Episode:545 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0160 dloss:1.0651 aloss2:62.4890 exploreP:0.0100\n",
      "Episode:546 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0067 dloss:1.0623 aloss2:62.5068 exploreP:0.0100\n",
      "Episode:547 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.0135 dloss:1.0681 aloss2:62.5834 exploreP:0.0100\n",
      "Episode:548 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0043 dloss:1.0782 aloss2:62.6137 exploreP:0.0100\n",
      "Episode:549 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0088 dloss:1.0644 aloss2:62.8849 exploreP:0.0100\n",
      "Episode:550 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0057 dloss:1.0639 aloss2:62.9425 exploreP:0.0100\n",
      "Episode:551 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0020 dloss:1.0659 aloss2:62.9003 exploreP:0.0100\n",
      "Episode:552 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9850 dloss:1.0648 aloss2:63.1122 exploreP:0.0100\n",
      "Episode:553 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9869 dloss:1.0631 aloss2:63.4694 exploreP:0.0100\n",
      "Episode:554 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:0.9724 dloss:1.0510 aloss2:63.7509 exploreP:0.0100\n",
      "Episode:555 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9677 dloss:1.0675 aloss2:64.2282 exploreP:0.0100\n",
      "Episode:556 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9616 dloss:1.0632 aloss2:64.2767 exploreP:0.0100\n",
      "Episode:557 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9481 dloss:1.0646 aloss2:64.5915 exploreP:0.0100\n",
      "Episode:558 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9338 dloss:1.0640 aloss2:65.3306 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:559 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9135 dloss:1.0486 aloss2:65.1091 exploreP:0.0100\n",
      "Episode:560 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9123 dloss:1.0588 aloss2:63.8780 exploreP:0.0100\n",
      "Episode:561 meanR:0.0200 R:1.0000 rate:0.0769 aloss:0.9189 dloss:1.0626 aloss2:63.4992 exploreP:0.0100\n",
      "Episode:562 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9045 dloss:1.0629 aloss2:63.5026 exploreP:0.0100\n",
      "Episode:563 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9172 dloss:1.0641 aloss2:63.3914 exploreP:0.0100\n",
      "Episode:564 meanR:0.0300 R:1.0000 rate:0.0769 aloss:0.8981 dloss:1.0597 aloss2:63.3177 exploreP:0.0100\n",
      "Episode:565 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9184 dloss:1.0654 aloss2:63.3338 exploreP:0.0100\n",
      "Episode:566 meanR:0.0400 R:1.0000 rate:0.0769 aloss:0.9186 dloss:1.0644 aloss2:63.3464 exploreP:0.0100\n",
      "Episode:567 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9196 dloss:1.0681 aloss2:63.3860 exploreP:0.0100\n",
      "Episode:568 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:0.9086 dloss:1.0711 aloss2:63.1477 exploreP:0.0100\n",
      "Episode:569 meanR:0.0500 R:1.0000 rate:0.0769 aloss:0.9093 dloss:1.0510 aloss2:63.4515 exploreP:0.0100\n",
      "Episode:570 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9180 dloss:1.0680 aloss2:63.3342 exploreP:0.0100\n",
      "Episode:571 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9028 dloss:1.0710 aloss2:63.1991 exploreP:0.0100\n",
      "Episode:572 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9150 dloss:1.0622 aloss2:63.4101 exploreP:0.0100\n",
      "Episode:573 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9079 dloss:1.0563 aloss2:63.3947 exploreP:0.0100\n",
      "Episode:574 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9066 dloss:1.0676 aloss2:63.4799 exploreP:0.0100\n",
      "Episode:575 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9208 dloss:1.0585 aloss2:63.5682 exploreP:0.0100\n",
      "Episode:576 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9058 dloss:1.0561 aloss2:63.4875 exploreP:0.0100\n",
      "Episode:577 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9185 dloss:1.0562 aloss2:63.4508 exploreP:0.0100\n",
      "Episode:578 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:0.9195 dloss:1.0739 aloss2:63.4918 exploreP:0.0100\n",
      "Episode:579 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9101 dloss:1.0603 aloss2:63.4562 exploreP:0.0100\n",
      "Episode:580 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9000 dloss:1.0524 aloss2:63.5191 exploreP:0.0100\n",
      "Episode:581 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9209 dloss:1.0514 aloss2:63.4096 exploreP:0.0100\n",
      "Episode:582 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9064 dloss:1.0589 aloss2:63.5457 exploreP:0.0100\n",
      "Episode:583 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9220 dloss:1.0662 aloss2:63.1403 exploreP:0.0100\n",
      "Episode:584 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9184 dloss:1.0566 aloss2:62.9401 exploreP:0.0100\n",
      "Episode:585 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9115 dloss:1.0712 aloss2:63.0259 exploreP:0.0100\n",
      "Episode:586 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9163 dloss:1.0696 aloss2:63.0958 exploreP:0.0100\n",
      "Episode:587 meanR:0.0400 R:1.0000 rate:0.0769 aloss:0.9179 dloss:1.0595 aloss2:63.1597 exploreP:0.0100\n",
      "Episode:588 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9207 dloss:1.0663 aloss2:63.2212 exploreP:0.0100\n",
      "Episode:589 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9148 dloss:1.0612 aloss2:63.3184 exploreP:0.0100\n",
      "Episode:590 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9272 dloss:1.0697 aloss2:63.2954 exploreP:0.0100\n",
      "Episode:591 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9236 dloss:1.0641 aloss2:63.5520 exploreP:0.0100\n",
      "Episode:592 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9276 dloss:1.0543 aloss2:63.3631 exploreP:0.0100\n",
      "Episode:593 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9322 dloss:1.0678 aloss2:63.3695 exploreP:0.0100\n",
      "Episode:594 meanR:0.0400 R:1.0000 rate:0.0769 aloss:0.9343 dloss:1.0629 aloss2:63.3030 exploreP:0.0100\n",
      "Episode:595 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9196 dloss:1.0541 aloss2:63.4149 exploreP:0.0100\n",
      "Episode:596 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9335 dloss:1.0624 aloss2:63.5342 exploreP:0.0100\n",
      "Episode:597 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9236 dloss:1.0590 aloss2:63.4177 exploreP:0.0100\n",
      "Episode:598 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9303 dloss:1.0627 aloss2:63.4741 exploreP:0.0100\n",
      "Episode:599 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9274 dloss:1.0562 aloss2:63.6469 exploreP:0.0100\n",
      "Episode:600 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9193 dloss:1.0638 aloss2:63.5896 exploreP:0.0100\n",
      "Episode:601 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9250 dloss:1.0663 aloss2:63.5030 exploreP:0.0100\n",
      "Episode:602 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9402 dloss:1.0646 aloss2:63.7036 exploreP:0.0100\n",
      "Episode:603 meanR:0.0500 R:1.0000 rate:0.0769 aloss:0.9392 dloss:1.0674 aloss2:63.5839 exploreP:0.0100\n",
      "Episode:604 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9223 dloss:1.0689 aloss2:63.4990 exploreP:0.0100\n",
      "Episode:605 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9295 dloss:1.0601 aloss2:63.7285 exploreP:0.0100\n",
      "Episode:606 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9298 dloss:1.0612 aloss2:63.7063 exploreP:0.0100\n",
      "Episode:607 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9312 dloss:1.0671 aloss2:63.7263 exploreP:0.0100\n",
      "Episode:608 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9380 dloss:1.0742 aloss2:63.9324 exploreP:0.0100\n",
      "Episode:609 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9367 dloss:1.0565 aloss2:63.9914 exploreP:0.0100\n",
      "Episode:610 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9384 dloss:1.0720 aloss2:63.8175 exploreP:0.0100\n",
      "Episode:611 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9384 dloss:1.0581 aloss2:63.7409 exploreP:0.0100\n",
      "Episode:612 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9507 dloss:1.0562 aloss2:64.0805 exploreP:0.0100\n",
      "Episode:613 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9401 dloss:1.0700 aloss2:63.8831 exploreP:0.0100\n",
      "Episode:614 meanR:0.0500 R:1.0000 rate:0.0769 aloss:0.9441 dloss:1.0609 aloss2:63.8988 exploreP:0.0100\n",
      "Episode:615 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9484 dloss:1.0576 aloss2:64.0514 exploreP:0.0100\n",
      "Episode:616 meanR:0.0500 R:-1.0000 rate:-0.0769 aloss:0.9626 dloss:1.0701 aloss2:63.9148 exploreP:0.0100\n",
      "Episode:617 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9468 dloss:1.0723 aloss2:63.9886 exploreP:0.0100\n",
      "Episode:618 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9475 dloss:1.0696 aloss2:63.8348 exploreP:0.0100\n",
      "Episode:619 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9552 dloss:1.0685 aloss2:63.9695 exploreP:0.0100\n",
      "Episode:620 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9540 dloss:1.0670 aloss2:64.0949 exploreP:0.0100\n",
      "Episode:621 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9474 dloss:1.0670 aloss2:63.8429 exploreP:0.0100\n",
      "Episode:622 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9570 dloss:1.0595 aloss2:64.1163 exploreP:0.0100\n",
      "Episode:623 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9594 dloss:1.0630 aloss2:64.1415 exploreP:0.0100\n",
      "Episode:624 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9792 dloss:1.0724 aloss2:64.4676 exploreP:0.0100\n",
      "Episode:625 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9689 dloss:1.0667 aloss2:64.1830 exploreP:0.0100\n",
      "Episode:626 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.9792 dloss:1.0604 aloss2:64.1913 exploreP:0.0100\n",
      "Episode:627 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9715 dloss:1.0586 aloss2:64.2056 exploreP:0.0100\n",
      "Episode:628 meanR:0.0400 R:0.0000 rate:0.0000 aloss:0.9687 dloss:1.0686 aloss2:64.0399 exploreP:0.0100\n",
      "Episode:629 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:0.9785 dloss:1.0614 aloss2:64.1308 exploreP:0.0100\n",
      "Episode:630 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9655 dloss:1.0612 aloss2:64.0944 exploreP:0.0100\n",
      "Episode:631 meanR:0.0100 R:-2.0000 rate:-0.1538 aloss:0.9815 dloss:1.0579 aloss2:63.9681 exploreP:0.0100\n",
      "Episode:632 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9818 dloss:1.0555 aloss2:64.1473 exploreP:0.0100\n",
      "Episode:633 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9763 dloss:1.0650 aloss2:63.9955 exploreP:0.0100\n",
      "Episode:634 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9889 dloss:1.0695 aloss2:64.1568 exploreP:0.0100\n",
      "Episode:635 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9833 dloss:1.0573 aloss2:64.0875 exploreP:0.0100\n",
      "Episode:636 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9835 dloss:1.0599 aloss2:64.1609 exploreP:0.0100\n",
      "Episode:637 meanR:0.0300 R:0.0000 rate:0.0000 aloss:0.9796 dloss:1.0607 aloss2:63.9441 exploreP:0.0100\n",
      "Episode:638 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9895 dloss:1.0637 aloss2:63.8998 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:639 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:0.9911 dloss:1.0590 aloss2:63.9151 exploreP:0.0100\n",
      "Episode:640 meanR:0.0000 R:0.0000 rate:0.0000 aloss:0.9841 dloss:1.0704 aloss2:63.8550 exploreP:0.0100\n",
      "Episode:641 meanR:0.0000 R:0.0000 rate:0.0000 aloss:0.9866 dloss:1.0707 aloss2:63.9254 exploreP:0.0100\n",
      "Episode:642 meanR:0.0100 R:1.0000 rate:0.0769 aloss:1.0060 dloss:1.0548 aloss2:64.1898 exploreP:0.0100\n",
      "Episode:643 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9935 dloss:1.0614 aloss2:64.0102 exploreP:0.0100\n",
      "Episode:644 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9895 dloss:1.0655 aloss2:63.9451 exploreP:0.0100\n",
      "Episode:645 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9895 dloss:1.0610 aloss2:64.1118 exploreP:0.0100\n",
      "Episode:646 meanR:0.0200 R:0.0000 rate:0.0000 aloss:0.9964 dloss:1.0596 aloss2:64.0078 exploreP:0.0100\n",
      "Episode:647 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9999 dloss:1.0572 aloss2:64.0024 exploreP:0.0100\n",
      "Episode:648 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9958 dloss:1.0680 aloss2:63.8832 exploreP:0.0100\n",
      "Episode:649 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9854 dloss:1.0685 aloss2:63.9264 exploreP:0.0100\n",
      "Episode:650 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9866 dloss:1.0650 aloss2:63.9730 exploreP:0.0100\n",
      "Episode:651 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0045 dloss:1.0549 aloss2:64.0498 exploreP:0.0100\n",
      "Episode:652 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:0.9960 dloss:1.0527 aloss2:64.0421 exploreP:0.0100\n",
      "Episode:653 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0025 dloss:1.0633 aloss2:63.8967 exploreP:0.0100\n",
      "Episode:654 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0014 dloss:1.0624 aloss2:63.6624 exploreP:0.0100\n",
      "Episode:655 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9915 dloss:1.0593 aloss2:63.7109 exploreP:0.0100\n",
      "Episode:656 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9920 dloss:1.0600 aloss2:63.7631 exploreP:0.0100\n",
      "Episode:657 meanR:0.0100 R:0.0000 rate:0.0000 aloss:0.9848 dloss:1.0650 aloss2:63.8249 exploreP:0.0100\n",
      "Episode:658 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.0000 dloss:1.0667 aloss2:63.7240 exploreP:0.0100\n",
      "Episode:659 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:0.9909 dloss:1.0577 aloss2:64.0429 exploreP:0.0100\n",
      "Episode:660 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.0016 dloss:1.0511 aloss2:64.0051 exploreP:0.0100\n",
      "Episode:661 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:0.9977 dloss:1.0604 aloss2:64.1068 exploreP:0.0100\n",
      "Episode:662 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.0047 dloss:1.0592 aloss2:64.0678 exploreP:0.0100\n",
      "Episode:663 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:0.9935 dloss:1.0661 aloss2:64.3741 exploreP:0.0100\n",
      "Episode:664 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.0048 dloss:1.0663 aloss2:64.3984 exploreP:0.0100\n",
      "Episode:665 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:0.9950 dloss:1.0740 aloss2:64.4932 exploreP:0.0100\n",
      "Episode:666 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:0.9929 dloss:1.0685 aloss2:64.1856 exploreP:0.0100\n",
      "Episode:667 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:0.9905 dloss:1.0574 aloss2:64.5128 exploreP:0.0100\n",
      "Episode:668 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:0.9953 dloss:1.0670 aloss2:64.5294 exploreP:0.0100\n",
      "Episode:669 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:0.9948 dloss:1.0669 aloss2:64.7910 exploreP:0.0100\n",
      "Episode:670 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:0.9991 dloss:1.0584 aloss2:65.1328 exploreP:0.0100\n",
      "Episode:671 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0059 dloss:1.0686 aloss2:65.0553 exploreP:0.0100\n",
      "Episode:672 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0224 dloss:1.0702 aloss2:65.0639 exploreP:0.0100\n",
      "Episode:673 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0193 dloss:1.0554 aloss2:65.0313 exploreP:0.0100\n",
      "Episode:674 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0255 dloss:1.0621 aloss2:65.1317 exploreP:0.0100\n",
      "Episode:675 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0312 dloss:1.0678 aloss2:64.8661 exploreP:0.0100\n",
      "Episode:676 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0272 dloss:1.0640 aloss2:65.0379 exploreP:0.0100\n",
      "Episode:677 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0372 dloss:1.0702 aloss2:64.7852 exploreP:0.0100\n",
      "Episode:678 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0399 dloss:1.0693 aloss2:65.0256 exploreP:0.0100\n",
      "Episode:679 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0425 dloss:1.0613 aloss2:65.0738 exploreP:0.0100\n",
      "Episode:680 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0439 dloss:1.0624 aloss2:64.9375 exploreP:0.0100\n",
      "Episode:681 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.0471 dloss:1.0612 aloss2:64.8096 exploreP:0.0100\n",
      "Episode:682 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.0549 dloss:1.0578 aloss2:65.5760 exploreP:0.0100\n",
      "Episode:683 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0579 dloss:1.0574 aloss2:65.0119 exploreP:0.0100\n",
      "Episode:684 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0684 dloss:1.0673 aloss2:64.9462 exploreP:0.0100\n",
      "Episode:685 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0686 dloss:1.0709 aloss2:64.8651 exploreP:0.0100\n",
      "Episode:686 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0795 dloss:1.0578 aloss2:65.1772 exploreP:0.0100\n",
      "Episode:687 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.0760 dloss:1.0603 aloss2:65.8403 exploreP:0.0100\n",
      "Episode:688 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:1.0935 dloss:1.0596 aloss2:65.2296 exploreP:0.0100\n",
      "Episode:689 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0947 dloss:1.0554 aloss2:65.0365 exploreP:0.0100\n",
      "Episode:690 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0990 dloss:1.0598 aloss2:65.2316 exploreP:0.0100\n",
      "Episode:691 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1144 dloss:1.0724 aloss2:65.3230 exploreP:0.0100\n",
      "Episode:692 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0996 dloss:1.0676 aloss2:66.0815 exploreP:0.0100\n",
      "Episode:693 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1038 dloss:1.0623 aloss2:65.1316 exploreP:0.0100\n",
      "Episode:694 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:1.1190 dloss:1.0589 aloss2:65.2260 exploreP:0.0100\n",
      "Episode:695 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1147 dloss:1.0664 aloss2:65.1343 exploreP:0.0100\n",
      "Episode:696 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1239 dloss:1.0564 aloss2:65.3063 exploreP:0.0100\n",
      "Episode:697 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0998 dloss:1.0576 aloss2:66.6753 exploreP:0.0100\n",
      "Episode:698 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0733 dloss:1.0683 aloss2:66.8215 exploreP:0.0100\n",
      "Episode:699 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1281 dloss:1.0641 aloss2:66.1978 exploreP:0.0100\n",
      "Episode:700 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.0942 dloss:1.0599 aloss2:65.4183 exploreP:0.0100\n",
      "Episode:701 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1415 dloss:1.0760 aloss2:65.4047 exploreP:0.0100\n",
      "Episode:702 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1334 dloss:1.0668 aloss2:65.1970 exploreP:0.0100\n",
      "Episode:703 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1760 dloss:1.0553 aloss2:65.2687 exploreP:0.0100\n",
      "Episode:704 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1840 dloss:1.0672 aloss2:65.6647 exploreP:0.0100\n",
      "Episode:705 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1769 dloss:1.0645 aloss2:65.1533 exploreP:0.0100\n",
      "Episode:706 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1763 dloss:1.0583 aloss2:64.8971 exploreP:0.0100\n",
      "Episode:707 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1901 dloss:1.0648 aloss2:64.6487 exploreP:0.0100\n",
      "Episode:708 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:1.2010 dloss:1.0775 aloss2:64.4083 exploreP:0.0100\n",
      "Episode:709 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1849 dloss:1.0620 aloss2:64.2311 exploreP:0.0100\n",
      "Episode:710 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1633 dloss:1.0647 aloss2:64.0476 exploreP:0.0100\n",
      "Episode:711 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:1.1742 dloss:1.0814 aloss2:64.0661 exploreP:0.0100\n",
      "Episode:712 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:1.1834 dloss:1.0709 aloss2:64.2519 exploreP:0.0100\n",
      "Episode:713 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.1830 dloss:1.0662 aloss2:64.4315 exploreP:0.0100\n",
      "Episode:714 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:1.1645 dloss:1.0681 aloss2:64.7412 exploreP:0.0100\n",
      "Episode:715 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.1595 dloss:1.0702 aloss2:64.5670 exploreP:0.0100\n",
      "Episode:716 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1990 dloss:1.0549 aloss2:64.9601 exploreP:0.0100\n",
      "Episode:717 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.1834 dloss:1.0685 aloss2:64.8374 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:718 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.1910 dloss:1.0711 aloss2:64.8348 exploreP:0.0100\n",
      "Episode:719 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.1918 dloss:1.0710 aloss2:65.1293 exploreP:0.0100\n",
      "Episode:720 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.1902 dloss:1.0620 aloss2:65.3323 exploreP:0.0100\n",
      "Episode:721 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2052 dloss:1.0689 aloss2:65.5043 exploreP:0.0100\n",
      "Episode:722 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.1933 dloss:1.0694 aloss2:65.3590 exploreP:0.0100\n",
      "Episode:723 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.2043 dloss:1.0680 aloss2:65.6868 exploreP:0.0100\n",
      "Episode:724 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1823 dloss:1.0755 aloss2:65.9763 exploreP:0.0100\n",
      "Episode:725 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2055 dloss:1.0717 aloss2:65.8231 exploreP:0.0100\n",
      "Episode:726 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2093 dloss:1.0629 aloss2:65.9816 exploreP:0.0100\n",
      "Episode:727 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2004 dloss:1.0704 aloss2:66.0843 exploreP:0.0100\n",
      "Episode:728 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1096 dloss:1.0762 aloss2:68.6966 exploreP:0.0100\n",
      "Episode:729 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.0534 dloss:1.0739 aloss2:68.8344 exploreP:0.0100\n",
      "Episode:730 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.0610 dloss:1.0664 aloss2:67.9592 exploreP:0.0100\n",
      "Episode:731 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0693 dloss:1.0748 aloss2:66.9812 exploreP:0.0100\n",
      "Episode:732 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0672 dloss:1.0647 aloss2:66.8753 exploreP:0.0100\n",
      "Episode:733 meanR:0.0100 R:1.0000 rate:0.0769 aloss:1.0913 dloss:1.0755 aloss2:67.4091 exploreP:0.0100\n",
      "Episode:734 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0746 dloss:1.0676 aloss2:66.6556 exploreP:0.0100\n",
      "Episode:735 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0760 dloss:1.0683 aloss2:66.4339 exploreP:0.0100\n",
      "Episode:736 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0762 dloss:1.0717 aloss2:66.3231 exploreP:0.0100\n",
      "Episode:737 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.0765 dloss:1.0779 aloss2:66.4547 exploreP:0.0100\n",
      "Episode:738 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1118 dloss:1.0721 aloss2:67.3110 exploreP:0.0100\n",
      "Episode:739 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.1076 dloss:1.0736 aloss2:66.4948 exploreP:0.0100\n",
      "Episode:740 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.0822 dloss:1.0665 aloss2:66.3388 exploreP:0.0100\n",
      "Episode:741 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.1185 dloss:1.0783 aloss2:66.2701 exploreP:0.0100\n",
      "Episode:742 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1218 dloss:1.0766 aloss2:66.1841 exploreP:0.0100\n",
      "Episode:743 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1365 dloss:1.0662 aloss2:67.3928 exploreP:0.0100\n",
      "Episode:744 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1277 dloss:1.0810 aloss2:66.2881 exploreP:0.0100\n",
      "Episode:745 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1232 dloss:1.0744 aloss2:65.9555 exploreP:0.0100\n",
      "Episode:746 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.1260 dloss:1.0662 aloss2:66.1030 exploreP:0.0100\n",
      "Episode:747 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1486 dloss:1.0761 aloss2:66.3279 exploreP:0.0100\n",
      "Episode:748 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1469 dloss:1.0734 aloss2:67.6636 exploreP:0.0100\n",
      "Episode:749 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1569 dloss:1.0675 aloss2:66.5053 exploreP:0.0100\n",
      "Episode:750 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1668 dloss:1.0722 aloss2:66.2346 exploreP:0.0100\n",
      "Episode:751 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1454 dloss:1.0709 aloss2:66.1436 exploreP:0.0100\n",
      "Episode:752 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1815 dloss:1.0845 aloss2:66.1751 exploreP:0.0100\n",
      "Episode:753 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1850 dloss:1.0646 aloss2:66.2782 exploreP:0.0100\n",
      "Episode:754 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1631 dloss:1.0667 aloss2:67.7046 exploreP:0.0100\n",
      "Episode:755 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2181 dloss:1.0752 aloss2:66.4452 exploreP:0.0100\n",
      "Episode:756 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2165 dloss:1.0671 aloss2:66.4085 exploreP:0.0100\n",
      "Episode:757 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2257 dloss:1.0628 aloss2:66.4486 exploreP:0.0100\n",
      "Episode:758 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.2049 dloss:1.0728 aloss2:66.1677 exploreP:0.0100\n",
      "Episode:759 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.2305 dloss:1.0602 aloss2:68.2017 exploreP:0.0100\n",
      "Episode:760 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:1.2414 dloss:1.0641 aloss2:66.7411 exploreP:0.0100\n",
      "Episode:761 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2158 dloss:1.0646 aloss2:66.6952 exploreP:0.0100\n",
      "Episode:762 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2230 dloss:1.0768 aloss2:66.5366 exploreP:0.0100\n",
      "Episode:763 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2698 dloss:1.0668 aloss2:66.8222 exploreP:0.0100\n",
      "Episode:764 meanR:0.0200 R:1.0000 rate:0.0769 aloss:1.2674 dloss:1.0715 aloss2:66.5552 exploreP:0.0100\n",
      "Episode:765 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2283 dloss:1.0664 aloss2:69.2101 exploreP:0.0100\n",
      "Episode:766 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2368 dloss:1.0628 aloss2:67.0450 exploreP:0.0100\n",
      "Episode:767 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.3022 dloss:1.0645 aloss2:67.0422 exploreP:0.0100\n",
      "Episode:768 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.3098 dloss:1.0785 aloss2:66.6553 exploreP:0.0100\n",
      "Episode:769 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2946 dloss:1.0680 aloss2:66.7169 exploreP:0.0100\n",
      "Episode:770 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.2938 dloss:1.0750 aloss2:69.9048 exploreP:0.0100\n",
      "Episode:771 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.3642 dloss:1.0836 aloss2:67.5550 exploreP:0.0100\n",
      "Episode:772 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.3602 dloss:1.0740 aloss2:67.1337 exploreP:0.0100\n",
      "Episode:773 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.3674 dloss:1.0643 aloss2:67.3209 exploreP:0.0100\n",
      "Episode:774 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.3268 dloss:1.0748 aloss2:67.3732 exploreP:0.0100\n",
      "Episode:775 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:1.3457 dloss:1.0776 aloss2:67.2371 exploreP:0.0100\n",
      "Episode:776 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.3842 dloss:1.0684 aloss2:67.9315 exploreP:0.0100\n",
      "Episode:777 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.3542 dloss:1.0698 aloss2:69.3695 exploreP:0.0100\n",
      "Episode:778 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2062 dloss:1.0745 aloss2:66.0463 exploreP:0.0100\n",
      "Episode:779 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2022 dloss:1.0756 aloss2:65.5108 exploreP:0.0100\n",
      "Episode:780 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.1796 dloss:1.0658 aloss2:65.2952 exploreP:0.0100\n",
      "Episode:781 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.2248 dloss:1.0711 aloss2:65.6190 exploreP:0.0100\n",
      "Episode:782 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2672 dloss:1.0690 aloss2:66.4142 exploreP:0.0100\n",
      "Episode:783 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2982 dloss:1.0795 aloss2:66.5469 exploreP:0.0100\n",
      "Episode:784 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2480 dloss:1.0747 aloss2:65.9447 exploreP:0.0100\n",
      "Episode:785 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2424 dloss:1.0679 aloss2:65.9429 exploreP:0.0100\n",
      "Episode:786 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.3154 dloss:1.0699 aloss2:66.4485 exploreP:0.0100\n",
      "Episode:787 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.3003 dloss:1.0755 aloss2:66.6003 exploreP:0.0100\n",
      "Episode:788 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.3237 dloss:1.0725 aloss2:66.8786 exploreP:0.0100\n",
      "Episode:789 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.3274 dloss:1.0684 aloss2:66.7495 exploreP:0.0100\n",
      "Episode:790 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.3296 dloss:1.0710 aloss2:66.9491 exploreP:0.0100\n",
      "Episode:791 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.3136 dloss:1.0765 aloss2:66.9572 exploreP:0.0100\n",
      "Episode:792 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.3132 dloss:1.0707 aloss2:67.0395 exploreP:0.0100\n",
      "Episode:793 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.3577 dloss:1.0765 aloss2:67.5974 exploreP:0.0100\n",
      "Episode:794 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2983 dloss:1.0729 aloss2:67.0300 exploreP:0.0100\n",
      "Episode:795 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2856 dloss:1.0835 aloss2:66.9148 exploreP:0.0100\n",
      "Episode:796 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.3173 dloss:1.0801 aloss2:67.5137 exploreP:0.0100\n",
      "Episode:797 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.3580 dloss:1.0677 aloss2:67.6233 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:798 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.4384 dloss:1.0783 aloss2:68.0078 exploreP:0.0100\n",
      "Episode:799 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2353 dloss:1.0681 aloss2:66.9538 exploreP:0.0100\n",
      "Episode:800 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.3313 dloss:1.0633 aloss2:67.1199 exploreP:0.0100\n",
      "Episode:801 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.3303 dloss:1.0788 aloss2:67.4743 exploreP:0.0100\n",
      "Episode:802 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:1.4802 dloss:1.0791 aloss2:68.8208 exploreP:0.0100\n",
      "Episode:803 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.3218 dloss:1.0706 aloss2:67.2977 exploreP:0.0100\n",
      "Episode:804 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.5374 dloss:1.0775 aloss2:68.9099 exploreP:0.0100\n",
      "Episode:805 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.3231 dloss:1.0780 aloss2:67.6232 exploreP:0.0100\n",
      "Episode:806 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.6917 dloss:1.0575 aloss2:69.2265 exploreP:0.0100\n",
      "Episode:807 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:1.2294 dloss:1.0683 aloss2:71.7703 exploreP:0.0100\n",
      "Episode:808 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.1638 dloss:1.0719 aloss2:74.2994 exploreP:0.0100\n",
      "Episode:809 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.2020 dloss:1.0695 aloss2:73.0545 exploreP:0.0100\n",
      "Episode:810 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.4111 dloss:1.0724 aloss2:69.9904 exploreP:0.0100\n",
      "Episode:811 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.3408 dloss:1.0777 aloss2:67.6446 exploreP:0.0100\n",
      "Episode:812 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.1858 dloss:1.0661 aloss2:65.8701 exploreP:0.0100\n",
      "Episode:813 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.6587 dloss:1.0680 aloss2:72.0374 exploreP:0.0100\n",
      "Episode:814 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3695 dloss:1.0684 aloss2:67.6264 exploreP:0.0100\n",
      "Episode:815 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3087 dloss:1.0743 aloss2:66.4682 exploreP:0.0100\n",
      "Episode:816 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2821 dloss:1.0675 aloss2:66.1292 exploreP:0.0100\n",
      "Episode:817 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.4234 dloss:1.0805 aloss2:67.8439 exploreP:0.0100\n",
      "Episode:818 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.1498 dloss:1.0806 aloss2:71.0607 exploreP:0.0100\n",
      "Episode:819 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.1888 dloss:1.0664 aloss2:71.0145 exploreP:0.0100\n",
      "Episode:820 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3675 dloss:1.0733 aloss2:68.4722 exploreP:0.0100\n",
      "Episode:821 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.2349 dloss:1.0830 aloss2:68.0869 exploreP:0.0100\n",
      "Episode:822 meanR:-0.0500 R:-2.0000 rate:-0.1538 aloss:1.2184 dloss:1.0701 aloss2:71.2931 exploreP:0.0100\n",
      "Episode:823 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:1.2648 dloss:1.0641 aloss2:73.2688 exploreP:0.0100\n",
      "Episode:824 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3068 dloss:1.0702 aloss2:73.5841 exploreP:0.0100\n",
      "Episode:825 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3208 dloss:1.0721 aloss2:74.5331 exploreP:0.0100\n",
      "Episode:826 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.2865 dloss:1.0707 aloss2:74.5889 exploreP:0.0100\n",
      "Episode:827 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2938 dloss:1.0749 aloss2:75.4000 exploreP:0.0100\n",
      "Episode:828 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.4293 dloss:1.0743 aloss2:73.8412 exploreP:0.0100\n",
      "Episode:829 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3721 dloss:1.0607 aloss2:72.2459 exploreP:0.0100\n",
      "Episode:830 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:1.3266 dloss:1.0756 aloss2:72.6304 exploreP:0.0100\n",
      "Episode:831 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.3075 dloss:1.0702 aloss2:73.3743 exploreP:0.0100\n",
      "Episode:832 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.3142 dloss:1.0657 aloss2:74.6942 exploreP:0.0100\n",
      "Episode:833 meanR:-0.0100 R:2.0000 rate:0.1538 aloss:1.4452 dloss:1.0692 aloss2:76.3755 exploreP:0.0100\n",
      "Episode:834 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.4048 dloss:1.0761 aloss2:74.8036 exploreP:0.0100\n",
      "Episode:835 meanR:0.0100 R:2.0000 rate:0.1538 aloss:1.4195 dloss:1.0800 aloss2:72.4542 exploreP:0.0100\n",
      "Episode:836 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.4220 dloss:1.0721 aloss2:73.3937 exploreP:0.0100\n",
      "Episode:837 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.4588 dloss:1.0892 aloss2:73.4728 exploreP:0.0100\n",
      "Episode:838 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4752 dloss:1.0804 aloss2:73.8148 exploreP:0.0100\n",
      "Episode:839 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:1.5327 dloss:1.0718 aloss2:74.7296 exploreP:0.0100\n",
      "Episode:840 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.4896 dloss:1.0755 aloss2:73.7103 exploreP:0.0100\n",
      "Episode:841 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.4818 dloss:1.0882 aloss2:73.7855 exploreP:0.0100\n",
      "Episode:842 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:1.4215 dloss:1.0719 aloss2:72.9176 exploreP:0.0100\n",
      "Episode:843 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.4810 dloss:1.0823 aloss2:73.3107 exploreP:0.0100\n",
      "Episode:844 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4957 dloss:1.0899 aloss2:72.2253 exploreP:0.0100\n",
      "Episode:845 meanR:0.0100 R:1.0000 rate:0.0769 aloss:1.5746 dloss:1.0812 aloss2:70.3615 exploreP:0.0100\n",
      "Episode:846 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.6057 dloss:1.0734 aloss2:71.9330 exploreP:0.0100\n",
      "Episode:847 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.4643 dloss:1.0869 aloss2:73.7082 exploreP:0.0100\n",
      "Episode:848 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.6759 dloss:1.0830 aloss2:72.0536 exploreP:0.0100\n",
      "Episode:849 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.6293 dloss:1.0778 aloss2:73.2233 exploreP:0.0100\n",
      "Episode:850 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:1.5272 dloss:1.0875 aloss2:73.7564 exploreP:0.0100\n",
      "Episode:851 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.4754 dloss:1.0760 aloss2:72.8295 exploreP:0.0100\n",
      "Episode:852 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.6796 dloss:1.0747 aloss2:73.7564 exploreP:0.0100\n",
      "Episode:853 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.4780 dloss:1.0711 aloss2:73.0393 exploreP:0.0100\n",
      "Episode:854 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.4364 dloss:1.0872 aloss2:74.9198 exploreP:0.0100\n",
      "Episode:855 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.9919 dloss:1.0865 aloss2:74.1915 exploreP:0.0100\n",
      "Episode:856 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.6801 dloss:1.0783 aloss2:72.8463 exploreP:0.0100\n",
      "Episode:857 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.7700 dloss:1.0789 aloss2:72.6583 exploreP:0.0100\n",
      "Episode:858 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.7167 dloss:1.0793 aloss2:71.3525 exploreP:0.0100\n",
      "Episode:859 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.5775 dloss:1.0850 aloss2:72.4509 exploreP:0.0100\n",
      "Episode:860 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.7222 dloss:1.0817 aloss2:73.2814 exploreP:0.0100\n",
      "Episode:861 meanR:0.0500 R:1.0000 rate:0.0769 aloss:1.5566 dloss:1.0790 aloss2:69.0039 exploreP:0.0100\n",
      "Episode:862 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.6405 dloss:1.0889 aloss2:69.9863 exploreP:0.0100\n",
      "Episode:863 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.7062 dloss:1.0740 aloss2:69.2588 exploreP:0.0100\n",
      "Episode:864 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.6657 dloss:1.0914 aloss2:69.0336 exploreP:0.0100\n",
      "Episode:865 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.7414 dloss:1.0811 aloss2:70.0813 exploreP:0.0100\n",
      "Episode:866 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.7681 dloss:1.0915 aloss2:72.4623 exploreP:0.0100\n",
      "Episode:867 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.7481 dloss:1.0811 aloss2:74.6805 exploreP:0.0100\n",
      "Episode:868 meanR:0.0200 R:-2.0000 rate:-0.1538 aloss:1.6578 dloss:1.0880 aloss2:75.7495 exploreP:0.0100\n",
      "Episode:869 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.6191 dloss:1.0865 aloss2:76.1439 exploreP:0.0100\n",
      "Episode:870 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.7442 dloss:1.0831 aloss2:70.5159 exploreP:0.0100\n",
      "Episode:871 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.8578 dloss:1.0791 aloss2:70.9056 exploreP:0.0100\n",
      "Episode:872 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.9414 dloss:1.0704 aloss2:71.7018 exploreP:0.0100\n",
      "Episode:873 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.8322 dloss:1.0771 aloss2:72.4619 exploreP:0.0100\n",
      "Episode:874 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.8193 dloss:1.0817 aloss2:77.4478 exploreP:0.0100\n",
      "Episode:875 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.6621 dloss:1.0796 aloss2:76.0765 exploreP:0.0100\n",
      "Episode:876 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.7608 dloss:1.0752 aloss2:78.2920 exploreP:0.0100\n",
      "Episode:877 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.7334 dloss:1.0754 aloss2:78.7578 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:878 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.6773 dloss:1.0892 aloss2:76.0138 exploreP:0.0100\n",
      "Episode:879 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.8487 dloss:1.0719 aloss2:77.7818 exploreP:0.0100\n",
      "Episode:880 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.8125 dloss:1.0713 aloss2:76.7541 exploreP:0.0100\n",
      "Episode:881 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.8853 dloss:1.0785 aloss2:78.9565 exploreP:0.0100\n",
      "Episode:882 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.9497 dloss:1.0810 aloss2:79.9746 exploreP:0.0100\n",
      "Episode:883 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.9888 dloss:1.0696 aloss2:79.5374 exploreP:0.0100\n",
      "Episode:884 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.9574 dloss:1.0820 aloss2:77.8883 exploreP:0.0100\n",
      "Episode:885 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0188 dloss:1.0761 aloss2:79.9669 exploreP:0.0100\n",
      "Episode:886 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1037 dloss:1.0704 aloss2:80.6584 exploreP:0.0100\n",
      "Episode:887 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.1542 dloss:1.0728 aloss2:78.2997 exploreP:0.0100\n",
      "Episode:888 meanR:-0.0200 R:-4.0000 rate:-0.3077 aloss:2.4649 dloss:1.0865 aloss2:76.2599 exploreP:0.0100\n",
      "Episode:889 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:2.2668 dloss:1.0716 aloss2:75.6813 exploreP:0.0100\n",
      "Episode:890 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:2.3491 dloss:1.0697 aloss2:76.0821 exploreP:0.0100\n",
      "Episode:891 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.1713 dloss:1.0901 aloss2:77.7764 exploreP:0.0100\n",
      "Episode:892 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.3069 dloss:1.0804 aloss2:78.3402 exploreP:0.0100\n",
      "Episode:893 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.3542 dloss:1.0688 aloss2:79.3327 exploreP:0.0100\n",
      "Episode:894 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.2794 dloss:1.0799 aloss2:79.0875 exploreP:0.0100\n",
      "Episode:895 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.2672 dloss:1.0752 aloss2:78.4155 exploreP:0.0100\n",
      "Episode:896 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.4277 dloss:1.0776 aloss2:80.4542 exploreP:0.0100\n",
      "Episode:897 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.4224 dloss:1.0843 aloss2:79.4349 exploreP:0.0100\n",
      "Episode:898 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.3449 dloss:1.0796 aloss2:77.9092 exploreP:0.0100\n",
      "Episode:899 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.4773 dloss:1.0703 aloss2:80.6524 exploreP:0.0100\n",
      "Episode:900 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.4267 dloss:1.0802 aloss2:79.7045 exploreP:0.0100\n",
      "Episode:901 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:2.7282 dloss:1.0787 aloss2:78.9000 exploreP:0.0100\n",
      "Episode:902 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.5278 dloss:1.0663 aloss2:78.8041 exploreP:0.0100\n",
      "Episode:903 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:2.5929 dloss:1.0714 aloss2:81.6939 exploreP:0.0100\n",
      "Episode:904 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.4793 dloss:1.0713 aloss2:77.8039 exploreP:0.0100\n",
      "Episode:905 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.4759 dloss:1.0724 aloss2:79.3405 exploreP:0.0100\n",
      "Episode:906 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.7172 dloss:1.0772 aloss2:80.4505 exploreP:0.0100\n",
      "Episode:907 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.5656 dloss:1.0755 aloss2:80.3806 exploreP:0.0100\n",
      "Episode:908 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.5551 dloss:1.0694 aloss2:78.3510 exploreP:0.0100\n",
      "Episode:909 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:2.6928 dloss:1.0631 aloss2:79.2792 exploreP:0.0100\n",
      "Episode:910 meanR:0.0300 R:1.0000 rate:0.0769 aloss:2.8948 dloss:1.0732 aloss2:80.7609 exploreP:0.0100\n",
      "Episode:911 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.7823 dloss:1.0830 aloss2:79.2682 exploreP:0.0100\n",
      "Episode:912 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.9840 dloss:1.0672 aloss2:78.4287 exploreP:0.0100\n",
      "Episode:913 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.7939 dloss:1.0697 aloss2:80.5419 exploreP:0.0100\n",
      "Episode:914 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.8613 dloss:1.0814 aloss2:79.1361 exploreP:0.0100\n",
      "Episode:915 meanR:0.0100 R:0.0000 rate:0.0000 aloss:3.1393 dloss:1.0801 aloss2:82.0218 exploreP:0.0100\n",
      "Episode:916 meanR:0.0100 R:0.0000 rate:0.0000 aloss:3.1308 dloss:1.0733 aloss2:79.1636 exploreP:0.0100\n",
      "Episode:917 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.9376 dloss:1.0771 aloss2:77.1439 exploreP:0.0100\n",
      "Episode:918 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.8576 dloss:1.0742 aloss2:77.4547 exploreP:0.0100\n",
      "Episode:919 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.7350 dloss:1.0701 aloss2:75.7549 exploreP:0.0100\n",
      "Episode:920 meanR:0.0100 R:0.0000 rate:0.0000 aloss:3.2415 dloss:1.0792 aloss2:79.2889 exploreP:0.0100\n",
      "Episode:921 meanR:0.0100 R:0.0000 rate:0.0000 aloss:3.0466 dloss:1.0747 aloss2:77.6432 exploreP:0.0100\n",
      "Episode:922 meanR:0.0300 R:0.0000 rate:0.0000 aloss:3.0225 dloss:1.0679 aloss2:78.1498 exploreP:0.0100\n",
      "Episode:923 meanR:0.0200 R:0.0000 rate:0.0000 aloss:3.1579 dloss:1.0582 aloss2:80.3617 exploreP:0.0100\n",
      "Episode:924 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:3.2712 dloss:1.0822 aloss2:81.7094 exploreP:0.0100\n",
      "Episode:925 meanR:0.0200 R:1.0000 rate:0.0769 aloss:3.1654 dloss:1.0644 aloss2:80.6774 exploreP:0.0100\n",
      "Episode:926 meanR:0.0200 R:1.0000 rate:0.0769 aloss:3.4688 dloss:1.0744 aloss2:81.6887 exploreP:0.0100\n",
      "Episode:927 meanR:0.0200 R:0.0000 rate:0.0000 aloss:3.2169 dloss:1.0626 aloss2:81.3584 exploreP:0.0100\n",
      "Episode:928 meanR:0.0200 R:0.0000 rate:0.0000 aloss:3.3627 dloss:1.0774 aloss2:81.6405 exploreP:0.0100\n",
      "Episode:929 meanR:0.0300 R:1.0000 rate:0.0769 aloss:3.4711 dloss:1.0684 aloss2:82.9860 exploreP:0.0100\n",
      "Episode:930 meanR:0.0300 R:0.0000 rate:0.0000 aloss:3.6843 dloss:1.0706 aloss2:82.8139 exploreP:0.0100\n",
      "Episode:931 meanR:0.0200 R:0.0000 rate:0.0000 aloss:3.7310 dloss:1.0747 aloss2:83.6347 exploreP:0.0100\n",
      "Episode:932 meanR:0.0200 R:0.0000 rate:0.0000 aloss:3.9072 dloss:1.0744 aloss2:83.7507 exploreP:0.0100\n",
      "Episode:933 meanR:0.0000 R:0.0000 rate:0.0000 aloss:3.7463 dloss:1.0766 aloss2:84.6196 exploreP:0.0100\n",
      "Episode:934 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:3.8285 dloss:1.0718 aloss2:84.2783 exploreP:0.0100\n",
      "Episode:935 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:3.8143 dloss:1.0747 aloss2:85.8434 exploreP:0.0100\n",
      "Episode:936 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.0489 dloss:1.0641 aloss2:84.8053 exploreP:0.0100\n",
      "Episode:937 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:3.9349 dloss:1.0703 aloss2:83.0846 exploreP:0.0100\n",
      "Episode:938 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:3.9389 dloss:1.0745 aloss2:83.5829 exploreP:0.0100\n",
      "Episode:939 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.2279 dloss:1.0669 aloss2:86.2384 exploreP:0.0100\n",
      "Episode:940 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.0263 dloss:1.0687 aloss2:83.5065 exploreP:0.0100\n",
      "Episode:941 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.0200 dloss:1.0768 aloss2:83.9716 exploreP:0.0100\n",
      "Episode:942 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:4.2172 dloss:1.0679 aloss2:82.0167 exploreP:0.0100\n",
      "Episode:943 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.2417 dloss:1.0681 aloss2:84.8321 exploreP:0.0100\n",
      "Episode:944 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.0921 dloss:1.0716 aloss2:83.6005 exploreP:0.0100\n",
      "Episode:945 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.0061 dloss:1.0763 aloss2:85.6995 exploreP:0.0100\n",
      "Episode:946 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.2382 dloss:1.0788 aloss2:82.2391 exploreP:0.0100\n",
      "Episode:947 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.3891 dloss:1.0706 aloss2:83.3189 exploreP:0.0100\n",
      "Episode:948 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:4.7250 dloss:1.0634 aloss2:83.3417 exploreP:0.0100\n",
      "Episode:949 meanR:-0.0500 R:1.0000 rate:0.0769 aloss:4.5599 dloss:1.0748 aloss2:83.3773 exploreP:0.0100\n",
      "Episode:950 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.6208 dloss:1.0683 aloss2:83.6494 exploreP:0.0100\n",
      "Episode:951 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.4817 dloss:1.0767 aloss2:83.4688 exploreP:0.0100\n",
      "Episode:952 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.3157 dloss:1.0670 aloss2:81.9635 exploreP:0.0100\n",
      "Episode:953 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.5269 dloss:1.0751 aloss2:84.6547 exploreP:0.0100\n",
      "Episode:954 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.2243 dloss:1.0723 aloss2:83.2924 exploreP:0.0100\n",
      "Episode:955 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.5673 dloss:1.0735 aloss2:86.6671 exploreP:0.0100\n",
      "Episode:956 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.7478 dloss:1.0807 aloss2:86.7663 exploreP:0.0100\n",
      "Episode:957 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.4451 dloss:1.0633 aloss2:83.9727 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:958 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.6522 dloss:1.0816 aloss2:86.0625 exploreP:0.0100\n",
      "Episode:959 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:4.6783 dloss:1.0706 aloss2:83.0466 exploreP:0.0100\n",
      "Episode:960 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.4038 dloss:1.0683 aloss2:82.5909 exploreP:0.0100\n",
      "Episode:961 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.6074 dloss:1.0679 aloss2:82.4024 exploreP:0.0100\n",
      "Episode:962 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.6221 dloss:1.0813 aloss2:87.9488 exploreP:0.0100\n",
      "Episode:963 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.7215 dloss:1.0717 aloss2:85.9879 exploreP:0.0100\n",
      "Episode:964 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.8574 dloss:1.0789 aloss2:81.7749 exploreP:0.0100\n",
      "Episode:965 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.5310 dloss:1.0800 aloss2:83.7759 exploreP:0.0100\n",
      "Episode:966 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.5427 dloss:1.0769 aloss2:84.2047 exploreP:0.0100\n",
      "Episode:967 meanR:-0.0600 R:-1.0000 rate:-0.0769 aloss:4.6934 dloss:1.0749 aloss2:85.3098 exploreP:0.0100\n",
      "Episode:968 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:4.8936 dloss:1.0771 aloss2:85.7135 exploreP:0.0100\n",
      "Episode:969 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.5953 dloss:1.0766 aloss2:83.1313 exploreP:0.0100\n",
      "Episode:970 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:4.7079 dloss:1.0711 aloss2:83.4166 exploreP:0.0100\n",
      "Episode:971 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.8210 dloss:1.0756 aloss2:84.5472 exploreP:0.0100\n",
      "Episode:972 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.7667 dloss:1.0686 aloss2:84.7418 exploreP:0.0100\n",
      "Episode:973 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:5.0555 dloss:1.0733 aloss2:87.5471 exploreP:0.0100\n",
      "Episode:974 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.9934 dloss:1.0675 aloss2:85.7686 exploreP:0.0100\n",
      "Episode:975 meanR:-0.0300 R:1.0000 rate:0.0769 aloss:5.0429 dloss:1.0763 aloss2:82.8835 exploreP:0.0100\n",
      "Episode:976 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.9492 dloss:1.0723 aloss2:83.4211 exploreP:0.0100\n",
      "Episode:977 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.8272 dloss:1.0797 aloss2:82.6411 exploreP:0.0100\n",
      "Episode:978 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:5.2111 dloss:1.0792 aloss2:83.5817 exploreP:0.0100\n",
      "Episode:979 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:5.4041 dloss:1.0832 aloss2:83.5083 exploreP:0.0100\n",
      "Episode:980 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:5.0180 dloss:1.0683 aloss2:83.6881 exploreP:0.0100\n",
      "Episode:981 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.7260 dloss:1.0758 aloss2:82.1264 exploreP:0.0100\n",
      "Episode:982 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:4.9945 dloss:1.0842 aloss2:82.8595 exploreP:0.0100\n",
      "Episode:983 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:4.9885 dloss:1.0639 aloss2:83.4625 exploreP:0.0100\n",
      "Episode:984 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.7919 dloss:1.0657 aloss2:82.6213 exploreP:0.0100\n",
      "Episode:985 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.9108 dloss:1.0774 aloss2:83.1714 exploreP:0.0100\n",
      "Episode:986 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.9746 dloss:1.0778 aloss2:81.7314 exploreP:0.0100\n",
      "Episode:987 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.9113 dloss:1.0703 aloss2:81.5877 exploreP:0.0100\n",
      "Episode:988 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.6869 dloss:1.0796 aloss2:81.6064 exploreP:0.0100\n",
      "Episode:989 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.8235 dloss:1.0845 aloss2:83.6194 exploreP:0.0100\n",
      "Episode:990 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.9426 dloss:1.0808 aloss2:80.2225 exploreP:0.0100\n",
      "Episode:991 meanR:-0.0300 R:-3.0000 rate:-0.2308 aloss:5.4225 dloss:1.0894 aloss2:81.3039 exploreP:0.0100\n",
      "Episode:992 meanR:-0.0300 R:0.0000 rate:0.0000 aloss:5.1709 dloss:1.0716 aloss2:83.3555 exploreP:0.0100\n",
      "Episode:993 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:5.0956 dloss:1.0757 aloss2:81.1057 exploreP:0.0100\n",
      "Episode:994 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:4.4510 dloss:1.0725 aloss2:79.9675 exploreP:0.0100\n",
      "Episode:995 meanR:-0.0500 R:-1.0000 rate:-0.0769 aloss:4.5403 dloss:1.0794 aloss2:81.6227 exploreP:0.0100\n",
      "Episode:996 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.5589 dloss:1.0661 aloss2:81.9528 exploreP:0.0100\n",
      "Episode:997 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.4480 dloss:1.0704 aloss2:81.0835 exploreP:0.0100\n",
      "Episode:998 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.2886 dloss:1.0762 aloss2:79.7643 exploreP:0.0100\n",
      "Episode:999 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:3.6583 dloss:1.0744 aloss2:78.0163 exploreP:0.0100\n",
      "Episode:1000 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.4641 dloss:1.0809 aloss2:82.6076 exploreP:0.0100\n",
      "Episode:1001 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.2466 dloss:1.0828 aloss2:80.5158 exploreP:0.0100\n",
      "Episode:1002 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:4.1889 dloss:1.0736 aloss2:79.4645 exploreP:0.0100\n",
      "Episode:1003 meanR:-0.0600 R:-1.0000 rate:-0.0769 aloss:4.1710 dloss:1.0737 aloss2:80.5943 exploreP:0.0100\n",
      "Episode:1004 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:3.6558 dloss:1.0694 aloss2:77.5509 exploreP:0.0100\n",
      "Episode:1005 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:3.7167 dloss:1.0667 aloss2:79.5673 exploreP:0.0100\n",
      "Episode:1006 meanR:-0.0600 R:0.0000 rate:0.0000 aloss:3.8631 dloss:1.0703 aloss2:79.3926 exploreP:0.0100\n",
      "Episode:1007 meanR:-0.0500 R:1.0000 rate:0.0769 aloss:4.0590 dloss:1.0655 aloss2:80.2360 exploreP:0.0100\n",
      "Episode:1008 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:3.7545 dloss:1.0838 aloss2:80.9485 exploreP:0.0100\n",
      "Episode:1009 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:3.5448 dloss:1.0788 aloss2:80.9840 exploreP:0.0100\n",
      "Episode:1010 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:3.1903 dloss:1.0630 aloss2:75.5048 exploreP:0.0100\n",
      "Episode:1011 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:3.3336 dloss:1.0777 aloss2:75.6984 exploreP:0.0100\n",
      "Episode:1012 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:3.5730 dloss:1.0717 aloss2:76.5039 exploreP:0.0100\n",
      "Episode:1013 meanR:-0.0600 R:-1.0000 rate:-0.0769 aloss:3.0797 dloss:1.0678 aloss2:73.1443 exploreP:0.0100\n",
      "Episode:1014 meanR:-0.0900 R:-3.0000 rate:-0.2308 aloss:3.1840 dloss:1.0734 aloss2:73.7797 exploreP:0.0100\n",
      "Episode:1015 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:3.4908 dloss:1.0691 aloss2:74.6552 exploreP:0.0100\n",
      "Episode:1016 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:3.5590 dloss:1.0763 aloss2:75.0035 exploreP:0.0100\n",
      "Episode:1017 meanR:-0.1000 R:-2.0000 rate:-0.1538 aloss:3.9218 dloss:1.0742 aloss2:75.6121 exploreP:0.0100\n",
      "Episode:1018 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:3.4326 dloss:1.0777 aloss2:73.6578 exploreP:0.0100\n",
      "Episode:1019 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:4.1883 dloss:1.0773 aloss2:75.0506 exploreP:0.0100\n",
      "Episode:1020 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:3.7589 dloss:1.0692 aloss2:75.2288 exploreP:0.0100\n",
      "Episode:1021 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:3.6329 dloss:1.0830 aloss2:76.0436 exploreP:0.0100\n",
      "Episode:1022 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:3.5875 dloss:1.0790 aloss2:74.7798 exploreP:0.0100\n",
      "Episode:1023 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:3.3570 dloss:1.0757 aloss2:73.0594 exploreP:0.0100\n",
      "Episode:1024 meanR:-0.1000 R:-1.0000 rate:-0.0769 aloss:3.7395 dloss:1.0763 aloss2:71.4429 exploreP:0.0100\n",
      "Episode:1025 meanR:-0.1200 R:-1.0000 rate:-0.0769 aloss:3.2116 dloss:1.0724 aloss2:73.0583 exploreP:0.0100\n",
      "Episode:1026 meanR:-0.1000 R:3.0000 rate:0.2308 aloss:3.7234 dloss:1.0773 aloss2:75.1386 exploreP:0.0100\n",
      "Episode:1027 meanR:-0.0900 R:1.0000 rate:0.0769 aloss:3.6735 dloss:1.0819 aloss2:75.9453 exploreP:0.0100\n",
      "Episode:1028 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:3.9095 dloss:1.0769 aloss2:74.9502 exploreP:0.0100\n",
      "Episode:1029 meanR:-0.0900 R:0.0000 rate:0.0000 aloss:4.1318 dloss:1.1004 aloss2:75.4787 exploreP:0.0100\n",
      "Episode:1030 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:5.0548 dloss:1.0864 aloss2:75.9958 exploreP:0.0100\n",
      "Episode:1031 meanR:-0.0700 R:1.0000 rate:0.0769 aloss:5.2836 dloss:1.0865 aloss2:75.9651 exploreP:0.0100\n",
      "Episode:1032 meanR:-0.0700 R:0.0000 rate:0.0000 aloss:3.7415 dloss:1.0812 aloss2:72.2368 exploreP:0.0100\n",
      "Episode:1033 meanR:-0.0900 R:-2.0000 rate:-0.1538 aloss:4.8411 dloss:1.0886 aloss2:75.3060 exploreP:0.0100\n",
      "Episode:1034 meanR:-0.0900 R:-1.0000 rate:-0.0769 aloss:4.0634 dloss:1.0795 aloss2:72.6007 exploreP:0.0100\n",
      "Episode:1035 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:4.4249 dloss:1.0836 aloss2:74.6046 exploreP:0.0100\n",
      "Episode:1036 meanR:-0.0900 R:-1.0000 rate:-0.0769 aloss:4.9157 dloss:1.0917 aloss2:75.2578 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1037 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:4.4904 dloss:1.0923 aloss2:74.5698 exploreP:0.0100\n",
      "Episode:1038 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:3.4590 dloss:1.0892 aloss2:71.4995 exploreP:0.0100\n",
      "Episode:1039 meanR:-0.0800 R:0.0000 rate:0.0000 aloss:3.8885 dloss:1.0824 aloss2:72.9927 exploreP:0.0100\n",
      "Episode:1040 meanR:-0.1000 R:-2.0000 rate:-0.1538 aloss:3.6646 dloss:1.0852 aloss2:73.0514 exploreP:0.0100\n",
      "Episode:1041 meanR:-0.0700 R:3.0000 rate:0.2308 aloss:6.1268 dloss:1.0867 aloss2:78.5154 exploreP:0.0100\n",
      "Episode:1042 meanR:-0.1000 R:-4.0000 rate:-0.3077 aloss:3.4525 dloss:1.0917 aloss2:72.5629 exploreP:0.0100\n",
      "Episode:1043 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:3.5961 dloss:1.1048 aloss2:72.8273 exploreP:0.0100\n",
      "Episode:1044 meanR:-0.0800 R:2.0000 rate:0.1538 aloss:3.5957 dloss:1.1035 aloss2:74.8439 exploreP:0.0100\n",
      "Episode:1045 meanR:-0.1000 R:-2.0000 rate:-0.1538 aloss:4.5125 dloss:1.0912 aloss2:75.6121 exploreP:0.0100\n",
      "Episode:1046 meanR:-0.0900 R:1.0000 rate:0.0769 aloss:4.6936 dloss:1.1055 aloss2:75.6418 exploreP:0.0100\n",
      "Episode:1047 meanR:-0.0800 R:1.0000 rate:0.0769 aloss:4.5100 dloss:1.0808 aloss2:74.8488 exploreP:0.0100\n",
      "Episode:1048 meanR:-0.0900 R:-2.0000 rate:-0.1538 aloss:3.4228 dloss:1.0934 aloss2:72.9193 exploreP:0.0100\n",
      "Episode:1049 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:4.0636 dloss:1.1092 aloss2:74.5592 exploreP:0.0100\n",
      "Episode:1050 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:3.7669 dloss:1.0966 aloss2:73.4903 exploreP:0.0100\n",
      "Episode:1051 meanR:-0.0900 R:2.0000 rate:0.1538 aloss:4.8666 dloss:1.0891 aloss2:75.1382 exploreP:0.0100\n",
      "Episode:1052 meanR:-0.0900 R:0.0000 rate:0.0000 aloss:4.3768 dloss:1.0891 aloss2:73.9449 exploreP:0.0100\n",
      "Episode:1053 meanR:-0.0900 R:0.0000 rate:0.0000 aloss:4.4499 dloss:1.1144 aloss2:72.3760 exploreP:0.0100\n",
      "Episode:1054 meanR:-0.0900 R:0.0000 rate:0.0000 aloss:3.4904 dloss:1.0910 aloss2:72.7949 exploreP:0.0100\n",
      "Episode:1055 meanR:-0.1200 R:-3.0000 rate:-0.2308 aloss:3.6088 dloss:1.0846 aloss2:73.6786 exploreP:0.0100\n",
      "Episode:1056 meanR:-0.1200 R:0.0000 rate:0.0000 aloss:5.9356 dloss:1.1000 aloss2:79.3071 exploreP:0.0100\n",
      "Episode:1057 meanR:-0.1100 R:1.0000 rate:0.0769 aloss:4.8853 dloss:1.0891 aloss2:75.9948 exploreP:0.0100\n",
      "Episode:1058 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:5.0658 dloss:1.1041 aloss2:75.4546 exploreP:0.0100\n",
      "Episode:1059 meanR:-0.0900 R:2.0000 rate:0.1538 aloss:4.2619 dloss:1.0918 aloss2:74.4151 exploreP:0.0100\n",
      "Episode:1060 meanR:-0.1100 R:-2.0000 rate:-0.1538 aloss:4.8338 dloss:1.0907 aloss2:74.7964 exploreP:0.0100\n",
      "Episode:1061 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:4.2236 dloss:1.1021 aloss2:72.3756 exploreP:0.0100\n",
      "Episode:1062 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:4.3830 dloss:1.1099 aloss2:72.9524 exploreP:0.0100\n",
      "Episode:1063 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:3.5317 dloss:1.0819 aloss2:72.4168 exploreP:0.0100\n",
      "Episode:1064 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:4.3804 dloss:1.1018 aloss2:71.7694 exploreP:0.0100\n",
      "Episode:1065 meanR:-0.1000 R:0.0000 rate:0.0000 aloss:3.6548 dloss:1.0981 aloss2:74.1390 exploreP:0.0100\n",
      "Episode:1066 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:4.1930 dloss:1.1018 aloss2:77.1266 exploreP:0.0100\n",
      "Episode:1067 meanR:-0.1200 R:-2.0000 rate:-0.1538 aloss:4.2378 dloss:1.0875 aloss2:76.7329 exploreP:0.0100\n",
      "Episode:1068 meanR:-0.0900 R:2.0000 rate:0.1538 aloss:4.0948 dloss:1.1001 aloss2:75.7501 exploreP:0.0100\n",
      "Episode:1069 meanR:-0.1000 R:-1.0000 rate:-0.0769 aloss:5.0951 dloss:1.1050 aloss2:76.7128 exploreP:0.0100\n",
      "Episode:1070 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:5.1194 dloss:1.0972 aloss2:75.4385 exploreP:0.0100\n",
      "Episode:1071 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:3.9364 dloss:1.1141 aloss2:73.5498 exploreP:0.0100\n",
      "Episode:1072 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:4.4793 dloss:1.0900 aloss2:74.1047 exploreP:0.0100\n",
      "Episode:1073 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:4.2078 dloss:1.1075 aloss2:74.5325 exploreP:0.0100\n",
      "Episode:1074 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:4.3555 dloss:1.1168 aloss2:73.4869 exploreP:0.0100\n",
      "Episode:1075 meanR:-0.1100 R:1.0000 rate:0.0769 aloss:3.2549 dloss:1.0967 aloss2:71.4172 exploreP:0.0100\n",
      "Episode:1076 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:3.1460 dloss:1.0920 aloss2:73.3917 exploreP:0.0100\n",
      "Episode:1077 meanR:-0.1200 R:-1.0000 rate:-0.0769 aloss:3.2652 dloss:1.0950 aloss2:73.2274 exploreP:0.0100\n",
      "Episode:1078 meanR:-0.1300 R:-1.0000 rate:-0.0769 aloss:3.6647 dloss:1.1077 aloss2:75.6197 exploreP:0.0100\n",
      "Episode:1079 meanR:-0.1500 R:-2.0000 rate:-0.1538 aloss:3.9528 dloss:1.0968 aloss2:76.7127 exploreP:0.0100\n",
      "Episode:1080 meanR:-0.1800 R:-3.0000 rate:-0.2308 aloss:3.6777 dloss:1.1001 aloss2:75.0181 exploreP:0.0100\n",
      "Episode:1081 meanR:-0.1800 R:0.0000 rate:0.0000 aloss:5.2013 dloss:1.1078 aloss2:76.0507 exploreP:0.0100\n",
      "Episode:1082 meanR:-0.1600 R:2.0000 rate:0.1538 aloss:5.1049 dloss:1.1099 aloss2:75.8472 exploreP:0.0100\n",
      "Episode:1083 meanR:-0.1500 R:0.0000 rate:0.0000 aloss:5.0095 dloss:1.1104 aloss2:75.3887 exploreP:0.0100\n",
      "Episode:1084 meanR:-0.1400 R:1.0000 rate:0.0769 aloss:4.5462 dloss:1.1105 aloss2:73.5504 exploreP:0.0100\n",
      "Episode:1085 meanR:-0.1200 R:2.0000 rate:0.1538 aloss:4.5361 dloss:1.0976 aloss2:73.1820 exploreP:0.0100\n",
      "Episode:1086 meanR:-0.1300 R:-1.0000 rate:-0.0769 aloss:4.8544 dloss:1.1247 aloss2:74.7183 exploreP:0.0100\n",
      "Episode:1087 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:4.7440 dloss:1.1055 aloss2:74.1217 exploreP:0.0100\n",
      "Episode:1088 meanR:-0.1200 R:1.0000 rate:0.0769 aloss:4.7998 dloss:1.0995 aloss2:74.1274 exploreP:0.0100\n",
      "Episode:1089 meanR:-0.1200 R:0.0000 rate:0.0000 aloss:4.5230 dloss:1.1073 aloss2:73.8638 exploreP:0.0100\n",
      "Episode:1090 meanR:-0.1400 R:-2.0000 rate:-0.1538 aloss:4.1979 dloss:1.1097 aloss2:72.6892 exploreP:0.0100\n",
      "Episode:1091 meanR:-0.1100 R:0.0000 rate:0.0000 aloss:4.2825 dloss:1.1042 aloss2:74.3122 exploreP:0.0100\n",
      "Episode:1092 meanR:-0.1300 R:-2.0000 rate:-0.1538 aloss:4.2242 dloss:1.1056 aloss2:73.8416 exploreP:0.0100\n",
      "Episode:1093 meanR:-0.1200 R:0.0000 rate:0.0000 aloss:4.4485 dloss:1.1195 aloss2:76.3890 exploreP:0.0100\n",
      "Episode:1094 meanR:-0.1300 R:-1.0000 rate:-0.0769 aloss:5.1035 dloss:1.1107 aloss2:74.8440 exploreP:0.0100\n",
      "Episode:1095 meanR:-0.1300 R:-1.0000 rate:-0.0769 aloss:4.9288 dloss:1.1060 aloss2:73.8568 exploreP:0.0100\n",
      "Episode:1096 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:4.7199 dloss:1.0985 aloss2:70.6168 exploreP:0.0100\n",
      "Episode:1097 meanR:-0.1400 R:-1.0000 rate:-0.0769 aloss:3.9962 dloss:1.1269 aloss2:73.4329 exploreP:0.0100\n",
      "Episode:1098 meanR:-0.1400 R:0.0000 rate:0.0000 aloss:3.9418 dloss:1.1041 aloss2:76.7566 exploreP:0.0100\n",
      "Episode:1099 meanR:-0.1600 R:-2.0000 rate:-0.1538 aloss:3.6264 dloss:1.1013 aloss2:74.4516 exploreP:0.0100\n",
      "Episode:1100 meanR:-0.1700 R:-1.0000 rate:-0.0769 aloss:4.2915 dloss:1.0950 aloss2:76.5153 exploreP:0.0100\n",
      "Episode:1101 meanR:-0.1700 R:0.0000 rate:0.0000 aloss:3.6652 dloss:1.1132 aloss2:72.2801 exploreP:0.0100\n",
      "Episode:1102 meanR:-0.2000 R:-3.0000 rate:-0.2308 aloss:3.8955 dloss:1.1123 aloss2:75.9728 exploreP:0.0100\n",
      "Episode:1103 meanR:-0.1900 R:0.0000 rate:0.0000 aloss:4.0175 dloss:1.1157 aloss2:75.6729 exploreP:0.0100\n",
      "Episode:1104 meanR:-0.2200 R:-3.0000 rate:-0.2308 aloss:3.9993 dloss:1.1049 aloss2:74.4901 exploreP:0.0100\n",
      "Episode:1105 meanR:-0.2100 R:1.0000 rate:0.0769 aloss:4.1634 dloss:1.1119 aloss2:75.1094 exploreP:0.0100\n",
      "Episode:1106 meanR:-0.2300 R:-2.0000 rate:-0.1538 aloss:4.1721 dloss:1.1011 aloss2:74.1495 exploreP:0.0100\n",
      "Episode:1107 meanR:-0.2600 R:-2.0000 rate:-0.1538 aloss:4.3217 dloss:1.1025 aloss2:74.8254 exploreP:0.0100\n",
      "Episode:1108 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:4.2069 dloss:1.1066 aloss2:71.3157 exploreP:0.0100\n",
      "Episode:1109 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:4.4760 dloss:1.1170 aloss2:75.1913 exploreP:0.0100\n",
      "Episode:1110 meanR:-0.2900 R:-3.0000 rate:-0.2308 aloss:4.2530 dloss:1.0975 aloss2:73.1553 exploreP:0.0100\n",
      "Episode:1111 meanR:-0.2900 R:0.0000 rate:0.0000 aloss:4.3979 dloss:1.1120 aloss2:74.0050 exploreP:0.0100\n",
      "Episode:1112 meanR:-0.2900 R:0.0000 rate:0.0000 aloss:3.5839 dloss:1.1068 aloss2:72.7939 exploreP:0.0100\n",
      "Episode:1113 meanR:-0.2800 R:0.0000 rate:0.0000 aloss:3.4338 dloss:1.1013 aloss2:72.4790 exploreP:0.0100\n",
      "Episode:1114 meanR:-0.2500 R:0.0000 rate:0.0000 aloss:4.1679 dloss:1.1148 aloss2:76.3676 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1115 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:3.9529 dloss:1.1169 aloss2:74.7586 exploreP:0.0100\n",
      "Episode:1116 meanR:-0.2400 R:2.0000 rate:0.1538 aloss:4.6171 dloss:1.1005 aloss2:77.0371 exploreP:0.0100\n",
      "Episode:1117 meanR:-0.2100 R:1.0000 rate:0.0769 aloss:5.0488 dloss:1.0995 aloss2:78.0471 exploreP:0.0100\n",
      "Episode:1118 meanR:-0.2300 R:-3.0000 rate:-0.2308 aloss:5.0988 dloss:1.1056 aloss2:76.8521 exploreP:0.0100\n",
      "Episode:1119 meanR:-0.2300 R:0.0000 rate:0.0000 aloss:5.1169 dloss:1.1128 aloss2:75.5577 exploreP:0.0100\n",
      "Episode:1120 meanR:-0.2500 R:-1.0000 rate:-0.0769 aloss:5.0144 dloss:1.1112 aloss2:73.8597 exploreP:0.0100\n",
      "Episode:1121 meanR:-0.2500 R:0.0000 rate:0.0000 aloss:5.2937 dloss:1.1053 aloss2:71.7727 exploreP:0.0100\n",
      "Episode:1122 meanR:-0.2500 R:-1.0000 rate:-0.0769 aloss:4.4687 dloss:1.1134 aloss2:73.2275 exploreP:0.0100\n",
      "Episode:1123 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:3.6971 dloss:1.0907 aloss2:72.7211 exploreP:0.0100\n",
      "Episode:1124 meanR:-0.2400 R:1.0000 rate:0.0769 aloss:3.9504 dloss:1.1052 aloss2:74.2439 exploreP:0.0100\n",
      "Episode:1125 meanR:-0.2300 R:0.0000 rate:0.0000 aloss:3.8097 dloss:1.1160 aloss2:74.4989 exploreP:0.0100\n",
      "Episode:1126 meanR:-0.2700 R:-1.0000 rate:-0.0769 aloss:3.1686 dloss:1.1023 aloss2:73.0574 exploreP:0.0100\n",
      "Episode:1127 meanR:-0.2800 R:0.0000 rate:0.0000 aloss:3.0031 dloss:1.0947 aloss2:72.3168 exploreP:0.0100\n",
      "Episode:1128 meanR:-0.2900 R:0.0000 rate:0.0000 aloss:3.3524 dloss:1.1148 aloss2:73.7607 exploreP:0.0100\n",
      "Episode:1129 meanR:-0.2800 R:1.0000 rate:0.0769 aloss:3.7211 dloss:1.1077 aloss2:76.0943 exploreP:0.0100\n",
      "Episode:1130 meanR:-0.2900 R:0.0000 rate:0.0000 aloss:4.5343 dloss:1.1006 aloss2:80.0830 exploreP:0.0100\n",
      "Episode:1131 meanR:-0.2900 R:1.0000 rate:0.0769 aloss:4.1371 dloss:1.1197 aloss2:77.3115 exploreP:0.0100\n",
      "Episode:1132 meanR:-0.2900 R:0.0000 rate:0.0000 aloss:4.6160 dloss:1.1180 aloss2:78.2823 exploreP:0.0100\n",
      "Episode:1133 meanR:-0.2900 R:-2.0000 rate:-0.1538 aloss:5.3204 dloss:1.1062 aloss2:78.3171 exploreP:0.0100\n",
      "Episode:1134 meanR:-0.2800 R:0.0000 rate:0.0000 aloss:5.0692 dloss:1.1165 aloss2:75.7123 exploreP:0.0100\n",
      "Episode:1135 meanR:-0.2700 R:2.0000 rate:0.1538 aloss:4.8875 dloss:1.1217 aloss2:75.3004 exploreP:0.0100\n",
      "Episode:1136 meanR:-0.2500 R:1.0000 rate:0.0769 aloss:4.8156 dloss:1.1203 aloss2:74.8284 exploreP:0.0100\n",
      "Episode:1137 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:4.4864 dloss:1.1107 aloss2:71.9955 exploreP:0.0100\n",
      "Episode:1138 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:4.7197 dloss:1.1227 aloss2:73.2397 exploreP:0.0100\n",
      "Episode:1139 meanR:-0.2600 R:0.0000 rate:0.0000 aloss:4.0937 dloss:1.1363 aloss2:73.9132 exploreP:0.0100\n",
      "Episode:1140 meanR:-0.2300 R:1.0000 rate:0.0769 aloss:3.6842 dloss:1.1003 aloss2:73.5671 exploreP:0.0100\n",
      "Episode:1141 meanR:-0.2500 R:1.0000 rate:0.0769 aloss:3.1822 dloss:1.1152 aloss2:74.2454 exploreP:0.0100\n",
      "Episode:1142 meanR:-0.2200 R:-1.0000 rate:-0.0769 aloss:3.2363 dloss:1.1141 aloss2:73.0288 exploreP:0.0100\n",
      "Episode:1143 meanR:-0.2100 R:1.0000 rate:0.0769 aloss:3.7008 dloss:1.1074 aloss2:75.2816 exploreP:0.0100\n",
      "Episode:1144 meanR:-0.2100 R:2.0000 rate:0.1538 aloss:4.0285 dloss:1.1227 aloss2:77.7508 exploreP:0.0100\n",
      "Episode:1145 meanR:-0.1600 R:3.0000 rate:0.2308 aloss:4.5074 dloss:1.1128 aloss2:79.3614 exploreP:0.0100\n",
      "Episode:1146 meanR:-0.1500 R:2.0000 rate:0.1538 aloss:4.1263 dloss:1.1325 aloss2:77.4729 exploreP:0.0100\n",
      "Episode:1147 meanR:-0.1700 R:-1.0000 rate:-0.0769 aloss:4.7206 dloss:1.1087 aloss2:77.7944 exploreP:0.0100\n",
      "Episode:1148 meanR:-0.1400 R:1.0000 rate:0.0769 aloss:4.3002 dloss:1.1204 aloss2:74.5660 exploreP:0.0100\n",
      "Episode:1149 meanR:-0.1400 R:0.0000 rate:0.0000 aloss:3.8709 dloss:1.1249 aloss2:73.6708 exploreP:0.0100\n",
      "Episode:1150 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:3.3103 dloss:1.1179 aloss2:72.9275 exploreP:0.0100\n",
      "Episode:1151 meanR:-0.1600 R:-1.0000 rate:-0.0769 aloss:3.3189 dloss:1.1297 aloss2:73.8262 exploreP:0.0100\n",
      "Episode:1152 meanR:-0.1500 R:1.0000 rate:0.0769 aloss:4.3018 dloss:1.1313 aloss2:78.9559 exploreP:0.0100\n",
      "Episode:1153 meanR:-0.1500 R:0.0000 rate:0.0000 aloss:4.6647 dloss:1.1084 aloss2:78.8608 exploreP:0.0100\n",
      "Episode:1154 meanR:-0.1400 R:1.0000 rate:0.0769 aloss:4.0376 dloss:1.1429 aloss2:75.6325 exploreP:0.0100\n",
      "Episode:1155 meanR:-0.1000 R:1.0000 rate:0.0769 aloss:4.6519 dloss:1.1415 aloss2:76.9505 exploreP:0.0100\n",
      "Episode:1156 meanR:-0.1100 R:-1.0000 rate:-0.0769 aloss:4.8287 dloss:1.1222 aloss2:77.5922 exploreP:0.0100\n",
      "Episode:1157 meanR:-0.1200 R:0.0000 rate:0.0000 aloss:4.0003 dloss:1.1252 aloss2:71.3497 exploreP:0.0100\n",
      "Episode:1158 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:4.0436 dloss:1.1463 aloss2:75.4229 exploreP:0.0100\n",
      "Episode:1159 meanR:-0.1500 R:0.0000 rate:0.0000 aloss:3.2898 dloss:1.1339 aloss2:73.4369 exploreP:0.0100\n",
      "Episode:1160 meanR:-0.1500 R:-2.0000 rate:-0.1538 aloss:3.3531 dloss:1.1150 aloss2:75.2101 exploreP:0.0100\n",
      "Episode:1161 meanR:-0.1700 R:-1.0000 rate:-0.0769 aloss:4.0497 dloss:1.1246 aloss2:78.5336 exploreP:0.0100\n",
      "Episode:1162 meanR:-0.1700 R:0.0000 rate:0.0000 aloss:3.7449 dloss:1.1353 aloss2:75.0003 exploreP:0.0100\n",
      "Episode:1163 meanR:-0.1700 R:0.0000 rate:0.0000 aloss:3.9885 dloss:1.1116 aloss2:76.8498 exploreP:0.0100\n",
      "Episode:1164 meanR:-0.1500 R:2.0000 rate:0.1538 aloss:3.9881 dloss:1.1215 aloss2:76.5051 exploreP:0.0100\n",
      "Episode:1165 meanR:-0.1500 R:0.0000 rate:0.0000 aloss:3.7686 dloss:1.1342 aloss2:72.8357 exploreP:0.0100\n",
      "Episode:1166 meanR:-0.1500 R:-1.0000 rate:-0.0769 aloss:3.6929 dloss:1.1327 aloss2:76.0028 exploreP:0.0100\n",
      "Episode:1167 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:3.5354 dloss:1.1144 aloss2:76.3478 exploreP:0.0100\n",
      "Episode:1168 meanR:-0.1400 R:1.0000 rate:0.0769 aloss:4.3583 dloss:1.1368 aloss2:79.9511 exploreP:0.0100\n",
      "Episode:1169 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:3.8493 dloss:1.1251 aloss2:76.0990 exploreP:0.0100\n",
      "Episode:1170 meanR:-0.1400 R:0.0000 rate:0.0000 aloss:4.2354 dloss:1.1339 aloss2:75.9338 exploreP:0.0100\n",
      "Episode:1171 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:3.3725 dloss:1.1193 aloss2:75.5200 exploreP:0.0100\n",
      "Episode:1172 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:3.7453 dloss:1.1223 aloss2:76.6082 exploreP:0.0100\n",
      "Episode:1173 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:4.3648 dloss:1.1218 aloss2:81.7203 exploreP:0.0100\n",
      "Episode:1174 meanR:-0.1300 R:0.0000 rate:0.0000 aloss:4.2282 dloss:1.1261 aloss2:81.0170 exploreP:0.0100\n",
      "Episode:1175 meanR:-0.1500 R:-1.0000 rate:-0.0769 aloss:4.3392 dloss:1.1191 aloss2:81.2641 exploreP:0.0100\n",
      "Episode:1176 meanR:-0.1300 R:2.0000 rate:0.1538 aloss:4.2622 dloss:1.1231 aloss2:80.8239 exploreP:0.0100\n",
      "Episode:1177 meanR:-0.1100 R:1.0000 rate:0.0769 aloss:4.3878 dloss:1.1081 aloss2:79.3079 exploreP:0.0100\n",
      "Episode:1178 meanR:-0.0800 R:2.0000 rate:0.1538 aloss:4.6188 dloss:1.1256 aloss2:77.7245 exploreP:0.0100\n",
      "Episode:1179 meanR:-0.0300 R:3.0000 rate:0.2308 aloss:4.2866 dloss:1.1234 aloss2:78.7126 exploreP:0.0100\n",
      "Episode:1180 meanR:0.0100 R:1.0000 rate:0.0769 aloss:3.8251 dloss:1.1108 aloss2:77.5930 exploreP:0.0100\n",
      "Episode:1181 meanR:0.0200 R:1.0000 rate:0.0769 aloss:4.1477 dloss:1.1351 aloss2:78.1841 exploreP:0.0100\n",
      "Episode:1182 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:3.8601 dloss:1.1394 aloss2:79.0443 exploreP:0.0100\n",
      "Episode:1183 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:3.9418 dloss:1.1359 aloss2:75.9416 exploreP:0.0100\n",
      "Episode:1184 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:3.9241 dloss:1.1345 aloss2:75.8210 exploreP:0.0100\n",
      "Episode:1185 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:3.7356 dloss:1.1180 aloss2:76.2048 exploreP:0.0100\n",
      "Episode:1186 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.0429 dloss:1.1275 aloss2:80.0739 exploreP:0.0100\n",
      "Episode:1187 meanR:0.0000 R:1.0000 rate:0.0769 aloss:4.4219 dloss:1.1144 aloss2:77.9180 exploreP:0.0100\n",
      "Episode:1188 meanR:0.0100 R:2.0000 rate:0.1538 aloss:5.2804 dloss:1.1274 aloss2:81.3755 exploreP:0.0100\n",
      "Episode:1189 meanR:0.0100 R:0.0000 rate:0.0000 aloss:4.5551 dloss:1.1427 aloss2:82.6459 exploreP:0.0100\n",
      "Episode:1190 meanR:0.0400 R:1.0000 rate:0.0769 aloss:4.6629 dloss:1.1226 aloss2:81.5177 exploreP:0.0100\n",
      "Episode:1191 meanR:0.0200 R:-2.0000 rate:-0.1538 aloss:4.6786 dloss:1.1394 aloss2:80.2794 exploreP:0.0100\n",
      "Episode:1192 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:5.1025 dloss:1.1196 aloss2:82.3247 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1193 meanR:0.0300 R:0.0000 rate:0.0000 aloss:4.3947 dloss:1.1252 aloss2:79.6097 exploreP:0.0100\n",
      "Episode:1194 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:3.8186 dloss:1.1337 aloss2:78.0697 exploreP:0.0100\n",
      "Episode:1195 meanR:0.0200 R:-2.0000 rate:-0.1538 aloss:3.8817 dloss:1.1324 aloss2:80.0111 exploreP:0.0100\n",
      "Episode:1196 meanR:0.0200 R:0.0000 rate:0.0000 aloss:4.2426 dloss:1.1253 aloss2:78.5235 exploreP:0.0100\n",
      "Episode:1197 meanR:0.0500 R:2.0000 rate:0.1538 aloss:4.0851 dloss:1.1347 aloss2:81.3125 exploreP:0.0100\n",
      "Episode:1198 meanR:0.0500 R:0.0000 rate:0.0000 aloss:4.1339 dloss:1.1338 aloss2:79.3762 exploreP:0.0100\n",
      "Episode:1199 meanR:0.0600 R:-1.0000 rate:-0.0769 aloss:4.7006 dloss:1.1406 aloss2:82.4949 exploreP:0.0100\n",
      "Episode:1200 meanR:0.0600 R:-1.0000 rate:-0.0769 aloss:4.2390 dloss:1.1330 aloss2:80.0752 exploreP:0.0100\n",
      "Episode:1201 meanR:0.0600 R:0.0000 rate:0.0000 aloss:4.0469 dloss:1.1386 aloss2:78.1863 exploreP:0.0100\n",
      "Episode:1202 meanR:0.0800 R:-1.0000 rate:-0.0769 aloss:3.6749 dloss:1.1206 aloss2:77.9155 exploreP:0.0100\n",
      "Episode:1203 meanR:0.0800 R:0.0000 rate:0.0000 aloss:3.9614 dloss:1.1359 aloss2:81.9666 exploreP:0.0100\n",
      "Episode:1204 meanR:0.1100 R:0.0000 rate:0.0000 aloss:3.7218 dloss:1.1347 aloss2:80.1567 exploreP:0.0100\n",
      "Episode:1205 meanR:0.0800 R:-2.0000 rate:-0.1538 aloss:3.8809 dloss:1.1342 aloss2:78.0982 exploreP:0.0100\n",
      "Episode:1206 meanR:0.1000 R:0.0000 rate:0.0000 aloss:3.9501 dloss:1.1425 aloss2:79.7364 exploreP:0.0100\n",
      "Episode:1207 meanR:0.1200 R:0.0000 rate:0.0000 aloss:3.3985 dloss:1.1320 aloss2:77.4560 exploreP:0.0100\n",
      "Episode:1208 meanR:0.1200 R:0.0000 rate:0.0000 aloss:3.8511 dloss:1.1461 aloss2:79.7427 exploreP:0.0100\n",
      "Episode:1209 meanR:0.1300 R:1.0000 rate:0.0769 aloss:4.7009 dloss:1.1431 aloss2:83.7821 exploreP:0.0100\n",
      "Episode:1210 meanR:0.1600 R:0.0000 rate:0.0000 aloss:3.6746 dloss:1.1259 aloss2:80.7718 exploreP:0.0100\n",
      "Episode:1211 meanR:0.1400 R:-2.0000 rate:-0.1538 aloss:3.8915 dloss:1.1385 aloss2:82.7375 exploreP:0.0100\n",
      "Episode:1212 meanR:0.1400 R:0.0000 rate:0.0000 aloss:3.5591 dloss:1.1330 aloss2:81.3795 exploreP:0.0100\n",
      "Episode:1213 meanR:0.1300 R:-1.0000 rate:-0.0769 aloss:3.6053 dloss:1.1303 aloss2:80.4908 exploreP:0.0100\n",
      "Episode:1214 meanR:0.1500 R:2.0000 rate:0.1538 aloss:3.5852 dloss:1.1349 aloss2:81.3682 exploreP:0.0100\n",
      "Episode:1215 meanR:0.1200 R:-3.0000 rate:-0.2308 aloss:4.1740 dloss:1.1507 aloss2:83.6358 exploreP:0.0100\n",
      "Episode:1216 meanR:0.1100 R:1.0000 rate:0.0769 aloss:4.1608 dloss:1.1495 aloss2:83.4640 exploreP:0.0100\n",
      "Episode:1217 meanR:0.1100 R:1.0000 rate:0.0769 aloss:3.3544 dloss:1.1343 aloss2:79.0392 exploreP:0.0100\n",
      "Episode:1218 meanR:0.1500 R:1.0000 rate:0.0769 aloss:4.7130 dloss:1.1617 aloss2:85.1325 exploreP:0.0100\n",
      "Episode:1219 meanR:0.1600 R:1.0000 rate:0.0769 aloss:3.7938 dloss:1.1577 aloss2:81.3788 exploreP:0.0100\n",
      "Episode:1220 meanR:0.1800 R:1.0000 rate:0.0769 aloss:4.5578 dloss:1.1342 aloss2:80.6185 exploreP:0.0100\n",
      "Episode:1221 meanR:0.1700 R:-1.0000 rate:-0.0769 aloss:4.3210 dloss:1.1454 aloss2:84.1390 exploreP:0.0100\n",
      "Episode:1222 meanR:0.1800 R:0.0000 rate:0.0000 aloss:4.2661 dloss:1.1549 aloss2:82.7296 exploreP:0.0100\n",
      "Episode:1223 meanR:0.1900 R:1.0000 rate:0.0769 aloss:3.8743 dloss:1.1378 aloss2:82.4549 exploreP:0.0100\n",
      "Episode:1224 meanR:0.1800 R:0.0000 rate:0.0000 aloss:4.4881 dloss:1.1528 aloss2:85.2239 exploreP:0.0100\n",
      "Episode:1225 meanR:0.1700 R:-1.0000 rate:-0.0769 aloss:4.2621 dloss:1.1551 aloss2:85.1174 exploreP:0.0100\n",
      "Episode:1226 meanR:0.1800 R:0.0000 rate:0.0000 aloss:4.9275 dloss:1.1602 aloss2:87.8644 exploreP:0.0100\n",
      "Episode:1227 meanR:0.1800 R:0.0000 rate:0.0000 aloss:3.6990 dloss:1.1222 aloss2:79.4950 exploreP:0.0100\n",
      "Episode:1228 meanR:0.1700 R:-1.0000 rate:-0.0769 aloss:4.4835 dloss:1.1341 aloss2:82.0597 exploreP:0.0100\n",
      "Episode:1229 meanR:0.1600 R:0.0000 rate:0.0000 aloss:4.1350 dloss:1.1379 aloss2:82.3792 exploreP:0.0100\n",
      "Episode:1230 meanR:0.1400 R:-2.0000 rate:-0.1538 aloss:4.0744 dloss:1.1207 aloss2:83.2501 exploreP:0.0100\n",
      "Episode:1231 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:4.6074 dloss:1.1436 aloss2:83.3139 exploreP:0.0100\n",
      "Episode:1232 meanR:0.1300 R:1.0000 rate:0.0769 aloss:4.4076 dloss:1.1404 aloss2:84.8108 exploreP:0.0100\n",
      "Episode:1233 meanR:0.1400 R:-1.0000 rate:-0.0769 aloss:3.7980 dloss:1.1423 aloss2:80.2624 exploreP:0.0100\n",
      "Episode:1234 meanR:0.1300 R:-1.0000 rate:-0.0769 aloss:4.9300 dloss:1.1532 aloss2:89.2122 exploreP:0.0100\n",
      "Episode:1235 meanR:0.1100 R:0.0000 rate:0.0000 aloss:3.6263 dloss:1.1450 aloss2:80.1509 exploreP:0.0100\n",
      "Episode:1236 meanR:0.1100 R:1.0000 rate:0.0769 aloss:4.5850 dloss:1.1575 aloss2:85.7400 exploreP:0.0100\n",
      "Episode:1237 meanR:0.1100 R:0.0000 rate:0.0000 aloss:4.7321 dloss:1.1466 aloss2:89.1713 exploreP:0.0100\n",
      "Episode:1238 meanR:0.1100 R:0.0000 rate:0.0000 aloss:4.1077 dloss:1.1593 aloss2:84.4388 exploreP:0.0100\n",
      "Episode:1239 meanR:0.1100 R:0.0000 rate:0.0000 aloss:4.7019 dloss:1.1568 aloss2:88.8493 exploreP:0.0100\n",
      "Episode:1240 meanR:0.1000 R:0.0000 rate:0.0000 aloss:3.8546 dloss:1.1454 aloss2:81.4427 exploreP:0.0100\n",
      "Episode:1241 meanR:0.1000 R:1.0000 rate:0.0769 aloss:4.0322 dloss:1.1487 aloss2:86.4759 exploreP:0.0100\n",
      "Episode:1242 meanR:0.1100 R:0.0000 rate:0.0000 aloss:4.1168 dloss:1.1554 aloss2:84.4657 exploreP:0.0100\n",
      "Episode:1243 meanR:0.1000 R:0.0000 rate:0.0000 aloss:4.1679 dloss:1.1408 aloss2:86.7986 exploreP:0.0100\n",
      "Episode:1244 meanR:0.0900 R:1.0000 rate:0.0769 aloss:3.7473 dloss:1.1390 aloss2:80.5367 exploreP:0.0100\n",
      "Episode:1245 meanR:0.0700 R:1.0000 rate:0.0769 aloss:4.5838 dloss:1.1503 aloss2:85.4079 exploreP:0.0100\n",
      "Episode:1246 meanR:0.0600 R:1.0000 rate:0.0769 aloss:4.3886 dloss:1.1371 aloss2:84.5874 exploreP:0.0100\n",
      "Episode:1247 meanR:0.0400 R:-3.0000 rate:-0.2308 aloss:3.9523 dloss:1.1345 aloss2:81.8647 exploreP:0.0100\n",
      "Episode:1248 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:4.3816 dloss:1.1545 aloss2:82.4274 exploreP:0.0100\n",
      "Episode:1249 meanR:0.0200 R:0.0000 rate:0.0000 aloss:5.4788 dloss:1.1686 aloss2:85.3444 exploreP:0.0100\n",
      "Episode:1250 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:5.2354 dloss:1.1375 aloss2:86.8849 exploreP:0.0100\n",
      "Episode:1251 meanR:0.0200 R:0.0000 rate:0.0000 aloss:4.9660 dloss:1.1511 aloss2:86.8847 exploreP:0.0100\n",
      "Episode:1252 meanR:0.0100 R:0.0000 rate:0.0000 aloss:4.7684 dloss:1.1558 aloss2:86.1328 exploreP:0.0100\n",
      "Episode:1253 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:3.3805 dloss:1.1321 aloss2:77.7842 exploreP:0.0100\n",
      "Episode:1254 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.1780 dloss:1.1469 aloss2:84.2312 exploreP:0.0100\n",
      "Episode:1255 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.9842 dloss:1.1412 aloss2:89.6672 exploreP:0.0100\n",
      "Episode:1256 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:4.7031 dloss:1.1537 aloss2:88.1121 exploreP:0.0100\n",
      "Episode:1257 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.6954 dloss:1.1342 aloss2:85.7898 exploreP:0.0100\n",
      "Episode:1258 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.9885 dloss:1.1463 aloss2:83.6748 exploreP:0.0100\n",
      "Episode:1259 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.0963 dloss:1.1529 aloss2:83.1823 exploreP:0.0100\n",
      "Episode:1260 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:3.5862 dloss:1.1378 aloss2:81.8271 exploreP:0.0100\n",
      "Episode:1261 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.5785 dloss:1.1612 aloss2:87.0617 exploreP:0.0100\n",
      "Episode:1262 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.0843 dloss:1.1464 aloss2:86.8514 exploreP:0.0100\n",
      "Episode:1263 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.2067 dloss:1.1423 aloss2:86.2257 exploreP:0.0100\n",
      "Episode:1264 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:4.2867 dloss:1.1465 aloss2:85.5349 exploreP:0.0100\n",
      "Episode:1265 meanR:-0.0300 R:-2.0000 rate:-0.1538 aloss:3.4138 dloss:1.1499 aloss2:82.9392 exploreP:0.0100\n",
      "Episode:1266 meanR:0.0000 R:2.0000 rate:0.1538 aloss:3.8841 dloss:1.1427 aloss2:83.0536 exploreP:0.0100\n",
      "Episode:1267 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.7803 dloss:1.1541 aloss2:84.5621 exploreP:0.0100\n",
      "Episode:1268 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.5419 dloss:1.1428 aloss2:88.7743 exploreP:0.0100\n",
      "Episode:1269 meanR:-0.0200 R:-1.0000 rate:-0.0769 aloss:4.5121 dloss:1.1615 aloss2:90.0847 exploreP:0.0100\n",
      "Episode:1270 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.0187 dloss:1.1518 aloss2:86.1578 exploreP:0.0100\n",
      "Episode:1271 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.4603 dloss:1.1560 aloss2:90.3159 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1272 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:3.7954 dloss:1.1560 aloss2:83.8494 exploreP:0.0100\n",
      "Episode:1273 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.8853 dloss:1.1461 aloss2:93.5108 exploreP:0.0100\n",
      "Episode:1274 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:4.0218 dloss:1.1464 aloss2:86.2232 exploreP:0.0100\n",
      "Episode:1275 meanR:0.0000 R:0.0000 rate:0.0000 aloss:4.7373 dloss:1.1581 aloss2:88.8350 exploreP:0.0100\n",
      "Episode:1276 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:4.6735 dloss:1.1659 aloss2:90.5585 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, dloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0 # each episode\n",
    "        aloss_batch, dloss_batch, aloss2_batch = [], [], []\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        num_step = 0 # each episode\n",
    "        rate = -1\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                #rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                #rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                #if np.count_nonzero(dones)==1 and len(dones) >= 1 and np.max(rates) > 0:\n",
    "                if len(dones) > 1:\n",
    "                    # print('np.count_nonzero(dones)==1 and len(dones) >= 1 and np.max(rates) > 0: ', \n",
    "                    #       np.count_nonzero(dones), len(dones), np.max(rates))\n",
    "                    break\n",
    "            #             if np.count_nonzero(dones)!=1 and len(dones) < 1 and np.max(rates) <= 0:\n",
    "            #                 print(np.count_nonzero(dones), len(dones), np.max(rates))\n",
    "            #                 break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                feed_dict = {model.states: states, \n",
    "                                            model.actions: actions,\n",
    "                                            model.next_states: next_states,\n",
    "                                            #model.rewards: rewards,\n",
    "                                            model.dones: dones,\n",
    "                                            model.rates: rates})\n",
    "            dloss, _ = sess.run([model.d_loss, model.d_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               #model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              #model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print('dones:', \n",
    "            #       len(dones), np.count_nonzero(dones), \n",
    "            #       len(dones1), np.count_nonzero(dones1), \n",
    "            #       len(dones2), np.count_nonzero(dones2))\n",
    "            aloss_batch.append(aloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss2_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
