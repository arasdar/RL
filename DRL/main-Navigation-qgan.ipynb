{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# for steps in range(1111111):\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         print(state.shape)\n",
    "#         break\n",
    "    \n",
    "# print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     #print(state)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# batch = []\n",
    "# while True: # infinite number of steps\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     #print(state, action, reward, done)\n",
    "#     batch.append([action, state, reward, done])\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    # rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('actor', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size,\n",
    "               states, actions, next_states, dones, rates):\n",
    "    actions_logits = actor(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ###############################################\n",
    "    next_states_logits = generator(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                   action_size=action_size, state_size=state_size)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    aloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, \n",
    "                                                                    labels=next_states_labels))\n",
    "    ####################################################\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, action_size=action_size)\n",
    "    #rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    # dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "    #                                                                labels=rates)) # 0-1\n",
    "    dQs = tf.nn.tanh(tf.reshape(dQs, shape=[-1]))\n",
    "    dloss = tf.reduce_mean(tf.square(dQs-rates)) # [-1, +1]\n",
    "    ####################################################\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states, action_size=action_size, \n",
    "                        reuse=True)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs))) # 0-1\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.ones_like(gQs))) # 0-1\n",
    "    #####################################################\n",
    "    next_actions_logits = actor(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    gQs2 = discriminator(actions=next_actions_logits, hidden_size=hidden_size, states=next_states, \n",
    "                         action_size=action_size, reuse=True)\n",
    "    gQs2 = tf.reshape(gQs2, shape=[-1]) * (1-dones)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs2))) # 0-1\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                     labels=tf.ones_like(gQs2))) # 0-1\n",
    "    return actions_logits, aloss, dloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, d_loss, a_loss2, a_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('actor')]\n",
    "    #g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, d_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.dones, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.d_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.d_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        d_loss=self.d_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 'continuous', 4, 'discrete')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_observation_space_type, \\\n",
    "brain.vector_action_space_size, brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "# gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              a_learning_rate=a_learning_rate, \n",
    "              d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        # state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-2.0000 R:-2.0000 rate:-0.1538 aloss:2.2151 dloss:1.4167 aloss2:1.5538 exploreP:0.9707\n",
      "Episode:1 meanR:-0.5000 R:1.0000 rate:0.0769 aloss:2.1404 dloss:1.2774 aloss2:1.8209 exploreP:0.9423\n",
      "Episode:2 meanR:-1.0000 R:-2.0000 rate:-0.1538 aloss:2.1091 dloss:1.2442 aloss2:1.8236 exploreP:0.9148\n",
      "Episode:3 meanR:-1.7500 R:-4.0000 rate:-0.3077 aloss:2.1278 dloss:1.2428 aloss2:1.8251 exploreP:0.8881\n",
      "Episode:4 meanR:-1.2000 R:1.0000 rate:0.0769 aloss:2.1899 dloss:1.2582 aloss2:1.8251 exploreP:0.8621\n",
      "Episode:5 meanR:-1.3333 R:-2.0000 rate:-0.1538 aloss:2.1781 dloss:1.2575 aloss2:1.8259 exploreP:0.8369\n",
      "Episode:6 meanR:-1.0000 R:1.0000 rate:0.0769 aloss:2.1630 dloss:1.2529 aloss2:1.8229 exploreP:0.8125\n",
      "Episode:7 meanR:-0.8750 R:0.0000 rate:0.0000 aloss:2.1727 dloss:1.2443 aloss2:1.8380 exploreP:0.7888\n",
      "Episode:8 meanR:-0.8889 R:-1.0000 rate:-0.0769 aloss:2.1818 dloss:1.2490 aloss2:1.8364 exploreP:0.7657\n",
      "Episode:9 meanR:-0.7000 R:1.0000 rate:0.0769 aloss:2.1780 dloss:1.2486 aloss2:1.8335 exploreP:0.7434\n",
      "Episode:10 meanR:-0.6364 R:0.0000 rate:0.0000 aloss:2.1766 dloss:1.2322 aloss2:1.8531 exploreP:0.7217\n",
      "Episode:11 meanR:-0.3333 R:3.0000 rate:0.2308 aloss:2.1762 dloss:1.2314 aloss2:1.8518 exploreP:0.7007\n",
      "Episode:12 meanR:-0.3077 R:0.0000 rate:0.0000 aloss:2.1992 dloss:1.2599 aloss2:1.8279 exploreP:0.6803\n",
      "Episode:13 meanR:-0.2143 R:1.0000 rate:0.0769 aloss:2.1655 dloss:1.2311 aloss2:1.8467 exploreP:0.6605\n",
      "Episode:14 meanR:-0.2000 R:0.0000 rate:0.0000 aloss:2.1812 dloss:1.2173 aloss2:1.8655 exploreP:0.6413\n",
      "Episode:15 meanR:-0.1250 R:1.0000 rate:0.0769 aloss:2.1965 dloss:1.2447 aloss2:1.8440 exploreP:0.6226\n",
      "Episode:16 meanR:-0.1176 R:0.0000 rate:0.0000 aloss:2.1787 dloss:1.2294 aloss2:1.8548 exploreP:0.6045\n",
      "Episode:17 meanR:-0.0556 R:1.0000 rate:0.0769 aloss:2.1855 dloss:1.2101 aloss2:1.8873 exploreP:0.5869\n",
      "Episode:18 meanR:-0.0526 R:0.0000 rate:0.0000 aloss:2.1740 dloss:1.2285 aloss2:1.8569 exploreP:0.5699\n",
      "Episode:19 meanR:0.0500 R:2.0000 rate:0.1538 aloss:2.1697 dloss:1.2014 aloss2:1.8865 exploreP:0.5533\n",
      "Episode:20 meanR:0.0476 R:0.0000 rate:0.0000 aloss:2.1790 dloss:1.2127 aloss2:1.8760 exploreP:0.5373\n",
      "Episode:21 meanR:0.0909 R:1.0000 rate:0.0769 aloss:2.1808 dloss:1.2005 aloss2:1.8978 exploreP:0.5217\n",
      "Episode:22 meanR:0.1739 R:2.0000 rate:0.1538 aloss:2.1763 dloss:1.1987 aloss2:1.8872 exploreP:0.5066\n",
      "Episode:23 meanR:0.2500 R:2.0000 rate:0.1538 aloss:2.1730 dloss:1.1746 aloss2:1.9283 exploreP:0.4919\n",
      "Episode:24 meanR:0.2800 R:1.0000 rate:0.0769 aloss:2.1871 dloss:1.1905 aloss2:1.9087 exploreP:0.4776\n",
      "Episode:25 meanR:0.3077 R:1.0000 rate:0.0769 aloss:2.1849 dloss:1.1721 aloss2:1.9354 exploreP:0.4638\n",
      "Episode:26 meanR:0.3333 R:1.0000 rate:0.0769 aloss:2.1755 dloss:1.1860 aloss2:1.9115 exploreP:0.4504\n",
      "Episode:27 meanR:0.3571 R:1.0000 rate:0.0769 aloss:2.1533 dloss:1.1312 aloss2:1.9737 exploreP:0.4374\n",
      "Episode:28 meanR:0.3103 R:-1.0000 rate:-0.0769 aloss:2.1888 dloss:1.1516 aloss2:1.9654 exploreP:0.4248\n",
      "Episode:29 meanR:0.3000 R:0.0000 rate:0.0000 aloss:2.1757 dloss:1.1556 aloss2:1.9534 exploreP:0.4125\n",
      "Episode:30 meanR:0.2581 R:-1.0000 rate:-0.0769 aloss:2.1668 dloss:1.1250 aloss2:1.9953 exploreP:0.4006\n",
      "Episode:31 meanR:0.2500 R:0.0000 rate:0.0000 aloss:2.1573 dloss:1.1065 aloss2:2.0016 exploreP:0.3891\n",
      "Episode:32 meanR:0.3030 R:2.0000 rate:0.1538 aloss:2.1704 dloss:1.1161 aloss2:1.9964 exploreP:0.3779\n",
      "Episode:33 meanR:0.3235 R:1.0000 rate:0.0769 aloss:2.1836 dloss:1.1187 aloss2:1.9981 exploreP:0.3670\n",
      "Episode:34 meanR:0.3143 R:0.0000 rate:0.0000 aloss:2.1724 dloss:1.0940 aloss2:2.0324 exploreP:0.3564\n",
      "Episode:35 meanR:0.3056 R:0.0000 rate:0.0000 aloss:2.1965 dloss:1.1331 aloss2:1.9864 exploreP:0.3462\n",
      "Episode:36 meanR:0.3243 R:1.0000 rate:0.0769 aloss:2.1931 dloss:1.0924 aloss2:2.0393 exploreP:0.3363\n",
      "Episode:37 meanR:0.2368 R:-3.0000 rate:-0.2308 aloss:2.1833 dloss:1.0802 aloss2:2.1137 exploreP:0.3266\n",
      "Episode:38 meanR:0.2564 R:1.0000 rate:0.0769 aloss:2.1829 dloss:1.1068 aloss2:2.0417 exploreP:0.3173\n",
      "Episode:39 meanR:0.2250 R:-1.0000 rate:-0.0769 aloss:2.1730 dloss:1.0818 aloss2:2.0882 exploreP:0.3082\n",
      "Episode:40 meanR:0.3171 R:4.0000 rate:0.3077 aloss:2.1720 dloss:1.0695 aloss2:2.1231 exploreP:0.2994\n",
      "Episode:41 meanR:0.3333 R:1.0000 rate:0.0769 aloss:2.1626 dloss:1.0555 aloss2:2.1696 exploreP:0.2908\n",
      "Episode:42 meanR:0.3256 R:0.0000 rate:0.0000 aloss:2.1602 dloss:1.0504 aloss2:2.2014 exploreP:0.2825\n",
      "Episode:43 meanR:0.3182 R:0.0000 rate:0.0000 aloss:2.1622 dloss:1.0246 aloss2:2.2408 exploreP:0.2745\n",
      "Episode:44 meanR:0.2889 R:-1.0000 rate:-0.0769 aloss:2.1603 dloss:1.0297 aloss2:2.2523 exploreP:0.2666\n",
      "Episode:45 meanR:0.2826 R:0.0000 rate:0.0000 aloss:2.1731 dloss:1.0422 aloss2:2.2252 exploreP:0.2591\n",
      "Episode:46 meanR:0.1915 R:-4.0000 rate:-0.3077 aloss:2.1758 dloss:1.0123 aloss2:2.3163 exploreP:0.2517\n",
      "Episode:47 meanR:0.1875 R:0.0000 rate:0.0000 aloss:2.1739 dloss:0.9712 aloss2:2.4350 exploreP:0.2446\n",
      "Episode:48 meanR:0.1837 R:0.0000 rate:0.0000 aloss:2.1557 dloss:0.9499 aloss2:2.4844 exploreP:0.2376\n",
      "Episode:49 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.1632 dloss:0.8902 aloss2:2.6536 exploreP:0.2309\n",
      "Episode:50 meanR:0.2157 R:2.0000 rate:0.1538 aloss:2.1911 dloss:1.0119 aloss2:2.6388 exploreP:0.2244\n",
      "Episode:51 meanR:0.2115 R:0.0000 rate:0.0000 aloss:2.1836 dloss:0.9998 aloss2:2.5467 exploreP:0.2180\n",
      "Episode:52 meanR:0.2264 R:1.0000 rate:0.0769 aloss:2.1851 dloss:0.8983 aloss2:2.7539 exploreP:0.2119\n",
      "Episode:53 meanR:0.2037 R:-1.0000 rate:-0.0769 aloss:2.1708 dloss:0.9039 aloss2:2.8990 exploreP:0.2059\n",
      "Episode:54 meanR:0.2000 R:0.0000 rate:0.0000 aloss:2.1694 dloss:0.9766 aloss2:2.4016 exploreP:0.2001\n",
      "Episode:55 meanR:0.2143 R:1.0000 rate:0.0769 aloss:2.1705 dloss:0.9014 aloss2:2.5282 exploreP:0.1945\n",
      "Episode:56 meanR:0.2456 R:2.0000 rate:0.1538 aloss:2.1760 dloss:0.8751 aloss2:2.8925 exploreP:0.1891\n",
      "Episode:57 meanR:0.2586 R:1.0000 rate:0.0769 aloss:2.1860 dloss:0.8810 aloss2:2.8091 exploreP:0.1838\n",
      "Episode:58 meanR:0.2542 R:0.0000 rate:0.0000 aloss:2.1553 dloss:0.8317 aloss2:2.8733 exploreP:0.1786\n",
      "Episode:59 meanR:0.2500 R:0.0000 rate:0.0000 aloss:2.1676 dloss:0.7931 aloss2:3.0928 exploreP:0.1736\n",
      "Episode:60 meanR:0.2295 R:-1.0000 rate:-0.0769 aloss:2.1666 dloss:0.7640 aloss2:3.2874 exploreP:0.1688\n",
      "Episode:61 meanR:0.2419 R:1.0000 rate:0.0769 aloss:2.1564 dloss:0.7607 aloss2:3.6118 exploreP:0.1641\n",
      "Episode:62 meanR:0.2540 R:1.0000 rate:0.0769 aloss:2.1737 dloss:0.9537 aloss2:2.7745 exploreP:0.1596\n",
      "Episode:63 meanR:0.2656 R:1.0000 rate:0.0769 aloss:2.1585 dloss:0.8994 aloss2:2.8450 exploreP:0.1551\n",
      "Episode:64 meanR:0.2769 R:1.0000 rate:0.0769 aloss:2.1580 dloss:0.8420 aloss2:3.1327 exploreP:0.1509\n",
      "Episode:65 meanR:0.2727 R:0.0000 rate:0.0000 aloss:2.1424 dloss:0.8040 aloss2:3.5006 exploreP:0.1467\n",
      "Episode:66 meanR:0.2687 R:0.0000 rate:0.0000 aloss:2.1522 dloss:0.7348 aloss2:3.8173 exploreP:0.1426\n",
      "Episode:67 meanR:0.2500 R:-1.0000 rate:-0.0769 aloss:2.1859 dloss:1.0190 aloss2:3.4013 exploreP:0.1387\n",
      "Episode:68 meanR:0.2464 R:0.0000 rate:0.0000 aloss:2.2037 dloss:1.2000 aloss2:4.0010 exploreP:0.1349\n",
      "Episode:69 meanR:0.2286 R:-1.0000 rate:-0.0769 aloss:2.2052 dloss:1.2890 aloss2:6.5935 exploreP:0.1312\n",
      "Episode:70 meanR:0.2254 R:0.0000 rate:0.0000 aloss:2.1686 dloss:1.1988 aloss2:9.8836 exploreP:0.1276\n",
      "Episode:71 meanR:0.2222 R:0.0000 rate:0.0000 aloss:2.1724 dloss:1.1993 aloss2:11.6336 exploreP:0.1242\n",
      "Episode:72 meanR:0.2192 R:0.0000 rate:0.0000 aloss:2.1583 dloss:1.2027 aloss2:12.7460 exploreP:0.1208\n",
      "Episode:73 meanR:0.2162 R:0.0000 rate:0.0000 aloss:2.1373 dloss:1.1984 aloss2:13.5875 exploreP:0.1175\n",
      "Episode:74 meanR:0.2133 R:0.0000 rate:0.0000 aloss:2.1294 dloss:1.1924 aloss2:14.2975 exploreP:0.1143\n",
      "Episode:75 meanR:0.2105 R:0.0000 rate:0.0000 aloss:2.1204 dloss:1.2066 aloss2:14.8642 exploreP:0.1113\n",
      "Episode:76 meanR:0.2078 R:0.0000 rate:0.0000 aloss:2.1059 dloss:1.1897 aloss2:15.3715 exploreP:0.1083\n",
      "Episode:77 meanR:0.1923 R:-1.0000 rate:-0.0769 aloss:2.1202 dloss:1.1887 aloss2:15.8731 exploreP:0.1054\n",
      "Episode:78 meanR:0.1899 R:0.0000 rate:0.0000 aloss:2.1143 dloss:1.1955 aloss2:16.2797 exploreP:0.1025\n",
      "Episode:79 meanR:0.1875 R:0.0000 rate:0.0000 aloss:2.1040 dloss:1.1971 aloss2:16.7411 exploreP:0.0998\n",
      "Episode:80 meanR:0.1852 R:0.0000 rate:0.0000 aloss:2.1011 dloss:1.1888 aloss2:17.1902 exploreP:0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:81 meanR:0.1829 R:0.0000 rate:0.0000 aloss:2.0954 dloss:1.1966 aloss2:17.5732 exploreP:0.0946\n",
      "Episode:82 meanR:0.1687 R:-1.0000 rate:-0.0769 aloss:2.1020 dloss:1.2042 aloss2:18.0006 exploreP:0.0921\n",
      "Episode:83 meanR:0.1667 R:0.0000 rate:0.0000 aloss:2.0957 dloss:1.1912 aloss2:18.3607 exploreP:0.0897\n",
      "Episode:84 meanR:0.1529 R:-1.0000 rate:-0.0769 aloss:2.1035 dloss:1.1962 aloss2:18.6704 exploreP:0.0873\n",
      "Episode:85 meanR:0.1512 R:0.0000 rate:0.0000 aloss:2.0986 dloss:1.1820 aloss2:19.1553 exploreP:0.0850\n",
      "Episode:86 meanR:0.1494 R:0.0000 rate:0.0000 aloss:2.0924 dloss:1.1966 aloss2:19.4838 exploreP:0.0828\n",
      "Episode:87 meanR:0.1477 R:0.0000 rate:0.0000 aloss:2.0838 dloss:1.1781 aloss2:19.8512 exploreP:0.0806\n",
      "Episode:88 meanR:0.1348 R:-1.0000 rate:-0.0769 aloss:2.0863 dloss:1.1897 aloss2:20.2636 exploreP:0.0786\n",
      "Episode:89 meanR:0.1333 R:0.0000 rate:0.0000 aloss:2.0860 dloss:1.2070 aloss2:20.6202 exploreP:0.0765\n",
      "Episode:90 meanR:0.1319 R:0.0000 rate:0.0000 aloss:2.0845 dloss:1.2004 aloss2:21.0000 exploreP:0.0746\n",
      "Episode:91 meanR:0.1304 R:0.0000 rate:0.0000 aloss:2.0826 dloss:1.1719 aloss2:21.4043 exploreP:0.0727\n",
      "Episode:92 meanR:0.1290 R:0.0000 rate:0.0000 aloss:2.0814 dloss:1.1894 aloss2:21.7674 exploreP:0.0708\n",
      "Episode:93 meanR:0.1170 R:-1.0000 rate:-0.0769 aloss:2.0819 dloss:1.1946 aloss2:22.1761 exploreP:0.0690\n",
      "Episode:94 meanR:0.1158 R:0.0000 rate:0.0000 aloss:2.0839 dloss:1.1918 aloss2:22.5571 exploreP:0.0673\n",
      "Episode:95 meanR:0.1146 R:0.0000 rate:0.0000 aloss:2.0823 dloss:1.1996 aloss2:22.9837 exploreP:0.0656\n",
      "Episode:96 meanR:0.1237 R:1.0000 rate:0.0769 aloss:2.0831 dloss:1.1910 aloss2:23.3156 exploreP:0.0639\n",
      "Episode:97 meanR:0.1224 R:0.0000 rate:0.0000 aloss:2.0807 dloss:1.1625 aloss2:23.6813 exploreP:0.0623\n",
      "Episode:98 meanR:0.1111 R:-1.0000 rate:-0.0769 aloss:2.0824 dloss:1.1948 aloss2:24.0858 exploreP:0.0608\n",
      "Episode:99 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0758 dloss:1.1886 aloss2:24.4768 exploreP:0.0593\n",
      "Episode:100 meanR:0.1300 R:0.0000 rate:0.0000 aloss:2.0762 dloss:1.1857 aloss2:24.7500 exploreP:0.0578\n",
      "Episode:101 meanR:0.1200 R:0.0000 rate:0.0000 aloss:2.0774 dloss:1.1765 aloss2:25.2485 exploreP:0.0564\n",
      "Episode:102 meanR:0.1400 R:0.0000 rate:0.0000 aloss:2.0758 dloss:1.1807 aloss2:25.5927 exploreP:0.0550\n",
      "Episode:103 meanR:0.1900 R:1.0000 rate:0.0769 aloss:2.0762 dloss:1.1664 aloss2:25.9568 exploreP:0.0537\n",
      "Episode:104 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.0778 dloss:1.1691 aloss2:26.3949 exploreP:0.0524\n",
      "Episode:105 meanR:0.2000 R:0.0000 rate:0.0000 aloss:2.0754 dloss:1.1939 aloss2:26.7034 exploreP:0.0512\n",
      "Episode:106 meanR:0.1800 R:-1.0000 rate:-0.0769 aloss:2.0761 dloss:1.1846 aloss2:27.1830 exploreP:0.0500\n",
      "Episode:107 meanR:0.1800 R:0.0000 rate:0.0000 aloss:2.0729 dloss:1.1775 aloss2:27.4169 exploreP:0.0488\n",
      "Episode:108 meanR:0.2000 R:1.0000 rate:0.0769 aloss:2.0756 dloss:1.1957 aloss2:27.8680 exploreP:0.0476\n",
      "Episode:109 meanR:0.1900 R:0.0000 rate:0.0000 aloss:2.0750 dloss:1.1702 aloss2:28.1798 exploreP:0.0465\n",
      "Episode:110 meanR:0.2000 R:1.0000 rate:0.0769 aloss:2.0770 dloss:1.1925 aloss2:28.6150 exploreP:0.0454\n",
      "Episode:111 meanR:0.1700 R:0.0000 rate:0.0000 aloss:2.0698 dloss:1.1745 aloss2:28.9042 exploreP:0.0444\n",
      "Episode:112 meanR:0.1700 R:0.0000 rate:0.0000 aloss:2.0774 dloss:1.1805 aloss2:29.4199 exploreP:0.0434\n",
      "Episode:113 meanR:0.1600 R:0.0000 rate:0.0000 aloss:2.0773 dloss:1.1908 aloss2:29.6680 exploreP:0.0424\n",
      "Episode:114 meanR:0.1600 R:0.0000 rate:0.0000 aloss:2.0748 dloss:1.1958 aloss2:29.9976 exploreP:0.0414\n",
      "Episode:115 meanR:0.1400 R:-1.0000 rate:-0.0769 aloss:2.0732 dloss:1.1921 aloss2:30.4602 exploreP:0.0405\n",
      "Episode:116 meanR:0.1400 R:0.0000 rate:0.0000 aloss:2.0756 dloss:1.1994 aloss2:30.7933 exploreP:0.0396\n",
      "Episode:117 meanR:0.1300 R:0.0000 rate:0.0000 aloss:2.0772 dloss:1.1727 aloss2:31.2312 exploreP:0.0387\n",
      "Episode:118 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:2.0725 dloss:1.1847 aloss2:31.4956 exploreP:0.0379\n",
      "Episode:119 meanR:0.1100 R:1.0000 rate:0.0769 aloss:2.0780 dloss:1.1798 aloss2:31.7857 exploreP:0.0371\n",
      "Episode:120 meanR:0.1100 R:0.0000 rate:0.0000 aloss:2.0768 dloss:1.1800 aloss2:32.2218 exploreP:0.0363\n",
      "Episode:121 meanR:0.0900 R:-1.0000 rate:-0.0769 aloss:2.0779 dloss:1.1835 aloss2:32.6222 exploreP:0.0355\n",
      "Episode:122 meanR:0.0700 R:0.0000 rate:0.0000 aloss:2.0797 dloss:1.1839 aloss2:32.9700 exploreP:0.0347\n",
      "Episode:123 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:2.0779 dloss:1.1997 aloss2:33.2825 exploreP:0.0340\n",
      "Episode:124 meanR:0.0300 R:0.0000 rate:0.0000 aloss:2.0775 dloss:1.1808 aloss2:33.6809 exploreP:0.0333\n",
      "Episode:125 meanR:0.0200 R:0.0000 rate:0.0000 aloss:2.0779 dloss:1.1682 aloss2:34.0141 exploreP:0.0326\n",
      "Episode:126 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0790 dloss:1.1966 aloss2:34.4543 exploreP:0.0319\n",
      "Episode:127 meanR:0.0000 R:0.0000 rate:0.0000 aloss:2.0792 dloss:1.1838 aloss2:34.7979 exploreP:0.0313\n",
      "Episode:128 meanR:0.0100 R:0.0000 rate:0.0000 aloss:2.0803 dloss:1.1708 aloss2:35.1367 exploreP:0.0306\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, dloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0 # each episode\n",
    "        aloss_batch, dloss_batch, aloss2_batch = [], [], []\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        num_step = 0 # each episode\n",
    "        rate = -1\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                #rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                #rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                #if np.count_nonzero(dones)==1 and len(dones) >= 1 and np.max(rates) > 0:\n",
    "                if len(dones) > 1:\n",
    "                    # print('np.count_nonzero(dones)==1 and len(dones) >= 1 and np.max(rates) > 0: ', \n",
    "                    #       np.count_nonzero(dones), len(dones), np.max(rates))\n",
    "                    break\n",
    "            #             if np.count_nonzero(dones)!=1 and len(dones) < 1 and np.max(rates) <= 0:\n",
    "            #                 print(np.count_nonzero(dones), len(dones), np.max(rates))\n",
    "            #                 break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                feed_dict = {model.states: states, \n",
    "                                            model.actions: actions,\n",
    "                                            model.next_states: next_states,\n",
    "                                            #model.rewards: rewards,\n",
    "                                            model.dones: dones,\n",
    "                                            model.rates: rates})\n",
    "            dloss, _ = sess.run([model.d_loss, model.d_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               #model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              #model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print('dones:', \n",
    "            #       len(dones), np.count_nonzero(dones), \n",
    "            #       len(dones1), np.count_nonzero(dones1), \n",
    "            #       len(dones2), np.count_nonzero(dones2))\n",
    "            aloss_batch.append(aloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss2_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
