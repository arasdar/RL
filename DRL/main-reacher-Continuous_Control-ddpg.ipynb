{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/aras/unity-envs/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the train mode\n",
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "# state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# #scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# num_steps = 0\n",
    "# while True:\n",
    "#     num_steps += 1\n",
    "#     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     #print(action)\n",
    "#     action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "#     #print(action)\n",
    "#     env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "#     next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "#     reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "#     done = env_info.local_done[0]                        # see if episode finished\n",
    "#     #scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     state = next_state                               # roll over states to next time step\n",
    "#     if done is True:                                  # exit loop if episode finished\n",
    "#         #print(action.shape, reward)\n",
    "#         #print(done)\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "# num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_shape], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    training = tf.placeholder(tf.bool, [], name='training')\n",
    "    return states, actions, targetQs, rates, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size) # [-inf, +inf]        \n",
    "        predictions = tf.nn.tanh(logits) # [-1, +1]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates, training):\n",
    "    actions_preds = generator(states=states, hidden_size=hidden_size, action_size=action_size, training=training)\n",
    "    gQs = discriminator(actions=actions_preds, hidden_size=hidden_size, states=states, training=training)\n",
    "    gloss = -tf.reduce_mean(gQs)\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, training=training, reuse=True)\n",
    "    rates = tf.reshape(rates, shape=[-1, 1]) # [0, 1]\n",
    "    dloss = tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # [-inf, +inf] to [0, 1]\n",
    "                                                    labels=rates) # [0, 1]\n",
    "                                                    #labels=(rates+1)/2) # [-1, +1] to [0, 1]\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    #dloss = tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dloss += tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # [-inf, +inf] to [0, 1]\n",
    "                                                     labels=tf.nn.sigmoid(targetQs)) # [0, 1]\n",
    "    return actions_preds, gQs, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates, self.training = model_input(\n",
    "            state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_preds, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            rates=self.rates, training=self.training) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss,\n",
    "                                           g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(buffer, batch_size):\n",
    "    idx = np.random.choice(np.arange(len(buffer)), \n",
    "                           size=batch_size, \n",
    "                           replace=False)\n",
    "    return [buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 33), (1, 4), 4, 0, 4, 33)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info.vector_observations.shape, env_info.previous_vector_actions.shape, \\\n",
    "brain.vector_action_space_size, brain.number_visual_observations, \\\n",
    "brain.vector_action_space_size, brain.vector_observation_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.01\n",
      "Progress: 0.02001\n",
      "Progress: 0.03002\n",
      "Progress: 0.04003\n",
      "Progress: 0.05004\n",
      "Progress: 0.06005\n",
      "Progress: 0.07006\n",
      "Progress: 0.08007\n",
      "Progress: 0.09008\n",
      "Progress: 0.10009\n",
      "Progress: 0.1101\n",
      "Progress: 0.12011\n",
      "Progress: 0.13012\n",
      "Progress: 0.14013\n",
      "Progress: 0.15014\n",
      "Progress: 0.16015\n",
      "Progress: 0.17016\n",
      "Progress: 0.18017\n",
      "Progress: 0.19018\n",
      "Progress: 0.20019\n",
      "Progress: 0.2102\n",
      "Progress: 0.22021\n",
      "Progress: 0.23022\n",
      "Progress: 0.24023\n",
      "Progress: 0.25024\n",
      "Progress: 0.26025\n",
      "Progress: 0.27026\n",
      "Progress: 0.28027\n",
      "Progress: 0.29028\n",
      "Progress: 0.30029\n",
      "Progress: 0.3103\n",
      "Progress: 0.32031\n",
      "Progress: 0.33032\n",
      "Progress: 0.34033\n",
      "Progress: 0.35034\n",
      "Progress: 0.36035\n",
      "Progress: 0.37036\n",
      "Progress: 0.38037\n",
      "Progress: 0.39038\n",
      "Progress: 0.40039\n",
      "Progress: 0.4104\n",
      "Progress: 0.42041\n",
      "Progress: 0.43042\n",
      "Progress: 0.44043\n",
      "Progress: 0.45044\n",
      "Progress: 0.46045\n",
      "Progress: 0.47046\n",
      "Progress: 0.48047\n",
      "Progress: 0.49048\n",
      "Progress: 0.50049\n",
      "Progress: 0.5105\n",
      "Progress: 0.52051\n",
      "Progress: 0.53052\n",
      "Progress: 0.54053\n",
      "Progress: 0.55054\n",
      "Progress: 0.56055\n",
      "Progress: 0.57056\n",
      "Progress: 0.58057\n",
      "Progress: 0.59058\n",
      "Progress: 0.60059\n",
      "Progress: 0.6106\n",
      "Progress: 0.62061\n",
      "Progress: 0.63062\n",
      "Progress: 0.64063\n",
      "Progress: 0.65064\n",
      "Progress: 0.66065\n",
      "Progress: 0.67066\n",
      "Progress: 0.68067\n",
      "Progress: 0.69068\n",
      "Progress: 0.70069\n",
      "Progress: 0.7107\n",
      "Progress: 0.72071\n",
      "Progress: 0.73072\n",
      "Progress: 0.74073\n",
      "Progress: 0.75074\n",
      "Progress: 0.76075\n",
      "Progress: 0.77076\n",
      "Progress: 0.78077\n",
      "Progress: 0.79078\n",
      "Progress: 0.80079\n",
      "Progress: 0.8108\n",
      "Progress: 0.82081\n",
      "Progress: 0.83082\n",
      "Progress: 0.84083\n",
      "Progress: 0.85084\n",
      "Progress: 0.86085\n",
      "Progress: 0.87086\n",
      "Progress: 0.88087\n",
      "Progress: 0.89088\n",
      "Progress: 0.90089\n",
      "Progress: 0.9109\n",
      "Progress: 0.92091\n",
      "Progress: 0.93092\n",
      "Progress: 0.94093\n",
      "Progress: 0.95094\n",
      "Progress: 0.96095\n",
      "Progress: 0.97096\n",
      "Progress: 0.98097\n",
      "Progress: 0.99098\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each_step in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    action = np.clip(action, -1, 1) # [-1, +1]\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    rate = -1 # success rate: [-1, +1]\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory updated\n",
    "    total_reward += reward # max reward 30\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        print('Progress:', each_step/memory_size)\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        rate = total_reward/30\n",
    "        rate = np.clip(rate, -1, 1) # [-1, +1]\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:2.6900 R:2.6900 rate:0.0897 gloss:2.2616 dloss:0.5702\n",
      "Episode:1 meanR:1.6600 R:0.6300 rate:0.0210 gloss:3.9771 dloss:0.2150\n",
      "Episode:2 meanR:1.2200 R:0.3400 rate:0.0113 gloss:3.9731 dloss:0.1952\n",
      "Episode:3 meanR:1.2450 R:1.3200 rate:0.0440 gloss:3.9787 dloss:0.1857\n",
      "Episode:4 meanR:1.2260 R:1.1500 rate:0.0383 gloss:3.9258 dloss:0.1872\n",
      "Episode:5 meanR:1.1717 R:0.9000 rate:0.0300 gloss:3.8400 dloss:0.1927\n",
      "Episode:6 meanR:1.0043 R:0.0000 rate:0.0000 gloss:3.7764 dloss:0.1964\n",
      "Episode:7 meanR:1.0662 R:1.5000 rate:0.0500 gloss:3.7653 dloss:0.1959\n",
      "Episode:8 meanR:0.9833 R:0.3200 rate:0.0107 gloss:3.7158 dloss:0.2028\n",
      "Episode:9 meanR:0.9310 R:0.4600 rate:0.0153 gloss:3.7487 dloss:0.1993\n",
      "Episode:10 meanR:0.9609 R:1.2600 rate:0.0420 gloss:3.7589 dloss:0.1985\n",
      "Episode:11 meanR:0.9217 R:0.4900 rate:0.0163 gloss:3.7416 dloss:0.2019\n",
      "Episode:12 meanR:0.9238 R:0.9500 rate:0.0317 gloss:3.7512 dloss:0.2013\n",
      "Episode:13 meanR:0.9043 R:0.6500 rate:0.0217 gloss:3.7172 dloss:0.2054\n",
      "Episode:14 meanR:0.8827 R:0.5800 rate:0.0193 gloss:3.7277 dloss:0.2050\n",
      "Episode:15 meanR:0.9050 R:1.2400 rate:0.0413 gloss:3.7423 dloss:0.2034\n",
      "Episode:16 meanR:0.8806 R:0.4900 rate:0.0163 gloss:3.6980 dloss:0.2093\n",
      "Episode:17 meanR:0.8972 R:1.1800 rate:0.0393 gloss:3.7084 dloss:0.2086\n",
      "Episode:18 meanR:0.8779 R:0.5300 rate:0.0177 gloss:3.6687 dloss:0.2140\n",
      "Episode:19 meanR:0.8855 R:1.0300 rate:0.0343 gloss:3.6796 dloss:0.2130\n",
      "Episode:20 meanR:0.8857 R:0.8900 rate:0.0297 gloss:3.6770 dloss:0.2134\n",
      "Episode:21 meanR:0.8768 R:0.6900 rate:0.0230 gloss:3.6626 dloss:0.2156\n",
      "Episode:22 meanR:0.8791 R:0.9300 rate:0.0310 gloss:3.6604 dloss:0.2164\n",
      "Episode:23 meanR:0.8558 R:0.3200 rate:0.0107 gloss:3.6644 dloss:0.2167\n",
      "Episode:24 meanR:0.8416 R:0.5000 rate:0.0167 gloss:3.6833 dloss:0.2142\n",
      "Episode:25 meanR:0.8262 R:0.4400 rate:0.0147 gloss:3.6792 dloss:0.2146\n",
      "Episode:26 meanR:0.8530 R:1.5500 rate:0.0517 gloss:3.6872 dloss:0.2140\n",
      "Episode:27 meanR:0.8379 R:0.4300 rate:0.0143 gloss:3.6602 dloss:0.2192\n",
      "Episode:28 meanR:0.8328 R:0.6900 rate:0.0230 gloss:3.6544 dloss:0.2208\n",
      "Episode:29 meanR:0.8220 R:0.5100 rate:0.0170 gloss:3.6627 dloss:0.2200\n",
      "Episode:30 meanR:0.8148 R:0.6000 rate:0.0200 gloss:3.6756 dloss:0.2182\n",
      "Episode:31 meanR:0.8191 R:0.9500 rate:0.0317 gloss:3.6681 dloss:0.2196\n",
      "Episode:32 meanR:0.8036 R:0.3100 rate:0.0103 gloss:3.6543 dloss:0.2213\n",
      "Episode:33 meanR:0.8415 R:2.0900 rate:0.0697 gloss:3.6658 dloss:0.2201\n",
      "Episode:34 meanR:0.8491 R:1.1100 rate:0.0370 gloss:3.6252 dloss:0.2273\n",
      "Episode:35 meanR:0.8506 R:0.9000 rate:0.0300 gloss:3.6093 dloss:0.2305\n",
      "Episode:36 meanR:0.8851 R:2.1300 rate:0.0710 gloss:3.5889 dloss:0.2335\n",
      "Episode:37 meanR:0.8668 R:0.1900 rate:0.0063 gloss:3.5334 dloss:0.2423\n",
      "Episode:38 meanR:0.8700 R:0.9900 rate:0.0330 gloss:3.5434 dloss:0.2412\n",
      "Episode:39 meanR:0.8892 R:1.6400 rate:0.0547 gloss:3.5353 dloss:0.2424\n",
      "Episode:40 meanR:0.9015 R:1.3900 rate:0.0463 gloss:3.5101 dloss:0.2474\n",
      "Episode:41 meanR:0.8981 R:0.7600 rate:0.0253 gloss:3.4853 dloss:0.2522\n",
      "Episode:42 meanR:0.8893 R:0.5200 rate:0.0173 gloss:3.4747 dloss:0.2542\n",
      "Episode:43 meanR:0.9250 R:2.4600 rate:0.0820 gloss:3.4877 dloss:0.2521\n",
      "Episode:44 meanR:0.9240 R:0.8800 rate:0.0293 gloss:3.4372 dloss:0.2609\n",
      "Episode:45 meanR:0.9176 R:0.6300 rate:0.0210 gloss:3.4349 dloss:0.2613\n",
      "Episode:46 meanR:0.9417 R:2.0500 rate:0.0683 gloss:3.4423 dloss:0.2598\n",
      "Episode:47 meanR:0.9381 R:0.7700 rate:0.0257 gloss:3.3951 dloss:0.2671\n",
      "Episode:48 meanR:0.9367 R:0.8700 rate:0.0290 gloss:3.3924 dloss:0.2681\n",
      "Episode:49 meanR:0.9422 R:1.2100 rate:0.0403 gloss:3.3859 dloss:0.2686\n",
      "Episode:50 meanR:0.9347 R:0.5600 rate:0.0187 gloss:3.3759 dloss:0.2710\n",
      "Episode:51 meanR:0.9350 R:0.9500 rate:0.0317 gloss:3.3829 dloss:0.2697\n",
      "Episode:52 meanR:0.9426 R:1.3400 rate:0.0447 gloss:3.3822 dloss:0.2704\n",
      "Episode:53 meanR:0.9380 R:0.6900 rate:0.0230 gloss:3.3655 dloss:0.2736\n",
      "Episode:54 meanR:0.9316 R:0.5900 rate:0.0197 gloss:3.3644 dloss:0.2742\n",
      "Episode:55 meanR:0.9320 R:0.9500 rate:0.0317 gloss:3.3573 dloss:0.2758\n",
      "Episode:56 meanR:0.9263 R:0.6100 rate:0.0203 gloss:3.3574 dloss:0.2759\n",
      "Episode:57 meanR:0.9343 R:1.3900 rate:0.0463 gloss:3.3678 dloss:0.2746\n",
      "Episode:58 meanR:0.9414 R:1.3500 rate:0.0450 gloss:3.3581 dloss:0.2769\n",
      "Episode:59 meanR:0.9257 R:0.0000 rate:0.0000 gloss:3.3427 dloss:0.2800\n",
      "Episode:60 meanR:0.9157 R:0.3200 rate:0.0107 gloss:3.3407 dloss:0.2803\n",
      "Episode:61 meanR:0.9153 R:0.8900 rate:0.0297 gloss:3.3551 dloss:0.2786\n",
      "Episode:62 meanR:0.9170 R:1.0200 rate:0.0340 gloss:3.3498 dloss:0.2797\n",
      "Episode:63 meanR:0.9205 R:1.1400 rate:0.0380 gloss:3.3421 dloss:0.2815\n",
      "Episode:64 meanR:0.9269 R:1.3400 rate:0.0447 gloss:3.3357 dloss:0.2824\n",
      "Episode:65 meanR:0.9280 R:1.0000 rate:0.0333 gloss:3.3244 dloss:0.2843\n",
      "Episode:66 meanR:0.9142 R:0.0000 rate:0.0000 gloss:3.3174 dloss:0.2863\n",
      "Episode:67 meanR:0.9100 R:0.6300 rate:0.0210 gloss:3.3148 dloss:0.2868\n",
      "Episode:68 meanR:0.9151 R:1.2600 rate:0.0420 gloss:3.3238 dloss:0.2853\n",
      "Episode:69 meanR:0.9053 R:0.2300 rate:0.0077 gloss:3.3211 dloss:0.2860\n",
      "Episode:70 meanR:0.9101 R:1.2500 rate:0.0417 gloss:3.3358 dloss:0.2839\n",
      "Episode:71 meanR:0.9026 R:0.3700 rate:0.0123 gloss:3.3265 dloss:0.2858\n",
      "Episode:72 meanR:0.9104 R:1.4700 rate:0.0490 gloss:3.3399 dloss:0.2839\n",
      "Episode:73 meanR:0.9004 R:0.1700 rate:0.0057 gloss:3.3263 dloss:0.2873\n",
      "Episode:74 meanR:0.9164 R:2.1000 rate:0.0700 gloss:3.3379 dloss:0.2851\n",
      "Episode:75 meanR:0.9143 R:0.7600 rate:0.0253 gloss:3.3226 dloss:0.2888\n",
      "Episode:76 meanR:0.9105 R:0.6200 rate:0.0207 gloss:3.3102 dloss:0.2902\n",
      "Episode:77 meanR:0.9197 R:1.6300 rate:0.0543 gloss:3.2536 dloss:0.2996\n",
      "Episode:78 meanR:0.9119 R:0.3000 rate:0.0100 gloss:3.2490 dloss:0.3015\n",
      "Episode:79 meanR:0.9129 R:0.9900 rate:0.0330 gloss:3.2795 dloss:0.2970\n",
      "Episode:80 meanR:0.9214 R:1.6000 rate:0.0533 gloss:3.2972 dloss:0.2943\n",
      "Episode:81 meanR:0.9204 R:0.8400 rate:0.0280 gloss:3.2967 dloss:0.2945\n",
      "Episode:82 meanR:0.9154 R:0.5100 rate:0.0170 gloss:3.2848 dloss:0.2967\n",
      "Episode:83 meanR:0.9121 R:0.6400 rate:0.0213 gloss:3.2915 dloss:0.2953\n",
      "Episode:84 meanR:0.9169 R:1.3200 rate:0.0440 gloss:3.2966 dloss:0.2947\n",
      "Episode:85 meanR:0.9364 R:2.5900 rate:0.0863 gloss:3.2870 dloss:0.2971\n",
      "Episode:86 meanR:0.9437 R:1.5700 rate:0.0523 gloss:3.2591 dloss:0.3030\n",
      "Episode:87 meanR:0.9490 R:1.4100 rate:0.0470 gloss:3.2446 dloss:0.3058\n",
      "Episode:88 meanR:0.9544 R:1.4300 rate:0.0477 gloss:3.2306 dloss:0.3083\n",
      "Episode:89 meanR:0.9523 R:0.7700 rate:0.0257 gloss:3.2239 dloss:0.3098\n",
      "Episode:90 meanR:0.9503 R:0.7700 rate:0.0257 gloss:3.2157 dloss:0.3111\n",
      "Episode:91 meanR:0.9440 R:0.3700 rate:0.0123 gloss:3.2201 dloss:0.3104\n",
      "Episode:92 meanR:0.9467 R:1.1900 rate:0.0397 gloss:3.2272 dloss:0.3094\n",
      "Episode:93 meanR:0.9484 R:1.1100 rate:0.0370 gloss:3.2294 dloss:0.3094\n",
      "Episode:94 meanR:0.9416 R:0.3000 rate:0.0100 gloss:3.2257 dloss:0.3104\n",
      "Episode:95 meanR:0.9342 R:0.2300 rate:0.0077 gloss:3.2216 dloss:0.3113\n",
      "Episode:96 meanR:0.9265 R:0.1900 rate:0.0063 gloss:3.2377 dloss:0.3086\n",
      "Episode:97 meanR:0.9252 R:0.8000 rate:0.0267 gloss:3.2545 dloss:0.3061\n",
      "Episode:98 meanR:0.9287 R:1.2700 rate:0.0423 gloss:3.2588 dloss:0.3055\n",
      "Episode:99 meanR:0.9280 R:0.8600 rate:0.0287 gloss:3.2580 dloss:0.3065\n",
      "Episode:100 meanR:0.9233 R:2.2200 rate:0.0740 gloss:3.2743 dloss:0.3033\n",
      "Episode:101 meanR:0.9415 R:2.4500 rate:0.0817 gloss:3.2537 dloss:0.3073\n",
      "Episode:102 meanR:0.9553 R:1.7200 rate:0.0573 gloss:3.2327 dloss:0.3113\n",
      "Episode:103 meanR:0.9692 R:2.7100 rate:0.0903 gloss:3.2007 dloss:0.3176\n",
      "Episode:104 meanR:0.9692 R:1.1500 rate:0.0383 gloss:3.1700 dloss:0.3223\n",
      "Episode:105 meanR:0.9720 R:1.1800 rate:0.0393 gloss:3.1510 dloss:0.3253\n",
      "Episode:106 meanR:0.9816 R:0.9600 rate:0.0320 gloss:3.1292 dloss:0.3285\n",
      "Episode:107 meanR:0.9731 R:0.6500 rate:0.0217 gloss:3.1378 dloss:0.3266\n",
      "Episode:108 meanR:0.9814 R:1.1500 rate:0.0383 gloss:3.1520 dloss:0.3241\n",
      "Episode:109 meanR:0.9857 R:0.8900 rate:0.0297 gloss:3.1523 dloss:0.3245\n",
      "Episode:110 meanR:0.9748 R:0.1700 rate:0.0057 gloss:3.1603 dloss:0.3235\n",
      "Episode:111 meanR:0.9758 R:0.5900 rate:0.0197 gloss:3.1737 dloss:0.3216\n",
      "Episode:112 meanR:0.9694 R:0.3100 rate:0.0103 gloss:3.1869 dloss:0.3195\n",
      "Episode:113 meanR:0.9743 R:1.1400 rate:0.0380 gloss:3.1962 dloss:0.3182\n",
      "Episode:114 meanR:0.9797 R:1.1200 rate:0.0373 gloss:3.1961 dloss:0.3184\n",
      "Episode:115 meanR:0.9864 R:1.9100 rate:0.0637 gloss:3.1994 dloss:0.3179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:116 meanR:0.9913 R:0.9800 rate:0.0327 gloss:3.1728 dloss:0.3229\n",
      "Episode:117 meanR:0.9923 R:1.2800 rate:0.0427 gloss:3.1673 dloss:0.3235\n",
      "Episode:118 meanR:0.9957 R:0.8700 rate:0.0290 gloss:3.1603 dloss:0.3251\n",
      "Episode:119 meanR:0.9957 R:1.0300 rate:0.0343 gloss:3.1616 dloss:0.3244\n",
      "Episode:120 meanR:0.9968 R:1.0000 rate:0.0333 gloss:3.1521 dloss:0.3260\n",
      "Episode:121 meanR:1.0032 R:1.3300 rate:0.0443 gloss:3.1464 dloss:0.3273\n",
      "Episode:122 meanR:1.0215 R:2.7600 rate:0.0920 gloss:3.1478 dloss:0.3272\n",
      "Episode:123 meanR:1.0268 R:0.8500 rate:0.0283 gloss:3.1257 dloss:0.3328\n",
      "Episode:124 meanR:1.0382 R:1.6400 rate:0.0547 gloss:3.1195 dloss:0.3342\n",
      "Episode:125 meanR:1.0461 R:1.2300 rate:0.0410 gloss:3.1167 dloss:0.3353\n",
      "Episode:126 meanR:1.0331 R:0.2500 rate:0.0083 gloss:3.1123 dloss:0.3361\n",
      "Episode:127 meanR:1.0318 R:0.3000 rate:0.0100 gloss:3.1207 dloss:0.3349\n",
      "Episode:128 meanR:1.0311 R:0.6200 rate:0.0207 gloss:3.1295 dloss:0.3335\n",
      "Episode:129 meanR:1.0377 R:1.1700 rate:0.0390 gloss:3.1367 dloss:0.3324\n",
      "Episode:130 meanR:1.0423 R:1.0600 rate:0.0353 gloss:3.1351 dloss:0.3332\n",
      "Episode:131 meanR:1.0470 R:1.4200 rate:0.0473 gloss:3.1332 dloss:0.3338\n",
      "Episode:132 meanR:1.0536 R:0.9700 rate:0.0323 gloss:3.1280 dloss:0.3355\n",
      "Episode:133 meanR:1.0398 R:0.7100 rate:0.0237 gloss:3.1290 dloss:0.3351\n",
      "Episode:134 meanR:1.0517 R:2.3000 rate:0.0767 gloss:3.1431 dloss:0.3322\n",
      "Episode:135 meanR:1.0554 R:1.2700 rate:0.0423 gloss:3.1262 dloss:0.3358\n",
      "Episode:136 meanR:1.0437 R:0.9600 rate:0.0320 gloss:3.1274 dloss:0.3351\n",
      "Episode:137 meanR:1.0512 R:0.9400 rate:0.0313 gloss:3.1294 dloss:0.3348\n",
      "Episode:138 meanR:1.0597 R:1.8400 rate:0.0613 gloss:3.1365 dloss:0.3339\n",
      "Episode:139 meanR:1.0560 R:1.2700 rate:0.0423 gloss:3.1165 dloss:0.3378\n",
      "Episode:140 meanR:1.0502 R:0.8100 rate:0.0270 gloss:3.1179 dloss:0.3367\n",
      "Episode:141 meanR:1.0478 R:0.5200 rate:0.0173 gloss:3.1217 dloss:0.3359\n",
      "Episode:142 meanR:1.0507 R:0.8100 rate:0.0270 gloss:3.1233 dloss:0.3356\n",
      "Episode:143 meanR:1.0358 R:0.9700 rate:0.0323 gloss:3.1319 dloss:0.3334\n",
      "Episode:144 meanR:1.0381 R:1.1100 rate:0.0370 gloss:3.1377 dloss:0.3323\n",
      "Episode:145 meanR:1.0429 R:1.1100 rate:0.0370 gloss:3.1339 dloss:0.3330\n",
      "Episode:146 meanR:1.0259 R:0.3500 rate:0.0117 gloss:3.1381 dloss:0.3320\n",
      "Episode:147 meanR:1.0445 R:2.6300 rate:0.0877 gloss:3.1532 dloss:0.3292\n",
      "Episode:148 meanR:1.0519 R:1.6100 rate:0.0537 gloss:3.1382 dloss:0.3329\n",
      "Episode:149 meanR:1.0508 R:1.1000 rate:0.0367 gloss:3.1252 dloss:0.3352\n",
      "Episode:150 meanR:1.0545 R:0.9300 rate:0.0310 gloss:3.1162 dloss:0.3372\n",
      "Episode:151 meanR:1.0607 R:1.5700 rate:0.0523 gloss:3.1183 dloss:0.3372\n",
      "Episode:152 meanR:1.0570 R:0.9700 rate:0.0323 gloss:3.1189 dloss:0.3370\n",
      "Episode:153 meanR:1.0764 R:2.6300 rate:0.0877 gloss:3.1259 dloss:0.3354\n",
      "Episode:154 meanR:1.0807 R:1.0200 rate:0.0340 gloss:3.0983 dloss:0.3419\n",
      "Episode:155 meanR:1.0886 R:1.7400 rate:0.0580 gloss:3.0937 dloss:0.3430\n",
      "Episode:156 meanR:1.0859 R:0.3400 rate:0.0113 gloss:3.0851 dloss:0.3450\n",
      "Episode:157 meanR:1.0870 R:1.5000 rate:0.0500 gloss:3.0977 dloss:0.3426\n",
      "Episode:158 meanR:1.0735 R:0.0000 rate:0.0000 gloss:3.0977 dloss:0.3431\n",
      "Episode:159 meanR:1.0892 R:1.5700 rate:0.0523 gloss:3.0998 dloss:0.3421\n",
      "Episode:160 meanR:1.0989 R:1.2900 rate:0.0430 gloss:3.0940 dloss:0.3441\n",
      "Episode:161 meanR:1.0932 R:0.3200 rate:0.0107 gloss:3.0908 dloss:0.3453\n",
      "Episode:162 meanR:1.0850 R:0.2000 rate:0.0067 gloss:3.1022 dloss:0.3433\n",
      "Episode:163 meanR:1.0793 R:0.5700 rate:0.0190 gloss:3.1160 dloss:0.3403\n",
      "Episode:164 meanR:1.0773 R:1.1400 rate:0.0380 gloss:3.1227 dloss:0.3395\n",
      "Episode:165 meanR:1.0766 R:0.9300 rate:0.0310 gloss:3.1264 dloss:0.3386\n",
      "Episode:166 meanR:1.0796 R:0.3000 rate:0.0100 gloss:3.1275 dloss:0.3382\n",
      "Episode:167 meanR:1.0834 R:1.0100 rate:0.0337 gloss:3.1335 dloss:0.3371\n",
      "Episode:168 meanR:1.0780 R:0.7200 rate:0.0240 gloss:3.1369 dloss:0.3366\n",
      "Episode:169 meanR:1.0869 R:1.1200 rate:0.0373 gloss:3.1355 dloss:0.3372\n",
      "Episode:170 meanR:1.0912 R:1.6800 rate:0.0560 gloss:3.1377 dloss:0.3369\n",
      "Episode:171 meanR:1.0981 R:1.0600 rate:0.0353 gloss:3.1276 dloss:0.3392\n",
      "Episode:172 meanR:1.0897 R:0.6300 rate:0.0210 gloss:3.1278 dloss:0.3394\n",
      "Episode:173 meanR:1.1044 R:1.6400 rate:0.0547 gloss:3.1299 dloss:0.3390\n",
      "Episode:174 meanR:1.0957 R:1.2300 rate:0.0410 gloss:3.1308 dloss:0.3390\n",
      "Episode:175 meanR:1.1153 R:2.7200 rate:0.0907 gloss:3.1337 dloss:0.3384\n",
      "Episode:176 meanR:1.1129 R:0.3800 rate:0.0127 gloss:3.1067 dloss:0.3440\n",
      "Episode:177 meanR:1.1268 R:3.0200 rate:0.1007 gloss:3.1109 dloss:0.3426\n",
      "Episode:178 meanR:1.1311 R:0.7300 rate:0.0243 gloss:3.0960 dloss:0.3469\n",
      "Episode:179 meanR:1.1287 R:0.7500 rate:0.0250 gloss:3.0948 dloss:0.3470\n",
      "Episode:180 meanR:1.1247 R:1.2000 rate:0.0400 gloss:3.1069 dloss:0.3448\n",
      "Episode:181 meanR:1.1298 R:1.3500 rate:0.0450 gloss:3.1093 dloss:0.3442\n",
      "Episode:182 meanR:1.1346 R:0.9900 rate:0.0330 gloss:3.0989 dloss:0.3470\n",
      "Episode:183 meanR:1.1387 R:1.0500 rate:0.0350 gloss:3.0898 dloss:0.3485\n",
      "Episode:184 meanR:1.1470 R:2.1500 rate:0.0717 gloss:3.0928 dloss:0.3479\n",
      "Episode:185 meanR:1.1361 R:1.5000 rate:0.0500 gloss:3.0927 dloss:0.3478\n",
      "Episode:186 meanR:1.1441 R:2.3700 rate:0.0790 gloss:3.0911 dloss:0.3479\n",
      "Episode:187 meanR:1.1325 R:0.2500 rate:0.0083 gloss:3.0787 dloss:0.3503\n",
      "Episode:188 meanR:1.1259 R:0.7700 rate:0.0257 gloss:3.0864 dloss:0.3480\n",
      "Episode:189 meanR:1.1312 R:1.3000 rate:0.0433 gloss:3.0871 dloss:0.3480\n",
      "Episode:190 meanR:1.1266 R:0.3100 rate:0.0103 gloss:3.0849 dloss:0.3488\n",
      "Episode:191 meanR:1.1261 R:0.3200 rate:0.0107 gloss:3.0926 dloss:0.3473\n",
      "Episode:192 meanR:1.1193 R:0.5100 rate:0.0170 gloss:3.1071 dloss:0.3446\n",
      "Episode:193 meanR:1.1082 R:0.0000 rate:0.0000 gloss:3.1182 dloss:0.3427\n",
      "Episode:194 meanR:1.1171 R:1.1900 rate:0.0397 gloss:3.1152 dloss:0.3438\n",
      "Episode:195 meanR:1.1313 R:1.6500 rate:0.0550 gloss:3.1084 dloss:0.3459\n",
      "Episode:196 meanR:1.1392 R:0.9800 rate:0.0327 gloss:3.0998 dloss:0.3480\n",
      "Episode:197 meanR:1.1495 R:1.8300 rate:0.0610 gloss:3.0974 dloss:0.3489\n",
      "Episode:198 meanR:1.1417 R:0.4900 rate:0.0163 gloss:3.0887 dloss:0.3511\n",
      "Episode:199 meanR:1.1458 R:1.2700 rate:0.0423 gloss:3.0934 dloss:0.3499\n",
      "Episode:200 meanR:1.1303 R:0.6700 rate:0.0223 gloss:3.0957 dloss:0.3490\n",
      "Episode:201 meanR:1.1187 R:1.2900 rate:0.0430 gloss:3.1096 dloss:0.3453\n",
      "Episode:202 meanR:1.1157 R:1.4200 rate:0.0473 gloss:3.1127 dloss:0.3440\n",
      "Episode:203 meanR:1.1035 R:1.4900 rate:0.0497 gloss:3.1129 dloss:0.3432\n",
      "Episode:204 meanR:1.1027 R:1.0700 rate:0.0357 gloss:3.1135 dloss:0.3430\n",
      "Episode:205 meanR:1.0943 R:0.3400 rate:0.0113 gloss:3.1191 dloss:0.3415\n",
      "Episode:206 meanR:1.1005 R:1.5800 rate:0.0527 gloss:3.1320 dloss:0.3391\n",
      "Episode:207 meanR:1.1042 R:1.0200 rate:0.0340 gloss:3.1221 dloss:0.3419\n",
      "Episode:208 meanR:1.1000 R:0.7300 rate:0.0243 gloss:3.1230 dloss:0.3417\n",
      "Episode:209 meanR:1.0982 R:0.7100 rate:0.0237 gloss:3.1262 dloss:0.3411\n",
      "Episode:210 meanR:1.1125 R:1.6000 rate:0.0533 gloss:3.1262 dloss:0.3413\n",
      "Episode:211 meanR:1.1154 R:0.8800 rate:0.0293 gloss:3.1132 dloss:0.3442\n",
      "Episode:212 meanR:1.1175 R:0.5200 rate:0.0173 gloss:3.1065 dloss:0.3450\n",
      "Episode:213 meanR:1.1160 R:0.9900 rate:0.0330 gloss:3.1168 dloss:0.3432\n",
      "Episode:214 meanR:1.1094 R:0.4600 rate:0.0153 gloss:3.1176 dloss:0.3433\n",
      "Episode:215 meanR:1.0958 R:0.5500 rate:0.0183 gloss:3.1327 dloss:0.3405\n",
      "Episode:216 meanR:1.0975 R:1.1500 rate:0.0383 gloss:3.1409 dloss:0.3385\n",
      "Episode:217 meanR:1.0976 R:1.2900 rate:0.0430 gloss:3.1431 dloss:0.3381\n",
      "Episode:218 meanR:1.0982 R:0.9300 rate:0.0310 gloss:3.1402 dloss:0.3383\n",
      "Episode:219 meanR:1.1013 R:1.3400 rate:0.0447 gloss:3.1383 dloss:0.3388\n",
      "Episode:220 meanR:1.0986 R:0.7300 rate:0.0243 gloss:3.1347 dloss:0.3399\n",
      "Episode:221 meanR:1.0893 R:0.4000 rate:0.0133 gloss:3.1388 dloss:0.3383\n",
      "Episode:222 meanR:1.0757 R:1.4000 rate:0.0467 gloss:3.1598 dloss:0.3341\n",
      "Episode:223 meanR:1.0722 R:0.5000 rate:0.0167 gloss:3.1624 dloss:0.3333\n",
      "Episode:224 meanR:1.0594 R:0.3600 rate:0.0120 gloss:3.1766 dloss:0.3307\n",
      "Episode:225 meanR:1.0503 R:0.3200 rate:0.0107 gloss:3.1868 dloss:0.3286\n",
      "Episode:226 meanR:1.0647 R:1.6900 rate:0.0563 gloss:3.1919 dloss:0.3276\n",
      "Episode:227 meanR:1.0875 R:2.5800 rate:0.0860 gloss:3.1733 dloss:0.3316\n",
      "Episode:228 meanR:1.0912 R:0.9900 rate:0.0330 gloss:3.1515 dloss:0.3362\n",
      "Episode:229 meanR:1.0820 R:0.2500 rate:0.0083 gloss:3.1417 dloss:0.3377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:230 meanR:1.0861 R:1.4700 rate:0.0490 gloss:3.1468 dloss:0.3363\n",
      "Episode:231 meanR:1.0807 R:0.8800 rate:0.0293 gloss:3.1509 dloss:0.3357\n",
      "Episode:232 meanR:1.0785 R:0.7500 rate:0.0250 gloss:3.1559 dloss:0.3347\n",
      "Episode:233 meanR:1.0794 R:0.8000 rate:0.0267 gloss:3.1561 dloss:0.3350\n",
      "Episode:234 meanR:1.0564 R:0.0000 rate:0.0000 gloss:3.1661 dloss:0.3327\n",
      "Episode:235 meanR:1.0504 R:0.6700 rate:0.0223 gloss:3.1715 dloss:0.3312\n",
      "Episode:236 meanR:1.0432 R:0.2400 rate:0.0080 gloss:3.1708 dloss:0.3311\n",
      "Episode:237 meanR:1.0411 R:0.7300 rate:0.0243 gloss:3.1819 dloss:0.3290\n",
      "Episode:238 meanR:1.0265 R:0.3800 rate:0.0127 gloss:3.1896 dloss:0.3275\n",
      "Episode:239 meanR:1.0201 R:0.6300 rate:0.0210 gloss:3.1710 dloss:0.3294\n",
      "Episode:240 meanR:1.0120 R:0.0000 rate:0.0000 gloss:3.1762 dloss:0.3283\n",
      "Episode:241 meanR:1.0134 R:0.6600 rate:0.0220 gloss:3.1757 dloss:0.3287\n",
      "Episode:242 meanR:1.0090 R:0.3700 rate:0.0123 gloss:3.1731 dloss:0.3288\n",
      "Episode:243 meanR:1.0005 R:0.1200 rate:0.0040 gloss:3.2064 dloss:0.3231\n",
      "Episode:244 meanR:1.0107 R:2.1300 rate:0.0710 gloss:3.2182 dloss:0.3209\n",
      "Episode:245 meanR:1.0067 R:0.7100 rate:0.0237 gloss:3.1938 dloss:0.3251\n",
      "Episode:246 meanR:1.0051 R:0.1900 rate:0.0063 gloss:3.1897 dloss:0.3254\n",
      "Episode:247 meanR:0.9806 R:0.1800 rate:0.0060 gloss:3.1979 dloss:0.3226\n",
      "Episode:248 meanR:0.9727 R:0.8200 rate:0.0273 gloss:3.2339 dloss:0.3165\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        num_step = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_preds = sess.run(model.actions_preds, feed_dict={model.states: state.reshape([1, -1]), \n",
    "                                                                    model.training: False})\n",
    "            noise = np.random.normal(loc=0, scale=0.1, size=action_size) # randomness\n",
    "            action = action_preds + noise\n",
    "            #print(action.shape, action_logits.shape, noise.shape)\n",
    "            action = np.clip(action, -1, 1) # [-1, +1]\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            rate = -1 # success rate: # [-1, +1]\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "            num_step += 1 # memory updated\n",
    "            total_reward += reward # max reward 30\n",
    "            state = next_state\n",
    "            \n",
    "            if done is True:\n",
    "                # Best 100-episode average reward was +30 \n",
    "                # (Reacher is considered \"solved\" \n",
    "                #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "                rate = total_reward/30\n",
    "                rate = np.clip(rate, -1, 1) # [-1, +1]\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1:\n",
    "                        memory.buffer[-1-idx][-1] = rate\n",
    "                        \n",
    "            # Training\n",
    "            allbatch = memory.buffer\n",
    "            rates = np.array([each[5] for each in allbatch])\n",
    "            ratedbatch = np.array(memory.buffer)[rates>0] # excluding -1 or unrated ones or min rated ones\n",
    "            batch = sample(batch_size=batch_size, buffer=ratedbatch)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states, \n",
    "                                                                   model.training: False})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # discrete DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # continuous DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.targetQs: targetQs, \n",
    "                                                       model.rates: rates, \n",
    "                                                       model.training: False})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        # Did not solve the environment. \n",
    "        # Best 100-episode average reward was +30 \n",
    "        # (Reacher is considered \"solved\" \n",
    "        #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "        if np.mean(episode_reward) >= 30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
