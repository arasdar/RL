{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.5525128278408378 -2.7101178137045903\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74, 6), (74, 4), (74,), (74, 4), (74,), (74,), (74,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:26.0000 R:26.0000 rate:0.0520 gloss:0.6220 dlossA:0.6543 dlossQ:0.9885 exploreP:0.9974\n",
      "Episode:1 meanR:24.5000 R:23.0000 rate:0.0460 gloss:0.6045 dlossA:0.6495 dlossQ:0.9936 exploreP:0.9952\n",
      "Episode:2 meanR:23.6667 R:22.0000 rate:0.0440 gloss:0.6037 dlossA:0.6521 dlossQ:0.9904 exploreP:0.9930\n",
      "Episode:3 meanR:23.5000 R:23.0000 rate:0.0460 gloss:0.5981 dlossA:0.6501 dlossQ:1.0008 exploreP:0.9907\n",
      "Episode:4 meanR:25.4000 R:33.0000 rate:0.0660 gloss:0.6035 dlossA:0.6498 dlossQ:0.9987 exploreP:0.9875\n",
      "Episode:5 meanR:25.1667 R:24.0000 rate:0.0480 gloss:0.6359 dlossA:0.6576 dlossQ:1.0047 exploreP:0.9852\n",
      "Episode:6 meanR:24.4286 R:20.0000 rate:0.0400 gloss:0.6055 dlossA:0.6520 dlossQ:1.0148 exploreP:0.9832\n",
      "Episode:7 meanR:24.3750 R:24.0000 rate:0.0480 gloss:0.5884 dlossA:0.6487 dlossQ:1.0007 exploreP:0.9809\n",
      "Episode:8 meanR:25.6667 R:36.0000 rate:0.0720 gloss:0.6141 dlossA:0.6554 dlossQ:1.0043 exploreP:0.9774\n",
      "Episode:9 meanR:24.1000 R:10.0000 rate:0.0200 gloss:0.6109 dlossA:0.6561 dlossQ:1.0048 exploreP:0.9764\n",
      "Episode:10 meanR:23.8182 R:21.0000 rate:0.0420 gloss:0.6091 dlossA:0.6534 dlossQ:1.0023 exploreP:0.9744\n",
      "Episode:11 meanR:23.0833 R:15.0000 rate:0.0300 gloss:0.5980 dlossA:0.6524 dlossQ:0.9910 exploreP:0.9730\n",
      "Episode:12 meanR:23.5385 R:29.0000 rate:0.0580 gloss:0.6280 dlossA:0.6580 dlossQ:1.0076 exploreP:0.9702\n",
      "Episode:13 meanR:23.5000 R:23.0000 rate:0.0460 gloss:0.6142 dlossA:0.6552 dlossQ:1.0066 exploreP:0.9680\n",
      "Episode:14 meanR:25.1333 R:48.0000 rate:0.0960 gloss:0.6032 dlossA:0.6533 dlossQ:1.0039 exploreP:0.9634\n",
      "Episode:15 meanR:25.1875 R:26.0000 rate:0.0520 gloss:0.6024 dlossA:0.6544 dlossQ:1.0027 exploreP:0.9609\n",
      "Episode:16 meanR:25.0000 R:22.0000 rate:0.0440 gloss:0.5978 dlossA:0.6516 dlossQ:1.0016 exploreP:0.9588\n",
      "Episode:17 meanR:24.2222 R:11.0000 rate:0.0220 gloss:0.5940 dlossA:0.6528 dlossQ:1.0029 exploreP:0.9578\n",
      "Episode:18 meanR:23.9474 R:19.0000 rate:0.0380 gloss:0.6078 dlossA:0.6572 dlossQ:1.0122 exploreP:0.9560\n",
      "Episode:19 meanR:23.4000 R:13.0000 rate:0.0260 gloss:0.6256 dlossA:0.6587 dlossQ:1.0145 exploreP:0.9547\n",
      "Episode:20 meanR:23.1905 R:19.0000 rate:0.0380 gloss:0.6086 dlossA:0.6556 dlossQ:1.0081 exploreP:0.9529\n",
      "Episode:21 meanR:26.2273 R:90.0000 rate:0.1800 gloss:0.5961 dlossA:0.6536 dlossQ:1.0042 exploreP:0.9445\n",
      "Episode:22 meanR:25.7826 R:16.0000 rate:0.0320 gloss:0.5991 dlossA:0.6516 dlossQ:1.0043 exploreP:0.9430\n",
      "Episode:23 meanR:25.7083 R:24.0000 rate:0.0480 gloss:0.5900 dlossA:0.6509 dlossQ:0.9946 exploreP:0.9408\n",
      "Episode:24 meanR:25.7600 R:27.0000 rate:0.0540 gloss:0.5989 dlossA:0.6512 dlossQ:1.0011 exploreP:0.9383\n",
      "Episode:25 meanR:25.3846 R:16.0000 rate:0.0320 gloss:0.6023 dlossA:0.6557 dlossQ:0.9984 exploreP:0.9368\n",
      "Episode:26 meanR:25.0741 R:17.0000 rate:0.0340 gloss:0.5808 dlossA:0.6495 dlossQ:0.9986 exploreP:0.9352\n",
      "Episode:27 meanR:24.8214 R:18.0000 rate:0.0360 gloss:0.5939 dlossA:0.6541 dlossQ:0.9991 exploreP:0.9335\n",
      "Episode:28 meanR:24.6552 R:20.0000 rate:0.0400 gloss:0.5823 dlossA:0.6501 dlossQ:0.9948 exploreP:0.9317\n",
      "Episode:29 meanR:25.0333 R:36.0000 rate:0.0720 gloss:0.5720 dlossA:0.6469 dlossQ:0.9878 exploreP:0.9284\n",
      "Episode:30 meanR:24.9355 R:22.0000 rate:0.0440 gloss:0.5740 dlossA:0.6462 dlossQ:0.9883 exploreP:0.9264\n",
      "Episode:31 meanR:25.0312 R:28.0000 rate:0.0560 gloss:0.5620 dlossA:0.6440 dlossQ:0.9918 exploreP:0.9238\n",
      "Episode:32 meanR:24.8485 R:19.0000 rate:0.0380 gloss:0.6017 dlossA:0.6579 dlossQ:0.9985 exploreP:0.9221\n",
      "Episode:33 meanR:24.5882 R:16.0000 rate:0.0320 gloss:0.6017 dlossA:0.6542 dlossQ:1.0029 exploreP:0.9206\n",
      "Episode:34 meanR:26.0000 R:74.0000 rate:0.1480 gloss:0.5932 dlossA:0.6524 dlossQ:0.9971 exploreP:0.9139\n",
      "Episode:35 meanR:26.1111 R:30.0000 rate:0.0600 gloss:0.5781 dlossA:0.6503 dlossQ:0.9897 exploreP:0.9112\n",
      "Episode:36 meanR:26.1892 R:29.0000 rate:0.0580 gloss:0.5760 dlossA:0.6497 dlossQ:0.9927 exploreP:0.9086\n",
      "Episode:37 meanR:25.8158 R:12.0000 rate:0.0240 gloss:0.5868 dlossA:0.6510 dlossQ:0.9983 exploreP:0.9075\n",
      "Episode:38 meanR:25.4103 R:10.0000 rate:0.0200 gloss:0.5640 dlossA:0.6463 dlossQ:0.9810 exploreP:0.9066\n",
      "Episode:39 meanR:25.1250 R:14.0000 rate:0.0280 gloss:0.6135 dlossA:0.6601 dlossQ:0.9957 exploreP:0.9053\n",
      "Episode:40 meanR:24.7561 R:10.0000 rate:0.0200 gloss:0.5881 dlossA:0.6493 dlossQ:0.9924 exploreP:0.9044\n",
      "Episode:41 meanR:25.3333 R:49.0000 rate:0.0980 gloss:0.5875 dlossA:0.6518 dlossQ:0.9911 exploreP:0.9001\n",
      "Episode:42 meanR:25.2791 R:23.0000 rate:0.0460 gloss:0.5728 dlossA:0.6477 dlossQ:0.9818 exploreP:0.8980\n",
      "Episode:43 meanR:26.1136 R:62.0000 rate:0.1240 gloss:0.5729 dlossA:0.6460 dlossQ:0.9829 exploreP:0.8925\n",
      "Episode:44 meanR:26.2667 R:33.0000 rate:0.0660 gloss:0.5698 dlossA:0.6450 dlossQ:0.9848 exploreP:0.8896\n",
      "Episode:45 meanR:26.4348 R:34.0000 rate:0.0680 gloss:0.5710 dlossA:0.6457 dlossQ:0.9831 exploreP:0.8866\n",
      "Episode:46 meanR:27.0213 R:54.0000 rate:0.1080 gloss:0.5578 dlossA:0.6435 dlossQ:0.9804 exploreP:0.8819\n",
      "Episode:47 meanR:26.7292 R:13.0000 rate:0.0260 gloss:0.5983 dlossA:0.6522 dlossQ:0.9935 exploreP:0.8808\n",
      "Episode:48 meanR:26.5306 R:17.0000 rate:0.0340 gloss:0.5670 dlossA:0.6460 dlossQ:0.9791 exploreP:0.8793\n",
      "Episode:49 meanR:26.6600 R:33.0000 rate:0.0660 gloss:0.5979 dlossA:0.6559 dlossQ:0.9941 exploreP:0.8765\n",
      "Episode:50 meanR:26.4118 R:14.0000 rate:0.0280 gloss:0.5863 dlossA:0.6521 dlossQ:0.9915 exploreP:0.8752\n",
      "Episode:51 meanR:26.9231 R:53.0000 rate:0.1060 gloss:0.5660 dlossA:0.6449 dlossQ:0.9889 exploreP:0.8707\n",
      "Episode:52 meanR:26.7925 R:20.0000 rate:0.0400 gloss:0.5688 dlossA:0.6475 dlossQ:0.9755 exploreP:0.8689\n",
      "Episode:53 meanR:27.6667 R:74.0000 rate:0.1480 gloss:0.5684 dlossA:0.6463 dlossQ:0.9820 exploreP:0.8626\n",
      "Episode:54 meanR:27.5636 R:22.0000 rate:0.0440 gloss:0.5402 dlossA:0.6389 dlossQ:0.9689 exploreP:0.8607\n",
      "Episode:55 meanR:27.6071 R:30.0000 rate:0.0600 gloss:0.5722 dlossA:0.6472 dlossQ:0.9832 exploreP:0.8582\n",
      "Episode:56 meanR:27.4211 R:17.0000 rate:0.0340 gloss:0.5388 dlossA:0.6420 dlossQ:0.9714 exploreP:0.8567\n",
      "Episode:57 meanR:27.5862 R:37.0000 rate:0.0740 gloss:0.5663 dlossA:0.6467 dlossQ:0.9779 exploreP:0.8536\n",
      "Episode:58 meanR:28.4407 R:78.0000 rate:0.1560 gloss:0.5664 dlossA:0.6456 dlossQ:0.9768 exploreP:0.8471\n",
      "Episode:59 meanR:28.2333 R:16.0000 rate:0.0320 gloss:0.5449 dlossA:0.6390 dlossQ:0.9637 exploreP:0.8457\n",
      "Episode:60 meanR:28.2623 R:30.0000 rate:0.0600 gloss:0.5413 dlossA:0.6393 dlossQ:0.9675 exploreP:0.8432\n",
      "Episode:61 meanR:28.5000 R:43.0000 rate:0.0860 gloss:0.5471 dlossA:0.6414 dlossQ:0.9663 exploreP:0.8397\n",
      "Episode:62 meanR:28.7143 R:42.0000 rate:0.0840 gloss:0.5323 dlossA:0.6380 dlossQ:0.9672 exploreP:0.8362\n",
      "Episode:63 meanR:29.3750 R:71.0000 rate:0.1420 gloss:0.5496 dlossA:0.6415 dlossQ:0.9683 exploreP:0.8303\n",
      "Episode:64 meanR:29.1077 R:12.0000 rate:0.0240 gloss:0.5560 dlossA:0.6435 dlossQ:0.9692 exploreP:0.8293\n",
      "Episode:65 meanR:28.9394 R:18.0000 rate:0.0360 gloss:0.5382 dlossA:0.6379 dlossQ:0.9686 exploreP:0.8279\n",
      "Episode:66 meanR:28.7463 R:16.0000 rate:0.0320 gloss:0.5479 dlossA:0.6424 dlossQ:0.9663 exploreP:0.8266\n",
      "Episode:67 meanR:28.5000 R:12.0000 rate:0.0240 gloss:0.5277 dlossA:0.6377 dlossQ:0.9709 exploreP:0.8256\n",
      "Episode:68 meanR:28.4058 R:22.0000 rate:0.0440 gloss:0.5525 dlossA:0.6436 dlossQ:0.9730 exploreP:0.8238\n",
      "Episode:69 meanR:28.1714 R:12.0000 rate:0.0240 gloss:0.5484 dlossA:0.6446 dlossQ:0.9722 exploreP:0.8228\n",
      "Episode:70 meanR:28.0141 R:17.0000 rate:0.0340 gloss:0.5264 dlossA:0.6318 dlossQ:0.9692 exploreP:0.8214\n",
      "Episode:71 meanR:27.8472 R:16.0000 rate:0.0320 gloss:0.5518 dlossA:0.6399 dlossQ:0.9666 exploreP:0.8201\n",
      "Episode:72 meanR:27.7123 R:18.0000 rate:0.0360 gloss:0.5471 dlossA:0.6436 dlossQ:0.9724 exploreP:0.8187\n",
      "Episode:73 meanR:27.6486 R:23.0000 rate:0.0460 gloss:0.5605 dlossA:0.6421 dlossQ:0.9602 exploreP:0.8168\n",
      "Episode:74 meanR:27.5867 R:23.0000 rate:0.0460 gloss:0.5445 dlossA:0.6429 dlossQ:0.9768 exploreP:0.8150\n",
      "Episode:75 meanR:27.7500 R:40.0000 rate:0.0800 gloss:0.5618 dlossA:0.6465 dlossQ:0.9754 exploreP:0.8118\n",
      "Episode:76 meanR:27.8312 R:34.0000 rate:0.0680 gloss:0.5475 dlossA:0.6371 dlossQ:1.0152 exploreP:0.8090\n",
      "Episode:77 meanR:28.1667 R:54.0000 rate:0.1080 gloss:0.5519 dlossA:0.6417 dlossQ:0.9718 exploreP:0.8047\n",
      "Episode:78 meanR:28.1392 R:26.0000 rate:0.0520 gloss:0.5534 dlossA:0.6426 dlossQ:0.9709 exploreP:0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:28.2750 R:39.0000 rate:0.0780 gloss:0.5451 dlossA:0.6408 dlossQ:0.9726 exploreP:0.7996\n",
      "Episode:80 meanR:28.7778 R:69.0000 rate:0.1380 gloss:0.5376 dlossA:0.6360 dlossQ:0.9674 exploreP:0.7942\n",
      "Episode:81 meanR:28.6585 R:19.0000 rate:0.0380 gloss:0.5199 dlossA:0.6317 dlossQ:0.9642 exploreP:0.7927\n",
      "Episode:82 meanR:28.7831 R:39.0000 rate:0.0780 gloss:0.5477 dlossA:0.6410 dlossQ:0.9629 exploreP:0.7896\n",
      "Episode:83 meanR:29.3810 R:79.0000 rate:0.1580 gloss:0.5433 dlossA:0.6400 dlossQ:0.9652 exploreP:0.7835\n",
      "Episode:84 meanR:29.4941 R:39.0000 rate:0.0780 gloss:0.5499 dlossA:0.6415 dlossQ:0.9740 exploreP:0.7805\n",
      "Episode:85 meanR:29.3256 R:15.0000 rate:0.0300 gloss:0.5500 dlossA:0.6427 dlossQ:0.9731 exploreP:0.7793\n",
      "Episode:86 meanR:30.0920 R:96.0000 rate:0.1920 gloss:0.5302 dlossA:0.6368 dlossQ:0.9605 exploreP:0.7720\n",
      "Episode:87 meanR:30.0795 R:29.0000 rate:0.0580 gloss:0.5300 dlossA:0.6358 dlossQ:0.9672 exploreP:0.7698\n",
      "Episode:88 meanR:29.9551 R:19.0000 rate:0.0380 gloss:0.5438 dlossA:0.6393 dlossQ:0.9720 exploreP:0.7683\n",
      "Episode:89 meanR:30.2556 R:57.0000 rate:0.1140 gloss:0.5326 dlossA:0.6344 dlossQ:0.9753 exploreP:0.7640\n",
      "Episode:90 meanR:30.4615 R:49.0000 rate:0.0980 gloss:0.5173 dlossA:0.6340 dlossQ:0.9581 exploreP:0.7603\n",
      "Episode:91 meanR:30.7065 R:53.0000 rate:0.1060 gloss:0.5390 dlossA:0.6388 dlossQ:0.9635 exploreP:0.7564\n",
      "Episode:92 meanR:31.6989 R:123.0000 rate:0.2460 gloss:0.5275 dlossA:0.6362 dlossQ:0.9584 exploreP:0.7472\n",
      "Episode:93 meanR:31.4894 R:12.0000 rate:0.0240 gloss:0.5383 dlossA:0.6329 dlossQ:0.9744 exploreP:0.7463\n",
      "Episode:94 meanR:31.5158 R:34.0000 rate:0.0680 gloss:0.5394 dlossA:0.6376 dlossQ:0.9698 exploreP:0.7439\n",
      "Episode:95 meanR:31.3125 R:12.0000 rate:0.0240 gloss:0.5304 dlossA:0.6378 dlossQ:0.9701 exploreP:0.7430\n",
      "Episode:96 meanR:31.0825 R:9.0000 rate:0.0180 gloss:0.5143 dlossA:0.6371 dlossQ:0.9514 exploreP:0.7423\n",
      "Episode:97 meanR:31.5816 R:80.0000 rate:0.1600 gloss:0.5314 dlossA:0.6382 dlossQ:0.9585 exploreP:0.7365\n",
      "Episode:98 meanR:31.5556 R:29.0000 rate:0.0580 gloss:0.5402 dlossA:0.6342 dlossQ:0.9727 exploreP:0.7344\n",
      "Episode:99 meanR:31.3700 R:13.0000 rate:0.0260 gloss:0.5160 dlossA:0.6321 dlossQ:0.9640 exploreP:0.7334\n",
      "Episode:100 meanR:31.2200 R:11.0000 rate:0.0220 gloss:0.5305 dlossA:0.6312 dlossQ:0.9704 exploreP:0.7326\n",
      "Episode:101 meanR:31.1700 R:18.0000 rate:0.0360 gloss:0.5308 dlossA:0.6352 dlossQ:0.9688 exploreP:0.7313\n",
      "Episode:102 meanR:31.4400 R:49.0000 rate:0.0980 gloss:0.5314 dlossA:0.6367 dlossQ:0.9682 exploreP:0.7278\n",
      "Episode:103 meanR:32.2000 R:99.0000 rate:0.1980 gloss:0.5243 dlossA:0.6356 dlossQ:0.9596 exploreP:0.7207\n",
      "Episode:104 meanR:32.1400 R:27.0000 rate:0.0540 gloss:0.5110 dlossA:0.6307 dlossQ:0.9621 exploreP:0.7188\n",
      "Episode:105 meanR:32.4500 R:55.0000 rate:0.1100 gloss:0.5270 dlossA:0.6332 dlossQ:0.9562 exploreP:0.7149\n",
      "Episode:106 meanR:32.4400 R:19.0000 rate:0.0380 gloss:0.5172 dlossA:0.6312 dlossQ:0.9581 exploreP:0.7136\n",
      "Episode:107 meanR:32.3500 R:15.0000 rate:0.0300 gloss:0.5316 dlossA:0.6357 dlossQ:0.9648 exploreP:0.7125\n",
      "Episode:108 meanR:32.3000 R:31.0000 rate:0.0620 gloss:0.5273 dlossA:0.6339 dlossQ:0.9667 exploreP:0.7104\n",
      "Episode:109 meanR:32.3800 R:18.0000 rate:0.0360 gloss:0.5510 dlossA:0.6433 dlossQ:0.9740 exploreP:0.7091\n",
      "Episode:110 meanR:32.6100 R:44.0000 rate:0.0880 gloss:0.5323 dlossA:0.6373 dlossQ:0.9690 exploreP:0.7060\n",
      "Episode:111 meanR:33.1100 R:65.0000 rate:0.1300 gloss:0.5338 dlossA:0.6348 dlossQ:0.9622 exploreP:0.7015\n",
      "Episode:112 meanR:32.9700 R:15.0000 rate:0.0300 gloss:0.5405 dlossA:0.6391 dlossQ:0.9638 exploreP:0.7005\n",
      "Episode:113 meanR:33.2000 R:46.0000 rate:0.0920 gloss:0.5481 dlossA:0.6399 dlossQ:0.9849 exploreP:0.6973\n",
      "Episode:114 meanR:32.9700 R:25.0000 rate:0.0500 gloss:0.5132 dlossA:0.6319 dlossQ:0.9637 exploreP:0.6956\n",
      "Episode:115 meanR:32.8400 R:13.0000 rate:0.0260 gloss:0.5251 dlossA:0.6329 dlossQ:0.9854 exploreP:0.6947\n",
      "Episode:116 meanR:32.9200 R:30.0000 rate:0.0600 gloss:0.5253 dlossA:0.6310 dlossQ:0.9690 exploreP:0.6927\n",
      "Episode:117 meanR:33.0200 R:21.0000 rate:0.0420 gloss:0.5308 dlossA:0.6357 dlossQ:0.9717 exploreP:0.6912\n",
      "Episode:118 meanR:33.0300 R:20.0000 rate:0.0400 gloss:0.5446 dlossA:0.6382 dlossQ:0.9861 exploreP:0.6899\n",
      "Episode:119 meanR:33.5200 R:62.0000 rate:0.1240 gloss:0.5053 dlossA:0.6270 dlossQ:0.9708 exploreP:0.6857\n",
      "Episode:120 meanR:33.8700 R:54.0000 rate:0.1080 gloss:0.5278 dlossA:0.6341 dlossQ:0.9626 exploreP:0.6820\n",
      "Episode:121 meanR:33.5300 R:56.0000 rate:0.1120 gloss:0.5010 dlossA:0.6239 dlossQ:0.9642 exploreP:0.6783\n",
      "Episode:122 meanR:33.5400 R:17.0000 rate:0.0340 gloss:0.5193 dlossA:0.6327 dlossQ:0.9632 exploreP:0.6771\n",
      "Episode:123 meanR:34.3500 R:105.0000 rate:0.2100 gloss:0.5236 dlossA:0.6320 dlossQ:0.9858 exploreP:0.6702\n",
      "Episode:124 meanR:35.0700 R:99.0000 rate:0.1980 gloss:0.5239 dlossA:0.6339 dlossQ:0.9763 exploreP:0.6637\n",
      "Episode:125 meanR:35.2900 R:38.0000 rate:0.0760 gloss:0.5288 dlossA:0.6371 dlossQ:0.9671 exploreP:0.6612\n",
      "Episode:126 meanR:36.3500 R:123.0000 rate:0.2460 gloss:0.5158 dlossA:0.6298 dlossQ:0.9665 exploreP:0.6532\n",
      "Episode:127 meanR:38.4600 R:229.0000 rate:0.4580 gloss:0.5207 dlossA:0.6330 dlossQ:0.9745 exploreP:0.6387\n",
      "Episode:128 meanR:38.4200 R:16.0000 rate:0.0320 gloss:0.4802 dlossA:0.6198 dlossQ:0.9517 exploreP:0.6377\n",
      "Episode:129 meanR:38.3600 R:30.0000 rate:0.0600 gloss:0.5290 dlossA:0.6342 dlossQ:0.9899 exploreP:0.6358\n",
      "Episode:130 meanR:38.7500 R:61.0000 rate:0.1220 gloss:0.5340 dlossA:0.6341 dlossQ:0.9764 exploreP:0.6320\n",
      "Episode:131 meanR:39.1100 R:64.0000 rate:0.1280 gloss:0.4970 dlossA:0.6243 dlossQ:0.9693 exploreP:0.6280\n",
      "Episode:132 meanR:39.6100 R:69.0000 rate:0.1380 gloss:0.5006 dlossA:0.6288 dlossQ:0.9765 exploreP:0.6238\n",
      "Episode:133 meanR:40.5200 R:107.0000 rate:0.2140 gloss:0.5232 dlossA:0.6346 dlossQ:0.9878 exploreP:0.6172\n",
      "Episode:134 meanR:40.4600 R:68.0000 rate:0.1360 gloss:0.5181 dlossA:0.6301 dlossQ:0.9694 exploreP:0.6131\n",
      "Episode:135 meanR:40.4400 R:28.0000 rate:0.0560 gloss:0.5462 dlossA:0.6425 dlossQ:0.9926 exploreP:0.6114\n",
      "Episode:136 meanR:40.6600 R:51.0000 rate:0.1020 gloss:0.5416 dlossA:0.6389 dlossQ:0.9804 exploreP:0.6084\n",
      "Episode:137 meanR:40.8300 R:29.0000 rate:0.0580 gloss:0.5307 dlossA:0.6310 dlossQ:0.9925 exploreP:0.6066\n",
      "Episode:138 meanR:41.4600 R:73.0000 rate:0.1460 gloss:0.5282 dlossA:0.6361 dlossQ:0.9984 exploreP:0.6023\n",
      "Episode:139 meanR:42.9600 R:164.0000 rate:0.3280 gloss:0.5312 dlossA:0.6344 dlossQ:0.9878 exploreP:0.5927\n",
      "Episode:140 meanR:43.5500 R:69.0000 rate:0.1380 gloss:0.5205 dlossA:0.6354 dlossQ:0.9774 exploreP:0.5887\n",
      "Episode:141 meanR:43.4600 R:40.0000 rate:0.0800 gloss:0.5173 dlossA:0.6280 dlossQ:1.0015 exploreP:0.5863\n",
      "Episode:142 meanR:43.7000 R:47.0000 rate:0.0940 gloss:0.5001 dlossA:0.6273 dlossQ:0.9797 exploreP:0.5836\n",
      "Episode:143 meanR:43.5300 R:45.0000 rate:0.0900 gloss:0.5325 dlossA:0.6357 dlossQ:0.9904 exploreP:0.5811\n",
      "Episode:144 meanR:45.4600 R:226.0000 rate:0.4520 gloss:0.5128 dlossA:0.6305 dlossQ:0.9825 exploreP:0.5683\n",
      "Episode:145 meanR:45.9800 R:86.0000 rate:0.1720 gloss:0.5099 dlossA:0.6329 dlossQ:0.9921 exploreP:0.5635\n",
      "Episode:146 meanR:46.8400 R:140.0000 rate:0.2800 gloss:0.5173 dlossA:0.6298 dlossQ:0.9910 exploreP:0.5558\n",
      "Episode:147 meanR:48.9900 R:228.0000 rate:0.4560 gloss:0.5150 dlossA:0.6314 dlossQ:0.9897 exploreP:0.5435\n",
      "Episode:148 meanR:49.5700 R:75.0000 rate:0.1500 gloss:0.5379 dlossA:0.6378 dlossQ:0.9932 exploreP:0.5395\n",
      "Episode:149 meanR:50.4200 R:118.0000 rate:0.2360 gloss:0.5320 dlossA:0.6343 dlossQ:1.0036 exploreP:0.5333\n",
      "Episode:150 meanR:51.1400 R:86.0000 rate:0.1720 gloss:0.5277 dlossA:0.6403 dlossQ:0.9951 exploreP:0.5288\n",
      "Episode:151 meanR:53.5500 R:294.0000 rate:0.5880 gloss:0.5263 dlossA:0.6382 dlossQ:0.9913 exploreP:0.5138\n",
      "Episode:152 meanR:57.6700 R:432.0000 rate:0.8640 gloss:0.5293 dlossA:0.6364 dlossQ:0.9990 exploreP:0.4925\n",
      "Episode:153 meanR:59.4600 R:253.0000 rate:0.5060 gloss:0.5306 dlossA:0.6392 dlossQ:0.9984 exploreP:0.4805\n",
      "Episode:154 meanR:62.6100 R:337.0000 rate:0.6740 gloss:0.5421 dlossA:0.6423 dlossQ:0.9958 exploreP:0.4649\n",
      "Episode:155 meanR:63.6700 R:136.0000 rate:0.2720 gloss:0.5415 dlossA:0.6383 dlossQ:1.0016 exploreP:0.4587\n",
      "Episode:156 meanR:65.4900 R:199.0000 rate:0.3980 gloss:0.5376 dlossA:0.6416 dlossQ:1.0062 exploreP:0.4499\n",
      "Episode:157 meanR:70.1200 R:500.0000 rate:1.0000 gloss:0.5414 dlossA:0.6435 dlossQ:0.9977 exploreP:0.4284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:70.5200 R:118.0000 rate:0.2360 gloss:0.5558 dlossA:0.6554 dlossQ:0.9947 exploreP:0.4235\n",
      "Episode:159 meanR:71.3200 R:96.0000 rate:0.1920 gloss:0.5705 dlossA:0.6578 dlossQ:0.9943 exploreP:0.4196\n",
      "Episode:160 meanR:71.3700 R:35.0000 rate:0.0700 gloss:0.5713 dlossA:0.6550 dlossQ:0.9952 exploreP:0.4181\n",
      "Episode:161 meanR:72.1800 R:124.0000 rate:0.2480 gloss:0.5527 dlossA:0.6509 dlossQ:0.9865 exploreP:0.4131\n",
      "Episode:162 meanR:72.6500 R:89.0000 rate:0.1780 gloss:0.5481 dlossA:0.6478 dlossQ:0.9885 exploreP:0.4095\n",
      "Episode:163 meanR:74.5000 R:256.0000 rate:0.5120 gloss:0.5643 dlossA:0.6555 dlossQ:0.9876 exploreP:0.3994\n",
      "Episode:164 meanR:74.6600 R:28.0000 rate:0.0560 gloss:0.5534 dlossA:0.6465 dlossQ:1.0032 exploreP:0.3983\n",
      "Episode:165 meanR:79.4800 R:500.0000 rate:1.0000 gloss:0.5700 dlossA:0.6605 dlossQ:0.9875 exploreP:0.3794\n",
      "Episode:166 meanR:84.3200 R:500.0000 rate:1.0000 gloss:0.5917 dlossA:0.6693 dlossQ:0.9796 exploreP:0.3614\n",
      "Episode:167 meanR:89.2000 R:500.0000 rate:1.0000 gloss:0.5941 dlossA:0.6733 dlossQ:0.9771 exploreP:0.3443\n",
      "Episode:168 meanR:93.1100 R:413.0000 rate:0.8260 gloss:0.6066 dlossA:0.6776 dlossQ:0.9739 exploreP:0.3307\n",
      "Episode:169 meanR:95.7300 R:274.0000 rate:0.5480 gloss:0.6197 dlossA:0.6859 dlossQ:0.9628 exploreP:0.3221\n",
      "Episode:170 meanR:100.5600 R:500.0000 rate:1.0000 gloss:0.6333 dlossA:0.6950 dlossQ:0.9553 exploreP:0.3068\n",
      "Episode:171 meanR:103.7000 R:330.0000 rate:0.6600 gloss:0.6410 dlossA:0.6984 dlossQ:0.9429 exploreP:0.2972\n",
      "Episode:172 meanR:107.5400 R:402.0000 rate:0.8040 gloss:0.6523 dlossA:0.7040 dlossQ:0.9479 exploreP:0.2859\n",
      "Episode:173 meanR:111.2600 R:395.0000 rate:0.7900 gloss:0.6619 dlossA:0.7103 dlossQ:0.9423 exploreP:0.2752\n",
      "Episode:174 meanR:116.0300 R:500.0000 rate:1.0000 gloss:0.6746 dlossA:0.7173 dlossQ:0.9368 exploreP:0.2623\n",
      "Episode:175 meanR:120.6300 R:500.0000 rate:1.0000 gloss:0.6947 dlossA:0.7210 dlossQ:0.9279 exploreP:0.2500\n",
      "Episode:176 meanR:125.2900 R:500.0000 rate:1.0000 gloss:0.7097 dlossA:0.7327 dlossQ:0.9147 exploreP:0.2383\n",
      "Episode:177 meanR:129.7500 R:500.0000 rate:1.0000 gloss:0.7346 dlossA:0.7447 dlossQ:0.9074 exploreP:0.2271\n",
      "Episode:178 meanR:134.4900 R:500.0000 rate:1.0000 gloss:0.7475 dlossA:0.7540 dlossQ:0.8961 exploreP:0.2165\n",
      "Episode:179 meanR:139.1000 R:500.0000 rate:1.0000 gloss:0.7715 dlossA:0.7668 dlossQ:0.8849 exploreP:0.2065\n",
      "Episode:180 meanR:143.4100 R:500.0000 rate:1.0000 gloss:0.7826 dlossA:0.7726 dlossQ:0.8748 exploreP:0.1969\n",
      "Episode:181 meanR:148.2200 R:500.0000 rate:1.0000 gloss:0.8094 dlossA:0.7885 dlossQ:0.8684 exploreP:0.1878\n",
      "Episode:182 meanR:152.8300 R:500.0000 rate:1.0000 gloss:0.8219 dlossA:0.7958 dlossQ:0.8604 exploreP:0.1791\n",
      "Episode:183 meanR:157.0400 R:500.0000 rate:1.0000 gloss:0.8406 dlossA:0.8103 dlossQ:0.8624 exploreP:0.1709\n",
      "Episode:184 meanR:161.6500 R:500.0000 rate:1.0000 gloss:0.8511 dlossA:0.8145 dlossQ:0.8519 exploreP:0.1630\n",
      "Episode:185 meanR:166.5000 R:500.0000 rate:1.0000 gloss:0.8750 dlossA:0.8316 dlossQ:0.8472 exploreP:0.1555\n",
      "Episode:186 meanR:170.5400 R:500.0000 rate:1.0000 gloss:0.8921 dlossA:0.8375 dlossQ:0.8418 exploreP:0.1484\n",
      "Episode:187 meanR:175.2500 R:500.0000 rate:1.0000 gloss:0.9091 dlossA:0.8498 dlossQ:0.8364 exploreP:0.1417\n",
      "Episode:188 meanR:180.0600 R:500.0000 rate:1.0000 gloss:0.9313 dlossA:0.8466 dlossQ:0.8300 exploreP:0.1353\n",
      "Episode:189 meanR:184.4900 R:500.0000 rate:1.0000 gloss:0.9550 dlossA:0.8806 dlossQ:0.8344 exploreP:0.1292\n",
      "Episode:190 meanR:189.0000 R:500.0000 rate:1.0000 gloss:0.9848 dlossA:0.8882 dlossQ:0.8258 exploreP:0.1234\n",
      "Episode:191 meanR:193.4700 R:500.0000 rate:1.0000 gloss:1.0080 dlossA:0.8950 dlossQ:0.8223 exploreP:0.1178\n",
      "Episode:192 meanR:197.2400 R:500.0000 rate:1.0000 gloss:1.0447 dlossA:0.9125 dlossQ:0.8203 exploreP:0.1126\n",
      "Episode:193 meanR:202.1200 R:500.0000 rate:1.0000 gloss:1.0788 dlossA:0.9168 dlossQ:0.8207 exploreP:0.1076\n",
      "Episode:194 meanR:206.7800 R:500.0000 rate:1.0000 gloss:1.1182 dlossA:0.9484 dlossQ:0.8166 exploreP:0.1028\n",
      "Episode:195 meanR:210.5700 R:391.0000 rate:0.7820 gloss:1.1468 dlossA:0.9953 dlossQ:0.8265 exploreP:0.0992\n",
      "Episode:196 meanR:215.4800 R:500.0000 rate:1.0000 gloss:1.1831 dlossA:1.0014 dlossQ:0.8301 exploreP:0.0949\n",
      "Episode:197 meanR:218.9400 R:426.0000 rate:0.8520 gloss:1.2227 dlossA:1.0282 dlossQ:0.8376 exploreP:0.0914\n",
      "Episode:198 meanR:223.6500 R:500.0000 rate:1.0000 gloss:1.2471 dlossA:1.0368 dlossQ:0.8451 exploreP:0.0874\n",
      "Episode:199 meanR:227.8000 R:428.0000 rate:0.8560 gloss:1.2881 dlossA:1.0598 dlossQ:0.8512 exploreP:0.0841\n",
      "Episode:200 meanR:232.6900 R:500.0000 rate:1.0000 gloss:1.3339 dlossA:1.0670 dlossQ:0.8569 exploreP:0.0805\n",
      "Episode:201 meanR:237.5100 R:500.0000 rate:1.0000 gloss:1.3819 dlossA:1.1007 dlossQ:0.8631 exploreP:0.0771\n",
      "Episode:202 meanR:242.0200 R:500.0000 rate:1.0000 gloss:1.4386 dlossA:1.1339 dlossQ:0.8700 exploreP:0.0738\n",
      "Episode:203 meanR:246.0300 R:500.0000 rate:1.0000 gloss:1.4842 dlossA:1.1459 dlossQ:0.8791 exploreP:0.0707\n",
      "Episode:204 meanR:250.7600 R:500.0000 rate:1.0000 gloss:1.5416 dlossA:1.2218 dlossQ:0.8809 exploreP:0.0677\n",
      "Episode:205 meanR:255.2100 R:500.0000 rate:1.0000 gloss:1.5859 dlossA:1.2252 dlossQ:0.8865 exploreP:0.0649\n",
      "Episode:206 meanR:260.0200 R:500.0000 rate:1.0000 gloss:1.6421 dlossA:1.2813 dlossQ:0.8901 exploreP:0.0622\n",
      "Episode:207 meanR:264.8700 R:500.0000 rate:1.0000 gloss:1.6972 dlossA:1.2630 dlossQ:0.8951 exploreP:0.0597\n",
      "Episode:208 meanR:269.3400 R:478.0000 rate:0.9560 gloss:1.7520 dlossA:1.3152 dlossQ:0.8959 exploreP:0.0574\n",
      "Episode:209 meanR:274.1600 R:500.0000 rate:1.0000 gloss:1.8126 dlossA:1.3654 dlossQ:0.8979 exploreP:0.0551\n",
      "Episode:210 meanR:278.7200 R:500.0000 rate:1.0000 gloss:1.8707 dlossA:1.4216 dlossQ:0.8983 exploreP:0.0529\n",
      "Episode:211 meanR:283.0700 R:500.0000 rate:1.0000 gloss:1.9309 dlossA:1.4869 dlossQ:0.8995 exploreP:0.0508\n",
      "Episode:212 meanR:287.9200 R:500.0000 rate:1.0000 gloss:1.9837 dlossA:1.4292 dlossQ:0.8971 exploreP:0.0488\n",
      "Episode:213 meanR:292.4600 R:500.0000 rate:1.0000 gloss:2.0699 dlossA:1.6070 dlossQ:0.8927 exploreP:0.0469\n",
      "Episode:214 meanR:297.2100 R:500.0000 rate:1.0000 gloss:2.1078 dlossA:1.5656 dlossQ:0.8924 exploreP:0.0451\n",
      "Episode:215 meanR:302.0800 R:500.0000 rate:1.0000 gloss:2.1840 dlossA:1.6274 dlossQ:0.8929 exploreP:0.0434\n",
      "Episode:216 meanR:306.7800 R:500.0000 rate:1.0000 gloss:2.2723 dlossA:1.7983 dlossQ:0.9001 exploreP:0.0418\n",
      "Episode:217 meanR:311.5700 R:500.0000 rate:1.0000 gloss:2.3250 dlossA:1.7324 dlossQ:0.8913 exploreP:0.0402\n",
      "Episode:218 meanR:316.3700 R:500.0000 rate:1.0000 gloss:2.3995 dlossA:1.8012 dlossQ:0.8899 exploreP:0.0387\n",
      "Episode:219 meanR:320.7500 R:500.0000 rate:1.0000 gloss:2.4818 dlossA:1.8885 dlossQ:0.8896 exploreP:0.0373\n",
      "Episode:220 meanR:325.2100 R:500.0000 rate:1.0000 gloss:2.5647 dlossA:1.9391 dlossQ:0.8923 exploreP:0.0360\n",
      "Episode:221 meanR:329.6500 R:500.0000 rate:1.0000 gloss:2.6529 dlossA:1.9999 dlossQ:0.8811 exploreP:0.0347\n",
      "Episode:222 meanR:334.4800 R:500.0000 rate:1.0000 gloss:2.7459 dlossA:2.1533 dlossQ:0.8922 exploreP:0.0335\n",
      "Episode:223 meanR:338.4300 R:500.0000 rate:1.0000 gloss:2.8162 dlossA:2.1321 dlossQ:0.8895 exploreP:0.0324\n",
      "Episode:224 meanR:342.4400 R:500.0000 rate:1.0000 gloss:2.9280 dlossA:2.2582 dlossQ:0.8946 exploreP:0.0313\n",
      "Episode:225 meanR:347.0600 R:500.0000 rate:1.0000 gloss:3.0072 dlossA:2.3152 dlossQ:0.8995 exploreP:0.0303\n",
      "Episode:226 meanR:350.8300 R:500.0000 rate:1.0000 gloss:3.1000 dlossA:2.3524 dlossQ:0.9087 exploreP:0.0293\n",
      "Episode:227 meanR:353.5400 R:500.0000 rate:1.0000 gloss:3.2323 dlossA:2.5247 dlossQ:0.8999 exploreP:0.0283\n",
      "Episode:228 meanR:358.3800 R:500.0000 rate:1.0000 gloss:3.3356 dlossA:2.6094 dlossQ:0.9188 exploreP:0.0274\n",
      "Episode:229 meanR:363.0800 R:500.0000 rate:1.0000 gloss:3.4396 dlossA:2.6561 dlossQ:0.9273 exploreP:0.0266\n",
      "Episode:230 meanR:367.4700 R:500.0000 rate:1.0000 gloss:3.5263 dlossA:2.6014 dlossQ:0.9209 exploreP:0.0258\n",
      "Episode:231 meanR:371.8300 R:500.0000 rate:1.0000 gloss:3.6778 dlossA:2.7607 dlossQ:0.9607 exploreP:0.0250\n",
      "Episode:232 meanR:376.1400 R:500.0000 rate:1.0000 gloss:3.8592 dlossA:3.0894 dlossQ:0.9749 exploreP:0.0243\n",
      "Episode:233 meanR:380.0700 R:500.0000 rate:1.0000 gloss:3.9427 dlossA:3.0567 dlossQ:0.9639 exploreP:0.0236\n",
      "Episode:234 meanR:384.3900 R:500.0000 rate:1.0000 gloss:4.1221 dlossA:3.2128 dlossQ:0.9996 exploreP:0.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:235 meanR:389.1100 R:500.0000 rate:1.0000 gloss:4.2207 dlossA:3.1422 dlossQ:1.0622 exploreP:0.0223\n",
      "Episode:236 meanR:393.6000 R:500.0000 rate:1.0000 gloss:4.3874 dlossA:3.4297 dlossQ:1.0561 exploreP:0.0217\n",
      "Episode:237 meanR:398.3100 R:500.0000 rate:1.0000 gloss:4.5502 dlossA:3.6323 dlossQ:1.0640 exploreP:0.0211\n",
      "Episode:238 meanR:402.5800 R:500.0000 rate:1.0000 gloss:4.6843 dlossA:3.7229 dlossQ:1.1115 exploreP:0.0206\n",
      "Episode:239 meanR:405.9400 R:500.0000 rate:1.0000 gloss:4.8556 dlossA:3.9066 dlossQ:1.0985 exploreP:0.0201\n",
      "Episode:240 meanR:410.2500 R:500.0000 rate:1.0000 gloss:4.9085 dlossA:3.6755 dlossQ:1.1925 exploreP:0.0196\n",
      "Episode:241 meanR:414.8500 R:500.0000 rate:1.0000 gloss:5.0865 dlossA:3.8150 dlossQ:1.3273 exploreP:0.0191\n",
      "Episode:242 meanR:419.3800 R:500.0000 rate:1.0000 gloss:5.2599 dlossA:4.0215 dlossQ:1.2035 exploreP:0.0187\n",
      "Episode:243 meanR:423.9300 R:500.0000 rate:1.0000 gloss:5.4314 dlossA:3.9892 dlossQ:1.2832 exploreP:0.0182\n",
      "Episode:244 meanR:426.6700 R:500.0000 rate:1.0000 gloss:5.6090 dlossA:4.3504 dlossQ:1.4521 exploreP:0.0178\n",
      "Episode:245 meanR:430.8100 R:500.0000 rate:1.0000 gloss:5.7170 dlossA:4.3095 dlossQ:1.2613 exploreP:0.0174\n",
      "Episode:246 meanR:434.4100 R:500.0000 rate:1.0000 gloss:5.8811 dlossA:4.4498 dlossQ:1.3829 exploreP:0.0171\n",
      "Episode:247 meanR:437.1300 R:500.0000 rate:1.0000 gloss:5.9924 dlossA:4.2374 dlossQ:1.2817 exploreP:0.0167\n",
      "Episode:248 meanR:441.3800 R:500.0000 rate:1.0000 gloss:6.2907 dlossA:4.5336 dlossQ:1.3888 exploreP:0.0164\n",
      "Episode:249 meanR:445.2000 R:500.0000 rate:1.0000 gloss:6.5752 dlossA:4.9486 dlossQ:1.4971 exploreP:0.0161\n",
      "Episode:250 meanR:449.3400 R:500.0000 rate:1.0000 gloss:6.8114 dlossA:5.0379 dlossQ:1.6451 exploreP:0.0158\n",
      "Episode:251 meanR:451.4000 R:500.0000 rate:1.0000 gloss:7.0341 dlossA:5.1691 dlossQ:1.5718 exploreP:0.0155\n",
      "Episode:252 meanR:452.0800 R:500.0000 rate:1.0000 gloss:7.3494 dlossA:5.5208 dlossQ:1.6529 exploreP:0.0152\n",
      "Episode:253 meanR:454.5500 R:500.0000 rate:1.0000 gloss:7.4924 dlossA:5.0431 dlossQ:1.7907 exploreP:0.0150\n",
      "Episode:254 meanR:456.1800 R:500.0000 rate:1.0000 gloss:7.9868 dlossA:6.1221 dlossQ:1.9478 exploreP:0.0148\n",
      "Episode:255 meanR:459.8200 R:500.0000 rate:1.0000 gloss:8.1889 dlossA:5.8694 dlossQ:1.8215 exploreP:0.0145\n",
      "Episode:256 meanR:462.8300 R:500.0000 rate:1.0000 gloss:8.5416 dlossA:6.3683 dlossQ:2.2146 exploreP:0.0143\n",
      "Episode:257 meanR:462.8300 R:500.0000 rate:1.0000 gloss:8.8915 dlossA:6.7338 dlossQ:2.2013 exploreP:0.0141\n",
      "Episode:258 meanR:466.6500 R:500.0000 rate:1.0000 gloss:9.1326 dlossA:6.7290 dlossQ:2.4907 exploreP:0.0139\n",
      "Episode:259 meanR:470.6900 R:500.0000 rate:1.0000 gloss:9.4458 dlossA:6.9270 dlossQ:2.3023 exploreP:0.0137\n",
      "Episode:260 meanR:475.3400 R:500.0000 rate:1.0000 gloss:9.7010 dlossA:6.6440 dlossQ:2.9664 exploreP:0.0135\n",
      "Episode:261 meanR:479.1000 R:500.0000 rate:1.0000 gloss:9.9579 dlossA:7.2548 dlossQ:2.9965 exploreP:0.0133\n",
      "Episode:262 meanR:483.2100 R:500.0000 rate:1.0000 gloss:9.9704 dlossA:7.1517 dlossQ:3.4028 exploreP:0.0132\n",
      "Episode:263 meanR:485.6500 R:500.0000 rate:1.0000 gloss:10.2715 dlossA:7.4032 dlossQ:2.8325 exploreP:0.0130\n",
      "Episode:264 meanR:490.3700 R:500.0000 rate:1.0000 gloss:10.3187 dlossA:6.7177 dlossQ:2.8399 exploreP:0.0129\n",
      "Episode:265 meanR:490.3700 R:500.0000 rate:1.0000 gloss:10.8489 dlossA:7.3121 dlossQ:3.1795 exploreP:0.0127\n",
      "Episode:266 meanR:490.3700 R:500.0000 rate:1.0000 gloss:11.4006 dlossA:8.2921 dlossQ:3.4449 exploreP:0.0126\n",
      "Episode:267 meanR:490.3700 R:500.0000 rate:1.0000 gloss:11.3142 dlossA:7.2589 dlossQ:3.3361 exploreP:0.0125\n",
      "Episode:268 meanR:491.2400 R:500.0000 rate:1.0000 gloss:11.9440 dlossA:8.2094 dlossQ:3.6930 exploreP:0.0124\n",
      "Episode:269 meanR:493.5000 R:500.0000 rate:1.0000 gloss:12.1889 dlossA:7.8322 dlossQ:3.7161 exploreP:0.0122\n",
      "Episode:270 meanR:493.5000 R:500.0000 rate:1.0000 gloss:12.3106 dlossA:7.2216 dlossQ:4.7088 exploreP:0.0121\n",
      "Episode:271 meanR:495.2000 R:500.0000 rate:1.0000 gloss:12.7663 dlossA:7.9193 dlossQ:4.1039 exploreP:0.0120\n",
      "Episode:272 meanR:496.1800 R:500.0000 rate:1.0000 gloss:13.0097 dlossA:8.8802 dlossQ:5.1334 exploreP:0.0119\n",
      "Episode:273 meanR:497.2300 R:500.0000 rate:1.0000 gloss:13.2144 dlossA:8.7762 dlossQ:4.7410 exploreP:0.0118\n",
      "Episode:274 meanR:497.2300 R:500.0000 rate:1.0000 gloss:13.2490 dlossA:8.5557 dlossQ:5.6997 exploreP:0.0117\n",
      "Episode:275 meanR:497.2300 R:500.0000 rate:1.0000 gloss:13.4669 dlossA:8.5704 dlossQ:5.9549 exploreP:0.0117\n",
      "Episode:276 meanR:497.2300 R:500.0000 rate:1.0000 gloss:13.5531 dlossA:8.5640 dlossQ:5.3836 exploreP:0.0116\n",
      "Episode:277 meanR:497.2300 R:500.0000 rate:1.0000 gloss:14.3264 dlossA:9.7124 dlossQ:5.9820 exploreP:0.0115\n",
      "Episode:278 meanR:497.2300 R:500.0000 rate:1.0000 gloss:14.4429 dlossA:9.0979 dlossQ:5.7035 exploreP:0.0114\n",
      "Episode:279 meanR:497.2300 R:500.0000 rate:1.0000 gloss:14.6670 dlossA:9.0903 dlossQ:5.9171 exploreP:0.0114\n",
      "Episode:280 meanR:497.2300 R:500.0000 rate:1.0000 gloss:14.9903 dlossA:8.4888 dlossQ:6.0635 exploreP:0.0113\n",
      "Episode:281 meanR:497.2300 R:500.0000 rate:1.0000 gloss:15.8524 dlossA:9.9098 dlossQ:6.4091 exploreP:0.0112\n",
      "Episode:282 meanR:497.2300 R:500.0000 rate:1.0000 gloss:16.5047 dlossA:10.2630 dlossQ:6.9588 exploreP:0.0112\n",
      "Episode:283 meanR:497.2300 R:500.0000 rate:1.0000 gloss:16.8167 dlossA:10.3207 dlossQ:6.7568 exploreP:0.0111\n",
      "Episode:284 meanR:497.2300 R:500.0000 rate:1.0000 gloss:16.9494 dlossA:9.4821 dlossQ:8.7460 exploreP:0.0111\n",
      "Episode:285 meanR:497.2300 R:500.0000 rate:1.0000 gloss:17.5573 dlossA:10.0995 dlossQ:7.3253 exploreP:0.0110\n",
      "Episode:286 meanR:497.2300 R:500.0000 rate:1.0000 gloss:17.9539 dlossA:11.0365 dlossQ:8.1623 exploreP:0.0110\n",
      "Episode:287 meanR:497.2300 R:500.0000 rate:1.0000 gloss:18.2736 dlossA:10.5352 dlossQ:8.1577 exploreP:0.0109\n",
      "Episode:288 meanR:497.2300 R:500.0000 rate:1.0000 gloss:18.3187 dlossA:10.2876 dlossQ:8.6909 exploreP:0.0109\n",
      "Episode:289 meanR:497.2300 R:500.0000 rate:1.0000 gloss:18.6024 dlossA:10.7377 dlossQ:8.6171 exploreP:0.0108\n",
      "Episode:290 meanR:497.2300 R:500.0000 rate:1.0000 gloss:19.0791 dlossA:10.9595 dlossQ:8.9319 exploreP:0.0108\n",
      "Episode:291 meanR:497.2300 R:500.0000 rate:1.0000 gloss:19.3217 dlossA:10.6844 dlossQ:9.2755 exploreP:0.0107\n",
      "Episode:292 meanR:497.2300 R:500.0000 rate:1.0000 gloss:19.6581 dlossA:10.6525 dlossQ:10.5203 exploreP:0.0107\n",
      "Episode:293 meanR:497.2300 R:500.0000 rate:1.0000 gloss:19.1546 dlossA:9.4489 dlossQ:9.6800 exploreP:0.0107\n",
      "Episode:294 meanR:497.2300 R:500.0000 rate:1.0000 gloss:19.7039 dlossA:9.5040 dlossQ:10.0072 exploreP:0.0106\n",
      "Episode:295 meanR:498.3200 R:500.0000 rate:1.0000 gloss:20.5505 dlossA:10.5968 dlossQ:10.9125 exploreP:0.0106\n",
      "Episode:296 meanR:498.3200 R:500.0000 rate:1.0000 gloss:21.0787 dlossA:10.9046 dlossQ:13.0473 exploreP:0.0106\n",
      "Episode:297 meanR:499.0600 R:500.0000 rate:1.0000 gloss:20.6822 dlossA:10.5990 dlossQ:10.7602 exploreP:0.0106\n",
      "Episode:298 meanR:499.0600 R:500.0000 rate:1.0000 gloss:20.3570 dlossA:9.6810 dlossQ:11.7380 exploreP:0.0105\n",
      "Episode:299 meanR:499.7800 R:500.0000 rate:1.0000 gloss:20.3742 dlossA:9.3290 dlossQ:10.9090 exploreP:0.0105\n",
      "Episode:300 meanR:499.7800 R:500.0000 rate:1.0000 gloss:20.6425 dlossA:9.0032 dlossQ:10.5997 exploreP:0.0105\n",
      "Episode:301 meanR:499.7800 R:500.0000 rate:1.0000 gloss:21.7960 dlossA:10.6143 dlossQ:11.4606 exploreP:0.0105\n",
      "Episode:302 meanR:499.7800 R:500.0000 rate:1.0000 gloss:21.0015 dlossA:9.8007 dlossQ:16.2306 exploreP:0.0104\n",
      "Episode:303 meanR:499.7800 R:500.0000 rate:1.0000 gloss:19.9822 dlossA:9.0381 dlossQ:12.9812 exploreP:0.0104\n",
      "Episode:304 meanR:499.7800 R:500.0000 rate:1.0000 gloss:19.4073 dlossA:7.9517 dlossQ:12.4359 exploreP:0.0104\n",
      "Episode:305 meanR:499.7800 R:500.0000 rate:1.0000 gloss:19.7118 dlossA:8.7083 dlossQ:13.1686 exploreP:0.0104\n",
      "Episode:306 meanR:499.7800 R:500.0000 rate:1.0000 gloss:20.2475 dlossA:9.4178 dlossQ:13.4774 exploreP:0.0104\n",
      "Episode:307 meanR:499.7800 R:500.0000 rate:1.0000 gloss:20.0760 dlossA:8.6585 dlossQ:13.7362 exploreP:0.0103\n",
      "Episode:308 meanR:500.0000 R:500.0000 rate:1.0000 gloss:21.3559 dlossA:9.4138 dlossQ:13.4800 exploreP:0.0103\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-a0b0af09d9f3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-a0b0af09d9f3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    plt.ylabel('G losses')`\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
