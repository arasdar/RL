{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.91213468440549 -3.0444881174756406\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 6), (70, 4), (70,), (70, 4), (70,), (70,), (70,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:19.0000 R:19.0000 rate:0.0380 gloss:0.5889 dlossA:0.6665 dlossQ:0.8911 exploreP:0.9981\n",
      "Episode:1 meanR:18.0000 R:17.0000 rate:0.0340 gloss:0.5847 dlossA:0.6643 dlossQ:0.8937 exploreP:0.9964\n",
      "Episode:2 meanR:16.0000 R:12.0000 rate:0.0240 gloss:0.5954 dlossA:0.6641 dlossQ:0.8999 exploreP:0.9953\n",
      "Episode:3 meanR:20.0000 R:32.0000 rate:0.0640 gloss:0.5864 dlossA:0.6645 dlossQ:0.8963 exploreP:0.9921\n",
      "Episode:4 meanR:19.0000 R:15.0000 rate:0.0300 gloss:0.5752 dlossA:0.6645 dlossQ:0.8888 exploreP:0.9906\n",
      "Episode:5 meanR:19.5000 R:22.0000 rate:0.0440 gloss:0.5992 dlossA:0.6710 dlossQ:0.9079 exploreP:0.9885\n",
      "Episode:6 meanR:18.4286 R:12.0000 rate:0.0240 gloss:0.5814 dlossA:0.6677 dlossQ:0.8875 exploreP:0.9873\n",
      "Episode:7 meanR:19.2500 R:25.0000 rate:0.0500 gloss:0.5904 dlossA:0.6725 dlossQ:0.8993 exploreP:0.9849\n",
      "Episode:8 meanR:18.8889 R:16.0000 rate:0.0320 gloss:0.5946 dlossA:0.6686 dlossQ:0.8992 exploreP:0.9833\n",
      "Episode:9 meanR:18.5000 R:15.0000 rate:0.0300 gloss:0.5973 dlossA:0.6720 dlossQ:0.9031 exploreP:0.9819\n",
      "Episode:10 meanR:18.2727 R:16.0000 rate:0.0320 gloss:0.5680 dlossA:0.6625 dlossQ:0.8911 exploreP:0.9803\n",
      "Episode:11 meanR:18.0833 R:16.0000 rate:0.0320 gloss:0.5689 dlossA:0.6595 dlossQ:0.8929 exploreP:0.9787\n",
      "Episode:12 meanR:18.1538 R:19.0000 rate:0.0380 gloss:0.5842 dlossA:0.6682 dlossQ:0.9010 exploreP:0.9769\n",
      "Episode:13 meanR:17.7143 R:12.0000 rate:0.0240 gloss:0.5497 dlossA:0.6610 dlossQ:0.8899 exploreP:0.9757\n",
      "Episode:14 meanR:17.4667 R:14.0000 rate:0.0280 gloss:0.5520 dlossA:0.6568 dlossQ:0.8963 exploreP:0.9744\n",
      "Episode:15 meanR:17.3125 R:15.0000 rate:0.0300 gloss:0.5878 dlossA:0.6709 dlossQ:0.9070 exploreP:0.9730\n",
      "Episode:16 meanR:17.0588 R:13.0000 rate:0.0260 gloss:0.5694 dlossA:0.6620 dlossQ:0.8893 exploreP:0.9717\n",
      "Episode:17 meanR:17.0556 R:17.0000 rate:0.0340 gloss:0.5754 dlossA:0.6655 dlossQ:0.8989 exploreP:0.9701\n",
      "Episode:18 meanR:17.5789 R:27.0000 rate:0.0540 gloss:0.5713 dlossA:0.6666 dlossQ:0.8902 exploreP:0.9675\n",
      "Episode:19 meanR:17.8000 R:22.0000 rate:0.0440 gloss:0.5653 dlossA:0.6642 dlossQ:0.8871 exploreP:0.9654\n",
      "Episode:20 meanR:19.8095 R:60.0000 rate:0.1200 gloss:0.5734 dlossA:0.6651 dlossQ:0.8994 exploreP:0.9597\n",
      "Episode:21 meanR:19.9545 R:23.0000 rate:0.0460 gloss:0.5621 dlossA:0.6621 dlossQ:0.8924 exploreP:0.9575\n",
      "Episode:22 meanR:19.7826 R:16.0000 rate:0.0320 gloss:0.5582 dlossA:0.6616 dlossQ:0.8904 exploreP:0.9560\n",
      "Episode:23 meanR:20.3333 R:33.0000 rate:0.0660 gloss:0.5697 dlossA:0.6673 dlossQ:0.8905 exploreP:0.9528\n",
      "Episode:24 meanR:20.4800 R:24.0000 rate:0.0480 gloss:0.5762 dlossA:0.6694 dlossQ:0.8992 exploreP:0.9506\n",
      "Episode:25 meanR:20.6154 R:24.0000 rate:0.0480 gloss:0.5559 dlossA:0.6619 dlossQ:0.8916 exploreP:0.9483\n",
      "Episode:26 meanR:20.4074 R:15.0000 rate:0.0300 gloss:0.5606 dlossA:0.6642 dlossQ:0.8926 exploreP:0.9469\n",
      "Episode:27 meanR:20.4286 R:21.0000 rate:0.0420 gloss:0.5650 dlossA:0.6651 dlossQ:0.9228 exploreP:0.9450\n",
      "Episode:28 meanR:20.3448 R:18.0000 rate:0.0360 gloss:0.5647 dlossA:0.6647 dlossQ:0.9017 exploreP:0.9433\n",
      "Episode:29 meanR:20.2000 R:16.0000 rate:0.0320 gloss:0.5548 dlossA:0.6642 dlossQ:0.8918 exploreP:0.9418\n",
      "Episode:30 meanR:21.0000 R:45.0000 rate:0.0900 gloss:0.5643 dlossA:0.6631 dlossQ:0.9094 exploreP:0.9376\n",
      "Episode:31 meanR:20.8438 R:16.0000 rate:0.0320 gloss:0.5610 dlossA:0.6626 dlossQ:0.8922 exploreP:0.9361\n",
      "Episode:32 meanR:21.0909 R:29.0000 rate:0.0580 gloss:0.5716 dlossA:0.6658 dlossQ:0.8958 exploreP:0.9334\n",
      "Episode:33 meanR:20.9706 R:17.0000 rate:0.0340 gloss:0.5576 dlossA:0.6617 dlossQ:0.9105 exploreP:0.9319\n",
      "Episode:34 meanR:20.7429 R:13.0000 rate:0.0260 gloss:0.5520 dlossA:0.6617 dlossQ:0.8911 exploreP:0.9307\n",
      "Episode:35 meanR:21.1944 R:37.0000 rate:0.0740 gloss:0.5746 dlossA:0.6689 dlossQ:0.9024 exploreP:0.9273\n",
      "Episode:36 meanR:21.3514 R:27.0000 rate:0.0540 gloss:0.5618 dlossA:0.6621 dlossQ:0.9065 exploreP:0.9248\n",
      "Episode:37 meanR:21.5000 R:27.0000 rate:0.0540 gloss:0.5689 dlossA:0.6660 dlossQ:0.9056 exploreP:0.9223\n",
      "Episode:38 meanR:21.2564 R:12.0000 rate:0.0240 gloss:0.5773 dlossA:0.6686 dlossQ:0.9030 exploreP:0.9212\n",
      "Episode:39 meanR:21.8250 R:44.0000 rate:0.0880 gloss:0.5625 dlossA:0.6652 dlossQ:0.8999 exploreP:0.9172\n",
      "Episode:40 meanR:21.7561 R:19.0000 rate:0.0380 gloss:0.5696 dlossA:0.6692 dlossQ:0.9051 exploreP:0.9155\n",
      "Episode:41 meanR:22.0238 R:33.0000 rate:0.0660 gloss:0.5643 dlossA:0.6658 dlossQ:0.9001 exploreP:0.9125\n",
      "Episode:42 meanR:22.0233 R:22.0000 rate:0.0440 gloss:0.5623 dlossA:0.6644 dlossQ:0.8917 exploreP:0.9105\n",
      "Episode:43 meanR:22.3409 R:36.0000 rate:0.0720 gloss:0.5752 dlossA:0.6702 dlossQ:0.9031 exploreP:0.9073\n",
      "Episode:44 meanR:22.2000 R:16.0000 rate:0.0320 gloss:0.5626 dlossA:0.6678 dlossQ:0.9041 exploreP:0.9059\n",
      "Episode:45 meanR:21.9130 R:9.0000 rate:0.0180 gloss:0.5580 dlossA:0.6655 dlossQ:0.8941 exploreP:0.9051\n",
      "Episode:46 meanR:21.7234 R:13.0000 rate:0.0260 gloss:0.5434 dlossA:0.6613 dlossQ:0.8930 exploreP:0.9039\n",
      "Episode:47 meanR:21.6042 R:16.0000 rate:0.0320 gloss:0.5507 dlossA:0.6582 dlossQ:0.9003 exploreP:0.9025\n",
      "Episode:48 meanR:21.8367 R:33.0000 rate:0.0660 gloss:0.5589 dlossA:0.6643 dlossQ:0.9024 exploreP:0.8995\n",
      "Episode:49 meanR:22.4800 R:54.0000 rate:0.1080 gloss:0.5633 dlossA:0.6659 dlossQ:0.9000 exploreP:0.8947\n",
      "Episode:50 meanR:22.2745 R:12.0000 rate:0.0240 gloss:0.5592 dlossA:0.6658 dlossQ:0.9038 exploreP:0.8937\n",
      "Episode:51 meanR:22.3654 R:27.0000 rate:0.0540 gloss:0.5703 dlossA:0.6694 dlossQ:0.8994 exploreP:0.8913\n",
      "Episode:52 meanR:23.5849 R:87.0000 rate:0.1740 gloss:0.5639 dlossA:0.6670 dlossQ:0.9004 exploreP:0.8837\n",
      "Episode:53 meanR:23.4074 R:14.0000 rate:0.0280 gloss:0.5602 dlossA:0.6670 dlossQ:0.8993 exploreP:0.8824\n",
      "Episode:54 meanR:23.3818 R:22.0000 rate:0.0440 gloss:0.5683 dlossA:0.6670 dlossQ:0.9070 exploreP:0.8805\n",
      "Episode:55 meanR:23.4286 R:26.0000 rate:0.0520 gloss:0.5625 dlossA:0.6655 dlossQ:0.9029 exploreP:0.8783\n",
      "Episode:56 meanR:23.9298 R:52.0000 rate:0.1040 gloss:0.5560 dlossA:0.6656 dlossQ:0.8971 exploreP:0.8738\n",
      "Episode:57 meanR:23.8966 R:22.0000 rate:0.0440 gloss:0.5636 dlossA:0.6658 dlossQ:0.9066 exploreP:0.8719\n",
      "Episode:58 meanR:24.1695 R:40.0000 rate:0.0800 gloss:0.5699 dlossA:0.6684 dlossQ:0.9030 exploreP:0.8684\n",
      "Episode:59 meanR:24.1167 R:21.0000 rate:0.0420 gloss:0.5708 dlossA:0.6704 dlossQ:0.9050 exploreP:0.8666\n",
      "Episode:60 meanR:24.0656 R:21.0000 rate:0.0420 gloss:0.5539 dlossA:0.6637 dlossQ:0.8973 exploreP:0.8648\n",
      "Episode:61 meanR:23.9839 R:19.0000 rate:0.0380 gloss:0.5721 dlossA:0.6676 dlossQ:0.8966 exploreP:0.8632\n",
      "Episode:62 meanR:24.7460 R:72.0000 rate:0.1440 gloss:0.5617 dlossA:0.6674 dlossQ:0.8990 exploreP:0.8571\n",
      "Episode:63 meanR:25.3594 R:64.0000 rate:0.1280 gloss:0.5624 dlossA:0.6675 dlossQ:0.9011 exploreP:0.8517\n",
      "Episode:64 meanR:25.3231 R:23.0000 rate:0.0460 gloss:0.5584 dlossA:0.6645 dlossQ:0.9075 exploreP:0.8498\n",
      "Episode:65 meanR:26.1364 R:79.0000 rate:0.1580 gloss:0.5686 dlossA:0.6668 dlossQ:0.9152 exploreP:0.8431\n",
      "Episode:66 meanR:26.3582 R:41.0000 rate:0.0820 gloss:0.5627 dlossA:0.6668 dlossQ:0.9062 exploreP:0.8397\n",
      "Episode:67 meanR:26.1765 R:14.0000 rate:0.0280 gloss:0.5593 dlossA:0.6671 dlossQ:0.9036 exploreP:0.8386\n",
      "Episode:68 meanR:25.9855 R:13.0000 rate:0.0260 gloss:0.5617 dlossA:0.6640 dlossQ:0.9008 exploreP:0.8375\n",
      "Episode:69 meanR:26.1571 R:38.0000 rate:0.0760 gloss:0.5530 dlossA:0.6625 dlossQ:0.8933 exploreP:0.8344\n",
      "Episode:70 meanR:26.3099 R:37.0000 rate:0.0740 gloss:0.5635 dlossA:0.6688 dlossQ:0.9050 exploreP:0.8313\n",
      "Episode:71 meanR:26.6806 R:53.0000 rate:0.1060 gloss:0.5662 dlossA:0.6668 dlossQ:0.9051 exploreP:0.8270\n",
      "Episode:72 meanR:26.4658 R:11.0000 rate:0.0220 gloss:0.5764 dlossA:0.6745 dlossQ:0.9186 exploreP:0.8261\n",
      "Episode:73 meanR:26.2297 R:9.0000 rate:0.0180 gloss:0.5584 dlossA:0.6696 dlossQ:0.8951 exploreP:0.8253\n",
      "Episode:74 meanR:26.7067 R:62.0000 rate:0.1240 gloss:0.5576 dlossA:0.6632 dlossQ:0.9031 exploreP:0.8203\n",
      "Episode:75 meanR:26.9211 R:43.0000 rate:0.0860 gloss:0.5631 dlossA:0.6658 dlossQ:0.9064 exploreP:0.8168\n",
      "Episode:76 meanR:26.7273 R:12.0000 rate:0.0240 gloss:0.5929 dlossA:0.6613 dlossQ:0.9191 exploreP:0.8159\n",
      "Episode:77 meanR:26.6410 R:20.0000 rate:0.0400 gloss:0.5526 dlossA:0.6636 dlossQ:0.9018 exploreP:0.8142\n",
      "Episode:78 meanR:26.5316 R:18.0000 rate:0.0360 gloss:0.5635 dlossA:0.6671 dlossQ:0.9034 exploreP:0.8128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:26.4875 R:23.0000 rate:0.0460 gloss:0.5538 dlossA:0.6641 dlossQ:0.9031 exploreP:0.8110\n",
      "Episode:80 meanR:26.4074 R:20.0000 rate:0.0400 gloss:0.5549 dlossA:0.6654 dlossQ:0.8920 exploreP:0.8094\n",
      "Episode:81 meanR:26.4634 R:31.0000 rate:0.0620 gloss:0.5567 dlossA:0.6648 dlossQ:0.9000 exploreP:0.8069\n",
      "Episode:82 meanR:26.3373 R:16.0000 rate:0.0320 gloss:0.5466 dlossA:0.6604 dlossQ:0.8935 exploreP:0.8056\n",
      "Episode:83 meanR:26.4881 R:39.0000 rate:0.0780 gloss:0.5492 dlossA:0.6630 dlossQ:0.8921 exploreP:0.8025\n",
      "Episode:84 meanR:26.6941 R:44.0000 rate:0.0880 gloss:0.5627 dlossA:0.6685 dlossQ:0.9057 exploreP:0.7990\n",
      "Episode:85 meanR:27.0349 R:56.0000 rate:0.1120 gloss:0.5646 dlossA:0.6675 dlossQ:0.9106 exploreP:0.7946\n",
      "Episode:86 meanR:27.7931 R:93.0000 rate:0.1860 gloss:0.5572 dlossA:0.6638 dlossQ:0.9067 exploreP:0.7874\n",
      "Episode:87 meanR:27.9091 R:38.0000 rate:0.0760 gloss:0.5625 dlossA:0.6645 dlossQ:0.9031 exploreP:0.7844\n",
      "Episode:88 meanR:27.9663 R:33.0000 rate:0.0660 gloss:0.5579 dlossA:0.6657 dlossQ:0.9067 exploreP:0.7819\n",
      "Episode:89 meanR:28.2778 R:56.0000 rate:0.1120 gloss:0.5593 dlossA:0.6649 dlossQ:0.9075 exploreP:0.7776\n",
      "Episode:90 meanR:28.0659 R:9.0000 rate:0.0180 gloss:0.5511 dlossA:0.6644 dlossQ:0.9057 exploreP:0.7769\n",
      "Episode:91 meanR:27.9565 R:18.0000 rate:0.0360 gloss:0.5438 dlossA:0.6625 dlossQ:0.9000 exploreP:0.7755\n",
      "Episode:92 meanR:28.1828 R:49.0000 rate:0.0980 gloss:0.5636 dlossA:0.6662 dlossQ:0.9130 exploreP:0.7717\n",
      "Episode:93 meanR:28.5319 R:61.0000 rate:0.1220 gloss:0.5614 dlossA:0.6650 dlossQ:0.9143 exploreP:0.7671\n",
      "Episode:94 meanR:28.4105 R:17.0000 rate:0.0340 gloss:0.5582 dlossA:0.6643 dlossQ:0.9101 exploreP:0.7658\n",
      "Episode:95 meanR:28.6354 R:50.0000 rate:0.1000 gloss:0.5674 dlossA:0.6681 dlossQ:0.9150 exploreP:0.7621\n",
      "Episode:96 meanR:28.5361 R:19.0000 rate:0.0380 gloss:0.5597 dlossA:0.6650 dlossQ:0.9045 exploreP:0.7606\n",
      "Episode:97 meanR:29.3061 R:104.0000 rate:0.2080 gloss:0.5631 dlossA:0.6655 dlossQ:0.9136 exploreP:0.7529\n",
      "Episode:98 meanR:29.2323 R:22.0000 rate:0.0440 gloss:0.5672 dlossA:0.6692 dlossQ:0.9193 exploreP:0.7512\n",
      "Episode:99 meanR:29.7600 R:82.0000 rate:0.1640 gloss:0.5715 dlossA:0.6692 dlossQ:0.9165 exploreP:0.7452\n",
      "Episode:100 meanR:29.6900 R:12.0000 rate:0.0240 gloss:0.5684 dlossA:0.6670 dlossQ:0.9145 exploreP:0.7443\n",
      "Episode:101 meanR:30.1600 R:64.0000 rate:0.1280 gloss:0.5626 dlossA:0.6660 dlossQ:0.9162 exploreP:0.7396\n",
      "Episode:102 meanR:30.3100 R:27.0000 rate:0.0540 gloss:0.5724 dlossA:0.6713 dlossQ:0.9235 exploreP:0.7376\n",
      "Episode:103 meanR:30.4000 R:41.0000 rate:0.0820 gloss:0.5683 dlossA:0.6692 dlossQ:0.9185 exploreP:0.7347\n",
      "Episode:104 meanR:30.5700 R:32.0000 rate:0.0640 gloss:0.5688 dlossA:0.6687 dlossQ:0.9171 exploreP:0.7323\n",
      "Episode:105 meanR:30.5200 R:17.0000 rate:0.0340 gloss:0.5747 dlossA:0.6730 dlossQ:0.9217 exploreP:0.7311\n",
      "Episode:106 meanR:30.5400 R:14.0000 rate:0.0280 gloss:0.5651 dlossA:0.6667 dlossQ:0.9219 exploreP:0.7301\n",
      "Episode:107 meanR:30.4700 R:18.0000 rate:0.0360 gloss:0.5684 dlossA:0.6682 dlossQ:0.9188 exploreP:0.7288\n",
      "Episode:108 meanR:30.4700 R:16.0000 rate:0.0320 gloss:0.5795 dlossA:0.6739 dlossQ:0.9198 exploreP:0.7277\n",
      "Episode:109 meanR:30.6500 R:33.0000 rate:0.0660 gloss:0.5653 dlossA:0.6642 dlossQ:0.9165 exploreP:0.7253\n",
      "Episode:110 meanR:31.6800 R:119.0000 rate:0.2380 gloss:0.5668 dlossA:0.6674 dlossQ:0.9186 exploreP:0.7168\n",
      "Episode:111 meanR:31.6600 R:14.0000 rate:0.0280 gloss:0.5770 dlossA:0.6731 dlossQ:0.9205 exploreP:0.7159\n",
      "Episode:112 meanR:32.4000 R:93.0000 rate:0.1860 gloss:0.5819 dlossA:0.6681 dlossQ:0.9285 exploreP:0.7093\n",
      "Episode:113 meanR:32.3900 R:11.0000 rate:0.0220 gloss:0.5685 dlossA:0.6685 dlossQ:0.9134 exploreP:0.7085\n",
      "Episode:114 meanR:32.4400 R:19.0000 rate:0.0380 gloss:0.5593 dlossA:0.6667 dlossQ:0.9113 exploreP:0.7072\n",
      "Episode:115 meanR:32.4800 R:19.0000 rate:0.0380 gloss:0.5642 dlossA:0.6661 dlossQ:0.9171 exploreP:0.7059\n",
      "Episode:116 meanR:32.6300 R:28.0000 rate:0.0560 gloss:0.5636 dlossA:0.6675 dlossQ:0.9207 exploreP:0.7040\n",
      "Episode:117 meanR:33.5500 R:109.0000 rate:0.2180 gloss:0.5750 dlossA:0.6707 dlossQ:0.9248 exploreP:0.6964\n",
      "Episode:118 meanR:33.8100 R:53.0000 rate:0.1060 gloss:0.5760 dlossA:0.6713 dlossQ:0.9234 exploreP:0.6928\n",
      "Episode:119 meanR:34.0000 R:41.0000 rate:0.0820 gloss:0.5781 dlossA:0.6702 dlossQ:0.9277 exploreP:0.6900\n",
      "Episode:120 meanR:33.7100 R:31.0000 rate:0.0620 gloss:0.5709 dlossA:0.6695 dlossQ:0.9250 exploreP:0.6879\n",
      "Episode:121 meanR:34.3600 R:88.0000 rate:0.1760 gloss:0.5770 dlossA:0.6710 dlossQ:0.9275 exploreP:0.6820\n",
      "Episode:122 meanR:34.9800 R:78.0000 rate:0.1560 gloss:0.5723 dlossA:0.6684 dlossQ:0.9241 exploreP:0.6767\n",
      "Episode:123 meanR:35.0900 R:44.0000 rate:0.0880 gloss:0.5746 dlossA:0.6674 dlossQ:0.9263 exploreP:0.6738\n",
      "Episode:124 meanR:35.1800 R:33.0000 rate:0.0660 gloss:0.5844 dlossA:0.6743 dlossQ:0.9231 exploreP:0.6716\n",
      "Episode:125 meanR:35.9500 R:101.0000 rate:0.2020 gloss:0.5720 dlossA:0.6689 dlossQ:0.9260 exploreP:0.6650\n",
      "Episode:126 meanR:36.0700 R:27.0000 rate:0.0540 gloss:0.5701 dlossA:0.6680 dlossQ:0.9303 exploreP:0.6632\n",
      "Episode:127 meanR:36.7900 R:93.0000 rate:0.1860 gloss:0.5764 dlossA:0.6694 dlossQ:0.9307 exploreP:0.6572\n",
      "Episode:128 meanR:36.9900 R:38.0000 rate:0.0760 gloss:0.5642 dlossA:0.6679 dlossQ:0.9195 exploreP:0.6547\n",
      "Episode:129 meanR:37.0200 R:19.0000 rate:0.0380 gloss:0.5734 dlossA:0.6694 dlossQ:0.9283 exploreP:0.6535\n",
      "Episode:130 meanR:36.7800 R:21.0000 rate:0.0420 gloss:0.5780 dlossA:0.6725 dlossQ:0.9311 exploreP:0.6521\n",
      "Episode:131 meanR:36.8900 R:27.0000 rate:0.0540 gloss:0.5769 dlossA:0.6711 dlossQ:0.9292 exploreP:0.6504\n",
      "Episode:132 meanR:37.6000 R:100.0000 rate:0.2000 gloss:0.5817 dlossA:0.6713 dlossQ:0.9315 exploreP:0.6440\n",
      "Episode:133 meanR:38.2700 R:84.0000 rate:0.1680 gloss:0.5735 dlossA:0.6698 dlossQ:0.9410 exploreP:0.6387\n",
      "Episode:134 meanR:38.9300 R:79.0000 rate:0.1580 gloss:0.5773 dlossA:0.6688 dlossQ:0.9368 exploreP:0.6338\n",
      "Episode:135 meanR:38.9100 R:35.0000 rate:0.0700 gloss:0.5897 dlossA:0.6732 dlossQ:0.9381 exploreP:0.6316\n",
      "Episode:136 meanR:38.8000 R:16.0000 rate:0.0320 gloss:0.5733 dlossA:0.6720 dlossQ:0.9358 exploreP:0.6306\n",
      "Episode:137 meanR:38.8300 R:30.0000 rate:0.0600 gloss:0.5882 dlossA:0.6734 dlossQ:0.9358 exploreP:0.6288\n",
      "Episode:138 meanR:39.8100 R:110.0000 rate:0.2200 gloss:0.5847 dlossA:0.6725 dlossQ:0.9400 exploreP:0.6220\n",
      "Episode:139 meanR:40.0400 R:67.0000 rate:0.1340 gloss:0.5797 dlossA:0.6715 dlossQ:0.9386 exploreP:0.6179\n",
      "Episode:140 meanR:40.3200 R:47.0000 rate:0.0940 gloss:0.5882 dlossA:0.6754 dlossQ:0.9419 exploreP:0.6150\n",
      "Episode:141 meanR:41.0400 R:105.0000 rate:0.2100 gloss:0.5884 dlossA:0.6758 dlossQ:0.9453 exploreP:0.6087\n",
      "Episode:142 meanR:41.2000 R:38.0000 rate:0.0760 gloss:0.5857 dlossA:0.6751 dlossQ:0.9437 exploreP:0.6065\n",
      "Episode:143 meanR:41.7500 R:91.0000 rate:0.1820 gloss:0.5809 dlossA:0.6715 dlossQ:0.9389 exploreP:0.6011\n",
      "Episode:144 meanR:41.8500 R:26.0000 rate:0.0520 gloss:0.5940 dlossA:0.6774 dlossQ:0.9486 exploreP:0.5995\n",
      "Episode:145 meanR:42.7600 R:100.0000 rate:0.2000 gloss:0.5879 dlossA:0.6747 dlossQ:0.9454 exploreP:0.5937\n",
      "Episode:146 meanR:44.4400 R:181.0000 rate:0.3620 gloss:0.5893 dlossA:0.6756 dlossQ:0.9458 exploreP:0.5832\n",
      "Episode:147 meanR:45.0900 R:81.0000 rate:0.1620 gloss:0.5913 dlossA:0.6772 dlossQ:0.9476 exploreP:0.5786\n",
      "Episode:148 meanR:46.0600 R:130.0000 rate:0.2600 gloss:0.5928 dlossA:0.6754 dlossQ:0.9481 exploreP:0.5712\n",
      "Episode:149 meanR:46.6200 R:110.0000 rate:0.2200 gloss:0.5941 dlossA:0.6753 dlossQ:0.9486 exploreP:0.5651\n",
      "Episode:150 meanR:46.8100 R:31.0000 rate:0.0620 gloss:0.5981 dlossA:0.6791 dlossQ:0.9549 exploreP:0.5634\n",
      "Episode:151 meanR:47.7800 R:124.0000 rate:0.2480 gloss:0.6003 dlossA:0.6789 dlossQ:0.9545 exploreP:0.5565\n",
      "Episode:152 meanR:47.8600 R:95.0000 rate:0.1900 gloss:0.5892 dlossA:0.6740 dlossQ:0.9554 exploreP:0.5514\n",
      "Episode:153 meanR:48.7300 R:101.0000 rate:0.2020 gloss:0.5986 dlossA:0.6790 dlossQ:0.9538 exploreP:0.5459\n",
      "Episode:154 meanR:49.1400 R:63.0000 rate:0.1260 gloss:0.5901 dlossA:0.6742 dlossQ:0.9587 exploreP:0.5426\n",
      "Episode:155 meanR:50.4800 R:160.0000 rate:0.3200 gloss:0.5926 dlossA:0.6757 dlossQ:0.9563 exploreP:0.5341\n",
      "Episode:156 meanR:50.5800 R:62.0000 rate:0.1240 gloss:0.6047 dlossA:0.6787 dlossQ:0.9564 exploreP:0.5309\n",
      "Episode:157 meanR:51.1300 R:77.0000 rate:0.1540 gloss:0.6045 dlossA:0.6813 dlossQ:0.9594 exploreP:0.5269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:51.5800 R:85.0000 rate:0.1700 gloss:0.6045 dlossA:0.6796 dlossQ:0.9598 exploreP:0.5225\n",
      "Episode:159 meanR:51.6900 R:32.0000 rate:0.0640 gloss:0.5983 dlossA:0.6788 dlossQ:0.9887 exploreP:0.5209\n",
      "Episode:160 meanR:51.7800 R:30.0000 rate:0.0600 gloss:0.6088 dlossA:0.6795 dlossQ:0.9632 exploreP:0.5193\n",
      "Episode:161 meanR:52.9000 R:131.0000 rate:0.2620 gloss:0.6061 dlossA:0.6809 dlossQ:0.9631 exploreP:0.5127\n",
      "Episode:162 meanR:52.7200 R:54.0000 rate:0.1080 gloss:0.6021 dlossA:0.6804 dlossQ:0.9621 exploreP:0.5100\n",
      "Episode:163 meanR:52.2000 R:12.0000 rate:0.0240 gloss:0.5959 dlossA:0.6791 dlossQ:0.9578 exploreP:0.5094\n",
      "Episode:164 meanR:52.8200 R:85.0000 rate:0.1700 gloss:0.6040 dlossA:0.6803 dlossQ:0.9656 exploreP:0.5052\n",
      "Episode:165 meanR:52.6400 R:61.0000 rate:0.1220 gloss:0.6111 dlossA:0.6821 dlossQ:0.9668 exploreP:0.5022\n",
      "Episode:166 meanR:53.6300 R:140.0000 rate:0.2800 gloss:0.6083 dlossA:0.6801 dlossQ:0.9713 exploreP:0.4953\n",
      "Episode:167 meanR:55.0400 R:155.0000 rate:0.3100 gloss:0.6058 dlossA:0.6814 dlossQ:0.9677 exploreP:0.4879\n",
      "Episode:168 meanR:55.8300 R:92.0000 rate:0.1840 gloss:0.6038 dlossA:0.6789 dlossQ:0.9765 exploreP:0.4835\n",
      "Episode:169 meanR:56.4900 R:104.0000 rate:0.2080 gloss:0.6091 dlossA:0.6827 dlossQ:0.9673 exploreP:0.4786\n",
      "Episode:170 meanR:56.9200 R:80.0000 rate:0.1600 gloss:0.6031 dlossA:0.6816 dlossQ:0.9648 exploreP:0.4748\n",
      "Episode:171 meanR:56.6400 R:25.0000 rate:0.0500 gloss:0.6131 dlossA:0.6857 dlossQ:0.9726 exploreP:0.4737\n",
      "Episode:172 meanR:56.6500 R:12.0000 rate:0.0240 gloss:0.6132 dlossA:0.6861 dlossQ:0.9798 exploreP:0.4731\n",
      "Episode:173 meanR:56.7300 R:17.0000 rate:0.0340 gloss:0.6154 dlossA:0.6868 dlossQ:0.9553 exploreP:0.4723\n",
      "Episode:174 meanR:56.6600 R:55.0000 rate:0.1100 gloss:0.6093 dlossA:0.6844 dlossQ:0.9597 exploreP:0.4698\n",
      "Episode:175 meanR:57.7200 R:149.0000 rate:0.2980 gloss:0.6113 dlossA:0.6835 dlossQ:0.9750 exploreP:0.4630\n",
      "Episode:176 meanR:58.7500 R:115.0000 rate:0.2300 gloss:0.6116 dlossA:0.6845 dlossQ:0.9744 exploreP:0.4578\n",
      "Episode:177 meanR:59.4100 R:86.0000 rate:0.1720 gloss:0.6170 dlossA:0.6850 dlossQ:0.9779 exploreP:0.4540\n",
      "Episode:178 meanR:60.8100 R:158.0000 rate:0.3160 gloss:0.6127 dlossA:0.6841 dlossQ:0.9734 exploreP:0.4470\n",
      "Episode:179 meanR:61.5500 R:97.0000 rate:0.1940 gloss:0.6184 dlossA:0.6875 dlossQ:0.9757 exploreP:0.4428\n",
      "Episode:180 meanR:62.3800 R:103.0000 rate:0.2060 gloss:0.6115 dlossA:0.6833 dlossQ:0.9703 exploreP:0.4384\n",
      "Episode:181 meanR:62.7900 R:72.0000 rate:0.1440 gloss:0.6145 dlossA:0.6859 dlossQ:0.9748 exploreP:0.4353\n",
      "Episode:182 meanR:63.7400 R:111.0000 rate:0.2220 gloss:0.6095 dlossA:0.6833 dlossQ:0.9748 exploreP:0.4306\n",
      "Episode:183 meanR:64.1700 R:82.0000 rate:0.1640 gloss:0.6135 dlossA:0.6831 dlossQ:0.9869 exploreP:0.4272\n",
      "Episode:184 meanR:64.4600 R:73.0000 rate:0.1460 gloss:0.6179 dlossA:0.6848 dlossQ:0.9802 exploreP:0.4241\n",
      "Episode:185 meanR:64.8900 R:99.0000 rate:0.1980 gloss:0.6140 dlossA:0.6836 dlossQ:0.9772 exploreP:0.4201\n",
      "Episode:186 meanR:65.1800 R:122.0000 rate:0.2440 gloss:0.6189 dlossA:0.6864 dlossQ:0.9794 exploreP:0.4151\n",
      "Episode:187 meanR:65.7700 R:97.0000 rate:0.1940 gloss:0.6169 dlossA:0.6853 dlossQ:0.9783 exploreP:0.4112\n",
      "Episode:188 meanR:67.1000 R:166.0000 rate:0.3320 gloss:0.6149 dlossA:0.6835 dlossQ:0.9829 exploreP:0.4046\n",
      "Episode:189 meanR:67.8300 R:129.0000 rate:0.2580 gloss:0.6133 dlossA:0.6832 dlossQ:0.9887 exploreP:0.3995\n",
      "Episode:190 meanR:68.9400 R:120.0000 rate:0.2400 gloss:0.6251 dlossA:0.6882 dlossQ:0.9861 exploreP:0.3949\n",
      "Episode:191 meanR:69.8000 R:104.0000 rate:0.2080 gloss:0.6225 dlossA:0.6873 dlossQ:0.9773 exploreP:0.3909\n",
      "Episode:192 meanR:70.9900 R:168.0000 rate:0.3360 gloss:0.6238 dlossA:0.6888 dlossQ:0.9857 exploreP:0.3845\n",
      "Episode:193 meanR:71.2000 R:82.0000 rate:0.1640 gloss:0.6320 dlossA:0.6938 dlossQ:0.9862 exploreP:0.3815\n",
      "Episode:194 meanR:72.0400 R:101.0000 rate:0.2020 gloss:0.6257 dlossA:0.6910 dlossQ:0.9843 exploreP:0.3778\n",
      "Episode:195 meanR:72.8800 R:134.0000 rate:0.2680 gloss:0.6250 dlossA:0.6898 dlossQ:0.9841 exploreP:0.3729\n",
      "Episode:196 meanR:73.8700 R:118.0000 rate:0.2360 gloss:0.6221 dlossA:0.6874 dlossQ:0.9846 exploreP:0.3686\n",
      "Episode:197 meanR:74.3700 R:154.0000 rate:0.3080 gloss:0.6195 dlossA:0.6863 dlossQ:0.9800 exploreP:0.3631\n",
      "Episode:198 meanR:75.3900 R:124.0000 rate:0.2480 gloss:0.6305 dlossA:0.6903 dlossQ:0.9858 exploreP:0.3588\n",
      "Episode:199 meanR:76.2400 R:167.0000 rate:0.3340 gloss:0.6241 dlossA:0.6896 dlossQ:0.9856 exploreP:0.3530\n",
      "Episode:200 meanR:77.1000 R:98.0000 rate:0.1960 gloss:0.6283 dlossA:0.6903 dlossQ:0.9880 exploreP:0.3496\n",
      "Episode:201 meanR:77.7200 R:126.0000 rate:0.2520 gloss:0.6292 dlossA:0.6907 dlossQ:0.9910 exploreP:0.3454\n",
      "Episode:202 meanR:78.6600 R:121.0000 rate:0.2420 gloss:0.6303 dlossA:0.6918 dlossQ:0.9920 exploreP:0.3414\n",
      "Episode:203 meanR:79.2700 R:102.0000 rate:0.2040 gloss:0.6406 dlossA:0.6938 dlossQ:1.0033 exploreP:0.3380\n",
      "Episode:204 meanR:79.8900 R:94.0000 rate:0.1880 gloss:0.6285 dlossA:0.6909 dlossQ:0.9895 exploreP:0.3349\n",
      "Episode:205 meanR:80.7800 R:106.0000 rate:0.2120 gloss:0.6295 dlossA:0.6909 dlossQ:0.9967 exploreP:0.3315\n",
      "Episode:206 meanR:81.4800 R:84.0000 rate:0.1680 gloss:0.6273 dlossA:0.6899 dlossQ:0.9914 exploreP:0.3288\n",
      "Episode:207 meanR:82.3200 R:102.0000 rate:0.2040 gloss:0.6343 dlossA:0.6938 dlossQ:0.9941 exploreP:0.3256\n",
      "Episode:208 meanR:83.5400 R:138.0000 rate:0.2760 gloss:0.6329 dlossA:0.6935 dlossQ:0.9950 exploreP:0.3213\n",
      "Episode:209 meanR:83.3900 R:18.0000 rate:0.0360 gloss:0.6244 dlossA:0.6935 dlossQ:0.9915 exploreP:0.3207\n",
      "Episode:210 meanR:83.3500 R:115.0000 rate:0.2300 gloss:0.6381 dlossA:0.6939 dlossQ:0.9941 exploreP:0.3171\n",
      "Episode:211 meanR:84.2200 R:101.0000 rate:0.2020 gloss:0.6394 dlossA:0.6947 dlossQ:0.9974 exploreP:0.3141\n",
      "Episode:212 meanR:85.4500 R:216.0000 rate:0.4320 gloss:0.6410 dlossA:0.6969 dlossQ:0.9995 exploreP:0.3076\n",
      "Episode:213 meanR:87.0200 R:168.0000 rate:0.3360 gloss:0.6372 dlossA:0.6945 dlossQ:1.0008 exploreP:0.3026\n",
      "Episode:214 meanR:88.9700 R:214.0000 rate:0.4280 gloss:0.6365 dlossA:0.6943 dlossQ:0.9941 exploreP:0.2964\n",
      "Episode:215 meanR:89.6800 R:90.0000 rate:0.1800 gloss:0.6433 dlossA:0.6958 dlossQ:0.9963 exploreP:0.2938\n",
      "Episode:216 meanR:90.2300 R:83.0000 rate:0.1660 gloss:0.6382 dlossA:0.6975 dlossQ:1.0013 exploreP:0.2915\n",
      "Episode:217 meanR:90.4400 R:130.0000 rate:0.2600 gloss:0.6451 dlossA:0.6964 dlossQ:0.9978 exploreP:0.2879\n",
      "Episode:218 meanR:91.1300 R:122.0000 rate:0.2440 gloss:0.6467 dlossA:0.6989 dlossQ:1.0037 exploreP:0.2845\n",
      "Episode:219 meanR:91.7800 R:106.0000 rate:0.2120 gloss:0.6496 dlossA:0.7013 dlossQ:1.0034 exploreP:0.2816\n",
      "Episode:220 meanR:93.2300 R:176.0000 rate:0.3520 gloss:0.6478 dlossA:0.6995 dlossQ:0.9997 exploreP:0.2769\n",
      "Episode:221 meanR:93.2400 R:89.0000 rate:0.1780 gloss:0.6463 dlossA:0.7001 dlossQ:1.0004 exploreP:0.2745\n",
      "Episode:222 meanR:93.4200 R:96.0000 rate:0.1920 gloss:0.6455 dlossA:0.6985 dlossQ:0.9974 exploreP:0.2720\n",
      "Episode:223 meanR:94.1700 R:119.0000 rate:0.2380 gloss:0.6527 dlossA:0.6996 dlossQ:1.0031 exploreP:0.2689\n",
      "Episode:224 meanR:95.5700 R:173.0000 rate:0.3460 gloss:0.6553 dlossA:0.7015 dlossQ:1.0031 exploreP:0.2644\n",
      "Episode:225 meanR:96.1300 R:157.0000 rate:0.3140 gloss:0.6497 dlossA:0.7000 dlossQ:1.0037 exploreP:0.2605\n",
      "Episode:226 meanR:97.1600 R:130.0000 rate:0.2600 gloss:0.6535 dlossA:0.7022 dlossQ:1.0010 exploreP:0.2572\n",
      "Episode:227 meanR:96.9900 R:76.0000 rate:0.1520 gloss:0.6580 dlossA:0.7039 dlossQ:1.0054 exploreP:0.2554\n",
      "Episode:228 meanR:97.8300 R:122.0000 rate:0.2440 gloss:0.6559 dlossA:0.7033 dlossQ:1.0078 exploreP:0.2524\n",
      "Episode:229 meanR:99.1200 R:148.0000 rate:0.2960 gloss:0.6586 dlossA:0.7044 dlossQ:1.0077 exploreP:0.2488\n",
      "Episode:230 meanR:100.0300 R:112.0000 rate:0.2240 gloss:0.6545 dlossA:0.7016 dlossQ:1.0054 exploreP:0.2462\n",
      "Episode:231 meanR:101.0700 R:131.0000 rate:0.2620 gloss:0.6579 dlossA:0.7023 dlossQ:1.0131 exploreP:0.2431\n",
      "Episode:232 meanR:101.0700 R:100.0000 rate:0.2000 gloss:0.6608 dlossA:0.7050 dlossQ:1.0061 exploreP:0.2408\n",
      "Episode:233 meanR:101.4100 R:118.0000 rate:0.2360 gloss:0.6603 dlossA:0.7038 dlossQ:1.0083 exploreP:0.2381\n",
      "Episode:234 meanR:101.5500 R:93.0000 rate:0.1860 gloss:0.6552 dlossA:0.7015 dlossQ:1.0061 exploreP:0.2359\n",
      "Episode:235 meanR:102.2100 R:101.0000 rate:0.2020 gloss:0.6637 dlossA:0.7059 dlossQ:1.0120 exploreP:0.2337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:103.5100 R:146.0000 rate:0.2920 gloss:0.6667 dlossA:0.7061 dlossQ:1.0050 exploreP:0.2304\n",
      "Episode:237 meanR:104.4400 R:123.0000 rate:0.2460 gloss:0.6642 dlossA:0.7063 dlossQ:1.0113 exploreP:0.2277\n",
      "Episode:238 meanR:104.2400 R:90.0000 rate:0.1800 gloss:0.6663 dlossA:0.7080 dlossQ:1.0073 exploreP:0.2258\n",
      "Episode:239 meanR:104.6100 R:104.0000 rate:0.2080 gloss:0.6607 dlossA:0.7072 dlossQ:1.0119 exploreP:0.2236\n",
      "Episode:240 meanR:105.0000 R:86.0000 rate:0.1720 gloss:0.6716 dlossA:0.7093 dlossQ:1.0094 exploreP:0.2217\n",
      "Episode:241 meanR:105.4700 R:152.0000 rate:0.3040 gloss:0.6652 dlossA:0.7095 dlossQ:1.0081 exploreP:0.2185\n",
      "Episode:242 meanR:106.2800 R:119.0000 rate:0.2380 gloss:0.6667 dlossA:0.7091 dlossQ:1.0100 exploreP:0.2161\n",
      "Episode:243 meanR:106.3900 R:102.0000 rate:0.2040 gloss:0.6701 dlossA:0.7097 dlossQ:1.0107 exploreP:0.2140\n",
      "Episode:244 meanR:107.8400 R:171.0000 rate:0.3420 gloss:0.6745 dlossA:0.7131 dlossQ:1.0144 exploreP:0.2105\n",
      "Episode:245 meanR:107.9800 R:114.0000 rate:0.2280 gloss:0.6754 dlossA:0.7117 dlossQ:1.0259 exploreP:0.2082\n",
      "Episode:246 meanR:107.7000 R:153.0000 rate:0.3060 gloss:0.6736 dlossA:0.7106 dlossQ:1.0151 exploreP:0.2052\n",
      "Episode:247 meanR:107.8900 R:100.0000 rate:0.2000 gloss:0.6707 dlossA:0.7098 dlossQ:1.0151 exploreP:0.2033\n",
      "Episode:248 meanR:107.6800 R:109.0000 rate:0.2180 gloss:0.6766 dlossA:0.7145 dlossQ:1.0166 exploreP:0.2012\n",
      "Episode:249 meanR:107.5600 R:98.0000 rate:0.1960 gloss:0.6749 dlossA:0.7140 dlossQ:1.0060 exploreP:0.1993\n",
      "Episode:250 meanR:108.0700 R:82.0000 rate:0.1640 gloss:0.6744 dlossA:0.7134 dlossQ:1.0150 exploreP:0.1978\n",
      "Episode:251 meanR:108.7400 R:191.0000 rate:0.3820 gloss:0.6771 dlossA:0.7138 dlossQ:1.0179 exploreP:0.1942\n",
      "Episode:252 meanR:109.0900 R:130.0000 rate:0.2600 gloss:0.6827 dlossA:0.7159 dlossQ:1.0198 exploreP:0.1919\n",
      "Episode:253 meanR:109.1500 R:107.0000 rate:0.2140 gloss:0.6865 dlossA:0.7159 dlossQ:1.0262 exploreP:0.1899\n",
      "Episode:254 meanR:109.3500 R:83.0000 rate:0.1660 gloss:0.6829 dlossA:0.7161 dlossQ:1.0208 exploreP:0.1884\n",
      "Episode:255 meanR:108.9900 R:124.0000 rate:0.2480 gloss:0.6849 dlossA:0.7168 dlossQ:1.0203 exploreP:0.1862\n",
      "Episode:256 meanR:110.0200 R:165.0000 rate:0.3300 gloss:0.6838 dlossA:0.7163 dlossQ:1.0204 exploreP:0.1833\n",
      "Episode:257 meanR:110.4000 R:115.0000 rate:0.2300 gloss:0.6853 dlossA:0.7167 dlossQ:1.0243 exploreP:0.1814\n",
      "Episode:258 meanR:110.5000 R:95.0000 rate:0.1900 gloss:0.6810 dlossA:0.7174 dlossQ:1.0165 exploreP:0.1797\n",
      "Episode:259 meanR:111.6200 R:144.0000 rate:0.2880 gloss:0.6963 dlossA:0.7191 dlossQ:1.0397 exploreP:0.1773\n",
      "Episode:260 meanR:112.3200 R:100.0000 rate:0.2000 gloss:0.6876 dlossA:0.7197 dlossQ:1.0144 exploreP:0.1757\n",
      "Episode:261 meanR:112.7200 R:171.0000 rate:0.3420 gloss:0.6998 dlossA:0.7223 dlossQ:1.0275 exploreP:0.1728\n",
      "Episode:262 meanR:113.2000 R:102.0000 rate:0.2040 gloss:0.6860 dlossA:0.7191 dlossQ:1.0226 exploreP:0.1712\n",
      "Episode:263 meanR:114.2300 R:115.0000 rate:0.2300 gloss:0.6961 dlossA:0.7177 dlossQ:1.0333 exploreP:0.1694\n",
      "Episode:264 meanR:114.2300 R:85.0000 rate:0.1700 gloss:0.6974 dlossA:0.7227 dlossQ:1.0181 exploreP:0.1680\n",
      "Episode:265 meanR:116.1800 R:256.0000 rate:0.5120 gloss:0.6882 dlossA:0.7187 dlossQ:1.0148 exploreP:0.1640\n",
      "Episode:266 meanR:116.5600 R:178.0000 rate:0.3560 gloss:0.6884 dlossA:0.7196 dlossQ:1.0152 exploreP:0.1613\n",
      "Episode:267 meanR:116.5600 R:155.0000 rate:0.3100 gloss:0.6906 dlossA:0.7214 dlossQ:1.0189 exploreP:0.1590\n",
      "Episode:268 meanR:116.8600 R:122.0000 rate:0.2440 gloss:0.6965 dlossA:0.7231 dlossQ:1.0234 exploreP:0.1572\n",
      "Episode:269 meanR:117.6400 R:182.0000 rate:0.3640 gloss:0.6984 dlossA:0.7244 dlossQ:1.0217 exploreP:0.1545\n",
      "Episode:270 meanR:119.2000 R:236.0000 rate:0.4720 gloss:0.7001 dlossA:0.7248 dlossQ:1.0182 exploreP:0.1511\n",
      "Episode:271 meanR:120.6100 R:166.0000 rate:0.3320 gloss:0.7018 dlossA:0.7242 dlossQ:1.0171 exploreP:0.1488\n",
      "Episode:272 meanR:121.7100 R:122.0000 rate:0.2440 gloss:0.7002 dlossA:0.7252 dlossQ:1.0207 exploreP:0.1471\n",
      "Episode:273 meanR:124.1200 R:258.0000 rate:0.5160 gloss:0.7059 dlossA:0.7294 dlossQ:1.0264 exploreP:0.1436\n",
      "Episode:274 meanR:125.4100 R:184.0000 rate:0.3680 gloss:0.7159 dlossA:0.7325 dlossQ:1.0270 exploreP:0.1412\n",
      "Episode:275 meanR:124.8000 R:88.0000 rate:0.1760 gloss:0.7135 dlossA:0.7321 dlossQ:1.0242 exploreP:0.1400\n",
      "Episode:276 meanR:125.2100 R:156.0000 rate:0.3120 gloss:0.7179 dlossA:0.7337 dlossQ:1.0239 exploreP:0.1380\n",
      "Episode:277 meanR:125.6000 R:125.0000 rate:0.2500 gloss:0.7038 dlossA:0.7283 dlossQ:1.0081 exploreP:0.1364\n",
      "Episode:278 meanR:124.9200 R:90.0000 rate:0.1800 gloss:0.7220 dlossA:0.7318 dlossQ:1.0258 exploreP:0.1353\n",
      "Episode:279 meanR:126.9400 R:299.0000 rate:0.5980 gloss:0.7167 dlossA:0.7353 dlossQ:1.0174 exploreP:0.1316\n",
      "Episode:280 meanR:127.4900 R:158.0000 rate:0.3160 gloss:0.7205 dlossA:0.7357 dlossQ:1.0279 exploreP:0.1297\n",
      "Episode:281 meanR:128.2900 R:152.0000 rate:0.3040 gloss:0.7209 dlossA:0.7363 dlossQ:1.0202 exploreP:0.1279\n",
      "Episode:282 meanR:129.7700 R:259.0000 rate:0.5180 gloss:0.7213 dlossA:0.7359 dlossQ:1.0177 exploreP:0.1249\n",
      "Episode:283 meanR:130.5700 R:162.0000 rate:0.3240 gloss:0.7272 dlossA:0.7401 dlossQ:1.0094 exploreP:0.1230\n",
      "Episode:284 meanR:131.2900 R:145.0000 rate:0.2900 gloss:0.7310 dlossA:0.7412 dlossQ:1.0152 exploreP:0.1214\n",
      "Episode:285 meanR:131.2000 R:90.0000 rate:0.1800 gloss:0.7277 dlossA:0.7429 dlossQ:1.0100 exploreP:0.1204\n",
      "Episode:286 meanR:132.4100 R:243.0000 rate:0.4860 gloss:0.7341 dlossA:0.7436 dlossQ:1.0219 exploreP:0.1178\n",
      "Episode:287 meanR:132.5200 R:108.0000 rate:0.2160 gloss:0.7369 dlossA:0.7432 dlossQ:1.0211 exploreP:0.1166\n",
      "Episode:288 meanR:133.3500 R:249.0000 rate:0.4980 gloss:0.7381 dlossA:0.7456 dlossQ:1.0172 exploreP:0.1140\n",
      "Episode:289 meanR:133.6100 R:155.0000 rate:0.3100 gloss:0.7396 dlossA:0.7466 dlossQ:1.0076 exploreP:0.1124\n",
      "Episode:290 meanR:133.3700 R:96.0000 rate:0.1920 gloss:0.7442 dlossA:0.7490 dlossQ:1.0064 exploreP:0.1114\n",
      "Episode:291 meanR:133.5500 R:122.0000 rate:0.2440 gloss:0.7428 dlossA:0.7493 dlossQ:1.0097 exploreP:0.1102\n",
      "Episode:292 meanR:133.0600 R:119.0000 rate:0.2380 gloss:0.7451 dlossA:0.7498 dlossQ:1.0124 exploreP:0.1090\n",
      "Episode:293 meanR:133.6100 R:137.0000 rate:0.2740 gloss:0.7482 dlossA:0.7515 dlossQ:1.0189 exploreP:0.1077\n",
      "Episode:294 meanR:133.7700 R:117.0000 rate:0.2340 gloss:0.7447 dlossA:0.7494 dlossQ:1.0132 exploreP:0.1065\n",
      "Episode:295 meanR:133.6500 R:122.0000 rate:0.2440 gloss:0.7541 dlossA:0.7518 dlossQ:1.0218 exploreP:0.1053\n",
      "Episode:296 meanR:134.4200 R:195.0000 rate:0.3900 gloss:0.7571 dlossA:0.7538 dlossQ:1.0167 exploreP:0.1035\n",
      "Episode:297 meanR:135.1900 R:231.0000 rate:0.4620 gloss:0.7584 dlossA:0.7563 dlossQ:1.0161 exploreP:0.1014\n",
      "Episode:298 meanR:136.0400 R:209.0000 rate:0.4180 gloss:0.7615 dlossA:0.7626 dlossQ:1.0104 exploreP:0.0995\n",
      "Episode:299 meanR:135.2800 R:91.0000 rate:0.1820 gloss:0.7634 dlossA:0.7538 dlossQ:1.0288 exploreP:0.0987\n",
      "Episode:300 meanR:137.4800 R:318.0000 rate:0.6360 gloss:0.7636 dlossA:0.7603 dlossQ:1.0106 exploreP:0.0959\n",
      "Episode:301 meanR:137.6100 R:139.0000 rate:0.2780 gloss:0.7685 dlossA:0.7633 dlossQ:1.0099 exploreP:0.0947\n",
      "Episode:302 meanR:137.4900 R:109.0000 rate:0.2180 gloss:0.7715 dlossA:0.7612 dlossQ:1.0141 exploreP:0.0938\n",
      "Episode:303 meanR:138.0400 R:157.0000 rate:0.3140 gloss:0.7714 dlossA:0.7678 dlossQ:1.0081 exploreP:0.0925\n",
      "Episode:304 meanR:138.8000 R:170.0000 rate:0.3400 gloss:0.7785 dlossA:0.7656 dlossQ:1.0086 exploreP:0.0911\n",
      "Episode:305 meanR:138.8500 R:111.0000 rate:0.2220 gloss:0.7807 dlossA:0.7719 dlossQ:1.0142 exploreP:0.0902\n",
      "Episode:306 meanR:139.7400 R:173.0000 rate:0.3460 gloss:0.7749 dlossA:0.7664 dlossQ:1.0136 exploreP:0.0888\n",
      "Episode:307 meanR:140.2700 R:155.0000 rate:0.3100 gloss:0.7893 dlossA:0.7700 dlossQ:1.0156 exploreP:0.0876\n",
      "Episode:308 meanR:140.2600 R:137.0000 rate:0.2740 gloss:0.7854 dlossA:0.7718 dlossQ:1.0109 exploreP:0.0866\n",
      "Episode:309 meanR:142.8200 R:274.0000 rate:0.5480 gloss:0.7900 dlossA:0.7722 dlossQ:1.0154 exploreP:0.0845\n",
      "Episode:310 meanR:144.9600 R:329.0000 rate:0.6580 gloss:0.8000 dlossA:0.7800 dlossQ:1.0143 exploreP:0.0821\n",
      "Episode:311 meanR:147.5500 R:360.0000 rate:0.7200 gloss:0.8073 dlossA:0.7831 dlossQ:1.0118 exploreP:0.0795\n",
      "Episode:312 meanR:147.3000 R:191.0000 rate:0.3820 gloss:0.8099 dlossA:0.7860 dlossQ:1.0088 exploreP:0.0782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:313 meanR:146.7600 R:114.0000 rate:0.2280 gloss:0.8159 dlossA:0.7897 dlossQ:1.0170 exploreP:0.0774\n",
      "Episode:314 meanR:145.8400 R:122.0000 rate:0.2440 gloss:0.8127 dlossA:0.7879 dlossQ:0.9992 exploreP:0.0766\n",
      "Episode:315 meanR:147.4400 R:250.0000 rate:0.5000 gloss:0.8172 dlossA:0.7869 dlossQ:1.0185 exploreP:0.0750\n",
      "Episode:316 meanR:148.9500 R:234.0000 rate:0.4680 gloss:0.8264 dlossA:0.7937 dlossQ:1.0133 exploreP:0.0735\n",
      "Episode:317 meanR:148.8600 R:121.0000 rate:0.2420 gloss:0.8300 dlossA:0.7938 dlossQ:1.0152 exploreP:0.0727\n",
      "Episode:318 meanR:148.6600 R:102.0000 rate:0.2040 gloss:0.8301 dlossA:0.7963 dlossQ:1.0132 exploreP:0.0721\n",
      "Episode:319 meanR:148.4800 R:88.0000 rate:0.1760 gloss:0.8303 dlossA:0.7993 dlossQ:1.0168 exploreP:0.0715\n",
      "Episode:320 meanR:148.1200 R:140.0000 rate:0.2800 gloss:0.8241 dlossA:0.7965 dlossQ:1.0102 exploreP:0.0707\n",
      "Episode:321 meanR:149.6400 R:241.0000 rate:0.4820 gloss:0.8248 dlossA:0.7956 dlossQ:1.0154 exploreP:0.0692\n",
      "Episode:322 meanR:150.8500 R:217.0000 rate:0.4340 gloss:0.8299 dlossA:0.7972 dlossQ:1.0029 exploreP:0.0680\n",
      "Episode:323 meanR:150.8300 R:117.0000 rate:0.2340 gloss:0.8359 dlossA:0.7952 dlossQ:1.0120 exploreP:0.0673\n",
      "Episode:324 meanR:150.3100 R:121.0000 rate:0.2420 gloss:0.8497 dlossA:0.8090 dlossQ:1.0138 exploreP:0.0666\n",
      "Episode:325 meanR:150.2000 R:146.0000 rate:0.2920 gloss:0.8404 dlossA:0.8004 dlossQ:1.0117 exploreP:0.0658\n",
      "Episode:326 meanR:150.0400 R:114.0000 rate:0.2280 gloss:0.8359 dlossA:0.7997 dlossQ:1.0048 exploreP:0.0651\n",
      "Episode:327 meanR:151.3100 R:203.0000 rate:0.4060 gloss:0.8552 dlossA:0.8157 dlossQ:1.0129 exploreP:0.0640\n",
      "Episode:328 meanR:152.4900 R:240.0000 rate:0.4800 gloss:0.8510 dlossA:0.8077 dlossQ:1.0140 exploreP:0.0628\n",
      "Episode:329 meanR:152.2800 R:127.0000 rate:0.2540 gloss:0.8522 dlossA:0.8079 dlossQ:1.0084 exploreP:0.0621\n",
      "Episode:330 meanR:154.2100 R:305.0000 rate:0.6100 gloss:0.8577 dlossA:0.8178 dlossQ:1.0005 exploreP:0.0605\n",
      "Episode:331 meanR:155.2300 R:233.0000 rate:0.4660 gloss:0.8655 dlossA:0.8196 dlossQ:1.0064 exploreP:0.0594\n",
      "Episode:332 meanR:157.5400 R:331.0000 rate:0.6620 gloss:0.8682 dlossA:0.8196 dlossQ:1.0104 exploreP:0.0578\n",
      "Episode:333 meanR:159.1000 R:274.0000 rate:0.5480 gloss:0.8785 dlossA:0.8249 dlossQ:1.0058 exploreP:0.0565\n",
      "Episode:334 meanR:161.1100 R:294.0000 rate:0.5880 gloss:0.8826 dlossA:0.8305 dlossQ:1.0027 exploreP:0.0551\n",
      "Episode:335 meanR:161.4600 R:136.0000 rate:0.2720 gloss:0.8949 dlossA:0.8346 dlossQ:1.0037 exploreP:0.0545\n",
      "Episode:336 meanR:162.6400 R:264.0000 rate:0.5280 gloss:0.8925 dlossA:0.8291 dlossQ:1.0131 exploreP:0.0533\n",
      "Episode:337 meanR:162.8700 R:146.0000 rate:0.2920 gloss:0.8950 dlossA:0.8262 dlossQ:1.0074 exploreP:0.0527\n",
      "Episode:338 meanR:163.4300 R:146.0000 rate:0.2920 gloss:0.9014 dlossA:0.8451 dlossQ:1.0049 exploreP:0.0521\n",
      "Episode:339 meanR:165.7300 R:334.0000 rate:0.6680 gloss:0.9098 dlossA:0.8478 dlossQ:1.0023 exploreP:0.0507\n",
      "Episode:340 meanR:166.2700 R:140.0000 rate:0.2800 gloss:0.9129 dlossA:0.8447 dlossQ:1.0031 exploreP:0.0501\n",
      "Episode:341 meanR:166.1300 R:138.0000 rate:0.2760 gloss:0.9161 dlossA:0.8475 dlossQ:1.0148 exploreP:0.0496\n",
      "Episode:342 meanR:166.2600 R:132.0000 rate:0.2640 gloss:0.9163 dlossA:0.8448 dlossQ:1.0077 exploreP:0.0491\n",
      "Episode:343 meanR:170.2400 R:500.0000 rate:1.0000 gloss:0.9251 dlossA:0.8535 dlossQ:1.0007 exploreP:0.0472\n",
      "Episode:344 meanR:170.8100 R:228.0000 rate:0.4560 gloss:0.9308 dlossA:0.8523 dlossQ:1.0047 exploreP:0.0463\n",
      "Episode:345 meanR:171.8800 R:221.0000 rate:0.4420 gloss:0.9370 dlossA:0.8581 dlossQ:1.0139 exploreP:0.0455\n",
      "Episode:346 meanR:171.7900 R:144.0000 rate:0.2880 gloss:0.9696 dlossA:0.8768 dlossQ:1.0026 exploreP:0.0450\n",
      "Episode:347 meanR:172.1100 R:132.0000 rate:0.2640 gloss:0.9609 dlossA:0.8698 dlossQ:1.0003 exploreP:0.0446\n",
      "Episode:348 meanR:173.7800 R:276.0000 rate:0.5520 gloss:0.9535 dlossA:0.8626 dlossQ:1.0048 exploreP:0.0436\n",
      "Episode:349 meanR:176.7600 R:396.0000 rate:0.7920 gloss:0.9681 dlossA:0.8762 dlossQ:1.0106 exploreP:0.0423\n",
      "Episode:350 meanR:177.8100 R:187.0000 rate:0.3740 gloss:0.9791 dlossA:0.8758 dlossQ:1.0057 exploreP:0.0417\n",
      "Episode:351 meanR:177.5500 R:165.0000 rate:0.3300 gloss:0.9951 dlossA:0.8966 dlossQ:1.0138 exploreP:0.0412\n",
      "Episode:352 meanR:178.3600 R:211.0000 rate:0.4220 gloss:0.9929 dlossA:0.8946 dlossQ:1.0004 exploreP:0.0406\n",
      "Episode:353 meanR:178.5300 R:124.0000 rate:0.2480 gloss:1.0062 dlossA:0.9012 dlossQ:1.0142 exploreP:0.0402\n",
      "Episode:354 meanR:181.1100 R:341.0000 rate:0.6820 gloss:1.0039 dlossA:0.8939 dlossQ:1.0163 exploreP:0.0392\n",
      "Episode:355 meanR:182.1700 R:230.0000 rate:0.4600 gloss:1.0143 dlossA:0.9007 dlossQ:1.0082 exploreP:0.0385\n",
      "Episode:356 meanR:183.7500 R:323.0000 rate:0.6460 gloss:1.0203 dlossA:0.9103 dlossQ:0.9989 exploreP:0.0376\n",
      "Episode:357 meanR:184.3000 R:170.0000 rate:0.3400 gloss:1.0227 dlossA:0.9114 dlossQ:1.0054 exploreP:0.0371\n",
      "Episode:358 meanR:185.1900 R:184.0000 rate:0.3680 gloss:1.0413 dlossA:0.9121 dlossQ:1.0307 exploreP:0.0366\n",
      "Episode:359 meanR:185.9800 R:223.0000 rate:0.4460 gloss:1.0562 dlossA:0.9196 dlossQ:1.0169 exploreP:0.0361\n",
      "Episode:360 meanR:189.9800 R:500.0000 rate:1.0000 gloss:1.0589 dlossA:0.9311 dlossQ:1.0122 exploreP:0.0348\n",
      "Episode:361 meanR:191.3600 R:309.0000 rate:0.6180 gloss:1.0862 dlossA:0.9433 dlossQ:1.0138 exploreP:0.0340\n",
      "Episode:362 meanR:192.0100 R:167.0000 rate:0.3340 gloss:1.0811 dlossA:0.9293 dlossQ:1.0208 exploreP:0.0336\n",
      "Episode:363 meanR:195.8600 R:500.0000 rate:1.0000 gloss:1.1106 dlossA:0.9588 dlossQ:1.0156 exploreP:0.0325\n",
      "Episode:364 meanR:196.6100 R:160.0000 rate:0.3200 gloss:1.1073 dlossA:0.9591 dlossQ:1.0173 exploreP:0.0321\n",
      "Episode:365 meanR:195.6400 R:159.0000 rate:0.3180 gloss:1.1332 dlossA:0.9844 dlossQ:1.0229 exploreP:0.0318\n",
      "Episode:366 meanR:196.5700 R:271.0000 rate:0.5420 gloss:1.1368 dlossA:0.9554 dlossQ:1.0165 exploreP:0.0312\n",
      "Episode:367 meanR:196.3200 R:130.0000 rate:0.2600 gloss:1.1520 dlossA:0.9555 dlossQ:1.0244 exploreP:0.0309\n",
      "Episode:368 meanR:200.0600 R:496.0000 rate:0.9920 gloss:1.1699 dlossA:0.9984 dlossQ:1.0164 exploreP:0.0299\n",
      "Episode:369 meanR:202.3000 R:406.0000 rate:0.8120 gloss:1.1963 dlossA:1.0008 dlossQ:1.0294 exploreP:0.0291\n",
      "Episode:370 meanR:201.2300 R:129.0000 rate:0.2580 gloss:1.1962 dlossA:0.9910 dlossQ:1.0152 exploreP:0.0289\n",
      "Episode:371 meanR:202.5500 R:298.0000 rate:0.5960 gloss:1.2258 dlossA:1.0187 dlossQ:1.0290 exploreP:0.0283\n",
      "Episode:372 meanR:202.8300 R:150.0000 rate:0.3000 gloss:1.2143 dlossA:0.9922 dlossQ:1.0216 exploreP:0.0280\n",
      "Episode:373 meanR:201.4900 R:124.0000 rate:0.2480 gloss:1.2616 dlossA:1.0565 dlossQ:1.0488 exploreP:0.0278\n",
      "Episode:374 meanR:200.9100 R:126.0000 rate:0.2520 gloss:1.2640 dlossA:1.0778 dlossQ:1.0284 exploreP:0.0276\n",
      "Episode:375 meanR:202.3500 R:232.0000 rate:0.4640 gloss:1.2829 dlossA:1.0742 dlossQ:1.0289 exploreP:0.0272\n",
      "Episode:376 meanR:203.0900 R:230.0000 rate:0.4600 gloss:1.2741 dlossA:1.0760 dlossQ:1.0172 exploreP:0.0268\n",
      "Episode:377 meanR:204.9500 R:311.0000 rate:0.6220 gloss:1.2731 dlossA:1.0558 dlossQ:1.0206 exploreP:0.0263\n",
      "Episode:378 meanR:205.1900 R:114.0000 rate:0.2280 gloss:1.2876 dlossA:1.0698 dlossQ:1.0367 exploreP:0.0261\n",
      "Episode:379 meanR:205.2200 R:302.0000 rate:0.6040 gloss:1.3129 dlossA:1.0962 dlossQ:1.0404 exploreP:0.0256\n",
      "Episode:380 meanR:205.9400 R:230.0000 rate:0.4600 gloss:1.3077 dlossA:1.0704 dlossQ:1.0204 exploreP:0.0253\n",
      "Episode:381 meanR:205.6600 R:124.0000 rate:0.2480 gloss:1.3421 dlossA:1.1080 dlossQ:1.0365 exploreP:0.0251\n",
      "Episode:382 meanR:205.7400 R:267.0000 rate:0.5340 gloss:1.3412 dlossA:1.1294 dlossQ:1.0481 exploreP:0.0247\n",
      "Episode:383 meanR:207.1000 R:298.0000 rate:0.5960 gloss:1.3499 dlossA:1.1043 dlossQ:1.0338 exploreP:0.0243\n",
      "Episode:384 meanR:207.8300 R:218.0000 rate:0.4360 gloss:1.3913 dlossA:1.1624 dlossQ:1.0351 exploreP:0.0239\n",
      "Episode:385 meanR:207.9900 R:106.0000 rate:0.2120 gloss:1.3987 dlossA:1.1804 dlossQ:1.0143 exploreP:0.0238\n",
      "Episode:386 meanR:209.3900 R:383.0000 rate:0.7660 gloss:1.3737 dlossA:1.1497 dlossQ:1.0275 exploreP:0.0233\n",
      "Episode:387 meanR:211.2300 R:292.0000 rate:0.5840 gloss:1.3940 dlossA:1.1412 dlossQ:1.0402 exploreP:0.0229\n",
      "Episode:388 meanR:213.7400 R:500.0000 rate:1.0000 gloss:1.4110 dlossA:1.1673 dlossQ:1.0242 exploreP:0.0223\n",
      "Episode:389 meanR:213.8700 R:168.0000 rate:0.3360 gloss:1.4082 dlossA:1.1390 dlossQ:1.0445 exploreP:0.0221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:390 meanR:216.0700 R:316.0000 rate:0.6320 gloss:1.4335 dlossA:1.1419 dlossQ:1.0516 exploreP:0.0217\n",
      "Episode:391 meanR:217.8900 R:304.0000 rate:0.6080 gloss:1.4861 dlossA:1.1984 dlossQ:1.0562 exploreP:0.0213\n",
      "Episode:392 meanR:217.9900 R:129.0000 rate:0.2580 gloss:1.5012 dlossA:1.2372 dlossQ:1.0428 exploreP:0.0212\n",
      "Episode:393 meanR:219.5500 R:293.0000 rate:0.5860 gloss:1.4884 dlossA:1.2053 dlossQ:1.0536 exploreP:0.0209\n",
      "Episode:394 meanR:220.7400 R:236.0000 rate:0.4720 gloss:1.5306 dlossA:1.2537 dlossQ:1.0657 exploreP:0.0206\n",
      "Episode:395 meanR:222.2000 R:268.0000 rate:0.5360 gloss:1.5395 dlossA:1.2663 dlossQ:1.0618 exploreP:0.0203\n",
      "Episode:396 meanR:221.7900 R:154.0000 rate:0.3080 gloss:1.5441 dlossA:1.2625 dlossQ:1.0571 exploreP:0.0202\n",
      "Episode:397 meanR:221.6500 R:217.0000 rate:0.4340 gloss:1.5438 dlossA:1.2757 dlossQ:1.0620 exploreP:0.0200\n",
      "Episode:398 meanR:220.5200 R:96.0000 rate:0.1920 gloss:1.5781 dlossA:1.3259 dlossQ:1.0947 exploreP:0.0199\n",
      "Episode:399 meanR:221.1200 R:151.0000 rate:0.3020 gloss:1.5528 dlossA:1.2720 dlossQ:1.0765 exploreP:0.0197\n",
      "Episode:400 meanR:219.7200 R:178.0000 rate:0.3560 gloss:1.5795 dlossA:1.2771 dlossQ:1.0861 exploreP:0.0195\n",
      "Episode:401 meanR:221.3500 R:302.0000 rate:0.6040 gloss:1.5882 dlossA:1.3117 dlossQ:1.0804 exploreP:0.0193\n",
      "Episode:402 meanR:221.4900 R:123.0000 rate:0.2460 gloss:1.5776 dlossA:1.3279 dlossQ:1.0475 exploreP:0.0191\n",
      "Episode:403 meanR:223.7900 R:387.0000 rate:0.7740 gloss:1.5904 dlossA:1.2748 dlossQ:1.0965 exploreP:0.0188\n",
      "Episode:404 meanR:223.6800 R:159.0000 rate:0.3180 gloss:1.6510 dlossA:1.3463 dlossQ:1.0891 exploreP:0.0187\n",
      "Episode:405 meanR:225.7000 R:313.0000 rate:0.6260 gloss:1.6414 dlossA:1.2958 dlossQ:1.0859 exploreP:0.0184\n",
      "Episode:406 meanR:227.1300 R:316.0000 rate:0.6320 gloss:1.6588 dlossA:1.3535 dlossQ:1.1100 exploreP:0.0181\n",
      "Episode:407 meanR:229.1500 R:357.0000 rate:0.7140 gloss:1.6977 dlossA:1.3744 dlossQ:1.1244 exploreP:0.0178\n",
      "Episode:408 meanR:230.9700 R:319.0000 rate:0.6380 gloss:1.7201 dlossA:1.3831 dlossQ:1.1274 exploreP:0.0176\n",
      "Episode:409 meanR:231.7500 R:352.0000 rate:0.7040 gloss:1.7230 dlossA:1.4111 dlossQ:1.1221 exploreP:0.0173\n",
      "Episode:410 meanR:233.3000 R:484.0000 rate:0.9680 gloss:1.7511 dlossA:1.4004 dlossQ:1.1443 exploreP:0.0170\n",
      "Episode:411 meanR:231.8000 R:210.0000 rate:0.4200 gloss:1.8021 dlossA:1.4497 dlossQ:1.1506 exploreP:0.0168\n",
      "Episode:412 meanR:233.0700 R:318.0000 rate:0.6360 gloss:1.7724 dlossA:1.4295 dlossQ:1.1512 exploreP:0.0166\n",
      "Episode:413 meanR:234.3400 R:241.0000 rate:0.4820 gloss:1.8454 dlossA:1.4831 dlossQ:1.1905 exploreP:0.0165\n",
      "Episode:414 meanR:238.1200 R:500.0000 rate:1.0000 gloss:1.8457 dlossA:1.4497 dlossQ:1.1720 exploreP:0.0162\n",
      "Episode:415 meanR:238.3600 R:274.0000 rate:0.5480 gloss:1.9156 dlossA:1.5491 dlossQ:1.1987 exploreP:0.0160\n",
      "Episode:416 meanR:238.6700 R:265.0000 rate:0.5300 gloss:1.9094 dlossA:1.5203 dlossQ:1.2075 exploreP:0.0158\n",
      "Episode:417 meanR:238.8700 R:141.0000 rate:0.2820 gloss:1.9105 dlossA:1.4718 dlossQ:1.2504 exploreP:0.0158\n",
      "Episode:418 meanR:241.5300 R:368.0000 rate:0.7360 gloss:1.9446 dlossA:1.5741 dlossQ:1.2144 exploreP:0.0155\n",
      "Episode:419 meanR:242.1400 R:149.0000 rate:0.2980 gloss:1.9205 dlossA:1.6034 dlossQ:1.2023 exploreP:0.0155\n",
      "Episode:420 meanR:243.9300 R:319.0000 rate:0.6380 gloss:1.9774 dlossA:1.5629 dlossQ:1.2623 exploreP:0.0153\n",
      "Episode:421 meanR:245.7000 R:418.0000 rate:0.8360 gloss:2.0197 dlossA:1.6464 dlossQ:1.2536 exploreP:0.0151\n",
      "Episode:422 meanR:246.9400 R:341.0000 rate:0.6820 gloss:2.0036 dlossA:1.6032 dlossQ:1.2665 exploreP:0.0149\n",
      "Episode:423 meanR:247.2800 R:151.0000 rate:0.3020 gloss:2.0453 dlossA:1.6438 dlossQ:1.3285 exploreP:0.0148\n",
      "Episode:424 meanR:249.6500 R:358.0000 rate:0.7160 gloss:2.0680 dlossA:1.6429 dlossQ:1.2804 exploreP:0.0147\n",
      "Episode:425 meanR:252.0000 R:381.0000 rate:0.7620 gloss:2.1476 dlossA:1.7557 dlossQ:1.3047 exploreP:0.0145\n",
      "Episode:426 meanR:255.4800 R:462.0000 rate:0.9240 gloss:2.1618 dlossA:1.7779 dlossQ:1.3230 exploreP:0.0143\n",
      "Episode:427 meanR:258.4500 R:500.0000 rate:1.0000 gloss:2.1545 dlossA:1.6956 dlossQ:1.3328 exploreP:0.0141\n",
      "Episode:428 meanR:257.7100 R:166.0000 rate:0.3320 gloss:2.1704 dlossA:1.7126 dlossQ:1.4077 exploreP:0.0140\n",
      "Episode:429 meanR:261.4400 R:500.0000 rate:1.0000 gloss:2.2342 dlossA:1.7963 dlossQ:1.3721 exploreP:0.0138\n",
      "Episode:430 meanR:261.9400 R:355.0000 rate:0.7100 gloss:2.2816 dlossA:1.8677 dlossQ:1.3800 exploreP:0.0137\n",
      "Episode:431 meanR:262.7900 R:318.0000 rate:0.6360 gloss:2.3590 dlossA:1.9196 dlossQ:1.4544 exploreP:0.0136\n",
      "Episode:432 meanR:262.8100 R:333.0000 rate:0.6660 gloss:2.3459 dlossA:1.8519 dlossQ:1.4192 exploreP:0.0134\n",
      "Episode:433 meanR:263.8500 R:378.0000 rate:0.7560 gloss:2.4043 dlossA:1.9720 dlossQ:1.4331 exploreP:0.0133\n",
      "Episode:434 meanR:265.9100 R:500.0000 rate:1.0000 gloss:2.4534 dlossA:1.9975 dlossQ:1.4915 exploreP:0.0132\n",
      "Episode:435 meanR:269.5500 R:500.0000 rate:1.0000 gloss:2.4579 dlossA:1.9819 dlossQ:1.4834 exploreP:0.0130\n",
      "Episode:436 meanR:269.7600 R:285.0000 rate:0.5700 gloss:2.5210 dlossA:2.0672 dlossQ:1.5076 exploreP:0.0129\n",
      "Episode:437 meanR:271.3400 R:304.0000 rate:0.6080 gloss:2.5882 dlossA:2.0856 dlossQ:1.5091 exploreP:0.0128\n",
      "Episode:438 meanR:271.2600 R:138.0000 rate:0.2760 gloss:2.6002 dlossA:2.0078 dlossQ:1.5272 exploreP:0.0128\n",
      "Episode:439 meanR:270.0500 R:213.0000 rate:0.4260 gloss:2.6173 dlossA:2.1536 dlossQ:1.5546 exploreP:0.0127\n",
      "Episode:440 meanR:271.6100 R:296.0000 rate:0.5920 gloss:2.6108 dlossA:2.1140 dlossQ:1.6381 exploreP:0.0127\n",
      "Episode:441 meanR:274.6700 R:444.0000 rate:0.8880 gloss:2.6770 dlossA:2.1645 dlossQ:1.6131 exploreP:0.0125\n",
      "Episode:442 meanR:277.4000 R:405.0000 rate:0.8100 gloss:2.7681 dlossA:2.2444 dlossQ:1.6479 exploreP:0.0124\n",
      "Episode:443 meanR:275.9400 R:354.0000 rate:0.7080 gloss:2.7671 dlossA:2.2428 dlossQ:1.6608 exploreP:0.0124\n",
      "Episode:444 meanR:278.6600 R:500.0000 rate:1.0000 gloss:2.8692 dlossA:2.3729 dlossQ:1.6762 exploreP:0.0122\n",
      "Episode:445 meanR:280.5200 R:407.0000 rate:0.8140 gloss:2.8519 dlossA:2.2891 dlossQ:1.7556 exploreP:0.0122\n",
      "Episode:446 meanR:280.9500 R:187.0000 rate:0.3740 gloss:2.9366 dlossA:2.3340 dlossQ:1.7453 exploreP:0.0121\n",
      "Episode:447 meanR:284.6300 R:500.0000 rate:1.0000 gloss:2.9810 dlossA:2.4242 dlossQ:1.8228 exploreP:0.0120\n",
      "Episode:448 meanR:286.8700 R:500.0000 rate:1.0000 gloss:3.0348 dlossA:2.4023 dlossQ:1.9000 exploreP:0.0119\n",
      "Episode:449 meanR:287.0000 R:409.0000 rate:0.8180 gloss:3.0989 dlossA:2.4756 dlossQ:1.8514 exploreP:0.0118\n",
      "Episode:450 meanR:290.1300 R:500.0000 rate:1.0000 gloss:3.2342 dlossA:2.6674 dlossQ:1.9016 exploreP:0.0117\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
