{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning or Q-network (QN)\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  array([ 0.04948415, -0.17727965,  0.04583192,  0.34480253]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}],\n",
       " (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.772122185233318 -2.5209229796650017\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return actions, states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs_logits, \n",
    "    #                                                               labels=tf.nn.sigmoid(Qs_labels)))    \n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 4) actions:(1000,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print(np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 1000              # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.10405424,  0.77265722, -0.18982688, -1.51150224]), 1.0, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 loss:1.3251 explore_p:0.9983\n",
      "Episode:1 meanR:19.0000 loss:2.2834 explore_p:0.9962\n",
      "Episode:2 meanR:17.6667 loss:3.4505 explore_p:0.9948\n",
      "Episode:3 meanR:19.2500 loss:6.1570 explore_p:0.9924\n",
      "Episode:4 meanR:24.8000 loss:68.7073 explore_p:0.9878\n",
      "Episode:5 meanR:23.6667 loss:347.5805 explore_p:0.9860\n",
      "Episode:6 meanR:21.8571 loss:639.1490 explore_p:0.9850\n",
      "Episode:7 meanR:26.3750 loss:533.3741 explore_p:0.9793\n",
      "Episode:8 meanR:27.0000 loss:66.7435 explore_p:0.9762\n",
      "Episode:9 meanR:25.2000 loss:47.1252 explore_p:0.9754\n",
      "Episode:10 meanR:26.9091 loss:43.7758 explore_p:0.9711\n",
      "Episode:11 meanR:27.0000 loss:47.4970 explore_p:0.9684\n",
      "Episode:12 meanR:26.0000 loss:52.2377 explore_p:0.9671\n",
      "Episode:13 meanR:25.5714 loss:54.6022 explore_p:0.9652\n",
      "Episode:14 meanR:24.8667 loss:56.9052 explore_p:0.9638\n",
      "Episode:15 meanR:24.3125 loss:59.8603 explore_p:0.9622\n",
      "Episode:16 meanR:23.7059 loss:62.7379 explore_p:0.9609\n",
      "Episode:17 meanR:23.6667 loss:67.8908 explore_p:0.9587\n",
      "Episode:18 meanR:23.7368 loss:74.2382 explore_p:0.9563\n",
      "Episode:19 meanR:24.4500 loss:91.4212 explore_p:0.9528\n",
      "Episode:20 meanR:23.9048 loss:106.9915 explore_p:0.9515\n",
      "Episode:21 meanR:23.4091 loss:117.2570 explore_p:0.9503\n",
      "Episode:22 meanR:22.9130 loss:134.1839 explore_p:0.9492\n",
      "Episode:23 meanR:22.5833 loss:141.5268 explore_p:0.9478\n",
      "Episode:24 meanR:22.7200 loss:165.5014 explore_p:0.9453\n",
      "Episode:25 meanR:22.6923 loss:222.8045 explore_p:0.9433\n",
      "Episode:26 meanR:22.5185 loss:293.6241 explore_p:0.9416\n",
      "Episode:27 meanR:23.3571 loss:448.2211 explore_p:0.9373\n",
      "Episode:28 meanR:22.9310 loss:697.7903 explore_p:0.9363\n",
      "Episode:29 meanR:23.1333 loss:993.1287 explore_p:0.9336\n",
      "Episode:30 meanR:22.8387 loss:1611.8785 explore_p:0.9323\n",
      "Episode:31 meanR:24.1875 loss:2756.3999 explore_p:0.9263\n",
      "Episode:32 meanR:23.7576 loss:4602.1504 explore_p:0.9253\n",
      "Episode:33 meanR:24.3235 loss:6506.9106 explore_p:0.9214\n",
      "Episode:34 meanR:25.3714 loss:12122.9736 explore_p:0.9159\n",
      "Episode:35 meanR:25.0278 loss:17049.6426 explore_p:0.9147\n",
      "Episode:36 meanR:24.9189 loss:18488.8184 explore_p:0.9128\n",
      "Episode:37 meanR:25.2105 loss:22123.7754 explore_p:0.9096\n",
      "Episode:38 meanR:24.9231 loss:21372.7520 explore_p:0.9083\n",
      "Episode:39 meanR:24.6250 loss:26055.7305 explore_p:0.9071\n",
      "Episode:40 meanR:24.4634 loss:28570.1797 explore_p:0.9055\n",
      "Episode:41 meanR:24.3810 loss:34896.4492 explore_p:0.9036\n",
      "Episode:42 meanR:24.3953 loss:40003.2734 explore_p:0.9014\n",
      "Episode:43 meanR:24.7500 loss:55970.6328 explore_p:0.8979\n",
      "Episode:44 meanR:24.7333 loss:78781.0781 explore_p:0.8957\n",
      "Episode:45 meanR:24.6087 loss:85506.2266 explore_p:0.8940\n",
      "Episode:46 meanR:25.8298 loss:138952.8281 explore_p:0.8868\n",
      "Episode:47 meanR:25.8542 loss:215399.4219 explore_p:0.8845\n",
      "Episode:48 meanR:26.4694 loss:324490.4688 explore_p:0.8796\n",
      "Episode:49 meanR:26.5800 loss:493228.1562 explore_p:0.8768\n",
      "Episode:50 meanR:27.1961 loss:699924.7500 explore_p:0.8718\n",
      "Episode:51 meanR:27.0577 loss:922292.3125 explore_p:0.8701\n",
      "Episode:52 meanR:27.4340 loss:1068143.5000 explore_p:0.8660\n",
      "Episode:53 meanR:27.2222 loss:1316472.0000 explore_p:0.8647\n",
      "Episode:54 meanR:27.0364 loss:1388042.6250 explore_p:0.8632\n",
      "Episode:55 meanR:27.1607 loss:1210100.3750 explore_p:0.8603\n",
      "Episode:56 meanR:27.5789 loss:1575373.6250 explore_p:0.8560\n",
      "Episode:57 meanR:27.6379 loss:1573519.0000 explore_p:0.8534\n",
      "Episode:58 meanR:27.5085 loss:1766416.6250 explore_p:0.8517\n",
      "Episode:59 meanR:27.6333 loss:1780055.0000 explore_p:0.8487\n",
      "Episode:60 meanR:27.9508 loss:2222185.0000 explore_p:0.8448\n",
      "Episode:61 meanR:27.8226 loss:2796741.7500 explore_p:0.8431\n",
      "Episode:62 meanR:27.6667 loss:2819009.0000 explore_p:0.8416\n",
      "Episode:63 meanR:27.5625 loss:2954122.0000 explore_p:0.8399\n",
      "Episode:64 meanR:27.8154 loss:3240652.0000 explore_p:0.8363\n",
      "Episode:65 meanR:27.7879 loss:3384044.2500 explore_p:0.8341\n",
      "Episode:66 meanR:27.8955 loss:3538889.2500 explore_p:0.8312\n",
      "Episode:67 meanR:27.7353 loss:4519725.5000 explore_p:0.8298\n",
      "Episode:68 meanR:27.9565 loss:5740611.5000 explore_p:0.8263\n",
      "Episode:69 meanR:28.5143 loss:9444804.0000 explore_p:0.8209\n",
      "Episode:70 meanR:28.7606 loss:19141812.0000 explore_p:0.8171\n",
      "Episode:71 meanR:28.5278 loss:20909990.0000 explore_p:0.8162\n",
      "Episode:72 meanR:28.4247 loss:18150864.0000 explore_p:0.8145\n",
      "Episode:73 meanR:28.3514 loss:18117372.0000 explore_p:0.8126\n",
      "Episode:74 meanR:28.7067 loss:16310577.0000 explore_p:0.8082\n",
      "Episode:75 meanR:28.5789 loss:16687818.0000 explore_p:0.8067\n",
      "Episode:76 meanR:28.5065 loss:17084876.0000 explore_p:0.8049\n",
      "Episode:77 meanR:28.3590 loss:19194928.0000 explore_p:0.8035\n",
      "Episode:78 meanR:28.5443 loss:18668206.0000 explore_p:0.8001\n",
      "Episode:79 meanR:28.7625 loss:22046046.0000 explore_p:0.7965\n",
      "Episode:80 meanR:28.5802 loss:22616566.0000 explore_p:0.7954\n",
      "Episode:81 meanR:28.4146 loss:24375826.0000 explore_p:0.7942\n",
      "Episode:82 meanR:28.4699 loss:29338452.0000 explore_p:0.7916\n",
      "Episode:83 meanR:28.4643 loss:36493484.0000 explore_p:0.7895\n",
      "Episode:84 meanR:28.6000 loss:47016448.0000 explore_p:0.7864\n",
      "Episode:85 meanR:28.6279 loss:58836424.0000 explore_p:0.7839\n",
      "Episode:86 meanR:28.4828 loss:64251136.0000 explore_p:0.7827\n",
      "Episode:87 meanR:28.4545 loss:74089368.0000 explore_p:0.7807\n",
      "Episode:88 meanR:28.4157 loss:83123792.0000 explore_p:0.7788\n",
      "Episode:89 meanR:28.3000 loss:92567928.0000 explore_p:0.7774\n",
      "Episode:90 meanR:28.6264 loss:89570728.0000 explore_p:0.7730\n",
      "Episode:91 meanR:29.0326 loss:99344032.0000 explore_p:0.7679\n",
      "Episode:92 meanR:28.8817 loss:99711272.0000 explore_p:0.7668\n",
      "Episode:93 meanR:28.7872 loss:115493288.0000 explore_p:0.7653\n",
      "Episode:94 meanR:28.9474 loss:119515672.0000 explore_p:0.7620\n",
      "Episode:95 meanR:28.9479 loss:137883120.0000 explore_p:0.7598\n",
      "Episode:96 meanR:28.8557 loss:149303072.0000 explore_p:0.7583\n",
      "Episode:97 meanR:28.8673 loss:164646160.0000 explore_p:0.7561\n",
      "Episode:98 meanR:28.7677 loss:188696464.0000 explore_p:0.7546\n",
      "Episode:99 meanR:29.2500 loss:239032592.0000 explore_p:0.7489\n",
      "Episode:100 meanR:29.4000 loss:244237088.0000 explore_p:0.7466\n",
      "Episode:101 meanR:29.3400 loss:259898608.0000 explore_p:0.7455\n",
      "Episode:102 meanR:29.3800 loss:254702352.0000 explore_p:0.7441\n",
      "Episode:103 meanR:29.8100 loss:260308080.0000 explore_p:0.7392\n",
      "Episode:104 meanR:29.7400 loss:324562848.0000 explore_p:0.7363\n",
      "Episode:105 meanR:30.1200 loss:405609216.0000 explore_p:0.7322\n",
      "Episode:106 meanR:30.9100 loss:587439744.0000 explore_p:0.7257\n",
      "Episode:107 meanR:31.2400 loss:885296320.0000 explore_p:0.7192\n",
      "Episode:108 meanR:31.2800 loss:1163640832.0000 explore_p:0.7167\n",
      "Episode:109 meanR:31.3600 loss:1360901120.0000 explore_p:0.7155\n",
      "Episode:110 meanR:31.3600 loss:1518255232.0000 explore_p:0.7124\n",
      "Episode:111 meanR:31.5400 loss:1970734976.0000 explore_p:0.7092\n",
      "Episode:112 meanR:31.6800 loss:2472152576.0000 explore_p:0.7072\n",
      "Episode:113 meanR:31.7100 loss:3040250368.0000 explore_p:0.7056\n",
      "Episode:114 meanR:32.0000 loss:3710535424.0000 explore_p:0.7026\n",
      "Episode:115 meanR:32.5800 loss:5268608000.0000 explore_p:0.6975\n",
      "Episode:116 meanR:32.9700 loss:9249922048.0000 explore_p:0.6938\n",
      "Episode:117 meanR:32.8700 loss:14253262848.0000 explore_p:0.6929\n",
      "Episode:118 meanR:32.8400 loss:14448577536.0000 explore_p:0.6914\n",
      "Episode:119 meanR:34.1900 loss:16663321600.0000 explore_p:0.6798\n",
      "Episode:120 meanR:34.7700 loss:24083841024.0000 explore_p:0.6750\n",
      "Episode:121 meanR:34.9400 loss:31402631168.0000 explore_p:0.6730\n",
      "Episode:122 meanR:34.9700 loss:34169847808.0000 explore_p:0.6720\n",
      "Episode:123 meanR:35.1500 loss:36231741440.0000 explore_p:0.6698\n",
      "Episode:124 meanR:35.3700 loss:41103478784.0000 explore_p:0.6667\n",
      "Episode:125 meanR:36.2700 loss:53549793280.0000 explore_p:0.6594\n",
      "Episode:126 meanR:36.4000 loss:57550303232.0000 explore_p:0.6574\n",
      "Episode:127 meanR:36.4200 loss:65039925248.0000 explore_p:0.6543\n",
      "Episode:128 meanR:37.7500 loss:80705511424.0000 explore_p:0.6451\n",
      "Episode:129 meanR:37.6600 loss:111807143936.0000 explore_p:0.6438\n",
      "Episode:130 meanR:37.8800 loss:108451110912.0000 explore_p:0.6415\n",
      "Episode:131 meanR:37.5800 loss:120581267456.0000 explore_p:0.6392\n",
      "Episode:132 meanR:37.8800 loss:133311643648.0000 explore_p:0.6367\n",
      "Episode:133 meanR:39.2000 loss:203815108608.0000 explore_p:0.6259\n",
      "Episode:134 meanR:39.1200 loss:279369744384.0000 explore_p:0.6226\n",
      "Episode:135 meanR:39.4100 loss:336655187968.0000 explore_p:0.6200\n",
      "Episode:136 meanR:39.6900 loss:367691890688.0000 explore_p:0.6170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:137 meanR:39.9400 loss:427907612672.0000 explore_p:0.6134\n",
      "Episode:138 meanR:40.4200 loss:535009034240.0000 explore_p:0.6096\n",
      "Episode:139 meanR:40.7500 loss:573563535360.0000 explore_p:0.6069\n",
      "Episode:140 meanR:40.9300 loss:602375323648.0000 explore_p:0.6047\n",
      "Episode:141 meanR:41.3100 loss:511473221632.0000 explore_p:0.6012\n",
      "Episode:142 meanR:41.6700 loss:589898645504.0000 explore_p:0.5976\n",
      "Episode:143 meanR:41.8100 loss:626501025792.0000 explore_p:0.5945\n",
      "Episode:144 meanR:43.4500 loss:549120507904.0000 explore_p:0.5836\n",
      "Episode:145 meanR:44.0300 loss:603626340352.0000 explore_p:0.5792\n",
      "Episode:146 meanR:44.8000 loss:385159200768.0000 explore_p:0.5702\n",
      "Episode:147 meanR:45.4400 loss:234915135488.0000 explore_p:0.5651\n",
      "Episode:148 meanR:45.1500 loss:197459247104.0000 explore_p:0.5636\n",
      "Episode:149 meanR:45.4900 loss:179704168448.0000 explore_p:0.5600\n",
      "Episode:150 meanR:45.0300 loss:166114197504.0000 explore_p:0.5593\n",
      "Episode:151 meanR:45.4100 loss:140949700608.0000 explore_p:0.5562\n",
      "Episode:152 meanR:45.1700 loss:155506196480.0000 explore_p:0.5549\n",
      "Episode:153 meanR:46.1400 loss:139931467776.0000 explore_p:0.5488\n",
      "Episode:154 meanR:46.3700 loss:174153695232.0000 explore_p:0.5466\n",
      "Episode:155 meanR:48.3800 loss:156192915456.0000 explore_p:0.5342\n",
      "Episode:156 meanR:48.3300 loss:232216870912.0000 explore_p:0.5318\n",
      "Episode:157 meanR:49.2700 loss:271573254144.0000 explore_p:0.5253\n",
      "Episode:158 meanR:50.6700 loss:217502416896.0000 explore_p:0.5171\n",
      "Episode:159 meanR:51.3500 loss:221388095488.0000 explore_p:0.5119\n",
      "Episode:160 meanR:51.0500 loss:231502708736.0000 explore_p:0.5110\n",
      "Episode:161 meanR:50.9600 loss:237891010560.0000 explore_p:0.5105\n",
      "Episode:162 meanR:51.1600 loss:246307799040.0000 explore_p:0.5086\n",
      "Episode:163 meanR:53.2200 loss:245883289600.0000 explore_p:0.4974\n",
      "Episode:164 meanR:52.9600 loss:162354642944.0000 explore_p:0.4965\n",
      "Episode:165 meanR:54.2300 loss:234334060544.0000 explore_p:0.4891\n",
      "Episode:166 meanR:55.8300 loss:234187128832.0000 explore_p:0.4799\n",
      "Episode:167 meanR:57.6300 loss:432999170048.0000 explore_p:0.4707\n",
      "Episode:168 meanR:60.3600 loss:872717615104.0000 explore_p:0.4564\n",
      "Episode:169 meanR:60.9400 loss:2074459766784.0000 explore_p:0.4509\n",
      "Episode:170 meanR:62.5500 loss:2222150909952.0000 explore_p:0.4418\n",
      "Episode:171 meanR:64.6000 loss:3114162192384.0000 explore_p:0.4325\n",
      "Episode:172 meanR:65.9100 loss:2672358064128.0000 explore_p:0.4262\n",
      "Episode:173 meanR:65.9000 loss:2682678935552.0000 explore_p:0.4253\n",
      "Episode:174 meanR:67.4700 loss:2796356632576.0000 explore_p:0.4165\n",
      "Episode:175 meanR:68.0400 loss:3233010679808.0000 explore_p:0.4135\n",
      "Episode:176 meanR:69.5500 loss:2951547191296.0000 explore_p:0.4065\n",
      "Episode:177 meanR:72.1400 loss:3403012898816.0000 explore_p:0.3957\n",
      "Episode:178 meanR:73.0100 loss:4088865488896.0000 explore_p:0.3907\n",
      "Episode:179 meanR:74.3600 loss:4024292868096.0000 explore_p:0.3839\n",
      "Episode:180 meanR:76.8600 loss:3411735740416.0000 explore_p:0.3742\n",
      "Episode:181 meanR:79.1000 loss:3355479638016.0000 explore_p:0.3656\n",
      "Episode:182 meanR:81.3600 loss:3746543173632.0000 explore_p:0.3565\n",
      "Episode:183 meanR:84.6500 loss:2736909713408.0000 explore_p:0.3443\n",
      "Episode:184 meanR:86.1300 loss:2808794316800.0000 explore_p:0.3381\n",
      "Episode:185 meanR:88.2100 loss:1475684859904.0000 explore_p:0.3303\n",
      "Episode:186 meanR:90.4500 loss:572652912640.0000 explore_p:0.3228\n",
      "Episode:187 meanR:93.2200 loss:1539714318336.0000 explore_p:0.3134\n",
      "Episode:188 meanR:96.2600 loss:3087763243008.0000 explore_p:0.3036\n",
      "Episode:189 meanR:99.7400 loss:4095499829248.0000 explore_p:0.2930\n",
      "Episode:190 meanR:103.5300 loss:3921207361536.0000 explore_p:0.2809\n",
      "Episode:191 meanR:106.4600 loss:3490080882688.0000 explore_p:0.2714\n",
      "Episode:192 meanR:110.2700 loss:2994830049280.0000 explore_p:0.2612\n",
      "Episode:193 meanR:110.7300 loss:2278044205056.0000 explore_p:0.2596\n",
      "Episode:194 meanR:115.2900 loss:1381613305856.0000 explore_p:0.2474\n",
      "Episode:195 meanR:116.1900 loss:213467938816.0000 explore_p:0.2446\n",
      "Episode:196 meanR:117.6300 loss:135923122176.0000 explore_p:0.2408\n",
      "Episode:197 meanR:122.3300 loss:126600749056.0000 explore_p:0.2295\n",
      "Episode:198 meanR:125.8700 loss:68782637056.0000 explore_p:0.2215\n",
      "Episode:199 meanR:130.1000 loss:151720804352.0000 explore_p:0.2112\n",
      "Episode:200 meanR:134.1600 loss:211035734016.0000 explore_p:0.2026\n",
      "Episode:201 meanR:136.9400 loss:306478120960.0000 explore_p:0.1970\n",
      "Episode:202 meanR:137.8900 loss:236269649920.0000 explore_p:0.1949\n",
      "Episode:203 meanR:137.3400 loss:83358253056.0000 explore_p:0.1947\n",
      "Episode:204 meanR:137.0300 loss:127719178240.0000 explore_p:0.1945\n",
      "Episode:205 meanR:136.5600 loss:179715244032.0000 explore_p:0.1943\n",
      "Episode:206 meanR:135.7700 loss:245423603712.0000 explore_p:0.1941\n",
      "Episode:207 meanR:134.9800 loss:323142483968.0000 explore_p:0.1939\n",
      "Episode:208 meanR:134.7400 loss:381798875136.0000 explore_p:0.1937\n",
      "Episode:209 meanR:134.6600 loss:407246471168.0000 explore_p:0.1935\n",
      "Episode:210 meanR:134.3200 loss:409082200064.0000 explore_p:0.1933\n",
      "Episode:211 meanR:135.7000 loss:387094511616.0000 explore_p:0.1900\n",
      "Episode:212 meanR:135.5300 loss:481652473856.0000 explore_p:0.1898\n",
      "Episode:213 meanR:135.4100 loss:500451278848.0000 explore_p:0.1896\n",
      "Episode:214 meanR:135.0800 loss:521998008320.0000 explore_p:0.1894\n",
      "Episode:215 meanR:134.4600 loss:546671198208.0000 explore_p:0.1892\n",
      "Episode:216 meanR:134.0400 loss:560684924928.0000 explore_p:0.1890\n",
      "Episode:217 meanR:135.1300 loss:483983818752.0000 explore_p:0.1868\n",
      "Episode:218 meanR:136.7300 loss:749178126336.0000 explore_p:0.1836\n",
      "Episode:219 meanR:137.7600 loss:1279685820416.0000 explore_p:0.1789\n",
      "Episode:220 meanR:142.0500 loss:534292660224.0000 explore_p:0.1707\n",
      "Episode:221 meanR:144.5900 loss:207189721088.0000 explore_p:0.1662\n",
      "Episode:222 meanR:147.8100 loss:335618670592.0000 explore_p:0.1610\n",
      "Episode:223 meanR:150.0400 loss:747809669120.0000 explore_p:0.1572\n",
      "Episode:224 meanR:151.8300 loss:1671023296512.0000 explore_p:0.1539\n",
      "Episode:225 meanR:152.9000 loss:2696394309632.0000 explore_p:0.1508\n",
      "Episode:226 meanR:154.7700 loss:3614568349696.0000 explore_p:0.1477\n",
      "Episode:227 meanR:156.5000 loss:4508256043008.0000 explore_p:0.1447\n",
      "Episode:228 meanR:157.0600 loss:5361052418048.0000 explore_p:0.1420\n",
      "Episode:229 meanR:158.9700 loss:5824221020160.0000 explore_p:0.1393\n",
      "Episode:230 meanR:160.7300 loss:6235745681408.0000 explore_p:0.1366\n",
      "Episode:231 meanR:162.4300 loss:6752879771648.0000 explore_p:0.1340\n",
      "Episode:232 meanR:164.1100 loss:7228912828416.0000 explore_p:0.1314\n",
      "Episode:233 meanR:164.8000 loss:7152118267904.0000 explore_p:0.1285\n",
      "Episode:234 meanR:165.9900 loss:7364167073792.0000 explore_p:0.1265\n",
      "Episode:235 meanR:167.3400 loss:7779128967168.0000 explore_p:0.1244\n",
      "Episode:236 meanR:168.9600 loss:7849560768512.0000 explore_p:0.1221\n",
      "Episode:237 meanR:170.2200 loss:8076088836096.0000 explore_p:0.1200\n",
      "Episode:238 meanR:171.6600 loss:8459482300416.0000 explore_p:0.1177\n",
      "Episode:239 meanR:173.4600 loss:8636631875584.0000 explore_p:0.1153\n",
      "Episode:240 meanR:174.8700 loss:8596278476800.0000 explore_p:0.1135\n",
      "Episode:241 meanR:176.3000 loss:8673233469440.0000 explore_p:0.1114\n",
      "Episode:242 meanR:177.7400 loss:8514711322624.0000 explore_p:0.1094\n",
      "Episode:243 meanR:179.1500 loss:8608962052096.0000 explore_p:0.1074\n",
      "Episode:244 meanR:179.0000 loss:8934219841536.0000 explore_p:0.1058\n",
      "Episode:245 meanR:180.4600 loss:8831361875968.0000 explore_p:0.1037\n",
      "Episode:246 meanR:180.8700 loss:8774673235968.0000 explore_p:0.1018\n",
      "Episode:247 meanR:182.1500 loss:8692907376640.0000 explore_p:0.0998\n",
      "Episode:248 meanR:183.8500 loss:8657471275008.0000 explore_p:0.0981\n",
      "Episode:249 meanR:184.8900 loss:8685483458560.0000 explore_p:0.0966\n",
      "Episode:250 meanR:186.5300 loss:9030212780032.0000 explore_p:0.0951\n",
      "Episode:251 meanR:187.7700 loss:9132559040512.0000 explore_p:0.0935\n",
      "Episode:252 meanR:189.4100 loss:9260957171712.0000 explore_p:0.0920\n",
      "Episode:253 meanR:190.0400 loss:9562307428352.0000 explore_p:0.0906\n",
      "Episode:254 meanR:191.2900 loss:9847179313152.0000 explore_p:0.0892\n",
      "Episode:255 meanR:191.0100 loss:9654532833280.0000 explore_p:0.0876\n",
      "Episode:256 meanR:192.5400 loss:9427437486080.0000 explore_p:0.0861\n",
      "Episode:257 meanR:193.1000 loss:9357028753408.0000 explore_p:0.0847\n",
      "Episode:258 meanR:193.2800 loss:9423145664512.0000 explore_p:0.0834\n",
      "Episode:259 meanR:193.9800 loss:9435259863040.0000 explore_p:0.0821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:260 meanR:195.9300 loss:9250670641152.0000 explore_p:0.0806\n",
      "Episode:261 meanR:197.7200 loss:9279679496192.0000 explore_p:0.0793\n",
      "Episode:262 meanR:199.3600 loss:9284785012736.0000 explore_p:0.0779\n",
      "Episode:263 meanR:199.1200 loss:9076744388608.0000 explore_p:0.0765\n",
      "Episode:264 meanR:200.8200 loss:8848535453696.0000 explore_p:0.0753\n",
      "Episode:265 meanR:201.2300 loss:8941834600448.0000 explore_p:0.0741\n",
      "Episode:266 meanR:201.3400 loss:8870695010304.0000 explore_p:0.0727\n",
      "Episode:267 meanR:201.2500 loss:8914982666240.0000 explore_p:0.0716\n",
      "Episode:268 meanR:200.0300 loss:8957768761344.0000 explore_p:0.0704\n",
      "Episode:269 meanR:200.7400 loss:8964662099968.0000 explore_p:0.0692\n",
      "Episode:270 meanR:200.7800 loss:8793369346048.0000 explore_p:0.0680\n",
      "Episode:271 meanR:200.5000 loss:8907036557312.0000 explore_p:0.0669\n",
      "Episode:272 meanR:200.9100 loss:8945967038464.0000 explore_p:0.0658\n",
      "Episode:273 meanR:202.5200 loss:8981446656000.0000 explore_p:0.0648\n",
      "Episode:274 meanR:202.2300 loss:9076394164224.0000 explore_p:0.0638\n",
      "Episode:275 meanR:203.4800 loss:9150677385216.0000 explore_p:0.0627\n",
      "Episode:276 meanR:203.6400 loss:9234422956032.0000 explore_p:0.0617\n",
      "Episode:277 meanR:203.0200 loss:9091892117504.0000 explore_p:0.0606\n",
      "Episode:278 meanR:203.9100 loss:8783112699904.0000 explore_p:0.0596\n",
      "Episode:279 meanR:204.1900 loss:8490312531968.0000 explore_p:0.0585\n",
      "Episode:280 meanR:203.4200 loss:8547841081344.0000 explore_p:0.0576\n",
      "Episode:281 meanR:202.9800 loss:8479879725056.0000 explore_p:0.0567\n",
      "Episode:282 meanR:202.3100 loss:8643348529152.0000 explore_p:0.0558\n",
      "Episode:283 meanR:200.8100 loss:8695993860096.0000 explore_p:0.0549\n",
      "Episode:284 meanR:200.9000 loss:8788385988608.0000 explore_p:0.0540\n",
      "Episode:285 meanR:200.5100 loss:8748908675072.0000 explore_p:0.0531\n",
      "Episode:286 meanR:200.2000 loss:8605623386112.0000 explore_p:0.0522\n",
      "Episode:287 meanR:199.1900 loss:8544549601280.0000 explore_p:0.0514\n",
      "Episode:288 meanR:197.8200 loss:8692607483904.0000 explore_p:0.0506\n",
      "Episode:289 meanR:195.9900 loss:8838089539584.0000 explore_p:0.0499\n",
      "Episode:290 meanR:193.7900 loss:8804176494592.0000 explore_p:0.0490\n",
      "Episode:291 meanR:192.0400 loss:8962677145600.0000 explore_p:0.0483\n",
      "Episode:292 meanR:189.9400 loss:9083461566464.0000 explore_p:0.0476\n",
      "Episode:293 meanR:191.1000 loss:9202341773312.0000 explore_p:0.0469\n",
      "Episode:294 meanR:187.8300 loss:9275234582528.0000 explore_p:0.0463\n",
      "Episode:295 meanR:188.5900 loss:9264856825856.0000 explore_p:0.0456\n",
      "Episode:296 meanR:189.0300 loss:9368859836416.0000 explore_p:0.0449\n",
      "Episode:297 meanR:185.9100 loss:9254719193088.0000 explore_p:0.0442\n",
      "Episode:298 meanR:184.0400 loss:9261804421120.0000 explore_p:0.0436\n",
      "Episode:299 meanR:181.0900 loss:9072734633984.0000 explore_p:0.0429\n",
      "Episode:300 meanR:178.5000 loss:9015090216960.0000 explore_p:0.0423\n",
      "Episode:301 meanR:177.2900 loss:9204770275328.0000 explore_p:0.0418\n",
      "Episode:302 meanR:177.9500 loss:9444157030400.0000 explore_p:0.0412\n",
      "Episode:303 meanR:179.7100 loss:9457284153344.0000 explore_p:0.0406\n",
      "Episode:304 meanR:181.5000 loss:9440354893824.0000 explore_p:0.0400\n",
      "Episode:305 meanR:183.4600 loss:9519698542592.0000 explore_p:0.0394\n",
      "Episode:306 meanR:185.3300 loss:9343178113024.0000 explore_p:0.0389\n",
      "Episode:307 meanR:187.0400 loss:9200768909312.0000 explore_p:0.0383\n",
      "Episode:308 meanR:188.7600 loss:9215493013504.0000 explore_p:0.0378\n",
      "Episode:309 meanR:190.6800 loss:9173884469248.0000 explore_p:0.0373\n",
      "Episode:310 meanR:192.3500 loss:9237573926912.0000 explore_p:0.0368\n",
      "Episode:311 meanR:192.6100 loss:9263955050496.0000 explore_p:0.0362\n",
      "Episode:312 meanR:194.3100 loss:9236923809792.0000 explore_p:0.0358\n",
      "Episode:313 meanR:196.3100 loss:9076453933056.0000 explore_p:0.0352\n",
      "Episode:314 meanR:198.2300 loss:8950656270336.0000 explore_p:0.0347\n",
      "Episode:315 meanR:200.0100 loss:8857911820288.0000 explore_p:0.0342\n",
      "Episode:316 meanR:201.7500 loss:8975724576768.0000 explore_p:0.0338\n",
      "Episode:317 meanR:202.5600 loss:8917629272064.0000 explore_p:0.0333\n",
      "Episode:318 meanR:202.7000 loss:8955530051584.0000 explore_p:0.0329\n",
      "Episode:319 meanR:201.7200 loss:9103748366336.0000 explore_p:0.0325\n",
      "Episode:320 meanR:198.5200 loss:9270161571840.0000 explore_p:0.0321\n",
      "Episode:321 meanR:197.5600 loss:9346812477440.0000 explore_p:0.0317\n",
      "Episode:322 meanR:196.1200 loss:9323820351488.0000 explore_p:0.0312\n",
      "Episode:323 meanR:195.4700 loss:9411734011904.0000 explore_p:0.0308\n",
      "Episode:324 meanR:195.0200 loss:9462440001536.0000 explore_p:0.0305\n",
      "Episode:325 meanR:194.7700 loss:9374770659328.0000 explore_p:0.0301\n",
      "Episode:326 meanR:194.3600 loss:9326089469952.0000 explore_p:0.0297\n",
      "Episode:327 meanR:194.1400 loss:9347205693440.0000 explore_p:0.0293\n",
      "Episode:328 meanR:193.9300 loss:9392056434688.0000 explore_p:0.0290\n",
      "Episode:329 meanR:193.7400 loss:9449424027648.0000 explore_p:0.0286\n",
      "Episode:330 meanR:193.5000 loss:9377862909952.0000 explore_p:0.0283\n",
      "Episode:331 meanR:193.4800 loss:9298016993280.0000 explore_p:0.0279\n",
      "Episode:332 meanR:193.3800 loss:9122441330688.0000 explore_p:0.0276\n",
      "Episode:333 meanR:192.6500 loss:9205772713984.0000 explore_p:0.0273\n",
      "Episode:334 meanR:193.0000 loss:9129912434688.0000 explore_p:0.0269\n",
      "Episode:335 meanR:192.9500 loss:9159167705088.0000 explore_p:0.0266\n",
      "Episode:336 meanR:192.9300 loss:9201447337984.0000 explore_p:0.0263\n",
      "Episode:337 meanR:193.2100 loss:9128581791744.0000 explore_p:0.0259\n",
      "Episode:338 meanR:193.2400 loss:8785735188480.0000 explore_p:0.0256\n",
      "Episode:339 meanR:192.7600 loss:8944077504512.0000 explore_p:0.0253\n",
      "Episode:340 meanR:192.9300 loss:8810479484928.0000 explore_p:0.0250\n",
      "Episode:341 meanR:192.9200 loss:8823626530816.0000 explore_p:0.0247\n",
      "Episode:342 meanR:192.8900 loss:8914355617792.0000 explore_p:0.0244\n",
      "Episode:343 meanR:192.9800 loss:8989824778240.0000 explore_p:0.0241\n",
      "Episode:344 meanR:192.9600 loss:9070159331328.0000 explore_p:0.0239\n",
      "Episode:345 meanR:192.6600 loss:9044589805568.0000 explore_p:0.0236\n",
      "Episode:346 meanR:192.5900 loss:9069787086848.0000 explore_p:0.0234\n",
      "Episode:347 meanR:192.3800 loss:9128223178752.0000 explore_p:0.0231\n",
      "Episode:348 meanR:192.3800 loss:9154023391232.0000 explore_p:0.0229\n",
      "Episode:349 meanR:192.5100 loss:9211264106496.0000 explore_p:0.0226\n",
      "Episode:350 meanR:192.7300 loss:9113752829952.0000 explore_p:0.0224\n",
      "Episode:351 meanR:192.8900 loss:9065319104512.0000 explore_p:0.0221\n",
      "Episode:352 meanR:192.8100 loss:9120224641024.0000 explore_p:0.0219\n",
      "Episode:353 meanR:192.8700 loss:9237752184832.0000 explore_p:0.0217\n",
      "Episode:354 meanR:193.0500 loss:9364355153920.0000 explore_p:0.0215\n",
      "Episode:355 meanR:192.7100 loss:9432539856896.0000 explore_p:0.0213\n",
      "Episode:356 meanR:192.5000 loss:9639705968640.0000 explore_p:0.0211\n",
      "Episode:357 meanR:192.4700 loss:9823651364864.0000 explore_p:0.0209\n",
      "Episode:358 meanR:192.4800 loss:9808159703040.0000 explore_p:0.0207\n",
      "Episode:359 meanR:192.6400 loss:9760390774784.0000 explore_p:0.0205\n",
      "Episode:360 meanR:192.7400 loss:9505585758208.0000 explore_p:0.0203\n",
      "Episode:361 meanR:192.8500 loss:9149517660160.0000 explore_p:0.0201\n",
      "Episode:362 meanR:192.8000 loss:8920469864448.0000 explore_p:0.0199\n",
      "Episode:363 meanR:192.6200 loss:8819041107968.0000 explore_p:0.0197\n",
      "Episode:364 meanR:192.6200 loss:8791827939328.0000 explore_p:0.0195\n",
      "Episode:365 meanR:192.6700 loss:8881287725056.0000 explore_p:0.0193\n",
      "Episode:366 meanR:192.5600 loss:8992659079168.0000 explore_p:0.0191\n",
      "Episode:367 meanR:192.4800 loss:9129287483392.0000 explore_p:0.0190\n",
      "Episode:368 meanR:192.4700 loss:9216923271168.0000 explore_p:0.0188\n",
      "Episode:369 meanR:192.3300 loss:9243732213760.0000 explore_p:0.0187\n",
      "Episode:370 meanR:192.0900 loss:9315255582720.0000 explore_p:0.0185\n",
      "Episode:371 meanR:191.9300 loss:9480293056512.0000 explore_p:0.0183\n",
      "Episode:372 meanR:191.8900 loss:9593397706752.0000 explore_p:0.0182\n",
      "Episode:373 meanR:191.8800 loss:9538815131648.0000 explore_p:0.0180\n",
      "Episode:374 meanR:191.8400 loss:9630490034176.0000 explore_p:0.0179\n",
      "Episode:375 meanR:191.7300 loss:9613298630656.0000 explore_p:0.0178\n",
      "Episode:376 meanR:191.5100 loss:9682648301568.0000 explore_p:0.0176\n",
      "Episode:377 meanR:191.5500 loss:9465075073024.0000 explore_p:0.0175\n",
      "Episode:378 meanR:191.4900 loss:9187957407744.0000 explore_p:0.0173\n",
      "Episode:379 meanR:191.2100 loss:9050421985280.0000 explore_p:0.0172\n",
      "Episode:380 meanR:191.3800 loss:8929345011712.0000 explore_p:0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:381 meanR:191.5200 loss:8568905400320.0000 explore_p:0.0169\n",
      "Episode:382 meanR:191.4100 loss:8862344151040.0000 explore_p:0.0168\n",
      "Episode:383 meanR:191.3400 loss:8971270225920.0000 explore_p:0.0166\n",
      "Episode:384 meanR:191.3800 loss:8893848616960.0000 explore_p:0.0165\n",
      "Episode:385 meanR:191.3200 loss:8934011174912.0000 explore_p:0.0164\n",
      "Episode:386 meanR:191.0900 loss:9086927110144.0000 explore_p:0.0162\n",
      "Episode:387 meanR:190.9200 loss:9184946946048.0000 explore_p:0.0161\n",
      "Episode:388 meanR:190.7900 loss:9223121403904.0000 explore_p:0.0160\n",
      "Episode:389 meanR:190.8600 loss:9360992370688.0000 explore_p:0.0159\n",
      "Episode:390 meanR:190.6600 loss:9422147420160.0000 explore_p:0.0158\n",
      "Episode:391 meanR:190.9400 loss:9282529525760.0000 explore_p:0.0157\n",
      "Episode:392 meanR:190.9700 loss:9147973107712.0000 explore_p:0.0156\n",
      "Episode:393 meanR:191.0900 loss:9039466463232.0000 explore_p:0.0155\n",
      "Episode:394 meanR:191.4500 loss:8834571567104.0000 explore_p:0.0153\n",
      "Episode:395 meanR:191.3700 loss:8872408383488.0000 explore_p:0.0153\n",
      "Episode:396 meanR:191.1500 loss:9010303467520.0000 explore_p:0.0152\n",
      "Episode:397 meanR:191.1800 loss:9096168210432.0000 explore_p:0.0151\n",
      "Episode:398 meanR:191.1800 loss:9131638390784.0000 explore_p:0.0150\n",
      "Episode:399 meanR:190.9700 loss:9233154179072.0000 explore_p:0.0149\n",
      "Episode:400 meanR:191.1000 loss:9399075602432.0000 explore_p:0.0148\n",
      "Episode:401 meanR:191.2700 loss:9376523878400.0000 explore_p:0.0147\n",
      "Episode:402 meanR:191.4600 loss:9305992462336.0000 explore_p:0.0146\n",
      "Episode:403 meanR:191.4500 loss:9265342316544.0000 explore_p:0.0145\n",
      "Episode:404 meanR:191.2900 loss:9316249632768.0000 explore_p:0.0144\n",
      "Episode:405 meanR:191.4000 loss:9228547784704.0000 explore_p:0.0143\n",
      "Episode:406 meanR:191.3700 loss:9077709078528.0000 explore_p:0.0143\n",
      "Episode:407 meanR:191.1700 loss:9220041736192.0000 explore_p:0.0142\n",
      "Episode:408 meanR:191.3800 loss:9315043770368.0000 explore_p:0.0141\n",
      "Episode:409 meanR:191.2500 loss:9180665610240.0000 explore_p:0.0140\n",
      "Episode:410 meanR:191.2900 loss:9173950529536.0000 explore_p:0.0140\n",
      "Episode:411 meanR:191.1000 loss:9418500472832.0000 explore_p:0.0139\n",
      "Episode:412 meanR:191.1300 loss:9439815925760.0000 explore_p:0.0138\n",
      "Episode:413 meanR:190.9500 loss:9272872140800.0000 explore_p:0.0137\n",
      "Episode:414 meanR:190.8500 loss:9371611299840.0000 explore_p:0.0137\n",
      "Episode:415 meanR:190.9900 loss:9247053053952.0000 explore_p:0.0136\n",
      "Episode:416 meanR:190.9600 loss:9175990009856.0000 explore_p:0.0135\n",
      "Episode:417 meanR:190.6800 loss:9290395942912.0000 explore_p:0.0135\n",
      "Episode:418 meanR:190.7600 loss:9252067344384.0000 explore_p:0.0134\n",
      "Episode:419 meanR:190.9400 loss:9164280561664.0000 explore_p:0.0133\n",
      "Episode:420 meanR:190.9600 loss:9217269301248.0000 explore_p:0.0133\n",
      "Episode:421 meanR:191.2500 loss:9179578236928.0000 explore_p:0.0132\n",
      "Episode:422 meanR:191.0700 loss:9026697953280.0000 explore_p:0.0131\n",
      "Episode:423 meanR:191.1100 loss:9067909087232.0000 explore_p:0.0131\n",
      "Episode:424 meanR:191.0700 loss:9229569097728.0000 explore_p:0.0130\n",
      "Episode:425 meanR:191.0200 loss:9320464908288.0000 explore_p:0.0130\n",
      "Episode:426 meanR:191.2100 loss:9289609510912.0000 explore_p:0.0129\n",
      "Episode:427 meanR:190.9800 loss:9485335658496.0000 explore_p:0.0129\n",
      "Episode:428 meanR:191.0100 loss:9480700952576.0000 explore_p:0.0128\n",
      "Episode:429 meanR:191.0800 loss:9475296591872.0000 explore_p:0.0128\n",
      "Episode:430 meanR:191.1000 loss:9329650434048.0000 explore_p:0.0127\n",
      "Episode:431 meanR:190.9900 loss:9339898167296.0000 explore_p:0.0127\n",
      "Episode:432 meanR:190.7800 loss:9397202845696.0000 explore_p:0.0126\n",
      "Episode:433 meanR:191.2600 loss:9168584966144.0000 explore_p:0.0125\n",
      "Episode:434 meanR:190.9100 loss:9138463571968.0000 explore_p:0.0125\n",
      "Episode:435 meanR:190.9400 loss:9316197203968.0000 explore_p:0.0125\n",
      "Episode:436 meanR:190.7200 loss:9379331964928.0000 explore_p:0.0124\n",
      "Episode:437 meanR:190.4300 loss:9439945949184.0000 explore_p:0.0124\n",
      "Episode:438 meanR:190.4200 loss:9291228512256.0000 explore_p:0.0123\n",
      "Episode:439 meanR:190.3700 loss:9455106260992.0000 explore_p:0.0123\n",
      "Episode:440 meanR:190.3000 loss:9366440771584.0000 explore_p:0.0122\n",
      "Episode:441 meanR:190.4600 loss:9106484101120.0000 explore_p:0.0122\n",
      "Episode:442 meanR:190.4400 loss:8889444597760.0000 explore_p:0.0121\n",
      "Episode:443 meanR:190.4400 loss:8884316012544.0000 explore_p:0.0121\n",
      "Episode:444 meanR:190.4900 loss:8938749689856.0000 explore_p:0.0121\n",
      "Episode:445 meanR:190.3800 loss:8981841969152.0000 explore_p:0.0120\n",
      "Episode:446 meanR:190.4500 loss:9083092467712.0000 explore_p:0.0120\n",
      "Episode:447 meanR:190.2300 loss:9276064006144.0000 explore_p:0.0120\n",
      "Episode:448 meanR:189.9200 loss:9537994096640.0000 explore_p:0.0119\n",
      "Episode:449 meanR:189.9500 loss:9752551620608.0000 explore_p:0.0119\n",
      "Episode:450 meanR:189.7600 loss:9692867723264.0000 explore_p:0.0119\n",
      "Episode:451 meanR:189.7200 loss:9646474526720.0000 explore_p:0.0118\n",
      "Episode:452 meanR:189.7100 loss:9766011142144.0000 explore_p:0.0118\n",
      "Episode:453 meanR:189.7700 loss:9692395864064.0000 explore_p:0.0118\n",
      "Episode:454 meanR:189.8800 loss:9474801664000.0000 explore_p:0.0117\n",
      "Episode:455 meanR:190.2700 loss:9289324298240.0000 explore_p:0.0117\n",
      "Episode:456 meanR:190.4500 loss:9076711882752.0000 explore_p:0.0117\n",
      "Episode:457 meanR:190.6200 loss:8986982088704.0000 explore_p:0.0116\n",
      "Episode:458 meanR:190.7800 loss:8865673379840.0000 explore_p:0.0116\n",
      "Episode:459 meanR:190.8100 loss:8885918236672.0000 explore_p:0.0116\n",
      "Episode:460 meanR:190.6200 loss:8947996033024.0000 explore_p:0.0115\n",
      "Episode:461 meanR:190.3200 loss:9136200744960.0000 explore_p:0.0115\n",
      "Episode:462 meanR:190.1200 loss:9298141773824.0000 explore_p:0.0115\n",
      "Episode:463 meanR:190.2500 loss:9373943332864.0000 explore_p:0.0114\n",
      "Episode:464 meanR:190.4700 loss:9243831828480.0000 explore_p:0.0114\n",
      "Episode:465 meanR:190.4600 loss:9146106642432.0000 explore_p:0.0114\n",
      "Episode:466 meanR:190.6000 loss:8991783518208.0000 explore_p:0.0114\n",
      "Episode:467 meanR:190.8100 loss:8639173099520.0000 explore_p:0.0113\n",
      "Episode:468 meanR:190.8000 loss:8653935476736.0000 explore_p:0.0113\n",
      "Episode:469 meanR:191.0900 loss:8673418018816.0000 explore_p:0.0113\n",
      "Episode:470 meanR:191.2100 loss:8662536421376.0000 explore_p:0.0113\n",
      "Episode:471 meanR:191.4100 loss:8779511365632.0000 explore_p:0.0112\n",
      "Episode:472 meanR:191.4400 loss:8846253752320.0000 explore_p:0.0112\n",
      "Episode:473 meanR:191.4800 loss:8893266657280.0000 explore_p:0.0112\n",
      "Episode:474 meanR:191.4300 loss:9068836028416.0000 explore_p:0.0112\n",
      "Episode:475 meanR:191.5500 loss:9232759914496.0000 explore_p:0.0111\n",
      "Episode:476 meanR:191.7100 loss:9298623070208.0000 explore_p:0.0111\n",
      "Episode:477 meanR:191.3000 loss:9407867912192.0000 explore_p:0.0111\n",
      "Episode:478 meanR:191.0000 loss:9532548841472.0000 explore_p:0.0111\n",
      "Episode:479 meanR:191.1900 loss:9486384234496.0000 explore_p:0.0111\n",
      "Episode:480 meanR:191.1000 loss:9323844468736.0000 explore_p:0.0110\n",
      "Episode:481 meanR:190.9900 loss:9317840322560.0000 explore_p:0.0110\n",
      "Episode:482 meanR:191.1900 loss:9139320258560.0000 explore_p:0.0110\n",
      "Episode:483 meanR:191.0900 loss:8985795100672.0000 explore_p:0.0110\n",
      "Episode:484 meanR:191.0400 loss:8951372447744.0000 explore_p:0.0110\n",
      "Episode:485 meanR:191.0700 loss:8951035854848.0000 explore_p:0.0109\n",
      "Episode:486 meanR:191.0400 loss:9042778914816.0000 explore_p:0.0109\n",
      "Episode:487 meanR:191.2300 loss:9071214198784.0000 explore_p:0.0109\n",
      "Episode:488 meanR:191.2900 loss:9094756827136.0000 explore_p:0.0109\n",
      "Episode:489 meanR:191.1400 loss:9222541541376.0000 explore_p:0.0109\n",
      "Episode:490 meanR:191.1900 loss:9269465317376.0000 explore_p:0.0109\n",
      "Episode:491 meanR:191.2100 loss:9119268339712.0000 explore_p:0.0108\n",
      "Episode:492 meanR:191.3300 loss:8985661931520.0000 explore_p:0.0108\n",
      "Episode:493 meanR:191.5700 loss:8787382501376.0000 explore_p:0.0108\n",
      "Episode:494 meanR:191.5500 loss:8469725839360.0000 explore_p:0.0108\n",
      "Episode:495 meanR:191.3900 loss:8703974047744.0000 explore_p:0.0108\n",
      "Episode:496 meanR:191.5800 loss:8807024427008.0000 explore_p:0.0108\n",
      "Episode:497 meanR:191.4200 loss:9027891232768.0000 explore_p:0.0107\n",
      "Episode:498 meanR:191.3500 loss:9209733185536.0000 explore_p:0.0107\n",
      "Episode:499 meanR:191.6500 loss:9340418260992.0000 explore_p:0.0107\n",
      "Episode:500 meanR:191.5200 loss:9350239223808.0000 explore_p:0.0107\n",
      "Episode:501 meanR:191.4400 loss:9299142115328.0000 explore_p:0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:502 meanR:191.5100 loss:9318250315776.0000 explore_p:0.0107\n",
      "Episode:503 meanR:191.3700 loss:9208925782016.0000 explore_p:0.0107\n",
      "Episode:504 meanR:191.6000 loss:9242964656128.0000 explore_p:0.0107\n",
      "Episode:505 meanR:191.2600 loss:9412776296448.0000 explore_p:0.0106\n",
      "Episode:506 meanR:191.1900 loss:9372949282816.0000 explore_p:0.0106\n",
      "Episode:507 meanR:191.4100 loss:9404943433728.0000 explore_p:0.0106\n",
      "Episode:508 meanR:191.1700 loss:9546430939136.0000 explore_p:0.0106\n",
      "Episode:509 meanR:191.1700 loss:9421291782144.0000 explore_p:0.0106\n",
      "Episode:510 meanR:191.2100 loss:9476172152832.0000 explore_p:0.0106\n",
      "Episode:511 meanR:191.3400 loss:9367064674304.0000 explore_p:0.0106\n",
      "Episode:512 meanR:191.5500 loss:9169801314304.0000 explore_p:0.0106\n",
      "Episode:513 meanR:191.3800 loss:9120174309376.0000 explore_p:0.0106\n",
      "Episode:514 meanR:191.5500 loss:9034291740672.0000 explore_p:0.0105\n",
      "Episode:515 meanR:191.5300 loss:8851408551936.0000 explore_p:0.0105\n",
      "Episode:516 meanR:191.5900 loss:8962824994816.0000 explore_p:0.0105\n",
      "Episode:517 meanR:191.6200 loss:9124887658496.0000 explore_p:0.0105\n",
      "Episode:518 meanR:191.3900 loss:9239159373824.0000 explore_p:0.0105\n",
      "Episode:519 meanR:191.2800 loss:9252723752960.0000 explore_p:0.0105\n",
      "Episode:520 meanR:191.4400 loss:9394969378816.0000 explore_p:0.0105\n",
      "Episode:521 meanR:191.5000 loss:9249293860864.0000 explore_p:0.0105\n",
      "Episode:522 meanR:191.5100 loss:9144149999616.0000 explore_p:0.0105\n",
      "Episode:523 meanR:191.3400 loss:9167361277952.0000 explore_p:0.0105\n",
      "Episode:524 meanR:191.2400 loss:9237227896832.0000 explore_p:0.0104\n",
      "Episode:525 meanR:191.2600 loss:9281970634752.0000 explore_p:0.0104\n",
      "Episode:526 meanR:191.2600 loss:9385251176448.0000 explore_p:0.0104\n",
      "Episode:527 meanR:191.6600 loss:9474163081216.0000 explore_p:0.0104\n",
      "Episode:528 meanR:191.8900 loss:9115490320384.0000 explore_p:0.0104\n",
      "Episode:529 meanR:191.9700 loss:8710253969408.0000 explore_p:0.0104\n",
      "Episode:530 meanR:192.0400 loss:8605675290624.0000 explore_p:0.0104\n",
      "Episode:531 meanR:191.9500 loss:8714389553152.0000 explore_p:0.0104\n",
      "Episode:532 meanR:192.1100 loss:8875264704512.0000 explore_p:0.0104\n",
      "Episode:533 meanR:191.7800 loss:9024914325504.0000 explore_p:0.0104\n",
      "Episode:534 meanR:192.0100 loss:9146807091200.0000 explore_p:0.0104\n",
      "Episode:535 meanR:192.0600 loss:9296974708736.0000 explore_p:0.0104\n",
      "Episode:536 meanR:192.2000 loss:9273091293184.0000 explore_p:0.0104\n",
      "Episode:537 meanR:192.3200 loss:9130099081216.0000 explore_p:0.0103\n",
      "Episode:538 meanR:192.2200 loss:9034559127552.0000 explore_p:0.0103\n",
      "Episode:539 meanR:192.1900 loss:9108470104064.0000 explore_p:0.0103\n",
      "Episode:540 meanR:192.2300 loss:9165392052224.0000 explore_p:0.0103\n",
      "Episode:541 meanR:192.0200 loss:9098294722560.0000 explore_p:0.0103\n",
      "Episode:542 meanR:191.9300 loss:9178512883712.0000 explore_p:0.0103\n",
      "Episode:543 meanR:192.0300 loss:9142021390336.0000 explore_p:0.0103\n",
      "Episode:544 meanR:192.1200 loss:9035734581248.0000 explore_p:0.0103\n",
      "Episode:545 meanR:192.3400 loss:8879834398720.0000 explore_p:0.0103\n",
      "Episode:546 meanR:192.2100 loss:8946652807168.0000 explore_p:0.0103\n",
      "Episode:547 meanR:192.3500 loss:8963744595968.0000 explore_p:0.0103\n",
      "Episode:548 meanR:192.7300 loss:9010025594880.0000 explore_p:0.0103\n",
      "Episode:549 meanR:192.8300 loss:9019430273024.0000 explore_p:0.0103\n",
      "Episode:550 meanR:192.7800 loss:9115673821184.0000 explore_p:0.0103\n",
      "Episode:551 meanR:192.6700 loss:9249049542656.0000 explore_p:0.0103\n",
      "Episode:552 meanR:192.8400 loss:9256691564544.0000 explore_p:0.0103\n",
      "Episode:553 meanR:192.7100 loss:9353447866368.0000 explore_p:0.0103\n",
      "Episode:554 meanR:192.4900 loss:9575043432448.0000 explore_p:0.0103\n",
      "Episode:555 meanR:192.3400 loss:9647780003840.0000 explore_p:0.0102\n",
      "Episode:556 meanR:192.2600 loss:9497263210496.0000 explore_p:0.0102\n",
      "Episode:557 meanR:192.2900 loss:9400896978944.0000 explore_p:0.0102\n",
      "Episode:558 meanR:192.3500 loss:9293754531840.0000 explore_p:0.0102\n",
      "Episode:559 meanR:192.4500 loss:9012328267776.0000 explore_p:0.0102\n",
      "Episode:560 meanR:192.3300 loss:8909280509952.0000 explore_p:0.0102\n",
      "Episode:561 meanR:192.5700 loss:8876640436224.0000 explore_p:0.0102\n",
      "Episode:562 meanR:192.7900 loss:8872388460544.0000 explore_p:0.0102\n",
      "Episode:563 meanR:192.6800 loss:8959767347200.0000 explore_p:0.0102\n",
      "Episode:564 meanR:192.6300 loss:8981128937472.0000 explore_p:0.0102\n",
      "Episode:565 meanR:192.6100 loss:8963427926016.0000 explore_p:0.0102\n",
      "Episode:566 meanR:192.5000 loss:8920566333440.0000 explore_p:0.0102\n",
      "Episode:567 meanR:192.2700 loss:9020023767040.0000 explore_p:0.0102\n",
      "Episode:568 meanR:192.3700 loss:9016648400896.0000 explore_p:0.0102\n",
      "Episode:569 meanR:192.2100 loss:8957134372864.0000 explore_p:0.0102\n",
      "Episode:570 meanR:192.0500 loss:9065461710848.0000 explore_p:0.0102\n",
      "Episode:571 meanR:192.0300 loss:9140222033920.0000 explore_p:0.0102\n",
      "Episode:572 meanR:192.2500 loss:9014741041152.0000 explore_p:0.0102\n",
      "Episode:573 meanR:192.3000 loss:8940469354496.0000 explore_p:0.0102\n",
      "Episode:574 meanR:192.3300 loss:9097784066048.0000 explore_p:0.0102\n",
      "Episode:575 meanR:192.1300 loss:9195637178368.0000 explore_p:0.0102\n",
      "Episode:576 meanR:192.1600 loss:9230984675328.0000 explore_p:0.0102\n",
      "Episode:577 meanR:192.3900 loss:9267921813504.0000 explore_p:0.0102\n",
      "Episode:578 meanR:192.5900 loss:9314010923008.0000 explore_p:0.0102\n",
      "Episode:579 meanR:192.5800 loss:9180581724160.0000 explore_p:0.0102\n",
      "Episode:580 meanR:192.7600 loss:8864023969792.0000 explore_p:0.0102\n",
      "Episode:581 meanR:192.5600 loss:8873767337984.0000 explore_p:0.0101\n",
      "Episode:582 meanR:192.2800 loss:9008250355712.0000 explore_p:0.0101\n",
      "Episode:583 meanR:192.2800 loss:9138485592064.0000 explore_p:0.0101\n",
      "Episode:584 meanR:192.2800 loss:9226720116736.0000 explore_p:0.0101\n",
      "Episode:585 meanR:192.2000 loss:9303237853184.0000 explore_p:0.0101\n",
      "Episode:586 meanR:192.2600 loss:9449994452992.0000 explore_p:0.0101\n",
      "Episode:587 meanR:192.1400 loss:9325197131776.0000 explore_p:0.0101\n",
      "Episode:588 meanR:192.2300 loss:9193413148672.0000 explore_p:0.0101\n",
      "Episode:589 meanR:192.4100 loss:9181561094144.0000 explore_p:0.0101\n",
      "Episode:590 meanR:192.3300 loss:9194859134976.0000 explore_p:0.0101\n",
      "Episode:591 meanR:192.0900 loss:9159501152256.0000 explore_p:0.0101\n",
      "Episode:592 meanR:192.0600 loss:9095893483520.0000 explore_p:0.0101\n",
      "Episode:593 meanR:191.8800 loss:9038231240704.0000 explore_p:0.0101\n",
      "Episode:594 meanR:191.7900 loss:8985501499392.0000 explore_p:0.0101\n",
      "Episode:595 meanR:191.9000 loss:9025170178048.0000 explore_p:0.0101\n",
      "Episode:596 meanR:191.6800 loss:9118926503936.0000 explore_p:0.0101\n",
      "Episode:597 meanR:191.8400 loss:9188112596992.0000 explore_p:0.0101\n",
      "Episode:598 meanR:191.9100 loss:9273578881024.0000 explore_p:0.0101\n",
      "Episode:599 meanR:191.7500 loss:9334067036160.0000 explore_p:0.0101\n",
      "Episode:600 meanR:191.8300 loss:9340346957824.0000 explore_p:0.0101\n",
      "Episode:601 meanR:191.9400 loss:9269807153152.0000 explore_p:0.0101\n",
      "Episode:602 meanR:191.7900 loss:9199825190912.0000 explore_p:0.0101\n",
      "Episode:603 meanR:191.8300 loss:9250371796992.0000 explore_p:0.0101\n",
      "Episode:604 meanR:191.9900 loss:9184953237504.0000 explore_p:0.0101\n",
      "Episode:605 meanR:192.0200 loss:9187172024320.0000 explore_p:0.0101\n",
      "Episode:606 meanR:192.0400 loss:9204514422784.0000 explore_p:0.0101\n",
      "Episode:607 meanR:192.1000 loss:9208547246080.0000 explore_p:0.0101\n",
      "Episode:608 meanR:192.1500 loss:9194979721216.0000 explore_p:0.0101\n",
      "Episode:609 meanR:192.2200 loss:9179899101184.0000 explore_p:0.0101\n",
      "Episode:610 meanR:192.1900 loss:9308598173696.0000 explore_p:0.0101\n",
      "Episode:611 meanR:191.9100 loss:9356654411776.0000 explore_p:0.0101\n",
      "Episode:612 meanR:191.6800 loss:9474428370944.0000 explore_p:0.0101\n",
      "Episode:613 meanR:191.7300 loss:9562185793536.0000 explore_p:0.0101\n",
      "Episode:614 meanR:191.3600 loss:9669885034496.0000 explore_p:0.0101\n",
      "Episode:615 meanR:191.1500 loss:9826861056000.0000 explore_p:0.0101\n",
      "Episode:616 meanR:191.0200 loss:9859309240320.0000 explore_p:0.0101\n",
      "Episode:617 meanR:191.2600 loss:9704453439488.0000 explore_p:0.0101\n",
      "Episode:618 meanR:191.4600 loss:9560427331584.0000 explore_p:0.0101\n",
      "Episode:619 meanR:191.4300 loss:9445465653248.0000 explore_p:0.0101\n",
      "Episode:620 meanR:191.2200 loss:9417999253504.0000 explore_p:0.0101\n",
      "Episode:621 meanR:191.0800 loss:9287684325376.0000 explore_p:0.0101\n",
      "Episode:622 meanR:191.1100 loss:9227415322624.0000 explore_p:0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:623 meanR:191.2100 loss:9347945988096.0000 explore_p:0.0101\n",
      "Episode:624 meanR:191.3900 loss:9438707580928.0000 explore_p:0.0101\n",
      "Episode:625 meanR:191.4200 loss:9348905435136.0000 explore_p:0.0101\n",
      "Episode:626 meanR:191.4000 loss:9257536716800.0000 explore_p:0.0101\n",
      "Episode:627 meanR:191.0300 loss:9378786705408.0000 explore_p:0.0101\n",
      "Episode:628 meanR:190.7600 loss:9397003616256.0000 explore_p:0.0101\n",
      "Episode:629 meanR:190.6000 loss:9403985035264.0000 explore_p:0.0101\n",
      "Episode:630 meanR:190.6700 loss:9280436568064.0000 explore_p:0.0101\n",
      "Episode:631 meanR:190.7500 loss:9220837605376.0000 explore_p:0.0101\n",
      "Episode:632 meanR:190.7700 loss:9209777225728.0000 explore_p:0.0101\n",
      "Episode:633 meanR:190.8100 loss:9060213587968.0000 explore_p:0.0101\n",
      "Episode:634 meanR:190.8500 loss:8956346892288.0000 explore_p:0.0101\n",
      "Episode:635 meanR:190.8300 loss:9072447324160.0000 explore_p:0.0101\n",
      "Episode:636 meanR:190.6500 loss:9226313269248.0000 explore_p:0.0101\n",
      "Episode:637 meanR:190.6900 loss:9230841020416.0000 explore_p:0.0101\n",
      "Episode:638 meanR:190.7400 loss:9163919851520.0000 explore_p:0.0101\n",
      "Episode:639 meanR:190.9900 loss:9147325087744.0000 explore_p:0.0100\n",
      "Episode:640 meanR:190.8200 loss:9228541493248.0000 explore_p:0.0100\n",
      "Episode:641 meanR:190.8200 loss:9172770881536.0000 explore_p:0.0100\n",
      "Episode:642 meanR:190.6700 loss:9213144203264.0000 explore_p:0.0100\n",
      "Episode:643 meanR:190.3900 loss:9378702819328.0000 explore_p:0.0100\n",
      "Episode:644 meanR:190.3200 loss:9548395970560.0000 explore_p:0.0100\n",
      "Episode:645 meanR:190.2400 loss:9559958618112.0000 explore_p:0.0100\n",
      "Episode:646 meanR:190.1800 loss:9456085630976.0000 explore_p:0.0100\n",
      "Episode:647 meanR:190.3800 loss:9395061653504.0000 explore_p:0.0100\n",
      "Episode:648 meanR:190.2500 loss:9155825893376.0000 explore_p:0.0100\n",
      "Episode:649 meanR:190.1200 loss:9120536068096.0000 explore_p:0.0100\n",
      "Episode:650 meanR:190.3300 loss:9097774628864.0000 explore_p:0.0100\n",
      "Episode:651 meanR:190.4200 loss:9078861463552.0000 explore_p:0.0100\n",
      "Episode:652 meanR:190.5700 loss:9029857312768.0000 explore_p:0.0100\n",
      "Episode:653 meanR:190.8500 loss:9033871261696.0000 explore_p:0.0100\n",
      "Episode:654 meanR:191.2200 loss:8793935052800.0000 explore_p:0.0100\n",
      "Episode:655 meanR:191.3800 loss:8618547085312.0000 explore_p:0.0100\n",
      "Episode:656 meanR:191.2200 loss:8748208750592.0000 explore_p:0.0100\n",
      "Episode:657 meanR:191.0600 loss:8974261813248.0000 explore_p:0.0100\n",
      "Episode:658 meanR:190.9200 loss:9058096513024.0000 explore_p:0.0100\n",
      "Episode:659 meanR:190.7600 loss:9204597260288.0000 explore_p:0.0100\n",
      "Episode:660 meanR:190.9000 loss:9315422306304.0000 explore_p:0.0100\n",
      "Episode:661 meanR:191.0000 loss:9294583955456.0000 explore_p:0.0100\n",
      "Episode:662 meanR:191.0000 loss:9026944368640.0000 explore_p:0.0100\n",
      "Episode:663 meanR:191.0700 loss:8889764413440.0000 explore_p:0.0100\n",
      "Episode:664 meanR:190.8800 loss:8856880021504.0000 explore_p:0.0100\n",
      "Episode:665 meanR:191.1800 loss:8717674217472.0000 explore_p:0.0100\n",
      "Episode:666 meanR:190.9300 loss:8976334848000.0000 explore_p:0.0100\n",
      "Episode:667 meanR:191.1400 loss:8976206921728.0000 explore_p:0.0100\n",
      "Episode:668 meanR:191.1400 loss:8956061679616.0000 explore_p:0.0100\n",
      "Episode:669 meanR:191.1500 loss:8862364073984.0000 explore_p:0.0100\n",
      "Episode:670 meanR:191.2600 loss:9030011453440.0000 explore_p:0.0100\n",
      "Episode:671 meanR:191.1800 loss:9093178720256.0000 explore_p:0.0100\n",
      "Episode:672 meanR:190.8600 loss:9119449743360.0000 explore_p:0.0100\n",
      "Episode:673 meanR:190.7200 loss:9275754676224.0000 explore_p:0.0100\n",
      "Episode:674 meanR:190.7900 loss:9463515840512.0000 explore_p:0.0100\n",
      "Episode:675 meanR:190.8500 loss:9518077444096.0000 explore_p:0.0100\n",
      "Episode:676 meanR:191.1200 loss:9423898542080.0000 explore_p:0.0100\n",
      "Episode:677 meanR:190.9500 loss:9306179108864.0000 explore_p:0.0100\n",
      "Episode:678 meanR:191.0100 loss:9144768659456.0000 explore_p:0.0100\n",
      "Episode:679 meanR:191.0400 loss:8886371221504.0000 explore_p:0.0100\n",
      "Episode:680 meanR:190.7700 loss:8872466055168.0000 explore_p:0.0100\n",
      "Episode:681 meanR:190.7000 loss:9128813527040.0000 explore_p:0.0100\n",
      "Episode:682 meanR:190.7900 loss:9309968662528.0000 explore_p:0.0100\n",
      "Episode:683 meanR:190.8000 loss:9318799769600.0000 explore_p:0.0100\n",
      "Episode:684 meanR:190.7700 loss:9426728648704.0000 explore_p:0.0100\n",
      "Episode:685 meanR:190.6800 loss:9559178477568.0000 explore_p:0.0100\n",
      "Episode:686 meanR:190.5500 loss:9625887834112.0000 explore_p:0.0100\n",
      "Episode:687 meanR:190.7500 loss:9390742568960.0000 explore_p:0.0100\n",
      "Episode:688 meanR:190.6200 loss:9257690857472.0000 explore_p:0.0100\n",
      "Episode:689 meanR:190.5700 loss:9302345515008.0000 explore_p:0.0100\n",
      "Episode:690 meanR:190.4000 loss:9366653632512.0000 explore_p:0.0100\n",
      "Episode:691 meanR:190.2500 loss:9365922775040.0000 explore_p:0.0100\n",
      "Episode:692 meanR:190.0400 loss:9392132980736.0000 explore_p:0.0100\n",
      "Episode:693 meanR:189.8000 loss:9713567662080.0000 explore_p:0.0100\n",
      "Episode:694 meanR:189.7100 loss:9712699441152.0000 explore_p:0.0100\n",
      "Episode:695 meanR:189.8200 loss:9703993114624.0000 explore_p:0.0100\n",
      "Episode:696 meanR:189.8300 loss:9620794900480.0000 explore_p:0.0100\n",
      "Episode:697 meanR:189.8700 loss:9511355023360.0000 explore_p:0.0100\n",
      "Episode:698 meanR:189.9200 loss:9368466620416.0000 explore_p:0.0100\n",
      "Episode:699 meanR:189.7300 loss:9324967493632.0000 explore_p:0.0100\n",
      "Episode:700 meanR:189.7500 loss:9348840423424.0000 explore_p:0.0100\n",
      "Episode:701 meanR:189.7500 loss:9358922481664.0000 explore_p:0.0100\n",
      "Episode:702 meanR:189.7400 loss:9282420473856.0000 explore_p:0.0100\n",
      "Episode:703 meanR:189.8500 loss:9293435764736.0000 explore_p:0.0100\n",
      "Episode:704 meanR:189.8000 loss:9195025858560.0000 explore_p:0.0100\n",
      "Episode:705 meanR:189.9000 loss:9028825513984.0000 explore_p:0.0100\n",
      "Episode:706 meanR:189.7300 loss:9099212226560.0000 explore_p:0.0100\n",
      "Episode:707 meanR:189.9000 loss:9107662700544.0000 explore_p:0.0100\n",
      "Episode:708 meanR:189.8400 loss:9105460690944.0000 explore_p:0.0100\n",
      "Episode:709 meanR:189.7800 loss:9197882179584.0000 explore_p:0.0100\n",
      "Episode:710 meanR:189.8600 loss:9350443696128.0000 explore_p:0.0100\n",
      "Episode:711 meanR:190.1800 loss:9275742093312.0000 explore_p:0.0100\n",
      "Episode:712 meanR:190.1300 loss:9226141302784.0000 explore_p:0.0100\n",
      "Episode:713 meanR:190.0600 loss:9451439390720.0000 explore_p:0.0100\n",
      "Episode:714 meanR:190.2500 loss:9425501814784.0000 explore_p:0.0100\n",
      "Episode:715 meanR:190.4500 loss:9339090763776.0000 explore_p:0.0100\n",
      "Episode:716 meanR:190.6900 loss:9282574614528.0000 explore_p:0.0100\n",
      "Episode:717 meanR:190.7900 loss:9201588895744.0000 explore_p:0.0100\n",
      "Episode:718 meanR:190.9200 loss:8707078881280.0000 explore_p:0.0100\n",
      "Episode:719 meanR:191.0900 loss:8585005760512.0000 explore_p:0.0100\n",
      "Episode:720 meanR:191.3300 loss:8596506017792.0000 explore_p:0.0100\n",
      "Episode:721 meanR:191.2100 loss:8580278255616.0000 explore_p:0.0100\n",
      "Episode:722 meanR:191.4900 loss:8641953923072.0000 explore_p:0.0100\n",
      "Episode:723 meanR:191.5200 loss:8842719002624.0000 explore_p:0.0100\n",
      "Episode:724 meanR:191.6800 loss:8810714365952.0000 explore_p:0.0100\n",
      "Episode:725 meanR:191.6000 loss:8909897072640.0000 explore_p:0.0100\n",
      "Episode:726 meanR:191.5900 loss:8962515664896.0000 explore_p:0.0100\n",
      "Episode:727 meanR:191.6700 loss:9064103804928.0000 explore_p:0.0100\n",
      "Episode:728 meanR:191.8100 loss:9166178484224.0000 explore_p:0.0100\n",
      "Episode:729 meanR:192.0700 loss:9082602782720.0000 explore_p:0.0100\n",
      "Episode:730 meanR:192.0000 loss:8982873767936.0000 explore_p:0.0100\n",
      "Episode:731 meanR:192.0000 loss:8984034541568.0000 explore_p:0.0100\n",
      "Episode:732 meanR:192.0700 loss:8873825009664.0000 explore_p:0.0100\n",
      "Episode:733 meanR:192.0600 loss:8876356272128.0000 explore_p:0.0100\n",
      "Episode:734 meanR:191.8900 loss:9074753142784.0000 explore_p:0.0100\n",
      "Episode:735 meanR:191.9700 loss:9206217310208.0000 explore_p:0.0100\n",
      "Episode:736 meanR:192.1700 loss:9177839697920.0000 explore_p:0.0100\n",
      "Episode:737 meanR:192.1300 loss:9116323938304.0000 explore_p:0.0100\n",
      "Episode:738 meanR:192.2200 loss:9004797394944.0000 explore_p:0.0100\n",
      "Episode:739 meanR:192.0800 loss:8920121737216.0000 explore_p:0.0100\n",
      "Episode:740 meanR:192.2900 loss:8845986365440.0000 explore_p:0.0100\n",
      "Episode:741 meanR:192.1500 loss:8941051314176.0000 explore_p:0.0100\n",
      "Episode:742 meanR:192.3500 loss:8986734624768.0000 explore_p:0.0100\n",
      "Episode:743 meanR:192.3900 loss:9110960472064.0000 explore_p:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:744 meanR:192.5200 loss:9235405471744.0000 explore_p:0.0100\n",
      "Episode:745 meanR:192.3900 loss:9231020326912.0000 explore_p:0.0100\n",
      "Episode:746 meanR:192.4700 loss:9307557986304.0000 explore_p:0.0100\n",
      "Episode:747 meanR:192.3800 loss:9261077757952.0000 explore_p:0.0100\n",
      "Episode:748 meanR:192.3800 loss:9241337266176.0000 explore_p:0.0100\n",
      "Episode:749 meanR:192.5100 loss:9229355188224.0000 explore_p:0.0100\n",
      "Episode:750 meanR:192.2700 loss:9316388044800.0000 explore_p:0.0100\n",
      "Episode:751 meanR:192.0600 loss:9425076092928.0000 explore_p:0.0100\n",
      "Episode:752 meanR:191.8300 loss:9508275355648.0000 explore_p:0.0100\n",
      "Episode:753 meanR:191.7100 loss:9615061286912.0000 explore_p:0.0100\n",
      "Episode:754 meanR:191.5600 loss:9585562746880.0000 explore_p:0.0100\n",
      "Episode:755 meanR:191.3300 loss:9571994173440.0000 explore_p:0.0100\n",
      "Episode:756 meanR:191.7200 loss:9244619309056.0000 explore_p:0.0100\n",
      "Episode:757 meanR:191.9200 loss:8926800117760.0000 explore_p:0.0100\n",
      "Episode:758 meanR:191.8100 loss:8965037490176.0000 explore_p:0.0100\n",
      "Episode:759 meanR:191.7200 loss:9077912502272.0000 explore_p:0.0100\n",
      "Episode:760 meanR:191.6000 loss:9147095449600.0000 explore_p:0.0100\n",
      "Episode:761 meanR:191.5000 loss:9186574336000.0000 explore_p:0.0100\n",
      "Episode:762 meanR:191.4600 loss:9312665600000.0000 explore_p:0.0100\n",
      "Episode:763 meanR:191.4500 loss:9331535773696.0000 explore_p:0.0100\n",
      "Episode:764 meanR:191.4900 loss:9179866595328.0000 explore_p:0.0100\n",
      "Episode:765 meanR:191.0700 loss:9165079576576.0000 explore_p:0.0100\n",
      "Episode:766 meanR:191.1700 loss:9257837658112.0000 explore_p:0.0100\n",
      "Episode:767 meanR:191.0000 loss:9379733569536.0000 explore_p:0.0100\n",
      "Episode:768 meanR:190.9800 loss:9417014640640.0000 explore_p:0.0100\n",
      "Episode:769 meanR:190.9100 loss:9374625955840.0000 explore_p:0.0100\n",
      "Episode:770 meanR:190.8800 loss:9333857320960.0000 explore_p:0.0100\n",
      "Episode:771 meanR:191.0800 loss:9183127666688.0000 explore_p:0.0100\n",
      "Episode:772 meanR:191.1700 loss:9047818371072.0000 explore_p:0.0100\n",
      "Episode:773 meanR:191.3100 loss:9018311442432.0000 explore_p:0.0100\n",
      "Episode:774 meanR:191.2800 loss:9140380368896.0000 explore_p:0.0100\n",
      "Episode:775 meanR:191.3600 loss:9167125348352.0000 explore_p:0.0100\n",
      "Episode:776 meanR:191.0700 loss:9188761665536.0000 explore_p:0.0100\n",
      "Episode:777 meanR:191.2200 loss:9242954170368.0000 explore_p:0.0100\n",
      "Episode:778 meanR:191.1000 loss:9210217627648.0000 explore_p:0.0100\n",
      "Episode:779 meanR:191.0300 loss:9165447626752.0000 explore_p:0.0100\n",
      "Episode:780 meanR:191.0700 loss:9132824330240.0000 explore_p:0.0100\n",
      "Episode:781 meanR:191.1500 loss:9225775349760.0000 explore_p:0.0100\n",
      "Episode:782 meanR:191.3200 loss:9204058292224.0000 explore_p:0.0100\n",
      "Episode:783 meanR:191.2500 loss:9260139282432.0000 explore_p:0.0100\n",
      "Episode:784 meanR:191.1400 loss:9331653214208.0000 explore_p:0.0100\n",
      "Episode:785 meanR:191.3300 loss:9352135049216.0000 explore_p:0.0100\n",
      "Episode:786 meanR:191.2800 loss:9363961937920.0000 explore_p:0.0100\n",
      "Episode:787 meanR:190.9900 loss:9404971745280.0000 explore_p:0.0100\n",
      "Episode:788 meanR:191.0400 loss:9506955198464.0000 explore_p:0.0100\n",
      "Episode:789 meanR:191.0800 loss:9440150421504.0000 explore_p:0.0100\n",
      "Episode:790 meanR:191.4300 loss:9266573344768.0000 explore_p:0.0100\n",
      "Episode:791 meanR:191.6200 loss:9150953160704.0000 explore_p:0.0100\n",
      "Episode:792 meanR:191.5300 loss:9127837302784.0000 explore_p:0.0100\n",
      "Episode:793 meanR:191.4700 loss:9255058931712.0000 explore_p:0.0100\n",
      "Episode:794 meanR:191.5600 loss:9288625946624.0000 explore_p:0.0100\n",
      "Episode:795 meanR:191.3600 loss:9373637148672.0000 explore_p:0.0100\n",
      "Episode:796 meanR:191.5500 loss:9604353228800.0000 explore_p:0.0100\n",
      "Episode:797 meanR:191.4400 loss:9578308698112.0000 explore_p:0.0100\n",
      "Episode:798 meanR:191.5400 loss:9323330666496.0000 explore_p:0.0100\n",
      "Episode:799 meanR:191.6000 loss:9191389396992.0000 explore_p:0.0100\n",
      "Episode:800 meanR:191.7800 loss:9107531628544.0000 explore_p:0.0100\n",
      "Episode:801 meanR:191.7400 loss:8999886913536.0000 explore_p:0.0100\n",
      "Episode:802 meanR:191.6000 loss:9115448377344.0000 explore_p:0.0100\n",
      "Episode:803 meanR:191.6400 loss:9172405977088.0000 explore_p:0.0100\n",
      "Episode:804 meanR:191.4100 loss:9257103654912.0000 explore_p:0.0100\n",
      "Episode:805 meanR:191.4700 loss:9256908619776.0000 explore_p:0.0100\n",
      "Episode:806 meanR:191.7400 loss:9275198930944.0000 explore_p:0.0100\n",
      "Episode:807 meanR:191.5800 loss:9177706528768.0000 explore_p:0.0100\n",
      "Episode:808 meanR:191.8100 loss:9012552663040.0000 explore_p:0.0100\n",
      "Episode:809 meanR:191.7200 loss:8999811416064.0000 explore_p:0.0100\n",
      "Episode:810 meanR:191.7300 loss:9035157864448.0000 explore_p:0.0100\n",
      "Episode:811 meanR:191.5700 loss:9106344640512.0000 explore_p:0.0100\n",
      "Episode:812 meanR:191.5200 loss:9233984651264.0000 explore_p:0.0100\n",
      "Episode:813 meanR:191.4500 loss:9464291786752.0000 explore_p:0.0100\n",
      "Episode:814 meanR:191.3600 loss:9735325614080.0000 explore_p:0.0100\n",
      "Episode:815 meanR:191.2200 loss:9690050199552.0000 explore_p:0.0100\n",
      "Episode:816 meanR:191.1500 loss:9713514184704.0000 explore_p:0.0100\n",
      "Episode:817 meanR:190.9100 loss:9737331539968.0000 explore_p:0.0100\n",
      "Episode:818 meanR:190.5300 loss:9617518100480.0000 explore_p:0.0100\n",
      "Episode:819 meanR:190.3500 loss:9496903548928.0000 explore_p:0.0100\n",
      "Episode:820 meanR:190.2700 loss:9458292883456.0000 explore_p:0.0100\n",
      "Episode:821 meanR:190.3200 loss:9369109397504.0000 explore_p:0.0100\n",
      "Episode:822 meanR:190.1500 loss:9302062399488.0000 explore_p:0.0100\n",
      "Episode:823 meanR:190.1000 loss:9284567957504.0000 explore_p:0.0100\n",
      "Episode:824 meanR:189.8300 loss:9289055862784.0000 explore_p:0.0100\n",
      "Episode:825 meanR:189.8300 loss:9332627341312.0000 explore_p:0.0100\n",
      "Episode:826 meanR:189.6100 loss:9453615185920.0000 explore_p:0.0100\n",
      "Episode:827 meanR:189.8500 loss:9538449178624.0000 explore_p:0.0100\n",
      "Episode:828 meanR:190.1100 loss:9302158868480.0000 explore_p:0.0100\n",
      "Episode:829 meanR:189.8100 loss:9069168427008.0000 explore_p:0.0100\n",
      "Episode:830 meanR:189.9500 loss:8807666155520.0000 explore_p:0.0100\n",
      "Episode:831 meanR:189.9500 loss:8561298505728.0000 explore_p:0.0100\n",
      "Episode:832 meanR:189.7300 loss:8850221563904.0000 explore_p:0.0100\n",
      "Episode:833 meanR:189.9400 loss:8925128687616.0000 explore_p:0.0100\n",
      "Episode:834 meanR:189.9600 loss:9027219095552.0000 explore_p:0.0100\n",
      "Episode:835 meanR:190.0900 loss:9074459541504.0000 explore_p:0.0100\n",
      "Episode:836 meanR:189.9200 loss:9178498203648.0000 explore_p:0.0100\n",
      "Episode:837 meanR:189.9600 loss:9071168061440.0000 explore_p:0.0100\n",
      "Episode:838 meanR:189.7000 loss:9075347685376.0000 explore_p:0.0100\n",
      "Episode:839 meanR:189.7700 loss:9150766514176.0000 explore_p:0.0100\n",
      "Episode:840 meanR:189.5700 loss:9232758865920.0000 explore_p:0.0100\n",
      "Episode:841 meanR:189.6600 loss:9363985006592.0000 explore_p:0.0100\n",
      "Episode:842 meanR:189.5100 loss:9425375985664.0000 explore_p:0.0100\n",
      "Episode:843 meanR:189.6300 loss:9481482141696.0000 explore_p:0.0100\n",
      "Episode:844 meanR:189.4800 loss:9461268742144.0000 explore_p:0.0100\n",
      "Episode:845 meanR:189.5000 loss:9498935689216.0000 explore_p:0.0100\n",
      "Episode:846 meanR:189.5600 loss:9380924751872.0000 explore_p:0.0100\n",
      "Episode:847 meanR:189.4700 loss:9322506485760.0000 explore_p:0.0100\n",
      "Episode:848 meanR:189.2600 loss:9351688355840.0000 explore_p:0.0100\n",
      "Episode:849 meanR:189.3400 loss:9466785300480.0000 explore_p:0.0100\n",
      "Episode:850 meanR:189.5500 loss:9275362508800.0000 explore_p:0.0100\n",
      "Episode:851 meanR:189.9100 loss:9139518439424.0000 explore_p:0.0100\n",
      "Episode:852 meanR:189.8600 loss:9144996200448.0000 explore_p:0.0100\n",
      "Episode:853 meanR:189.8900 loss:9110989832192.0000 explore_p:0.0100\n",
      "Episode:854 meanR:190.0500 loss:8939724865536.0000 explore_p:0.0100\n",
      "Episode:855 meanR:190.1000 loss:8924839280640.0000 explore_p:0.0100\n",
      "Episode:856 meanR:189.7900 loss:9078551085056.0000 explore_p:0.0100\n",
      "Episode:857 meanR:189.6300 loss:9185525760000.0000 explore_p:0.0100\n",
      "Episode:858 meanR:189.7700 loss:9159927922688.0000 explore_p:0.0100\n",
      "Episode:859 meanR:189.9400 loss:9204430536704.0000 explore_p:0.0100\n",
      "Episode:860 meanR:189.8000 loss:9372696576000.0000 explore_p:0.0100\n",
      "Episode:861 meanR:189.8600 loss:9396998373376.0000 explore_p:0.0100\n",
      "Episode:862 meanR:189.9300 loss:9182104256512.0000 explore_p:0.0100\n",
      "Episode:863 meanR:190.1800 loss:8911705866240.0000 explore_p:0.0100\n",
      "Episode:864 meanR:190.1700 loss:8903343472640.0000 explore_p:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:865 meanR:190.0400 loss:8984964628480.0000 explore_p:0.0100\n",
      "Episode:866 meanR:189.9000 loss:9142580281344.0000 explore_p:0.0100\n",
      "Episode:867 meanR:190.0300 loss:9291082760192.0000 explore_p:0.0100\n",
      "Episode:868 meanR:190.0900 loss:9328081764352.0000 explore_p:0.0100\n",
      "Episode:869 meanR:190.2600 loss:9375043289088.0000 explore_p:0.0100\n",
      "Episode:870 meanR:190.3700 loss:9156319772672.0000 explore_p:0.0100\n",
      "Episode:871 meanR:190.2000 loss:8907249418240.0000 explore_p:0.0100\n",
      "Episode:872 meanR:190.1200 loss:8914496126976.0000 explore_p:0.0100\n",
      "Episode:873 meanR:190.1700 loss:8973989183488.0000 explore_p:0.0100\n",
      "Episode:874 meanR:190.2300 loss:9072077176832.0000 explore_p:0.0100\n",
      "Episode:875 meanR:190.1100 loss:9225973530624.0000 explore_p:0.0100\n",
      "Episode:876 meanR:190.0900 loss:9348546822144.0000 explore_p:0.0100\n",
      "Episode:877 meanR:189.8500 loss:9429932048384.0000 explore_p:0.0100\n",
      "Episode:878 meanR:189.5400 loss:9598339645440.0000 explore_p:0.0100\n",
      "Episode:879 meanR:189.6100 loss:9688663982080.0000 explore_p:0.0100\n",
      "Episode:880 meanR:189.6400 loss:9599053725696.0000 explore_p:0.0100\n",
      "Episode:881 meanR:189.5400 loss:9649178804224.0000 explore_p:0.0100\n",
      "Episode:882 meanR:189.4900 loss:9665039564800.0000 explore_p:0.0100\n",
      "Episode:883 meanR:189.4800 loss:9528503435264.0000 explore_p:0.0100\n",
      "Episode:884 meanR:189.5800 loss:9389829259264.0000 explore_p:0.0100\n",
      "Episode:885 meanR:189.5700 loss:9446953582592.0000 explore_p:0.0100\n",
      "Episode:886 meanR:189.7500 loss:9381019123712.0000 explore_p:0.0100\n",
      "Episode:887 meanR:189.7800 loss:9265275207680.0000 explore_p:0.0100\n",
      "Episode:888 meanR:189.9300 loss:9214193827840.0000 explore_p:0.0100\n",
      "Episode:889 meanR:189.9500 loss:9120934526976.0000 explore_p:0.0100\n",
      "Episode:890 meanR:189.8300 loss:9063087734784.0000 explore_p:0.0100\n",
      "Episode:891 meanR:189.9000 loss:8983225040896.0000 explore_p:0.0100\n",
      "Episode:892 meanR:190.0100 loss:9007585558528.0000 explore_p:0.0100\n",
      "Episode:893 meanR:190.1000 loss:9120217300992.0000 explore_p:0.0100\n",
      "Episode:894 meanR:190.0500 loss:9199056584704.0000 explore_p:0.0100\n",
      "Episode:895 meanR:190.0000 loss:9335309598720.0000 explore_p:0.0100\n",
      "Episode:896 meanR:190.0200 loss:9459336216576.0000 explore_p:0.0100\n",
      "Episode:897 meanR:190.1000 loss:9427087261696.0000 explore_p:0.0100\n",
      "Episode:898 meanR:190.0300 loss:9264827465728.0000 explore_p:0.0100\n",
      "Episode:899 meanR:189.9900 loss:9242897547264.0000 explore_p:0.0100\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episodes_total_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states}) \n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episodes_total_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episodes_total_reward)),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'explore_p:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episodes_total_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episodes_total_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-qn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model-qn.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model-qn.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        for _ in range(111111111111111111):\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {:.2f}'.format(total_reward))\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
