{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning or Q-network (QN)\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  array([-0.04449294, -0.21327739, -0.01225429,  0.26880974]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}],\n",
       " (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.570289819880813 -3.026688140247116\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return actions, states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs_logits, \n",
    "    #                                                               labels=tf.nn.sigmoid(Qs_labels)))    \n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 4) actions:(1000,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print(np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 500               # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.05272068, -0.26944343,  0.07353636,  0.8886292 ]), 1.0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:29.0000 loss:1.5087\n",
      "Episode:1 meanR:19.5000 loss:1.9406\n",
      "Episode:2 meanR:25.3333 loss:4.9186\n",
      "Episode:3 meanR:24.0000 loss:30.0577\n",
      "Episode:4 meanR:24.6000 loss:182.0217\n",
      "Episode:5 meanR:26.3333 loss:942.8831\n",
      "Episode:6 meanR:27.5714 loss:1078.2013\n",
      "Episode:7 meanR:28.3750 loss:1046.4786\n",
      "Episode:8 meanR:28.5556 loss:4776.6450\n",
      "Episode:9 meanR:27.7000 loss:19666.2090\n",
      "Episode:10 meanR:26.9091 loss:58615.6172\n",
      "Episode:11 meanR:27.5833 loss:151073.7188\n",
      "Episode:12 meanR:26.3077 loss:347713.0625\n",
      "Episode:13 meanR:25.2857 loss:508659.7188\n",
      "Episode:14 meanR:24.2000 loss:819122.0000\n",
      "Episode:15 meanR:23.7500 loss:1150222.3750\n",
      "Episode:16 meanR:23.8824 loss:1003401.6875\n",
      "Episode:17 meanR:23.4444 loss:615355.3750\n",
      "Episode:18 meanR:23.1053 loss:427991.7500\n",
      "Episode:19 meanR:23.2000 loss:289963.1875\n",
      "Episode:20 meanR:22.7143 loss:211147.8594\n",
      "Episode:21 meanR:22.6818 loss:166758.6406\n",
      "Episode:22 meanR:22.5652 loss:130900.0781\n",
      "Episode:23 meanR:22.0000 loss:108322.1484\n",
      "Episode:24 meanR:23.4800 loss:65550.6016\n",
      "Episode:25 meanR:23.2308 loss:38990.3281\n",
      "Episode:26 meanR:23.2593 loss:32100.3965\n",
      "Episode:27 meanR:22.8929 loss:27674.9570\n",
      "Episode:28 meanR:22.5862 loss:25054.3301\n",
      "Episode:29 meanR:22.5333 loss:20999.4902\n",
      "Episode:30 meanR:23.1935 loss:16580.3594\n",
      "Episode:31 meanR:24.8750 loss:13226.9072\n",
      "Episode:32 meanR:24.6667 loss:12107.0273\n",
      "Episode:33 meanR:25.2353 loss:11118.1865\n",
      "Episode:34 meanR:25.0000 loss:9589.1348\n",
      "Episode:35 meanR:24.8333 loss:9138.4404\n",
      "Episode:36 meanR:24.8649 loss:8591.0674\n",
      "Episode:37 meanR:25.2895 loss:7428.5762\n",
      "Episode:38 meanR:25.7436 loss:6756.3813\n",
      "Episode:39 meanR:25.5500 loss:6433.1504\n",
      "Episode:40 meanR:26.0000 loss:6604.6113\n",
      "Episode:41 meanR:25.7143 loss:7324.4751\n",
      "Episode:42 meanR:25.6279 loss:8320.6768\n",
      "Episode:43 meanR:25.5000 loss:10164.1377\n",
      "Episode:44 meanR:25.1111 loss:11757.2480\n",
      "Episode:45 meanR:26.5217 loss:14132.9463\n",
      "Episode:46 meanR:26.3617 loss:16367.3467\n",
      "Episode:47 meanR:26.3958 loss:19105.3789\n",
      "Episode:48 meanR:26.7959 loss:20133.9648\n",
      "Episode:49 meanR:27.0400 loss:21622.6641\n",
      "Episode:50 meanR:26.9216 loss:21299.9531\n",
      "Episode:51 meanR:27.0577 loss:22188.0781\n",
      "Episode:52 meanR:26.9057 loss:21555.8594\n",
      "Episode:53 meanR:26.8333 loss:22895.8477\n",
      "Episode:54 meanR:26.5818 loss:24298.8555\n",
      "Episode:55 meanR:26.7679 loss:22304.4902\n",
      "Episode:56 meanR:26.6140 loss:21402.9844\n",
      "Episode:57 meanR:26.5517 loss:20800.6582\n",
      "Episode:58 meanR:26.4068 loss:21168.7168\n",
      "Episode:59 meanR:26.1500 loss:21933.8555\n",
      "Episode:60 meanR:25.9508 loss:21179.6348\n",
      "Episode:61 meanR:25.8871 loss:20056.9668\n",
      "Episode:62 meanR:25.9048 loss:18286.7012\n",
      "Episode:63 meanR:25.8906 loss:17232.4648\n",
      "Episode:64 meanR:27.7846 loss:18938.2852\n",
      "Episode:65 meanR:27.5303 loss:25648.3770\n",
      "Episode:66 meanR:27.7313 loss:25914.8359\n",
      "Episode:67 meanR:27.6029 loss:28038.9941\n",
      "Episode:68 meanR:28.1014 loss:25222.6094\n",
      "Episode:69 meanR:27.8571 loss:26327.0508\n",
      "Episode:70 meanR:28.4648 loss:23582.1680\n",
      "Episode:71 meanR:28.3472 loss:26901.1250\n",
      "Episode:72 meanR:28.1096 loss:29079.3242\n",
      "Episode:73 meanR:28.4865 loss:27953.4180\n",
      "Episode:74 meanR:29.1067 loss:26862.7441\n",
      "Episode:75 meanR:29.0789 loss:25589.5039\n",
      "Episode:76 meanR:29.0130 loss:23598.7324\n",
      "Episode:77 meanR:29.1154 loss:19566.9727\n",
      "Episode:78 meanR:29.2278 loss:17611.5078\n",
      "Episode:79 meanR:29.0125 loss:17474.2129\n",
      "Episode:80 meanR:29.1975 loss:16055.4277\n",
      "Episode:81 meanR:29.0366 loss:17308.6348\n",
      "Episode:82 meanR:28.8795 loss:18518.7891\n",
      "Episode:83 meanR:28.8095 loss:18035.0918\n",
      "Episode:84 meanR:28.9647 loss:16683.0293\n",
      "Episode:85 meanR:28.9884 loss:13415.7256\n",
      "Episode:86 meanR:29.0460 loss:12910.3545\n",
      "Episode:87 meanR:28.8864 loss:11766.6543\n",
      "Episode:88 meanR:29.2135 loss:12106.7637\n",
      "Episode:89 meanR:29.1667 loss:14662.9375\n",
      "Episode:90 meanR:29.0769 loss:13333.9043\n",
      "Episode:91 meanR:28.9783 loss:16468.6074\n",
      "Episode:92 meanR:28.8710 loss:13663.9814\n",
      "Episode:93 meanR:29.1809 loss:15272.1807\n",
      "Episode:94 meanR:29.4000 loss:19159.9473\n",
      "Episode:95 meanR:29.9583 loss:20589.2480\n",
      "Episode:96 meanR:30.3299 loss:21823.5098\n",
      "Episode:97 meanR:30.1735 loss:16583.6660\n",
      "Episode:98 meanR:30.0505 loss:12659.8975\n",
      "Episode:99 meanR:29.9300 loss:11155.5283\n",
      "Episode:100 meanR:29.8200 loss:12675.3955\n",
      "Episode:101 meanR:30.0700 loss:8464.9736\n",
      "Episode:102 meanR:30.5400 loss:9451.5293\n",
      "Episode:103 meanR:30.4500 loss:7995.8872\n",
      "Episode:104 meanR:30.3000 loss:6533.8169\n",
      "Episode:105 meanR:30.0800 loss:7923.4819\n",
      "Episode:106 meanR:29.9000 loss:8269.5459\n",
      "Episode:107 meanR:29.6800 loss:8000.5562\n",
      "Episode:108 meanR:29.8900 loss:7121.5654\n",
      "Episode:109 meanR:29.8400 loss:6287.2749\n",
      "Episode:110 meanR:29.7500 loss:7420.0923\n",
      "Episode:111 meanR:29.9100 loss:6914.4717\n",
      "Episode:112 meanR:29.9100 loss:7659.2144\n",
      "Episode:113 meanR:29.9600 loss:6916.1055\n",
      "Episode:114 meanR:30.0100 loss:5131.1631\n",
      "Episode:115 meanR:30.3200 loss:8255.8838\n",
      "Episode:116 meanR:30.2900 loss:18116.9121\n",
      "Episode:117 meanR:30.2300 loss:18723.0078\n",
      "Episode:118 meanR:30.1700 loss:17259.4590\n",
      "Episode:119 meanR:30.2200 loss:37471.9180\n",
      "Episode:120 meanR:30.7000 loss:55746.6289\n",
      "Episode:121 meanR:30.8800 loss:55047.8633\n",
      "Episode:122 meanR:30.8200 loss:76941.3984\n",
      "Episode:123 meanR:31.0800 loss:47928.8398\n",
      "Episode:124 meanR:30.6900 loss:57641.6875\n",
      "Episode:125 meanR:30.6200 loss:65091.4883\n",
      "Episode:126 meanR:30.5400 loss:47158.4922\n",
      "Episode:127 meanR:30.8900 loss:43945.3320\n",
      "Episode:128 meanR:30.9000 loss:54419.3320\n",
      "Episode:129 meanR:31.2300 loss:37491.9180\n",
      "Episode:130 meanR:31.1400 loss:38584.4297\n",
      "Episode:131 meanR:30.8800 loss:33848.2070\n",
      "Episode:132 meanR:31.6700 loss:44374.8867\n",
      "Episode:133 meanR:31.4300 loss:74910.2344\n",
      "Episode:134 meanR:31.5600 loss:64155.1406\n",
      "Episode:135 meanR:31.7900 loss:59726.5469\n",
      "Episode:136 meanR:31.8500 loss:72712.3594\n",
      "Episode:137 meanR:32.2200 loss:88123.5859\n",
      "Episode:138 meanR:32.0000 loss:135772.8281\n",
      "Episode:139 meanR:32.1900 loss:176827.1719\n",
      "Episode:140 meanR:32.0800 loss:231438.3594\n",
      "Episode:141 meanR:32.2800 loss:310936.9688\n",
      "Episode:142 meanR:32.6800 loss:488899.9375\n",
      "Episode:143 meanR:32.8800 loss:780028.8750\n",
      "Episode:144 meanR:32.9400 loss:1241872.6250\n",
      "Episode:145 meanR:32.6700 loss:2075242.1250\n",
      "Episode:146 meanR:32.7400 loss:4066880.2500\n",
      "Episode:147 meanR:33.7600 loss:11101017.0000\n",
      "Episode:148 meanR:33.5000 loss:25371748.0000\n",
      "Episode:149 meanR:33.9400 loss:27428504.0000\n",
      "Episode:150 meanR:33.9600 loss:21884854.0000\n",
      "Episode:151 meanR:34.0300 loss:19755860.0000\n",
      "Episode:152 meanR:34.5700 loss:12677581.0000\n",
      "Episode:153 meanR:34.7800 loss:12129112.0000\n",
      "Episode:154 meanR:35.5300 loss:14170572.0000\n",
      "Episode:155 meanR:36.5100 loss:30553946.0000\n",
      "Episode:156 meanR:36.6200 loss:4295869.0000\n",
      "Episode:157 meanR:36.9800 loss:3550839.0000\n",
      "Episode:158 meanR:37.3500 loss:5605355.5000\n",
      "Episode:159 meanR:37.7600 loss:6680787.0000\n",
      "Episode:160 meanR:38.6300 loss:4426981.5000\n",
      "Episode:161 meanR:38.7200 loss:8936041.0000\n",
      "Episode:162 meanR:39.1400 loss:9047148.0000\n",
      "Episode:163 meanR:39.1100 loss:15243519.0000\n",
      "Episode:164 meanR:38.7800 loss:13547359.0000\n",
      "Episode:165 meanR:39.0400 loss:12872297.0000\n",
      "Episode:166 meanR:39.6000 loss:12761415.0000\n",
      "Episode:167 meanR:41.6800 loss:18621578.0000\n",
      "Episode:168 meanR:41.6500 loss:75247736.0000\n",
      "Episode:169 meanR:44.1500 loss:80241696.0000\n",
      "Episode:170 meanR:43.6400 loss:202808528.0000\n",
      "Episode:171 meanR:43.6600 loss:136349744.0000\n",
      "Episode:172 meanR:44.7500 loss:53358456.0000\n",
      "Episode:173 meanR:46.1000 loss:6946437.5000\n",
      "Episode:174 meanR:45.7800 loss:5957157.5000\n",
      "Episode:175 meanR:46.1500 loss:6465731.0000\n",
      "Episode:176 meanR:46.3700 loss:6700683.0000\n",
      "Episode:177 meanR:46.8900 loss:7077111.5000\n",
      "Episode:178 meanR:46.7400 loss:10765980.0000\n",
      "Episode:179 meanR:48.9300 loss:20073526.0000\n",
      "Episode:180 meanR:49.2500 loss:1901962.1250\n",
      "Episode:181 meanR:51.3500 loss:890513.5625\n",
      "Episode:182 meanR:52.9500 loss:2409553.7500\n",
      "Episode:183 meanR:54.2700 loss:26190446.0000\n",
      "Episode:184 meanR:54.1400 loss:75732248.0000\n",
      "Episode:185 meanR:54.2100 loss:107485192.0000\n",
      "Episode:186 meanR:55.5200 loss:125103280.0000\n",
      "Episode:187 meanR:56.8600 loss:140508416.0000\n",
      "Episode:188 meanR:57.4000 loss:179989168.0000\n",
      "Episode:189 meanR:57.2600 loss:188535952.0000\n",
      "Episode:190 meanR:58.0500 loss:224275104.0000\n",
      "Episode:191 meanR:59.4100 loss:252348992.0000\n",
      "Episode:192 meanR:59.8400 loss:262545120.0000\n",
      "Episode:193 meanR:59.8100 loss:226885680.0000\n",
      "Episode:194 meanR:61.0600 loss:173735488.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:195 meanR:60.9500 loss:241786768.0000\n",
      "Episode:196 meanR:62.0800 loss:224036736.0000\n",
      "Episode:197 meanR:62.8300 loss:262583360.0000\n",
      "Episode:198 meanR:64.2500 loss:269702880.0000\n",
      "Episode:199 meanR:65.8500 loss:270262016.0000\n",
      "Episode:200 meanR:67.2200 loss:299463520.0000\n",
      "Episode:201 meanR:68.6700 loss:340469760.0000\n",
      "Episode:202 meanR:69.3700 loss:364549472.0000\n",
      "Episode:203 meanR:70.7000 loss:371376768.0000\n",
      "Episode:204 meanR:72.1300 loss:383944928.0000\n",
      "Episode:205 meanR:73.5400 loss:399730496.0000\n",
      "Episode:206 meanR:75.2900 loss:383824128.0000\n",
      "Episode:207 meanR:76.7400 loss:371764192.0000\n",
      "Episode:208 meanR:76.3900 loss:367819648.0000\n",
      "Episode:209 meanR:77.9900 loss:341518528.0000\n",
      "Episode:210 meanR:79.4500 loss:356220992.0000\n",
      "Episode:211 meanR:80.8700 loss:330579712.0000\n",
      "Episode:212 meanR:82.5100 loss:345485536.0000\n",
      "Episode:213 meanR:84.3100 loss:339575776.0000\n",
      "Episode:214 meanR:85.0500 loss:369909344.0000\n",
      "Episode:215 meanR:86.2700 loss:295321408.0000\n",
      "Episode:216 meanR:87.6800 loss:310236448.0000\n",
      "Episode:217 meanR:89.5400 loss:328668960.0000\n",
      "Episode:218 meanR:91.2100 loss:369927584.0000\n",
      "Episode:219 meanR:92.7300 loss:348371424.0000\n",
      "Episode:220 meanR:93.7800 loss:360694688.0000\n",
      "Episode:221 meanR:95.2400 loss:354313952.0000\n",
      "Episode:222 meanR:97.0000 loss:355661056.0000\n",
      "Episode:223 meanR:98.5300 loss:345165920.0000\n",
      "Episode:224 meanR:100.0300 loss:349073856.0000\n",
      "Episode:225 meanR:101.4600 loss:371670880.0000\n",
      "Episode:226 meanR:103.0800 loss:375268288.0000\n",
      "Episode:227 meanR:103.0200 loss:373250464.0000\n",
      "Episode:228 meanR:104.4100 loss:354403520.0000\n",
      "Episode:229 meanR:105.6300 loss:342253664.0000\n",
      "Episode:230 meanR:107.1000 loss:347638752.0000\n",
      "Episode:231 meanR:107.0300 loss:379943328.0000\n",
      "Episode:232 meanR:107.8300 loss:328934976.0000\n",
      "Episode:233 meanR:109.3800 loss:325135904.0000\n",
      "Episode:234 meanR:110.7600 loss:331761504.0000\n",
      "Episode:235 meanR:112.0300 loss:369130496.0000\n",
      "Episode:236 meanR:113.3900 loss:376639776.0000\n",
      "Episode:237 meanR:114.2400 loss:377331648.0000\n",
      "Episode:238 meanR:115.8000 loss:365463072.0000\n",
      "Episode:239 meanR:117.2600 loss:354611648.0000\n",
      "Episode:240 meanR:118.4900 loss:367677440.0000\n",
      "Episode:241 meanR:119.8100 loss:379295872.0000\n",
      "Episode:242 meanR:121.2000 loss:365902176.0000\n",
      "Episode:243 meanR:122.3700 loss:353503328.0000\n",
      "Episode:244 meanR:124.0100 loss:346985376.0000\n",
      "Episode:245 meanR:125.4000 loss:347252224.0000\n",
      "Episode:246 meanR:126.7400 loss:339023424.0000\n",
      "Episode:247 meanR:127.0600 loss:350735648.0000\n",
      "Episode:248 meanR:128.6800 loss:372397440.0000\n",
      "Episode:249 meanR:129.4200 loss:375429664.0000\n",
      "Episode:250 meanR:131.0000 loss:365146336.0000\n",
      "Episode:251 meanR:132.4100 loss:361211552.0000\n",
      "Episode:252 meanR:133.4100 loss:346959456.0000\n",
      "Episode:253 meanR:134.6800 loss:351208704.0000\n",
      "Episode:254 meanR:135.4500 loss:362862816.0000\n",
      "Episode:255 meanR:135.6900 loss:376877984.0000\n",
      "Episode:256 meanR:137.4600 loss:358021664.0000\n",
      "Episode:257 meanR:138.6900 loss:342468704.0000\n",
      "Episode:258 meanR:139.7300 loss:337375552.0000\n",
      "Episode:259 meanR:141.0100 loss:357935744.0000\n",
      "Episode:260 meanR:141.9600 loss:350758464.0000\n",
      "Episode:261 meanR:143.3700 loss:341833696.0000\n",
      "Episode:262 meanR:144.3900 loss:349882912.0000\n",
      "Episode:263 meanR:146.0800 loss:352954880.0000\n",
      "Episode:264 meanR:146.8800 loss:335965664.0000\n",
      "Episode:265 meanR:148.3500 loss:328232672.0000\n",
      "Episode:266 meanR:149.3000 loss:329679392.0000\n",
      "Episode:267 meanR:148.8500 loss:336084096.0000\n",
      "Episode:268 meanR:149.9600 loss:344435328.0000\n",
      "Episode:269 meanR:149.1500 loss:355224224.0000\n",
      "Episode:270 meanR:150.8500 loss:349119328.0000\n",
      "Episode:271 meanR:152.5900 loss:332864160.0000\n",
      "Episode:272 meanR:153.2300 loss:327421376.0000\n",
      "Episode:273 meanR:153.2900 loss:325966848.0000\n",
      "Episode:274 meanR:154.6100 loss:335913856.0000\n",
      "Episode:275 meanR:155.6700 loss:346639808.0000\n",
      "Episode:276 meanR:156.8800 loss:369120480.0000\n",
      "Episode:277 meanR:157.6600 loss:376565696.0000\n",
      "Episode:278 meanR:159.2000 loss:367936256.0000\n",
      "Episode:279 meanR:158.4500 loss:372611776.0000\n",
      "Episode:280 meanR:159.5100 loss:361002592.0000\n",
      "Episode:281 meanR:158.8900 loss:372820320.0000\n",
      "Episode:282 meanR:158.9700 loss:357862720.0000\n",
      "Episode:283 meanR:159.3600 loss:350578464.0000\n",
      "Episode:284 meanR:160.7600 loss:342824928.0000\n",
      "Episode:285 meanR:162.0300 loss:355138400.0000\n",
      "Episode:286 meanR:162.3000 loss:357888608.0000\n",
      "Episode:287 meanR:162.4800 loss:356943776.0000\n",
      "Episode:288 meanR:163.1600 loss:347794080.0000\n",
      "Episode:289 meanR:164.6600 loss:368764416.0000\n",
      "Episode:290 meanR:165.3700 loss:364706752.0000\n",
      "Episode:291 meanR:165.4000 loss:378558592.0000\n",
      "Episode:292 meanR:166.5600 loss:367065056.0000\n",
      "Episode:293 meanR:167.8200 loss:364106880.0000\n",
      "Episode:294 meanR:167.7400 loss:358098816.0000\n",
      "Episode:295 meanR:168.8900 loss:353932512.0000\n",
      "Episode:296 meanR:168.8000 loss:357090208.0000\n",
      "Episode:297 meanR:169.6700 loss:345859584.0000\n",
      "Episode:298 meanR:169.9900 loss:344140640.0000\n",
      "Episode:299 meanR:170.2000 loss:329150816.0000\n",
      "Episode:300 meanR:170.5100 loss:325163040.0000\n",
      "Episode:301 meanR:170.5300 loss:329937728.0000\n",
      "Episode:302 meanR:170.8300 loss:340577536.0000\n",
      "Episode:303 meanR:171.4800 loss:331029152.0000\n",
      "Episode:304 meanR:171.6600 loss:329444736.0000\n",
      "Episode:305 meanR:171.7800 loss:340543392.0000\n",
      "Episode:306 meanR:171.6100 loss:366900288.0000\n",
      "Episode:307 meanR:171.7600 loss:368194720.0000\n",
      "Episode:308 meanR:173.3900 loss:359755456.0000\n",
      "Episode:309 meanR:173.3400 loss:361630336.0000\n",
      "Episode:310 meanR:173.7100 loss:348599520.0000\n",
      "Episode:311 meanR:173.5700 loss:345422240.0000\n",
      "Episode:312 meanR:173.6500 loss:337684192.0000\n",
      "Episode:313 meanR:173.3300 loss:357071616.0000\n",
      "Episode:314 meanR:174.3600 loss:350781344.0000\n",
      "Episode:315 meanR:174.3500 loss:357643200.0000\n",
      "Episode:316 meanR:174.3200 loss:359153120.0000\n",
      "Episode:317 meanR:174.0600 loss:376014112.0000\n",
      "Episode:318 meanR:173.9200 loss:378533952.0000\n",
      "Episode:319 meanR:173.8200 loss:368202976.0000\n",
      "Episode:320 meanR:174.1700 loss:350539680.0000\n",
      "Episode:321 meanR:174.4400 loss:325327488.0000\n",
      "Episode:322 meanR:174.2700 loss:317721056.0000\n",
      "Episode:323 meanR:174.2800 loss:329613248.0000\n",
      "Episode:324 meanR:174.2200 loss:356189824.0000\n",
      "Episode:325 meanR:174.4600 loss:353827584.0000\n",
      "Episode:326 meanR:174.3400 loss:370626048.0000\n",
      "Episode:327 meanR:175.6000 loss:369571072.0000\n",
      "Episode:328 meanR:176.0600 loss:356988800.0000\n",
      "Episode:329 meanR:176.1700 loss:340092480.0000\n",
      "Episode:330 meanR:176.3600 loss:318620640.0000\n",
      "Episode:331 meanR:177.8500 loss:320666080.0000\n",
      "Episode:332 meanR:177.9600 loss:320367456.0000\n",
      "Episode:333 meanR:178.1400 loss:327678400.0000\n",
      "Episode:334 meanR:178.4200 loss:325569216.0000\n",
      "Episode:335 meanR:178.6800 loss:319215488.0000\n",
      "Episode:336 meanR:178.7000 loss:328136864.0000\n",
      "Episode:337 meanR:179.0100 loss:335938752.0000\n",
      "Episode:338 meanR:179.0800 loss:342174144.0000\n",
      "Episode:339 meanR:179.3300 loss:321827968.0000\n",
      "Episode:340 meanR:179.6600 loss:321516064.0000\n",
      "Episode:341 meanR:180.0300 loss:314443680.0000\n",
      "Episode:342 meanR:179.9700 loss:319975424.0000\n",
      "Episode:343 meanR:180.3100 loss:318542240.0000\n",
      "Episode:344 meanR:180.4200 loss:327842944.0000\n",
      "Episode:345 meanR:180.4900 loss:323016352.0000\n",
      "Episode:346 meanR:180.7900 loss:319131872.0000\n",
      "Episode:347 meanR:181.0900 loss:318788640.0000\n",
      "Episode:348 meanR:181.1300 loss:330377280.0000\n",
      "Episode:349 meanR:181.4700 loss:328796928.0000\n",
      "Episode:350 meanR:181.5500 loss:327895296.0000\n",
      "Episode:351 meanR:181.5200 loss:330769696.0000\n",
      "Episode:352 meanR:181.5100 loss:343665856.0000\n",
      "Episode:353 meanR:181.6800 loss:350300512.0000\n",
      "Episode:354 meanR:181.9300 loss:344689056.0000\n",
      "Episode:355 meanR:182.0400 loss:341885504.0000\n",
      "Episode:356 meanR:181.8700 loss:342606848.0000\n",
      "Episode:357 meanR:181.7800 loss:350192288.0000\n",
      "Episode:358 meanR:181.8700 loss:352832224.0000\n",
      "Episode:359 meanR:181.9400 loss:358583200.0000\n",
      "Episode:360 meanR:181.8200 loss:351869856.0000\n",
      "Episode:361 meanR:181.9100 loss:342339520.0000\n",
      "Episode:362 meanR:181.9400 loss:350842848.0000\n",
      "Episode:363 meanR:182.0300 loss:343692384.0000\n",
      "Episode:364 meanR:181.9400 loss:335149600.0000\n",
      "Episode:365 meanR:182.1800 loss:319086432.0000\n",
      "Episode:366 meanR:182.3100 loss:315330816.0000\n",
      "Episode:367 meanR:182.2900 loss:311771040.0000\n",
      "Episode:368 meanR:182.5000 loss:327267104.0000\n",
      "Episode:369 meanR:182.7700 loss:329618560.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:370 meanR:182.7100 loss:319660480.0000\n",
      "Episode:371 meanR:182.4000 loss:333055296.0000\n",
      "Episode:372 meanR:182.3900 loss:353926464.0000\n",
      "Episode:373 meanR:182.1600 loss:362285216.0000\n",
      "Episode:374 meanR:182.0000 loss:365481248.0000\n",
      "Episode:375 meanR:182.1100 loss:369215648.0000\n",
      "Episode:376 meanR:182.2100 loss:367437120.0000\n",
      "Episode:377 meanR:182.2200 loss:360275584.0000\n",
      "Episode:378 meanR:182.3900 loss:352763552.0000\n",
      "Episode:379 meanR:182.8600 loss:335400992.0000\n",
      "Episode:380 meanR:182.8300 loss:324774240.0000\n",
      "Episode:381 meanR:182.8900 loss:340145248.0000\n",
      "Episode:382 meanR:182.8100 loss:360160064.0000\n",
      "Episode:383 meanR:182.6600 loss:358778624.0000\n",
      "Episode:384 meanR:182.6100 loss:361212704.0000\n",
      "Episode:385 meanR:182.8400 loss:354576352.0000\n",
      "Episode:386 meanR:182.5500 loss:363771392.0000\n",
      "Episode:387 meanR:182.8100 loss:346212736.0000\n",
      "Episode:388 meanR:182.7500 loss:351376032.0000\n",
      "Episode:389 meanR:182.9000 loss:341621920.0000\n",
      "Episode:390 meanR:183.0800 loss:347876928.0000\n",
      "Episode:391 meanR:183.3100 loss:342238016.0000\n",
      "Episode:392 meanR:183.2300 loss:344233632.0000\n",
      "Episode:393 meanR:183.3000 loss:347038208.0000\n",
      "Episode:394 meanR:183.3300 loss:354667584.0000\n",
      "Episode:395 meanR:183.1700 loss:352916800.0000\n",
      "Episode:396 meanR:183.3400 loss:355288608.0000\n",
      "Episode:397 meanR:183.4700 loss:341357184.0000\n",
      "Episode:398 meanR:183.3900 loss:333339968.0000\n",
      "Episode:399 meanR:183.3900 loss:329856800.0000\n",
      "Episode:400 meanR:183.3300 loss:332491104.0000\n",
      "Episode:401 meanR:183.4400 loss:329744288.0000\n",
      "Episode:402 meanR:183.1800 loss:353510080.0000\n",
      "Episode:403 meanR:182.7400 loss:366151488.0000\n",
      "Episode:404 meanR:182.7200 loss:383240544.0000\n",
      "Episode:405 meanR:182.9900 loss:360287168.0000\n",
      "Episode:406 meanR:183.0500 loss:348194976.0000\n",
      "Episode:407 meanR:183.2200 loss:336736928.0000\n",
      "Episode:408 meanR:183.3200 loss:339680480.0000\n",
      "Episode:409 meanR:183.3000 loss:345011008.0000\n",
      "Episode:410 meanR:183.1700 loss:352693184.0000\n",
      "Episode:411 meanR:183.1200 loss:362214496.0000\n",
      "Episode:412 meanR:182.9900 loss:358608352.0000\n",
      "Episode:413 meanR:183.4700 loss:342230368.0000\n",
      "Episode:414 meanR:183.4600 loss:325424704.0000\n",
      "Episode:415 meanR:183.4900 loss:320668928.0000\n",
      "Episode:416 meanR:183.8700 loss:337969472.0000\n",
      "Episode:417 meanR:184.0300 loss:339098336.0000\n",
      "Episode:418 meanR:184.2700 loss:328229696.0000\n",
      "Episode:419 meanR:184.1700 loss:347530272.0000\n",
      "Episode:420 meanR:183.8100 loss:362850592.0000\n",
      "Episode:421 meanR:183.5700 loss:364419680.0000\n",
      "Episode:422 meanR:183.5800 loss:356743840.0000\n",
      "Episode:423 meanR:183.4400 loss:351690880.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episodes_total_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(111111111111111):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states}) \n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episodes_total_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episodes_total_reward)),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episodes_total_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episodes_total_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-qn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model-qn.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model-qn.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        for _ in range(111111111111111111):\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {:.2f}'.format(total_reward))\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
