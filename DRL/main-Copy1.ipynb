{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.349380847965382 -2.602122511534467\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 0.7310585786300049)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    # G\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    #g_loss = -tf.reduce_mean(Qs)\n",
    "    \n",
    "    # D\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, Qs, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 4) actions:(1000,)\n",
      "action size:2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "# state_size = 37\n",
    "# state_size_ = (84, 84, 3)\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 64             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "gamma = 0.99                   # future reward discount\n",
    "memory_size = 1000            # memory capacity\n",
    "batch_size = 1000             # experience mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#for _ in range(batch_size):\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 R:17.0 gloss:0.6995 dloss:1.0029 exploreP:0.9983\n",
      "Episode:1 meanR:14.0000 R:11.0 gloss:0.7397 dloss:0.9983 exploreP:0.9972\n",
      "Episode:2 meanR:13.6667 R:13.0 gloss:0.7764 dloss:0.9974 exploreP:0.9959\n",
      "Episode:3 meanR:14.7500 R:18.0 gloss:0.8250 dloss:1.0005 exploreP:0.9942\n",
      "Episode:4 meanR:14.0000 R:11.0 gloss:0.8725 dloss:1.0073 exploreP:0.9931\n",
      "Episode:5 meanR:13.3333 R:10.0 gloss:0.9091 dloss:1.0145 exploreP:0.9921\n",
      "Episode:6 meanR:15.4286 R:28.0 gloss:0.9780 dloss:1.0336 exploreP:0.9894\n",
      "Episode:7 meanR:15.5000 R:16.0 gloss:1.0668 dloss:1.0634 exploreP:0.9878\n",
      "Episode:8 meanR:17.5556 R:34.0 gloss:1.1835 dloss:1.1067 exploreP:0.9845\n",
      "Episode:9 meanR:18.0000 R:22.0 gloss:1.3335 dloss:1.1752 exploreP:0.9823\n",
      "Episode:10 meanR:19.6364 R:36.0 gloss:1.5212 dloss:1.2884 exploreP:0.9788\n",
      "Episode:11 meanR:18.8333 R:10.0 gloss:1.6844 dloss:1.4028 exploreP:0.9779\n",
      "Episode:12 meanR:18.6923 R:17.0 gloss:1.7944 dloss:1.5063 exploreP:0.9762\n",
      "Episode:13 meanR:19.7143 R:33.0 gloss:2.0090 dloss:1.6926 exploreP:0.9730\n",
      "Episode:14 meanR:19.2000 R:12.0 gloss:2.2254 dloss:1.9284 exploreP:0.9719\n",
      "Episode:15 meanR:20.1875 R:35.0 gloss:2.4598 dloss:2.1390 exploreP:0.9685\n",
      "Episode:16 meanR:20.1176 R:19.0 gloss:2.7569 dloss:2.4396 exploreP:0.9667\n",
      "Episode:17 meanR:19.4444 R:8.0 gloss:2.9013 dloss:2.5198 exploreP:0.9659\n",
      "Episode:18 meanR:20.1053 R:32.0 gloss:3.1434 dloss:2.7223 exploreP:0.9629\n",
      "Episode:19 meanR:19.9000 R:16.0 gloss:3.4358 dloss:2.9322 exploreP:0.9614\n",
      "Episode:20 meanR:20.4286 R:31.0 gloss:3.7487 dloss:3.1686 exploreP:0.9584\n",
      "Episode:21 meanR:20.0455 R:12.0 gloss:4.0464 dloss:3.3426 exploreP:0.9573\n",
      "Episode:22 meanR:19.6957 R:12.0 gloss:4.2235 dloss:3.4467 exploreP:0.9562\n",
      "Episode:23 meanR:19.8333 R:23.0 gloss:4.4913 dloss:3.5534 exploreP:0.9540\n",
      "Episode:24 meanR:20.1200 R:27.0 gloss:4.9061 dloss:3.8056 exploreP:0.9514\n",
      "Episode:25 meanR:19.7692 R:11.0 gloss:5.2493 dloss:4.0539 exploreP:0.9504\n",
      "Episode:26 meanR:20.0741 R:28.0 gloss:5.6127 dloss:4.3716 exploreP:0.9478\n",
      "Episode:27 meanR:20.7857 R:40.0 gloss:6.2570 dloss:4.8658 exploreP:0.9440\n",
      "Episode:28 meanR:21.6552 R:46.0 gloss:7.1271 dloss:5.5919 exploreP:0.9397\n",
      "Episode:29 meanR:21.3333 R:12.0 gloss:7.7239 dloss:6.2756 exploreP:0.9386\n",
      "Episode:30 meanR:21.1935 R:17.0 gloss:8.0116 dloss:6.8388 exploreP:0.9370\n",
      "Episode:31 meanR:20.9688 R:14.0 gloss:8.3041 dloss:7.3384 exploreP:0.9358\n",
      "Episode:32 meanR:20.8485 R:17.0 gloss:8.5723 dloss:7.6950 exploreP:0.9342\n",
      "Episode:33 meanR:21.3235 R:37.0 gloss:9.0458 dloss:8.3130 exploreP:0.9308\n",
      "Episode:34 meanR:21.4571 R:26.0 gloss:9.5835 dloss:9.0174 exploreP:0.9284\n",
      "Episode:35 meanR:21.2778 R:15.0 gloss:9.9200 dloss:9.6803 exploreP:0.9270\n",
      "Episode:36 meanR:21.6216 R:34.0 gloss:10.2332 dloss:10.4233 exploreP:0.9239\n",
      "Episode:37 meanR:21.3421 R:11.0 gloss:10.4845 dloss:10.9203 exploreP:0.9229\n",
      "Episode:38 meanR:21.1026 R:12.0 gloss:10.5964 dloss:11.1785 exploreP:0.9218\n",
      "Episode:39 meanR:20.9000 R:13.0 gloss:10.7163 dloss:11.4048 exploreP:0.9206\n",
      "Episode:40 meanR:20.6829 R:12.0 gloss:10.8178 dloss:11.7295 exploreP:0.9195\n",
      "Episode:41 meanR:20.4762 R:12.0 gloss:10.8702 dloss:12.0686 exploreP:0.9184\n",
      "Episode:42 meanR:21.2093 R:52.0 gloss:11.0339 dloss:12.0874 exploreP:0.9137\n",
      "Episode:43 meanR:21.2500 R:23.0 gloss:11.2948 dloss:12.4891 exploreP:0.9116\n",
      "Episode:44 meanR:21.4889 R:32.0 gloss:11.4655 dloss:12.6958 exploreP:0.9088\n",
      "Episode:45 meanR:21.4783 R:21.0 gloss:11.6232 dloss:12.8000 exploreP:0.9069\n",
      "Episode:46 meanR:21.2979 R:13.0 gloss:11.7116 dloss:12.9660 exploreP:0.9057\n",
      "Episode:47 meanR:21.1250 R:13.0 gloss:11.7534 dloss:13.3228 exploreP:0.9045\n",
      "Episode:48 meanR:20.9796 R:14.0 gloss:11.7778 dloss:13.4434 exploreP:0.9033\n",
      "Episode:49 meanR:20.9000 R:17.0 gloss:11.8276 dloss:13.4226 exploreP:0.9018\n",
      "Episode:50 meanR:20.9412 R:23.0 gloss:11.8770 dloss:13.4588 exploreP:0.8997\n",
      "Episode:51 meanR:20.9808 R:23.0 gloss:11.9405 dloss:13.3059 exploreP:0.8977\n",
      "Episode:52 meanR:20.8491 R:14.0 gloss:12.0063 dloss:13.6439 exploreP:0.8964\n",
      "Episode:53 meanR:20.7407 R:15.0 gloss:12.0471 dloss:13.7840 exploreP:0.8951\n",
      "Episode:54 meanR:21.0000 R:35.0 gloss:12.1012 dloss:13.8586 exploreP:0.8920\n",
      "Episode:55 meanR:20.8393 R:12.0 gloss:12.1581 dloss:13.9924 exploreP:0.8910\n",
      "Episode:56 meanR:20.7368 R:15.0 gloss:12.1298 dloss:14.1074 exploreP:0.8896\n",
      "Episode:57 meanR:20.5690 R:11.0 gloss:12.0832 dloss:14.0902 exploreP:0.8887\n",
      "Episode:58 meanR:20.8983 R:40.0 gloss:12.0398 dloss:14.0140 exploreP:0.8852\n",
      "Episode:59 meanR:20.7667 R:13.0 gloss:12.0938 dloss:13.8491 exploreP:0.8840\n",
      "Episode:60 meanR:20.7869 R:22.0 gloss:12.0796 dloss:13.9086 exploreP:0.8821\n",
      "Episode:61 meanR:20.6935 R:15.0 gloss:12.0751 dloss:14.0020 exploreP:0.8808\n",
      "Episode:62 meanR:20.5873 R:14.0 gloss:12.0412 dloss:13.9113 exploreP:0.8796\n",
      "Episode:63 meanR:21.3594 R:70.0 gloss:11.9911 dloss:13.6907 exploreP:0.8735\n",
      "Episode:64 meanR:21.2154 R:12.0 gloss:12.0861 dloss:13.8500 exploreP:0.8725\n",
      "Episode:65 meanR:21.2273 R:22.0 gloss:12.1442 dloss:13.9442 exploreP:0.8706\n",
      "Episode:66 meanR:21.1642 R:17.0 gloss:12.1651 dloss:14.0510 exploreP:0.8691\n",
      "Episode:67 meanR:21.0000 R:10.0 gloss:12.1460 dloss:14.3610 exploreP:0.8683\n",
      "Episode:68 meanR:21.2174 R:36.0 gloss:12.1575 dloss:14.0043 exploreP:0.8652\n",
      "Episode:69 meanR:21.4000 R:34.0 gloss:12.2842 dloss:14.1710 exploreP:0.8623\n",
      "Episode:70 meanR:21.2958 R:14.0 gloss:12.3213 dloss:14.2832 exploreP:0.8611\n",
      "Episode:71 meanR:21.2917 R:21.0 gloss:12.3128 dloss:14.2036 exploreP:0.8593\n",
      "Episode:72 meanR:21.2192 R:16.0 gloss:12.2834 dloss:14.3500 exploreP:0.8579\n",
      "Episode:73 meanR:21.1757 R:18.0 gloss:12.2321 dloss:14.4297 exploreP:0.8564\n",
      "Episode:74 meanR:21.1333 R:18.0 gloss:12.1467 dloss:14.5037 exploreP:0.8549\n",
      "Episode:75 meanR:21.0395 R:14.0 gloss:12.0519 dloss:14.3817 exploreP:0.8537\n",
      "Episode:76 meanR:20.9740 R:16.0 gloss:11.9475 dloss:14.4691 exploreP:0.8524\n",
      "Episode:77 meanR:21.2308 R:41.0 gloss:11.7329 dloss:13.9788 exploreP:0.8489\n",
      "Episode:78 meanR:21.7089 R:59.0 gloss:11.7857 dloss:13.5391 exploreP:0.8440\n",
      "Episode:79 meanR:21.6750 R:19.0 gloss:11.9815 dloss:13.9998 exploreP:0.8424\n",
      "Episode:80 meanR:21.6790 R:22.0 gloss:11.9894 dloss:14.1309 exploreP:0.8406\n",
      "Episode:81 meanR:21.6585 R:20.0 gloss:11.9878 dloss:14.0816 exploreP:0.8389\n",
      "Episode:82 meanR:21.8072 R:34.0 gloss:11.9729 dloss:14.1542 exploreP:0.8361\n",
      "Episode:83 meanR:21.6905 R:12.0 gloss:11.9389 dloss:13.9938 exploreP:0.8351\n",
      "Episode:84 meanR:21.9059 R:40.0 gloss:12.0244 dloss:13.8541 exploreP:0.8318\n",
      "Episode:85 meanR:21.9419 R:25.0 gloss:12.2696 dloss:14.1894 exploreP:0.8298\n",
      "Episode:86 meanR:22.0115 R:28.0 gloss:12.3136 dloss:14.7267 exploreP:0.8275\n",
      "Episode:87 meanR:21.9205 R:14.0 gloss:12.2921 dloss:14.7347 exploreP:0.8263\n",
      "Episode:88 meanR:21.7978 R:11.0 gloss:12.2425 dloss:14.8181 exploreP:0.8254\n",
      "Episode:89 meanR:21.7333 R:16.0 gloss:12.1701 dloss:14.8334 exploreP:0.8241\n",
      "Episode:90 meanR:21.8462 R:32.0 gloss:12.0013 dloss:14.5344 exploreP:0.8215\n",
      "Episode:91 meanR:21.8043 R:18.0 gloss:11.9257 dloss:14.1877 exploreP:0.8201\n",
      "Episode:92 meanR:21.7527 R:17.0 gloss:11.9479 dloss:14.1541 exploreP:0.8187\n",
      "Episode:93 meanR:21.7660 R:23.0 gloss:11.9742 dloss:14.1625 exploreP:0.8168\n",
      "Episode:94 meanR:21.7263 R:18.0 gloss:12.0448 dloss:14.2975 exploreP:0.8154\n",
      "Episode:95 meanR:21.6354 R:13.0 gloss:12.0773 dloss:14.4222 exploreP:0.8143\n",
      "Episode:96 meanR:21.6392 R:22.0 gloss:12.0256 dloss:14.4530 exploreP:0.8126\n",
      "Episode:97 meanR:21.5510 R:13.0 gloss:11.9701 dloss:14.3230 exploreP:0.8115\n",
      "Episode:98 meanR:21.7475 R:41.0 gloss:11.9435 dloss:14.2575 exploreP:0.8082\n",
      "Episode:99 meanR:21.9000 R:37.0 gloss:12.0087 dloss:14.1293 exploreP:0.8053\n",
      "Episode:100 meanR:22.2200 R:49.0 gloss:12.1913 dloss:14.3979 exploreP:0.8014\n",
      "Episode:101 meanR:22.2800 R:17.0 gloss:12.3157 dloss:14.6778 exploreP:0.8001\n",
      "Episode:102 meanR:22.3600 R:21.0 gloss:12.3275 dloss:14.8898 exploreP:0.7984\n",
      "Episode:103 meanR:22.4700 R:29.0 gloss:12.4064 dloss:14.7871 exploreP:0.7961\n",
      "Episode:104 meanR:22.5700 R:21.0 gloss:12.5075 dloss:15.1220 exploreP:0.7945\n",
      "Episode:105 meanR:22.7400 R:27.0 gloss:12.4862 dloss:15.3502 exploreP:0.7924\n",
      "Episode:106 meanR:22.8900 R:43.0 gloss:12.3290 dloss:14.8588 exploreP:0.7890\n",
      "Episode:107 meanR:22.9800 R:25.0 gloss:12.3949 dloss:14.7827 exploreP:0.7871\n",
      "Episode:108 meanR:22.9300 R:29.0 gloss:12.5472 dloss:14.8801 exploreP:0.7848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:109 meanR:22.9400 R:23.0 gloss:12.7152 dloss:15.2435 exploreP:0.7830\n",
      "Episode:110 meanR:22.9900 R:41.0 gloss:12.6760 dloss:15.1259 exploreP:0.7799\n",
      "Episode:111 meanR:23.1300 R:24.0 gloss:12.8144 dloss:15.1220 exploreP:0.7780\n",
      "Episode:112 meanR:23.1600 R:20.0 gloss:12.8668 dloss:15.2661 exploreP:0.7765\n",
      "Episode:113 meanR:23.1900 R:36.0 gloss:13.0036 dloss:15.2785 exploreP:0.7737\n",
      "Episode:114 meanR:23.3300 R:26.0 gloss:13.1341 dloss:15.3569 exploreP:0.7717\n",
      "Episode:115 meanR:23.1700 R:19.0 gloss:13.2686 dloss:15.8176 exploreP:0.7703\n",
      "Episode:116 meanR:23.1300 R:15.0 gloss:13.2091 dloss:16.1829 exploreP:0.7692\n",
      "Episode:117 meanR:23.1600 R:11.0 gloss:13.0833 dloss:15.8950 exploreP:0.7683\n",
      "Episode:118 meanR:23.0900 R:25.0 gloss:12.9167 dloss:15.8223 exploreP:0.7664\n",
      "Episode:119 meanR:23.0300 R:10.0 gloss:12.7687 dloss:15.7991 exploreP:0.7657\n",
      "Episode:120 meanR:23.0400 R:32.0 gloss:12.4883 dloss:15.2803 exploreP:0.7633\n",
      "Episode:121 meanR:23.2900 R:37.0 gloss:12.4689 dloss:14.8850 exploreP:0.7605\n",
      "Episode:122 meanR:23.2700 R:10.0 gloss:12.6830 dloss:15.1639 exploreP:0.7597\n",
      "Episode:123 meanR:23.3500 R:31.0 gloss:12.6516 dloss:15.2389 exploreP:0.7574\n",
      "Episode:124 meanR:23.2500 R:17.0 gloss:12.5118 dloss:14.9934 exploreP:0.7561\n",
      "Episode:125 meanR:23.2400 R:10.0 gloss:12.5134 dloss:15.2167 exploreP:0.7554\n",
      "Episode:126 meanR:23.0700 R:11.0 gloss:12.4774 dloss:15.4845 exploreP:0.7546\n",
      "Episode:127 meanR:22.8500 R:18.0 gloss:12.2772 dloss:15.2349 exploreP:0.7532\n",
      "Episode:128 meanR:22.6400 R:25.0 gloss:12.0652 dloss:14.7230 exploreP:0.7514\n",
      "Episode:129 meanR:22.8800 R:36.0 gloss:12.0322 dloss:14.5412 exploreP:0.7487\n",
      "Episode:130 meanR:23.0300 R:32.0 gloss:12.1823 dloss:14.4570 exploreP:0.7463\n",
      "Episode:131 meanR:23.0600 R:17.0 gloss:12.4183 dloss:14.9822 exploreP:0.7451\n",
      "Episode:132 meanR:23.0000 R:11.0 gloss:12.4117 dloss:15.3383 exploreP:0.7443\n",
      "Episode:133 meanR:22.9200 R:29.0 gloss:12.2406 dloss:14.8586 exploreP:0.7422\n",
      "Episode:134 meanR:22.7700 R:11.0 gloss:12.2392 dloss:14.8404 exploreP:0.7414\n",
      "Episode:135 meanR:22.8500 R:23.0 gloss:12.2051 dloss:14.9179 exploreP:0.7397\n",
      "Episode:136 meanR:22.7700 R:26.0 gloss:12.1728 dloss:14.7575 exploreP:0.7378\n",
      "Episode:137 meanR:22.8300 R:17.0 gloss:12.3011 dloss:14.8655 exploreP:0.7365\n",
      "Episode:138 meanR:22.8700 R:16.0 gloss:12.3596 dloss:15.0552 exploreP:0.7354\n",
      "Episode:139 meanR:22.9200 R:18.0 gloss:12.3444 dloss:14.9761 exploreP:0.7341\n",
      "Episode:140 meanR:22.9200 R:12.0 gloss:12.3154 dloss:15.2810 exploreP:0.7332\n",
      "Episode:141 meanR:22.9600 R:16.0 gloss:12.1341 dloss:15.1594 exploreP:0.7321\n",
      "Episode:142 meanR:22.5900 R:15.0 gloss:11.9759 dloss:14.8086 exploreP:0.7310\n",
      "Episode:143 meanR:22.4600 R:10.0 gloss:11.8782 dloss:14.8998 exploreP:0.7303\n",
      "Episode:144 meanR:22.2700 R:13.0 gloss:11.7104 dloss:14.7354 exploreP:0.7293\n",
      "Episode:145 meanR:22.2600 R:20.0 gloss:11.5545 dloss:14.3980 exploreP:0.7279\n",
      "Episode:146 meanR:22.2200 R:9.0 gloss:11.4918 dloss:14.4556 exploreP:0.7272\n",
      "Episode:147 meanR:22.1900 R:10.0 gloss:11.3754 dloss:14.4707 exploreP:0.7265\n",
      "Episode:148 meanR:22.1900 R:14.0 gloss:11.2043 dloss:14.0854 exploreP:0.7255\n",
      "Episode:149 meanR:22.6700 R:65.0 gloss:11.1642 dloss:13.6385 exploreP:0.7209\n",
      "Episode:150 meanR:22.6200 R:18.0 gloss:11.3390 dloss:14.0011 exploreP:0.7196\n",
      "Episode:151 meanR:22.7200 R:33.0 gloss:11.2795 dloss:13.9317 exploreP:0.7173\n",
      "Episode:152 meanR:22.7500 R:17.0 gloss:11.2971 dloss:14.1048 exploreP:0.7161\n",
      "Episode:153 meanR:22.7800 R:18.0 gloss:11.1762 dloss:14.1448 exploreP:0.7148\n",
      "Episode:154 meanR:22.7700 R:34.0 gloss:11.0033 dloss:13.7089 exploreP:0.7124\n",
      "Episode:155 meanR:22.8500 R:20.0 gloss:10.9424 dloss:13.8743 exploreP:0.7110\n",
      "Episode:156 meanR:22.8300 R:13.0 gloss:10.9032 dloss:13.8432 exploreP:0.7101\n",
      "Episode:157 meanR:22.8200 R:10.0 gloss:10.8497 dloss:13.9579 exploreP:0.7094\n",
      "Episode:158 meanR:22.6500 R:23.0 gloss:10.7214 dloss:13.6994 exploreP:0.7078\n",
      "Episode:159 meanR:22.7100 R:19.0 gloss:10.6125 dloss:13.6187 exploreP:0.7065\n",
      "Episode:160 meanR:22.6600 R:17.0 gloss:10.5024 dloss:13.4336 exploreP:0.7053\n",
      "Episode:161 meanR:22.7000 R:19.0 gloss:10.4550 dloss:13.3437 exploreP:0.7040\n",
      "Episode:162 meanR:22.8100 R:25.0 gloss:10.4209 dloss:13.2780 exploreP:0.7022\n",
      "Episode:163 meanR:22.2200 R:11.0 gloss:10.3987 dloss:13.5023 exploreP:0.7015\n",
      "Episode:164 meanR:22.4000 R:30.0 gloss:10.2952 dloss:13.2090 exploreP:0.6994\n",
      "Episode:165 meanR:22.5600 R:38.0 gloss:10.3368 dloss:13.0496 exploreP:0.6968\n",
      "Episode:166 meanR:22.6400 R:25.0 gloss:10.5297 dloss:13.2499 exploreP:0.6951\n",
      "Episode:167 meanR:23.0300 R:49.0 gloss:10.6808 dloss:13.4621 exploreP:0.6917\n",
      "Episode:168 meanR:22.9500 R:28.0 gloss:10.8429 dloss:13.7456 exploreP:0.6898\n",
      "Episode:169 meanR:22.9800 R:37.0 gloss:10.7623 dloss:13.5977 exploreP:0.6873\n",
      "Episode:170 meanR:23.0200 R:18.0 gloss:10.9027 dloss:13.9634 exploreP:0.6861\n",
      "Episode:171 meanR:23.1400 R:33.0 gloss:10.9305 dloss:13.6825 exploreP:0.6838\n",
      "Episode:172 meanR:23.1100 R:13.0 gloss:11.1403 dloss:13.9157 exploreP:0.6830\n",
      "Episode:173 meanR:23.2000 R:27.0 gloss:11.2005 dloss:14.1290 exploreP:0.6812\n",
      "Episode:174 meanR:23.5500 R:53.0 gloss:11.3035 dloss:14.0987 exploreP:0.6776\n",
      "Episode:175 meanR:23.6600 R:25.0 gloss:11.3498 dloss:14.3201 exploreP:0.6759\n",
      "Episode:176 meanR:23.6100 R:11.0 gloss:11.2763 dloss:14.2961 exploreP:0.6752\n",
      "Episode:177 meanR:23.4000 R:20.0 gloss:11.2548 dloss:14.0927 exploreP:0.6739\n",
      "Episode:178 meanR:23.0600 R:25.0 gloss:11.2946 dloss:14.2510 exploreP:0.6722\n",
      "Episode:179 meanR:22.9900 R:12.0 gloss:11.2655 dloss:14.1480 exploreP:0.6714\n",
      "Episode:180 meanR:22.8800 R:11.0 gloss:11.2888 dloss:14.3599 exploreP:0.6707\n",
      "Episode:181 meanR:23.2400 R:56.0 gloss:11.1081 dloss:14.0806 exploreP:0.6670\n",
      "Episode:182 meanR:23.1000 R:20.0 gloss:11.1971 dloss:14.4116 exploreP:0.6657\n",
      "Episode:183 meanR:23.1800 R:20.0 gloss:11.2913 dloss:14.4957 exploreP:0.6644\n",
      "Episode:184 meanR:23.0000 R:22.0 gloss:11.3575 dloss:14.4666 exploreP:0.6630\n",
      "Episode:185 meanR:23.0100 R:26.0 gloss:11.4322 dloss:14.5490 exploreP:0.6613\n",
      "Episode:186 meanR:22.8900 R:16.0 gloss:11.6387 dloss:14.8149 exploreP:0.6602\n",
      "Episode:187 meanR:23.0000 R:25.0 gloss:11.7137 dloss:14.8939 exploreP:0.6586\n",
      "Episode:188 meanR:23.0100 R:12.0 gloss:11.9380 dloss:14.9440 exploreP:0.6578\n",
      "Episode:189 meanR:23.0400 R:19.0 gloss:12.0746 dloss:15.1979 exploreP:0.6566\n",
      "Episode:190 meanR:22.8400 R:12.0 gloss:12.0997 dloss:15.5009 exploreP:0.6558\n",
      "Episode:191 meanR:22.9700 R:31.0 gloss:11.7960 dloss:15.1028 exploreP:0.6538\n",
      "Episode:192 meanR:23.0100 R:21.0 gloss:11.6428 dloss:14.7286 exploreP:0.6525\n",
      "Episode:193 meanR:22.9300 R:15.0 gloss:11.6739 dloss:14.7106 exploreP:0.6515\n",
      "Episode:194 meanR:23.3100 R:56.0 gloss:11.7070 dloss:14.6445 exploreP:0.6479\n",
      "Episode:195 meanR:23.5900 R:41.0 gloss:11.9535 dloss:15.0284 exploreP:0.6453\n",
      "Episode:196 meanR:23.5700 R:20.0 gloss:12.0235 dloss:15.1073 exploreP:0.6440\n",
      "Episode:197 meanR:23.9200 R:48.0 gloss:12.2248 dloss:15.1133 exploreP:0.6410\n",
      "Episode:198 meanR:23.6700 R:16.0 gloss:12.5678 dloss:15.7494 exploreP:0.6400\n",
      "Episode:199 meanR:23.5400 R:24.0 gloss:12.5356 dloss:15.5522 exploreP:0.6385\n",
      "Episode:200 meanR:23.3700 R:32.0 gloss:12.5774 dloss:15.6576 exploreP:0.6365\n",
      "Episode:201 meanR:23.5800 R:38.0 gloss:12.7196 dloss:15.7590 exploreP:0.6341\n",
      "Episode:202 meanR:23.7500 R:38.0 gloss:12.8241 dloss:16.0467 exploreP:0.6317\n",
      "Episode:203 meanR:23.8500 R:39.0 gloss:12.6104 dloss:15.9287 exploreP:0.6293\n",
      "Episode:204 meanR:24.2400 R:60.0 gloss:12.6455 dloss:16.1130 exploreP:0.6256\n",
      "Episode:205 meanR:24.1400 R:17.0 gloss:12.7191 dloss:16.3891 exploreP:0.6246\n",
      "Episode:206 meanR:23.9700 R:26.0 gloss:12.5739 dloss:16.2915 exploreP:0.6230\n",
      "Episode:207 meanR:23.8600 R:14.0 gloss:12.4289 dloss:16.0024 exploreP:0.6221\n",
      "Episode:208 meanR:23.7800 R:21.0 gloss:12.4182 dloss:15.9621 exploreP:0.6208\n",
      "Episode:209 meanR:23.7200 R:17.0 gloss:12.3849 dloss:16.1862 exploreP:0.6198\n",
      "Episode:210 meanR:23.4800 R:17.0 gloss:12.2091 dloss:15.6957 exploreP:0.6187\n",
      "Episode:211 meanR:23.8100 R:57.0 gloss:12.4389 dloss:15.8790 exploreP:0.6153\n",
      "Episode:212 meanR:23.9900 R:38.0 gloss:12.7333 dloss:15.9168 exploreP:0.6130\n",
      "Episode:213 meanR:24.3400 R:71.0 gloss:12.9245 dloss:16.1811 exploreP:0.6087\n",
      "Episode:214 meanR:24.3500 R:27.0 gloss:13.3821 dloss:16.6630 exploreP:0.6071\n",
      "Episode:215 meanR:24.3500 R:19.0 gloss:13.5379 dloss:17.3426 exploreP:0.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:216 meanR:24.5400 R:34.0 gloss:13.1648 dloss:16.6589 exploreP:0.6040\n",
      "Episode:217 meanR:24.6000 R:17.0 gloss:13.2167 dloss:16.7117 exploreP:0.6029\n",
      "Episode:218 meanR:24.4600 R:11.0 gloss:13.1961 dloss:16.9715 exploreP:0.6023\n",
      "Episode:219 meanR:24.4700 R:11.0 gloss:13.0788 dloss:16.7509 exploreP:0.6016\n",
      "Episode:220 meanR:24.2700 R:12.0 gloss:12.8684 dloss:16.7371 exploreP:0.6009\n",
      "Episode:221 meanR:24.2700 R:37.0 gloss:12.6662 dloss:16.1550 exploreP:0.5988\n",
      "Episode:222 meanR:24.3100 R:14.0 gloss:12.8151 dloss:16.5046 exploreP:0.5979\n",
      "Episode:223 meanR:24.3200 R:32.0 gloss:12.7402 dloss:16.3624 exploreP:0.5960\n",
      "Episode:224 meanR:24.5000 R:35.0 gloss:12.9939 dloss:16.4979 exploreP:0.5940\n",
      "Episode:225 meanR:24.5000 R:10.0 gloss:13.3630 dloss:17.2178 exploreP:0.5934\n",
      "Episode:226 meanR:25.1300 R:74.0 gloss:13.3023 dloss:16.8726 exploreP:0.5891\n",
      "Episode:227 meanR:25.2400 R:29.0 gloss:13.6339 dloss:17.4952 exploreP:0.5874\n",
      "Episode:228 meanR:25.2000 R:21.0 gloss:13.4390 dloss:16.9777 exploreP:0.5862\n",
      "Episode:229 meanR:24.9600 R:12.0 gloss:13.4963 dloss:17.4134 exploreP:0.5855\n",
      "Episode:230 meanR:25.0300 R:39.0 gloss:13.1288 dloss:16.6460 exploreP:0.5833\n",
      "Episode:231 meanR:25.2200 R:36.0 gloss:13.2247 dloss:17.0699 exploreP:0.5812\n",
      "Episode:232 meanR:25.5400 R:43.0 gloss:13.2930 dloss:16.7556 exploreP:0.5788\n",
      "Episode:233 meanR:25.4000 R:15.0 gloss:13.5718 dloss:17.2860 exploreP:0.5779\n",
      "Episode:234 meanR:25.4100 R:12.0 gloss:13.4162 dloss:17.4687 exploreP:0.5773\n",
      "Episode:235 meanR:25.4600 R:28.0 gloss:13.0468 dloss:16.7235 exploreP:0.5757\n",
      "Episode:236 meanR:25.7100 R:51.0 gloss:12.9396 dloss:16.7601 exploreP:0.5728\n",
      "Episode:237 meanR:26.1900 R:65.0 gloss:13.0709 dloss:16.8423 exploreP:0.5691\n",
      "Episode:238 meanR:26.1700 R:14.0 gloss:13.2127 dloss:17.5155 exploreP:0.5684\n",
      "Episode:239 meanR:26.4800 R:49.0 gloss:12.7982 dloss:16.5917 exploreP:0.5656\n",
      "Episode:240 meanR:26.4800 R:12.0 gloss:13.0447 dloss:17.4030 exploreP:0.5650\n",
      "Episode:241 meanR:26.8800 R:56.0 gloss:13.0357 dloss:16.8754 exploreP:0.5619\n",
      "Episode:242 meanR:27.1700 R:44.0 gloss:13.6066 dloss:17.4855 exploreP:0.5594\n",
      "Episode:243 meanR:27.3000 R:23.0 gloss:13.8092 dloss:17.9193 exploreP:0.5582\n",
      "Episode:244 meanR:27.2900 R:12.0 gloss:13.7831 dloss:17.4770 exploreP:0.5575\n",
      "Episode:245 meanR:27.6800 R:59.0 gloss:13.6501 dloss:17.1710 exploreP:0.5543\n",
      "Episode:246 meanR:27.7600 R:17.0 gloss:13.7431 dloss:17.7516 exploreP:0.5534\n",
      "Episode:247 meanR:27.9600 R:30.0 gloss:13.3051 dloss:16.9944 exploreP:0.5518\n",
      "Episode:248 meanR:28.0000 R:18.0 gloss:13.2822 dloss:16.8251 exploreP:0.5508\n",
      "Episode:249 meanR:27.5200 R:17.0 gloss:13.2444 dloss:16.8824 exploreP:0.5499\n",
      "Episode:250 meanR:27.8300 R:49.0 gloss:13.1494 dloss:16.7127 exploreP:0.5472\n",
      "Episode:251 meanR:27.9400 R:44.0 gloss:13.3325 dloss:16.8716 exploreP:0.5449\n",
      "Episode:252 meanR:28.4000 R:63.0 gloss:13.8923 dloss:17.7045 exploreP:0.5415\n",
      "Episode:253 meanR:28.5500 R:33.0 gloss:14.0070 dloss:18.2478 exploreP:0.5397\n",
      "Episode:254 meanR:28.8900 R:68.0 gloss:14.0755 dloss:18.3110 exploreP:0.5362\n",
      "Episode:255 meanR:28.9600 R:27.0 gloss:14.0379 dloss:18.7258 exploreP:0.5347\n",
      "Episode:256 meanR:29.2300 R:40.0 gloss:13.8000 dloss:18.1847 exploreP:0.5326\n",
      "Episode:257 meanR:29.4200 R:29.0 gloss:13.8666 dloss:18.3203 exploreP:0.5311\n",
      "Episode:258 meanR:29.3700 R:18.0 gloss:13.9516 dloss:18.4269 exploreP:0.5302\n",
      "Episode:259 meanR:29.2800 R:10.0 gloss:13.9983 dloss:19.1379 exploreP:0.5297\n",
      "Episode:260 meanR:29.3400 R:23.0 gloss:13.5736 dloss:18.4027 exploreP:0.5285\n",
      "Episode:261 meanR:29.6400 R:49.0 gloss:13.3829 dloss:18.0490 exploreP:0.5259\n",
      "Episode:262 meanR:30.1600 R:77.0 gloss:13.5261 dloss:18.3950 exploreP:0.5220\n",
      "Episode:263 meanR:30.2700 R:22.0 gloss:14.0265 dloss:19.1258 exploreP:0.5209\n",
      "Episode:264 meanR:30.2500 R:28.0 gloss:13.7421 dloss:19.0230 exploreP:0.5194\n",
      "Episode:265 meanR:30.0100 R:14.0 gloss:13.6414 dloss:18.6190 exploreP:0.5187\n",
      "Episode:266 meanR:30.4400 R:68.0 gloss:13.4207 dloss:18.4430 exploreP:0.5153\n",
      "Episode:267 meanR:30.4300 R:48.0 gloss:13.7428 dloss:18.7878 exploreP:0.5129\n",
      "Episode:268 meanR:30.4400 R:29.0 gloss:13.7747 dloss:18.5553 exploreP:0.5114\n",
      "Episode:269 meanR:30.3600 R:29.0 gloss:13.8320 dloss:19.1545 exploreP:0.5099\n",
      "Episode:270 meanR:30.4900 R:31.0 gloss:13.4260 dloss:18.4898 exploreP:0.5084\n",
      "Episode:271 meanR:30.8400 R:68.0 gloss:13.5114 dloss:18.5521 exploreP:0.5050\n",
      "Episode:272 meanR:31.0200 R:31.0 gloss:14.5851 dloss:19.4490 exploreP:0.5035\n",
      "Episode:273 meanR:30.8700 R:12.0 gloss:14.3564 dloss:19.3861 exploreP:0.5029\n",
      "Episode:274 meanR:30.9800 R:64.0 gloss:14.3017 dloss:18.7009 exploreP:0.4998\n",
      "Episode:275 meanR:30.9100 R:18.0 gloss:14.5192 dloss:19.3154 exploreP:0.4989\n",
      "Episode:276 meanR:30.9100 R:11.0 gloss:14.4229 dloss:19.4618 exploreP:0.4983\n",
      "Episode:277 meanR:31.1000 R:39.0 gloss:14.2135 dloss:19.1649 exploreP:0.4964\n",
      "Episode:278 meanR:31.1400 R:29.0 gloss:14.1860 dloss:18.9638 exploreP:0.4950\n",
      "Episode:279 meanR:31.4200 R:40.0 gloss:14.0055 dloss:18.8535 exploreP:0.4931\n",
      "Episode:280 meanR:31.7100 R:40.0 gloss:13.8716 dloss:19.0097 exploreP:0.4912\n",
      "Episode:281 meanR:31.3600 R:21.0 gloss:13.8156 dloss:18.6571 exploreP:0.4902\n",
      "Episode:282 meanR:31.5600 R:40.0 gloss:13.7258 dloss:18.5692 exploreP:0.4882\n",
      "Episode:283 meanR:31.6200 R:26.0 gloss:13.4880 dloss:18.8833 exploreP:0.4870\n",
      "Episode:284 meanR:31.5900 R:19.0 gloss:13.0939 dloss:18.4212 exploreP:0.4861\n",
      "Episode:285 meanR:31.7800 R:45.0 gloss:12.9715 dloss:17.9970 exploreP:0.4840\n",
      "Episode:286 meanR:31.7600 R:14.0 gloss:13.1613 dloss:18.8779 exploreP:0.4833\n",
      "Episode:287 meanR:31.8100 R:30.0 gloss:12.8340 dloss:17.9336 exploreP:0.4819\n",
      "Episode:288 meanR:32.0800 R:39.0 gloss:12.9210 dloss:17.8906 exploreP:0.4800\n",
      "Episode:289 meanR:32.2800 R:39.0 gloss:13.2283 dloss:18.5390 exploreP:0.4782\n",
      "Episode:290 meanR:32.2500 R:9.0 gloss:13.2545 dloss:19.3348 exploreP:0.4778\n",
      "Episode:291 meanR:32.0700 R:13.0 gloss:13.0286 dloss:19.0275 exploreP:0.4772\n",
      "Episode:292 meanR:32.2900 R:43.0 gloss:12.6794 dloss:17.9203 exploreP:0.4752\n",
      "Episode:293 meanR:32.6600 R:52.0 gloss:12.2734 dloss:17.8535 exploreP:0.4728\n",
      "Episode:294 meanR:32.5300 R:43.0 gloss:12.1639 dloss:17.9821 exploreP:0.4708\n",
      "Episode:295 meanR:32.4700 R:35.0 gloss:12.2062 dloss:18.2291 exploreP:0.4692\n",
      "Episode:296 meanR:32.4300 R:16.0 gloss:12.4036 dloss:18.7755 exploreP:0.4684\n",
      "Episode:297 meanR:32.1000 R:15.0 gloss:12.0901 dloss:18.2758 exploreP:0.4677\n",
      "Episode:298 meanR:32.6300 R:69.0 gloss:11.9191 dloss:17.6328 exploreP:0.4646\n",
      "Episode:299 meanR:33.2300 R:84.0 gloss:12.1198 dloss:18.0431 exploreP:0.4608\n",
      "Episode:300 meanR:33.3000 R:39.0 gloss:12.4653 dloss:18.5347 exploreP:0.4590\n",
      "Episode:301 meanR:33.6700 R:75.0 gloss:12.2710 dloss:18.2241 exploreP:0.4557\n",
      "Episode:302 meanR:33.7000 R:41.0 gloss:12.6356 dloss:18.9615 exploreP:0.4539\n",
      "Episode:303 meanR:33.5900 R:28.0 gloss:12.2603 dloss:18.5308 exploreP:0.4526\n",
      "Episode:304 meanR:33.2900 R:30.0 gloss:12.4086 dloss:18.5141 exploreP:0.4513\n",
      "Episode:305 meanR:33.9400 R:82.0 gloss:12.6114 dloss:19.0381 exploreP:0.4477\n",
      "Episode:306 meanR:34.1700 R:49.0 gloss:12.8641 dloss:19.3655 exploreP:0.4455\n",
      "Episode:307 meanR:34.1500 R:12.0 gloss:12.9497 dloss:19.5970 exploreP:0.4450\n",
      "Episode:308 meanR:34.0800 R:14.0 gloss:12.7421 dloss:19.5281 exploreP:0.4444\n",
      "Episode:309 meanR:34.2700 R:36.0 gloss:12.6157 dloss:19.0866 exploreP:0.4429\n",
      "Episode:310 meanR:34.8900 R:79.0 gloss:12.8790 dloss:19.2584 exploreP:0.4394\n",
      "Episode:311 meanR:34.7900 R:47.0 gloss:13.0965 dloss:19.5732 exploreP:0.4374\n",
      "Episode:312 meanR:34.7900 R:38.0 gloss:13.3894 dloss:20.1233 exploreP:0.4358\n",
      "Episode:313 meanR:35.1600 R:108.0 gloss:13.9010 dloss:20.3730 exploreP:0.4312\n",
      "Episode:314 meanR:35.5900 R:70.0 gloss:15.2789 dloss:21.1946 exploreP:0.4283\n",
      "Episode:315 meanR:35.8000 R:40.0 gloss:15.2963 dloss:21.4039 exploreP:0.4266\n",
      "Episode:316 meanR:35.8300 R:37.0 gloss:15.3955 dloss:21.3404 exploreP:0.4251\n",
      "Episode:317 meanR:35.7700 R:11.0 gloss:15.5218 dloss:21.5861 exploreP:0.4246\n",
      "Episode:318 meanR:35.8600 R:20.0 gloss:15.3712 dloss:21.4273 exploreP:0.4238\n",
      "Episode:319 meanR:36.0700 R:32.0 gloss:15.0806 dloss:21.5894 exploreP:0.4225\n",
      "Episode:320 meanR:36.1200 R:17.0 gloss:14.7014 dloss:21.6179 exploreP:0.4218\n",
      "Episode:321 meanR:35.8400 R:9.0 gloss:14.3848 dloss:21.5541 exploreP:0.4214\n",
      "Episode:322 meanR:36.2800 R:58.0 gloss:13.9648 dloss:20.4917 exploreP:0.4190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:323 meanR:36.6800 R:72.0 gloss:13.6003 dloss:20.3463 exploreP:0.4161\n",
      "Episode:324 meanR:37.0300 R:70.0 gloss:13.5597 dloss:20.6116 exploreP:0.4133\n",
      "Episode:325 meanR:37.2800 R:35.0 gloss:13.2605 dloss:20.5697 exploreP:0.4119\n",
      "Episode:326 meanR:37.4100 R:87.0 gloss:13.5185 dloss:20.5977 exploreP:0.4084\n",
      "Episode:327 meanR:37.8600 R:74.0 gloss:13.5631 dloss:20.9808 exploreP:0.4054\n",
      "Episode:328 meanR:38.2900 R:64.0 gloss:13.8831 dloss:20.9112 exploreP:0.4029\n",
      "Episode:329 meanR:39.0000 R:83.0 gloss:14.5756 dloss:21.9077 exploreP:0.3997\n",
      "Episode:330 meanR:38.8200 R:21.0 gloss:14.5045 dloss:22.3210 exploreP:0.3989\n",
      "Episode:331 meanR:39.0200 R:56.0 gloss:14.2327 dloss:21.4943 exploreP:0.3967\n",
      "Episode:332 meanR:38.8300 R:24.0 gloss:14.7648 dloss:21.8332 exploreP:0.3958\n",
      "Episode:333 meanR:39.1800 R:50.0 gloss:14.4786 dloss:21.5456 exploreP:0.3938\n",
      "Episode:334 meanR:39.7200 R:66.0 gloss:13.9311 dloss:21.4666 exploreP:0.3913\n",
      "Episode:335 meanR:39.6600 R:22.0 gloss:13.7931 dloss:21.2300 exploreP:0.3905\n",
      "Episode:336 meanR:39.4900 R:34.0 gloss:13.5213 dloss:21.2460 exploreP:0.3892\n",
      "Episode:337 meanR:39.0000 R:16.0 gloss:13.1654 dloss:21.2421 exploreP:0.3886\n",
      "Episode:338 meanR:38.9800 R:12.0 gloss:12.9834 dloss:20.5199 exploreP:0.3881\n",
      "Episode:339 meanR:38.8000 R:31.0 gloss:12.5948 dloss:20.3700 exploreP:0.3869\n",
      "Episode:340 meanR:39.1800 R:50.0 gloss:12.5920 dloss:19.9999 exploreP:0.3851\n",
      "Episode:341 meanR:39.2700 R:65.0 gloss:13.2758 dloss:20.6479 exploreP:0.3826\n",
      "Episode:342 meanR:39.1900 R:36.0 gloss:13.9137 dloss:21.2462 exploreP:0.3813\n",
      "Episode:343 meanR:39.4300 R:47.0 gloss:13.9990 dloss:21.3464 exploreP:0.3796\n",
      "Episode:344 meanR:39.8500 R:54.0 gloss:13.9129 dloss:21.5276 exploreP:0.3776\n",
      "Episode:345 meanR:40.0000 R:74.0 gloss:13.6054 dloss:21.2398 exploreP:0.3749\n",
      "Episode:346 meanR:40.1300 R:30.0 gloss:13.7099 dloss:21.3531 exploreP:0.3738\n",
      "Episode:347 meanR:39.9800 R:15.0 gloss:13.6338 dloss:21.2680 exploreP:0.3732\n",
      "Episode:348 meanR:40.0500 R:25.0 gloss:13.4026 dloss:21.3208 exploreP:0.3723\n",
      "Episode:349 meanR:40.0300 R:15.0 gloss:13.0907 dloss:21.2233 exploreP:0.3718\n",
      "Episode:350 meanR:39.8400 R:30.0 gloss:12.6073 dloss:20.4196 exploreP:0.3707\n",
      "Episode:351 meanR:39.6500 R:25.0 gloss:12.5058 dloss:20.0091 exploreP:0.3698\n",
      "Episode:352 meanR:39.1600 R:14.0 gloss:12.5380 dloss:20.6325 exploreP:0.3693\n",
      "Episode:353 meanR:39.2900 R:46.0 gloss:11.9590 dloss:19.5193 exploreP:0.3676\n",
      "Episode:354 meanR:39.4100 R:80.0 gloss:11.7511 dloss:19.4940 exploreP:0.3648\n",
      "Episode:355 meanR:39.4700 R:33.0 gloss:11.6020 dloss:19.8415 exploreP:0.3636\n",
      "Episode:356 meanR:39.3400 R:27.0 gloss:11.2325 dloss:19.4194 exploreP:0.3627\n",
      "Episode:357 meanR:39.6300 R:58.0 gloss:11.1944 dloss:19.1325 exploreP:0.3606\n",
      "Episode:358 meanR:39.6000 R:15.0 gloss:11.1712 dloss:19.8676 exploreP:0.3601\n",
      "Episode:359 meanR:39.7600 R:26.0 gloss:10.7964 dloss:18.8870 exploreP:0.3592\n",
      "Episode:360 meanR:40.1000 R:57.0 gloss:10.7879 dloss:19.0484 exploreP:0.3572\n",
      "Episode:361 meanR:39.7500 R:14.0 gloss:10.9208 dloss:19.3209 exploreP:0.3567\n",
      "Episode:362 meanR:39.3400 R:36.0 gloss:10.7203 dloss:19.0006 exploreP:0.3555\n",
      "Episode:363 meanR:39.2200 R:10.0 gloss:10.5539 dloss:19.1382 exploreP:0.3551\n",
      "Episode:364 meanR:39.0400 R:10.0 gloss:10.3873 dloss:18.6783 exploreP:0.3548\n",
      "Episode:365 meanR:39.5100 R:61.0 gloss:10.3045 dloss:18.2247 exploreP:0.3527\n",
      "Episode:366 meanR:39.3000 R:47.0 gloss:10.6755 dloss:18.7663 exploreP:0.3511\n",
      "Episode:367 meanR:39.2500 R:43.0 gloss:10.7651 dloss:19.1170 exploreP:0.3496\n",
      "Episode:368 meanR:39.3900 R:43.0 gloss:10.7607 dloss:19.0117 exploreP:0.3482\n",
      "Episode:369 meanR:39.4700 R:37.0 gloss:10.6761 dloss:18.8533 exploreP:0.3469\n",
      "Episode:370 meanR:39.5000 R:34.0 gloss:10.6327 dloss:18.8532 exploreP:0.3458\n",
      "Episode:371 meanR:39.2000 R:38.0 gloss:10.6836 dloss:18.8815 exploreP:0.3445\n",
      "Episode:372 meanR:39.4100 R:52.0 gloss:10.7016 dloss:18.7199 exploreP:0.3428\n",
      "Episode:373 meanR:39.4000 R:11.0 gloss:10.7312 dloss:18.8246 exploreP:0.3424\n",
      "Episode:374 meanR:39.3200 R:56.0 gloss:10.5148 dloss:18.4876 exploreP:0.3405\n",
      "Episode:375 meanR:39.4100 R:27.0 gloss:10.2753 dloss:18.2647 exploreP:0.3396\n",
      "Episode:376 meanR:39.8400 R:54.0 gloss:10.5753 dloss:18.3234 exploreP:0.3379\n",
      "Episode:377 meanR:39.9300 R:48.0 gloss:10.9750 dloss:18.8089 exploreP:0.3363\n",
      "Episode:378 meanR:39.9000 R:26.0 gloss:11.0396 dloss:19.1416 exploreP:0.3354\n",
      "Episode:379 meanR:40.1600 R:66.0 gloss:11.1939 dloss:19.2380 exploreP:0.3333\n",
      "Episode:380 meanR:40.1400 R:38.0 gloss:11.4504 dloss:19.6377 exploreP:0.3321\n",
      "Episode:381 meanR:40.5300 R:60.0 gloss:10.9517 dloss:19.2048 exploreP:0.3302\n",
      "Episode:382 meanR:40.4900 R:36.0 gloss:11.1359 dloss:19.3222 exploreP:0.3290\n",
      "Episode:383 meanR:40.8500 R:62.0 gloss:11.1226 dloss:19.4336 exploreP:0.3270\n",
      "Episode:384 meanR:41.3400 R:68.0 gloss:11.2811 dloss:19.8112 exploreP:0.3249\n",
      "Episode:385 meanR:41.4700 R:58.0 gloss:11.0922 dloss:19.9342 exploreP:0.3231\n",
      "Episode:386 meanR:41.5700 R:24.0 gloss:11.2779 dloss:20.5274 exploreP:0.3223\n",
      "Episode:387 meanR:41.5800 R:31.0 gloss:11.4180 dloss:20.2341 exploreP:0.3213\n",
      "Episode:388 meanR:41.5200 R:33.0 gloss:11.5304 dloss:20.9115 exploreP:0.3203\n",
      "Episode:389 meanR:41.4500 R:32.0 gloss:11.5336 dloss:20.3106 exploreP:0.3193\n",
      "Episode:390 meanR:41.4800 R:12.0 gloss:11.6595 dloss:21.1164 exploreP:0.3190\n",
      "Episode:391 meanR:41.4900 R:14.0 gloss:11.3468 dloss:20.4131 exploreP:0.3185\n",
      "Episode:392 meanR:41.5900 R:53.0 gloss:11.1272 dloss:19.6460 exploreP:0.3169\n",
      "Episode:393 meanR:41.3200 R:25.0 gloss:11.2855 dloss:20.2923 exploreP:0.3161\n",
      "Episode:394 meanR:41.2600 R:37.0 gloss:10.9152 dloss:19.6425 exploreP:0.3150\n",
      "Episode:395 meanR:41.3000 R:39.0 gloss:10.9242 dloss:19.7304 exploreP:0.3138\n",
      "Episode:396 meanR:41.3700 R:23.0 gloss:11.0220 dloss:19.8534 exploreP:0.3131\n",
      "Episode:397 meanR:41.3700 R:15.0 gloss:10.8009 dloss:20.0804 exploreP:0.3127\n",
      "Episode:398 meanR:41.0900 R:41.0 gloss:10.7707 dloss:19.3793 exploreP:0.3114\n",
      "Episode:399 meanR:40.3900 R:14.0 gloss:10.6259 dloss:19.6429 exploreP:0.3110\n",
      "Episode:400 meanR:40.3100 R:31.0 gloss:10.6312 dloss:19.0664 exploreP:0.3101\n",
      "Episode:401 meanR:39.8000 R:24.0 gloss:10.5813 dloss:19.6191 exploreP:0.3093\n",
      "Episode:402 meanR:39.7900 R:40.0 gloss:10.4314 dloss:18.8372 exploreP:0.3082\n",
      "Episode:403 meanR:39.8400 R:33.0 gloss:10.4561 dloss:19.2906 exploreP:0.3072\n",
      "Episode:404 meanR:39.8300 R:29.0 gloss:10.1692 dloss:18.8576 exploreP:0.3063\n",
      "Episode:405 meanR:39.3000 R:29.0 gloss:10.1846 dloss:19.0558 exploreP:0.3055\n",
      "Episode:406 meanR:38.9100 R:10.0 gloss:10.4712 dloss:18.8705 exploreP:0.3052\n",
      "Episode:407 meanR:39.4500 R:66.0 gloss:10.2147 dloss:18.5682 exploreP:0.3032\n",
      "Episode:408 meanR:39.6000 R:29.0 gloss:10.1211 dloss:18.5701 exploreP:0.3024\n",
      "Episode:409 meanR:39.6500 R:41.0 gloss:10.1955 dloss:18.4250 exploreP:0.3012\n",
      "Episode:410 meanR:39.2300 R:37.0 gloss:10.1960 dloss:18.6220 exploreP:0.3001\n",
      "Episode:411 meanR:39.2800 R:52.0 gloss:10.0311 dloss:18.2807 exploreP:0.2986\n",
      "Episode:412 meanR:39.0700 R:17.0 gloss:10.1800 dloss:18.6079 exploreP:0.2981\n",
      "Episode:413 meanR:38.1600 R:17.0 gloss:10.0768 dloss:18.6182 exploreP:0.2976\n",
      "Episode:414 meanR:37.8500 R:39.0 gloss:9.7803 dloss:17.7704 exploreP:0.2965\n",
      "Episode:415 meanR:37.5700 R:12.0 gloss:9.9414 dloss:18.4143 exploreP:0.2961\n",
      "Episode:416 meanR:37.5300 R:33.0 gloss:9.5634 dloss:17.7100 exploreP:0.2952\n",
      "Episode:417 meanR:37.6400 R:22.0 gloss:9.4817 dloss:17.3306 exploreP:0.2946\n",
      "Episode:418 meanR:37.7800 R:34.0 gloss:9.4306 dloss:17.3668 exploreP:0.2936\n",
      "Episode:419 meanR:37.9900 R:53.0 gloss:9.7094 dloss:17.0527 exploreP:0.2921\n",
      "Episode:420 meanR:38.0700 R:25.0 gloss:10.0643 dloss:17.4407 exploreP:0.2914\n",
      "Episode:421 meanR:38.3800 R:40.0 gloss:9.8407 dloss:17.2426 exploreP:0.2903\n",
      "Episode:422 meanR:37.9800 R:18.0 gloss:9.7963 dloss:17.5859 exploreP:0.2898\n",
      "Episode:423 meanR:37.7600 R:50.0 gloss:9.8054 dloss:17.3699 exploreP:0.2884\n",
      "Episode:424 meanR:37.3400 R:28.0 gloss:9.9310 dloss:17.7244 exploreP:0.2876\n",
      "Episode:425 meanR:37.4600 R:47.0 gloss:9.8039 dloss:17.5058 exploreP:0.2863\n",
      "Episode:426 meanR:36.8500 R:26.0 gloss:9.8529 dloss:17.7076 exploreP:0.2856\n",
      "Episode:427 meanR:36.4400 R:33.0 gloss:9.6859 dloss:17.5809 exploreP:0.2847\n",
      "Episode:428 meanR:36.0500 R:25.0 gloss:9.5266 dloss:17.5790 exploreP:0.2840\n",
      "Episode:429 meanR:35.3300 R:11.0 gloss:9.5929 dloss:17.7010 exploreP:0.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:430 meanR:35.5500 R:43.0 gloss:9.2953 dloss:17.5232 exploreP:0.2825\n",
      "Episode:431 meanR:35.4900 R:50.0 gloss:9.2967 dloss:17.6757 exploreP:0.2812\n",
      "Episode:432 meanR:35.8700 R:62.0 gloss:9.4646 dloss:18.0042 exploreP:0.2795\n",
      "Episode:433 meanR:35.6200 R:25.0 gloss:9.5457 dloss:18.3695 exploreP:0.2788\n",
      "Episode:434 meanR:35.3100 R:35.0 gloss:9.5134 dloss:18.0277 exploreP:0.2779\n",
      "Episode:435 meanR:35.4400 R:35.0 gloss:9.5653 dloss:18.1101 exploreP:0.2769\n",
      "Episode:436 meanR:35.5600 R:46.0 gloss:9.6875 dloss:18.5578 exploreP:0.2757\n",
      "Episode:437 meanR:35.5500 R:15.0 gloss:9.5815 dloss:18.6030 exploreP:0.2753\n",
      "Episode:438 meanR:35.9300 R:50.0 gloss:9.2979 dloss:18.0256 exploreP:0.2740\n",
      "Episode:439 meanR:35.9500 R:33.0 gloss:9.3050 dloss:18.2557 exploreP:0.2731\n",
      "Episode:440 meanR:35.5800 R:13.0 gloss:9.4716 dloss:18.6649 exploreP:0.2728\n",
      "Episode:441 meanR:35.3500 R:42.0 gloss:9.4406 dloss:17.9791 exploreP:0.2717\n",
      "Episode:442 meanR:35.3800 R:39.0 gloss:9.2838 dloss:17.8713 exploreP:0.2707\n",
      "Episode:443 meanR:35.3300 R:42.0 gloss:9.4083 dloss:18.1978 exploreP:0.2696\n",
      "Episode:444 meanR:35.1900 R:40.0 gloss:9.5441 dloss:18.1934 exploreP:0.2685\n",
      "Episode:445 meanR:35.0900 R:64.0 gloss:9.9296 dloss:18.6930 exploreP:0.2669\n",
      "Episode:446 meanR:34.9100 R:12.0 gloss:10.1647 dloss:19.5084 exploreP:0.2666\n",
      "Episode:447 meanR:35.0400 R:28.0 gloss:9.9327 dloss:18.7642 exploreP:0.2659\n",
      "Episode:448 meanR:35.4100 R:62.0 gloss:9.7357 dloss:18.5394 exploreP:0.2643\n",
      "Episode:449 meanR:35.5200 R:26.0 gloss:9.9350 dloss:19.1656 exploreP:0.2636\n",
      "Episode:450 meanR:35.5600 R:34.0 gloss:9.9895 dloss:18.5642 exploreP:0.2628\n",
      "Episode:451 meanR:35.7100 R:40.0 gloss:9.9691 dloss:18.9014 exploreP:0.2617\n",
      "Episode:452 meanR:35.8200 R:25.0 gloss:10.0746 dloss:18.8951 exploreP:0.2611\n",
      "Episode:453 meanR:35.9600 R:60.0 gloss:10.2489 dloss:18.7147 exploreP:0.2596\n",
      "Episode:454 meanR:35.6400 R:48.0 gloss:10.4800 dloss:18.9820 exploreP:0.2584\n",
      "Episode:455 meanR:35.5000 R:19.0 gloss:10.5594 dloss:19.4164 exploreP:0.2579\n",
      "Episode:456 meanR:35.8300 R:60.0 gloss:10.5268 dloss:19.1637 exploreP:0.2565\n",
      "Episode:457 meanR:35.3900 R:14.0 gloss:10.6685 dloss:19.7717 exploreP:0.2561\n",
      "Episode:458 meanR:35.8300 R:59.0 gloss:10.3734 dloss:19.2158 exploreP:0.2547\n",
      "Episode:459 meanR:35.7800 R:21.0 gloss:10.3526 dloss:19.6769 exploreP:0.2542\n",
      "Episode:460 meanR:35.7000 R:49.0 gloss:10.0395 dloss:18.8077 exploreP:0.2530\n",
      "Episode:461 meanR:35.8900 R:33.0 gloss:10.0901 dloss:19.2603 exploreP:0.2522\n",
      "Episode:462 meanR:35.8100 R:28.0 gloss:10.1458 dloss:19.2823 exploreP:0.2515\n",
      "Episode:463 meanR:36.0400 R:33.0 gloss:9.9729 dloss:19.1266 exploreP:0.2507\n",
      "Episode:464 meanR:36.4000 R:46.0 gloss:9.8637 dloss:18.9104 exploreP:0.2496\n",
      "Episode:465 meanR:36.5800 R:79.0 gloss:9.9441 dloss:19.2545 exploreP:0.2477\n",
      "Episode:466 meanR:36.4600 R:35.0 gloss:10.2741 dloss:19.9736 exploreP:0.2469\n",
      "Episode:467 meanR:36.4500 R:42.0 gloss:10.2947 dloss:19.7174 exploreP:0.2459\n",
      "Episode:468 meanR:36.4900 R:47.0 gloss:10.4901 dloss:19.7080 exploreP:0.2448\n",
      "Episode:469 meanR:36.4300 R:31.0 gloss:10.6744 dloss:19.9597 exploreP:0.2440\n",
      "Episode:470 meanR:36.6300 R:54.0 gloss:10.6521 dloss:19.7388 exploreP:0.2428\n",
      "Episode:471 meanR:36.5700 R:32.0 gloss:10.5981 dloss:19.4158 exploreP:0.2420\n",
      "Episode:472 meanR:36.3300 R:28.0 gloss:10.9773 dloss:19.9661 exploreP:0.2414\n",
      "Episode:473 meanR:36.6900 R:47.0 gloss:10.8857 dloss:19.8613 exploreP:0.2403\n",
      "Episode:474 meanR:36.7100 R:58.0 gloss:10.9194 dloss:19.6727 exploreP:0.2390\n",
      "Episode:475 meanR:36.6600 R:22.0 gloss:11.1422 dloss:20.1422 exploreP:0.2385\n",
      "Episode:476 meanR:36.4000 R:28.0 gloss:10.9130 dloss:20.0117 exploreP:0.2378\n",
      "Episode:477 meanR:36.2700 R:35.0 gloss:10.6782 dloss:19.5670 exploreP:0.2370\n",
      "Episode:478 meanR:36.3700 R:36.0 gloss:10.4626 dloss:20.0192 exploreP:0.2362\n",
      "Episode:479 meanR:36.1100 R:40.0 gloss:10.0205 dloss:19.5457 exploreP:0.2353\n",
      "Episode:480 meanR:36.0800 R:35.0 gloss:9.8293 dloss:19.5811 exploreP:0.2345\n",
      "Episode:481 meanR:35.7600 R:28.0 gloss:9.7422 dloss:19.6375 exploreP:0.2339\n",
      "Episode:482 meanR:35.8100 R:41.0 gloss:9.6271 dloss:19.5986 exploreP:0.2330\n",
      "Episode:483 meanR:35.4700 R:28.0 gloss:9.5976 dloss:19.3061 exploreP:0.2324\n",
      "Episode:484 meanR:35.4100 R:62.0 gloss:9.5812 dloss:19.5022 exploreP:0.2310\n",
      "Episode:485 meanR:35.5200 R:69.0 gloss:10.1017 dloss:19.7249 exploreP:0.2295\n",
      "Episode:486 meanR:35.8300 R:55.0 gloss:10.1468 dloss:19.9548 exploreP:0.2283\n",
      "Episode:487 meanR:35.6200 R:10.0 gloss:10.6187 dloss:21.0529 exploreP:0.2280\n",
      "Episode:488 meanR:35.7100 R:42.0 gloss:10.3105 dloss:20.0041 exploreP:0.2271\n",
      "Episode:489 meanR:35.6900 R:30.0 gloss:10.3869 dloss:19.9909 exploreP:0.2265\n",
      "Episode:490 meanR:35.9500 R:38.0 gloss:10.3777 dloss:20.0210 exploreP:0.2257\n",
      "Episode:491 meanR:36.1900 R:38.0 gloss:10.2940 dloss:19.4860 exploreP:0.2248\n",
      "Episode:492 meanR:36.1000 R:44.0 gloss:10.3144 dloss:19.5443 exploreP:0.2239\n",
      "Episode:493 meanR:36.3000 R:45.0 gloss:10.3167 dloss:19.5984 exploreP:0.2229\n",
      "Episode:494 meanR:36.4800 R:55.0 gloss:10.2729 dloss:19.6191 exploreP:0.2218\n",
      "Episode:495 meanR:36.5000 R:41.0 gloss:9.9997 dloss:19.8368 exploreP:0.2209\n",
      "Episode:496 meanR:36.8700 R:60.0 gloss:9.8016 dloss:19.5959 exploreP:0.2196\n",
      "Episode:497 meanR:37.1000 R:38.0 gloss:9.8511 dloss:20.0302 exploreP:0.2188\n",
      "Episode:498 meanR:37.0500 R:36.0 gloss:9.5313 dloss:19.9184 exploreP:0.2181\n",
      "Episode:499 meanR:37.2000 R:29.0 gloss:9.3359 dloss:20.1801 exploreP:0.2175\n",
      "Episode:500 meanR:37.6200 R:73.0 gloss:8.8475 dloss:19.6542 exploreP:0.2160\n",
      "Episode:501 meanR:37.5000 R:12.0 gloss:9.1137 dloss:20.8048 exploreP:0.2157\n",
      "Episode:502 meanR:37.3800 R:28.0 gloss:8.8714 dloss:19.9833 exploreP:0.2152\n",
      "Episode:503 meanR:37.6700 R:62.0 gloss:8.7726 dloss:19.9773 exploreP:0.2139\n",
      "Episode:504 meanR:37.7800 R:40.0 gloss:8.8744 dloss:20.0416 exploreP:0.2131\n",
      "Episode:505 meanR:37.5900 R:10.0 gloss:8.9439 dloss:20.4858 exploreP:0.2129\n",
      "Episode:506 meanR:37.6200 R:13.0 gloss:8.8060 dloss:20.8026 exploreP:0.2126\n",
      "Episode:507 meanR:37.5300 R:57.0 gloss:8.7369 dloss:19.6155 exploreP:0.2115\n",
      "Episode:508 meanR:37.6300 R:39.0 gloss:8.9736 dloss:20.0376 exploreP:0.2107\n",
      "Episode:509 meanR:37.6500 R:43.0 gloss:8.8523 dloss:19.9088 exploreP:0.2098\n",
      "Episode:510 meanR:37.6400 R:36.0 gloss:8.7747 dloss:19.9941 exploreP:0.2091\n",
      "Episode:511 meanR:37.5500 R:43.0 gloss:8.4249 dloss:19.4848 exploreP:0.2082\n",
      "Episode:512 meanR:37.8600 R:48.0 gloss:8.4835 dloss:19.5316 exploreP:0.2073\n",
      "Episode:513 meanR:38.1600 R:47.0 gloss:8.4378 dloss:19.7210 exploreP:0.2064\n",
      "Episode:514 meanR:38.4400 R:67.0 gloss:8.5920 dloss:19.8913 exploreP:0.2051\n",
      "Episode:515 meanR:38.6200 R:30.0 gloss:8.7489 dloss:20.3304 exploreP:0.2045\n",
      "Episode:516 meanR:38.8400 R:55.0 gloss:8.7157 dloss:20.0007 exploreP:0.2034\n",
      "Episode:517 meanR:39.0500 R:43.0 gloss:8.7634 dloss:20.2165 exploreP:0.2026\n",
      "Episode:518 meanR:39.0400 R:33.0 gloss:8.7324 dloss:20.3295 exploreP:0.2019\n",
      "Episode:519 meanR:38.9900 R:48.0 gloss:8.5597 dloss:19.9568 exploreP:0.2010\n",
      "Episode:520 meanR:39.1900 R:45.0 gloss:8.6861 dloss:20.0740 exploreP:0.2002\n",
      "Episode:521 meanR:39.4700 R:68.0 gloss:8.7624 dloss:19.9565 exploreP:0.1989\n",
      "Episode:522 meanR:39.6400 R:35.0 gloss:9.0290 dloss:20.2197 exploreP:0.1982\n",
      "Episode:523 meanR:39.5700 R:43.0 gloss:8.9429 dloss:20.1210 exploreP:0.1974\n",
      "Episode:524 meanR:39.7800 R:49.0 gloss:8.8044 dloss:20.3695 exploreP:0.1965\n",
      "Episode:525 meanR:39.7500 R:44.0 gloss:8.7023 dloss:20.0406 exploreP:0.1957\n",
      "Episode:526 meanR:40.2800 R:79.0 gloss:9.0244 dloss:20.4418 exploreP:0.1942\n",
      "Episode:527 meanR:40.3900 R:44.0 gloss:8.9661 dloss:20.4987 exploreP:0.1934\n",
      "Episode:528 meanR:40.6200 R:48.0 gloss:9.2007 dloss:21.2866 exploreP:0.1925\n",
      "Episode:529 meanR:40.7500 R:24.0 gloss:9.1035 dloss:21.0754 exploreP:0.1921\n",
      "Episode:530 meanR:40.8300 R:51.0 gloss:9.5212 dloss:20.9498 exploreP:0.1912\n",
      "Episode:531 meanR:40.8700 R:54.0 gloss:9.2535 dloss:21.1386 exploreP:0.1902\n",
      "Episode:532 meanR:41.0000 R:75.0 gloss:9.0467 dloss:21.2484 exploreP:0.1888\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            #batch = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
