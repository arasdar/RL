{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66, 6), (66, 4), (66,), (66, 4), (66,), (66,), (66,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 R:17.0000 rate:0.0340 gloss:0.6487 dlossA:0.7176 dlossQ:0.7791 exploreP:0.9983\n",
      "Episode:1 meanR:20.5000 R:24.0000 rate:0.0480 gloss:0.6516 dlossA:0.7274 dlossQ:0.7762 exploreP:0.9959\n",
      "Episode:2 meanR:20.3333 R:20.0000 rate:0.0400 gloss:0.6557 dlossA:0.7273 dlossQ:0.7804 exploreP:0.9940\n",
      "Episode:3 meanR:20.5000 R:21.0000 rate:0.0420 gloss:0.6638 dlossA:0.7314 dlossQ:0.7767 exploreP:0.9919\n",
      "Episode:4 meanR:21.4000 R:25.0000 rate:0.0500 gloss:0.6516 dlossA:0.7231 dlossQ:0.7868 exploreP:0.9895\n",
      "Episode:5 meanR:19.8333 R:12.0000 rate:0.0240 gloss:0.6518 dlossA:0.7304 dlossQ:0.7834 exploreP:0.9883\n",
      "Episode:6 meanR:21.1429 R:29.0000 rate:0.0580 gloss:0.6381 dlossA:0.7186 dlossQ:0.8007 exploreP:0.9855\n",
      "Episode:7 meanR:23.3750 R:39.0000 rate:0.0780 gloss:0.6706 dlossA:0.7294 dlossQ:0.7866 exploreP:0.9817\n",
      "Episode:8 meanR:22.4444 R:15.0000 rate:0.0300 gloss:0.6852 dlossA:0.7277 dlossQ:0.7918 exploreP:0.9802\n",
      "Episode:9 meanR:24.7000 R:45.0000 rate:0.0900 gloss:0.6590 dlossA:0.7321 dlossQ:0.7848 exploreP:0.9758\n",
      "Episode:10 meanR:23.7273 R:14.0000 rate:0.0280 gloss:0.6549 dlossA:0.7347 dlossQ:0.7824 exploreP:0.9745\n",
      "Episode:11 meanR:22.8333 R:13.0000 rate:0.0260 gloss:0.6375 dlossA:0.7204 dlossQ:0.7821 exploreP:0.9732\n",
      "Episode:12 meanR:23.5385 R:32.0000 rate:0.0640 gloss:0.6641 dlossA:0.7294 dlossQ:0.8009 exploreP:0.9702\n",
      "Episode:13 meanR:23.2857 R:20.0000 rate:0.0400 gloss:0.6578 dlossA:0.7289 dlossQ:0.7927 exploreP:0.9682\n",
      "Episode:14 meanR:25.6000 R:58.0000 rate:0.1160 gloss:0.6563 dlossA:0.7320 dlossQ:0.7857 exploreP:0.9627\n",
      "Episode:15 meanR:24.9375 R:15.0000 rate:0.0300 gloss:0.6614 dlossA:0.7310 dlossQ:0.7857 exploreP:0.9613\n",
      "Episode:16 meanR:26.7647 R:56.0000 rate:0.1120 gloss:0.6490 dlossA:0.7257 dlossQ:0.7933 exploreP:0.9560\n",
      "Episode:17 meanR:26.6111 R:24.0000 rate:0.0480 gloss:0.6573 dlossA:0.7308 dlossQ:0.7832 exploreP:0.9537\n",
      "Episode:18 meanR:27.7368 R:48.0000 rate:0.0960 gloss:0.6570 dlossA:0.7310 dlossQ:0.7872 exploreP:0.9492\n",
      "Episode:19 meanR:27.4500 R:22.0000 rate:0.0440 gloss:0.6508 dlossA:0.7284 dlossQ:0.7927 exploreP:0.9471\n",
      "Episode:20 meanR:26.8571 R:15.0000 rate:0.0300 gloss:0.6664 dlossA:0.7372 dlossQ:0.7900 exploreP:0.9457\n",
      "Episode:21 meanR:26.1364 R:11.0000 rate:0.0220 gloss:0.6541 dlossA:0.7345 dlossQ:0.8098 exploreP:0.9447\n",
      "Episode:22 meanR:26.6522 R:38.0000 rate:0.0760 gloss:0.6616 dlossA:0.7343 dlossQ:0.7941 exploreP:0.9411\n",
      "Episode:23 meanR:26.2500 R:17.0000 rate:0.0340 gloss:0.6432 dlossA:0.7153 dlossQ:0.7946 exploreP:0.9396\n",
      "Episode:24 meanR:26.2800 R:27.0000 rate:0.0540 gloss:0.6614 dlossA:0.7273 dlossQ:0.7959 exploreP:0.9370\n",
      "Episode:25 meanR:26.6538 R:36.0000 rate:0.0720 gloss:0.6628 dlossA:0.7270 dlossQ:0.7948 exploreP:0.9337\n",
      "Episode:26 meanR:26.7778 R:30.0000 rate:0.0600 gloss:0.6660 dlossA:0.7376 dlossQ:0.7911 exploreP:0.9309\n",
      "Episode:27 meanR:26.4286 R:17.0000 rate:0.0340 gloss:0.6545 dlossA:0.7320 dlossQ:0.7881 exploreP:0.9294\n",
      "Episode:28 meanR:26.8276 R:38.0000 rate:0.0760 gloss:0.6576 dlossA:0.7267 dlossQ:0.7956 exploreP:0.9259\n",
      "Episode:29 meanR:27.8000 R:56.0000 rate:0.1120 gloss:0.6619 dlossA:0.7281 dlossQ:0.7981 exploreP:0.9208\n",
      "Episode:30 meanR:27.8065 R:28.0000 rate:0.0560 gloss:0.6659 dlossA:0.7311 dlossQ:0.7952 exploreP:0.9182\n",
      "Episode:31 meanR:27.8438 R:29.0000 rate:0.0580 gloss:0.6558 dlossA:0.7282 dlossQ:0.7942 exploreP:0.9156\n",
      "Episode:32 meanR:27.4242 R:14.0000 rate:0.0280 gloss:0.6732 dlossA:0.7377 dlossQ:0.7952 exploreP:0.9143\n",
      "Episode:33 meanR:28.2059 R:54.0000 rate:0.1080 gloss:0.6534 dlossA:0.7249 dlossQ:0.7979 exploreP:0.9095\n",
      "Episode:34 meanR:28.0857 R:24.0000 rate:0.0480 gloss:0.6562 dlossA:0.7259 dlossQ:0.7976 exploreP:0.9073\n",
      "Episode:35 meanR:27.8611 R:20.0000 rate:0.0400 gloss:0.6495 dlossA:0.7161 dlossQ:0.8079 exploreP:0.9055\n",
      "Episode:36 meanR:27.6216 R:19.0000 rate:0.0380 gloss:0.6618 dlossA:0.7311 dlossQ:0.8045 exploreP:0.9038\n",
      "Episode:37 meanR:28.1579 R:48.0000 rate:0.0960 gloss:0.6571 dlossA:0.7210 dlossQ:0.8024 exploreP:0.8995\n",
      "Episode:38 meanR:27.7949 R:14.0000 rate:0.0280 gloss:0.6810 dlossA:0.7379 dlossQ:0.7928 exploreP:0.8983\n",
      "Episode:39 meanR:27.4500 R:14.0000 rate:0.0280 gloss:0.6417 dlossA:0.7080 dlossQ:0.8158 exploreP:0.8971\n",
      "Episode:40 meanR:27.1707 R:16.0000 rate:0.0320 gloss:0.6543 dlossA:0.7277 dlossQ:0.7973 exploreP:0.8956\n",
      "Episode:41 meanR:28.3571 R:77.0000 rate:0.1540 gloss:0.6441 dlossA:0.7203 dlossQ:0.8049 exploreP:0.8888\n",
      "Episode:42 meanR:28.5581 R:37.0000 rate:0.0740 gloss:0.6579 dlossA:0.7215 dlossQ:0.8098 exploreP:0.8856\n",
      "Episode:43 meanR:28.5909 R:30.0000 rate:0.0600 gloss:0.6466 dlossA:0.7202 dlossQ:0.8055 exploreP:0.8830\n",
      "Episode:44 meanR:28.4000 R:20.0000 rate:0.0400 gloss:0.6586 dlossA:0.7268 dlossQ:0.8005 exploreP:0.8812\n",
      "Episode:45 meanR:29.6957 R:88.0000 rate:0.1760 gloss:0.6571 dlossA:0.7271 dlossQ:0.8061 exploreP:0.8736\n",
      "Episode:46 meanR:29.4468 R:18.0000 rate:0.0360 gloss:0.6537 dlossA:0.7228 dlossQ:0.8100 exploreP:0.8720\n",
      "Episode:47 meanR:29.0417 R:10.0000 rate:0.0200 gloss:0.6385 dlossA:0.7180 dlossQ:0.8002 exploreP:0.8712\n",
      "Episode:48 meanR:28.7143 R:13.0000 rate:0.0260 gloss:0.6787 dlossA:0.7346 dlossQ:0.8053 exploreP:0.8701\n",
      "Episode:49 meanR:28.4800 R:17.0000 rate:0.0340 gloss:0.6590 dlossA:0.7282 dlossQ:0.8070 exploreP:0.8686\n",
      "Episode:50 meanR:28.8431 R:47.0000 rate:0.0940 gloss:0.6498 dlossA:0.7207 dlossQ:0.8066 exploreP:0.8646\n",
      "Episode:51 meanR:28.6346 R:18.0000 rate:0.0360 gloss:0.6511 dlossA:0.7253 dlossQ:0.8089 exploreP:0.8630\n",
      "Episode:52 meanR:28.6415 R:29.0000 rate:0.0580 gloss:0.6656 dlossA:0.7288 dlossQ:0.8098 exploreP:0.8606\n",
      "Episode:53 meanR:28.5741 R:25.0000 rate:0.0500 gloss:0.6711 dlossA:0.7303 dlossQ:0.8067 exploreP:0.8584\n",
      "Episode:54 meanR:28.4727 R:23.0000 rate:0.0460 gloss:0.6580 dlossA:0.7236 dlossQ:0.8156 exploreP:0.8565\n",
      "Episode:55 meanR:28.2500 R:16.0000 rate:0.0320 gloss:0.6581 dlossA:0.7270 dlossQ:0.8161 exploreP:0.8551\n",
      "Episode:56 meanR:28.0702 R:18.0000 rate:0.0360 gloss:0.6573 dlossA:0.7271 dlossQ:0.8149 exploreP:0.8536\n",
      "Episode:57 meanR:28.4483 R:50.0000 rate:0.1000 gloss:0.6544 dlossA:0.7268 dlossQ:0.8038 exploreP:0.8494\n",
      "Episode:58 meanR:28.3898 R:25.0000 rate:0.0500 gloss:0.6423 dlossA:0.7182 dlossQ:0.8066 exploreP:0.8473\n",
      "Episode:59 meanR:28.1833 R:16.0000 rate:0.0320 gloss:0.6605 dlossA:0.7243 dlossQ:0.8034 exploreP:0.8460\n",
      "Episode:60 meanR:27.9672 R:15.0000 rate:0.0300 gloss:0.6448 dlossA:0.7117 dlossQ:0.8260 exploreP:0.8447\n",
      "Episode:61 meanR:27.6935 R:11.0000 rate:0.0220 gloss:0.6814 dlossA:0.7360 dlossQ:0.8101 exploreP:0.8438\n",
      "Episode:62 meanR:27.7778 R:33.0000 rate:0.0660 gloss:0.6581 dlossA:0.7243 dlossQ:0.8122 exploreP:0.8411\n",
      "Episode:63 meanR:28.2656 R:59.0000 rate:0.1180 gloss:0.6572 dlossA:0.7249 dlossQ:0.8142 exploreP:0.8362\n",
      "Episode:64 meanR:28.3231 R:32.0000 rate:0.0640 gloss:0.6697 dlossA:0.7308 dlossQ:0.8053 exploreP:0.8335\n",
      "Episode:65 meanR:28.4697 R:38.0000 rate:0.0760 gloss:0.6584 dlossA:0.7269 dlossQ:0.8146 exploreP:0.8304\n",
      "Episode:66 meanR:28.2090 R:11.0000 rate:0.0220 gloss:0.6640 dlossA:0.7267 dlossQ:0.8109 exploreP:0.8295\n",
      "Episode:67 meanR:28.0588 R:18.0000 rate:0.0360 gloss:0.6598 dlossA:0.7272 dlossQ:0.8121 exploreP:0.8280\n",
      "Episode:68 meanR:28.5072 R:59.0000 rate:0.1180 gloss:0.6417 dlossA:0.7143 dlossQ:0.8219 exploreP:0.8232\n",
      "Episode:69 meanR:28.4429 R:24.0000 rate:0.0480 gloss:0.6622 dlossA:0.7253 dlossQ:0.8192 exploreP:0.8213\n",
      "Episode:70 meanR:28.2535 R:15.0000 rate:0.0300 gloss:0.6422 dlossA:0.7174 dlossQ:0.8186 exploreP:0.8201\n",
      "Episode:71 meanR:28.2639 R:29.0000 rate:0.0580 gloss:0.6712 dlossA:0.7319 dlossQ:0.8085 exploreP:0.8177\n",
      "Episode:72 meanR:28.6986 R:60.0000 rate:0.1200 gloss:0.6593 dlossA:0.7262 dlossQ:0.8152 exploreP:0.8129\n",
      "Episode:73 meanR:28.5946 R:21.0000 rate:0.0420 gloss:0.6712 dlossA:0.7264 dlossQ:0.8109 exploreP:0.8112\n",
      "Episode:74 meanR:28.3733 R:12.0000 rate:0.0240 gloss:0.6488 dlossA:0.7108 dlossQ:0.8230 exploreP:0.8102\n",
      "Episode:75 meanR:28.2895 R:22.0000 rate:0.0440 gloss:0.6713 dlossA:0.7222 dlossQ:0.8134 exploreP:0.8085\n",
      "Episode:76 meanR:28.4416 R:40.0000 rate:0.0800 gloss:0.6576 dlossA:0.7202 dlossQ:0.8141 exploreP:0.8053\n",
      "Episode:77 meanR:29.0513 R:76.0000 rate:0.1520 gloss:0.6611 dlossA:0.7259 dlossQ:0.8174 exploreP:0.7993\n",
      "Episode:78 meanR:29.1899 R:40.0000 rate:0.0800 gloss:0.6648 dlossA:0.7252 dlossQ:0.8128 exploreP:0.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:29.4250 R:48.0000 rate:0.0960 gloss:0.6592 dlossA:0.7268 dlossQ:0.8164 exploreP:0.7924\n",
      "Episode:80 meanR:29.6173 R:45.0000 rate:0.0900 gloss:0.6716 dlossA:0.7316 dlossQ:0.8144 exploreP:0.7888\n",
      "Episode:81 meanR:29.5244 R:22.0000 rate:0.0440 gloss:0.6413 dlossA:0.7137 dlossQ:0.8247 exploreP:0.7871\n",
      "Episode:82 meanR:29.9759 R:67.0000 rate:0.1340 gloss:0.6669 dlossA:0.7253 dlossQ:0.8249 exploreP:0.7819\n",
      "Episode:83 meanR:30.7500 R:95.0000 rate:0.1900 gloss:0.6614 dlossA:0.7232 dlossQ:0.8235 exploreP:0.7746\n",
      "Episode:84 meanR:30.8000 R:35.0000 rate:0.0700 gloss:0.6532 dlossA:0.7209 dlossQ:0.8220 exploreP:0.7720\n",
      "Episode:85 meanR:30.6628 R:19.0000 rate:0.0380 gloss:0.6665 dlossA:0.7239 dlossQ:0.8259 exploreP:0.7705\n",
      "Episode:86 meanR:30.7011 R:34.0000 rate:0.0680 gloss:0.6692 dlossA:0.7300 dlossQ:0.8194 exploreP:0.7679\n",
      "Episode:87 meanR:30.6250 R:24.0000 rate:0.0480 gloss:0.6767 dlossA:0.7331 dlossQ:0.8192 exploreP:0.7661\n",
      "Episode:88 meanR:30.5393 R:23.0000 rate:0.0460 gloss:0.6644 dlossA:0.7277 dlossQ:0.8264 exploreP:0.7644\n",
      "Episode:89 meanR:30.9444 R:67.0000 rate:0.1340 gloss:0.6597 dlossA:0.7198 dlossQ:0.8321 exploreP:0.7593\n",
      "Episode:90 meanR:30.8681 R:24.0000 rate:0.0480 gloss:0.6608 dlossA:0.7158 dlossQ:0.8366 exploreP:0.7576\n",
      "Episode:91 meanR:30.7935 R:24.0000 rate:0.0480 gloss:0.6618 dlossA:0.7243 dlossQ:0.8167 exploreP:0.7558\n",
      "Episode:92 meanR:30.5914 R:12.0000 rate:0.0240 gloss:0.6736 dlossA:0.7221 dlossQ:0.8277 exploreP:0.7549\n",
      "Episode:93 meanR:30.3936 R:12.0000 rate:0.0240 gloss:0.6763 dlossA:0.7331 dlossQ:0.8237 exploreP:0.7540\n",
      "Episode:94 meanR:30.4000 R:31.0000 rate:0.0620 gloss:0.6684 dlossA:0.7256 dlossQ:0.8278 exploreP:0.7517\n",
      "Episode:95 meanR:30.4583 R:36.0000 rate:0.0720 gloss:0.6608 dlossA:0.7193 dlossQ:0.8270 exploreP:0.7490\n",
      "Episode:96 meanR:30.4845 R:33.0000 rate:0.0660 gloss:0.6528 dlossA:0.7178 dlossQ:0.8245 exploreP:0.7466\n",
      "Episode:97 meanR:30.5918 R:41.0000 rate:0.0820 gloss:0.6679 dlossA:0.7265 dlossQ:0.8265 exploreP:0.7436\n",
      "Episode:98 meanR:30.7576 R:47.0000 rate:0.0940 gloss:0.6640 dlossA:0.7238 dlossQ:0.8271 exploreP:0.7401\n",
      "Episode:99 meanR:30.8300 R:38.0000 rate:0.0760 gloss:0.6624 dlossA:0.7249 dlossQ:0.8271 exploreP:0.7373\n",
      "Episode:100 meanR:30.8900 R:23.0000 rate:0.0460 gloss:0.6576 dlossA:0.7237 dlossQ:0.8267 exploreP:0.7357\n",
      "Episode:101 meanR:31.1100 R:46.0000 rate:0.0920 gloss:0.6622 dlossA:0.7207 dlossQ:0.8310 exploreP:0.7323\n",
      "Episode:102 meanR:31.4200 R:51.0000 rate:0.1020 gloss:0.6690 dlossA:0.7256 dlossQ:0.8271 exploreP:0.7287\n",
      "Episode:103 meanR:31.3800 R:17.0000 rate:0.0340 gloss:0.6639 dlossA:0.7252 dlossQ:0.8416 exploreP:0.7275\n",
      "Episode:104 meanR:31.6300 R:50.0000 rate:0.1000 gloss:0.6751 dlossA:0.7295 dlossQ:0.8300 exploreP:0.7239\n",
      "Episode:105 meanR:31.6600 R:15.0000 rate:0.0300 gloss:0.6570 dlossA:0.7215 dlossQ:0.8335 exploreP:0.7228\n",
      "Episode:106 meanR:31.8800 R:51.0000 rate:0.1020 gloss:0.6636 dlossA:0.7229 dlossQ:0.8300 exploreP:0.7192\n",
      "Episode:107 meanR:31.6200 R:13.0000 rate:0.0260 gloss:0.6547 dlossA:0.7186 dlossQ:0.8311 exploreP:0.7183\n",
      "Episode:108 meanR:31.7100 R:24.0000 rate:0.0480 gloss:0.6630 dlossA:0.7247 dlossQ:0.8273 exploreP:0.7166\n",
      "Episode:109 meanR:31.4400 R:18.0000 rate:0.0360 gloss:0.6733 dlossA:0.7266 dlossQ:0.8345 exploreP:0.7153\n",
      "Episode:110 meanR:31.5700 R:27.0000 rate:0.0540 gloss:0.6594 dlossA:0.7218 dlossQ:0.8348 exploreP:0.7134\n",
      "Episode:111 meanR:31.7100 R:27.0000 rate:0.0540 gloss:0.6647 dlossA:0.7209 dlossQ:0.8262 exploreP:0.7115\n",
      "Episode:112 meanR:31.5000 R:11.0000 rate:0.0220 gloss:0.6626 dlossA:0.7195 dlossQ:0.8331 exploreP:0.7107\n",
      "Episode:113 meanR:31.5200 R:22.0000 rate:0.0440 gloss:0.6701 dlossA:0.7297 dlossQ:0.8296 exploreP:0.7092\n",
      "Episode:114 meanR:31.4600 R:52.0000 rate:0.1040 gloss:0.6632 dlossA:0.7246 dlossQ:0.8299 exploreP:0.7056\n",
      "Episode:115 meanR:31.8500 R:54.0000 rate:0.1080 gloss:0.6685 dlossA:0.7232 dlossQ:0.8383 exploreP:0.7018\n",
      "Episode:116 meanR:31.5400 R:25.0000 rate:0.0500 gloss:0.6754 dlossA:0.7304 dlossQ:0.8281 exploreP:0.7001\n",
      "Episode:117 meanR:32.1300 R:83.0000 rate:0.1660 gloss:0.6818 dlossA:0.7342 dlossQ:0.8293 exploreP:0.6944\n",
      "Episode:118 meanR:31.9700 R:32.0000 rate:0.0640 gloss:0.6809 dlossA:0.7349 dlossQ:0.8306 exploreP:0.6922\n",
      "Episode:119 meanR:32.1400 R:39.0000 rate:0.0780 gloss:0.6814 dlossA:0.7268 dlossQ:0.8377 exploreP:0.6895\n",
      "Episode:120 meanR:32.4600 R:47.0000 rate:0.0940 gloss:0.6648 dlossA:0.7214 dlossQ:0.8394 exploreP:0.6863\n",
      "Episode:121 meanR:32.6600 R:31.0000 rate:0.0620 gloss:0.6636 dlossA:0.7141 dlossQ:0.8582 exploreP:0.6843\n",
      "Episode:122 meanR:32.5300 R:25.0000 rate:0.0500 gloss:0.6681 dlossA:0.7242 dlossQ:0.8365 exploreP:0.6826\n",
      "Episode:123 meanR:32.9000 R:54.0000 rate:0.1080 gloss:0.6729 dlossA:0.7240 dlossQ:0.8388 exploreP:0.6789\n",
      "Episode:124 meanR:32.9900 R:36.0000 rate:0.0720 gloss:0.6640 dlossA:0.7218 dlossQ:0.8410 exploreP:0.6765\n",
      "Episode:125 meanR:32.9600 R:33.0000 rate:0.0660 gloss:0.6718 dlossA:0.7284 dlossQ:0.8340 exploreP:0.6743\n",
      "Episode:126 meanR:33.2900 R:63.0000 rate:0.1260 gloss:0.6679 dlossA:0.7249 dlossQ:0.8383 exploreP:0.6702\n",
      "Episode:127 meanR:33.3600 R:24.0000 rate:0.0480 gloss:0.6599 dlossA:0.7235 dlossQ:0.8398 exploreP:0.6686\n",
      "Episode:128 meanR:33.3800 R:40.0000 rate:0.0800 gloss:0.6650 dlossA:0.7265 dlossQ:0.8340 exploreP:0.6660\n",
      "Episode:129 meanR:33.0500 R:23.0000 rate:0.0460 gloss:0.6889 dlossA:0.7343 dlossQ:0.8293 exploreP:0.6645\n",
      "Episode:130 meanR:33.2100 R:44.0000 rate:0.0880 gloss:0.6774 dlossA:0.7257 dlossQ:0.8411 exploreP:0.6616\n",
      "Episode:131 meanR:34.2100 R:129.0000 rate:0.2580 gloss:0.6714 dlossA:0.7229 dlossQ:0.8447 exploreP:0.6532\n",
      "Episode:132 meanR:34.6500 R:58.0000 rate:0.1160 gloss:0.6819 dlossA:0.7326 dlossQ:0.8346 exploreP:0.6495\n",
      "Episode:133 meanR:34.4100 R:30.0000 rate:0.0600 gloss:0.6720 dlossA:0.7247 dlossQ:0.8407 exploreP:0.6476\n",
      "Episode:134 meanR:34.3200 R:15.0000 rate:0.0300 gloss:0.6804 dlossA:0.7331 dlossQ:0.8240 exploreP:0.6466\n",
      "Episode:135 meanR:34.4400 R:32.0000 rate:0.0640 gloss:0.6764 dlossA:0.7294 dlossQ:0.8366 exploreP:0.6446\n",
      "Episode:136 meanR:34.5800 R:33.0000 rate:0.0660 gloss:0.6741 dlossA:0.7253 dlossQ:0.8455 exploreP:0.6425\n",
      "Episode:137 meanR:34.6000 R:50.0000 rate:0.1000 gloss:0.6754 dlossA:0.7208 dlossQ:0.8497 exploreP:0.6394\n",
      "Episode:138 meanR:35.3500 R:89.0000 rate:0.1780 gloss:0.6683 dlossA:0.7238 dlossQ:0.8426 exploreP:0.6338\n",
      "Episode:139 meanR:35.4200 R:21.0000 rate:0.0420 gloss:0.6709 dlossA:0.7252 dlossQ:0.8479 exploreP:0.6325\n",
      "Episode:140 meanR:35.7500 R:49.0000 rate:0.0980 gloss:0.6796 dlossA:0.7240 dlossQ:0.8497 exploreP:0.6294\n",
      "Episode:141 meanR:35.4700 R:49.0000 rate:0.0980 gloss:0.6750 dlossA:0.7224 dlossQ:0.8402 exploreP:0.6264\n",
      "Episode:142 meanR:35.8200 R:72.0000 rate:0.1440 gloss:0.6761 dlossA:0.7239 dlossQ:0.8523 exploreP:0.6220\n",
      "Episode:143 meanR:35.8300 R:31.0000 rate:0.0620 gloss:0.6756 dlossA:0.7216 dlossQ:0.8496 exploreP:0.6201\n",
      "Episode:144 meanR:36.5300 R:90.0000 rate:0.1800 gloss:0.6820 dlossA:0.7311 dlossQ:0.8415 exploreP:0.6146\n",
      "Episode:145 meanR:35.8100 R:16.0000 rate:0.0320 gloss:0.6714 dlossA:0.7256 dlossQ:0.8417 exploreP:0.6137\n",
      "Episode:146 meanR:36.0600 R:43.0000 rate:0.0860 gloss:0.6710 dlossA:0.7257 dlossQ:0.8494 exploreP:0.6111\n",
      "Episode:147 meanR:36.5000 R:54.0000 rate:0.1080 gloss:0.6757 dlossA:0.7281 dlossQ:0.8404 exploreP:0.6078\n",
      "Episode:148 meanR:36.5300 R:16.0000 rate:0.0320 gloss:0.6669 dlossA:0.7240 dlossQ:0.8486 exploreP:0.6069\n",
      "Episode:149 meanR:37.1500 R:79.0000 rate:0.1580 gloss:0.6688 dlossA:0.7186 dlossQ:0.8550 exploreP:0.6022\n",
      "Episode:150 meanR:37.0800 R:40.0000 rate:0.0800 gloss:0.6766 dlossA:0.7230 dlossQ:0.8485 exploreP:0.5998\n",
      "Episode:151 meanR:37.1900 R:29.0000 rate:0.0580 gloss:0.6723 dlossA:0.7269 dlossQ:0.8480 exploreP:0.5981\n",
      "Episode:152 meanR:37.0500 R:15.0000 rate:0.0300 gloss:0.6758 dlossA:0.7311 dlossQ:0.8423 exploreP:0.5972\n",
      "Episode:153 meanR:37.4400 R:64.0000 rate:0.1280 gloss:0.6610 dlossA:0.7126 dlossQ:0.8649 exploreP:0.5935\n",
      "Episode:154 meanR:38.1200 R:91.0000 rate:0.1820 gloss:0.6715 dlossA:0.7241 dlossQ:0.8462 exploreP:0.5882\n",
      "Episode:155 meanR:38.4600 R:50.0000 rate:0.1000 gloss:0.6776 dlossA:0.7254 dlossQ:0.8474 exploreP:0.5853\n",
      "Episode:156 meanR:38.6300 R:35.0000 rate:0.0700 gloss:0.6745 dlossA:0.7274 dlossQ:0.8486 exploreP:0.5833\n",
      "Episode:157 meanR:38.6200 R:49.0000 rate:0.0980 gloss:0.6775 dlossA:0.7262 dlossQ:0.8476 exploreP:0.5805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:38.9400 R:57.0000 rate:0.1140 gloss:0.6755 dlossA:0.7258 dlossQ:0.8418 exploreP:0.5773\n",
      "Episode:159 meanR:39.1900 R:41.0000 rate:0.0820 gloss:0.6750 dlossA:0.7248 dlossQ:0.8520 exploreP:0.5749\n",
      "Episode:160 meanR:39.5000 R:46.0000 rate:0.0920 gloss:0.6675 dlossA:0.7214 dlossQ:0.8511 exploreP:0.5723\n",
      "Episode:161 meanR:39.6700 R:28.0000 rate:0.0560 gloss:0.6776 dlossA:0.7221 dlossQ:0.8570 exploreP:0.5708\n",
      "Episode:162 meanR:39.8200 R:48.0000 rate:0.0960 gloss:0.6726 dlossA:0.7256 dlossQ:0.8435 exploreP:0.5681\n",
      "Episode:163 meanR:40.7300 R:150.0000 rate:0.3000 gloss:0.6810 dlossA:0.7269 dlossQ:0.8583 exploreP:0.5598\n",
      "Episode:164 meanR:41.2300 R:82.0000 rate:0.1640 gloss:0.6703 dlossA:0.7226 dlossQ:0.8604 exploreP:0.5553\n",
      "Episode:165 meanR:42.0200 R:117.0000 rate:0.2340 gloss:0.6717 dlossA:0.7239 dlossQ:0.8530 exploreP:0.5489\n",
      "Episode:166 meanR:42.3000 R:39.0000 rate:0.0780 gloss:0.6782 dlossA:0.7215 dlossQ:0.8587 exploreP:0.5468\n",
      "Episode:167 meanR:42.9300 R:81.0000 rate:0.1620 gloss:0.6869 dlossA:0.7293 dlossQ:0.8495 exploreP:0.5425\n",
      "Episode:168 meanR:43.4700 R:113.0000 rate:0.2260 gloss:0.6811 dlossA:0.7274 dlossQ:0.8484 exploreP:0.5365\n",
      "Episode:169 meanR:44.1800 R:95.0000 rate:0.1900 gloss:0.6819 dlossA:0.7282 dlossQ:0.8517 exploreP:0.5315\n",
      "Episode:170 meanR:44.7200 R:69.0000 rate:0.1380 gloss:0.6821 dlossA:0.7265 dlossQ:0.8559 exploreP:0.5280\n",
      "Episode:171 meanR:44.6600 R:23.0000 rate:0.0460 gloss:0.6800 dlossA:0.7169 dlossQ:0.8694 exploreP:0.5268\n",
      "Episode:172 meanR:44.7900 R:73.0000 rate:0.1460 gloss:0.6724 dlossA:0.7227 dlossQ:0.8559 exploreP:0.5230\n",
      "Episode:173 meanR:45.7900 R:121.0000 rate:0.2420 gloss:0.6845 dlossA:0.7296 dlossQ:0.8541 exploreP:0.5168\n",
      "Episode:174 meanR:46.4200 R:75.0000 rate:0.1500 gloss:0.6791 dlossA:0.7283 dlossQ:0.8533 exploreP:0.5131\n",
      "Episode:175 meanR:47.6300 R:143.0000 rate:0.2860 gloss:0.6786 dlossA:0.7261 dlossQ:0.8570 exploreP:0.5059\n",
      "Episode:176 meanR:47.7300 R:50.0000 rate:0.1000 gloss:0.6837 dlossA:0.7264 dlossQ:0.8592 exploreP:0.5034\n",
      "Episode:177 meanR:47.4700 R:50.0000 rate:0.1000 gloss:0.6745 dlossA:0.7185 dlossQ:0.8653 exploreP:0.5010\n",
      "Episode:178 meanR:47.5700 R:50.0000 rate:0.1000 gloss:0.6715 dlossA:0.7210 dlossQ:0.8630 exploreP:0.4985\n",
      "Episode:179 meanR:48.2000 R:111.0000 rate:0.2220 gloss:0.6772 dlossA:0.7229 dlossQ:0.8626 exploreP:0.4931\n",
      "Episode:180 meanR:48.1800 R:43.0000 rate:0.0860 gloss:0.6847 dlossA:0.7275 dlossQ:0.8568 exploreP:0.4911\n",
      "Episode:181 meanR:48.7500 R:79.0000 rate:0.1580 gloss:0.6832 dlossA:0.7266 dlossQ:0.8611 exploreP:0.4873\n",
      "Episode:182 meanR:48.5200 R:44.0000 rate:0.0880 gloss:0.6833 dlossA:0.7296 dlossQ:0.8611 exploreP:0.4852\n",
      "Episode:183 meanR:47.9500 R:38.0000 rate:0.0760 gloss:0.6733 dlossA:0.7214 dlossQ:0.8612 exploreP:0.4834\n",
      "Episode:184 meanR:47.8800 R:28.0000 rate:0.0560 gloss:0.6883 dlossA:0.7282 dlossQ:0.8514 exploreP:0.4821\n",
      "Episode:185 meanR:47.8900 R:20.0000 rate:0.0400 gloss:0.6760 dlossA:0.7206 dlossQ:0.8645 exploreP:0.4811\n",
      "Episode:186 meanR:48.0800 R:53.0000 rate:0.1060 gloss:0.6815 dlossA:0.7257 dlossQ:0.8612 exploreP:0.4786\n",
      "Episode:187 meanR:48.0500 R:21.0000 rate:0.0420 gloss:0.6922 dlossA:0.7285 dlossQ:0.8529 exploreP:0.4776\n",
      "Episode:188 meanR:48.7200 R:90.0000 rate:0.1800 gloss:0.6817 dlossA:0.7259 dlossQ:0.8610 exploreP:0.4735\n",
      "Episode:189 meanR:48.7300 R:68.0000 rate:0.1360 gloss:0.6742 dlossA:0.7152 dlossQ:0.8714 exploreP:0.4703\n",
      "Episode:190 meanR:49.1000 R:61.0000 rate:0.1220 gloss:0.6821 dlossA:0.7275 dlossQ:0.8614 exploreP:0.4675\n",
      "Episode:191 meanR:49.6400 R:78.0000 rate:0.1560 gloss:0.6796 dlossA:0.7243 dlossQ:0.8605 exploreP:0.4640\n",
      "Episode:192 meanR:51.6000 R:208.0000 rate:0.4160 gloss:0.6787 dlossA:0.7253 dlossQ:0.8614 exploreP:0.4546\n",
      "Episode:193 meanR:51.8600 R:38.0000 rate:0.0760 gloss:0.6808 dlossA:0.7269 dlossQ:0.8569 exploreP:0.4529\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextQs_logits.shape, next_states.shape, actions.shape, rates.shape, batch.shape, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "batch.shape\n",
    "rates = np.array([each[5] for each in batch])\n",
    "rates.shape\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
