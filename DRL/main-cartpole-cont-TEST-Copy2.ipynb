{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66, 6), (66, 4), (66,), (66, 4), (66,), (66,), (66,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 R:17.0000 rate:0.0340 gloss:0.6487 dlossA:0.7176 dlossQ:0.7791 exploreP:0.9983\n",
      "Episode:1 meanR:20.5000 R:24.0000 rate:0.0480 gloss:0.6516 dlossA:0.7274 dlossQ:0.7762 exploreP:0.9959\n",
      "Episode:2 meanR:20.3333 R:20.0000 rate:0.0400 gloss:0.6557 dlossA:0.7273 dlossQ:0.7804 exploreP:0.9940\n",
      "Episode:3 meanR:20.5000 R:21.0000 rate:0.0420 gloss:0.6638 dlossA:0.7314 dlossQ:0.7767 exploreP:0.9919\n",
      "Episode:4 meanR:21.4000 R:25.0000 rate:0.0500 gloss:0.6516 dlossA:0.7231 dlossQ:0.7868 exploreP:0.9895\n",
      "Episode:5 meanR:19.8333 R:12.0000 rate:0.0240 gloss:0.6518 dlossA:0.7304 dlossQ:0.7834 exploreP:0.9883\n",
      "Episode:6 meanR:21.1429 R:29.0000 rate:0.0580 gloss:0.6381 dlossA:0.7186 dlossQ:0.8007 exploreP:0.9855\n",
      "Episode:7 meanR:23.3750 R:39.0000 rate:0.0780 gloss:0.6706 dlossA:0.7294 dlossQ:0.7866 exploreP:0.9817\n",
      "Episode:8 meanR:22.4444 R:15.0000 rate:0.0300 gloss:0.6852 dlossA:0.7277 dlossQ:0.7918 exploreP:0.9802\n",
      "Episode:9 meanR:24.7000 R:45.0000 rate:0.0900 gloss:0.6590 dlossA:0.7321 dlossQ:0.7848 exploreP:0.9758\n",
      "Episode:10 meanR:23.7273 R:14.0000 rate:0.0280 gloss:0.6549 dlossA:0.7347 dlossQ:0.7824 exploreP:0.9745\n",
      "Episode:11 meanR:22.8333 R:13.0000 rate:0.0260 gloss:0.6375 dlossA:0.7204 dlossQ:0.7821 exploreP:0.9732\n",
      "Episode:12 meanR:23.5385 R:32.0000 rate:0.0640 gloss:0.6641 dlossA:0.7294 dlossQ:0.8009 exploreP:0.9702\n",
      "Episode:13 meanR:23.2857 R:20.0000 rate:0.0400 gloss:0.6578 dlossA:0.7289 dlossQ:0.7927 exploreP:0.9682\n",
      "Episode:14 meanR:25.6000 R:58.0000 rate:0.1160 gloss:0.6563 dlossA:0.7320 dlossQ:0.7857 exploreP:0.9627\n",
      "Episode:15 meanR:24.9375 R:15.0000 rate:0.0300 gloss:0.6614 dlossA:0.7310 dlossQ:0.7857 exploreP:0.9613\n",
      "Episode:16 meanR:26.7647 R:56.0000 rate:0.1120 gloss:0.6490 dlossA:0.7257 dlossQ:0.7933 exploreP:0.9560\n",
      "Episode:17 meanR:26.6111 R:24.0000 rate:0.0480 gloss:0.6573 dlossA:0.7308 dlossQ:0.7832 exploreP:0.9537\n",
      "Episode:18 meanR:27.7368 R:48.0000 rate:0.0960 gloss:0.6570 dlossA:0.7310 dlossQ:0.7872 exploreP:0.9492\n",
      "Episode:19 meanR:27.4500 R:22.0000 rate:0.0440 gloss:0.6508 dlossA:0.7284 dlossQ:0.7927 exploreP:0.9471\n",
      "Episode:20 meanR:26.8571 R:15.0000 rate:0.0300 gloss:0.6664 dlossA:0.7372 dlossQ:0.7900 exploreP:0.9457\n",
      "Episode:21 meanR:26.1364 R:11.0000 rate:0.0220 gloss:0.6541 dlossA:0.7345 dlossQ:0.8098 exploreP:0.9447\n",
      "Episode:22 meanR:26.6522 R:38.0000 rate:0.0760 gloss:0.6616 dlossA:0.7343 dlossQ:0.7941 exploreP:0.9411\n",
      "Episode:23 meanR:26.2500 R:17.0000 rate:0.0340 gloss:0.6432 dlossA:0.7153 dlossQ:0.7946 exploreP:0.9396\n",
      "Episode:24 meanR:26.2800 R:27.0000 rate:0.0540 gloss:0.6614 dlossA:0.7273 dlossQ:0.7959 exploreP:0.9370\n",
      "Episode:25 meanR:26.6538 R:36.0000 rate:0.0720 gloss:0.6628 dlossA:0.7270 dlossQ:0.7948 exploreP:0.9337\n",
      "Episode:26 meanR:26.7778 R:30.0000 rate:0.0600 gloss:0.6660 dlossA:0.7376 dlossQ:0.7911 exploreP:0.9309\n",
      "Episode:27 meanR:26.4286 R:17.0000 rate:0.0340 gloss:0.6545 dlossA:0.7320 dlossQ:0.7881 exploreP:0.9294\n",
      "Episode:28 meanR:26.8276 R:38.0000 rate:0.0760 gloss:0.6576 dlossA:0.7267 dlossQ:0.7956 exploreP:0.9259\n",
      "Episode:29 meanR:27.8000 R:56.0000 rate:0.1120 gloss:0.6619 dlossA:0.7281 dlossQ:0.7981 exploreP:0.9208\n",
      "Episode:30 meanR:27.8065 R:28.0000 rate:0.0560 gloss:0.6659 dlossA:0.7311 dlossQ:0.7952 exploreP:0.9182\n",
      "Episode:31 meanR:27.8438 R:29.0000 rate:0.0580 gloss:0.6558 dlossA:0.7282 dlossQ:0.7942 exploreP:0.9156\n",
      "Episode:32 meanR:27.4242 R:14.0000 rate:0.0280 gloss:0.6732 dlossA:0.7377 dlossQ:0.7952 exploreP:0.9143\n",
      "Episode:33 meanR:28.2059 R:54.0000 rate:0.1080 gloss:0.6534 dlossA:0.7249 dlossQ:0.7979 exploreP:0.9095\n",
      "Episode:34 meanR:28.0857 R:24.0000 rate:0.0480 gloss:0.6562 dlossA:0.7259 dlossQ:0.7976 exploreP:0.9073\n",
      "Episode:35 meanR:27.8611 R:20.0000 rate:0.0400 gloss:0.6495 dlossA:0.7161 dlossQ:0.8079 exploreP:0.9055\n",
      "Episode:36 meanR:27.6216 R:19.0000 rate:0.0380 gloss:0.6618 dlossA:0.7311 dlossQ:0.8045 exploreP:0.9038\n",
      "Episode:37 meanR:28.1579 R:48.0000 rate:0.0960 gloss:0.6571 dlossA:0.7210 dlossQ:0.8024 exploreP:0.8995\n",
      "Episode:38 meanR:27.7949 R:14.0000 rate:0.0280 gloss:0.6810 dlossA:0.7379 dlossQ:0.7928 exploreP:0.8983\n",
      "Episode:39 meanR:27.4500 R:14.0000 rate:0.0280 gloss:0.6417 dlossA:0.7080 dlossQ:0.8158 exploreP:0.8971\n",
      "Episode:40 meanR:27.1707 R:16.0000 rate:0.0320 gloss:0.6543 dlossA:0.7277 dlossQ:0.7973 exploreP:0.8956\n",
      "Episode:41 meanR:28.3571 R:77.0000 rate:0.1540 gloss:0.6441 dlossA:0.7203 dlossQ:0.8049 exploreP:0.8888\n",
      "Episode:42 meanR:28.5581 R:37.0000 rate:0.0740 gloss:0.6579 dlossA:0.7215 dlossQ:0.8098 exploreP:0.8856\n",
      "Episode:43 meanR:28.5909 R:30.0000 rate:0.0600 gloss:0.6466 dlossA:0.7202 dlossQ:0.8055 exploreP:0.8830\n",
      "Episode:44 meanR:28.4000 R:20.0000 rate:0.0400 gloss:0.6586 dlossA:0.7268 dlossQ:0.8005 exploreP:0.8812\n",
      "Episode:45 meanR:29.6957 R:88.0000 rate:0.1760 gloss:0.6571 dlossA:0.7271 dlossQ:0.8061 exploreP:0.8736\n",
      "Episode:46 meanR:29.4468 R:18.0000 rate:0.0360 gloss:0.6537 dlossA:0.7228 dlossQ:0.8100 exploreP:0.8720\n",
      "Episode:47 meanR:29.0417 R:10.0000 rate:0.0200 gloss:0.6385 dlossA:0.7180 dlossQ:0.8002 exploreP:0.8712\n",
      "Episode:48 meanR:28.7143 R:13.0000 rate:0.0260 gloss:0.6787 dlossA:0.7346 dlossQ:0.8053 exploreP:0.8701\n",
      "Episode:49 meanR:28.4800 R:17.0000 rate:0.0340 gloss:0.6590 dlossA:0.7282 dlossQ:0.8070 exploreP:0.8686\n",
      "Episode:50 meanR:28.8431 R:47.0000 rate:0.0940 gloss:0.6498 dlossA:0.7207 dlossQ:0.8066 exploreP:0.8646\n",
      "Episode:51 meanR:28.6346 R:18.0000 rate:0.0360 gloss:0.6511 dlossA:0.7253 dlossQ:0.8089 exploreP:0.8630\n",
      "Episode:52 meanR:28.6415 R:29.0000 rate:0.0580 gloss:0.6656 dlossA:0.7288 dlossQ:0.8098 exploreP:0.8606\n",
      "Episode:53 meanR:28.5741 R:25.0000 rate:0.0500 gloss:0.6711 dlossA:0.7303 dlossQ:0.8067 exploreP:0.8584\n",
      "Episode:54 meanR:28.4727 R:23.0000 rate:0.0460 gloss:0.6580 dlossA:0.7236 dlossQ:0.8156 exploreP:0.8565\n",
      "Episode:55 meanR:28.2500 R:16.0000 rate:0.0320 gloss:0.6581 dlossA:0.7270 dlossQ:0.8161 exploreP:0.8551\n",
      "Episode:56 meanR:28.0702 R:18.0000 rate:0.0360 gloss:0.6573 dlossA:0.7271 dlossQ:0.8149 exploreP:0.8536\n",
      "Episode:57 meanR:28.4483 R:50.0000 rate:0.1000 gloss:0.6544 dlossA:0.7268 dlossQ:0.8038 exploreP:0.8494\n",
      "Episode:58 meanR:28.3898 R:25.0000 rate:0.0500 gloss:0.6423 dlossA:0.7182 dlossQ:0.8066 exploreP:0.8473\n",
      "Episode:59 meanR:28.1833 R:16.0000 rate:0.0320 gloss:0.6605 dlossA:0.7243 dlossQ:0.8034 exploreP:0.8460\n",
      "Episode:60 meanR:27.9672 R:15.0000 rate:0.0300 gloss:0.6448 dlossA:0.7117 dlossQ:0.8260 exploreP:0.8447\n",
      "Episode:61 meanR:27.6935 R:11.0000 rate:0.0220 gloss:0.6814 dlossA:0.7360 dlossQ:0.8101 exploreP:0.8438\n",
      "Episode:62 meanR:27.7778 R:33.0000 rate:0.0660 gloss:0.6581 dlossA:0.7243 dlossQ:0.8122 exploreP:0.8411\n",
      "Episode:63 meanR:28.2656 R:59.0000 rate:0.1180 gloss:0.6572 dlossA:0.7249 dlossQ:0.8142 exploreP:0.8362\n",
      "Episode:64 meanR:28.3231 R:32.0000 rate:0.0640 gloss:0.6697 dlossA:0.7308 dlossQ:0.8053 exploreP:0.8335\n",
      "Episode:65 meanR:28.4697 R:38.0000 rate:0.0760 gloss:0.6584 dlossA:0.7269 dlossQ:0.8146 exploreP:0.8304\n",
      "Episode:66 meanR:28.2090 R:11.0000 rate:0.0220 gloss:0.6640 dlossA:0.7267 dlossQ:0.8109 exploreP:0.8295\n",
      "Episode:67 meanR:28.0588 R:18.0000 rate:0.0360 gloss:0.6598 dlossA:0.7272 dlossQ:0.8121 exploreP:0.8280\n",
      "Episode:68 meanR:28.5072 R:59.0000 rate:0.1180 gloss:0.6417 dlossA:0.7143 dlossQ:0.8219 exploreP:0.8232\n",
      "Episode:69 meanR:28.4429 R:24.0000 rate:0.0480 gloss:0.6622 dlossA:0.7253 dlossQ:0.8192 exploreP:0.8213\n",
      "Episode:70 meanR:28.2535 R:15.0000 rate:0.0300 gloss:0.6422 dlossA:0.7174 dlossQ:0.8186 exploreP:0.8201\n",
      "Episode:71 meanR:28.2639 R:29.0000 rate:0.0580 gloss:0.6712 dlossA:0.7319 dlossQ:0.8085 exploreP:0.8177\n",
      "Episode:72 meanR:28.6986 R:60.0000 rate:0.1200 gloss:0.6593 dlossA:0.7262 dlossQ:0.8152 exploreP:0.8129\n",
      "Episode:73 meanR:28.5946 R:21.0000 rate:0.0420 gloss:0.6712 dlossA:0.7264 dlossQ:0.8109 exploreP:0.8112\n",
      "Episode:74 meanR:28.3733 R:12.0000 rate:0.0240 gloss:0.6488 dlossA:0.7108 dlossQ:0.8230 exploreP:0.8102\n",
      "Episode:75 meanR:28.2895 R:22.0000 rate:0.0440 gloss:0.6713 dlossA:0.7222 dlossQ:0.8134 exploreP:0.8085\n",
      "Episode:76 meanR:28.4416 R:40.0000 rate:0.0800 gloss:0.6576 dlossA:0.7202 dlossQ:0.8141 exploreP:0.8053\n",
      "Episode:77 meanR:29.0513 R:76.0000 rate:0.1520 gloss:0.6611 dlossA:0.7259 dlossQ:0.8174 exploreP:0.7993\n",
      "Episode:78 meanR:29.1899 R:40.0000 rate:0.0800 gloss:0.6648 dlossA:0.7252 dlossQ:0.8128 exploreP:0.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:29.4250 R:48.0000 rate:0.0960 gloss:0.6592 dlossA:0.7268 dlossQ:0.8164 exploreP:0.7924\n",
      "Episode:80 meanR:29.6173 R:45.0000 rate:0.0900 gloss:0.6716 dlossA:0.7316 dlossQ:0.8144 exploreP:0.7888\n",
      "Episode:81 meanR:29.5244 R:22.0000 rate:0.0440 gloss:0.6413 dlossA:0.7137 dlossQ:0.8247 exploreP:0.7871\n",
      "Episode:82 meanR:29.9759 R:67.0000 rate:0.1340 gloss:0.6669 dlossA:0.7253 dlossQ:0.8249 exploreP:0.7819\n",
      "Episode:83 meanR:30.7500 R:95.0000 rate:0.1900 gloss:0.6614 dlossA:0.7232 dlossQ:0.8235 exploreP:0.7746\n",
      "Episode:84 meanR:30.8000 R:35.0000 rate:0.0700 gloss:0.6532 dlossA:0.7209 dlossQ:0.8220 exploreP:0.7720\n",
      "Episode:85 meanR:30.6628 R:19.0000 rate:0.0380 gloss:0.6665 dlossA:0.7239 dlossQ:0.8259 exploreP:0.7705\n",
      "Episode:86 meanR:30.7011 R:34.0000 rate:0.0680 gloss:0.6692 dlossA:0.7300 dlossQ:0.8194 exploreP:0.7679\n",
      "Episode:87 meanR:30.6250 R:24.0000 rate:0.0480 gloss:0.6767 dlossA:0.7331 dlossQ:0.8192 exploreP:0.7661\n",
      "Episode:88 meanR:30.5393 R:23.0000 rate:0.0460 gloss:0.6644 dlossA:0.7277 dlossQ:0.8264 exploreP:0.7644\n",
      "Episode:89 meanR:30.9444 R:67.0000 rate:0.1340 gloss:0.6597 dlossA:0.7198 dlossQ:0.8321 exploreP:0.7593\n",
      "Episode:90 meanR:30.8681 R:24.0000 rate:0.0480 gloss:0.6608 dlossA:0.7158 dlossQ:0.8366 exploreP:0.7576\n",
      "Episode:91 meanR:30.7935 R:24.0000 rate:0.0480 gloss:0.6618 dlossA:0.7243 dlossQ:0.8167 exploreP:0.7558\n",
      "Episode:92 meanR:30.5914 R:12.0000 rate:0.0240 gloss:0.6736 dlossA:0.7221 dlossQ:0.8277 exploreP:0.7549\n",
      "Episode:93 meanR:30.3936 R:12.0000 rate:0.0240 gloss:0.6763 dlossA:0.7331 dlossQ:0.8237 exploreP:0.7540\n",
      "Episode:94 meanR:30.4000 R:31.0000 rate:0.0620 gloss:0.6684 dlossA:0.7256 dlossQ:0.8278 exploreP:0.7517\n",
      "Episode:95 meanR:30.4583 R:36.0000 rate:0.0720 gloss:0.6608 dlossA:0.7193 dlossQ:0.8270 exploreP:0.7490\n",
      "Episode:96 meanR:30.4845 R:33.0000 rate:0.0660 gloss:0.6528 dlossA:0.7178 dlossQ:0.8245 exploreP:0.7466\n",
      "Episode:97 meanR:30.5918 R:41.0000 rate:0.0820 gloss:0.6679 dlossA:0.7265 dlossQ:0.8265 exploreP:0.7436\n",
      "Episode:98 meanR:30.7576 R:47.0000 rate:0.0940 gloss:0.6640 dlossA:0.7238 dlossQ:0.8271 exploreP:0.7401\n",
      "Episode:99 meanR:30.8300 R:38.0000 rate:0.0760 gloss:0.6624 dlossA:0.7249 dlossQ:0.8271 exploreP:0.7373\n",
      "Episode:100 meanR:30.8900 R:23.0000 rate:0.0460 gloss:0.6576 dlossA:0.7237 dlossQ:0.8267 exploreP:0.7357\n",
      "Episode:101 meanR:31.1100 R:46.0000 rate:0.0920 gloss:0.6622 dlossA:0.7207 dlossQ:0.8310 exploreP:0.7323\n",
      "Episode:102 meanR:31.4200 R:51.0000 rate:0.1020 gloss:0.6690 dlossA:0.7256 dlossQ:0.8271 exploreP:0.7287\n",
      "Episode:103 meanR:31.3800 R:17.0000 rate:0.0340 gloss:0.6639 dlossA:0.7252 dlossQ:0.8416 exploreP:0.7275\n",
      "Episode:104 meanR:31.6300 R:50.0000 rate:0.1000 gloss:0.6751 dlossA:0.7295 dlossQ:0.8300 exploreP:0.7239\n",
      "Episode:105 meanR:31.6600 R:15.0000 rate:0.0300 gloss:0.6570 dlossA:0.7215 dlossQ:0.8335 exploreP:0.7228\n",
      "Episode:106 meanR:31.8800 R:51.0000 rate:0.1020 gloss:0.6636 dlossA:0.7229 dlossQ:0.8300 exploreP:0.7192\n",
      "Episode:107 meanR:31.6200 R:13.0000 rate:0.0260 gloss:0.6547 dlossA:0.7186 dlossQ:0.8311 exploreP:0.7183\n",
      "Episode:108 meanR:31.7100 R:24.0000 rate:0.0480 gloss:0.6630 dlossA:0.7247 dlossQ:0.8273 exploreP:0.7166\n",
      "Episode:109 meanR:31.4400 R:18.0000 rate:0.0360 gloss:0.6733 dlossA:0.7266 dlossQ:0.8345 exploreP:0.7153\n",
      "Episode:110 meanR:31.5700 R:27.0000 rate:0.0540 gloss:0.6594 dlossA:0.7218 dlossQ:0.8348 exploreP:0.7134\n",
      "Episode:111 meanR:31.7100 R:27.0000 rate:0.0540 gloss:0.6647 dlossA:0.7209 dlossQ:0.8262 exploreP:0.7115\n",
      "Episode:112 meanR:31.5000 R:11.0000 rate:0.0220 gloss:0.6626 dlossA:0.7195 dlossQ:0.8331 exploreP:0.7107\n",
      "Episode:113 meanR:31.5200 R:22.0000 rate:0.0440 gloss:0.6701 dlossA:0.7297 dlossQ:0.8296 exploreP:0.7092\n",
      "Episode:114 meanR:31.4600 R:52.0000 rate:0.1040 gloss:0.6632 dlossA:0.7246 dlossQ:0.8299 exploreP:0.7056\n",
      "Episode:115 meanR:31.8500 R:54.0000 rate:0.1080 gloss:0.6685 dlossA:0.7232 dlossQ:0.8383 exploreP:0.7018\n",
      "Episode:116 meanR:31.5400 R:25.0000 rate:0.0500 gloss:0.6754 dlossA:0.7304 dlossQ:0.8281 exploreP:0.7001\n",
      "Episode:117 meanR:32.1300 R:83.0000 rate:0.1660 gloss:0.6818 dlossA:0.7342 dlossQ:0.8293 exploreP:0.6944\n",
      "Episode:118 meanR:31.9700 R:32.0000 rate:0.0640 gloss:0.6809 dlossA:0.7349 dlossQ:0.8306 exploreP:0.6922\n",
      "Episode:119 meanR:32.1400 R:39.0000 rate:0.0780 gloss:0.6814 dlossA:0.7268 dlossQ:0.8377 exploreP:0.6895\n",
      "Episode:120 meanR:32.4600 R:47.0000 rate:0.0940 gloss:0.6648 dlossA:0.7214 dlossQ:0.8394 exploreP:0.6863\n",
      "Episode:121 meanR:32.6600 R:31.0000 rate:0.0620 gloss:0.6636 dlossA:0.7141 dlossQ:0.8582 exploreP:0.6843\n",
      "Episode:122 meanR:32.5300 R:25.0000 rate:0.0500 gloss:0.6681 dlossA:0.7242 dlossQ:0.8365 exploreP:0.6826\n",
      "Episode:123 meanR:32.9000 R:54.0000 rate:0.1080 gloss:0.6729 dlossA:0.7240 dlossQ:0.8388 exploreP:0.6789\n",
      "Episode:124 meanR:32.9900 R:36.0000 rate:0.0720 gloss:0.6640 dlossA:0.7218 dlossQ:0.8410 exploreP:0.6765\n",
      "Episode:125 meanR:32.9600 R:33.0000 rate:0.0660 gloss:0.6718 dlossA:0.7284 dlossQ:0.8340 exploreP:0.6743\n",
      "Episode:126 meanR:33.2900 R:63.0000 rate:0.1260 gloss:0.6679 dlossA:0.7249 dlossQ:0.8383 exploreP:0.6702\n",
      "Episode:127 meanR:33.3600 R:24.0000 rate:0.0480 gloss:0.6599 dlossA:0.7235 dlossQ:0.8398 exploreP:0.6686\n",
      "Episode:128 meanR:33.3800 R:40.0000 rate:0.0800 gloss:0.6650 dlossA:0.7265 dlossQ:0.8340 exploreP:0.6660\n",
      "Episode:129 meanR:33.0500 R:23.0000 rate:0.0460 gloss:0.6889 dlossA:0.7343 dlossQ:0.8293 exploreP:0.6645\n",
      "Episode:130 meanR:33.2100 R:44.0000 rate:0.0880 gloss:0.6774 dlossA:0.7257 dlossQ:0.8411 exploreP:0.6616\n",
      "Episode:131 meanR:34.2100 R:129.0000 rate:0.2580 gloss:0.6714 dlossA:0.7229 dlossQ:0.8447 exploreP:0.6532\n",
      "Episode:132 meanR:34.6500 R:58.0000 rate:0.1160 gloss:0.6819 dlossA:0.7326 dlossQ:0.8346 exploreP:0.6495\n",
      "Episode:133 meanR:34.4100 R:30.0000 rate:0.0600 gloss:0.6720 dlossA:0.7247 dlossQ:0.8407 exploreP:0.6476\n",
      "Episode:134 meanR:34.3200 R:15.0000 rate:0.0300 gloss:0.6804 dlossA:0.7331 dlossQ:0.8240 exploreP:0.6466\n",
      "Episode:135 meanR:34.4400 R:32.0000 rate:0.0640 gloss:0.6764 dlossA:0.7294 dlossQ:0.8366 exploreP:0.6446\n",
      "Episode:136 meanR:34.5800 R:33.0000 rate:0.0660 gloss:0.6741 dlossA:0.7253 dlossQ:0.8455 exploreP:0.6425\n",
      "Episode:137 meanR:34.6000 R:50.0000 rate:0.1000 gloss:0.6754 dlossA:0.7208 dlossQ:0.8497 exploreP:0.6394\n",
      "Episode:138 meanR:35.3500 R:89.0000 rate:0.1780 gloss:0.6683 dlossA:0.7238 dlossQ:0.8426 exploreP:0.6338\n",
      "Episode:139 meanR:35.4200 R:21.0000 rate:0.0420 gloss:0.6709 dlossA:0.7252 dlossQ:0.8479 exploreP:0.6325\n",
      "Episode:140 meanR:35.7500 R:49.0000 rate:0.0980 gloss:0.6796 dlossA:0.7240 dlossQ:0.8497 exploreP:0.6294\n",
      "Episode:141 meanR:35.4700 R:49.0000 rate:0.0980 gloss:0.6750 dlossA:0.7224 dlossQ:0.8402 exploreP:0.6264\n",
      "Episode:142 meanR:35.8200 R:72.0000 rate:0.1440 gloss:0.6761 dlossA:0.7239 dlossQ:0.8523 exploreP:0.6220\n",
      "Episode:143 meanR:35.8300 R:31.0000 rate:0.0620 gloss:0.6756 dlossA:0.7216 dlossQ:0.8496 exploreP:0.6201\n",
      "Episode:144 meanR:36.5300 R:90.0000 rate:0.1800 gloss:0.6820 dlossA:0.7311 dlossQ:0.8415 exploreP:0.6146\n",
      "Episode:145 meanR:35.8100 R:16.0000 rate:0.0320 gloss:0.6714 dlossA:0.7256 dlossQ:0.8417 exploreP:0.6137\n",
      "Episode:146 meanR:36.0600 R:43.0000 rate:0.0860 gloss:0.6710 dlossA:0.7257 dlossQ:0.8494 exploreP:0.6111\n",
      "Episode:147 meanR:36.5000 R:54.0000 rate:0.1080 gloss:0.6757 dlossA:0.7281 dlossQ:0.8404 exploreP:0.6078\n",
      "Episode:148 meanR:36.5300 R:16.0000 rate:0.0320 gloss:0.6669 dlossA:0.7240 dlossQ:0.8486 exploreP:0.6069\n",
      "Episode:149 meanR:37.1500 R:79.0000 rate:0.1580 gloss:0.6688 dlossA:0.7186 dlossQ:0.8550 exploreP:0.6022\n",
      "Episode:150 meanR:37.0800 R:40.0000 rate:0.0800 gloss:0.6766 dlossA:0.7230 dlossQ:0.8485 exploreP:0.5998\n",
      "Episode:151 meanR:37.1900 R:29.0000 rate:0.0580 gloss:0.6723 dlossA:0.7269 dlossQ:0.8480 exploreP:0.5981\n",
      "Episode:152 meanR:37.0500 R:15.0000 rate:0.0300 gloss:0.6758 dlossA:0.7311 dlossQ:0.8423 exploreP:0.5972\n",
      "Episode:153 meanR:37.4400 R:64.0000 rate:0.1280 gloss:0.6610 dlossA:0.7126 dlossQ:0.8649 exploreP:0.5935\n",
      "Episode:154 meanR:38.1200 R:91.0000 rate:0.1820 gloss:0.6715 dlossA:0.7241 dlossQ:0.8462 exploreP:0.5882\n",
      "Episode:155 meanR:38.4600 R:50.0000 rate:0.1000 gloss:0.6776 dlossA:0.7254 dlossQ:0.8474 exploreP:0.5853\n",
      "Episode:156 meanR:38.6300 R:35.0000 rate:0.0700 gloss:0.6745 dlossA:0.7274 dlossQ:0.8486 exploreP:0.5833\n",
      "Episode:157 meanR:38.6200 R:49.0000 rate:0.0980 gloss:0.6775 dlossA:0.7262 dlossQ:0.8476 exploreP:0.5805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:38.9400 R:57.0000 rate:0.1140 gloss:0.6755 dlossA:0.7258 dlossQ:0.8418 exploreP:0.5773\n",
      "Episode:159 meanR:39.1900 R:41.0000 rate:0.0820 gloss:0.6750 dlossA:0.7248 dlossQ:0.8520 exploreP:0.5749\n",
      "Episode:160 meanR:39.5000 R:46.0000 rate:0.0920 gloss:0.6675 dlossA:0.7214 dlossQ:0.8511 exploreP:0.5723\n",
      "Episode:161 meanR:39.6700 R:28.0000 rate:0.0560 gloss:0.6776 dlossA:0.7221 dlossQ:0.8570 exploreP:0.5708\n",
      "Episode:162 meanR:39.8200 R:48.0000 rate:0.0960 gloss:0.6726 dlossA:0.7256 dlossQ:0.8435 exploreP:0.5681\n",
      "Episode:163 meanR:40.7300 R:150.0000 rate:0.3000 gloss:0.6810 dlossA:0.7269 dlossQ:0.8583 exploreP:0.5598\n",
      "Episode:164 meanR:41.2300 R:82.0000 rate:0.1640 gloss:0.6703 dlossA:0.7226 dlossQ:0.8604 exploreP:0.5553\n",
      "Episode:165 meanR:42.0200 R:117.0000 rate:0.2340 gloss:0.6717 dlossA:0.7239 dlossQ:0.8530 exploreP:0.5489\n",
      "Episode:166 meanR:42.3000 R:39.0000 rate:0.0780 gloss:0.6782 dlossA:0.7215 dlossQ:0.8587 exploreP:0.5468\n",
      "Episode:167 meanR:42.9300 R:81.0000 rate:0.1620 gloss:0.6869 dlossA:0.7293 dlossQ:0.8495 exploreP:0.5425\n",
      "Episode:168 meanR:43.4700 R:113.0000 rate:0.2260 gloss:0.6811 dlossA:0.7274 dlossQ:0.8484 exploreP:0.5365\n",
      "Episode:169 meanR:44.1800 R:95.0000 rate:0.1900 gloss:0.6819 dlossA:0.7282 dlossQ:0.8517 exploreP:0.5315\n",
      "Episode:170 meanR:44.7200 R:69.0000 rate:0.1380 gloss:0.6821 dlossA:0.7265 dlossQ:0.8559 exploreP:0.5280\n",
      "Episode:171 meanR:44.6600 R:23.0000 rate:0.0460 gloss:0.6800 dlossA:0.7169 dlossQ:0.8694 exploreP:0.5268\n",
      "Episode:172 meanR:44.7900 R:73.0000 rate:0.1460 gloss:0.6724 dlossA:0.7227 dlossQ:0.8559 exploreP:0.5230\n",
      "Episode:173 meanR:45.7900 R:121.0000 rate:0.2420 gloss:0.6845 dlossA:0.7296 dlossQ:0.8541 exploreP:0.5168\n",
      "Episode:174 meanR:46.4200 R:75.0000 rate:0.1500 gloss:0.6791 dlossA:0.7283 dlossQ:0.8533 exploreP:0.5131\n",
      "Episode:175 meanR:47.6300 R:143.0000 rate:0.2860 gloss:0.6786 dlossA:0.7261 dlossQ:0.8570 exploreP:0.5059\n",
      "Episode:176 meanR:47.7300 R:50.0000 rate:0.1000 gloss:0.6837 dlossA:0.7264 dlossQ:0.8592 exploreP:0.5034\n",
      "Episode:177 meanR:47.4700 R:50.0000 rate:0.1000 gloss:0.6745 dlossA:0.7185 dlossQ:0.8653 exploreP:0.5010\n",
      "Episode:178 meanR:47.5700 R:50.0000 rate:0.1000 gloss:0.6715 dlossA:0.7210 dlossQ:0.8630 exploreP:0.4985\n",
      "Episode:179 meanR:48.2000 R:111.0000 rate:0.2220 gloss:0.6772 dlossA:0.7229 dlossQ:0.8626 exploreP:0.4931\n",
      "Episode:180 meanR:48.1800 R:43.0000 rate:0.0860 gloss:0.6847 dlossA:0.7275 dlossQ:0.8568 exploreP:0.4911\n",
      "Episode:181 meanR:48.7500 R:79.0000 rate:0.1580 gloss:0.6832 dlossA:0.7266 dlossQ:0.8611 exploreP:0.4873\n",
      "Episode:182 meanR:48.5200 R:44.0000 rate:0.0880 gloss:0.6833 dlossA:0.7296 dlossQ:0.8611 exploreP:0.4852\n",
      "Episode:183 meanR:47.9500 R:38.0000 rate:0.0760 gloss:0.6733 dlossA:0.7214 dlossQ:0.8612 exploreP:0.4834\n",
      "Episode:184 meanR:47.8800 R:28.0000 rate:0.0560 gloss:0.6883 dlossA:0.7282 dlossQ:0.8514 exploreP:0.4821\n",
      "Episode:185 meanR:47.8900 R:20.0000 rate:0.0400 gloss:0.6760 dlossA:0.7206 dlossQ:0.8645 exploreP:0.4811\n",
      "Episode:186 meanR:48.0800 R:53.0000 rate:0.1060 gloss:0.6815 dlossA:0.7257 dlossQ:0.8612 exploreP:0.4786\n",
      "Episode:187 meanR:48.0500 R:21.0000 rate:0.0420 gloss:0.6922 dlossA:0.7285 dlossQ:0.8529 exploreP:0.4776\n",
      "Episode:188 meanR:48.7200 R:90.0000 rate:0.1800 gloss:0.6817 dlossA:0.7259 dlossQ:0.8610 exploreP:0.4735\n",
      "Episode:189 meanR:48.7300 R:68.0000 rate:0.1360 gloss:0.6742 dlossA:0.7152 dlossQ:0.8714 exploreP:0.4703\n",
      "Episode:190 meanR:49.1000 R:61.0000 rate:0.1220 gloss:0.6821 dlossA:0.7275 dlossQ:0.8614 exploreP:0.4675\n",
      "Episode:191 meanR:49.6400 R:78.0000 rate:0.1560 gloss:0.6796 dlossA:0.7243 dlossQ:0.8605 exploreP:0.4640\n",
      "Episode:192 meanR:51.6000 R:208.0000 rate:0.4160 gloss:0.6787 dlossA:0.7253 dlossQ:0.8614 exploreP:0.4546\n",
      "Episode:193 meanR:51.8600 R:38.0000 rate:0.0760 gloss:0.6808 dlossA:0.7269 dlossQ:0.8569 exploreP:0.4529\n",
      "Episode:194 meanR:52.0600 R:51.0000 rate:0.1020 gloss:0.6676 dlossA:0.7168 dlossQ:0.8677 exploreP:0.4507\n",
      "Episode:195 meanR:52.6500 R:95.0000 rate:0.1900 gloss:0.6815 dlossA:0.7279 dlossQ:0.8598 exploreP:0.4465\n",
      "Episode:196 meanR:54.3500 R:203.0000 rate:0.4060 gloss:0.6759 dlossA:0.7244 dlossQ:0.8618 exploreP:0.4377\n",
      "Episode:197 meanR:58.7900 R:485.0000 rate:0.9700 gloss:0.6797 dlossA:0.7243 dlossQ:0.8640 exploreP:0.4175\n",
      "Episode:198 meanR:58.8400 R:52.0000 rate:0.1040 gloss:0.6766 dlossA:0.7244 dlossQ:0.8687 exploreP:0.4154\n",
      "Episode:199 meanR:58.8600 R:40.0000 rate:0.0800 gloss:0.6757 dlossA:0.7219 dlossQ:0.8700 exploreP:0.4138\n",
      "Episode:200 meanR:63.6300 R:500.0000 rate:1.0000 gloss:0.6780 dlossA:0.7241 dlossQ:0.8672 exploreP:0.3941\n",
      "Episode:201 meanR:63.5100 R:34.0000 rate:0.0680 gloss:0.6760 dlossA:0.7218 dlossQ:0.8683 exploreP:0.3928\n",
      "Episode:202 meanR:63.5000 R:50.0000 rate:0.1000 gloss:0.6854 dlossA:0.7285 dlossQ:0.8646 exploreP:0.3908\n",
      "Episode:203 meanR:64.2100 R:88.0000 rate:0.1760 gloss:0.6908 dlossA:0.7317 dlossQ:0.8624 exploreP:0.3875\n",
      "Episode:204 meanR:65.4800 R:177.0000 rate:0.3540 gloss:0.6874 dlossA:0.7287 dlossQ:0.8640 exploreP:0.3809\n",
      "Episode:205 meanR:67.2900 R:196.0000 rate:0.3920 gloss:0.6930 dlossA:0.7345 dlossQ:0.8620 exploreP:0.3737\n",
      "Episode:206 meanR:68.5700 R:179.0000 rate:0.3580 gloss:0.6927 dlossA:0.7328 dlossQ:0.8659 exploreP:0.3672\n",
      "Episode:207 meanR:73.4400 R:500.0000 rate:1.0000 gloss:0.6976 dlossA:0.7331 dlossQ:0.8685 exploreP:0.3498\n",
      "Episode:208 meanR:73.8900 R:69.0000 rate:0.1380 gloss:0.7159 dlossA:0.7346 dlossQ:0.8565 exploreP:0.3475\n",
      "Episode:209 meanR:74.5600 R:85.0000 rate:0.1700 gloss:0.7074 dlossA:0.7394 dlossQ:0.8629 exploreP:0.3446\n",
      "Episode:210 meanR:75.0300 R:74.0000 rate:0.1480 gloss:0.7029 dlossA:0.7436 dlossQ:0.8628 exploreP:0.3422\n",
      "Episode:211 meanR:76.6900 R:193.0000 rate:0.3860 gloss:0.7020 dlossA:0.7367 dlossQ:0.8647 exploreP:0.3358\n",
      "Episode:212 meanR:77.3800 R:80.0000 rate:0.1600 gloss:0.7056 dlossA:0.7395 dlossQ:0.8622 exploreP:0.3332\n",
      "Episode:213 meanR:77.9700 R:81.0000 rate:0.1620 gloss:0.7108 dlossA:0.7434 dlossQ:0.8574 exploreP:0.3306\n",
      "Episode:214 meanR:78.7300 R:128.0000 rate:0.2560 gloss:0.7161 dlossA:0.7442 dlossQ:0.8646 exploreP:0.3265\n",
      "Episode:215 meanR:79.9500 R:176.0000 rate:0.3520 gloss:0.7161 dlossA:0.7420 dlossQ:0.8595 exploreP:0.3210\n",
      "Episode:216 meanR:80.8800 R:118.0000 rate:0.2360 gloss:0.7126 dlossA:0.7429 dlossQ:0.8691 exploreP:0.3174\n",
      "Episode:217 meanR:80.5600 R:51.0000 rate:0.1020 gloss:0.7115 dlossA:0.7461 dlossQ:0.8637 exploreP:0.3158\n",
      "Episode:218 meanR:85.2400 R:500.0000 rate:1.0000 gloss:0.7145 dlossA:0.7446 dlossQ:0.8623 exploreP:0.3009\n",
      "Episode:219 meanR:89.8500 R:500.0000 rate:1.0000 gloss:0.7243 dlossA:0.7528 dlossQ:0.8593 exploreP:0.2867\n",
      "Episode:220 meanR:90.3000 R:92.0000 rate:0.1840 gloss:0.7232 dlossA:0.7554 dlossQ:0.8577 exploreP:0.2842\n",
      "Episode:221 meanR:90.5600 R:57.0000 rate:0.1140 gloss:0.7240 dlossA:0.7402 dlossQ:0.8567 exploreP:0.2826\n",
      "Episode:222 meanR:95.3100 R:500.0000 rate:1.0000 gloss:0.7380 dlossA:0.7575 dlossQ:0.8575 exploreP:0.2693\n",
      "Episode:223 meanR:96.9500 R:218.0000 rate:0.4360 gloss:0.7504 dlossA:0.7622 dlossQ:0.8538 exploreP:0.2637\n",
      "Episode:224 meanR:101.2200 R:463.0000 rate:0.9260 gloss:0.7552 dlossA:0.7705 dlossQ:0.8527 exploreP:0.2522\n",
      "Episode:225 meanR:104.1400 R:325.0000 rate:0.6500 gloss:0.7620 dlossA:0.7710 dlossQ:0.8504 exploreP:0.2445\n",
      "Episode:226 meanR:107.4600 R:395.0000 rate:0.7900 gloss:0.7742 dlossA:0.7819 dlossQ:0.8438 exploreP:0.2354\n",
      "Episode:227 meanR:107.6800 R:46.0000 rate:0.0920 gloss:0.7820 dlossA:0.7683 dlossQ:0.8378 exploreP:0.2344\n",
      "Episode:228 meanR:109.7400 R:246.0000 rate:0.4920 gloss:0.7774 dlossA:0.7861 dlossQ:0.8381 exploreP:0.2289\n",
      "Episode:229 meanR:112.3800 R:287.0000 rate:0.5740 gloss:0.7912 dlossA:0.7882 dlossQ:0.8370 exploreP:0.2227\n",
      "Episode:230 meanR:114.9300 R:299.0000 rate:0.5980 gloss:0.7982 dlossA:0.7928 dlossQ:0.8347 exploreP:0.2165\n",
      "Episode:231 meanR:116.0000 R:236.0000 rate:0.4720 gloss:0.8048 dlossA:0.7984 dlossQ:0.8273 exploreP:0.2116\n",
      "Episode:232 meanR:117.8000 R:238.0000 rate:0.4760 gloss:0.8192 dlossA:0.8033 dlossQ:0.8258 exploreP:0.2069\n",
      "Episode:233 meanR:120.4600 R:296.0000 rate:0.5920 gloss:0.8215 dlossA:0.8099 dlossQ:0.8191 exploreP:0.2012\n",
      "Episode:234 meanR:124.7700 R:446.0000 rate:0.8920 gloss:0.8305 dlossA:0.8221 dlossQ:0.8203 exploreP:0.1928\n",
      "Episode:235 meanR:125.3400 R:89.0000 rate:0.1780 gloss:0.8287 dlossA:0.8099 dlossQ:0.8171 exploreP:0.1912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:128.4100 R:340.0000 rate:0.6800 gloss:0.8408 dlossA:0.8237 dlossQ:0.8135 exploreP:0.1851\n",
      "Episode:237 meanR:128.6900 R:78.0000 rate:0.1560 gloss:0.8494 dlossA:0.8453 dlossQ:0.8156 exploreP:0.1838\n",
      "Episode:238 meanR:132.5100 R:471.0000 rate:0.9420 gloss:0.8565 dlossA:0.8375 dlossQ:0.8130 exploreP:0.1758\n",
      "Episode:239 meanR:133.0800 R:78.0000 rate:0.1560 gloss:0.8536 dlossA:0.8395 dlossQ:0.8069 exploreP:0.1745\n",
      "Episode:240 meanR:137.5900 R:500.0000 rate:1.0000 gloss:0.8703 dlossA:0.8397 dlossQ:0.8049 exploreP:0.1665\n",
      "Episode:241 meanR:140.5200 R:342.0000 rate:0.6840 gloss:0.8777 dlossA:0.8516 dlossQ:0.7992 exploreP:0.1612\n",
      "Episode:242 meanR:140.8100 R:101.0000 rate:0.2020 gloss:0.8879 dlossA:0.8579 dlossQ:0.7950 exploreP:0.1597\n",
      "Episode:243 meanR:143.7400 R:324.0000 rate:0.6480 gloss:0.8934 dlossA:0.8549 dlossQ:0.7929 exploreP:0.1549\n",
      "Episode:244 meanR:143.9700 R:113.0000 rate:0.2260 gloss:0.9042 dlossA:0.8668 dlossQ:0.7944 exploreP:0.1533\n",
      "Episode:245 meanR:146.4200 R:261.0000 rate:0.5220 gloss:0.9074 dlossA:0.8742 dlossQ:0.7877 exploreP:0.1496\n",
      "Episode:246 meanR:150.2000 R:421.0000 rate:0.8420 gloss:0.9173 dlossA:0.8722 dlossQ:0.7850 exploreP:0.1438\n",
      "Episode:247 meanR:152.3500 R:269.0000 rate:0.5380 gloss:0.9318 dlossA:0.8829 dlossQ:0.7817 exploreP:0.1403\n",
      "Episode:248 meanR:155.2100 R:302.0000 rate:0.6040 gloss:0.9491 dlossA:0.8747 dlossQ:0.7798 exploreP:0.1364\n",
      "Episode:249 meanR:157.0900 R:267.0000 rate:0.5340 gloss:0.9562 dlossA:0.8997 dlossQ:0.7777 exploreP:0.1331\n",
      "Episode:250 meanR:159.8000 R:311.0000 rate:0.6220 gloss:0.9753 dlossA:0.8969 dlossQ:0.7730 exploreP:0.1293\n",
      "Episode:251 meanR:160.9700 R:146.0000 rate:0.2920 gloss:0.9796 dlossA:0.9214 dlossQ:0.7679 exploreP:0.1276\n",
      "Episode:252 meanR:162.3800 R:156.0000 rate:0.3120 gloss:0.9870 dlossA:0.8977 dlossQ:0.7621 exploreP:0.1258\n",
      "Episode:253 meanR:164.8800 R:314.0000 rate:0.6280 gloss:0.9871 dlossA:0.9253 dlossQ:0.7655 exploreP:0.1222\n",
      "Episode:254 meanR:166.7200 R:275.0000 rate:0.5500 gloss:0.9911 dlossA:0.9243 dlossQ:0.7676 exploreP:0.1191\n",
      "Episode:255 meanR:168.6800 R:246.0000 rate:0.4920 gloss:1.0024 dlossA:0.9343 dlossQ:0.7676 exploreP:0.1165\n",
      "Episode:256 meanR:170.4700 R:214.0000 rate:0.4280 gloss:0.9989 dlossA:0.9413 dlossQ:0.7646 exploreP:0.1142\n",
      "Episode:257 meanR:174.8100 R:483.0000 rate:0.9660 gloss:1.0129 dlossA:0.9409 dlossQ:0.7567 exploreP:0.1093\n",
      "Episode:258 meanR:177.6900 R:345.0000 rate:0.6900 gloss:1.0230 dlossA:0.9544 dlossQ:0.7557 exploreP:0.1060\n",
      "Episode:259 meanR:179.5200 R:224.0000 rate:0.4480 gloss:1.0442 dlossA:0.9513 dlossQ:0.7569 exploreP:0.1038\n",
      "Episode:260 meanR:181.3900 R:233.0000 rate:0.4660 gloss:1.0547 dlossA:0.9597 dlossQ:0.7499 exploreP:0.1017\n",
      "Episode:261 meanR:183.5500 R:244.0000 rate:0.4880 gloss:1.0606 dlossA:0.9505 dlossQ:0.7435 exploreP:0.0995\n",
      "Episode:262 meanR:187.2600 R:419.0000 rate:0.8380 gloss:1.0733 dlossA:0.9884 dlossQ:0.7472 exploreP:0.0958\n",
      "Episode:263 meanR:187.4400 R:168.0000 rate:0.3360 gloss:1.0729 dlossA:1.0050 dlossQ:0.7468 exploreP:0.0944\n",
      "Episode:264 meanR:191.0500 R:443.0000 rate:0.8860 gloss:1.0918 dlossA:0.9762 dlossQ:0.7461 exploreP:0.0907\n",
      "Episode:265 meanR:192.4400 R:256.0000 rate:0.5120 gloss:1.1085 dlossA:1.0037 dlossQ:0.7417 exploreP:0.0887\n",
      "Episode:266 meanR:194.5900 R:254.0000 rate:0.5080 gloss:1.1184 dlossA:1.0024 dlossQ:0.7393 exploreP:0.0867\n",
      "Episode:267 meanR:196.2000 R:242.0000 rate:0.4840 gloss:1.1189 dlossA:1.0289 dlossQ:0.7491 exploreP:0.0849\n",
      "Episode:268 meanR:197.0600 R:199.0000 rate:0.3980 gloss:1.1295 dlossA:1.0097 dlossQ:0.7380 exploreP:0.0834\n",
      "Episode:269 meanR:198.4300 R:232.0000 rate:0.4640 gloss:1.1535 dlossA:1.0008 dlossQ:0.7369 exploreP:0.0817\n",
      "Episode:270 meanR:199.7300 R:199.0000 rate:0.3980 gloss:1.1608 dlossA:1.0216 dlossQ:0.7372 exploreP:0.0803\n",
      "Episode:271 meanR:203.6600 R:416.0000 rate:0.8320 gloss:1.1628 dlossA:1.0429 dlossQ:0.7360 exploreP:0.0774\n",
      "Episode:272 meanR:206.1500 R:322.0000 rate:0.6440 gloss:1.1765 dlossA:1.0295 dlossQ:0.7357 exploreP:0.0753\n",
      "Episode:273 meanR:206.9700 R:203.0000 rate:0.4060 gloss:1.1844 dlossA:1.0670 dlossQ:0.7317 exploreP:0.0740\n",
      "Episode:274 meanR:208.1200 R:190.0000 rate:0.3800 gloss:1.1945 dlossA:1.0467 dlossQ:0.7352 exploreP:0.0728\n",
      "Episode:275 meanR:210.4500 R:376.0000 rate:0.7520 gloss:1.1903 dlossA:1.0755 dlossQ:0.7308 exploreP:0.0705\n",
      "Episode:276 meanR:212.2400 R:229.0000 rate:0.4580 gloss:1.2172 dlossA:1.0729 dlossQ:0.7413 exploreP:0.0691\n",
      "Episode:277 meanR:216.7400 R:500.0000 rate:1.0000 gloss:1.2193 dlossA:1.0812 dlossQ:0.7346 exploreP:0.0662\n",
      "Episode:278 meanR:219.0500 R:281.0000 rate:0.5620 gloss:1.2192 dlossA:1.0776 dlossQ:0.7265 exploreP:0.0646\n",
      "Episode:279 meanR:221.0500 R:311.0000 rate:0.6220 gloss:1.2479 dlossA:1.1004 dlossQ:0.7358 exploreP:0.0630\n",
      "Episode:280 meanR:222.3700 R:175.0000 rate:0.3500 gloss:1.2357 dlossA:1.0867 dlossQ:0.7357 exploreP:0.0621\n",
      "Episode:281 meanR:226.5800 R:500.0000 rate:1.0000 gloss:1.2741 dlossA:1.1080 dlossQ:0.7322 exploreP:0.0595\n",
      "Episode:282 meanR:231.1400 R:500.0000 rate:1.0000 gloss:1.2833 dlossA:1.1125 dlossQ:0.7302 exploreP:0.0571\n",
      "Episode:283 meanR:233.3000 R:254.0000 rate:0.5080 gloss:1.2972 dlossA:1.1264 dlossQ:0.7284 exploreP:0.0559\n",
      "Episode:284 meanR:238.0200 R:500.0000 rate:1.0000 gloss:1.3076 dlossA:1.1465 dlossQ:0.7310 exploreP:0.0537\n",
      "Episode:285 meanR:239.7300 R:191.0000 rate:0.3820 gloss:1.3452 dlossA:1.1183 dlossQ:0.7328 exploreP:0.0529\n",
      "Episode:286 meanR:242.0000 R:280.0000 rate:0.5600 gloss:1.3379 dlossA:1.1626 dlossQ:0.7250 exploreP:0.0517\n",
      "Episode:287 meanR:243.7100 R:192.0000 rate:0.3840 gloss:1.3773 dlossA:1.1885 dlossQ:0.7296 exploreP:0.0509\n",
      "Episode:288 meanR:243.6800 R:87.0000 rate:0.1740 gloss:1.3774 dlossA:1.1667 dlossQ:0.7246 exploreP:0.0505\n",
      "Episode:289 meanR:245.2900 R:229.0000 rate:0.4580 gloss:1.3667 dlossA:1.1712 dlossQ:0.7281 exploreP:0.0496\n",
      "Episode:290 meanR:248.3200 R:364.0000 rate:0.7280 gloss:1.3955 dlossA:1.1935 dlossQ:0.7291 exploreP:0.0482\n",
      "Episode:291 meanR:252.4900 R:495.0000 rate:0.9900 gloss:1.4012 dlossA:1.2126 dlossQ:0.7267 exploreP:0.0463\n",
      "Episode:292 meanR:255.4100 R:500.0000 rate:1.0000 gloss:1.4196 dlossA:1.2007 dlossQ:0.7297 exploreP:0.0446\n",
      "Episode:293 meanR:260.0300 R:500.0000 rate:1.0000 gloss:1.4253 dlossA:1.2347 dlossQ:0.7238 exploreP:0.0429\n",
      "Episode:294 meanR:264.0100 R:449.0000 rate:0.8980 gloss:1.4629 dlossA:1.2535 dlossQ:0.7312 exploreP:0.0414\n",
      "Episode:295 meanR:266.5000 R:344.0000 rate:0.6880 gloss:1.4702 dlossA:1.2659 dlossQ:0.7235 exploreP:0.0404\n",
      "Episode:296 meanR:269.4700 R:500.0000 rate:1.0000 gloss:1.5073 dlossA:1.3009 dlossQ:0.7454 exploreP:0.0389\n",
      "Episode:297 meanR:266.9200 R:230.0000 rate:0.4600 gloss:1.5143 dlossA:1.2970 dlossQ:0.7385 exploreP:0.0382\n",
      "Episode:298 meanR:271.4000 R:500.0000 rate:1.0000 gloss:1.5256 dlossA:1.2773 dlossQ:0.7290 exploreP:0.0369\n",
      "Episode:299 meanR:272.9800 R:198.0000 rate:0.3960 gloss:1.5663 dlossA:1.3111 dlossQ:0.7424 exploreP:0.0363\n",
      "Episode:300 meanR:272.9600 R:498.0000 rate:0.9960 gloss:1.5490 dlossA:1.3326 dlossQ:0.7503 exploreP:0.0351\n",
      "Episode:301 meanR:277.6200 R:500.0000 rate:1.0000 gloss:1.5854 dlossA:1.2918 dlossQ:0.7374 exploreP:0.0338\n",
      "Episode:302 meanR:282.1200 R:500.0000 rate:1.0000 gloss:1.6230 dlossA:1.3288 dlossQ:0.7393 exploreP:0.0327\n",
      "Episode:303 meanR:286.2400 R:500.0000 rate:1.0000 gloss:1.6490 dlossA:1.3968 dlossQ:0.7409 exploreP:0.0316\n",
      "Episode:304 meanR:288.6500 R:418.0000 rate:0.8360 gloss:1.6535 dlossA:1.3999 dlossQ:0.7500 exploreP:0.0307\n",
      "Episode:305 meanR:291.6900 R:500.0000 rate:1.0000 gloss:1.6935 dlossA:1.3969 dlossQ:0.7548 exploreP:0.0297\n",
      "Episode:306 meanR:294.9000 R:500.0000 rate:1.0000 gloss:1.7145 dlossA:1.4116 dlossQ:0.7488 exploreP:0.0287\n",
      "Episode:307 meanR:294.9000 R:500.0000 rate:1.0000 gloss:1.7414 dlossA:1.4497 dlossQ:0.7611 exploreP:0.0278\n",
      "Episode:308 meanR:297.2800 R:307.0000 rate:0.6140 gloss:1.7827 dlossA:1.4926 dlossQ:0.7558 exploreP:0.0273\n",
      "Episode:309 meanR:299.5600 R:313.0000 rate:0.6260 gloss:1.7916 dlossA:1.5341 dlossQ:0.7550 exploreP:0.0267\n",
      "Episode:310 meanR:303.8200 R:500.0000 rate:1.0000 gloss:1.7983 dlossA:1.4974 dlossQ:0.7583 exploreP:0.0259\n",
      "Episode:311 meanR:305.9000 R:401.0000 rate:0.8020 gloss:1.8477 dlossA:1.6191 dlossQ:0.7645 exploreP:0.0253\n",
      "Episode:312 meanR:307.7500 R:265.0000 rate:0.5300 gloss:1.8702 dlossA:1.5470 dlossQ:0.7989 exploreP:0.0249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:313 meanR:308.7100 R:177.0000 rate:0.3540 gloss:1.8448 dlossA:1.5609 dlossQ:0.8079 exploreP:0.0246\n",
      "Episode:314 meanR:312.4300 R:500.0000 rate:1.0000 gloss:1.8551 dlossA:1.5381 dlossQ:0.8177 exploreP:0.0239\n",
      "Episode:315 meanR:313.2600 R:259.0000 rate:0.5180 gloss:1.8969 dlossA:1.5148 dlossQ:0.7766 exploreP:0.0236\n",
      "Episode:316 meanR:315.3600 R:328.0000 rate:0.6560 gloss:1.9166 dlossA:1.5891 dlossQ:0.7755 exploreP:0.0231\n",
      "Episode:317 meanR:318.9700 R:412.0000 rate:0.8240 gloss:1.9195 dlossA:1.6103 dlossQ:0.8011 exploreP:0.0226\n",
      "Episode:318 meanR:316.6900 R:272.0000 rate:0.5440 gloss:1.9526 dlossA:1.6131 dlossQ:0.8027 exploreP:0.0223\n",
      "Episode:319 meanR:315.5300 R:384.0000 rate:0.7680 gloss:1.9483 dlossA:1.6041 dlossQ:0.8222 exploreP:0.0218\n",
      "Episode:320 meanR:317.7500 R:314.0000 rate:0.6280 gloss:1.9850 dlossA:1.5875 dlossQ:0.7966 exploreP:0.0214\n",
      "Episode:321 meanR:319.9800 R:280.0000 rate:0.5600 gloss:1.9992 dlossA:1.6587 dlossQ:0.8284 exploreP:0.0211\n",
      "Episode:322 meanR:317.7100 R:273.0000 rate:0.5460 gloss:1.9846 dlossA:1.6984 dlossQ:0.8040 exploreP:0.0208\n",
      "Episode:323 meanR:318.5500 R:302.0000 rate:0.6040 gloss:1.9990 dlossA:1.6496 dlossQ:0.7938 exploreP:0.0205\n",
      "Episode:324 meanR:317.7700 R:385.0000 rate:0.7700 gloss:2.0540 dlossA:1.7813 dlossQ:0.8312 exploreP:0.0201\n",
      "Episode:325 meanR:316.9500 R:243.0000 rate:0.4860 gloss:2.0564 dlossA:1.7004 dlossQ:0.8158 exploreP:0.0199\n",
      "Episode:326 meanR:315.7100 R:271.0000 rate:0.5420 gloss:2.0107 dlossA:1.7471 dlossQ:0.8459 exploreP:0.0196\n",
      "Episode:327 meanR:320.2500 R:500.0000 rate:1.0000 gloss:2.0753 dlossA:1.6881 dlossQ:0.8815 exploreP:0.0191\n",
      "Episode:328 meanR:321.1600 R:337.0000 rate:0.6740 gloss:2.0722 dlossA:1.7185 dlossQ:0.8268 exploreP:0.0188\n",
      "Episode:329 meanR:320.1200 R:183.0000 rate:0.3660 gloss:2.0406 dlossA:1.7652 dlossQ:0.8360 exploreP:0.0187\n",
      "Episode:330 meanR:322.1300 R:500.0000 rate:1.0000 gloss:2.1235 dlossA:1.7790 dlossQ:0.8184 exploreP:0.0182\n",
      "Episode:331 meanR:322.4000 R:263.0000 rate:0.5260 gloss:2.2077 dlossA:1.8444 dlossQ:0.8346 exploreP:0.0180\n",
      "Episode:332 meanR:322.5400 R:252.0000 rate:0.5040 gloss:2.1693 dlossA:1.7296 dlossQ:0.8259 exploreP:0.0178\n",
      "Episode:333 meanR:322.0700 R:249.0000 rate:0.4980 gloss:2.1463 dlossA:1.8329 dlossQ:0.8367 exploreP:0.0176\n",
      "Episode:334 meanR:321.8900 R:428.0000 rate:0.8560 gloss:2.1898 dlossA:1.8199 dlossQ:0.8537 exploreP:0.0173\n",
      "Episode:335 meanR:326.0000 R:500.0000 rate:1.0000 gloss:2.2403 dlossA:1.9057 dlossQ:0.8616 exploreP:0.0170\n",
      "Episode:336 meanR:325.9600 R:336.0000 rate:0.6720 gloss:2.2361 dlossA:1.8657 dlossQ:0.8800 exploreP:0.0167\n",
      "Episode:337 meanR:330.1800 R:500.0000 rate:1.0000 gloss:2.2749 dlossA:1.9274 dlossQ:0.8494 exploreP:0.0164\n",
      "Episode:338 meanR:328.2700 R:280.0000 rate:0.5600 gloss:2.3336 dlossA:1.9245 dlossQ:0.8372 exploreP:0.0162\n",
      "Episode:339 meanR:332.4900 R:500.0000 rate:1.0000 gloss:2.2795 dlossA:1.8587 dlossQ:0.8602 exploreP:0.0159\n",
      "Episode:340 meanR:330.2600 R:277.0000 rate:0.5540 gloss:2.3934 dlossA:2.1081 dlossQ:0.8696 exploreP:0.0158\n",
      "Episode:341 meanR:331.8400 R:500.0000 rate:1.0000 gloss:2.4124 dlossA:2.0779 dlossQ:0.8811 exploreP:0.0155\n",
      "Episode:342 meanR:335.8300 R:500.0000 rate:1.0000 gloss:2.3890 dlossA:2.0602 dlossQ:0.8651 exploreP:0.0152\n",
      "Episode:343 meanR:335.6900 R:310.0000 rate:0.6200 gloss:2.4770 dlossA:2.0753 dlossQ:0.9269 exploreP:0.0150\n",
      "Episode:344 meanR:337.9500 R:339.0000 rate:0.6780 gloss:2.4317 dlossA:2.0111 dlossQ:0.9119 exploreP:0.0149\n",
      "Episode:345 meanR:337.2700 R:193.0000 rate:0.3860 gloss:2.5107 dlossA:2.0468 dlossQ:0.8734 exploreP:0.0148\n",
      "Episode:346 meanR:334.7000 R:164.0000 rate:0.3280 gloss:2.4866 dlossA:2.0428 dlossQ:0.9179 exploreP:0.0147\n",
      "Episode:347 meanR:337.0100 R:500.0000 rate:1.0000 gloss:2.5463 dlossA:2.0407 dlossQ:0.9494 exploreP:0.0145\n",
      "Episode:348 meanR:335.1500 R:116.0000 rate:0.2320 gloss:2.5819 dlossA:2.0641 dlossQ:1.0433 exploreP:0.0144\n",
      "Episode:349 meanR:335.9200 R:344.0000 rate:0.6880 gloss:2.7248 dlossA:2.1947 dlossQ:0.8938 exploreP:0.0143\n",
      "Episode:350 meanR:336.7200 R:391.0000 rate:0.7820 gloss:2.6677 dlossA:2.2246 dlossQ:0.9533 exploreP:0.0141\n",
      "Episode:351 meanR:336.5600 R:130.0000 rate:0.2600 gloss:2.5569 dlossA:2.1437 dlossQ:0.9458 exploreP:0.0141\n",
      "Episode:352 meanR:337.6100 R:261.0000 rate:0.5220 gloss:2.7145 dlossA:2.3339 dlossQ:0.9971 exploreP:0.0140\n",
      "Episode:353 meanR:339.4700 R:500.0000 rate:1.0000 gloss:2.6745 dlossA:2.3534 dlossQ:1.0302 exploreP:0.0138\n",
      "Episode:354 meanR:341.1900 R:447.0000 rate:0.8940 gloss:2.7647 dlossA:2.3428 dlossQ:1.0029 exploreP:0.0136\n",
      "Episode:355 meanR:340.4100 R:168.0000 rate:0.3360 gloss:2.6392 dlossA:2.1222 dlossQ:1.0132 exploreP:0.0135\n",
      "Episode:356 meanR:343.2500 R:498.0000 rate:0.9960 gloss:2.7709 dlossA:2.2740 dlossQ:0.9929 exploreP:0.0134\n",
      "Episode:357 meanR:342.8100 R:439.0000 rate:0.8780 gloss:2.7601 dlossA:2.3694 dlossQ:1.0714 exploreP:0.0132\n",
      "Episode:358 meanR:344.2500 R:489.0000 rate:0.9780 gloss:2.9479 dlossA:2.4869 dlossQ:1.0137 exploreP:0.0131\n",
      "Episode:359 meanR:346.3700 R:436.0000 rate:0.8720 gloss:2.9418 dlossA:2.4487 dlossQ:1.2520 exploreP:0.0129\n",
      "Episode:360 meanR:346.2200 R:218.0000 rate:0.4360 gloss:2.8737 dlossA:2.3096 dlossQ:1.1617 exploreP:0.0129\n",
      "Episode:361 meanR:346.0100 R:223.0000 rate:0.4460 gloss:2.8952 dlossA:2.4983 dlossQ:1.2511 exploreP:0.0128\n",
      "Episode:362 meanR:344.9800 R:316.0000 rate:0.6320 gloss:2.9431 dlossA:2.4658 dlossQ:1.1902 exploreP:0.0127\n",
      "Episode:363 meanR:348.3000 R:500.0000 rate:1.0000 gloss:2.9292 dlossA:2.4332 dlossQ:1.1707 exploreP:0.0126\n",
      "Episode:364 meanR:348.8700 R:500.0000 rate:1.0000 gloss:2.9216 dlossA:2.4482 dlossQ:1.2326 exploreP:0.0125\n",
      "Episode:365 meanR:349.3700 R:306.0000 rate:0.6120 gloss:2.8722 dlossA:2.5153 dlossQ:1.4094 exploreP:0.0124\n",
      "Episode:366 meanR:349.4100 R:258.0000 rate:0.5160 gloss:3.0006 dlossA:2.4852 dlossQ:1.1930 exploreP:0.0123\n",
      "Episode:367 meanR:351.9900 R:500.0000 rate:1.0000 gloss:3.0501 dlossA:2.5105 dlossQ:1.3307 exploreP:0.0122\n",
      "Episode:368 meanR:352.3300 R:233.0000 rate:0.4660 gloss:2.8519 dlossA:2.2696 dlossQ:1.3093 exploreP:0.0122\n",
      "Episode:369 meanR:353.1800 R:317.0000 rate:0.6340 gloss:3.1536 dlossA:2.6558 dlossQ:1.3761 exploreP:0.0121\n",
      "Episode:370 meanR:354.0000 R:281.0000 rate:0.5620 gloss:2.9688 dlossA:2.4001 dlossQ:1.2160 exploreP:0.0120\n",
      "Episode:371 meanR:353.4200 R:358.0000 rate:0.7160 gloss:3.0369 dlossA:2.5529 dlossQ:1.2890 exploreP:0.0120\n",
      "Episode:372 meanR:355.2000 R:500.0000 rate:1.0000 gloss:3.0881 dlossA:2.5482 dlossQ:1.2786 exploreP:0.0119\n",
      "Episode:373 meanR:357.4800 R:431.0000 rate:0.8620 gloss:3.1276 dlossA:2.6401 dlossQ:1.2713 exploreP:0.0118\n",
      "Episode:374 meanR:360.5800 R:500.0000 rate:1.0000 gloss:3.1353 dlossA:2.5335 dlossQ:1.3053 exploreP:0.0117\n",
      "Episode:375 meanR:360.9700 R:415.0000 rate:0.8300 gloss:3.2901 dlossA:2.6147 dlossQ:1.3179 exploreP:0.0116\n",
      "Episode:376 meanR:361.5600 R:288.0000 rate:0.5760 gloss:3.1244 dlossA:2.5639 dlossQ:1.2697 exploreP:0.0116\n",
      "Episode:377 meanR:358.1800 R:162.0000 rate:0.3240 gloss:3.1428 dlossA:2.5646 dlossQ:1.3219 exploreP:0.0116\n",
      "Episode:378 meanR:357.9200 R:255.0000 rate:0.5100 gloss:3.4186 dlossA:3.0380 dlossQ:1.3134 exploreP:0.0115\n",
      "Episode:379 meanR:359.8100 R:500.0000 rate:1.0000 gloss:3.3642 dlossA:2.7381 dlossQ:1.4158 exploreP:0.0115\n",
      "Episode:380 meanR:360.9200 R:286.0000 rate:0.5720 gloss:3.3761 dlossA:2.8731 dlossQ:1.3920 exploreP:0.0114\n",
      "Episode:381 meanR:360.9200 R:500.0000 rate:1.0000 gloss:3.3965 dlossA:2.8217 dlossQ:1.3457 exploreP:0.0113\n",
      "Episode:382 meanR:357.1900 R:127.0000 rate:0.2540 gloss:3.5276 dlossA:2.8340 dlossQ:1.6593 exploreP:0.0113\n",
      "Episode:383 meanR:359.3100 R:466.0000 rate:0.9320 gloss:3.4484 dlossA:2.9036 dlossQ:1.4319 exploreP:0.0113\n",
      "Episode:384 meanR:356.5700 R:226.0000 rate:0.4520 gloss:3.5127 dlossA:2.7956 dlossQ:1.4894 exploreP:0.0112\n",
      "Episode:385 meanR:357.5700 R:291.0000 rate:0.5820 gloss:3.5628 dlossA:2.9593 dlossQ:1.7752 exploreP:0.0112\n",
      "Episode:386 meanR:356.2300 R:146.0000 rate:0.2920 gloss:3.4678 dlossA:3.1047 dlossQ:1.7066 exploreP:0.0112\n",
      "Episode:387 meanR:356.3500 R:204.0000 rate:0.4080 gloss:3.5217 dlossA:3.0938 dlossQ:1.5949 exploreP:0.0112\n",
      "Episode:388 meanR:357.3700 R:189.0000 rate:0.3780 gloss:3.5167 dlossA:2.8355 dlossQ:1.8011 exploreP:0.0111\n",
      "Episode:389 meanR:360.0800 R:500.0000 rate:1.0000 gloss:3.4754 dlossA:2.9333 dlossQ:1.6176 exploreP:0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:390 meanR:361.2000 R:476.0000 rate:0.9520 gloss:3.3966 dlossA:2.7801 dlossQ:1.8053 exploreP:0.0110\n",
      "Episode:391 meanR:361.2500 R:500.0000 rate:1.0000 gloss:3.5161 dlossA:2.9363 dlossQ:1.5355 exploreP:0.0110\n",
      "Episode:392 meanR:359.4400 R:319.0000 rate:0.6380 gloss:3.5824 dlossA:2.9575 dlossQ:1.7574 exploreP:0.0109\n",
      "Episode:393 meanR:359.4400 R:500.0000 rate:1.0000 gloss:3.6113 dlossA:2.9527 dlossQ:1.5201 exploreP:0.0109\n",
      "Episode:394 meanR:358.4200 R:347.0000 rate:0.6940 gloss:3.7369 dlossA:2.9161 dlossQ:1.5588 exploreP:0.0109\n",
      "Episode:395 meanR:358.8000 R:382.0000 rate:0.7640 gloss:3.7974 dlossA:3.1884 dlossQ:1.7704 exploreP:0.0108\n",
      "Episode:396 meanR:355.4200 R:162.0000 rate:0.3240 gloss:3.6482 dlossA:3.0908 dlossQ:1.6573 exploreP:0.0108\n",
      "Episode:397 meanR:357.2500 R:413.0000 rate:0.8260 gloss:3.6132 dlossA:2.8049 dlossQ:1.7426 exploreP:0.0108\n",
      "Episode:398 meanR:355.9800 R:373.0000 rate:0.7460 gloss:3.6404 dlossA:2.8696 dlossQ:1.9927 exploreP:0.0108\n",
      "Episode:399 meanR:358.5400 R:454.0000 rate:0.9080 gloss:3.8654 dlossA:3.3022 dlossQ:1.6509 exploreP:0.0107\n",
      "Episode:400 meanR:357.3600 R:380.0000 rate:0.7600 gloss:3.9072 dlossA:3.2948 dlossQ:2.0053 exploreP:0.0107\n",
      "Episode:401 meanR:356.1200 R:376.0000 rate:0.7520 gloss:3.7713 dlossA:3.0951 dlossQ:1.7953 exploreP:0.0107\n",
      "Episode:402 meanR:354.4400 R:332.0000 rate:0.6640 gloss:3.7369 dlossA:3.1967 dlossQ:1.6745 exploreP:0.0107\n",
      "Episode:403 meanR:353.3600 R:392.0000 rate:0.7840 gloss:3.9323 dlossA:3.2702 dlossQ:1.7999 exploreP:0.0106\n",
      "Episode:404 meanR:354.1800 R:500.0000 rate:1.0000 gloss:3.9713 dlossA:3.1907 dlossQ:1.9518 exploreP:0.0106\n",
      "Episode:405 meanR:352.7800 R:360.0000 rate:0.7200 gloss:3.9834 dlossA:3.5364 dlossQ:1.9015 exploreP:0.0106\n",
      "Episode:406 meanR:352.7800 R:500.0000 rate:1.0000 gloss:4.0572 dlossA:3.3289 dlossQ:2.1472 exploreP:0.0105\n",
      "Episode:407 meanR:352.7800 R:500.0000 rate:1.0000 gloss:4.0788 dlossA:3.3688 dlossQ:2.3195 exploreP:0.0105\n",
      "Episode:408 meanR:353.3600 R:365.0000 rate:0.7300 gloss:4.0151 dlossA:3.3724 dlossQ:2.5316 exploreP:0.0105\n",
      "Episode:409 meanR:355.2300 R:500.0000 rate:1.0000 gloss:3.8929 dlossA:3.1674 dlossQ:2.0858 exploreP:0.0105\n",
      "Episode:410 meanR:352.8700 R:264.0000 rate:0.5280 gloss:4.0347 dlossA:3.0848 dlossQ:1.9974 exploreP:0.0105\n",
      "Episode:411 meanR:353.8600 R:500.0000 rate:1.0000 gloss:4.1098 dlossA:3.4328 dlossQ:2.2262 exploreP:0.0104\n",
      "Episode:412 meanR:355.0700 R:386.0000 rate:0.7720 gloss:4.1549 dlossA:3.2994 dlossQ:1.9470 exploreP:0.0104\n",
      "Episode:413 meanR:356.4300 R:313.0000 rate:0.6260 gloss:4.0674 dlossA:3.3587 dlossQ:2.0640 exploreP:0.0104\n",
      "Episode:414 meanR:356.0600 R:463.0000 rate:0.9260 gloss:4.1851 dlossA:3.3721 dlossQ:2.2202 exploreP:0.0104\n",
      "Episode:415 meanR:356.8400 R:337.0000 rate:0.6740 gloss:4.4059 dlossA:3.6014 dlossQ:2.3345 exploreP:0.0104\n",
      "Episode:416 meanR:358.5600 R:500.0000 rate:1.0000 gloss:4.4279 dlossA:3.6110 dlossQ:2.2770 exploreP:0.0104\n",
      "Episode:417 meanR:357.7500 R:331.0000 rate:0.6620 gloss:4.1491 dlossA:3.2542 dlossQ:2.3109 exploreP:0.0104\n",
      "Episode:418 meanR:357.2100 R:218.0000 rate:0.4360 gloss:4.3502 dlossA:3.7966 dlossQ:2.5223 exploreP:0.0103\n",
      "Episode:419 meanR:358.3600 R:499.0000 rate:0.9980 gloss:4.3953 dlossA:3.5962 dlossQ:2.6116 exploreP:0.0103\n",
      "Episode:420 meanR:360.2200 R:500.0000 rate:1.0000 gloss:4.3869 dlossA:3.8166 dlossQ:2.8383 exploreP:0.0103\n",
      "Episode:421 meanR:362.4200 R:500.0000 rate:1.0000 gloss:4.3052 dlossA:3.4085 dlossQ:2.7235 exploreP:0.0103\n",
      "Episode:422 meanR:364.6900 R:500.0000 rate:1.0000 gloss:4.4410 dlossA:3.3446 dlossQ:2.8340 exploreP:0.0103\n",
      "Episode:423 meanR:365.5600 R:389.0000 rate:0.7780 gloss:4.2517 dlossA:3.2666 dlossQ:2.2057 exploreP:0.0103\n",
      "Episode:424 meanR:364.6600 R:295.0000 rate:0.5900 gloss:4.4302 dlossA:3.4355 dlossQ:2.7747 exploreP:0.0103\n",
      "Episode:425 meanR:364.4800 R:225.0000 rate:0.4500 gloss:4.4785 dlossA:3.7920 dlossQ:2.9679 exploreP:0.0103\n",
      "Episode:426 meanR:364.4100 R:264.0000 rate:0.5280 gloss:4.2103 dlossA:3.1898 dlossQ:2.4778 exploreP:0.0103\n",
      "Episode:427 meanR:363.2800 R:387.0000 rate:0.7740 gloss:4.1042 dlossA:3.2895 dlossQ:2.1900 exploreP:0.0102\n",
      "Episode:428 meanR:363.4100 R:350.0000 rate:0.7000 gloss:4.3160 dlossA:3.4037 dlossQ:2.1922 exploreP:0.0102\n",
      "Episode:429 meanR:364.3100 R:273.0000 rate:0.5460 gloss:4.0249 dlossA:3.4184 dlossQ:2.4113 exploreP:0.0102\n",
      "Episode:430 meanR:364.3100 R:500.0000 rate:1.0000 gloss:4.4788 dlossA:3.9463 dlossQ:2.7761 exploreP:0.0102\n",
      "Episode:431 meanR:363.6500 R:197.0000 rate:0.3940 gloss:4.1054 dlossA:3.1319 dlossQ:4.2851 exploreP:0.0102\n",
      "Episode:432 meanR:363.5000 R:237.0000 rate:0.4740 gloss:4.3128 dlossA:3.5340 dlossQ:3.1392 exploreP:0.0102\n",
      "Episode:433 meanR:366.0100 R:500.0000 rate:1.0000 gloss:4.3564 dlossA:3.6624 dlossQ:2.9955 exploreP:0.0102\n",
      "Episode:434 meanR:366.1800 R:445.0000 rate:0.8900 gloss:4.3148 dlossA:3.5704 dlossQ:2.3658 exploreP:0.0102\n",
      "Episode:435 meanR:364.4600 R:328.0000 rate:0.6560 gloss:4.3575 dlossA:3.7402 dlossQ:2.6038 exploreP:0.0102\n",
      "Episode:436 meanR:366.1000 R:500.0000 rate:1.0000 gloss:4.2082 dlossA:3.4502 dlossQ:2.5445 exploreP:0.0102\n",
      "Episode:437 meanR:362.9300 R:183.0000 rate:0.3660 gloss:4.3899 dlossA:3.5707 dlossQ:2.8478 exploreP:0.0102\n",
      "Episode:438 meanR:363.4000 R:327.0000 rate:0.6540 gloss:4.3465 dlossA:3.4426 dlossQ:2.4007 exploreP:0.0102\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
