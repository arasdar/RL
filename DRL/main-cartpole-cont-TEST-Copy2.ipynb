{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 6), (11, 4), (11,), (11, 4), (11,), (11,), (11,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:21.0000 R:21.0000 rate:0.0420 gloss:0.7585 dlossA:0.6903 dlossQ:1.3014 exploreP:0.9979\n",
      "Episode:1 meanR:20.0000 R:19.0000 rate:0.0380 gloss:0.8064 dlossA:0.7124 dlossQ:1.3407 exploreP:0.9960\n",
      "Episode:2 meanR:19.3333 R:18.0000 rate:0.0360 gloss:0.7654 dlossA:0.6940 dlossQ:1.3156 exploreP:0.9943\n",
      "Episode:3 meanR:26.0000 R:46.0000 rate:0.0920 gloss:0.7726 dlossA:0.7004 dlossQ:1.2923 exploreP:0.9898\n",
      "Episode:4 meanR:24.0000 R:16.0000 rate:0.0320 gloss:0.7255 dlossA:0.6770 dlossQ:1.2613 exploreP:0.9882\n",
      "Episode:5 meanR:24.5000 R:27.0000 rate:0.0540 gloss:0.7428 dlossA:0.6808 dlossQ:1.2778 exploreP:0.9856\n",
      "Episode:6 meanR:23.2857 R:16.0000 rate:0.0320 gloss:0.7543 dlossA:0.6882 dlossQ:1.2553 exploreP:0.9840\n",
      "Episode:7 meanR:22.7500 R:19.0000 rate:0.0380 gloss:0.7262 dlossA:0.6791 dlossQ:1.2647 exploreP:0.9821\n",
      "Episode:8 meanR:21.3333 R:10.0000 rate:0.0200 gloss:0.7612 dlossA:0.6929 dlossQ:1.3051 exploreP:0.9812\n",
      "Episode:9 meanR:20.5000 R:13.0000 rate:0.0260 gloss:0.7422 dlossA:0.6889 dlossQ:1.2803 exploreP:0.9799\n",
      "Episode:10 meanR:22.6364 R:44.0000 rate:0.0880 gloss:0.7483 dlossA:0.6877 dlossQ:1.2725 exploreP:0.9757\n",
      "Episode:11 meanR:22.5833 R:22.0000 rate:0.0440 gloss:0.7280 dlossA:0.6810 dlossQ:1.2568 exploreP:0.9735\n",
      "Episode:12 meanR:22.0769 R:16.0000 rate:0.0320 gloss:0.7378 dlossA:0.6824 dlossQ:1.2822 exploreP:0.9720\n",
      "Episode:13 meanR:21.3571 R:12.0000 rate:0.0240 gloss:0.7772 dlossA:0.6962 dlossQ:1.3638 exploreP:0.9708\n",
      "Episode:14 meanR:21.2667 R:20.0000 rate:0.0400 gloss:0.7225 dlossA:0.6758 dlossQ:1.2755 exploreP:0.9689\n",
      "Episode:15 meanR:20.9375 R:16.0000 rate:0.0320 gloss:0.7068 dlossA:0.6710 dlossQ:1.1929 exploreP:0.9674\n",
      "Episode:16 meanR:20.5294 R:14.0000 rate:0.0280 gloss:0.7053 dlossA:0.6686 dlossQ:1.2191 exploreP:0.9660\n",
      "Episode:17 meanR:20.5000 R:20.0000 rate:0.0400 gloss:0.7295 dlossA:0.6835 dlossQ:1.2455 exploreP:0.9641\n",
      "Episode:18 meanR:20.4211 R:19.0000 rate:0.0380 gloss:0.7616 dlossA:0.6928 dlossQ:1.2983 exploreP:0.9623\n",
      "Episode:19 meanR:20.0000 R:12.0000 rate:0.0240 gloss:0.7087 dlossA:0.6732 dlossQ:1.2451 exploreP:0.9612\n",
      "Episode:20 meanR:19.7143 R:14.0000 rate:0.0280 gloss:0.7175 dlossA:0.6789 dlossQ:1.2310 exploreP:0.9599\n",
      "Episode:21 meanR:19.2727 R:10.0000 rate:0.0200 gloss:0.7385 dlossA:0.6856 dlossQ:1.2680 exploreP:0.9589\n",
      "Episode:22 meanR:19.1304 R:16.0000 rate:0.0320 gloss:0.7692 dlossA:0.6962 dlossQ:1.3003 exploreP:0.9574\n",
      "Episode:23 meanR:19.3750 R:25.0000 rate:0.0500 gloss:0.7594 dlossA:0.6973 dlossQ:1.2787 exploreP:0.9550\n",
      "Episode:24 meanR:19.8400 R:31.0000 rate:0.0620 gloss:0.6990 dlossA:0.6690 dlossQ:1.2294 exploreP:0.9521\n",
      "Episode:25 meanR:19.8077 R:19.0000 rate:0.0380 gloss:0.7359 dlossA:0.6843 dlossQ:1.2524 exploreP:0.9503\n",
      "Episode:26 meanR:20.1481 R:29.0000 rate:0.0580 gloss:0.7114 dlossA:0.6752 dlossQ:1.2272 exploreP:0.9476\n",
      "Episode:27 meanR:20.1429 R:20.0000 rate:0.0400 gloss:0.6811 dlossA:0.6604 dlossQ:1.2450 exploreP:0.9457\n",
      "Episode:28 meanR:20.8621 R:41.0000 rate:0.0820 gloss:0.7151 dlossA:0.6769 dlossQ:1.2298 exploreP:0.9419\n",
      "Episode:29 meanR:20.8000 R:19.0000 rate:0.0380 gloss:0.6958 dlossA:0.6705 dlossQ:1.2081 exploreP:0.9401\n",
      "Episode:30 meanR:20.9677 R:26.0000 rate:0.0520 gloss:0.7066 dlossA:0.6718 dlossQ:1.2225 exploreP:0.9377\n",
      "Episode:31 meanR:23.1562 R:91.0000 rate:0.1820 gloss:0.7072 dlossA:0.6731 dlossQ:1.2298 exploreP:0.9293\n",
      "Episode:32 meanR:22.7879 R:11.0000 rate:0.0220 gloss:0.7214 dlossA:0.6823 dlossQ:1.2543 exploreP:0.9283\n",
      "Episode:33 meanR:22.5294 R:14.0000 rate:0.0280 gloss:0.7380 dlossA:0.6783 dlossQ:1.2891 exploreP:0.9270\n",
      "Episode:34 meanR:23.4571 R:55.0000 rate:0.1100 gloss:0.7659 dlossA:0.6971 dlossQ:1.2896 exploreP:0.9220\n",
      "Episode:35 meanR:23.2778 R:17.0000 rate:0.0340 gloss:0.7199 dlossA:0.6776 dlossQ:1.2744 exploreP:0.9204\n",
      "Episode:36 meanR:23.8919 R:46.0000 rate:0.0920 gloss:0.7214 dlossA:0.6818 dlossQ:1.2275 exploreP:0.9162\n",
      "Episode:37 meanR:23.9474 R:26.0000 rate:0.0520 gloss:0.7792 dlossA:0.7050 dlossQ:1.2732 exploreP:0.9139\n",
      "Episode:38 meanR:23.9231 R:23.0000 rate:0.0460 gloss:0.7167 dlossA:0.6778 dlossQ:1.2429 exploreP:0.9118\n",
      "Episode:39 meanR:24.1000 R:31.0000 rate:0.0620 gloss:0.7227 dlossA:0.6781 dlossQ:1.2523 exploreP:0.9090\n",
      "Episode:40 meanR:24.6829 R:48.0000 rate:0.0960 gloss:0.6953 dlossA:0.6672 dlossQ:1.2200 exploreP:0.9047\n",
      "Episode:41 meanR:25.0952 R:42.0000 rate:0.0840 gloss:0.7198 dlossA:0.6815 dlossQ:1.2320 exploreP:0.9010\n",
      "Episode:42 meanR:24.9535 R:19.0000 rate:0.0380 gloss:0.6941 dlossA:0.6676 dlossQ:1.2210 exploreP:0.8993\n",
      "Episode:43 meanR:24.9091 R:23.0000 rate:0.0460 gloss:0.7636 dlossA:0.7111 dlossQ:1.2890 exploreP:0.8972\n",
      "Episode:44 meanR:25.0444 R:31.0000 rate:0.0620 gloss:0.7148 dlossA:0.6796 dlossQ:1.2228 exploreP:0.8945\n",
      "Episode:45 meanR:24.8478 R:16.0000 rate:0.0320 gloss:0.7362 dlossA:0.6874 dlossQ:1.1866 exploreP:0.8931\n",
      "Episode:46 meanR:24.6170 R:14.0000 rate:0.0280 gloss:0.6943 dlossA:0.6706 dlossQ:1.2066 exploreP:0.8918\n",
      "Episode:47 meanR:24.5625 R:22.0000 rate:0.0440 gloss:0.6917 dlossA:0.6687 dlossQ:1.2087 exploreP:0.8899\n",
      "Episode:48 meanR:24.6939 R:31.0000 rate:0.0620 gloss:0.7214 dlossA:0.6827 dlossQ:1.2255 exploreP:0.8872\n",
      "Episode:49 meanR:25.4000 R:60.0000 rate:0.1200 gloss:0.7178 dlossA:0.6785 dlossQ:1.2232 exploreP:0.8819\n",
      "Episode:50 meanR:25.2353 R:17.0000 rate:0.0340 gloss:0.7137 dlossA:0.6756 dlossQ:1.2232 exploreP:0.8804\n",
      "Episode:51 meanR:26.4038 R:86.0000 rate:0.1720 gloss:0.7350 dlossA:0.6848 dlossQ:1.2433 exploreP:0.8730\n",
      "Episode:52 meanR:26.2075 R:16.0000 rate:0.0320 gloss:0.7034 dlossA:0.6831 dlossQ:1.1841 exploreP:0.8716\n",
      "Episode:53 meanR:26.0185 R:16.0000 rate:0.0320 gloss:0.6881 dlossA:0.6654 dlossQ:1.2081 exploreP:0.8702\n",
      "Episode:54 meanR:26.0000 R:25.0000 rate:0.0500 gloss:0.7266 dlossA:0.6844 dlossQ:1.2440 exploreP:0.8681\n",
      "Episode:55 meanR:25.7857 R:14.0000 rate:0.0280 gloss:0.7323 dlossA:0.6970 dlossQ:1.2637 exploreP:0.8669\n",
      "Episode:56 meanR:26.9123 R:90.0000 rate:0.1800 gloss:0.6960 dlossA:0.6716 dlossQ:1.2067 exploreP:0.8592\n",
      "Episode:57 meanR:26.7241 R:16.0000 rate:0.0320 gloss:0.7151 dlossA:0.6787 dlossQ:1.2151 exploreP:0.8579\n",
      "Episode:58 meanR:26.6949 R:25.0000 rate:0.0500 gloss:0.7177 dlossA:0.6793 dlossQ:1.2073 exploreP:0.8557\n",
      "Episode:59 meanR:27.1333 R:53.0000 rate:0.1060 gloss:0.7145 dlossA:0.6792 dlossQ:1.2337 exploreP:0.8513\n",
      "Episode:60 meanR:27.0820 R:24.0000 rate:0.0480 gloss:0.6916 dlossA:0.6703 dlossQ:1.2059 exploreP:0.8492\n",
      "Episode:61 meanR:27.3710 R:45.0000 rate:0.0900 gloss:0.7209 dlossA:0.6827 dlossQ:1.2370 exploreP:0.8455\n",
      "Episode:62 meanR:27.4286 R:31.0000 rate:0.0620 gloss:0.7233 dlossA:0.6843 dlossQ:1.2358 exploreP:0.8429\n",
      "Episode:63 meanR:27.5000 R:32.0000 rate:0.0640 gloss:0.7331 dlossA:0.6835 dlossQ:1.2466 exploreP:0.8402\n",
      "Episode:64 meanR:27.5231 R:29.0000 rate:0.0580 gloss:0.7217 dlossA:0.6775 dlossQ:1.2511 exploreP:0.8378\n",
      "Episode:65 meanR:27.3182 R:14.0000 rate:0.0280 gloss:0.7037 dlossA:0.6727 dlossQ:1.2179 exploreP:0.8367\n",
      "Episode:66 meanR:28.1940 R:86.0000 rate:0.1720 gloss:0.7234 dlossA:0.6818 dlossQ:1.2437 exploreP:0.8296\n",
      "Episode:67 meanR:27.9706 R:13.0000 rate:0.0260 gloss:0.7326 dlossA:0.6837 dlossQ:1.2657 exploreP:0.8285\n",
      "Episode:68 meanR:27.8551 R:20.0000 rate:0.0400 gloss:0.7345 dlossA:0.6907 dlossQ:1.2040 exploreP:0.8269\n",
      "Episode:69 meanR:28.5571 R:77.0000 rate:0.1540 gloss:0.7103 dlossA:0.6841 dlossQ:1.1994 exploreP:0.8206\n",
      "Episode:70 meanR:28.5070 R:25.0000 rate:0.0500 gloss:0.7036 dlossA:0.6757 dlossQ:1.2136 exploreP:0.8186\n",
      "Episode:71 meanR:28.3472 R:17.0000 rate:0.0340 gloss:0.7637 dlossA:0.6981 dlossQ:1.2459 exploreP:0.8172\n",
      "Episode:72 meanR:28.4247 R:34.0000 rate:0.0680 gloss:0.7233 dlossA:0.6801 dlossQ:1.2323 exploreP:0.8145\n",
      "Episode:73 meanR:28.4189 R:28.0000 rate:0.0560 gloss:0.7143 dlossA:0.6741 dlossQ:1.2421 exploreP:0.8122\n",
      "Episode:74 meanR:28.6400 R:45.0000 rate:0.0900 gloss:0.7222 dlossA:0.6887 dlossQ:1.1983 exploreP:0.8086\n",
      "Episode:75 meanR:28.6316 R:28.0000 rate:0.0560 gloss:0.6992 dlossA:0.6786 dlossQ:1.1981 exploreP:0.8064\n",
      "Episode:76 meanR:28.6104 R:27.0000 rate:0.0540 gloss:0.7399 dlossA:0.6917 dlossQ:1.2438 exploreP:0.8043\n",
      "Episode:77 meanR:28.4487 R:16.0000 rate:0.0320 gloss:0.7112 dlossA:0.6771 dlossQ:1.2012 exploreP:0.8030\n",
      "Episode:78 meanR:28.3165 R:18.0000 rate:0.0360 gloss:0.7552 dlossA:0.6999 dlossQ:1.2370 exploreP:0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:28.4250 R:37.0000 rate:0.0740 gloss:0.7233 dlossA:0.6832 dlossQ:1.2271 exploreP:0.7986\n",
      "Episode:80 meanR:28.4321 R:29.0000 rate:0.0580 gloss:0.7002 dlossA:0.6741 dlossQ:1.2103 exploreP:0.7964\n",
      "Episode:81 meanR:28.2561 R:14.0000 rate:0.0280 gloss:0.6811 dlossA:0.6677 dlossQ:1.1501 exploreP:0.7953\n",
      "Episode:82 meanR:28.6747 R:63.0000 rate:0.1260 gloss:0.7161 dlossA:0.6789 dlossQ:1.2242 exploreP:0.7903\n",
      "Episode:83 meanR:28.7500 R:35.0000 rate:0.0700 gloss:0.7207 dlossA:0.6829 dlossQ:1.2357 exploreP:0.7876\n",
      "Episode:84 meanR:28.6000 R:16.0000 rate:0.0320 gloss:0.6901 dlossA:0.6708 dlossQ:1.2019 exploreP:0.7864\n",
      "Episode:85 meanR:28.4419 R:15.0000 rate:0.0300 gloss:0.6833 dlossA:0.6681 dlossQ:1.1570 exploreP:0.7852\n",
      "Episode:86 meanR:29.3218 R:105.0000 rate:0.2100 gloss:0.7275 dlossA:0.6880 dlossQ:1.2274 exploreP:0.7771\n",
      "Episode:87 meanR:29.2386 R:22.0000 rate:0.0440 gloss:0.7206 dlossA:0.6835 dlossQ:1.2046 exploreP:0.7754\n",
      "Episode:88 meanR:29.1910 R:25.0000 rate:0.0500 gloss:0.7380 dlossA:0.6910 dlossQ:1.2285 exploreP:0.7735\n",
      "Episode:89 meanR:29.2111 R:31.0000 rate:0.0620 gloss:0.7097 dlossA:0.6827 dlossQ:1.2049 exploreP:0.7711\n",
      "Episode:90 meanR:29.1099 R:20.0000 rate:0.0400 gloss:0.7020 dlossA:0.6739 dlossQ:1.2097 exploreP:0.7696\n",
      "Episode:91 meanR:29.2174 R:39.0000 rate:0.0780 gloss:0.7079 dlossA:0.6743 dlossQ:1.2030 exploreP:0.7667\n",
      "Episode:92 meanR:29.1183 R:20.0000 rate:0.0400 gloss:0.7198 dlossA:0.6792 dlossQ:1.2550 exploreP:0.7651\n",
      "Episode:93 meanR:29.1702 R:34.0000 rate:0.0680 gloss:0.7313 dlossA:0.6840 dlossQ:1.2292 exploreP:0.7626\n",
      "Episode:94 meanR:29.3053 R:42.0000 rate:0.0840 gloss:0.7346 dlossA:0.6926 dlossQ:1.2148 exploreP:0.7594\n",
      "Episode:95 meanR:29.4167 R:40.0000 rate:0.0800 gloss:0.7020 dlossA:0.6761 dlossQ:1.2067 exploreP:0.7564\n",
      "Episode:96 meanR:29.3505 R:23.0000 rate:0.0460 gloss:0.7096 dlossA:0.6713 dlossQ:1.2211 exploreP:0.7547\n",
      "Episode:97 meanR:29.5714 R:51.0000 rate:0.1020 gloss:0.7179 dlossA:0.6802 dlossQ:1.2181 exploreP:0.7509\n",
      "Episode:98 meanR:29.4242 R:15.0000 rate:0.0300 gloss:0.7070 dlossA:0.6769 dlossQ:1.2054 exploreP:0.7498\n",
      "Episode:99 meanR:29.5000 R:37.0000 rate:0.0740 gloss:0.7058 dlossA:0.6772 dlossQ:1.2087 exploreP:0.7471\n",
      "Episode:100 meanR:29.8000 R:51.0000 rate:0.1020 gloss:0.7218 dlossA:0.6860 dlossQ:1.2024 exploreP:0.7433\n",
      "Episode:101 meanR:30.2600 R:65.0000 rate:0.1300 gloss:0.7327 dlossA:0.6893 dlossQ:1.2152 exploreP:0.7386\n",
      "Episode:102 meanR:30.2500 R:17.0000 rate:0.0340 gloss:0.7280 dlossA:0.6883 dlossQ:1.2206 exploreP:0.7373\n",
      "Episode:103 meanR:30.4500 R:66.0000 rate:0.1320 gloss:0.7274 dlossA:0.6870 dlossQ:1.2273 exploreP:0.7326\n",
      "Episode:104 meanR:30.5400 R:25.0000 rate:0.0500 gloss:0.7246 dlossA:0.6831 dlossQ:1.2596 exploreP:0.7308\n",
      "Episode:105 meanR:30.7100 R:44.0000 rate:0.0880 gloss:0.7030 dlossA:0.6777 dlossQ:1.2040 exploreP:0.7276\n",
      "Episode:106 meanR:31.5200 R:97.0000 rate:0.1940 gloss:0.7058 dlossA:0.6799 dlossQ:1.1929 exploreP:0.7207\n",
      "Episode:107 meanR:31.7700 R:44.0000 rate:0.0880 gloss:0.7170 dlossA:0.6788 dlossQ:1.2099 exploreP:0.7175\n",
      "Episode:108 meanR:32.2800 R:61.0000 rate:0.1220 gloss:0.7339 dlossA:0.6920 dlossQ:1.2358 exploreP:0.7132\n",
      "Episode:109 meanR:32.3800 R:23.0000 rate:0.0460 gloss:0.6963 dlossA:0.6744 dlossQ:1.1891 exploreP:0.7116\n",
      "Episode:110 meanR:32.1700 R:23.0000 rate:0.0460 gloss:0.7236 dlossA:0.6874 dlossQ:1.2129 exploreP:0.7100\n",
      "Episode:111 meanR:32.2800 R:33.0000 rate:0.0660 gloss:0.7086 dlossA:0.6791 dlossQ:1.2044 exploreP:0.7077\n",
      "Episode:112 meanR:32.2900 R:17.0000 rate:0.0340 gloss:0.7645 dlossA:0.7081 dlossQ:1.2515 exploreP:0.7065\n",
      "Episode:113 meanR:33.3300 R:116.0000 rate:0.2320 gloss:0.7129 dlossA:0.6843 dlossQ:1.2023 exploreP:0.6985\n",
      "Episode:114 meanR:33.9900 R:86.0000 rate:0.1720 gloss:0.7236 dlossA:0.6908 dlossQ:1.2113 exploreP:0.6926\n",
      "Episode:115 meanR:34.2900 R:46.0000 rate:0.0920 gloss:0.6896 dlossA:0.6715 dlossQ:1.1835 exploreP:0.6895\n",
      "Episode:116 meanR:34.8100 R:66.0000 rate:0.1320 gloss:0.7211 dlossA:0.6849 dlossQ:1.2226 exploreP:0.6850\n",
      "Episode:117 meanR:35.6400 R:103.0000 rate:0.2060 gloss:0.7178 dlossA:0.6838 dlossQ:1.2139 exploreP:0.6781\n",
      "Episode:118 meanR:36.0600 R:61.0000 rate:0.1220 gloss:0.7124 dlossA:0.6909 dlossQ:1.2016 exploreP:0.6740\n",
      "Episode:119 meanR:36.6400 R:70.0000 rate:0.1400 gloss:0.7192 dlossA:0.6852 dlossQ:1.1958 exploreP:0.6694\n",
      "Episode:120 meanR:36.8700 R:37.0000 rate:0.0740 gloss:0.7229 dlossA:0.6847 dlossQ:1.2203 exploreP:0.6669\n",
      "Episode:121 meanR:36.9800 R:21.0000 rate:0.0420 gloss:0.6920 dlossA:0.6737 dlossQ:1.1572 exploreP:0.6656\n",
      "Episode:122 meanR:37.3300 R:51.0000 rate:0.1020 gloss:0.7140 dlossA:0.6809 dlossQ:1.1944 exploreP:0.6622\n",
      "Episode:123 meanR:37.4600 R:38.0000 rate:0.0760 gloss:0.7026 dlossA:0.6872 dlossQ:1.1699 exploreP:0.6598\n",
      "Episode:124 meanR:37.3000 R:15.0000 rate:0.0300 gloss:0.7262 dlossA:0.6913 dlossQ:1.2128 exploreP:0.6588\n",
      "Episode:125 meanR:37.7000 R:59.0000 rate:0.1180 gloss:0.7019 dlossA:0.6780 dlossQ:1.1998 exploreP:0.6550\n",
      "Episode:126 meanR:37.6100 R:20.0000 rate:0.0400 gloss:0.7055 dlossA:0.6808 dlossQ:1.1729 exploreP:0.6537\n",
      "Episode:127 meanR:37.9600 R:55.0000 rate:0.1100 gloss:0.7271 dlossA:0.6905 dlossQ:1.2003 exploreP:0.6502\n",
      "Episode:128 meanR:38.0300 R:48.0000 rate:0.0960 gloss:0.6943 dlossA:0.6737 dlossQ:1.1824 exploreP:0.6471\n",
      "Episode:129 meanR:38.0900 R:25.0000 rate:0.0500 gloss:0.7531 dlossA:0.7024 dlossQ:1.2029 exploreP:0.6455\n",
      "Episode:130 meanR:38.3500 R:52.0000 rate:0.1040 gloss:0.7100 dlossA:0.6795 dlossQ:1.2381 exploreP:0.6422\n",
      "Episode:131 meanR:37.8900 R:45.0000 rate:0.0900 gloss:0.7270 dlossA:0.6884 dlossQ:1.2303 exploreP:0.6394\n",
      "Episode:132 meanR:38.6100 R:83.0000 rate:0.1660 gloss:0.7320 dlossA:0.6942 dlossQ:1.2241 exploreP:0.6342\n",
      "Episode:133 meanR:39.4100 R:94.0000 rate:0.1880 gloss:0.7109 dlossA:0.6836 dlossQ:1.1916 exploreP:0.6283\n",
      "Episode:134 meanR:39.5000 R:64.0000 rate:0.1280 gloss:0.7026 dlossA:0.6786 dlossQ:1.1821 exploreP:0.6244\n",
      "Episode:135 meanR:39.8600 R:53.0000 rate:0.1060 gloss:0.7295 dlossA:0.6890 dlossQ:1.1903 exploreP:0.6211\n",
      "Episode:136 meanR:40.4100 R:101.0000 rate:0.2020 gloss:0.7291 dlossA:0.6968 dlossQ:1.1961 exploreP:0.6150\n",
      "Episode:137 meanR:40.5700 R:42.0000 rate:0.0840 gloss:0.7608 dlossA:0.7060 dlossQ:1.2289 exploreP:0.6125\n",
      "Episode:138 meanR:40.9500 R:61.0000 rate:0.1220 gloss:0.7282 dlossA:0.6914 dlossQ:1.2239 exploreP:0.6088\n",
      "Episode:139 meanR:40.9100 R:27.0000 rate:0.0540 gloss:0.6813 dlossA:0.6712 dlossQ:1.1533 exploreP:0.6072\n",
      "Episode:140 meanR:41.8600 R:143.0000 rate:0.2860 gloss:0.7055 dlossA:0.6819 dlossQ:1.1705 exploreP:0.5987\n",
      "Episode:141 meanR:41.7000 R:26.0000 rate:0.0520 gloss:0.7112 dlossA:0.6802 dlossQ:1.1911 exploreP:0.5972\n",
      "Episode:142 meanR:41.7400 R:23.0000 rate:0.0460 gloss:0.6931 dlossA:0.6871 dlossQ:1.1336 exploreP:0.5958\n",
      "Episode:143 meanR:42.5500 R:104.0000 rate:0.2080 gloss:0.6973 dlossA:0.6765 dlossQ:1.1863 exploreP:0.5898\n",
      "Episode:144 meanR:42.3500 R:11.0000 rate:0.0220 gloss:0.6897 dlossA:0.6721 dlossQ:1.1652 exploreP:0.5891\n",
      "Episode:145 meanR:43.4200 R:123.0000 rate:0.2460 gloss:0.7250 dlossA:0.6891 dlossQ:1.2093 exploreP:0.5820\n",
      "Episode:146 meanR:43.6400 R:36.0000 rate:0.0720 gloss:0.7217 dlossA:0.6886 dlossQ:1.1965 exploreP:0.5800\n",
      "Episode:147 meanR:44.1900 R:77.0000 rate:0.1540 gloss:0.7095 dlossA:0.6843 dlossQ:1.1876 exploreP:0.5756\n",
      "Episode:148 meanR:44.1100 R:23.0000 rate:0.0460 gloss:0.7258 dlossA:0.6889 dlossQ:1.2033 exploreP:0.5743\n",
      "Episode:149 meanR:44.0500 R:54.0000 rate:0.1080 gloss:0.7439 dlossA:0.6941 dlossQ:1.2323 exploreP:0.5713\n",
      "Episode:150 meanR:45.1400 R:126.0000 rate:0.2520 gloss:0.7193 dlossA:0.6869 dlossQ:1.1991 exploreP:0.5642\n",
      "Episode:151 meanR:44.9000 R:62.0000 rate:0.1240 gloss:0.7398 dlossA:0.6969 dlossQ:1.2150 exploreP:0.5608\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
