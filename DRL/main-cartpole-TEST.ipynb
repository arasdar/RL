{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.00523918 -0.00796275  0.00417821  0.02770183] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00539844 -0.20314437  0.00473225  0.3217001 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00946132 -0.00809012  0.01116625  0.03051328] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00962313  0.18686993  0.01177651 -0.25862576] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00588573 -0.00841814  0.006604    0.03774826] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00605409  0.18660849  0.00735896 -0.25284374] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00232192  0.38162459  0.00230209 -0.54319646] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00531057  0.57671411 -0.00856184 -0.83515316] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01684485  0.77195197 -0.0252649  -1.13051637] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03228389  0.96739548 -0.04787523 -1.43101516] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0516318   1.16307458 -0.07649553 -1.73826744] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07489329  1.35898032 -0.11126088 -2.05373446] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1020729   1.16515882 -0.15233557 -1.7974431 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12537608  0.97203503 -0.18828443 -1.5557258 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.02117269 -0.02973982 -0.04549135 -0.00332643] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02057789 -0.22418083 -0.04555788  0.27466351] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01609427 -0.41862417 -0.04006461  0.55263649] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00772179 -0.61316124 -0.02901188  0.8324321 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00454143 -0.80787498 -0.01236323  1.1158515 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02069893 -1.00283248  0.0099538   1.4046307 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04075558 -0.80783552  0.03804641  1.11507613] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05691229 -1.00343574  0.06034793  1.41944707] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07698101 -0.80911033  0.08873687  1.14622111] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09316322 -0.61525214  0.1116613   0.88263234] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10546826 -0.81169903  0.12931394  1.20822782] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12170224 -1.00823214  0.1534785   1.53847851] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14186688 -0.81525388  0.18424807  1.29735947] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00354984  0.00737555 -0.04333106  0.01804875] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00340233  0.20309127 -0.04297008 -0.28798461] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0006595   0.39879882 -0.04872978 -0.59390427] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00863547  0.2043916  -0.06060786 -0.3169605 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0127233   0.4003221  -0.06694707 -0.62812443] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02072975  0.20619522 -0.07950956 -0.35725415] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02485365  0.40235222 -0.08665464 -0.67391128] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03290069  0.20853469 -0.10013287 -0.40972043] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03707139  0.40492314 -0.10832728 -0.7322174 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04516985  0.60136197 -0.12297163 -1.05693443] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05719709  0.40806501 -0.14411031 -0.80524111] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06535839  0.60483742 -0.16021514 -1.13956207] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07745514  0.80164936 -0.18300638 -1.47790111] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04966253  0.02695497  0.01936459 -0.04982201] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05020163 -0.16843922  0.01836815  0.24890716] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04683284  0.02641566  0.02334629 -0.03792593] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04736116 -0.16903316  0.02258777  0.2620306 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04398049  0.0257592   0.02782838 -0.02344317] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04449568  0.22047124  0.02735952 -0.30721766] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0489051   0.41519287  0.02121517 -0.59114809] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05720896  0.61001147  0.00939221 -0.87707345] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06940919  0.80500452 -0.00814926 -1.16678885] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08550928  0.60998956 -0.03148504 -0.87667198] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09770907  0.80552498 -0.04901848 -1.17908481] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11381957  0.6110726  -0.07260018 -0.90216226] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12604102  0.80709902 -0.09064342 -1.21675303] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.142183    1.00326547 -0.11497848 -1.53640741] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16224831  1.19956862 -0.14570663 -1.86264717] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.18623968  1.3959564  -0.18295957 -2.19679364] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 1.28658498e-02  3.41215962e-02  9.41142829e-05 -2.57337049e-02] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01354828  0.2292422  -0.00042056 -0.31838694] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01813313  0.03412624 -0.0067883  -0.02583667] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01881565 -0.16089771 -0.00730503  0.26469677] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0155977   0.03432774 -0.0020111  -0.03028126] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01628425  0.22947848 -0.00261672 -0.32359803] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02087382  0.03439388 -0.00908868 -0.03174145] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0215617   0.22964498 -0.00972351 -0.32727804] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0261546   0.03466281 -0.01626907 -0.03767727] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02684785  0.23001423 -0.01702262 -0.33544853] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03144814  0.03513863 -0.02373159 -0.04818188] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03215091 -0.15963514 -0.02469523  0.23692007] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02895821 -0.35439573 -0.01995682  0.52171222] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02187029 -0.54923117 -0.00952258  0.80804027] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01088567 -0.74422133  0.00663823  1.09771266] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00399876 -0.54918741  0.02859248  0.80711983] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0149825  -0.35446876  0.04473488  0.5235663 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02207188 -0.55019082  0.0552062   0.83000334] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0330757  -0.74602219  0.07180627  1.13952526] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04799614 -0.94200571  0.09459677  1.45383588] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06683625 -0.7481641   0.12367349  1.19214344] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08179954 -0.554842    0.14751636  0.94064251] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09289638 -0.75161107  0.16632921  1.27580319] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1079286  -0.55895427  0.19184527  1.03948186] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04431358  0.02917552 -0.00412171 -0.02316081] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04373007 -0.16588708 -0.00458493  0.26821882] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04704781  0.02930001  0.00077945 -0.02590669] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-4.64618103e-02 -1.65833113e-01  2.61317301e-04  2.67022061e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04977847  0.02928511  0.00560176 -0.02557843] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04919277  0.22432628  0.00509019 -0.31648869] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04470624  0.41937536 -0.00123958 -0.60756201] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03631874  0.61451462 -0.01339082 -0.90063511] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02402845  0.80981543 -0.03140353 -1.19749685] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00783214  1.00532943 -0.05535346 -1.49985437] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01227445  0.81092183 -0.08535055 -1.22495521] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02849289  1.00703282 -0.10984966 -1.5431135 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04863354  1.20329026 -0.14071193 -1.86795616] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07269935  1.00996061 -0.17807105 -1.6220608 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03241554 -0.03880095  0.03702348 -0.00951378] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03319156  0.15577101  0.03683321 -0.29028929] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03007614 -0.03985626  0.03102742  0.01377911] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03087326 -0.23540913  0.031303    0.31608782] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03558144 -0.04074672  0.03762476  0.03343883] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03639638  0.15381604  0.03829354 -0.2471397 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03332006 -0.04183129  0.03335074  0.05737171] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03415668  0.15279699  0.03449818 -0.22460506] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03110074 -0.0428006   0.03000608  0.0787572 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03195676 -0.23833957  0.03158122  0.38075408] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03672355 -0.04367997  0.0391963   0.09819362] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03759715 -0.23934113  0.04116017  0.40298069] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04238397 -0.43502194  0.04921979  0.70835099] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05108441 -0.63078988  0.06338681  1.01611205] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06370021 -0.43656777  0.08370905  0.7439872 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07243156 -0.24269475  0.09858879  0.47877769] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07728546 -0.43906025  0.10816435  0.8008338 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08606666 -0.24557491  0.12418102  0.54404024] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09097816 -0.44220305  0.13506183  0.87312669] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09982222 -0.63887745  0.15252436  1.20503918] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11259977 -0.83560552  0.17662514  1.54137398] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12931188 -1.03235644  0.20745262  1.88356709] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00979322  0.02705709 -0.03136236  0.00106666] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00925208  0.22261446 -0.03134103 -0.30134416] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00479979  0.0279529  -0.03736791 -0.01870797] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00424073  0.22359027 -0.03774207 -0.32294278] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00023107  0.02902551 -0.04420093 -0.04239701] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00081158  0.22475251 -0.04504887 -0.34869159] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00530663  0.42048526 -0.0520227  -0.65523275] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01371634  0.61629143 -0.06512736 -0.96383243] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02604217  0.81222506 -0.08440401 -1.2762437 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04228667  1.00831581 -0.10992888 -1.59411805] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06245299  0.81465628 -0.14181124 -1.33763623] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07874611  1.01125078 -0.16856397 -1.67112065] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09897113  1.20788262 -0.20198638 -2.01121147] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00317539 -0.04334449 -0.03610739 -0.03575596] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00404228  0.15227615 -0.0368225  -0.33960892] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00099675 -0.04230304 -0.04361468 -0.05876136] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00184281  0.15341625 -0.04478991 -0.36487975] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00122551  0.34914516 -0.05208751 -0.67134251] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00820841  0.15478453 -0.06551436 -0.39550377] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01130411 -0.03934964 -0.07342443 -0.12417467] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01051711  0.15674322 -0.07590792 -0.43908884] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01365198 -0.03722692 -0.0846897  -0.17126698] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01290744 -0.23104104 -0.08811504  0.09354324] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00828662 -0.42479679 -0.08624418  0.357177  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-2.09318002e-04 -6.18593502e-01 -7.91006357e-02  6.21467271e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01258119 -0.42246124 -0.06667129  0.30495744] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02103041 -0.22645566 -0.06057214 -0.00798458] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02555953 -0.42065897 -0.06073183  0.26498884] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03397271 -0.61486389 -0.05543206  0.53791497] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04626998 -0.41900822 -0.04467376  0.22829403] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05465015 -0.61346422 -0.04010788  0.50655731] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06691943 -0.41780074 -0.02997673  0.20150956] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07527545 -0.22226318 -0.02594654 -0.10047685] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07972071 -0.02677916 -0.02795608 -0.4012316 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08025629  0.16872794 -0.03598071 -0.70259568] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07688173 -0.02587734 -0.05003262 -0.42145274] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07739928 -0.22025604 -0.05846168 -0.14495301] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0818044  -0.41449415 -0.06136074  0.12872847] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09009429 -0.21854931 -0.05878617 -0.18266439] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09446527 -0.41278298 -0.06243945  0.09090965] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10272093 -0.21682421 -0.06062126 -0.22080097] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10705741 -0.02089049 -0.06503728 -0.53197342] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10747522  0.17508305 -0.07567675 -0.84441973] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10397356 -0.01892918 -0.09256514 -0.57646205] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10435215  0.17736005 -0.10409439 -0.89681069] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10080495 -0.01620841 -0.1220306  -0.63857817] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.10112911 -0.20943652 -0.13480216 -0.38667945] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10531784 -0.01268427 -0.14253575 -0.71864231] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10557153 -0.20557639 -0.1569086  -0.47400208] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10968306 -0.00862704 -0.16638864 -0.8117389 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1098556   0.18833557 -0.18262342 -1.15179539] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10608889 -0.00399604 -0.20565933 -0.92148885] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00965944 -0.03722796  0.0228503   0.01641958] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00891488  0.15755896  0.02317869 -0.26896713] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01206606 -0.03788597  0.01779935  0.03093545] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01130834  0.15697627  0.01841806 -0.25607888] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01444787 -0.03840374  0.01329648  0.04235603] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01367979  0.15652505  0.0141436  -0.24610225] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01681029 -0.03879602  0.00922156  0.05100813] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01603437  0.1561925   0.01024172 -0.23875111] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01915822 -0.03907426  0.0054667   0.05714467] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01837674  0.15596889  0.00660959 -0.23380847] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02149611 -0.03924688  0.00193342  0.06095198] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02071118 -0.2343965   0.00315246  0.35424428] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01602325 -0.42956313  0.01023734  0.6479196 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00743198 -0.23458529  0.02319574  0.35847789] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00274028 -0.43002918  0.03036529  0.6583838 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00586031 -0.23534274  0.04353297  0.37541478] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01056716 -0.43105512  0.05104127  0.68149988] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01918826 -0.23667781  0.06467126  0.40531324] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02392182 -0.43265441  0.07277753  0.71766372] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03257491 -0.62870403  0.0871308   1.03233742] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04514899 -0.82487008  0.10777755  1.35105432] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06164639 -1.02116802  0.13479864  1.67541802] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08206975 -1.21757225  0.168307    2.00686271] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10642119 -1.02455684  0.20844425  1.7706796 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.0258226   0.03568439  0.02499132 -0.01516958] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02510891 -0.15978689  0.02468793  0.28529246] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02830465  0.03497442  0.03039378  0.00049704] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02760516 -0.16056994  0.03040372  0.3026125 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03081656 -0.35611172  0.03645597  0.60472693] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03793879 -0.55172402  0.04855051  0.90866611] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04897327 -0.35729168  0.06672383  0.63162949] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05611911 -0.55327801  0.07935642  0.94455707] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06718467 -0.7493741   0.09824756  1.26108183] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08217215 -0.55563632  0.1234692   1.00071492] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09328487 -0.75217262  0.1434835   1.32948291] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10832833 -0.55912255  0.17007316  1.08492307] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11951078 -0.75602973  0.19177162  1.42578329] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.0012344   0.04086402 -0.00643906 -0.03974061] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00205168  0.23607771 -0.00723388 -0.33444815] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00677323  0.43130186 -0.01392284 -0.62940346] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01539927  0.23637694 -0.02651091 -0.34113761] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02012681  0.04164202 -0.03333366 -0.05693116] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02095965 -0.15298652 -0.03447228  0.22505119] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01789992  0.04261072 -0.02997126 -0.07830321] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01875214  0.23814922 -0.03153732 -0.38028946] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02351512  0.04348897 -0.03914311 -0.09771472] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0243849  -0.15105075 -0.04109741  0.1823662 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02136388 -0.34556127 -0.03745008  0.46180639] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01445266 -0.54013447 -0.02821396  0.74245379] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00364997 -0.73485584 -0.01336488  1.0261258 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01104715 -0.53955852  0.00715764  0.72927682] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02183832 -0.73477867  0.02174317  1.02420388] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03653389 -0.53995294  0.04222725  0.73842634] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04733295 -0.7356318   0.05699578  1.0440941 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06204559 -0.9314623   0.07787766  1.35411055] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08067483 -1.12747064  0.10495987  1.67010482] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10322424 -0.93371345  0.13836197  1.41187006] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12189851 -0.74055122  0.16659937  1.16544446] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13670954 -0.5479424   0.18990826  0.92928536] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14766839 -0.74504993  0.20849396  1.27512916] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03135535  0.04762952  0.03667306 -0.02450281] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03230794  0.24220688  0.03618301 -0.30539314] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03715207  0.43679503  0.03007514 -0.58644883] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04588797  0.63148313  0.01834617 -0.86950815] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 5.85176371e-02  8.26350759e-01  9.56004527e-04 -1.15636694e+00] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07504465  0.63121636 -0.02217133 -0.86338441] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08766898  0.43640316 -0.03943902 -0.57775426] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09639704  0.24185552 -0.05099411 -0.2977518 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10123415  0.4376659  -0.05694914 -0.60607134] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10998747  0.63353599 -0.06907057 -0.91613417] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12265819  0.43941261 -0.08739325 -0.64593401] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13144644  0.63563663 -0.10031193 -0.96480767] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14415918  0.44199482 -0.11960809 -0.70524739] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15299907  0.24871537 -0.13371303 -0.45248054] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15797338  0.44544971 -0.14276265 -0.78414249] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16688237  0.25254785 -0.1584455  -0.53956316] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.17193333  0.44950072 -0.16923676 -0.87768014] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.18092334  0.64646825 -0.18679036 -1.21842895] 1 1.0 True {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [0.01919626 0.04100982 0.03189605 0.00080401] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02001646  0.23566016  0.03191213 -0.28164714] 0 1.0 False {}\n",
      "state, action, reward, done, info: [0.02472966 0.04009789 0.02627919 0.02092748] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02553162 -0.15539088  0.02669774  0.32178459] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0224238  -0.35088264  0.03313343  0.62276594] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01540615 -0.15623862  0.04558875  0.3406996 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [0.01228138 0.03820604 0.05240274 0.0627341 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0130455   0.23253902  0.05365742 -0.21296588] 0 1.0 False {}\n",
      "state, action, reward, done, info: [0.01769628 0.03669261 0.04939811 0.09614865] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01843013  0.23107304  0.05132108 -0.18054918] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02305159  0.42542447  0.0477101  -0.45661072] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03156008  0.61984058  0.03857788 -0.73388158] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04395689  0.8144089   0.02390025 -1.01417784] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06024507  0.61897646  0.00361669 -0.7140869 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0726246   0.42380463 -0.01066505 -0.42026776] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08110069  0.61907606 -0.0190704  -0.71629371] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09348221  0.8144567  -0.03339628 -1.01491774] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10977135  0.61979567 -0.05369463 -0.73290569] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12216726  0.42545513 -0.06835274 -0.45759365] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13067636  0.2313627  -0.07750462 -0.18721492] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13530362  0.42750305 -0.08124892 -0.50330578] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14385368  0.62367051 -0.09131503 -0.82044717] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15632709  0.42990889 -0.10772397 -0.55782497] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16492527  0.62636502 -0.11888047 -0.88241064] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.17745257  0.82288352 -0.13652869 -1.20997644] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.19391024  1.01947837 -0.16072822 -1.54213943] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.2142998   1.21612606 -0.191571   -1.88035998] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03318484 -0.03540944  0.01224405  0.03098174] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03389303 -0.23070482  0.01286368  0.32750251] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03850712 -0.42600752  0.01941373  0.62421415] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04702727 -0.62139506  0.03189802  0.9229475 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05945517 -0.42671825  0.05035697  0.64045725] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06798954 -0.62250473  0.06316611  0.94856339] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08043963 -0.42828748  0.08213738  0.676377  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08900538 -0.62444887  0.09566492  0.99374834] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10149436 -0.43072784  0.11553989  0.73257805] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11010892 -0.23737586  0.13019145  0.47837698] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11485643 -0.43407236  0.13975899  0.80909304] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12353788 -0.63080435  0.15594085  1.1422692 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13615397 -0.82758163  0.17878623  1.47951457] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1527056  -0.63503523  0.20837652  1.24758224] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00706296  0.00215664 -0.00387153  0.03173116] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00701982 -0.19290958 -0.00323691  0.32319007] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01087801  0.00225831  0.0032269   0.02948812] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01083285  0.19733384  0.00381666 -0.26217495] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00688617  0.3924011  -0.00142684 -0.55365161] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00096185  0.58754306 -0.01249987 -0.84678374] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01271271  0.39259384 -0.02943555 -0.55805763] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02056459  0.58811637 -0.0405967  -0.85986716] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03232692  0.39357016 -0.05779404 -0.58022045] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04019832  0.19930367 -0.06939845 -0.30628923] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04418439  0.00523574 -0.07552424 -0.03627481] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04428911 -0.1887265  -0.07624973  0.23165615] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04051458  0.00739749 -0.07161661 -0.08407075] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04066253  0.20346907 -0.07329803 -0.3984615 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04473191  0.39955017 -0.08126726 -0.71332374] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05272291  0.20564179 -0.09553373 -0.4472874 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05683575  0.40197618 -0.10447948 -0.7684903 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06487527  0.20843567 -0.11984928 -0.51042373] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06904399  0.01518787 -0.13005776 -0.25778276] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06934774  0.21190357 -0.13521341 -0.58849351] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07358581  0.408634   -0.14698328 -0.92052743] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08175849  0.60540379 -0.16539383 -1.25555723] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09386657  0.80221114 -0.19050498 -1.59514409] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00063186 -0.03396552  0.03342348  0.00138923] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-4.74550696e-05  1.60661541e-01  3.34512649e-02 -2.80563641e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00316578 -0.03492122  0.02783999  0.02247921] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00246735 -0.23043113  0.02828958  0.32381433] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00214127 -0.03572318  0.03476586  0.0401853 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00285573  0.15888343  0.03556957 -0.24132912] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 3.21933876e-04  3.53479706e-01  3.07429864e-02 -5.22583652e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00739153  0.54815577  0.02029131 -0.80542274] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01835464  0.74299376  0.00418286 -1.09165434] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03321452  0.93806034 -0.01765023 -1.38302189] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05197573  1.13339798 -0.04531067 -1.68117161] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07464368  1.3290146  -0.0789341  -1.98761216] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10122398  1.52487087 -0.11868634 -2.30366434] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13172139  1.7208649  -0.16475963 -2.63039909] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04104375  0.02995598  0.01220615 -0.00153624] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04044463 -0.16533888  0.01217543  0.29497275] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0437514  -0.36063228  0.01807488  0.59147062] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05096405 -0.55600256  0.02990429  0.88979189] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0620841  -0.36129886  0.04770013  0.60665748] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06931008 -0.16687519  0.05983328  0.3293722 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07264758  0.02734623  0.06642072  0.0561419 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07210066 -0.16866211  0.06754356  0.36901944] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0754739   0.02543838  0.07492395  0.09837529] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07496513  0.21941101  0.07689146 -0.16976033] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07057691  0.41335299  0.07349625 -0.43722993] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06230985  0.60736184  0.06475165 -0.70586912] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05016262  0.80152972  0.05063427 -0.97748689] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03413202  0.99593749  0.03108453 -1.25384468] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01421327  1.19064783  0.00600764 -1.53663173] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00959969  0.99545408 -0.024725   -1.24208015] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.02950877  0.80065808 -0.0495666  -0.95724386] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04552193  0.60623645 -0.06871148 -0.68053621] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05764666  0.41213275 -0.0823222  -0.41025299] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06588931  0.60831943 -0.09052726 -0.72771181] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0780557   0.80456853 -0.1050815  -1.04745873] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09414707  0.61098609 -0.12603067 -0.7895239 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10636679  0.80759269 -0.14182115 -1.11904977] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12251865  0.61458696 -0.16420214 -0.87400356] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13481039  0.42203246 -0.18168222 -0.63711566] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14325104  0.61916059 -0.19442453 -0.98106229] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00238646 -0.02564118  0.00954293  0.03216312] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00187363  0.16934263  0.01018619 -0.25749369] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00526049 -0.02592325  0.00503632  0.03838463] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00474202  0.16912612  0.00580401 -0.25270505] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00812454 -0.02607823  0.00074991  0.04180291] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00760298 -0.22121093  0.00158597  0.33472234] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00317876 -0.41635541  0.00828041  0.62790498] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00514835 -0.61159195  0.02083851  0.92318416] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01738019 -0.41675763  0.0393022   0.63712221] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02571534 -0.22220516  0.05204464  0.35707074] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03015944 -0.02786028  0.05918606  0.08124235] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03071665 -0.22377852  0.0608109   0.39199568] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03519222 -0.02956991  0.06865082  0.11908863] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03578362 -0.22560488  0.07103259  0.43261592] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04029572 -0.03155678  0.07968491  0.1631438 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04092685 -0.22772368  0.08294778  0.47986303] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04548132 -0.42391262  0.09254504  0.79749332] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05395958 -0.62017426  0.10849491  1.11779542] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06636306 -0.81653954  0.13085082  1.44244643] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08269385 -0.62324876  0.15969975  1.1933493 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09515883 -0.43051408  0.18356673  0.95467849] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10376911 -0.23827238  0.2026603   0.72482679] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03139492  0.04653364 -0.00473956 -0.03625927] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03232559  0.24172323 -0.00546475 -0.3304338 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03716006  0.0466795  -0.01207342 -0.03947919] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03809365 -0.14826726 -0.01286301  0.24937013] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0351283   0.047036   -0.0078756  -0.04734215] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03606902  0.24226999 -0.00882245 -0.34249945] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04091442  0.43751634 -0.01567244 -0.63795135] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04966475  0.2426164  -0.02843146 -0.35024485] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05451708  0.43813092 -0.03543636 -0.65175582] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06327969  0.63372801 -0.04847148 -0.95538337] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07595425  0.43929037 -0.06757914 -0.67831451] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08474006  0.63528285 -0.08114543 -0.99148532] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09744572  0.44133504 -0.10097514 -0.72535146] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10627242  0.63769756 -0.11548217 -1.0480312 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11902637  0.83414669 -0.13644279 -1.37461852] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1357093   0.6409681  -0.16393516 -1.12753353] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14852867  0.44832833 -0.18648584 -0.8904294 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15749523  0.64542401 -0.20429442 -1.23545662] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00372144  0.02044929 -0.00389727  0.0301014 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00413043 -0.17461655 -0.00329524  0.32155215] 1 1.0 False {}\n",
      "state, action, reward, done, info: [0.0006381  0.02055217 0.00313581 0.02783186] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00104914  0.21562901  0.00369244 -0.26386004] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00536172  0.41069806 -0.00158476 -0.55537604] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01357568  0.60584223 -0.01269228 -0.84855784] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02569253  0.80113497 -0.02966344 -1.14520479] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04171523  0.99663156 -0.05256753 -1.44704043] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06164786  0.80219405 -0.08150834 -1.1712348 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07769174  0.608221   -0.10493304 -0.90517812] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08985616  0.80459546 -0.1230366  -1.22891333] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10594807  0.61125254 -0.14761486 -0.97717286] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11817312  0.80801242 -0.16715832 -1.31234428] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13433337  0.61535404 -0.19340521 -1.07630075] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00967622 -0.02348997  0.01715392  0.01779955] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00920643  0.17138182  0.01750991 -0.26942213] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01263406  0.36624958  0.01212147 -0.55653131] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01995905  0.56119927  0.00099084 -0.84537074] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03118304  0.75630769 -0.01591657 -1.13774192] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04630919  0.95163415 -0.03867141 -1.43537377] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06534188  0.75700982 -0.06737889 -1.15502211] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08048207  0.95294255 -0.09047933 -1.46804872] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09954092  1.1490479  -0.11984031 -1.78756815] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12252188  1.34529418 -0.15559167 -2.11497506] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14942776  1.15203112 -0.19789117 -1.874142  ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00284725  0.04936293 -0.04883389  0.04158829] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00185999  0.2451499  -0.04800213 -0.26609351] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00304301  0.44092291 -0.053324   -0.57352198] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01186147  0.24658755 -0.06479443 -0.2981031 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01679322  0.44257041 -0.0707565  -0.61049675] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02564463  0.63860633 -0.08296643 -0.92459991] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03841675  0.83474501 -0.10145843 -1.24216008] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05511165  0.64106108 -0.12630163 -0.98290584] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06793287  0.44783688 -0.14595975 -0.73241226] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07688961  0.25500096 -0.16060799 -0.48899404] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08198963  0.45198095 -0.17038787 -0.82767741] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09102925  0.25954711 -0.18694142 -0.59305857] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09622019  0.06746585 -0.19880259 -0.36459777] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09756951 -0.12435766 -0.20609455 -0.14059352] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09508236 -0.31602355 -0.20890642  0.08065738] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08876189 -0.50763419 -0.20729327  0.30085169] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0786092  -0.69929183 -0.20127624  0.52167483] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06462337 -0.5019907  -0.19084274  0.17292127] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05458355 -0.30472265 -0.18738432 -0.17337642] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0484891  -0.10748212 -0.19085185 -0.51882729] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04633946 -0.29947725 -0.20122839 -0.29183411] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04034991 -0.49124661 -0.20706507 -0.0687471 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03052498 -0.29385064 -0.20844002 -0.41895909] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.0292378  -0.0020652   0.02115271 -0.00419824] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0292791   0.19274711  0.02106874 -0.29013289] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02542416 -0.00266883  0.01526608  0.00911972] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02547754  0.19223089  0.01544848 -0.27870775] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02163292 -0.003108    0.00987432  0.01880729] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02169508  0.19187096  0.01025047 -0.27074389] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01785766 -0.00339575  0.00483559  0.02515438] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01792557  0.19165652  0.00533868 -0.26599895] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-1.40924445e-02  3.86701873e-01  1.86989053e-05 -5.56993245e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00635841  0.58182356 -0.01112117 -0.84967028] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00527806  0.38685502 -0.02811457 -0.56050511] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01301516  0.58236005 -0.03932467 -0.86191135] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02466237  0.387795   -0.0565629  -0.58184788] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03241827  0.19350927 -0.06819986 -0.30750599] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03628845  0.38953338 -0.07434998 -0.62089383] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04407912  0.19552418 -0.08676785 -0.35252277] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0479896   0.39176594 -0.09381831 -0.67125577] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05582492  0.1980648  -0.10724343 -0.40952396] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05978622  0.00461377 -0.1154339  -0.15248367] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05987849  0.20118313 -0.11848358 -0.47923767] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06390216  0.00791566 -0.12806833 -0.22612168] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06406047 -0.18516554 -0.13259077  0.023579  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06035716 -0.37816141 -0.13211919  0.27166427] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05279393 -0.57117495 -0.1266859   0.51993125] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04137043 -0.76430715 -0.11628727  0.77016182] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02608429 -0.95765305 -0.10088404  1.02411051] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00693123 -1.15129731 -0.08040183  1.2834904 ] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.01609472 -1.34530865 -0.05473202  1.54995461] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04300089 -1.53973288 -0.02373293  1.82507106] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07379555 -1.34435585  0.01276849  1.52511124] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10068267 -1.5396296   0.04327072  1.82175182] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13147526 -1.34501403  0.07970575  1.54281933] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15837554 -1.15093547  0.11056214  1.27603442] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18139425 -1.34727997  0.13608283  1.60119201] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.20833985 -1.15400658  0.16810667  1.35384713] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.23141998 -0.96134581  0.19518361  1.1181207 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.01082876 -0.01753487 -0.0216284   0.00107892] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01047806 -0.21234007 -0.02160682  0.28686012] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00623126 -0.40714732 -0.01586962  0.57265089] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00191168 -0.21180649 -0.0044166   0.27501106] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00614781 -0.0166218   0.00108362 -0.01906159] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00648025  0.17848459  0.00070239 -0.31140242] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00291056  0.37359653 -0.00552566 -0.60386375] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00456137  0.17855229 -0.01760294 -0.3129264 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00813242  0.37392053 -0.02386146 -0.61110836] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01561083  0.17914008 -0.03608363 -0.32603545] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01919363  0.3747567  -0.04260434 -0.62987567] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02668877  0.18025435 -0.05520185 -0.35090878] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03029385 -0.01404088 -0.06222003 -0.07613097] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03001303 -0.20821823 -0.06374265  0.19629055] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02584867 -0.40237324 -0.05981684  0.46820396] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01780121 -0.20645949 -0.05045276  0.15728325] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01367202 -0.01065285 -0.04730709 -0.15087978] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01345896  0.18511346 -0.05032469 -0.45810382] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01716123  0.38090937 -0.05948677 -0.76621507] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02477942  0.57679772 -0.07481107 -1.07700667] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03631537  0.38273941 -0.0963512  -0.80870733] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04397016  0.57904033 -0.11252535 -1.13007643] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05555096  0.38555717 -0.13512688 -0.87469984] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06326211  0.58223181 -0.15262087 -1.20662905] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07490674  0.38937504 -0.17675345 -0.96540309] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08269424  0.58637388 -0.19606151 -1.30799276] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03867436  0.04434378 -0.01312049 -0.01839195] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03956124  0.23965142 -0.01348833 -0.3151855 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04435426  0.43496288 -0.01979204 -0.61209149] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05305352  0.24012305 -0.03203387 -0.32570746] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05785598  0.04547149 -0.03854802 -0.04329627] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05876541 -0.1490771  -0.03941394  0.23697952] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05578387  0.04658513 -0.03467435 -0.06787059] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05671557  0.24218661 -0.03603176 -0.37128866] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06155931  0.43780144 -0.04345754 -0.67511151] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07031533  0.24330949 -0.05695977 -0.39642151] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07518152  0.04904    -0.0648882  -0.12222699] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07616232  0.2450287  -0.06733274 -0.43465508] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0810629   0.05092137 -0.07602584 -0.16393474] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08208133 -0.14303458 -0.07930453  0.10382873] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07922063  0.05312904 -0.07722796 -0.21278299] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08028322 -0.14080869 -0.08148362  0.05457407] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07746704 -0.33467347 -0.08039214  0.32047671] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07077357 -0.528564   -0.0739826   0.58676335] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06020229 -0.33248811 -0.06224733  0.271723  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05355253 -0.52666914 -0.05681287  0.54414154] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04301915 -0.33079678 -0.04593004  0.23411302] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03640321 -0.52523341 -0.04124778  0.51196171] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02589854 -0.32955547 -0.03100855  0.20657104] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01930743 -0.5242206  -0.02687713  0.48931331] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00882302 -0.32872998 -0.01709086  0.18828237] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00224842 -0.13336774 -0.01332521 -0.10974267] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00041893 -0.32829623 -0.01552007  0.17870662] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00698486 -0.52319269 -0.01194594  0.46645338] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01744871 -0.327904   -0.00261687  0.17002923] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02400679 -0.5229884   0.00078372  0.46188547] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03446656 -0.32787754  0.01002143  0.16944967] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04102411 -0.52314148  0.01341042  0.46527714] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05148694 -0.32821157  0.02271596  0.1768511 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05805117 -0.52365112  0.02625298  0.47661271] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06852419 -0.71913373  0.03578524  0.77745307] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08290687 -0.91472904  0.0513343   1.08117681] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10120145 -1.11048967  0.07295784  1.3895166 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12341124 -0.91634851  0.10074817  1.12051013] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14173821 -0.72268182  0.12315837  0.86105305] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15619185 -0.91924632  0.14037943  1.18978361] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17457677 -0.72619458  0.1641751   0.94419071] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18910067 -0.92310195  0.18305892  1.28363113] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.20756271 -0.73072091  0.20873154  1.05340043] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00794168 -0.00178017  0.0288697   0.00801078] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00790608 -0.197304    0.02902991  0.30966083] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00396    -0.00260743  0.03522313  0.02627262] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00390785 -0.19821634  0.03574858  0.32985745] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-5.64778700e-05 -3.93828468e-01  4.23457317e-02  6.33596013e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00793305 -0.58951475  0.05501765  0.93930823] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01972334 -0.78533349  0.07380382  1.24875893] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03543001 -0.9813199   0.098779    1.56361689] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05505641 -0.78750808  0.13005133  1.30331013] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07080657 -0.5942532   0.15611754  1.0540032 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08269164 -0.79106125  0.1771976   1.39133736] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09851286 -0.9878912   0.20502435  1.7337831 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04808775  0.02030256 -0.04331367  0.02230707] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0484938  -0.17417233 -0.04286753  0.30101557] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04501036  0.02153357 -0.03684722 -0.00487278] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04544103  0.21716406 -0.03694467 -0.30894996] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04978431  0.02258745 -0.04312367 -0.02814347] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05023606 -0.1718904  -0.04368654  0.25062767] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04679825  0.02382729 -0.03867399 -0.05550853] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0472748  -0.1707194  -0.03978416  0.22472599] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04386041  0.0249479  -0.03528964 -0.08023618] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04435937  0.2205575  -0.03689436 -0.38384089] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04877052  0.41618331 -0.04457118 -0.6879244 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05709418  0.61189463 -0.05832967 -0.99429942] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06933208  0.80774629 -0.07821566 -1.30471648] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.085487    1.00376806 -0.10430999 -1.62082216] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10556236  1.19995266 -0.13672643 -1.94411111] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12956142  1.3962417  -0.17560865 -2.27586931] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.01033244 -0.02486033  0.04436783 -0.04405307] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00983523  0.16959825  0.04348676 -0.32241403] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0132272   0.36407483  0.03703848 -0.60107205] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02050869  0.16845486  0.02501704 -0.29695651] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02387779 -0.02701462  0.01907791  0.0035102 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0233375   0.16782861  0.01914812 -0.28309288] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02669407  0.36267228  0.01348626 -0.56967562] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03394751  0.55760252  0.00209275 -0.85807954] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04509957  0.7526959  -0.01506884 -1.1501037 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06015348  0.55777382 -0.03807092 -0.86218385] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07130896  0.36319035 -0.05531459 -0.58171029] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07857277  0.55904193 -0.0669488  -0.89129253] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08975361  0.36488898 -0.08477465 -0.62038378] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09705138  0.1710469  -0.09718233 -0.35555876] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10047232  0.3674066  -0.1042935  -0.67723518] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10782045  0.17387643 -0.11783821 -0.41912402] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11129798  0.3704538  -0.12622069 -0.74650952] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11870706  0.17727843 -0.14115088 -0.49606203] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12225263  0.37407913 -0.15107212 -0.82968793] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12973421  0.18130937 -0.16766588 -0.58807311] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1333604  -0.01111701 -0.17942734 -0.35254429] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13313806  0.1860422  -0.18647822 -0.69600437] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1368589  -0.00607139 -0.20039831 -0.46733732] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04180185  0.02220958  0.03593954 -0.03882097] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04224604 -0.17340882  0.03516312  0.2649811 ] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.03877786 -0.36901455  0.04046274  0.56854414] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03139757 -0.56467998  0.05183362  0.87369485] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02010397 -0.37029966  0.06930752  0.59774824] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01269798 -0.56631948  0.08126248  0.91143215] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00137159 -0.37238554  0.09949113  0.6453561 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00607612 -0.56874284  0.11239825  0.96763767] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01745098 -0.76517992  0.131751    1.29340956] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03275458 -0.57195527  0.15761919  1.04470732] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04419368 -0.37923686  0.17851334  0.80536157] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05177842 -0.18695226  0.19462057  0.57372275] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05551746  0.00498533  0.20609502  0.34810964] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00670934  0.00813713  0.00250381 -0.03073835] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00687208 -0.18702064  0.00188904  0.26273351] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00313167  0.0080743   0.00714372 -0.029353  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00329316 -0.18714936  0.00655666  0.26557526] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00044983  0.0078784   0.01186816 -0.02503245] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-2.92261408e-04  2.02828154e-01  1.13675115e-02 -3.13947333e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0037643   0.39778634  0.00508856 -0.60302374] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01172003  0.59283675 -0.00697191 -0.89409954] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02357676  0.39781004 -0.0248539  -0.60361631] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03153296  0.20304435 -0.03692623 -0.3188643 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03559385  0.39867223 -0.04330351 -0.62295992] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0435673   0.20418082 -0.05576271 -0.34422354] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04765091  0.00989464 -0.06264718 -0.069633  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04784881  0.2058562  -0.06403984 -0.38140494] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05196593  0.40182626 -0.07166794 -0.69357231] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06000245  0.59786536 -0.08553939 -1.00792844] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07195976  0.40398307 -0.10569796 -0.74328625] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08003942  0.21046645 -0.12056368 -0.48564868] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08424875  0.0172337  -0.13027665 -0.23326348] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08459343  0.21395313 -0.13494192 -0.56403521] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08887249  0.41068465 -0.14622263 -0.89600243] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09708618  0.21781545 -0.16414268 -0.65262236] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10144249  0.41479664 -0.17719512 -0.99216418] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10973842  0.22243111 -0.19703841 -0.75995717] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03123636  0.00373591 -0.01142086 -0.0202562 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03116165 -0.19122041 -0.01182598  0.26880154] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03498605  0.0040683  -0.00644995 -0.02758779] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03490469  0.19928215 -0.00700171 -0.32229875] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03091904  0.0042606  -0.01344768 -0.03183208] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03083383  0.19957279 -0.01408432 -0.32872738] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02684238  0.00465414 -0.02065887 -0.04051906] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02674929  0.20006615 -0.02146925 -0.33964781] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02274797  0.00525616 -0.02826221 -0.05381164] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02264285 -0.18944939 -0.02933844  0.22982207] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02643184  0.00607926 -0.024742   -0.07196894] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02631025 -0.18867941 -0.02618138  0.21280623] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03008384 -0.38341744 -0.02192525  0.49711673] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03775219 -0.57822349 -0.01198292  0.78281006] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04931666 -0.77317872  0.00367328  1.07169901] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06478023 -0.96834904  0.02510726  1.36553246] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08414721 -0.77355033  0.05241791  1.08080724] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09961822 -0.57915808  0.07403406  0.80502319] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11120138 -0.38512492  0.09013452  0.53651685] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11890388 -0.19137823  0.10086486  0.27354022] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12273144  0.00217072  0.10633566  0.01429639] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12268803  0.19561971  0.10662159 -0.2430343 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11877563  0.38906994  0.1017609  -0.50027444] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11099424  0.5826212   0.09175541 -0.75923332] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09934181  0.3863628   0.07657075 -0.43914583] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09161456  0.19024537  0.06778783 -0.12334184] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08780965 -0.00577897  0.06532099  0.18993371] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08792523 -0.20177166  0.06911967  0.50248648] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09196066 -0.00768856  0.0791694   0.23236199] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09211443  0.18621815  0.08381664 -0.03433553] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08839007  0.38004437  0.08312993 -0.29944094] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08078918  0.57388907  0.07714111 -0.56479183] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0693114   0.37777439  0.06584527 -0.24883766] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06175591  0.57189723  0.06086852 -0.52004639] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05031797  0.37597359  0.05046759 -0.20882089] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0427985   0.18016769  0.04629117  0.09934518] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03919514  0.37459669  0.04827808 -0.17838115] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03170321  0.56899571  0.04471045 -0.45545168] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02032329  0.76345792  0.03560142 -0.733713  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00505414  0.56786265  0.02092716 -0.4300414 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00630312  0.7626821   0.01232633 -0.71605445] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02155676  0.95763129 -0.00199476 -1.00483217] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04070939  1.15277983 -0.0220914  -1.29814085] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06376498  0.95794524 -0.04805422 -1.01245462] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08292389  1.15367421 -0.06830331 -1.31983166] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10599737  1.34958992 -0.09469994 -1.63308555] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13298917  1.54568757 -0.12736165 -1.95371424] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16390292  1.74191175 -0.16643594 -2.28300969] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03859096  0.02720681 -0.0253826  -0.04461868] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03804682 -0.16754215 -0.02627497  0.23994897] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04139766 -0.36227909 -0.02147599  0.5242296 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04864325 -0.16686159 -0.0109914   0.22485749] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05198048 -0.36182474 -0.00649425  0.51405314] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05921697 -0.55685463  0.00378681  0.80468251] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07035406 -0.75202829  0.01988046  1.09855422] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08539463 -0.55717361  0.04185154  0.81217442] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0965381  -0.75284308  0.05809503  1.11772239] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11159496 -0.55852959  0.08044948  0.84381421] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12276556 -0.75465183  0.09732576  1.16067273] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13785859 -0.56092301  0.12053922  0.90002333] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14907705 -0.75745401  0.13853969  1.22803355] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16422613 -0.56435986  0.16310036  0.98176797] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17551333 -0.37175464  0.18273572  0.74443353] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18294842 -0.56886482  0.19762439  1.08860184] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01830813  0.03734924  0.03433505  0.04326217] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01756115 -0.15824781  0.03520029  0.34657734] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02072611 -0.35385232  0.04213184  0.6501491 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02780315 -0.54953502  0.05513482  0.95579576] 1 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.03879385 -0.35519625  0.07425074  0.68093199] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04589778 -0.55126665  0.08786938  0.9960369 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05692311 -0.35742269  0.10779011  0.73219311] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06407156 -0.55385594  0.12243398  1.05676188] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07514868 -0.36055028  0.14356921  0.80487966] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08235969 -0.16765765  0.15966681  0.56058202] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08571284 -0.3646177   0.17087845  0.89900704] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09300519 -0.17217206  0.18885859  0.66453695] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09644864  0.01989089  0.20214933  0.43676002] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03429728  0.03069628 -0.04855396 -0.01462368] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03491121  0.22647971 -0.04884643 -0.32222199] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0394408   0.42226198 -0.05529087 -0.62990026] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04788604  0.61811011 -0.06788888 -0.93947076] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06024824  0.8140783  -0.0866783  -1.25268991] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07652981  1.01019705 -0.11173209 -1.57121515] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09673375  0.81657178 -0.1431564  -1.31536922] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11306518  0.62352166 -0.16946378 -1.07070058] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12553562  0.82042926 -0.19087779 -1.41141526] 1 1.0 True {}\n",
      "state, action, reward, done, info: [0.00928616 0.04706158 0.00561179 0.02831368] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01022739  0.24210261  0.00617806 -0.2625934 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01506944  0.43713583  0.00092619 -0.55332131] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02381216  0.24200089 -0.01014023 -0.26034672] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02865218  0.43726611 -0.01534717 -0.55621068] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0373975   0.24236295 -0.02647138 -0.26840226] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04224476  0.04762859 -0.03183943  0.01581528] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04319733  0.24319234 -0.03152312 -0.28674089] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04806118  0.0485338  -0.03725794 -0.00416441] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04903185  0.24416973 -0.03734123 -0.30836594] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05391525  0.04959919 -0.04350855 -0.02768935] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05490723  0.2453172  -0.04406233 -0.33377603] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05981357  0.44103768 -0.05073785 -0.64002193] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06863433  0.24665845 -0.06353829 -0.36373884] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0735675   0.44262318 -0.07081307 -0.67576047] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08241996  0.63865397 -0.08432828 -0.98987137] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09519304  0.44475582 -0.10412571 -0.72482082] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10408816  0.6411518  -0.11862212 -1.04837688] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11691119  0.44778641 -0.13958966 -0.79516023] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12586692  0.25482797 -0.15549287 -0.54944411] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13096348  0.06219269 -0.16648175 -0.30950822] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13220733 -0.13021441 -0.17267191 -0.07360777] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12960305  0.06690828 -0.17414407 -0.4154084 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13094121 -0.12537318 -0.18245223 -0.18228852] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12843375 -0.31747931 -0.18609801  0.04774388] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12208416 -0.12024411 -0.18514313 -0.29739817] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11967928 -0.31231074 -0.19109109 -0.06834353] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11343306 -0.50425262 -0.19245796  0.1584891 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10334801 -0.69617302 -0.18928818  0.38482059] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08942455 -0.49893891 -0.18159177  0.03893481] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07944577 -0.69105524 -0.18081307  0.26927593] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06562467 -0.49387561 -0.17542755 -0.07454168] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05574716 -0.68610548 -0.17691839  0.15806604] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04202505 -0.87831144 -0.17375707  0.3901316 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02445882 -1.07059646 -0.16595443  0.62338813] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00304689 -1.26306017 -0.15348667  0.85955039] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02221431 -1.06621825 -0.13629566  0.52281051] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04353868 -0.86946776 -0.12583945  0.19047475] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06092803 -0.67279121 -0.12202996 -0.13910653] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07438386 -0.86597324 -0.12481209  0.11272382] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09170332 -1.05910628 -0.12255761  0.36357026] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11288545 -0.86247501 -0.11528621  0.03489389] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13013495 -0.66590475 -0.11458833 -0.29182333] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14345304 -0.85922227 -0.1204248  -0.03736403] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16063749 -0.6625977  -0.12117208 -0.36548424] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17388944 -0.46598088 -0.12848176 -0.69378445] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18320906 -0.6591086  -0.14235745 -0.44415043] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.19639123 -0.4622897  -0.15124046 -0.77810421] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.20563703 -0.2654476  -0.16680254 -1.11429228] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.21094598 -0.45803416 -0.18908839 -0.87823476] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.22010666 -0.2609156  -0.20665308 -1.22390373] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02709812  0.01517598 -0.02982829 -0.00978976] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0267946   0.21071273 -0.03002408 -0.31173263] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02258034  0.40624927 -0.03625874 -0.61373101] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01445536  0.21165226 -0.04853336 -0.33268508] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01022231  0.0172535  -0.05518706 -0.0556935 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00987724 -0.17703551 -0.05630093  0.21907957] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01341795 -0.37130935 -0.05191934  0.49348486] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02084414 -0.17549506 -0.04204964  0.18490178] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02435404  0.02020253 -0.0383516  -0.12074407] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02394999 -0.17434956 -0.04076649  0.15959687] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02743698 -0.36886488 -0.03757455  0.43914541] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03481428 -0.17323183 -0.02879164  0.13485864] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03827892  0.02229044 -0.02609447 -0.16676679] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03783311 -0.17244846 -0.0294298   0.11757121] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04128208 -0.36713665 -0.02707838  0.40082593] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04862481 -0.56186424 -0.01906186  0.68485017] 1 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.0598621  -0.3664829  -0.00536486  0.38622755] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06719175 -0.56152828  0.00235969  0.67721415] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07842232 -0.3664392   0.01590398  0.38527509] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0857511  -0.56178328  0.02360948  0.68292972] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09698677 -0.75722498  0.03726807  0.98295113] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11213127 -0.56262166  0.0569271   0.70220311] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1233837  -0.75848451  0.07097116  1.01224898] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13855339 -0.95447787  0.09121614  1.32634695] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15764295 -1.15062533  0.11774308  1.64612534] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18065546 -0.95706132  0.15066558  1.39232613] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.19979668 -0.76410179  0.17851211  1.15029407] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.21507872 -0.96104624  0.20151799  1.49322054] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.0122828   0.04513989  0.01490078 -0.03864528] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0131856  -0.15019254  0.01412788  0.2587015 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01018175  0.0447249   0.01930191 -0.02949202] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01107625 -0.15066846  0.01871207  0.26921783] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00806288 -0.34605238  0.02409643  0.56774343] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00114183 -0.15127656  0.03545129  0.28274819] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0018837  -0.34688577  0.04110626  0.58639813] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00882142 -0.15236293  0.05283422  0.30694219] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01186868 -0.34819637  0.05897306  0.61580849] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0188326  -0.54409053  0.07128923  0.92646694] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02971441 -0.34999991  0.08981857  0.65701179] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03671441 -0.15623547  0.10295881  0.39390789] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03983912 -0.35265622  0.11083697  0.7171952 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04689225 -0.54912336  0.12518087  1.04260698] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05787471 -0.35586601  0.14603301  0.79169642] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06499203 -0.16301855  0.16186694  0.54828546] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0682524   0.02950372  0.17283265  0.31065835] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06766233 -0.1676051   0.17904581  0.65247686] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07101443 -0.36470891  0.19209535  0.99576493] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00563921 -0.04928428 -0.02098259 -0.00269889] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00662489 -0.24409912 -0.02103657  0.28329062] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01150688 -0.43891481 -0.01537076  0.56926519] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02028517 -0.24358069 -0.00398546  0.2717798 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02515678 -0.43864554  0.00145014  0.56320303] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0339297  -0.63378781  0.0127142   0.85634247] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04660545 -0.82908068  0.02984105  1.15299597] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06318707 -1.02457891  0.05290097  1.45488472] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08367864 -1.22030889  0.08199866  1.7636146 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10808482 -1.02620439  0.11727096  1.49751675] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12860891 -0.83268676  0.14722129  1.24363271] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14526264 -0.63972827  0.17209395  1.00045285] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15805721 -0.44727169  0.192103    0.76637858] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16700264 -0.25524028  0.20743057  0.53976168] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04715303  0.04636829 -0.02278253  0.04022493] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04622567 -0.14841967 -0.02197803  0.32563358] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04919406  0.04700819 -0.01546536  0.02610155] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0482539   0.24234848 -0.01494333 -0.27142047] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04340693  0.43768044 -0.02037174 -0.56877887] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03465332  0.63308211 -0.03174731 -0.86780943] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02199167  0.82862132 -0.0491035  -1.17030259] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00541925  1.02434627 -0.07250955 -1.47796677] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01506768  1.22027484 -0.10206889 -1.79238651] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03947317  1.41638213 -0.13791662 -2.11497266] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06780082  1.22288076 -0.18021607 -1.86789574] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.0081941  -0.03332121  0.02235024 -0.00777224] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00752767 -0.22875644  0.02219479  0.29187782] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00295254 -0.03395787  0.02803235  0.00627666] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00227339  0.16075108  0.02815788 -0.27743159] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00548841  0.35546024  0.02260925 -0.56110223] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01259761  0.55025771  0.0113872  -0.8465772 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02360277  0.35498228 -0.00554434 -0.55033528] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03070241  0.15993864 -0.01655105 -0.25940435] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03390119  0.35529291 -0.02173913 -0.55726141] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04100704  0.16048278 -0.03288436 -0.27150606] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0442167   0.35605817 -0.03831448 -0.5743768 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05133786  0.55169573 -0.04980202 -0.87887948] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06237178  0.74745773 -0.06737961 -1.18679413] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07732093  0.94338557 -0.09111549 -1.49981363] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09618864  1.13948855 -0.12111176 -1.81949881] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11897842  0.94590313 -0.15750174 -1.56676759] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13789648  0.75297447 -0.18883709 -1.32707095] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03897016 -0.00053329 -0.03988717 -0.0347136 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03895949 -0.19506122 -0.04058144  0.24512249] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03505827  0.00061614 -0.03567899 -0.06007957] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03507059 -0.19397656 -0.03688058  0.22113632] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03119106 -0.38855248 -0.03245785  0.50196128] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02342001 -0.58320224 -0.02241863  0.78424121] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01175596 -0.77800907 -0.0067338   1.0697875 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00380422 -0.58279871  0.01466195  0.77499888] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01546019 -0.38788148  0.03016192  0.48696492] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02321782 -0.19319781  0.03990122  0.2039385 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02708178  0.00133147  0.04397999 -0.07589533] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02705515  0.19579624  0.04246209 -0.35438447] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-2.31392232e-02  9.70583242e-05  3.53743956e-02 -4.86201933e-02] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02313728  0.19469438  0.03440199 -0.32993566] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01924339 -0.00089997  0.02780328 -0.02660554] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01926139 -0.19640938  0.02727117  0.27471834] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02318958 -0.00168693  0.03276553 -0.00924   ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02322332  0.19295017  0.03258073 -0.2914076 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01936432  0.38759278  0.02675258 -0.57363953] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01161246  0.58232964  0.01527979 -0.85777597] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 3.41319133e-05  7.77240127e-01 -1.87572744e-03 -1.14561549e+00] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01557893  0.58214273 -0.02478804 -0.85352136] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02722179  0.38736726 -0.04185846 -0.56873486] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03496913  0.58305055 -0.05323316 -0.87430547] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04663015  0.38869118 -0.07071927 -0.59882256] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05440397  0.19462625 -0.08269572 -0.32922692] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05829649  0.39082209 -0.08928026 -0.64679982] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06611294  0.58706711 -0.10221626 -0.96620848] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07785428  0.78340229 -0.12154043 -1.28917336] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09352232  0.59001771 -0.14732389 -1.03688134] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10532268  0.39712831 -0.16806152 -0.79383818] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11326524  0.59410926 -0.18393828 -1.13432449] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12514743  0.79109827 -0.20662477 -1.47859692] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02757008 -0.0305797  -0.00188351  0.00408387] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.02818167 -0.22567459 -0.00180183  0.29617193] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03269516 -0.42077081  0.00412161  0.58828605] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04111058 -0.22570681  0.01588733  0.29690429] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04562471 -0.03081491  0.02182542  0.009274  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04624101 -0.22624295  0.0220109   0.30876242] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05076587 -0.03144143  0.02818614  0.02310159] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0513947  -0.22695602  0.02864818  0.32454262] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05593382 -0.03225344  0.03513903  0.04103001] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05657889 -0.2278612   0.03595963  0.34458927] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06113611 -0.03326876  0.04285141  0.06345915] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06180149  0.16121345  0.0441206  -0.2154019 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05857722  0.35567778  0.03981256 -0.49384728] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05146366  0.55021628  0.02993561 -0.77372201] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04045934  0.35469557  0.01446117 -0.47177258] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03336543  0.15937238  0.00502572 -0.17456699] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03017798 -0.03582114  0.00153438  0.11969714] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0308944   0.1592788   0.00392832 -0.17250131] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02770883  0.3543443   0.0004783  -0.46394239] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02062194  0.54945949 -0.00880055 -0.75647452] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00963275  0.74470163 -0.02393004 -1.05191371] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00526128  0.54990507 -0.04496831 -0.76683729] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01625938  0.35543013 -0.06030506 -0.488636  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02336799  0.16120852 -0.07007778 -0.21555212] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02659216 -0.03284525 -0.07438882  0.0542281 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02593525 -0.22682611 -0.07330426  0.32254452] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02139873 -0.42083182 -0.06685337  0.59123942] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01298209 -0.22484073 -0.05502858  0.27826925] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00848528 -0.02897869 -0.0494632  -0.03124934] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00790571 -0.2233577  -0.05008818  0.24542632] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00343855 -0.02755747 -0.04517966 -0.06262545] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0028874  -0.22200351 -0.04643217  0.21546768] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00155267 -0.41643196 -0.04212281  0.49314989] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00988131 -0.22074201 -0.03225982  0.18749488] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01429615 -0.41538791 -0.02850992  0.46982909] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02260391 -0.21987507 -0.01911334  0.16829823] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02700141 -0.4147183  -0.01574737  0.45489071] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03529577 -0.60961409 -0.00664956  0.74256861] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04748805 -0.80464363  0.00820182  1.03315149] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06358093 -0.9998737   0.02886485  1.32839805] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0835784  -0.80512766  0.05543281  1.04488577] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09968095 -1.00093998  0.07633052  1.35444199] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11969975 -1.1969325   0.10341936  1.66999414] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1436384  -1.0031534   0.13681924  1.41122967] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16370147 -0.8099672   0.16504384  1.16425787] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17990082 -1.00680732  0.188329    1.50380871] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03513874  0.02882165  0.02899025  0.03453609] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03456231  0.22351613  0.02968097 -0.24886098] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03009199  0.02798316  0.02470375  0.05303422] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02953232  0.22274234  0.02576444 -0.23175325] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02507748  0.02726189  0.02112937  0.06894401] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02453224  0.22207464  0.02250825 -0.21699831] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02009075  0.02663828  0.01816829  0.08269885] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01955798  0.22149514  0.01982226 -0.20419703] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01512808  0.41632809  0.01573832 -0.49056168] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00680152  0.61122453  0.00592709 -0.77824326] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00542297  0.41602158 -0.00963778 -0.48370142] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0137434   0.61127822 -0.0193118  -0.77940624] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02596897  0.41642703 -0.03489993 -0.49286127] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03429751  0.6120234  -0.04475715 -0.79633566] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04653798  0.80773003 -0.06068387 -1.10275583] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06269258  0.61345657 -0.08273898 -0.82971215] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07496171  0.4195572  -0.09933323 -0.56415559] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08335285  0.22595901 -0.11061634 -0.30434577] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08787203  0.42246925 -0.11670325 -0.62976453] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09632142  0.61900971 -0.12929855 -0.95680282] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10870161  0.42584132 -0.1484346  -0.70737685] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11721844  0.23305334 -0.16258214 -0.46485448] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12187951  0.04055709 -0.17187923 -0.22750675] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12269065 -0.15174514 -0.17642936  0.00641222] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11965574 -0.34395577 -0.17630112  0.23865098] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11277663 -0.53617846 -0.1715281   0.47095116] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10205306 -0.33910157 -0.16210908  0.12949162] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09527103 -0.53157493 -0.15951924  0.3669648 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08463953 -0.72411316 -0.15217995  0.60540537] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07015727 -0.91681634 -0.14007184  0.84655112] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05182094 -1.10977803 -0.12314082  1.09211225] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02962538 -0.91326772 -0.10129857  0.76346685] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01136003 -1.10685947 -0.08602924  1.02263498] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01077716 -1.30073665 -0.06557654  1.28711384] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0367919  -1.49496571 -0.03983426  1.55856554] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06669121 -1.29939016 -0.00866295  1.25372675] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09267901 -1.10415833  0.01641159  0.95834313] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11476218 -1.29949705  0.03557845  1.25613654] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14075212 -1.10484818  0.06070118  0.97480586] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16284909 -1.30072955  0.0801973   1.28592196] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.18886368 -1.49677542  0.10591574  1.60259835] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.21879918 -1.69297968  0.1379677   1.92633653] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.25265878 -1.49958114  0.17649443  1.6794293 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.02745518  0.04111546 -0.02762286  0.0248555 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02827749 -0.15359969 -0.02712575  0.30869671] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0252055  -0.34832485 -0.02095182  0.592703  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.018239   -0.15291596 -0.00909776  0.29349462] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01518068  0.04233451 -0.00322787 -0.00204367] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01602737  0.23750261 -0.00326874 -0.29574328] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02077742  0.04242741 -0.00918361 -0.00409305] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02162597 -0.15256164 -0.00926547  0.28567825] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01857474  0.04269122 -0.0035519  -0.00991248] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01942856 -0.15237961 -0.00375015  0.28164766] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01638097  0.04279563  0.0018828  -0.01221568] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01723688  0.23789053  0.00163849 -0.30430397] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02199469  0.04274526 -0.00444759 -0.01110475] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0228496  -0.15231262 -0.00466969  0.2801716 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01980335 -0.34736765  0.00093375  0.57137807] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01285599 -0.54250268  0.01236131  0.86435501] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00200594 -0.7377907   0.02964841  1.16089879] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01274987 -0.93328606  0.05286638  1.4627283 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03141559 -1.12901449  0.08212095  1.77144625] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05399588 -1.32496128  0.11754987  2.08849235] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08049511 -1.13120561  0.15931972  1.83434464] 0 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    print('state, action, reward, done, info:', \n",
    "          state, action, reward, done, info)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-0.00523918, -0.00796275,  0.00417821,  0.02770183]),\n",
       "  0,\n",
       "  array([-0.00539844, -0.20314437,  0.00473225,  0.3217001 ]),\n",
       "  1.0,\n",
       "  0.0],\n",
       " (4,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 float64\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.0884923515191147 -2.630399089525059\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, targetQs, action_size, hidden_size):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    #Qs = tf.reduce_max(actions_logits, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 24*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty`\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([-0.19282622,  0.18785127, -0.05589764, -0.97058444]),\n",
       "  0,\n",
       "  array([-0.18906919, -0.00647766, -0.07530933, -0.69597167]),\n",
       "  1.0,\n",
       "  0.0],\n",
       " -1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[-1], memory.rates[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:16.0000 R:16.0 loss:1.0828 exploreP:0.9984\n",
      "Episode:1 meanR:23.5000 R:31.0 loss:1.0745 exploreP:0.9954\n",
      "Episode:2 meanR:30.3333 R:44.0 loss:1.1311 exploreP:0.9910\n",
      "Episode:3 meanR:26.0000 R:13.0 loss:1.2126 exploreP:0.9898\n",
      "Episode:4 meanR:26.4000 R:28.0 loss:1.3376 exploreP:0.9870\n",
      "Episode:5 meanR:24.6667 R:16.0 loss:1.5148 exploreP:0.9855\n",
      "Episode:6 meanR:22.8571 R:12.0 loss:1.6208 exploreP:0.9843\n",
      "Episode:7 meanR:25.2500 R:42.0 loss:1.5718 exploreP:0.9802\n",
      "Episode:8 meanR:24.2222 R:16.0 loss:1.7628 exploreP:0.9787\n",
      "Episode:9 meanR:24.1000 R:23.0 loss:1.6126 exploreP:0.9764\n",
      "Episode:10 meanR:25.8182 R:43.0 loss:2.4291 exploreP:0.9723\n",
      "Episode:11 meanR:25.0833 R:17.0 loss:2.7405 exploreP:0.9706\n",
      "Episode:12 meanR:24.7692 R:21.0 loss:2.2269 exploreP:0.9686\n",
      "Episode:13 meanR:24.2143 R:17.0 loss:2.4840 exploreP:0.9670\n",
      "Episode:14 meanR:23.2000 R:9.0 loss:2.0295 exploreP:0.9661\n",
      "Episode:15 meanR:22.7500 R:16.0 loss:3.4667 exploreP:0.9646\n",
      "Episode:16 meanR:22.8824 R:25.0 loss:3.2667 exploreP:0.9622\n",
      "Episode:17 meanR:22.6111 R:18.0 loss:1.8417 exploreP:0.9605\n",
      "Episode:18 meanR:22.3158 R:17.0 loss:4.0510 exploreP:0.9589\n",
      "Episode:19 meanR:22.2500 R:21.0 loss:3.2290 exploreP:0.9569\n",
      "Episode:20 meanR:22.5238 R:28.0 loss:4.0319 exploreP:0.9543\n",
      "Episode:21 meanR:22.1364 R:14.0 loss:4.6797 exploreP:0.9529\n",
      "Episode:22 meanR:22.6087 R:33.0 loss:5.8893 exploreP:0.9498\n",
      "Episode:23 meanR:22.4583 R:19.0 loss:4.2652 exploreP:0.9481\n",
      "Episode:24 meanR:23.2000 R:41.0 loss:6.2200 exploreP:0.9442\n",
      "Episode:25 meanR:23.0769 R:20.0 loss:9.9997 exploreP:0.9423\n",
      "Episode:26 meanR:22.9259 R:19.0 loss:6.9365 exploreP:0.9406\n",
      "Episode:27 meanR:22.9286 R:23.0 loss:8.2721 exploreP:0.9384\n",
      "Episode:28 meanR:22.6897 R:16.0 loss:8.9374 exploreP:0.9370\n",
      "Episode:29 meanR:22.2667 R:10.0 loss:10.3133 exploreP:0.9360\n",
      "Episode:30 meanR:22.0645 R:16.0 loss:10.9480 exploreP:0.9345\n",
      "Episode:31 meanR:22.6875 R:42.0 loss:12.0374 exploreP:0.9307\n",
      "Episode:32 meanR:22.3636 R:12.0 loss:6.6487 exploreP:0.9296\n",
      "Episode:33 meanR:22.2059 R:17.0 loss:19.9903 exploreP:0.9280\n",
      "Episode:34 meanR:21.9714 R:14.0 loss:17.6284 exploreP:0.9267\n",
      "Episode:35 meanR:22.0833 R:26.0 loss:16.0240 exploreP:0.9243\n",
      "Episode:36 meanR:21.9459 R:17.0 loss:16.8149 exploreP:0.9228\n",
      "Episode:37 meanR:22.1053 R:28.0 loss:18.7237 exploreP:0.9202\n",
      "Episode:38 meanR:22.1026 R:22.0 loss:22.3790 exploreP:0.9182\n",
      "Episode:39 meanR:22.0250 R:19.0 loss:17.5429 exploreP:0.9165\n",
      "Episode:40 meanR:21.7317 R:10.0 loss:24.6353 exploreP:0.9156\n",
      "Episode:41 meanR:21.5238 R:13.0 loss:22.1913 exploreP:0.9144\n",
      "Episode:42 meanR:21.3953 R:16.0 loss:30.0454 exploreP:0.9130\n",
      "Episode:43 meanR:21.3636 R:20.0 loss:24.5477 exploreP:0.9112\n",
      "Episode:44 meanR:21.6000 R:32.0 loss:20.4882 exploreP:0.9083\n",
      "Episode:45 meanR:21.7174 R:27.0 loss:27.2449 exploreP:0.9059\n",
      "Episode:46 meanR:21.8085 R:26.0 loss:31.9603 exploreP:0.9036\n",
      "Episode:47 meanR:21.6875 R:16.0 loss:34.6345 exploreP:0.9021\n",
      "Episode:48 meanR:21.5306 R:14.0 loss:25.7891 exploreP:0.9009\n",
      "Episode:49 meanR:21.3200 R:11.0 loss:40.6721 exploreP:0.8999\n",
      "Episode:50 meanR:21.1961 R:15.0 loss:36.5421 exploreP:0.8986\n",
      "Episode:51 meanR:21.8654 R:56.0 loss:35.8188 exploreP:0.8936\n",
      "Episode:52 meanR:21.8868 R:23.0 loss:40.4753 exploreP:0.8916\n",
      "Episode:53 meanR:21.9074 R:23.0 loss:36.9670 exploreP:0.8895\n",
      "Episode:54 meanR:22.0182 R:28.0 loss:48.7649 exploreP:0.8871\n",
      "Episode:55 meanR:21.9286 R:17.0 loss:56.8825 exploreP:0.8856\n",
      "Episode:56 meanR:22.0000 R:26.0 loss:39.1355 exploreP:0.8833\n",
      "Episode:57 meanR:22.1207 R:29.0 loss:46.6115 exploreP:0.8808\n",
      "Episode:58 meanR:21.9492 R:12.0 loss:47.2922 exploreP:0.8797\n",
      "Episode:59 meanR:22.1500 R:34.0 loss:57.4810 exploreP:0.8768\n",
      "Episode:60 meanR:22.0164 R:14.0 loss:67.5812 exploreP:0.8756\n",
      "Episode:61 meanR:21.8871 R:14.0 loss:58.6017 exploreP:0.8744\n",
      "Episode:62 meanR:21.7460 R:13.0 loss:63.0587 exploreP:0.8733\n",
      "Episode:63 meanR:21.8281 R:27.0 loss:87.2374 exploreP:0.8709\n",
      "Episode:64 meanR:21.6923 R:13.0 loss:63.2879 exploreP:0.8698\n",
      "Episode:65 meanR:21.7121 R:23.0 loss:77.9249 exploreP:0.8678\n",
      "Episode:66 meanR:21.7015 R:21.0 loss:61.5112 exploreP:0.8660\n",
      "Episode:67 meanR:21.6324 R:17.0 loss:82.8886 exploreP:0.8646\n",
      "Episode:68 meanR:21.7826 R:32.0 loss:89.6254 exploreP:0.8618\n",
      "Episode:69 meanR:21.6143 R:10.0 loss:51.6399 exploreP:0.8610\n",
      "Episode:70 meanR:21.8310 R:37.0 loss:98.2327 exploreP:0.8579\n",
      "Episode:71 meanR:22.1806 R:47.0 loss:134.1013 exploreP:0.8539\n",
      "Episode:72 meanR:22.0274 R:11.0 loss:155.7626 exploreP:0.8529\n",
      "Episode:73 meanR:21.9189 R:14.0 loss:105.3434 exploreP:0.8518\n",
      "Episode:74 meanR:21.9467 R:24.0 loss:148.9570 exploreP:0.8498\n",
      "Episode:75 meanR:21.8684 R:16.0 loss:129.9944 exploreP:0.8484\n",
      "Episode:76 meanR:22.0000 R:32.0 loss:141.2424 exploreP:0.8457\n",
      "Episode:77 meanR:21.8718 R:12.0 loss:158.5355 exploreP:0.8447\n",
      "Episode:78 meanR:21.9241 R:26.0 loss:146.3770 exploreP:0.8426\n",
      "Episode:79 meanR:21.8500 R:16.0 loss:197.6637 exploreP:0.8412\n",
      "Episode:80 meanR:21.7407 R:13.0 loss:102.7311 exploreP:0.8401\n",
      "Episode:81 meanR:21.6707 R:16.0 loss:223.3055 exploreP:0.8388\n",
      "Episode:82 meanR:21.6506 R:20.0 loss:192.4724 exploreP:0.8372\n",
      "Episode:83 meanR:21.8095 R:35.0 loss:245.5248 exploreP:0.8343\n",
      "Episode:84 meanR:21.8588 R:26.0 loss:180.7609 exploreP:0.8321\n",
      "Episode:85 meanR:21.8953 R:25.0 loss:238.3392 exploreP:0.8301\n",
      "Episode:86 meanR:21.9310 R:25.0 loss:230.4132 exploreP:0.8280\n",
      "Episode:87 meanR:21.8182 R:12.0 loss:198.8137 exploreP:0.8271\n",
      "Episode:88 meanR:21.7865 R:19.0 loss:369.0097 exploreP:0.8255\n",
      "Episode:89 meanR:21.7222 R:16.0 loss:355.2986 exploreP:0.8242\n",
      "Episode:90 meanR:21.5934 R:10.0 loss:269.8188 exploreP:0.8234\n",
      "Episode:91 meanR:21.4674 R:10.0 loss:256.1865 exploreP:0.8226\n",
      "Episode:92 meanR:21.3763 R:13.0 loss:232.6794 exploreP:0.8215\n",
      "Episode:93 meanR:21.4149 R:25.0 loss:286.1686 exploreP:0.8195\n",
      "Episode:94 meanR:21.3684 R:17.0 loss:234.2309 exploreP:0.8181\n",
      "Episode:95 meanR:21.2917 R:14.0 loss:235.0302 exploreP:0.8170\n",
      "Episode:96 meanR:21.2577 R:18.0 loss:365.5271 exploreP:0.8155\n",
      "Episode:97 meanR:21.2755 R:23.0 loss:241.9239 exploreP:0.8137\n",
      "Episode:98 meanR:21.1818 R:12.0 loss:319.3084 exploreP:0.8127\n",
      "Episode:99 meanR:21.2500 R:28.0 loss:475.0201 exploreP:0.8105\n",
      "Episode:100 meanR:21.2700 R:18.0 loss:345.5251 exploreP:0.8090\n",
      "Episode:101 meanR:21.2000 R:24.0 loss:426.8220 exploreP:0.8071\n",
      "Episode:102 meanR:21.0700 R:31.0 loss:424.7051 exploreP:0.8047\n",
      "Episode:103 meanR:21.2000 R:26.0 loss:411.5548 exploreP:0.8026\n",
      "Episode:104 meanR:21.2300 R:31.0 loss:465.4261 exploreP:0.8001\n",
      "Episode:105 meanR:21.2500 R:18.0 loss:341.1848 exploreP:0.7987\n",
      "Episode:106 meanR:21.3400 R:21.0 loss:287.0544 exploreP:0.7971\n",
      "Episode:107 meanR:21.2000 R:28.0 loss:637.3095 exploreP:0.7949\n",
      "Episode:108 meanR:21.1500 R:11.0 loss:515.1586 exploreP:0.7940\n",
      "Episode:109 meanR:21.0700 R:15.0 loss:520.4910 exploreP:0.7928\n",
      "Episode:110 meanR:20.8600 R:22.0 loss:533.3055 exploreP:0.7911\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "\n",
    "            # Training maxrated batch\n",
    "            batch, rates = memory.sample(batch_size=batch_size)\n",
    "            rates = np.array(rates)\n",
    "            percentage = 0.9\n",
    "            states = np.array([each[0] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "            actions = np.array([each[1] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "            next_states = np.array([each[2] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = np.array([each[3] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "            dones = np.array([each[4] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "            #maxrates = rates[rates >= (np.max(rates)*percentage)]            \n",
    "            #nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average losses')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Creating a gym env\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# A training graph session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "# Close the env at the end\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
