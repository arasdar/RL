{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.01335056  0.02418345  0.01333172 -0.03371267] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01383423 -0.17112712  0.01265747  0.26314656] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01041168  0.02381189  0.0179204  -0.02551734] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01088792  0.21867233  0.01741005 -0.31249274] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01526137  0.02330673  0.0111602  -0.01437046] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0157275   0.21826687  0.01087279 -0.30351143] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02009284  0.41323219  0.00480256 -0.59274559] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02835748  0.60828659 -0.00705235 -0.88391186] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04052322  0.80350359 -0.02473059 -1.17880345] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05659329  0.9989378  -0.04830666 -1.47913518] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07657204  1.19461501 -0.07788936 -1.78650523] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10046434  1.39052042 -0.11361946 -2.10234874] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12827475  1.19670736 -0.15566644 -1.84683593] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1522089   1.00360572 -0.19260316 -1.60626715] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02506717  0.0475219   0.04395751  0.04738059] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02411673 -0.1482019   0.04490513  0.35360205] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02708077 -0.34393265  0.05197717  0.66009989] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03395942 -0.53973793  0.06517916  0.96868556] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04475418 -0.73567156  0.08455288  1.28111103] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05946761 -0.93176279  0.1101751   1.59902602] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07810287 -0.73810519  0.14215562  1.34262686] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09286497 -0.93470059  0.16900815  1.67619849] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11155898 -0.74189465  0.20253212  1.44056239] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04130322  0.00364788  0.03141872  0.04346237] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04123026 -0.19191018  0.03228797  0.34589029] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04506847 -0.38747619  0.03920577  0.64857741] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05281799 -0.19292171  0.05217732  0.3684935 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05667642  0.00142153  0.05954719  0.09270855] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05664799  0.19564167  0.06140136 -0.18060878] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05273516  0.3898337   0.05778919 -0.45330724] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04493849  0.1939442   0.04872304 -0.14298293] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0410596   0.38833573  0.04586338 -0.41990528] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03329289  0.19259494  0.03746528 -0.11312374] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02944099  0.38716058  0.0352028  -0.39375519] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02169778  0.19155724  0.0273277  -0.09018447] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01786663  0.38627705  0.02552401 -0.37412178] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01014109  0.58102731  0.01804157 -0.65864878] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00147946  0.38565896  0.0048686  -0.36034001] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00919264  0.58071137 -0.0023382  -0.65148379] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02080686  0.77586581 -0.01536788 -0.94490208] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03632418  0.5809542  -0.03426592 -0.65708717] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04794326  0.38632557 -0.04740766 -0.37538781] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05566977  0.58208772 -0.05491542 -0.68263374] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06731153  0.38776963 -0.06856809 -0.40773303] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07506692  0.58379343 -0.07672275 -0.72122101] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08674279  0.77988819 -0.09114717 -1.03703223] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10234055  0.97609566 -0.11188782 -1.35688191] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12186247  1.17242923 -0.13902546 -1.68236722] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14531105  1.36886074 -0.1726728  -2.01491353] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04797992 -0.00689849  0.00457469 -0.04524731] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04811789  0.18815756  0.00366974 -0.33648338] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04435474 -0.00701642 -0.00305992 -0.04264547] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04449507  0.18814928 -0.00391283 -0.33629226] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04073209  0.38332669 -0.01063868 -0.63020651] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03306555  0.57859547 -0.02324281 -0.92622081] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02149364  0.77402346 -0.04176722 -1.22611639] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00601317  0.57946339 -0.06628955 -0.94680681] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0055761   0.77541241 -0.08522569 -1.25955963] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02108434  0.58147788 -0.11041688 -0.99473969] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0327139   0.38799215 -0.13031167 -0.73867517] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04047374  0.58464974 -0.14508518 -1.06936378] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05216674  0.78136119 -0.16647245 -1.40383642] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06779396  0.58865141 -0.19454918 -1.1674851 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04954282  0.02254293 -0.00474425 -0.01373724] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04999368 -0.17251066 -0.00501899  0.27744506] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04654346  0.02268254  0.00052991 -0.01681662] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 4.69971130e-02 -1.72447011e-01  1.93574006e-04  2.76033447e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04354817 -0.36757172  0.00571424  0.56877742] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03619674 -0.56277335  0.01708979  0.86325506] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02494127 -0.75812375  0.03435489  1.16126202] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0097788  -0.95367594  0.05758013  1.46451558] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00929472 -0.75930463  0.08687044  1.19036101] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02448082 -0.56540913  0.11067766  0.92612229] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.035789   -0.76183766  0.12920011  1.25143515] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05102575 -0.95835643  0.15422881  1.58163565] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07019288 -1.15494098  0.18586153  1.91817609] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02162293  0.02157175 -0.01631283  0.01996745] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02119149  0.21692379 -0.01591348 -0.27781739] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01685302  0.02203244 -0.02146982  0.00980426] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01641237  0.21745561 -0.02127374 -0.28957458] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01206325  0.02264338 -0.02706523 -0.00367636] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01161039 -0.17208017 -0.02713876  0.28034579] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01505199  0.02341818 -0.02153184 -0.02077158] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01458363 -0.17138847 -0.02194727  0.26504079] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0180114   0.02403975 -0.01664646 -0.03448292] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0175306   0.21939641 -0.01733612 -0.33237114] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01314267  0.41476077 -0.02398354 -0.63047021] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00484746  0.61020902 -0.03659294 -0.93060877] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00735672  0.80580524 -0.05520512 -1.2345626 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02347283  1.00159164 -0.07989637 -1.54401687] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04350466  0.80751548 -0.11077671 -1.27729584] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05965497  1.00386162 -0.13632263 -1.60251159] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0797322   1.20030854 -0.16837286 -1.93440298] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10373837  1.00734067 -0.20706092 -1.69831244] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.02787172 -0.01106728 -0.0292021  -0.00189019] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02765037  0.18446104 -0.0292399  -0.30364182] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03133959 -0.01023226 -0.03531274 -0.02032201] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03113495 -0.20483046 -0.03571918  0.26101353] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02703834 -0.00921731 -0.03049891 -0.04271825] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02685399  0.18632841 -0.03135327 -0.3448657 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03058056 -0.00833382 -0.03825059 -0.06223221] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03041388  0.18731508 -0.03949523 -0.36673384] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03416018 -0.00722403 -0.04682991 -0.08676131] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0340157  -0.20164452 -0.04856513  0.19078674] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02998281 -0.00586265 -0.0447494  -0.11681234] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02986556 -0.20031581 -0.04708565  0.16142347] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02585924 -0.39473316 -0.04385718  0.43888822] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01796458 -0.58920783 -0.03507941  0.71742984] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00618042 -0.39361842 -0.02073081  0.41391498] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00169194 -0.19820885 -0.01245251  0.11476919] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00565612 -0.39315018 -0.01015713  0.40349757] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01351912 -0.19788566 -0.00208718  0.10762975] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-1.74768376e-02 -3.92977639e-01  6.54154905e-05  3.99653455e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02533639 -0.58810052  0.00805848  0.692357  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0370984  -0.78333334  0.02190562  0.9875659 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05276507 -0.97874165  0.04165694  1.28704777] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0723399  -1.1743682   0.0673979   1.59247664] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09582726 -0.98010775  0.09924743  1.32154752] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11542942 -1.17633394  0.12567838  1.64356922] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1389561  -0.98288753  0.15854977  1.39254047] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15861385 -1.17958792  0.18640058  1.7303089 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03223223 -0.02079736  0.02716854  0.01212584] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03264818  0.17392463  0.02741106 -0.2718628 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02916969  0.36864494  0.0219738  -0.55577569] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02179679  0.5634516   0.01085829 -0.84145535] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01052776  0.75842365 -0.00597082 -1.13070393] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00464072  0.56338039 -0.0285849  -0.83989966] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01590832  0.75888071 -0.04538289 -1.14143316] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03108594  0.95456549 -0.06821155 -1.44799609] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05017725  1.15045666 -0.09717147 -1.76118817] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07318638  1.3465348  -0.13239524 -2.08244137] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10011708  1.15297778 -0.17404406 -1.83345729] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03519585 -0.00208661  0.03861875  0.01000426] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03523759  0.19246082  0.03881884 -0.27024822] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03138837 -0.00319296  0.03341387  0.03442123] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03145223 -0.19877775  0.0341023   0.3374565 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03542778 -0.00415726  0.04085143  0.05571964] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03551093 -0.19984043  0.04196582  0.36100647] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03950774 -0.39553299  0.04918595  0.6666212 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0474184  -0.59130325  0.06251837  0.9743763 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05924446 -0.78720565  0.0820059   1.28602424] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07498858 -0.59321759  0.10772638  1.02010275] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08685293 -0.39968322  0.12812844  0.76309436] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09484659 -0.59631517  0.14339033  1.09319131] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1067729  -0.79300498  0.16525415  1.42721059] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.122633   -0.60026484  0.19379836  1.19040293] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01967327 -0.0467785  -0.04195219 -0.02496155] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02060884  0.14891919 -0.04245142 -0.33058013] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01763046  0.34461891 -0.04906303 -0.63634221] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01073808  0.54038954 -0.06178987 -0.94406353] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 6.97095157e-05  7.36287040e-01 -8.06711410e-02 -1.25550368e+00] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01479545  0.54228527 -0.10578121 -0.98913901] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02564116  0.34872619 -0.12556399 -0.73146498] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03261568  0.15554247 -0.14019329 -0.48078963] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03572653  0.35233608 -0.14980909 -0.81416588] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04277325  0.15954832 -0.1660924  -0.5721025 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04596422  0.35656188 -0.17753445 -0.91216296] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05309545  0.55358379 -0.19577771 -1.25497204] 1 1.0 True {}\n",
      "state, action, reward, done, info: [0.00442518 0.01826461 0.04057079 0.0474114 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00479048 -0.17741491  0.04151902  0.35261368] 1 1.0 False {}\n",
      "state, action, reward, done, info: [0.00124218 0.0170928  0.04857129 0.07330664] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00158403 -0.17869062  0.05003742  0.38090976] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00198978 -0.37448606  0.05765562  0.6889402 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0094795  -0.18020966  0.07143442  0.41495119] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01308369 -0.37626754  0.07973345  0.72927184] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02060904 -0.18233293  0.09431888  0.46271094] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0242557   0.01133833  0.1035731   0.20118392] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02402894 -0.18510069  0.10759678  0.52465948] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02773095 -0.38155923  0.11808997  0.84921559] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03536213 -0.18822862  0.13507428  0.59587718] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03912671 -0.38495657  0.14699183  0.9278724 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04682584 -0.19209217  0.16554928  0.68475567] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05066768 -0.38907838  0.17924439  1.02464466] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05844925 -0.19673708  0.19973728  0.79317116] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00672129  0.04632119 -0.03349595 -0.01003214] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00579486  0.24190711 -0.03369659 -0.31309242] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00095672  0.04728099 -0.03995844 -0.03122389] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-1.11003013e-05  2.42952511e-01 -4.05829158e-02 -3.36241537e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00484795  0.04843089 -0.04730775 -0.05662746] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00581657 -0.14598194 -0.0484403   0.22076224] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00289693 -0.34037923 -0.04402505  0.49778025] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00391066 -0.14466509 -0.03406945  0.1915543 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00680396 -0.33928351 -0.03023836  0.47329805] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01358963 -0.14374784 -0.0207724   0.17123991] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01646458 -0.33856642 -0.0173476   0.45729808] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02323591 -0.14320357 -0.00820164  0.15919796] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02609998 -0.33820714 -0.00501768  0.44928223] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03286413 -0.53325776  0.00396796  0.74037929] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04352928 -0.72843427  0.01877555  1.03430833] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05809797 -0.9238008   0.03946172  1.33282615] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07657398 -1.11939745  0.06611824  1.63759145] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09896193 -0.92511017  0.09887007  1.36622191] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11746414 -1.1213212   0.12619451  1.68812158] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13989056 -1.31765632  0.15995694  2.01728518] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16624369 -1.12451562  0.20030264  1.77809959] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.01519023  0.02617474 -0.01811685  0.02536072] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01571373 -0.16868278 -0.01760964  0.31227297] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01234007 -0.36354949 -0.01136418  0.59935075] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00506908 -0.1682704   0.00062284  0.30311004] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00170368 -0.36340123  0.00668504  0.59598933] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00556435 -0.55861609  0.01860482  0.89077047] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01673667 -0.36375143  0.03642023  0.60399357] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0240117  -0.55936331  0.0485001   0.90792198] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03519897 -0.36493028  0.06665854  0.63086883] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04249757 -0.56091589  0.07927592  0.94377747] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05371589 -0.75701121  0.09815147  1.26027935] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06885611 -0.56327216  0.12335706  0.99987969] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08012156 -0.36999553  0.14335465  0.74834283] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08752147 -0.56677342  0.15832151  1.08248399] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09885694 -0.76358997  0.17997119  1.42036885] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11412873 -0.96042381  0.20837856  1.76347732] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02932603  0.02271278 -0.02589371  0.02998371] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02887177  0.21819631 -0.02529403 -0.27075519] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02450785  0.02344426 -0.03070914  0.01384378] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02403896 -0.17122413 -0.03043226  0.29668163] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02746344 -0.36589933 -0.02449863  0.57961352] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03478143 -0.56066955 -0.01290636  0.8644793 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04599482 -0.75561346  0.00438323  1.15307645] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06110709 -0.56054896  0.02744476  0.86177117] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07231807 -0.75603366  0.04468018  1.16295554] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08743874 -0.56152109  0.06793929  0.88460948] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09866916 -0.75749648  0.08563148  1.19785319] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11381909 -0.95361583  0.10958854  1.51609919] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13289141 -1.14987985  0.13991053  1.84088377] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15588901 -0.95655237  0.1767282   1.59472584] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17502005 -0.763912    0.20862272  1.36195412] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01353016  0.03635967  0.0134126  -0.00397895] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01280297 -0.15895204  0.01333302  0.29290546] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01598201 -0.35426153  0.01919113  0.58976344] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02306724 -0.15941347  0.0309864   0.30318703] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02625551  0.03525348  0.03705014  0.02043532] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02555044  0.22982504  0.03745885 -0.26033149] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02095394  0.42439279  0.03225222 -0.54096805] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01246608  0.2288327   0.02143286 -0.23829996] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00788943  0.03341122  0.01666686  0.06106571] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0072212  -0.16194568  0.01788817  0.35896019] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01046012  0.03291747  0.02506737  0.07197107] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00980177  0.22767124  0.0265068  -0.21269877] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00524834  0.03218054  0.02225282  0.08822636] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00460473  0.22697657  0.02401735 -0.19735356] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-6.51998163e-05  3.15194682e-02  2.00702759e-02  1.02807962e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00056519 -0.16388428  0.02212644  0.40175481] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0027125   0.03091696  0.03016153  0.11612913] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00209416 -0.16462388  0.03248411  0.41817313] 1 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.00538663  0.03002304  0.04084758  0.13590542] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00478617  0.22453683  0.04356568 -0.14361604] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-2.95436863e-04  4.19008667e-01  4.06933641e-02 -4.22242743e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00808474  0.61353118  0.03224851 -0.70182441] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02035536  0.41797743  0.01821202 -0.39916688] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02871491  0.22260192  0.01022868 -0.10079817] 0 1.0 False {}\n",
      "state, action, reward, done, info: [0.03316695 0.02733488 0.00821272 0.19509424] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03371364  0.2223384   0.0121146  -0.09498666] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03816041  0.41728464  0.01021487 -0.38382295] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04650611  0.61226008  0.00253841 -0.67326774] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05875131  0.41710294 -0.01092694 -0.37978668] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06709337  0.61237834 -0.01852268 -0.67589476] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07934093  0.80775272 -0.03204057 -0.9743514 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09549599  1.00328951 -0.0515276  -1.27692436] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11556178  1.19902921 -0.07706609 -1.58528663] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13954236  1.00490355 -0.10877182 -1.31759744] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15964043  0.81131234 -0.13512377 -1.06084265] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.17586668  1.00793966 -0.15634062 -1.39270075] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.19602547  1.20462386 -0.18419464 -1.72990594] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04009682 -0.04207385 -0.02625008  0.03668076] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0409383  -0.23680972 -0.02551646  0.32096731] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0456745  -0.43155918 -0.01909712  0.60549537] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05430568 -0.23617546 -0.00698721  0.30685901] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05902919 -0.04095464 -0.00085003  0.01198072] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05984828 -0.23606439 -0.00061042  0.30439533] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06456957 -0.04093375  0.00547749  0.01151995] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06538824  0.15410922  0.00570789 -0.27942974] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-6.23060599e-02  3.49149286e-01  1.19295543e-04 -5.70306957e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05532307  0.54426956 -0.01128684 -0.8629523 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04443768  0.73954335 -0.02854589 -1.15916258] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02964682  0.54480476 -0.05172914 -0.87556506] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01875072  0.35042271 -0.06924044 -0.59958359] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01174227  0.15633431 -0.08123211 -0.32948902] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00861558  0.35251302 -0.08782189 -0.64664272] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00156532  0.15871741 -0.10075475 -0.38285605] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00160903  0.3551148  -0.10841187 -0.7055299 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00871132  0.16164866 -0.12252247 -0.44884509] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0119443   0.35827131 -0.13149937 -0.77749965] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01910972  0.55493276 -0.14704936 -1.10849447] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03020838  0.36201683 -0.16921925 -0.86532047] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03744872  0.16955207 -0.18652566 -0.63026027] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04083976 -0.02254493 -0.19913087 -0.40163392] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04038886  0.17476187 -0.20716355 -0.74990395] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00236316  0.04122173  0.00259625 -0.01600574] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00153872 -0.15393736  0.00227613  0.2774952 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00461747  0.04115205  0.00782603 -0.01446897] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00379443  0.23616089  0.00753666 -0.30467245] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00092879  0.43117463  0.00144321 -0.594969  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00955228  0.62627636 -0.01045617 -0.88719697] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02207781  0.43129788 -0.02820011 -0.59781931] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03070377  0.62680284 -0.0401565  -0.89924989] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04323982  0.43224745 -0.0581415  -0.6194549 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05188477  0.2379837  -0.0705306  -0.34563562] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05664445  0.04393221 -0.07744331 -0.07600089] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05752309 -0.14999903 -0.07896333  0.1912785 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05452311  0.04615855 -0.07513776 -0.12523213] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05544628  0.24227202 -0.0776424  -0.44064206] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06029172  0.43840198 -0.08645524 -0.75675462] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06905976  0.24457125 -0.10159033 -0.49248064] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07395119  0.44096835 -0.11143994 -0.8153736 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08277055  0.24753433 -0.12774742 -0.55971795] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08772124  0.05441501 -0.13894178 -0.30985512] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08880954  0.25121487 -0.14513888 -0.64292767] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09383384  0.44802965 -0.15799743 -0.977567  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10279443  0.64487672 -0.17754877 -1.31541806] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11569197  0.84174431 -0.20385713 -1.6580027 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.02294628 -0.02450045  0.04488424 -0.02676706] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02245627  0.16995005  0.0443489  -0.30495751] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02585527 -0.02577492  0.03824975  0.00137558] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02533977 -0.22142396  0.03827726  0.30587717] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02091129 -0.02686778  0.04439481  0.02550743] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02037394  0.16759032  0.04490496 -0.25284454] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02372574  0.36204326  0.03984807 -0.5310323 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03096661  0.55658271  0.02922742 -0.81089759] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04209826  0.75129232  0.01300947 -1.09424563] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05712411  0.9462405  -0.00887544 -1.38281848] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07604892  1.14147205 -0.03653181 -1.67826361] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09887836  1.336998   -0.07009709 -1.98209483] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12561832  1.53278323 -0.10973898 -2.29564312] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15627399  1.72873134 -0.15565185 -2.6199958 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.19084861  1.53510807 -0.20805176 -2.37864915] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00819488 -0.01342362  0.02726525 -0.02993285] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00792641 -0.20892575  0.02666659  0.27122634] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0037479  -0.40441787  0.03209112  0.57219932] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00434046 -0.20976025  0.0435351   0.28979639] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00853567 -0.01528526  0.04933103  0.01115558] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00884137  0.17909578  0.04955414 -0.26556384] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00525946 -0.01669712  0.04424287  0.04232796] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0055934   0.17776339  0.04508943 -0.23607422] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00203813  0.37221311  0.04036794 -0.51420056] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00540613  0.56674399  0.03008393 -0.79389419] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01674101  0.76144036  0.01420605 -1.07696326] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03196982  0.95637179 -0.00733322 -1.36515448] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05109725  1.15158481 -0.03463631 -1.66012208] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07412895  1.34709283 -0.06783875 -1.96338926] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10107081  1.15275121 -0.10710654 -1.69247665] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12412583  0.95901685 -0.14095607 -1.43496953] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14330617  0.76588644 -0.16965546 -1.18945106] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1586239   0.5733198  -0.19344448 -0.9543879 ] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02241319  0.01255275 -0.03941549 -0.00287882] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02216214  0.20821717 -0.03947307 -0.30773264] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0179978   0.01367925 -0.04562772 -0.02775514] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01772421  0.20942483 -0.04618283 -0.33447782] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01353571  0.01498955 -0.05287238 -0.05670879] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01323592 -0.179336   -0.05400656  0.21883496] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01682264  0.0165147  -0.04962986 -0.09038297] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01649235 -0.17786205 -0.05143752  0.18623795] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02004959  0.01795668 -0.04771276 -0.12221761] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01969046 -0.17645038 -0.05015711  0.15503888] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02321946  0.0193525  -0.04705633 -0.15303609] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02283241 -0.17506518 -0.05011706  0.12443817] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02633372 -0.36943465 -0.04762829  0.40089805] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03372241 -0.5638498  -0.03961033  0.67819264] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04499941 -0.75839972 -0.02604648  0.95814622] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0601674  -0.56293742 -0.00688355  0.65739542] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07142615 -0.36772033  0.00626435  0.36255298] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07878056 -0.17268797  0.01351541  0.07185189] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08223432 -0.36800105  0.01495245  0.36876816] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08959434 -0.17309471  0.02232782  0.0808372 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09305623 -0.3685295   0.02394456  0.38048014] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10042682 -0.17375561  0.03155416  0.09544208] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10390193 -0.36931527  0.033463    0.39791093] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11128824 -0.56489558  0.04142122  0.70095345] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12258615 -0.76056649  0.05544029  1.00638243] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13779748 -0.95638316  0.07556794  1.31594725] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.15692514 -0.76229424  0.10188688  1.04784056] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17217103 -0.56866103  0.1228437   0.78879995] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18354425 -0.37542108  0.13861969  0.53714964] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.19105267 -0.57219189  0.14936269  0.87009841] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.20249651 -0.37938279  0.16676466  0.62785437] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.21008416 -0.57639127  0.17932174  0.9680693 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.22161199 -0.77340848  0.19868313  1.31129483] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03466235 -0.03781744 -0.04185361  0.00620267] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03541869  0.15787898 -0.04172955 -0.29938626] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03226111  0.35357013 -0.04771728 -0.60493238] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02518971  0.15914681 -0.05981592 -0.32765292] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02200678  0.35506708 -0.06636898 -0.63858312] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01490543  0.5510486  -0.07914065 -0.95140636] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00388446  0.35707581 -0.09816877 -0.68460093] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00325705  0.55341379 -0.11186079 -1.00650524] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01432533  0.35994881 -0.1319909  -0.75093999] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02152431  0.55662013 -0.1470097  -1.08207371] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03265671  0.75334399 -0.16865117 -1.41704189] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04772359  0.56066353 -0.19699201 -1.18147054] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03571888 -0.02209587  0.01730095 -0.01024665] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03527697  0.17277374  0.01709602 -0.2974211 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03873244  0.36764786  0.01114759 -0.58466359] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0460854   0.17237155 -0.00054568 -0.28848999] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04953283  0.36750128 -0.00631548 -0.58134497] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05688286  0.56271115 -0.01794238 -0.87601067] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06813708  0.75807233 -0.03546259 -1.17428005] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08329852  0.95363675 -0.05894819 -1.47786607] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10237126  1.14942689 -0.08850551 -1.7883619 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1253598   1.34542368 -0.12427275 -2.10719303] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15226827  1.15174646 -0.16641661 -1.85535986] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1753032   0.95879843 -0.20352381 -1.61863723] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02139224 -0.00431559  0.00067905  0.04603098] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02147855 -0.19944727  0.00159967  0.33892808] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0254675  -0.00434812  0.00837824  0.04675003] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02555446 -0.1995892   0.00931324  0.34206455] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02954624 -0.39484241  0.01615453  0.63766971] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03744309 -0.19994941  0.02890792  0.35011758] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04144208 -0.00525025  0.03591027  0.06668861] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04154708  0.18933894  0.03724405 -0.21445165] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03776031  0.38390918  0.03295501 -0.4951574 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03008212  0.57855127  0.02305186 -0.77727498] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0185111   0.38312002  0.00750636 -0.47742935] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0108487   0.57813519 -0.00204222 -0.76773701] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 7.14008183e-04  7.73285195e-01 -1.73969622e-02 -1.06106182e+00] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01617971  0.96863316 -0.0386182  -1.35915387] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03555238  0.77401615 -0.06580128 -1.07879694] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0510327   0.57982194 -0.08737721 -0.80746763] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06262914  0.7760258  -0.10352657 -1.12630688] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07814965  0.97234058 -0.12605271 -1.4495849 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09759646  0.77897322 -0.1550444  -1.19879659] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11317593  0.97572345 -0.17902033 -1.53578246] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04359765 -0.04600244 -0.04774618  0.01379442] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0445177   0.14977058 -0.04747029 -0.29356249] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04152229 -0.04464356 -0.05334154 -0.01622082] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04241516 -0.23896156 -0.05366595  0.25916663] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04719439 -0.43327794 -0.04848262  0.53445136] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05585995 -0.23750892 -0.03779359  0.22689458] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06061013 -0.43207094 -0.0332557   0.50742054] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06925155 -0.62670892 -0.02310729  0.7894407 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08178573 -0.82150603 -0.00731848  1.07476543] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09821585 -1.01653052  0.01417683  1.3651427 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11854646 -1.21182711  0.04147968  1.662226  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.142783   -1.40740687  0.0747242   1.96753483] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.17093114 -1.21314985  0.1140749   1.6989109 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.19519414 -1.0195128   0.14805312  1.44380846] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.21558439 -0.82649058  0.17692929  1.20081136] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.2321142  -1.02340338  0.20094552  1.54331574] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02924228 -0.01387761  0.01210781 -0.01817488] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02951983  0.18106863  0.01174431 -0.30701319] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02589846  0.37602128  0.00560405 -0.59596925] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01837803  0.57106435 -0.00631534 -0.88688169] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00695674  0.37602869 -0.02405297 -0.59619073] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 5.63829604e-04  5.71478855e-01 -3.59767852e-02 -8.96352056e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01199341  0.37686265 -0.05390383 -0.61519154] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01953066  0.57269467 -0.06620766 -0.92435297] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03098455  0.3785265  -0.08469472 -0.65318966] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03855508  0.57471936 -0.09775851 -0.97129457] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05004947  0.77100774 -0.1171844  -1.2930166 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06546963  0.9674078  -0.14304473 -1.61996967] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08481778  0.77423171 -0.17544413 -1.37507692] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10030242  0.97105772 -0.20294566 -1.71710011] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.01206213  0.02463637 -0.02998441  0.02814563] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01255486 -0.17004304 -0.0294215   0.31121938] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.009154    0.02548545 -0.02319711  0.0094049 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00966371 -0.16929627 -0.02300901  0.29467954] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00627778  0.02614603 -0.01711542 -0.00517036] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0068007  -0.16872633 -0.01721883  0.28206368] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00342618 -0.3635985  -0.01157756  0.56926651] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-3.84579373e-03 -1.68316101e-01 -1.92225646e-04  2.72958818e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00721212 -0.36343531  0.00526695  0.56558111] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01448082 -0.55863075  0.01657857  0.85991869] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02565344 -0.75397454  0.03377695  1.15776798] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04073293 -0.94952005  0.05693231  1.46084747] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05972333 -1.14529188  0.08614926  1.77075849] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08262917 -1.34127385  0.12156443  2.0889378 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10945464 -1.53739481  0.16334318  2.4166002 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.00418008  0.00681701  0.03417989 -0.02459611] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00431642 -0.18877801  0.03368797  0.27867199] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00054086  0.00584757  0.03926141 -0.00319834] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00065781 -0.18981481  0.03919744  0.30160895] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00313849  0.00472718  0.04522962  0.0215412 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00304395  0.19917229  0.04566045 -0.25653521] 0 1.0 False {}\n",
      "state, action, reward, done, info: [0.0009395  0.00342918 0.04052974 0.05019294] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00100808  0.19794725  0.0415336  -0.22943206] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00496703  0.39245183  0.03694496 -0.50872995] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01281607  0.58703432  0.02677036 -0.78954517] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02455675  0.39155515  0.01097946 -0.48856193] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03238785  0.19628003  0.00120822 -0.19243901] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03631346  0.39138468 -0.00264056 -0.48474055] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04414115  0.58654379 -0.01233537 -0.77825453] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05587202  0.78183317 -0.02790046 -1.07479282] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07150869  0.97731248 -0.04939632 -1.37609933] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09105494  0.78284126 -0.07691831 -1.09926493] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10671176  0.97888669 -0.0989036  -1.41505528] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1262895   0.78511927 -0.12720471 -1.15485483] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14199188  0.98164941 -0.15030181 -1.48456561] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16162487  0.78864536 -0.17999312 -1.24234599] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.17739778  0.59623104 -0.20484004 -1.01101991] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01867288 -0.02518158  0.00654896  0.02557538] 0 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.01917651 -0.22039683  0.00706047  0.32031736] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02358445 -0.02537614  0.01346682  0.02986938] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02409197  0.16955013  0.0140642  -0.25853439] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02070097  0.3644685   0.00889351 -0.54674829] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0134116   0.55946438 -0.00204145 -0.83661588] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00222231  0.75461415 -0.01877377 -1.12994012] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01286997  0.55974302 -0.04137257 -0.8432041 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02406483  0.75540446 -0.05823665 -1.14860517] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03917292  0.56108907 -0.08120876 -0.87473812] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0503947   0.75721557 -0.09870352 -1.19180747] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06553901  0.56350108 -0.12253967 -0.93162176] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07680903  0.76004452 -0.1411721  -1.26016278] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09200992  0.56698207 -0.16637536 -1.01481693] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10334957  0.76388476 -0.1866717  -1.35478158] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03061836 -0.02779168 -0.00272189 -0.02194996] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03006252 -0.22287449 -0.00316089  0.26987293] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02560503 -0.41795119  0.00223657  0.56155723] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01724601 -0.61310446  0.01346771  0.85494395] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00498392 -0.41816861  0.03056659  0.56652608] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00337945 -0.22348849  0.04189712  0.28362749] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00784922 -0.02898838  0.04756967  0.00444751] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00842899 -0.22475913  0.04765862  0.31175165] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01292417 -0.42052652  0.05389365  0.61907562] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0213347  -0.2261971   0.06627516  0.34384179] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02585864 -0.42219622  0.073152    0.65666565] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03430257 -0.61825616  0.08628531  0.97145656] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04666769 -0.42439153  0.10571444  0.70707788] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05515552 -0.62080683  0.119856    1.03107826] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06757166 -0.81730171  0.14047756  1.35885941] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08391769 -1.0138777   0.16765475  1.69198346] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10419525 -0.82104219  0.20149442  1.45584578] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00726109  0.02493997 -0.04823223 -0.00950189] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00676229  0.22071926 -0.04842227 -0.31700408] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00234791  0.02631926 -0.05476235 -0.03997645] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00182152  0.22218199 -0.05556188 -0.34942221] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00262212  0.02789246 -0.06255032 -0.07476444] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00317997  0.22385277 -0.06404561 -0.38650787] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00765702  0.41982266 -0.07177577 -0.69867656] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01605347  0.22576542 -0.0857493  -0.42942414] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02056878  0.03195583 -0.09433778 -0.16495732] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0212079   0.22829273 -0.09763693 -0.48584734] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02577375  0.03467433 -0.10735387 -0.22546263] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02646724  0.23115373 -0.11186313 -0.54998651] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03109031  0.03776609 -0.12286286 -0.29453732] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03184564  0.23440592 -0.1287536  -0.62330449] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03653375  0.04129443 -0.14121969 -0.37378397] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03735964 -0.15156866 -0.14869537 -0.12874997] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03432827 -0.34428231 -0.15127037  0.11357514] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02744262 -0.53694952 -0.14899887  0.35497163] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01670363 -0.34005792 -0.14189944  0.01926275] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00990248 -0.14321625 -0.14151418 -0.31461068] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00703815 -0.33606858 -0.14780639 -0.06969039] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 3.16778520e-04 -1.39170770e-01 -1.49200202e-01 -4.05113822e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00246664 -0.33189679 -0.15730248 -0.16293879] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00910457 -0.13491369 -0.16056125 -0.50082178] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01180285  0.06206406 -0.17057769 -0.83949049] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01056157 -0.13036971 -0.1873675  -0.60493605] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01316896  0.0668098  -0.19946622 -0.95029306] 0 1.0 True {}\n",
      "state, action, reward, done, info: [0.03305993 0.01279738 0.04695463 0.03709238] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03331588 -0.18296536  0.04769648  0.34421244] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02965657 -0.37873223  0.05458073  0.65154625] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02208192 -0.57457016  0.06761166  0.96090445] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01059052 -0.7705325   0.08682975  1.27403905] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00482013 -0.57661894  0.11231053  1.0097604 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01635251 -0.38316049  0.13250573  0.75435089] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02401572 -0.57983606  0.14759275  1.08562093] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03561244 -0.77656371  0.16930517  1.42073906] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05114371 -0.97332704  0.19771995  1.76120235] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.03613451 -0.04100717  0.04971792  0.02582543] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03695466  0.15336783  0.05023443 -0.25076574] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0338873   0.34773778  0.04521912 -0.52719016] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02693254  0.15200972  0.03467531 -0.2206083 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02389235 -0.04359028  0.03026315  0.08280782] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02476415  0.15108508  0.0319193  -0.2001755 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02174245 -0.04447851  0.02791579  0.10240314] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02263202  0.15023249  0.02996386 -0.18134337] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01962737  0.34491314  0.02633699 -0.46442522] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01272911  0.53965322  0.01704848 -0.74869185] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00193605  0.7345359   0.00207465 -1.03596137] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01275467  0.92963021 -0.01864458 -1.32799226] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03134728  1.12498244 -0.04520442 -1.62645087] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05384692  1.32060595 -0.07773344 -1.93287195] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08025904  1.51646876 -0.11639088 -2.24860962] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11058842  1.71247863 -0.16136307 -2.57477689] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03902667 -0.02836618 -0.01971508 -0.02976211] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03845934  0.16703287 -0.02031032 -0.32859958] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0418     -0.02779414 -0.02688231 -0.04239016] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04124412 -0.2225205  -0.02773011  0.24169139] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03679371 -0.02701364 -0.02289629 -0.05960776] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03625343  0.16842899 -0.02408844 -0.35942578] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03962201  0.36388494 -0.03127696 -0.65960596] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04689971  0.55942789 -0.04446908 -0.96197086] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05808827  0.36493084 -0.06370849 -0.6835833 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06538689  0.17074866 -0.07738016 -0.41161856] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06880186  0.36687739 -0.08561253 -0.72765841] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07613941  0.17303679 -0.1001657  -0.46310177] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07960014 -0.02053751 -0.10942773 -0.20359521] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07918939  0.17596539 -0.11349964 -0.52869416] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0827087  -0.01739216 -0.12407352 -0.27382127] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08236086  0.17926155 -0.12954995 -0.60291835] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08594609 -0.01383319 -0.14160831 -0.35368462] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08566943  0.18298849 -0.14868201 -0.6874558 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0893292  -0.00979119 -0.16243112 -0.44502827] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08913337 -0.20228728 -0.17133169 -0.20762989] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08508763 -0.0051824  -0.17548428 -0.54908382] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08498398 -0.19746152 -0.18646596 -0.31642263] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08103475 -0.389506   -0.19279441 -0.08785326] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07324463 -0.58141693 -0.19455148  0.13834867] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06161629 -0.77329747 -0.19178451  0.3639016 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04615034 -0.96525022 -0.18450647  0.59051211] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02684534 -0.7680898  -0.17269623  0.24585345] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01148354 -0.96037865 -0.16777916  0.4794756 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00772403 -0.76333492 -0.15818965  0.13896435] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02299073 -0.95587921 -0.15541036  0.37786061] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04210832 -1.14849196 -0.14785315  0.61779076] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06507816 -0.95164778 -0.13549734  0.28243533] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08411111 -0.75487953 -0.12984863 -0.04972619] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0992087  -0.55815786 -0.13084315 -0.38039284] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11037186 -0.36144421 -0.13845101 -0.71129935] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11760074 -0.55440505 -0.152677   -0.46520245] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12868884 -0.74707729 -0.16198105 -0.22426884] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14363039 -0.55005558 -0.16646642 -0.56334725] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1546315  -0.74249871 -0.17773337 -0.32738833] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16948148 -0.54535054 -0.18428113 -0.670431  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.18038849 -0.34820985 -0.19768975 -1.01500764] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01977677 -0.04037218  0.03530815 -0.04910778] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02058421  0.15422616  0.034326   -0.33044492] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01749969 -0.04136718  0.0277171  -0.02713788] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01832703 -0.23687543  0.02717434  0.27415963] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02306454 -0.04215153  0.03265754 -0.0098301 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02390757 -0.23772624  0.03246093  0.29297516] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0286621  -0.43329561  0.03832044  0.59571651] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03732801 -0.23873033  0.05023477  0.31534629] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04210262 -0.04435859  0.05654169  0.0389193 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04298979  0.14990888  0.05732008 -0.23540184] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03999161 -0.04598318  0.05261204  0.07479655] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04091128 -0.24181836  0.05410797  0.38360366] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04574764 -0.43766511  0.06178005  0.69284384] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05450094 -0.24345218  0.07563692  0.42023199] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05936999 -0.04947886  0.08404156  0.15231996] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06035957 -0.24569736  0.08708796  0.47028805] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06527351 -0.05190657  0.09649372  0.20627501] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06631164 -0.24826645  0.10061922  0.5277698 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07127697 -0.44464942  0.11117462  0.85038651] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08016996 -0.64109747  0.12818235  1.17585746] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09299191 -0.44785239  0.1516995   0.92595045] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10194896 -0.25506854  0.17021851  0.68452445] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10705033 -0.45209366  0.183909    1.02559388] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1160922  -0.25983297  0.20442087  0.79583113] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00896268  0.00511375 -0.0312074  -0.01424788] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00886041  0.20066903 -0.03149236 -0.31661146] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00484703  0.00600947 -0.03782459 -0.03402413] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00472684 -0.18855022 -0.03850507  0.2464889 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00849784  0.0070999  -0.03357529 -0.05808646] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00835585 -0.18752495 -0.03473702  0.22381701] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01210634 -0.38213365 -0.03026068  0.50534332] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01974902 -0.57681638 -0.02015381  0.78833844] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03128534 -0.7716558  -0.00438704  1.0746134 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04671846 -0.9667195   0.01710522  1.36591636] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06605285 -0.77181586  0.04442355  1.07863235] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08148917 -0.57730783  0.0659962   0.80021437] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09303532 -0.3831502   0.08200048  0.52900074] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10069833 -0.18927181  0.0925805   0.26324147] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10448376  0.0044151   0.09784533  0.00113511] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10439546  0.19800757  0.09786803 -0.259144  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10043531  0.39160605  0.09268515 -0.5194248 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09260319  0.58530931  0.08229666 -0.78152214] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.080897    0.38915839  0.06666621 -0.46412365] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07311384  0.19316083  0.05738374 -0.15119502] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06925062 -0.00273386  0.05435984  0.15902491] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0693053  -0.19859023  0.05754034  0.46834943] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0732771  -0.00432637  0.06690733  0.19434343] 1 1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.07336363  0.18977787  0.07079419 -0.0765054 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06956807  0.38381728  0.06926409 -0.34603935] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06189173  0.57788913  0.0623433  -0.61610173] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05033394  0.38195412  0.05002127 -0.30445313] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04269486  0.57632885  0.0439322  -0.58095042] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03116828  0.38061974  0.03231319 -0.27475809] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02355589  0.18505201  0.02681803  0.02793879] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01985485  0.37977931  0.02737681 -0.25616357] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01225926  0.57449991  0.02225354 -0.54008725] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-7.69264800e-04  7.69302094e-01  1.14517918e-02 -8.25676172e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01461678  0.57402541 -0.00506173 -0.5294136 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02609729  0.7692182  -0.01565    -0.82368718] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04148165  0.9645507  -0.03212375 -1.12125097] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06077266  1.16007889 -0.05454877 -1.42383468] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08397424  0.96567214 -0.08302546 -1.1486877 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10328768  1.1617739  -0.10599921 -1.46620853] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12652316  1.35802203 -0.13532338 -1.79003397] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1536836   1.55437787 -0.17112406 -2.12153878] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.0092408  -0.02283862 -0.04239068 -0.00036536] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00878403  0.17286483 -0.04239799 -0.30611595] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01224133 -0.02162811 -0.04852031 -0.02709999] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01180877 -0.21602187 -0.04906231  0.24988811] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00748833 -0.02023487 -0.04406454 -0.05785753] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00708363 -0.21469822 -0.04522169  0.22060357] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00278967 -0.40914558 -0.04080962  0.49868578] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00539324 -0.60366912 -0.03083591  0.77823345] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01746663 -0.40813703 -0.01527124  0.47601043] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02562937 -0.60304006 -0.00575103  0.7638413 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03769017 -0.40783937  0.0095258   0.46935433] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04584696 -0.60309459  0.01891288  0.7650244 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05790885 -0.40823811  0.03421337  0.47835202] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06607361 -0.60382597  0.04378041  0.78161888] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07815013 -0.40933226  0.05941279  0.5030252 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08633677 -0.21509583  0.06947329  0.2296417 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09063869 -0.41113823  0.07406613  0.54340466] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09886146 -0.21713103  0.08493422  0.27494724] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10320408 -0.41335563  0.09043317  0.59316378] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11147119 -0.60961934  0.10229644  0.91290749] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12366358 -0.80596531  0.12055459  1.23590951] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13978288 -0.61258102  0.14527278  0.98329568] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1520345  -0.80931898  0.16493869  1.31785423] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.16822088 -0.61662174  0.19129578  1.08100358] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.01072279  0.04803409 -0.01387615 -0.00937027] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01168348  0.24335227 -0.01406356 -0.30639879] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01655052  0.43867177 -0.02019154 -0.60348362] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02532396  0.24383795 -0.03226121 -0.31722837] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03020072  0.4394042  -0.03860578 -0.61990825] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0389888   0.63504347 -0.05100394 -0.92449578] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05168967  0.44064618 -0.06949386 -0.6482677 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06050259  0.63666395 -0.08245921 -0.9619994 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07323587  0.83279141 -0.1016992  -1.27940652] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0898917   1.02905169 -0.12728733 -1.60212391] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11047273  1.22543003 -0.15932981 -1.93162983] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13498133  1.4218592  -0.1979624  -2.2691866 ] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00455177 -0.03261926  0.01491354 -0.02586098] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00520415 -0.22795187  0.01439632  0.27148978] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00976319 -0.03303828  0.01982612 -0.01661794] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01042396  0.16179382  0.01949376 -0.30298015] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00718808  0.3566326   0.01343416 -0.58945199] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-5.54272206e-05  5.51563887e-01  1.64511766e-03 -8.77873017e-01] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01097585  0.35641962 -0.01591234 -0.58467334] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01810424  0.55176081 -0.02760581 -0.88232607] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02913946  0.35702446 -0.04525233 -0.59844792] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03627995  0.16256392 -0.05722129 -0.32035536] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03953123  0.35845211 -0.0636284  -0.63052061] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04670027  0.16427301 -0.07623881 -0.3585355 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04998573  0.3603913  -0.08340952 -0.67425093] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05719356  0.16652154 -0.09689454 -0.40895125] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06052399  0.36287417 -0.10507356 -0.73054111] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06778147  0.55927927 -0.11968438 -1.05435878] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07896705  0.36592962 -0.14077156 -0.80151229] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08628565  0.56267254 -0.15680181 -1.1349577 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0975391   0.75945919 -0.17950096 -1.4724274 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11272828  0.56692803 -0.20894951 -1.24076111] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.02838949  0.01469405 -0.02280001  0.01267317] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02868337 -0.18009363 -0.02254655  0.29807612] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02508149 -0.37488705 -0.01658503  0.58356386] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01758375 -0.56977278 -0.00491375  0.87097647] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0061883  -0.76482755  0.01250578  1.16211047] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00910825 -0.56987069  0.03574799  0.87337464] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02050567 -0.76546     0.05321548  1.17707884] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03581487 -0.96123127  0.07675706  1.48595802] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05503949 -0.76712414  0.10647622  1.21819902] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07038198 -0.96344549  0.1308402   1.54225822] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08965089 -1.15987513  0.16168536  1.8727416 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11284839 -1.3563523   0.1991402   2.21094215] 1 1.0 True {}\n",
      "state, action, reward, done, info: [0.03756169 0.033154   0.00925591 0.00107984] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03822477 -0.16209946  0.00927751  0.29666869] 1 1.0 False {}\n",
      "state, action, reward, done, info: [0.03498279 0.03288901 0.01521088 0.00692612] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03564057 -0.16244774  0.01534941  0.30436918] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03239161 -0.35778504  0.02143679  0.60185316] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02523591 -0.55320019  0.03347385  0.90121046] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01417191 -0.7487593   0.05149806  1.20422428] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-8.03279981e-04 -5.54339465e-01  7.55825475e-02  9.28114893e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01189007 -0.36031485  0.09414485  0.66010899] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01909637 -0.16662039  0.10734703  0.39849164] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02242877 -0.36308843  0.11531686  0.7229968 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02969054 -0.55960053  0.12977679  1.04963528] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04088255 -0.36641658  0.1507695   0.8003435 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04821088 -0.1736486   0.16677637  0.55863023] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05168386 -0.37067045  0.17794897  0.89886776] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05909727 -0.17834841  0.19592633  0.66698151] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06266423  0.01358727  0.20926596  0.44182074] 0 1.0 True {}\n",
      "state, action, reward, done, info: [0.04539285 0.01107252 0.03817856 0.04003914] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0456143   0.20562679  0.03897935 -0.24035783] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04972684  0.40017087  0.03417219 -0.52049525] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05773026  0.20458495  0.02376228 -0.21724299] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06182196  0.39935929  0.01941742 -0.50233657] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06980914  0.20396911  0.00937069 -0.20359818] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07388852  0.3989558   0.00529873 -0.49331044] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08186764  0.59400262 -0.00456748 -0.78431874] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09374769  0.39894373 -0.02025385 -0.49307629] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10172657  0.20411321 -0.03011538 -0.20684468] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10580883  0.00943456 -0.03425227  0.07618831] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10599752 -0.18518004 -0.03272851  0.35787078] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10229392  0.01039154 -0.02557109  0.05505005] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10250175  0.20587063 -0.02447009 -0.24558981] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10661916  0.01110657 -0.02938189  0.03927534] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.1068413  -0.183582   -0.02859638  0.32254511] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10316966  0.01193525 -0.02214548  0.02098296] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10340836  0.20736767 -0.02172582 -0.27860404] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10755571  0.01256228 -0.0272979   0.00714819] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10780696 -0.18215776 -0.02715494  0.29109485] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10416381 -0.3768822  -0.02133304  0.57509126] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09662616 -0.18146778 -0.00983121  0.27576489] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09299681 -0.3764481  -0.00431592  0.5653309 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08546784 -0.18126587  0.0069907   0.2712914 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08184253 -0.37648687  0.01241653  0.566171  ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07431279 -0.57178079  0.02373995  0.86273967] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06287717 -0.37698997  0.04099474  0.57761466] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05533737 -0.5726618   0.05254704  0.88292489] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04388414 -0.7684565   0.07020554  1.19165328] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02851501 -0.96441425  0.0940386   1.50548931] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00922672 -0.77055046  0.12414839  1.24358508] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00618429 -0.57722082  0.14902009  0.9922285 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0177287  -0.77398846  0.16886466  1.32775875] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03320847 -0.97079061  0.19541983  1.66817366] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02603528  0.04569871  0.03990303 -0.01621584] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02512131  0.24022636  0.03957871 -0.29604667] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02031678  0.04456319  0.03365778  0.00885136] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01942552 -0.15102489  0.03383481  0.31196083] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02244602  0.0435991   0.04007402  0.03013746] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02157403 -0.15207394  0.04067677  0.33518998] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02461551  0.04244621  0.04738057  0.05560679] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02376659 -0.15332195  0.04849271  0.36285416] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02683303 -0.34909838  0.05574979  0.67042486] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.033815   -0.15479402  0.06915829  0.39580308] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03691088  0.03928201  0.07707435  0.12570127] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03612524 -0.15685465  0.07958837  0.44167027] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03926233 -0.35300735  0.08842178  0.75834223] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04632248 -0.15920793  0.10358862  0.49474091] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04950664  0.0343123   0.11348344  0.23641722] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04882039  0.22764544  0.11821179 -0.01842455] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04426748  0.03104404  0.1178433   0.30909285] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0436466  -0.16554256  0.12402515  0.63649341] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04695745 -0.36215598  0.13675502  0.96551816] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05420057 -0.55882339  0.15606538  1.2978458 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06537704 -0.7555442   0.1820223   1.63503794] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04043631 -0.04425819  0.04488115 -0.02477837] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04132147 -0.23999408  0.04438558  0.28172044] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04612135 -0.43572012  0.05001999  0.58806543] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05483576 -0.63150556  0.0617813   0.89607601] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06746587 -0.82740828  0.07970282  1.20752161] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08401403 -1.02346425  0.10385325  1.5240798 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10448332 -0.82973843  0.13433485  1.26553474] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12107809 -0.63656422  0.15964554  1.01775975] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13380937 -0.44388874  0.18000074  0.77915838] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14268715 -0.2516377   0.19558391  0.54807587] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1477199  -0.05972313  0.20654542  0.32282824] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.01977512 -0.01043634  0.03792584 -0.00178153] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01998385 -0.2060811   0.03789021  0.3026221 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02410547 -0.01151907  0.04394265  0.02212559] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02433585  0.18294605  0.04438516 -0.25637553] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02067693  0.37740712  0.03925765 -0.53473491] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01312879  0.57195566  0.02856296 -0.81479409] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00168967  0.76667508  0.01226707 -1.09835771] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01364383  0.96163341 -0.00970008 -1.38716677] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0328765   1.15687491 -0.03744342 -1.68286703] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.056014    1.35240987 -0.07110076 -1.98696968] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08306219  1.1581023  -0.11084015 -1.71713074] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10622424  0.96441218 -0.14518276 -1.46089875] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12551248  0.77133769 -0.17440074 -1.21686781] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14093924  0.96822596 -0.1987381  -1.55873453] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04992822 -0.04994402  0.01978038 -0.0339323 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04892934  0.14488877  0.01910173 -0.32030927] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05182712  0.33973355  0.01269554 -0.60690755] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05862179  0.14443641  0.00055739 -0.31025306] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06151052 -0.05069348 -0.00564767 -0.0173944 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06049665  0.14450901 -0.00599556 -0.31185387] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06338683 -0.05052701 -0.01223263 -0.02106777] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06237629  0.14476821 -0.01265399 -0.317585  ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06527165  0.34006809 -0.01900569 -0.61423153] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07207301  0.53545038 -0.03129032 -0.91283938] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08278202  0.73098135 -0.04954711 -1.21519012] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09740165  0.92670624 -0.07385091 -1.52297805] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.11593577  0.73254995 -0.10431047 -1.25422988] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.13058677  0.53890695 -0.12939507 -0.99595586] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14136491  0.34573054 -0.14931418 -0.74654972] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.14827952  0.15294955 -0.16424518 -0.5043312 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15133851  0.34995892 -0.1743318  -0.84393951] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.15833769  0.15758974 -0.19121059 -0.61075377] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.16148949 -0.03441805 -0.20342567 -0.38386512] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.00083394 -0.0011852   0.00964879 -0.00778098] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00085764 -0.19644419  0.00949317  0.28793057] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00478652 -0.0014589   0.01525178 -0.00174325] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0048157   0.19344104  0.01521691 -0.2895753 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00094688  0.38834273  0.00942541 -0.5774204 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00681997  0.58333131 -0.002123   -0.86711925] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0184866   0.77848209 -0.01946539 -1.16046892] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03405624  0.58361906 -0.04267476 -0.87395221] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04572862  0.77929444 -0.06015381 -1.17974062] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06131451  0.58500285 -0.08374862 -0.90650492] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.07301457  0.39110858 -0.10187872 -0.6412762 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08083674  0.5874921  -0.11470424 -0.96422462] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.09258658  0.78395257 -0.13398874 -1.29062795] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.10826563  0.98049945 -0.15980129 -1.62208071] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.12787562  1.17710167 -0.19224291 -1.96000853] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.0206947  -0.00522662  0.04088634  0.01445499] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02059017 -0.20091036  0.04117544  0.31975252] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01657196 -0.39659379  0.04757049  0.62513104] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00864008 -0.20216708  0.06007311  0.34780135] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00459674 -0.39808974  0.06702914  0.65880609] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00336505 -0.59407741  0.08020526  0.97181924] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0152466  -0.40011808  0.09964164  0.7053706 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02324896 -0.20650759  0.11374906  0.44564156] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02737911 -0.0131632   0.12266189  0.19086935] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02764238 -0.20980696  0.12647927  0.51959097] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03183852 -0.40646134  0.13687109  0.84930212] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03996774 -0.21344481  0.15385714  0.6025996 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04423664 -0.02077155  0.16590913  0.36205944] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04465207 -0.2178151   0.17315032  0.70212007] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04900837 -0.41486012  0.18719272  1.04392015] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05730558 -0.6119072   0.20807112  1.38904112] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02196907 -0.04694983 -0.045124    0.01163372] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02290806  0.14878924 -0.04489133 -0.29493797] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01993228 -0.04566492 -0.05079009 -0.0167442 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02084558  0.15014724 -0.05112497 -0.32500954] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01784263 -0.04421091 -0.05762516 -0.04887744] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01872685  0.15168796 -0.05860271 -0.35917042] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01569309 -0.04255408 -0.06578612 -0.08552686] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01654417  0.15344617 -0.06749666 -0.39821845] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01347525  0.34945751 -0.07546103 -0.71139493] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0064861   0.54553881 -0.08968892 -1.02684435] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00442468  0.74173284 -0.11022581 -1.34628574] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01925933  0.54815579 -0.13715153 -1.09002312] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03022245  0.35508182 -0.15895199 -0.84332801] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03732409  0.16244458 -0.17581855 -0.60455017] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04057298  0.35953281 -0.18790955 -0.9470497 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04776363  0.55661996 -0.20685055 -1.29239728] 1 1.0 True {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.01329251 -0.03931383 -0.00585371  0.03238091] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01407879  0.15589158 -0.0052061  -0.26214316] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01096096 -0.03915568 -0.01044896  0.02889317] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01174407 -0.23412624 -0.0098711   0.3182611 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0164266  -0.42910622 -0.00350587  0.60781477] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02500872 -0.62417898  0.00865042  0.89939139] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0374923  -0.81941709  0.02663825  1.19478078] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05388064 -1.01487365  0.05053386  1.49569233] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07417811 -0.82040125  0.08044771  1.21920676] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09058614 -0.6264032   0.10483185  0.95277694] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.1031142  -0.8227676   0.12388739  1.27647067] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11956956 -0.62942385  0.1494168   1.02500758] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.13215803 -0.82618528  0.16991695  1.36062654] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14868174 -1.02298027  0.19712948  1.70128346] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.034351   -0.03457996 -0.04646785  0.00023725] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0336594  -0.22900575 -0.04646311  0.27790451] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02907928 -0.03325281 -0.04090502 -0.02906325] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02841423 -0.22776501 -0.04148628  0.25043826] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02385893 -0.42227073 -0.03647752  0.52975233] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01541351 -0.61686105 -0.02588247  0.81072163] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00307629 -0.81161902 -0.00966804  1.09515215] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01315609 -0.61637107  0.012235    0.79945155] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02548351 -0.81165871  0.02822403  1.09595808] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04171668 -1.00714076  0.0501432   1.39736114] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0618595  -0.81267699  0.07809042  1.12076805] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.07811304 -0.61866122  0.10050578  0.85356654] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09048626 -0.81499893  0.11757711  1.17608587] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10678624 -1.01143553  0.14109883  1.50319447] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.12701495 -0.81827989  0.17116272  1.25768591] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.14338055 -1.01512818  0.19631644  1.59872014] 1 1.0 True {}\n",
      "state, action, reward, done, info: [-0.04983638  0.0136783   0.03830483  0.00179355] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04956282 -0.18197146  0.0383407   0.30631179] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05320225  0.01258376  0.04446694  0.02596284] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05295057 -0.18314675  0.0449862   0.33233735] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05661351  0.01130698  0.05163294  0.05417334] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05638737  0.20565202  0.05271641 -0.22178239] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.05227433  0.39998239  0.04828076 -0.49738155] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04427468  0.20421411  0.03833313 -0.18988229] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0401904   0.39876729  0.03453549 -0.47023049] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03221505  0.20317496  0.02513088 -0.16686552] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02815155  0.39792833  0.02179357 -0.45151563] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02019298  0.59273539  0.01276325 -0.73725004] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00833828  0.39743951 -0.00198175 -0.44057778] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00038949  0.20234566 -0.0107933  -0.14852022] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00365743  0.00737992 -0.01376371  0.14073819] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00380503  0.20269627 -0.01094894 -0.15625498] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00785895  0.39797326 -0.01407404 -0.45237183] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01581842  0.20305314 -0.02312148 -0.16415824] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01987948  0.00826967 -0.02640464  0.12114175] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02004487 -0.18646422 -0.02398181  0.40537871] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01631559 -0.38123802 -0.01587423  0.69040542] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00869083 -0.57613614 -0.00206613  0.97804893] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00283189 -0.77123032  0.01749485  1.27008215] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0182565  -0.57633606  0.0428965   0.98292871] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02978322 -0.7720057   0.06255507  1.28877088] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04522334 -0.96786514  0.08833049  1.6003646 ] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06458064 -1.16391524  0.12033778  1.91922868] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08785894 -1.36010811  0.15872235  2.24668371] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.11506111 -1.16679944  0.20365603  2.00683422] 0 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.03841113 -0.02238464 -0.00924393  0.04345583] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03796343 -0.21737282 -0.00837481  0.33320793] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03361598 -0.41237458 -0.00171065  0.62323814] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02536848 -0.21722878  0.01075411  0.33001695] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02102391 -0.02226156  0.01735445  0.04074467] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02057868 -0.21762801  0.01816934  0.33885217] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01622612 -0.02276925  0.02494639  0.05195377] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01577073 -0.21823986  0.02598546  0.35240192] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01140594 -0.41372151  0.0330335   0.65316427] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00313151 -0.60928751  0.04609679  0.95606331] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00905424 -0.41481484  0.06521805  0.67821184] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01735054 -0.61077931  0.07878229  0.99069416] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02956613 -0.80686222  0.09859617  1.30704427] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04570337 -1.00308585  0.12473706  1.62889056] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06576509 -0.80963137  0.15731487  1.37754109] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08195772 -0.6167848   0.18486569  1.13790332] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.09429341 -0.42449728  0.20762376  0.90842811] 0 1.0 True {}\n",
      "state, action, reward, done, info: [-0.02623716  0.02866692  0.04433077 -0.04961241] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02566383 -0.16706174  0.04333852  0.25672111] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02900506  0.02741553  0.04847295 -0.02198334] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02845675 -0.16836685  0.04803328  0.28559075] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03182409  0.02603832  0.05374509  0.00843569] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03130332  0.22034995  0.05391381 -0.26681744] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02689632  0.02450165  0.04857746  0.04237114] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02640629 -0.17128202  0.04942488  0.34997635] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02983193 -0.36707077  0.05642441  0.65782585] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03717335 -0.56293083  0.06958093  0.96772836] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.04843196 -0.75891461  0.08893549  1.28143297] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.06361025 -0.95504981  0.11456415  1.60058562] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.08271125 -1.15132706  0.14657586  1.92667973] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.10573779 -1.34768573  0.18510946  2.26099856] 1 1.0 True {}\n",
      "state, action, reward, done, info: [ 0.04559919  0.00379448 -0.04067171  0.02814658] 0 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    print('state, action, reward, done, info:', \n",
    "          state, action, reward, done, info)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 0.01335056,  0.02418345,  0.01333172, -0.03371267]),\n",
       "  0,\n",
       "  array([ 0.01383423, -0.17112712,  0.01265747,  0.26314656]),\n",
       "  1.0,\n",
       "  0.0],\n",
       " (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 float64\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.4166002006047873 -2.619995804883048\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, targetQs, action_size, hidden_size):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    #Qs = tf.reduce_max(actions_logits, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 4) actions:(1000,)\n",
      "action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:', np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 2\n",
    "state_size = 4\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100               # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:9.0000 R:9.0 loss:0.9672 exploreP:0.9991\n",
      "Episode:1 meanR:8.5000 R:8.0 loss:0.9733 exploreP:0.9983\n",
      "Episode:2 meanR:11.6667 R:18.0 loss:0.9733 exploreP:0.9965\n",
      "Episode:3 meanR:15.7500 R:28.0 loss:0.9925 exploreP:0.9938\n",
      "Episode:4 meanR:42.2000 R:148.0 loss:1.1189 exploreP:0.9793\n",
      "Episode:5 meanR:48.5000 R:80.0 loss:1.1456 exploreP:0.9716\n",
      "Episode:6 meanR:46.0000 R:31.0 loss:1.1919 exploreP:0.9686\n",
      "Episode:7 meanR:42.3750 R:17.0 loss:1.2483 exploreP:0.9670\n",
      "Episode:8 meanR:40.2222 R:23.0 loss:1.3709 exploreP:0.9648\n",
      "Episode:9 meanR:37.2000 R:10.0 loss:1.4133 exploreP:0.9638\n",
      "Episode:10 meanR:34.7273 R:10.0 loss:1.5101 exploreP:0.9629\n",
      "Episode:11 meanR:32.6667 R:10.0 loss:1.5010 exploreP:0.9619\n",
      "Episode:12 meanR:30.8462 R:9.0 loss:1.5377 exploreP:0.9611\n",
      "Episode:13 meanR:29.2143 R:8.0 loss:1.7463 exploreP:0.9603\n",
      "Episode:14 meanR:27.8000 R:8.0 loss:1.7460 exploreP:0.9596\n",
      "Episode:15 meanR:26.6875 R:10.0 loss:1.9559 exploreP:0.9586\n",
      "Episode:16 meanR:25.6471 R:9.0 loss:1.9099 exploreP:0.9578\n",
      "Episode:17 meanR:24.7778 R:10.0 loss:1.9807 exploreP:0.9568\n",
      "Episode:18 meanR:24.0000 R:10.0 loss:2.2774 exploreP:0.9559\n",
      "Episode:19 meanR:23.3000 R:10.0 loss:2.2794 exploreP:0.9549\n",
      "Episode:20 meanR:22.5714 R:8.0 loss:2.3076 exploreP:0.9542\n",
      "Episode:21 meanR:22.0000 R:10.0 loss:2.7455 exploreP:0.9532\n",
      "Episode:22 meanR:21.4348 R:9.0 loss:3.4186 exploreP:0.9524\n",
      "Episode:23 meanR:20.9167 R:9.0 loss:2.8100 exploreP:0.9515\n",
      "Episode:24 meanR:20.4800 R:10.0 loss:3.5937 exploreP:0.9506\n",
      "Episode:25 meanR:20.0769 R:10.0 loss:3.3223 exploreP:0.9496\n",
      "Episode:26 meanR:19.6667 R:9.0 loss:3.2370 exploreP:0.9488\n",
      "Episode:27 meanR:19.3214 R:10.0 loss:4.2221 exploreP:0.9479\n",
      "Episode:28 meanR:19.0000 R:10.0 loss:5.2057 exploreP:0.9469\n",
      "Episode:29 meanR:18.6333 R:8.0 loss:4.9554 exploreP:0.9462\n",
      "Episode:30 meanR:18.3548 R:10.0 loss:4.5736 exploreP:0.9452\n",
      "Episode:31 meanR:18.0938 R:10.0 loss:4.4828 exploreP:0.9443\n",
      "Episode:32 meanR:17.8485 R:10.0 loss:4.9546 exploreP:0.9434\n",
      "Episode:33 meanR:17.5588 R:8.0 loss:6.3316 exploreP:0.9426\n",
      "Episode:34 meanR:17.3429 R:10.0 loss:5.5750 exploreP:0.9417\n",
      "Episode:35 meanR:17.1111 R:9.0 loss:6.4351 exploreP:0.9409\n",
      "Episode:36 meanR:16.8919 R:9.0 loss:8.0215 exploreP:0.9400\n",
      "Episode:37 meanR:16.7105 R:10.0 loss:6.3348 exploreP:0.9391\n",
      "Episode:38 meanR:16.5385 R:10.0 loss:7.1563 exploreP:0.9382\n",
      "Episode:39 meanR:16.4000 R:11.0 loss:8.5710 exploreP:0.9371\n",
      "Episode:40 meanR:16.2195 R:9.0 loss:8.2231 exploreP:0.9363\n",
      "Episode:41 meanR:16.0476 R:9.0 loss:8.7509 exploreP:0.9355\n",
      "Episode:42 meanR:15.8837 R:9.0 loss:9.4850 exploreP:0.9346\n",
      "Episode:43 meanR:15.7273 R:9.0 loss:8.2144 exploreP:0.9338\n",
      "Episode:44 meanR:15.5778 R:9.0 loss:8.0969 exploreP:0.9330\n",
      "Episode:45 meanR:15.4348 R:9.0 loss:11.5436 exploreP:0.9321\n",
      "Episode:46 meanR:15.3191 R:10.0 loss:10.4717 exploreP:0.9312\n",
      "Episode:47 meanR:15.2292 R:11.0 loss:12.0687 exploreP:0.9302\n",
      "Episode:48 meanR:15.1020 R:9.0 loss:11.4485 exploreP:0.9294\n",
      "Episode:49 meanR:14.9600 R:8.0 loss:9.7848 exploreP:0.9286\n",
      "Episode:50 meanR:14.8627 R:10.0 loss:12.1383 exploreP:0.9277\n",
      "Episode:51 meanR:14.7500 R:9.0 loss:12.9615 exploreP:0.9269\n",
      "Episode:52 meanR:14.6792 R:11.0 loss:15.2971 exploreP:0.9259\n",
      "Episode:53 meanR:14.5556 R:8.0 loss:13.9598 exploreP:0.9252\n",
      "Episode:54 meanR:14.4364 R:8.0 loss:10.6716 exploreP:0.9244\n",
      "Episode:55 meanR:14.3571 R:10.0 loss:12.5129 exploreP:0.9235\n",
      "Episode:56 meanR:14.2807 R:10.0 loss:13.5243 exploreP:0.9226\n",
      "Episode:57 meanR:14.1724 R:8.0 loss:13.6206 exploreP:0.9219\n",
      "Episode:58 meanR:14.0678 R:8.0 loss:13.1415 exploreP:0.9211\n",
      "Episode:59 meanR:13.9833 R:9.0 loss:16.0934 exploreP:0.9203\n",
      "Episode:60 meanR:13.9016 R:9.0 loss:17.6450 exploreP:0.9195\n",
      "Episode:61 meanR:13.8065 R:8.0 loss:18.8755 exploreP:0.9188\n",
      "Episode:62 meanR:13.7302 R:9.0 loss:14.0725 exploreP:0.9180\n",
      "Episode:63 meanR:13.6562 R:9.0 loss:13.2122 exploreP:0.9171\n",
      "Episode:64 meanR:13.6154 R:11.0 loss:11.5043 exploreP:0.9162\n",
      "Episode:65 meanR:13.5606 R:10.0 loss:14.0676 exploreP:0.9152\n",
      "Episode:66 meanR:13.5075 R:10.0 loss:17.2551 exploreP:0.9143\n",
      "Episode:67 meanR:13.4412 R:9.0 loss:19.5046 exploreP:0.9135\n",
      "Episode:68 meanR:13.3768 R:9.0 loss:16.9208 exploreP:0.9127\n",
      "Episode:69 meanR:13.3286 R:10.0 loss:13.2862 exploreP:0.9118\n",
      "Episode:70 meanR:13.2817 R:10.0 loss:15.7654 exploreP:0.9109\n",
      "Episode:71 meanR:13.2222 R:9.0 loss:15.0175 exploreP:0.9101\n",
      "Episode:72 meanR:13.1644 R:9.0 loss:17.5537 exploreP:0.9093\n",
      "Episode:73 meanR:13.1081 R:9.0 loss:20.6742 exploreP:0.9085\n",
      "Episode:74 meanR:13.0667 R:10.0 loss:19.0211 exploreP:0.9076\n",
      "Episode:75 meanR:13.0132 R:9.0 loss:14.8893 exploreP:0.9068\n",
      "Episode:76 meanR:12.9610 R:9.0 loss:18.1462 exploreP:0.9060\n",
      "Episode:77 meanR:12.9103 R:9.0 loss:17.5051 exploreP:0.9052\n",
      "Episode:78 meanR:12.8734 R:10.0 loss:19.1931 exploreP:0.9043\n",
      "Episode:79 meanR:12.8375 R:10.0 loss:15.9998 exploreP:0.9034\n",
      "Episode:80 meanR:12.8025 R:10.0 loss:19.8248 exploreP:0.9025\n",
      "Episode:81 meanR:12.7683 R:10.0 loss:18.2098 exploreP:0.9016\n",
      "Episode:82 meanR:12.7349 R:10.0 loss:13.3696 exploreP:0.9007\n",
      "Episode:83 meanR:12.6905 R:9.0 loss:16.4440 exploreP:0.8999\n",
      "Episode:84 meanR:12.6353 R:8.0 loss:20.2063 exploreP:0.8992\n",
      "Episode:85 meanR:12.6047 R:10.0 loss:20.4646 exploreP:0.8983\n",
      "Episode:86 meanR:12.5747 R:10.0 loss:16.4777 exploreP:0.8974\n",
      "Episode:87 meanR:12.5341 R:9.0 loss:18.1172 exploreP:0.8966\n",
      "Episode:88 meanR:12.4944 R:9.0 loss:16.1025 exploreP:0.8958\n",
      "Episode:89 meanR:12.4556 R:9.0 loss:14.4702 exploreP:0.8950\n",
      "Episode:90 meanR:12.4286 R:10.0 loss:22.1901 exploreP:0.8941\n",
      "Episode:91 meanR:12.3913 R:9.0 loss:22.5807 exploreP:0.8933\n",
      "Episode:92 meanR:12.3548 R:9.0 loss:15.6119 exploreP:0.8925\n",
      "Episode:93 meanR:12.3298 R:10.0 loss:18.7170 exploreP:0.8917\n",
      "Episode:94 meanR:12.3053 R:10.0 loss:18.5284 exploreP:0.8908\n",
      "Episode:95 meanR:12.2604 R:8.0 loss:18.4442 exploreP:0.8901\n",
      "Episode:96 meanR:12.2371 R:10.0 loss:20.6397 exploreP:0.8892\n",
      "Episode:97 meanR:12.2041 R:9.0 loss:17.2459 exploreP:0.8884\n",
      "Episode:98 meanR:12.1818 R:10.0 loss:14.4022 exploreP:0.8875\n",
      "Episode:99 meanR:12.1500 R:9.0 loss:14.8359 exploreP:0.8867\n",
      "Episode:100 meanR:12.1600 R:10.0 loss:18.4993 exploreP:0.8859\n",
      "Episode:101 meanR:12.1900 R:11.0 loss:18.8188 exploreP:0.8849\n",
      "Episode:102 meanR:12.1000 R:9.0 loss:12.8298 exploreP:0.8841\n",
      "Episode:103 meanR:11.9100 R:9.0 loss:17.7106 exploreP:0.8833\n",
      "Episode:104 meanR:10.5300 R:10.0 loss:16.6436 exploreP:0.8824\n",
      "Episode:105 meanR:9.8800 R:15.0 loss:14.5896 exploreP:0.8811\n",
      "Episode:106 meanR:9.7600 R:19.0 loss:17.0786 exploreP:0.8795\n",
      "Episode:107 meanR:9.7500 R:16.0 loss:17.5375 exploreP:0.8781\n",
      "Episode:108 meanR:9.6300 R:11.0 loss:16.2112 exploreP:0.8771\n",
      "Episode:109 meanR:9.6500 R:12.0 loss:14.1662 exploreP:0.8761\n",
      "Episode:110 meanR:9.6700 R:12.0 loss:15.8341 exploreP:0.8751\n",
      "Episode:111 meanR:9.7000 R:13.0 loss:14.8095 exploreP:0.8739\n",
      "Episode:112 meanR:9.7200 R:11.0 loss:15.0808 exploreP:0.8730\n",
      "Episode:113 meanR:9.7900 R:15.0 loss:14.3392 exploreP:0.8717\n",
      "Episode:114 meanR:9.8200 R:11.0 loss:19.2804 exploreP:0.8708\n",
      "Episode:115 meanR:9.8400 R:12.0 loss:11.6793 exploreP:0.8697\n",
      "Episode:116 meanR:9.9100 R:16.0 loss:16.8230 exploreP:0.8683\n",
      "Episode:117 meanR:9.9400 R:13.0 loss:16.0183 exploreP:0.8672\n",
      "Episode:118 meanR:9.9600 R:12.0 loss:18.0703 exploreP:0.8662\n",
      "Episode:119 meanR:10.0200 R:16.0 loss:18.2094 exploreP:0.8648\n",
      "Episode:120 meanR:10.0700 R:13.0 loss:16.5354 exploreP:0.8637\n",
      "Episode:121 meanR:10.1200 R:15.0 loss:14.6366 exploreP:0.8624\n",
      "Episode:122 meanR:10.1700 R:14.0 loss:16.5298 exploreP:0.8612\n",
      "Episode:123 meanR:10.2300 R:15.0 loss:16.0614 exploreP:0.8600\n",
      "Episode:124 meanR:10.2700 R:14.0 loss:14.7523 exploreP:0.8588\n",
      "Episode:125 meanR:10.3500 R:18.0 loss:16.0207 exploreP:0.8573\n",
      "Episode:126 meanR:10.4400 R:18.0 loss:13.1367 exploreP:0.8557\n",
      "Episode:127 meanR:10.5400 R:20.0 loss:17.2811 exploreP:0.8540\n",
      "Episode:128 meanR:10.6800 R:24.0 loss:13.8596 exploreP:0.8520\n",
      "Episode:129 meanR:10.8400 R:24.0 loss:16.9871 exploreP:0.8500\n",
      "Episode:130 meanR:10.9100 R:17.0 loss:17.1548 exploreP:0.8486\n",
      "Episode:131 meanR:11.0400 R:23.0 loss:14.8423 exploreP:0.8466\n",
      "Episode:132 meanR:11.2000 R:26.0 loss:15.5219 exploreP:0.8445\n",
      "Episode:133 meanR:11.3200 R:20.0 loss:16.6109 exploreP:0.8428\n",
      "Episode:134 meanR:11.4400 R:22.0 loss:18.5314 exploreP:0.8410\n",
      "Episode:135 meanR:11.6000 R:25.0 loss:12.5558 exploreP:0.8389\n",
      "Episode:136 meanR:11.7500 R:24.0 loss:13.0184 exploreP:0.8369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:137 meanR:11.9800 R:33.0 loss:17.4014 exploreP:0.8342\n",
      "Episode:138 meanR:12.2400 R:36.0 loss:15.0728 exploreP:0.8312\n",
      "Episode:139 meanR:12.4200 R:29.0 loss:15.1546 exploreP:0.8289\n",
      "Episode:140 meanR:12.6600 R:33.0 loss:17.3517 exploreP:0.8262\n",
      "Episode:141 meanR:13.1300 R:56.0 loss:17.3563 exploreP:0.8216\n",
      "Episode:142 meanR:13.4500 R:41.0 loss:16.3160 exploreP:0.8183\n",
      "Episode:143 meanR:13.6700 R:31.0 loss:17.4247 exploreP:0.8158\n",
      "Episode:144 meanR:14.2700 R:69.0 loss:16.2656 exploreP:0.8102\n",
      "Episode:145 meanR:14.5700 R:39.0 loss:17.6684 exploreP:0.8071\n",
      "Episode:146 meanR:14.8400 R:37.0 loss:18.6422 exploreP:0.8042\n",
      "Episode:147 meanR:14.9600 R:23.0 loss:16.5591 exploreP:0.8024\n",
      "Episode:148 meanR:15.1400 R:27.0 loss:22.2811 exploreP:0.8002\n",
      "Episode:149 meanR:15.5400 R:48.0 loss:20.9096 exploreP:0.7964\n",
      "Episode:150 meanR:15.9700 R:53.0 loss:19.1125 exploreP:0.7923\n",
      "Episode:151 meanR:16.3700 R:49.0 loss:20.9581 exploreP:0.7885\n",
      "Episode:152 meanR:16.5400 R:28.0 loss:20.5374 exploreP:0.7863\n",
      "Episode:153 meanR:16.8900 R:43.0 loss:23.0122 exploreP:0.7829\n",
      "Episode:154 meanR:17.5300 R:72.0 loss:20.6310 exploreP:0.7774\n",
      "Episode:155 meanR:17.9800 R:55.0 loss:21.6063 exploreP:0.7732\n",
      "Episode:156 meanR:18.6100 R:73.0 loss:22.5580 exploreP:0.7676\n",
      "Episode:157 meanR:19.1300 R:60.0 loss:22.9058 exploreP:0.7631\n",
      "Episode:158 meanR:19.5200 R:47.0 loss:19.8559 exploreP:0.7596\n",
      "Episode:159 meanR:19.9200 R:49.0 loss:22.5039 exploreP:0.7559\n",
      "Episode:160 meanR:20.3600 R:53.0 loss:24.0310 exploreP:0.7520\n",
      "Episode:161 meanR:20.8300 R:55.0 loss:26.1144 exploreP:0.7479\n",
      "Episode:162 meanR:21.3200 R:58.0 loss:23.6006 exploreP:0.7436\n",
      "Episode:163 meanR:21.5200 R:29.0 loss:25.7373 exploreP:0.7415\n",
      "Episode:164 meanR:21.9800 R:57.0 loss:23.2606 exploreP:0.7373\n",
      "Episode:165 meanR:22.5700 R:69.0 loss:25.6876 exploreP:0.7323\n",
      "Episode:166 meanR:23.3600 R:89.0 loss:27.2443 exploreP:0.7259\n",
      "Episode:167 meanR:24.2300 R:96.0 loss:28.1762 exploreP:0.7191\n",
      "Episode:168 meanR:24.8200 R:68.0 loss:26.9815 exploreP:0.7143\n",
      "Episode:169 meanR:25.4700 R:75.0 loss:24.7570 exploreP:0.7090\n",
      "Episode:170 meanR:25.8800 R:51.0 loss:30.8175 exploreP:0.7055\n",
      "Episode:171 meanR:26.4000 R:61.0 loss:28.1775 exploreP:0.7013\n",
      "Episode:172 meanR:26.9000 R:59.0 loss:25.9409 exploreP:0.6972\n",
      "Episode:173 meanR:27.3200 R:51.0 loss:26.8528 exploreP:0.6937\n",
      "Episode:174 meanR:28.8800 R:166.0 loss:28.8896 exploreP:0.6824\n",
      "Episode:175 meanR:29.5900 R:80.0 loss:29.9916 exploreP:0.6771\n",
      "Episode:176 meanR:30.5400 R:104.0 loss:27.2769 exploreP:0.6702\n",
      "Episode:177 meanR:31.1700 R:72.0 loss:33.9435 exploreP:0.6654\n",
      "Episode:178 meanR:32.0200 R:95.0 loss:30.6294 exploreP:0.6592\n",
      "Episode:179 meanR:33.3800 R:146.0 loss:30.9840 exploreP:0.6498\n",
      "Episode:180 meanR:33.9500 R:67.0 loss:32.7406 exploreP:0.6456\n",
      "Episode:181 meanR:35.0800 R:123.0 loss:33.1146 exploreP:0.6378\n",
      "Episode:182 meanR:35.5700 R:59.0 loss:33.0603 exploreP:0.6341\n",
      "Episode:183 meanR:36.1100 R:63.0 loss:32.6029 exploreP:0.6302\n",
      "Episode:184 meanR:37.0800 R:105.0 loss:33.1574 exploreP:0.6237\n",
      "Episode:185 meanR:37.9100 R:93.0 loss:30.4876 exploreP:0.6180\n",
      "Episode:186 meanR:38.6900 R:88.0 loss:29.8050 exploreP:0.6127\n",
      "Episode:187 meanR:39.5000 R:90.0 loss:34.7999 exploreP:0.6073\n",
      "Episode:188 meanR:40.2400 R:83.0 loss:35.7283 exploreP:0.6024\n",
      "Episode:189 meanR:40.7900 R:64.0 loss:36.3643 exploreP:0.5986\n",
      "Episode:190 meanR:41.1500 R:46.0 loss:33.7013 exploreP:0.5959\n",
      "Episode:191 meanR:42.0700 R:101.0 loss:34.2476 exploreP:0.5900\n",
      "Episode:192 meanR:43.2900 R:131.0 loss:35.2347 exploreP:0.5824\n",
      "Episode:193 meanR:44.0200 R:83.0 loss:35.3174 exploreP:0.5777\n",
      "Episode:194 meanR:45.1200 R:120.0 loss:33.6018 exploreP:0.5709\n",
      "Episode:195 meanR:45.9700 R:93.0 loss:36.3759 exploreP:0.5657\n",
      "Episode:196 meanR:46.5600 R:69.0 loss:30.2439 exploreP:0.5619\n",
      "Episode:197 meanR:47.7600 R:129.0 loss:35.4305 exploreP:0.5548\n",
      "Episode:198 meanR:48.7400 R:108.0 loss:33.9893 exploreP:0.5490\n",
      "Episode:199 meanR:49.3000 R:65.0 loss:30.6664 exploreP:0.5455\n",
      "Episode:200 meanR:50.9400 R:174.0 loss:32.4869 exploreP:0.5363\n",
      "Episode:201 meanR:52.6400 R:181.0 loss:31.5395 exploreP:0.5268\n",
      "Episode:202 meanR:53.1900 R:64.0 loss:38.7401 exploreP:0.5235\n",
      "Episode:203 meanR:54.8000 R:170.0 loss:30.8077 exploreP:0.5149\n",
      "Episode:204 meanR:55.4000 R:70.0 loss:34.0124 exploreP:0.5114\n",
      "Episode:205 meanR:57.3200 R:207.0 loss:31.7921 exploreP:0.5011\n",
      "Episode:206 meanR:58.4400 R:131.0 loss:29.8069 exploreP:0.4947\n",
      "Episode:207 meanR:60.8800 R:260.0 loss:30.7707 exploreP:0.4822\n",
      "Episode:208 meanR:63.7300 R:296.0 loss:26.5859 exploreP:0.4685\n",
      "Episode:209 meanR:65.1500 R:154.0 loss:25.4629 exploreP:0.4615\n",
      "Episode:210 meanR:67.9500 R:292.0 loss:25.1355 exploreP:0.4485\n",
      "Episode:211 meanR:69.9300 R:211.0 loss:24.7623 exploreP:0.4393\n",
      "Episode:212 meanR:70.8400 R:102.0 loss:23.3829 exploreP:0.4350\n",
      "Episode:213 meanR:75.6900 R:500.0 loss:20.8290 exploreP:0.4142\n",
      "Episode:214 meanR:77.6800 R:210.0 loss:19.8411 exploreP:0.4058\n",
      "Episode:215 meanR:79.6200 R:206.0 loss:19.7568 exploreP:0.3978\n",
      "Episode:216 meanR:84.4600 R:500.0 loss:17.8421 exploreP:0.3789\n",
      "Episode:217 meanR:85.1700 R:84.0 loss:15.1699 exploreP:0.3758\n",
      "Episode:218 meanR:85.9500 R:90.0 loss:16.2882 exploreP:0.3725\n",
      "Episode:219 meanR:87.9400 R:215.0 loss:14.4493 exploreP:0.3648\n",
      "Episode:220 meanR:92.8100 R:500.0 loss:15.0184 exploreP:0.3475\n",
      "Episode:221 meanR:94.3400 R:168.0 loss:14.4667 exploreP:0.3419\n",
      "Episode:222 meanR:96.9800 R:278.0 loss:11.9452 exploreP:0.3328\n",
      "Episode:223 meanR:98.1400 R:131.0 loss:11.2063 exploreP:0.3286\n",
      "Episode:224 meanR:98.9400 R:94.0 loss:12.9964 exploreP:0.3256\n",
      "Episode:225 meanR:100.1200 R:136.0 loss:13.4814 exploreP:0.3213\n",
      "Episode:226 meanR:101.5900 R:165.0 loss:11.9893 exploreP:0.3162\n",
      "Episode:227 meanR:103.1900 R:180.0 loss:9.3397 exploreP:0.3108\n",
      "Episode:228 meanR:103.8500 R:90.0 loss:10.4347 exploreP:0.3081\n",
      "Episode:229 meanR:106.0200 R:241.0 loss:8.4328 exploreP:0.3010\n",
      "Episode:230 meanR:107.4800 R:163.0 loss:6.8575 exploreP:0.2963\n",
      "Episode:231 meanR:111.1400 R:389.0 loss:6.4595 exploreP:0.2853\n",
      "Episode:232 meanR:112.1300 R:125.0 loss:4.6496 exploreP:0.2819\n",
      "Episode:233 meanR:112.6200 R:69.0 loss:6.0701 exploreP:0.2800\n",
      "Episode:234 meanR:113.3300 R:93.0 loss:4.8052 exploreP:0.2775\n",
      "Episode:235 meanR:114.4900 R:141.0 loss:4.9594 exploreP:0.2738\n",
      "Episode:236 meanR:115.0900 R:84.0 loss:4.6637 exploreP:0.2716\n",
      "Episode:237 meanR:116.7600 R:200.0 loss:4.9605 exploreP:0.2664\n",
      "Episode:238 meanR:121.4000 R:500.0 loss:6.5572 exploreP:0.2539\n",
      "Episode:239 meanR:123.0700 R:196.0 loss:11.2854 exploreP:0.2492\n",
      "Episode:240 meanR:123.4100 R:67.0 loss:13.4338 exploreP:0.2476\n",
      "Episode:241 meanR:125.0700 R:222.0 loss:18.7659 exploreP:0.2424\n",
      "Episode:242 meanR:125.3900 R:73.0 loss:22.4518 exploreP:0.2407\n",
      "Episode:243 meanR:130.0800 R:500.0 loss:39.2668 exploreP:0.2294\n",
      "Episode:244 meanR:130.4300 R:104.0 loss:65.6729 exploreP:0.2272\n",
      "Episode:245 meanR:132.3800 R:234.0 loss:89.1064 exploreP:0.2221\n",
      "Episode:246 meanR:137.0100 R:500.0 loss:129.5652 exploreP:0.2118\n",
      "Episode:247 meanR:138.0400 R:126.0 loss:190.9392 exploreP:0.2093\n",
      "Episode:248 meanR:138.4800 R:71.0 loss:208.8297 exploreP:0.2078\n",
      "Episode:249 meanR:138.7700 R:77.0 loss:238.8792 exploreP:0.2063\n",
      "Episode:250 meanR:143.2400 R:500.0 loss:307.8026 exploreP:0.1968\n",
      "Episode:251 meanR:143.7000 R:95.0 loss:396.4314 exploreP:0.1950\n",
      "Episode:252 meanR:144.3600 R:94.0 loss:466.9376 exploreP:0.1933\n",
      "Episode:253 meanR:148.4000 R:447.0 loss:586.2009 exploreP:0.1852\n",
      "Episode:254 meanR:148.6100 R:93.0 loss:729.9265 exploreP:0.1836\n",
      "Episode:255 meanR:153.0600 R:500.0 loss:942.1768 exploreP:0.1752\n",
      "Episode:256 meanR:153.0500 R:72.0 loss:1103.6356 exploreP:0.1740\n",
      "Episode:257 meanR:155.7400 R:329.0 loss:1316.3105 exploreP:0.1687\n",
      "Episode:258 meanR:157.3800 R:211.0 loss:1397.0120 exploreP:0.1654\n",
      "Episode:259 meanR:157.9200 R:103.0 loss:1639.4326 exploreP:0.1638\n",
      "Episode:260 meanR:158.2000 R:81.0 loss:1632.4221 exploreP:0.1625\n",
      "Episode:261 meanR:159.5600 R:191.0 loss:1681.2819 exploreP:0.1596\n",
      "Episode:262 meanR:160.1900 R:121.0 loss:1852.1318 exploreP:0.1578\n",
      "Episode:263 meanR:160.5000 R:60.0 loss:1809.1616 exploreP:0.1570\n",
      "Episode:264 meanR:161.3600 R:143.0 loss:1975.6989 exploreP:0.1549\n",
      "Episode:265 meanR:162.2500 R:158.0 loss:2111.8396 exploreP:0.1526\n",
      "Episode:266 meanR:166.3600 R:500.0 loss:2100.8018 exploreP:0.1456\n",
      "Episode:267 meanR:170.4000 R:500.0 loss:2086.6274 exploreP:0.1390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:268 meanR:174.7200 R:500.0 loss:2048.4980 exploreP:0.1327\n",
      "Episode:269 meanR:178.9700 R:500.0 loss:2088.4614 exploreP:0.1267\n",
      "Episode:270 meanR:183.4600 R:500.0 loss:1863.2841 exploreP:0.1211\n",
      "Episode:271 meanR:187.8500 R:500.0 loss:1714.3323 exploreP:0.1156\n",
      "Episode:272 meanR:192.2600 R:500.0 loss:1250.5732 exploreP:0.1105\n",
      "Episode:273 meanR:196.7500 R:500.0 loss:795.5960 exploreP:0.1056\n",
      "Episode:274 meanR:200.0900 R:500.0 loss:535.1031 exploreP:0.1009\n",
      "Episode:275 meanR:204.2900 R:500.0 loss:445.7021 exploreP:0.0965\n",
      "Episode:276 meanR:208.2500 R:500.0 loss:356.9906 exploreP:0.0923\n",
      "Episode:277 meanR:212.5300 R:500.0 loss:287.7647 exploreP:0.0883\n",
      "Episode:278 meanR:216.5800 R:500.0 loss:307.6475 exploreP:0.0844\n",
      "Episode:279 meanR:220.1200 R:500.0 loss:253.4693 exploreP:0.0808\n",
      "Episode:280 meanR:224.4500 R:500.0 loss:256.5277 exploreP:0.0774\n",
      "Episode:281 meanR:228.2200 R:500.0 loss:239.6517 exploreP:0.0741\n",
      "Episode:282 meanR:232.6300 R:500.0 loss:255.9679 exploreP:0.0709\n",
      "Episode:283 meanR:237.0000 R:500.0 loss:218.7220 exploreP:0.0680\n",
      "Episode:284 meanR:240.9500 R:500.0 loss:172.1443 exploreP:0.0651\n",
      "Episode:285 meanR:245.0200 R:500.0 loss:76.3460 exploreP:0.0625\n",
      "Episode:286 meanR:249.1400 R:500.0 loss:12.4775 exploreP:0.0599\n",
      "Episode:287 meanR:253.2400 R:500.0 loss:13.1600 exploreP:0.0575\n",
      "Episode:288 meanR:257.4100 R:500.0 loss:15.2634 exploreP:0.0552\n",
      "Episode:289 meanR:261.7700 R:500.0 loss:13.4045 exploreP:0.0529\n",
      "Episode:290 meanR:266.3100 R:500.0 loss:13.9352 exploreP:0.0509\n",
      "Episode:291 meanR:270.3000 R:500.0 loss:11.9090 exploreP:0.0489\n",
      "Episode:292 meanR:273.9900 R:500.0 loss:14.6601 exploreP:0.0470\n",
      "Episode:293 meanR:278.1600 R:500.0 loss:12.1816 exploreP:0.0452\n",
      "Episode:294 meanR:281.9600 R:500.0 loss:12.6476 exploreP:0.0434\n",
      "Episode:295 meanR:286.0300 R:500.0 loss:12.7773 exploreP:0.0418\n",
      "Episode:296 meanR:290.3400 R:500.0 loss:12.0993 exploreP:0.0403\n",
      "Episode:297 meanR:294.0500 R:500.0 loss:14.4325 exploreP:0.0388\n",
      "Episode:298 meanR:297.8400 R:487.0 loss:12.8848 exploreP:0.0374\n",
      "Episode:299 meanR:302.1900 R:500.0 loss:24.9358 exploreP:0.0361\n",
      "Episode:300 meanR:305.4500 R:500.0 loss:24.6624 exploreP:0.0348\n",
      "Episode:301 meanR:308.6400 R:500.0 loss:26.3421 exploreP:0.0336\n",
      "Episode:302 meanR:313.0000 R:500.0 loss:24.7761 exploreP:0.0325\n",
      "Episode:303 meanR:316.3000 R:500.0 loss:27.1648 exploreP:0.0314\n",
      "Episode:304 meanR:320.6000 R:500.0 loss:25.8673 exploreP:0.0303\n",
      "Episode:305 meanR:323.5300 R:500.0 loss:30.6078 exploreP:0.0293\n",
      "Episode:306 meanR:327.2200 R:500.0 loss:24.1983 exploreP:0.0284\n",
      "Episode:307 meanR:329.6200 R:500.0 loss:23.0195 exploreP:0.0275\n",
      "Episode:308 meanR:331.6600 R:500.0 loss:29.5836 exploreP:0.0266\n",
      "Episode:309 meanR:335.1200 R:500.0 loss:27.6323 exploreP:0.0258\n",
      "Episode:310 meanR:337.2000 R:500.0 loss:25.2424 exploreP:0.0250\n",
      "Episode:311 meanR:340.0900 R:500.0 loss:24.1953 exploreP:0.0243\n",
      "Episode:312 meanR:344.0700 R:500.0 loss:24.7067 exploreP:0.0236\n",
      "Episode:313 meanR:344.0700 R:500.0 loss:25.6653 exploreP:0.0230\n",
      "Episode:314 meanR:346.9700 R:500.0 loss:24.2132 exploreP:0.0223\n",
      "Episode:315 meanR:349.9100 R:500.0 loss:24.4797 exploreP:0.0217\n",
      "Episode:316 meanR:349.9100 R:500.0 loss:20.4227 exploreP:0.0211\n",
      "Episode:317 meanR:354.0700 R:500.0 loss:18.7828 exploreP:0.0206\n",
      "Episode:318 meanR:358.1700 R:500.0 loss:17.2398 exploreP:0.0201\n",
      "Episode:319 meanR:361.0200 R:500.0 loss:15.9807 exploreP:0.0196\n",
      "Episode:320 meanR:361.0200 R:500.0 loss:14.9381 exploreP:0.0191\n",
      "Episode:321 meanR:364.3400 R:500.0 loss:15.1222 exploreP:0.0187\n",
      "Episode:322 meanR:366.5600 R:500.0 loss:13.7421 exploreP:0.0183\n",
      "Episode:323 meanR:370.2500 R:500.0 loss:15.6686 exploreP:0.0179\n",
      "Episode:324 meanR:374.3100 R:500.0 loss:14.8651 exploreP:0.0175\n",
      "Episode:325 meanR:377.9500 R:500.0 loss:17.1428 exploreP:0.0171\n",
      "Episode:326 meanR:381.3000 R:500.0 loss:16.3731 exploreP:0.0168\n",
      "Episode:327 meanR:384.5000 R:500.0 loss:16.5026 exploreP:0.0164\n",
      "Episode:328 meanR:388.6000 R:500.0 loss:15.4171 exploreP:0.0161\n",
      "Episode:329 meanR:391.1900 R:500.0 loss:15.2785 exploreP:0.0158\n",
      "Episode:330 meanR:394.5600 R:500.0 loss:14.0333 exploreP:0.0155\n",
      "Episode:331 meanR:395.6700 R:500.0 loss:15.9212 exploreP:0.0153\n",
      "Episode:332 meanR:399.4200 R:500.0 loss:12.9101 exploreP:0.0150\n",
      "Episode:333 meanR:403.7300 R:500.0 loss:18.4421 exploreP:0.0148\n",
      "Episode:334 meanR:407.8000 R:500.0 loss:16.6769 exploreP:0.0145\n",
      "Episode:335 meanR:411.3900 R:500.0 loss:17.7205 exploreP:0.0143\n",
      "Episode:336 meanR:415.5500 R:500.0 loss:20.6743 exploreP:0.0141\n",
      "Episode:337 meanR:418.5500 R:500.0 loss:18.9627 exploreP:0.0139\n",
      "Episode:338 meanR:418.5500 R:500.0 loss:16.7814 exploreP:0.0137\n",
      "Episode:339 meanR:421.5900 R:500.0 loss:13.6150 exploreP:0.0135\n",
      "Episode:340 meanR:425.9200 R:500.0 loss:12.3766 exploreP:0.0134\n",
      "Episode:341 meanR:428.7000 R:500.0 loss:12.5132 exploreP:0.0132\n",
      "Episode:342 meanR:432.9700 R:500.0 loss:13.1258 exploreP:0.0130\n",
      "Episode:343 meanR:432.9700 R:500.0 loss:12.2475 exploreP:0.0129\n",
      "Episode:344 meanR:436.9300 R:500.0 loss:12.3390 exploreP:0.0127\n",
      "Episode:345 meanR:439.5900 R:500.0 loss:11.5953 exploreP:0.0126\n",
      "Episode:346 meanR:439.5900 R:500.0 loss:11.9540 exploreP:0.0125\n",
      "Episode:347 meanR:443.3300 R:500.0 loss:13.3615 exploreP:0.0124\n",
      "Episode:348 meanR:447.6200 R:500.0 loss:13.0290 exploreP:0.0123\n",
      "Episode:349 meanR:451.8500 R:500.0 loss:13.8061 exploreP:0.0121\n",
      "Episode:350 meanR:451.8500 R:500.0 loss:12.1119 exploreP:0.0120\n",
      "Episode:351 meanR:455.9000 R:500.0 loss:12.3492 exploreP:0.0119\n",
      "Episode:352 meanR:459.9600 R:500.0 loss:11.0899 exploreP:0.0118\n",
      "Episode:353 meanR:460.4900 R:500.0 loss:11.1053 exploreP:0.0118\n",
      "Episode:354 meanR:464.5600 R:500.0 loss:10.9776 exploreP:0.0117\n",
      "Episode:355 meanR:464.5600 R:500.0 loss:13.9273 exploreP:0.0116\n",
      "Episode:356 meanR:468.8400 R:500.0 loss:14.2910 exploreP:0.0115\n",
      "Episode:357 meanR:470.5500 R:500.0 loss:11.5711 exploreP:0.0114\n",
      "Episode:358 meanR:473.4400 R:500.0 loss:13.0181 exploreP:0.0114\n",
      "Episode:359 meanR:477.4100 R:500.0 loss:11.8281 exploreP:0.0113\n",
      "Episode:360 meanR:481.6000 R:500.0 loss:13.8833 exploreP:0.0112\n",
      "Episode:361 meanR:484.6900 R:500.0 loss:12.7291 exploreP:0.0112\n",
      "Episode:362 meanR:488.4800 R:500.0 loss:12.1157 exploreP:0.0111\n",
      "Episode:363 meanR:492.8800 R:500.0 loss:12.3461 exploreP:0.0111\n",
      "Episode:364 meanR:496.4500 R:500.0 loss:15.6346 exploreP:0.0110\n",
      "Episode:365 meanR:499.8700 R:500.0 loss:12.8692 exploreP:0.0110\n",
      "Episode:366 meanR:499.8700 R:500.0 loss:12.5243 exploreP:0.0109\n",
      "Episode:367 meanR:499.8700 R:500.0 loss:13.2744 exploreP:0.0109\n",
      "Episode:368 meanR:499.8700 R:500.0 loss:15.1069 exploreP:0.0108\n",
      "Episode:369 meanR:499.8700 R:500.0 loss:15.8413 exploreP:0.0108\n",
      "Episode:370 meanR:499.8700 R:500.0 loss:20.0623 exploreP:0.0107\n",
      "Episode:371 meanR:499.8700 R:500.0 loss:16.9382 exploreP:0.0107\n",
      "Episode:372 meanR:499.8700 R:500.0 loss:17.6143 exploreP:0.0107\n",
      "Episode:373 meanR:499.8700 R:500.0 loss:14.6921 exploreP:0.0106\n",
      "Episode:374 meanR:499.8700 R:500.0 loss:16.1902 exploreP:0.0106\n",
      "Episode:375 meanR:499.8700 R:500.0 loss:17.8008 exploreP:0.0106\n",
      "Episode:376 meanR:499.8700 R:500.0 loss:14.8547 exploreP:0.0106\n",
      "Episode:377 meanR:499.8700 R:500.0 loss:13.8632 exploreP:0.0105\n",
      "Episode:378 meanR:499.8700 R:500.0 loss:16.6324 exploreP:0.0105\n",
      "Episode:379 meanR:499.8700 R:500.0 loss:14.6312 exploreP:0.0105\n",
      "Episode:380 meanR:499.8700 R:500.0 loss:14.5475 exploreP:0.0105\n",
      "Episode:381 meanR:499.8700 R:500.0 loss:17.7823 exploreP:0.0104\n",
      "Episode:382 meanR:499.8700 R:500.0 loss:12.1576 exploreP:0.0104\n",
      "Episode:383 meanR:499.8700 R:500.0 loss:13.6915 exploreP:0.0104\n",
      "Episode:384 meanR:499.8700 R:500.0 loss:16.5905 exploreP:0.0104\n",
      "Episode:385 meanR:499.8700 R:500.0 loss:17.7880 exploreP:0.0104\n",
      "Episode:386 meanR:495.5300 R:66.0 loss:16.8634 exploreP:0.0104\n",
      "Episode:387 meanR:495.5300 R:500.0 loss:16.4687 exploreP:0.0103\n",
      "Episode:388 meanR:495.5300 R:500.0 loss:16.3971 exploreP:0.0103\n",
      "Episode:389 meanR:495.5300 R:500.0 loss:14.6302 exploreP:0.0103\n",
      "Episode:390 meanR:495.5300 R:500.0 loss:15.8919 exploreP:0.0103\n",
      "Episode:391 meanR:495.5300 R:500.0 loss:14.6535 exploreP:0.0103\n",
      "Episode:392 meanR:495.5300 R:500.0 loss:14.2361 exploreP:0.0103\n",
      "Episode:393 meanR:495.5300 R:500.0 loss:15.5627 exploreP:0.0102\n",
      "Episode:394 meanR:495.5300 R:500.0 loss:15.6892 exploreP:0.0102\n",
      "Episode:395 meanR:495.5300 R:500.0 loss:19.2458 exploreP:0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:396 meanR:495.5300 R:500.0 loss:15.8455 exploreP:0.0102\n",
      "Episode:397 meanR:495.5300 R:500.0 loss:14.9503 exploreP:0.0102\n",
      "Episode:398 meanR:495.6600 R:500.0 loss:16.2248 exploreP:0.0102\n",
      "Episode:399 meanR:495.6600 R:500.0 loss:17.6666 exploreP:0.0102\n",
      "Episode:400 meanR:495.6600 R:500.0 loss:18.6294 exploreP:0.0102\n",
      "Episode:401 meanR:495.6600 R:500.0 loss:13.3931 exploreP:0.0102\n",
      "Episode:402 meanR:495.6600 R:500.0 loss:14.8358 exploreP:0.0102\n",
      "Episode:403 meanR:495.6600 R:500.0 loss:15.9419 exploreP:0.0102\n",
      "Episode:404 meanR:495.6600 R:500.0 loss:15.6859 exploreP:0.0101\n",
      "Episode:405 meanR:495.6600 R:500.0 loss:15.9385 exploreP:0.0101\n",
      "Episode:406 meanR:495.6600 R:500.0 loss:14.7938 exploreP:0.0101\n",
      "Episode:407 meanR:495.6600 R:500.0 loss:13.7874 exploreP:0.0101\n",
      "Episode:408 meanR:495.6600 R:500.0 loss:15.3504 exploreP:0.0101\n",
      "Episode:409 meanR:495.6600 R:500.0 loss:14.5296 exploreP:0.0101\n",
      "Episode:410 meanR:495.6600 R:500.0 loss:14.3065 exploreP:0.0101\n",
      "Episode:411 meanR:495.6600 R:500.0 loss:14.4257 exploreP:0.0101\n",
      "Episode:412 meanR:495.6600 R:500.0 loss:15.6897 exploreP:0.0101\n",
      "Episode:413 meanR:495.6600 R:500.0 loss:13.0144 exploreP:0.0101\n",
      "Episode:414 meanR:495.6600 R:500.0 loss:16.2950 exploreP:0.0101\n",
      "Episode:415 meanR:495.6600 R:500.0 loss:15.8383 exploreP:0.0101\n",
      "Episode:416 meanR:495.6600 R:500.0 loss:16.4163 exploreP:0.0101\n",
      "Episode:417 meanR:495.6600 R:500.0 loss:15.7391 exploreP:0.0101\n",
      "Episode:418 meanR:495.6600 R:500.0 loss:15.8256 exploreP:0.0101\n",
      "Episode:419 meanR:495.6600 R:500.0 loss:17.5628 exploreP:0.0101\n",
      "Episode:420 meanR:495.6600 R:500.0 loss:14.8222 exploreP:0.0101\n",
      "Episode:421 meanR:495.6600 R:500.0 loss:17.4185 exploreP:0.0101\n",
      "Episode:422 meanR:495.6600 R:500.0 loss:19.3338 exploreP:0.0101\n",
      "Episode:423 meanR:495.6600 R:500.0 loss:16.9615 exploreP:0.0101\n",
      "Episode:424 meanR:495.6600 R:500.0 loss:16.5183 exploreP:0.0101\n",
      "Episode:425 meanR:495.6600 R:500.0 loss:17.6491 exploreP:0.0101\n",
      "Episode:426 meanR:495.6600 R:500.0 loss:20.3069 exploreP:0.0100\n",
      "Episode:427 meanR:495.6600 R:500.0 loss:22.1365 exploreP:0.0100\n",
      "Episode:428 meanR:495.6600 R:500.0 loss:19.2580 exploreP:0.0100\n",
      "Episode:429 meanR:495.6600 R:500.0 loss:20.8159 exploreP:0.0100\n",
      "Episode:430 meanR:495.6600 R:500.0 loss:19.6440 exploreP:0.0100\n",
      "Episode:431 meanR:495.6600 R:500.0 loss:20.4011 exploreP:0.0100\n",
      "Episode:432 meanR:495.6600 R:500.0 loss:19.3600 exploreP:0.0100\n",
      "Episode:433 meanR:495.6600 R:500.0 loss:22.5465 exploreP:0.0100\n",
      "Episode:434 meanR:495.6600 R:500.0 loss:22.8436 exploreP:0.0100\n",
      "Episode:435 meanR:495.6600 R:500.0 loss:23.7450 exploreP:0.0100\n",
      "Episode:436 meanR:495.6600 R:500.0 loss:30.7729 exploreP:0.0100\n",
      "Episode:437 meanR:495.6600 R:500.0 loss:33.5268 exploreP:0.0100\n",
      "Episode:438 meanR:495.6600 R:500.0 loss:34.0405 exploreP:0.0100\n",
      "Episode:439 meanR:495.6600 R:500.0 loss:33.4583 exploreP:0.0100\n",
      "Episode:440 meanR:495.6600 R:500.0 loss:31.1106 exploreP:0.0100\n",
      "Episode:441 meanR:495.6600 R:500.0 loss:29.7730 exploreP:0.0100\n",
      "Episode:442 meanR:495.6600 R:500.0 loss:38.5442 exploreP:0.0100\n",
      "Episode:443 meanR:495.6600 R:500.0 loss:33.3214 exploreP:0.0100\n",
      "Episode:444 meanR:495.6600 R:500.0 loss:20.5974 exploreP:0.0100\n",
      "Episode:445 meanR:495.6600 R:500.0 loss:15.3471 exploreP:0.0100\n",
      "Episode:446 meanR:495.6600 R:500.0 loss:15.2080 exploreP:0.0100\n",
      "Episode:447 meanR:495.6600 R:500.0 loss:13.9047 exploreP:0.0100\n",
      "Episode:448 meanR:495.6600 R:500.0 loss:15.0934 exploreP:0.0100\n",
      "Episode:449 meanR:495.6600 R:500.0 loss:16.3491 exploreP:0.0100\n",
      "Episode:450 meanR:495.6600 R:500.0 loss:21.4165 exploreP:0.0100\n",
      "Episode:451 meanR:495.6600 R:500.0 loss:27.4620 exploreP:0.0100\n",
      "Episode:452 meanR:495.6600 R:500.0 loss:24.9530 exploreP:0.0100\n",
      "Episode:453 meanR:495.6600 R:500.0 loss:23.5297 exploreP:0.0100\n",
      "Episode:454 meanR:495.6600 R:500.0 loss:26.9244 exploreP:0.0100\n",
      "Episode:455 meanR:495.6600 R:500.0 loss:25.8792 exploreP:0.0100\n",
      "Episode:456 meanR:495.6600 R:500.0 loss:27.0495 exploreP:0.0100\n",
      "Episode:457 meanR:495.6600 R:500.0 loss:29.9269 exploreP:0.0100\n",
      "Episode:458 meanR:495.6600 R:500.0 loss:30.8935 exploreP:0.0100\n",
      "Episode:459 meanR:495.6600 R:500.0 loss:26.2679 exploreP:0.0100\n",
      "Episode:460 meanR:495.6600 R:500.0 loss:27.6794 exploreP:0.0100\n",
      "Episode:461 meanR:495.6600 R:500.0 loss:21.8669 exploreP:0.0100\n",
      "Episode:462 meanR:495.6600 R:500.0 loss:20.7453 exploreP:0.0100\n",
      "Episode:463 meanR:495.6600 R:500.0 loss:20.0052 exploreP:0.0100\n",
      "Episode:464 meanR:495.6600 R:500.0 loss:19.5351 exploreP:0.0100\n",
      "Episode:465 meanR:495.6600 R:500.0 loss:20.5556 exploreP:0.0100\n",
      "Episode:466 meanR:495.6600 R:500.0 loss:16.0391 exploreP:0.0100\n",
      "Episode:467 meanR:495.6600 R:500.0 loss:17.8139 exploreP:0.0100\n",
      "Episode:468 meanR:495.6600 R:500.0 loss:15.9528 exploreP:0.0100\n",
      "Episode:469 meanR:495.6600 R:500.0 loss:15.8493 exploreP:0.0100\n",
      "Episode:470 meanR:495.6600 R:500.0 loss:15.6291 exploreP:0.0100\n",
      "Episode:471 meanR:495.6600 R:500.0 loss:14.1909 exploreP:0.0100\n",
      "Episode:472 meanR:495.6600 R:500.0 loss:15.8416 exploreP:0.0100\n",
      "Episode:473 meanR:495.6600 R:500.0 loss:12.0158 exploreP:0.0100\n",
      "Episode:474 meanR:495.6600 R:500.0 loss:13.6074 exploreP:0.0100\n",
      "Episode:475 meanR:495.6600 R:500.0 loss:15.7461 exploreP:0.0100\n",
      "Episode:476 meanR:495.6600 R:500.0 loss:13.6736 exploreP:0.0100\n",
      "Episode:477 meanR:495.6600 R:500.0 loss:12.6429 exploreP:0.0100\n",
      "Episode:478 meanR:495.6600 R:500.0 loss:14.7198 exploreP:0.0100\n",
      "Episode:479 meanR:495.6600 R:500.0 loss:15.0347 exploreP:0.0100\n",
      "Episode:480 meanR:495.6600 R:500.0 loss:15.5040 exploreP:0.0100\n",
      "Episode:481 meanR:495.6600 R:500.0 loss:15.8892 exploreP:0.0100\n",
      "Episode:482 meanR:495.6600 R:500.0 loss:14.1508 exploreP:0.0100\n",
      "Episode:483 meanR:495.6600 R:500.0 loss:18.4911 exploreP:0.0100\n",
      "Episode:484 meanR:495.6600 R:500.0 loss:15.7085 exploreP:0.0100\n",
      "Episode:485 meanR:495.6600 R:500.0 loss:17.3130 exploreP:0.0100\n",
      "Episode:486 meanR:500.0000 R:500.0 loss:14.8219 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            # if explore_p > np.random.rand():\n",
    "            #     action = env.action_space.sample()\n",
    "            # else:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average losses')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Creating a gym env\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# A training graph session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "# Close the env at the end\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
