{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq. DQN for bipedal env.\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), Box(4,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "# Discrete/int or continuos/float\n",
    "env.action_space.dtype, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24,), (24,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 24) (1111, 4) (1111,)\n",
      "dtypes: float64 float64 float32 bool\n",
      "states: 2.5304067929585776 -1.8729597727457683\n",
      "actions: 0.99992806 -0.9998551\n",
      "rewards: 0.2917162125532814 -100.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09762701,  0.43037874,  0.20552675,  0.08976637],\n",
       "       [-0.1526904 ,  0.29178822, -0.12482557,  0.78354603],\n",
       "       [ 0.92732555, -0.23311697,  0.5834501 ,  0.05778984],\n",
       "       [ 0.13608912,  0.85119325, -0.85792786, -0.8257414 ],\n",
       "       [-0.9595632 ,  0.6652397 ,  0.5563135 ,  0.74002427],\n",
       "       [ 0.9572367 ,  0.59831715, -0.07704128,  0.56105834],\n",
       "       [-0.76345116,  0.27984205, -0.71329343,  0.88933784],\n",
       "       [ 0.04369664, -0.17067613, -0.47088876,  0.5484674 ],\n",
       "       [-0.08769934,  0.1368679 , -0.9624204 ,  0.23527099],\n",
       "       [ 0.22419144,  0.23386799,  0.8874962 ,  0.3636406 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01678042, -0.04400099, -0.18430857, -0.14925065, -0.07505553,\n",
       "       -0.13483308,  0.01482175,  0.07228765,  0.07965715, -0.04776169])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5724162381680259, 3.7200759760208356e-44)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.002917162125532814 -1.0\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, lstm_size=hidden_size,\n",
    "                                              cell=cell, initial_state=g_initial_state)\n",
    "    # actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    actions_labels = tf.nn.sigmoid(actions)\n",
    "    # neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "    #                                                                   labels=actions_labels)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=actions_labels)\n",
    "    # g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs, d_final_state = discriminator(states=states, actions=actions_logits, lstm_size=hidden_size, \n",
    "                                      cell=cell, initial_state=d_initial_state)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * Qs)\n",
    "    \n",
    "    return actions_logits, Qs, g_final_state, d_final_state, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            action_size=action_size, state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1111, 24) actions:(1111, 4)\n",
      "action size:2.999783158302307\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "# state_size = 37\n",
    "# state_size_ = (84, 84, 3)\n",
    "state_size = 24\n",
    "action_size = 4\n",
    "hidden_size = 24*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "gamma = 0.99                   # future reward discount\n",
    "memory_size = 128            # memory capacity\n",
    "batch_size = 128             # experience mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 24) (?, 48)\n",
      "(1, ?, 48) (1, 48)\n",
      "(1, ?, 48) (1, 48)\n",
      "(?, 48)\n",
      "(?, 4)\n",
      "(?, 24) (?, 48)\n",
      "(1, ?, 48) (1, 48)\n",
      "(1, ?, 48) (1, 48)\n",
      "(?, 48)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.74747936e-03, -6.47998764e-06,  5.04063684e-04, -1.59999549e-02,\n",
       "         9.20596421e-02, -6.65190804e-04,  8.60213369e-01,  1.89474629e-03,\n",
       "         1.00000000e+00,  3.24652232e-02, -6.65142899e-04,  8.53762254e-01,\n",
       "         4.70428534e-04,  1.00000000e+00,  4.40814018e-01,  4.45820123e-01,\n",
       "         4.61422771e-01,  4.89550203e-01,  5.34102798e-01,  6.02461040e-01,\n",
       "         7.09148884e-01,  8.85931849e-01,  1.00000000e+00,  1.00000000e+00]),\n",
       " array([ 0.603741  , -0.1230465 , -0.4461088 ,  0.90563905], dtype=float32),\n",
       " array([ 0.00481188, -0.00427009, -0.00373932,  0.03136588,  0.39623171,\n",
       "         0.77119899,  0.16533393, -0.9999524 ,  1.        , -0.38297048,\n",
       "        -0.7196914 ,  1.7560308 ,  0.99794277,  1.        ,  0.45381129,\n",
       "         0.458965  ,  0.47502771,  0.50398445,  0.54985064,  0.62022442,\n",
       "         0.73005795,  0.91205329,  1.        ,  1.        ]),\n",
       " -0.06635343202079455,\n",
       " 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-111.6312 R:-111.6312 gloss:-0.3806 dloss:1.5693 exploreP:0.9946\n",
      "Episode:1 meanR:-111.9802 R:-112.3291 gloss:-2.6368 dloss:75.4108 exploreP:0.9880\n",
      "Episode:2 meanR:-116.6246 R:-125.9134 gloss:-11.4978 dloss:102.2556 exploreP:0.9759\n",
      "Episode:3 meanR:-108.2071 R:-82.9546 gloss:-39.6893 dloss:5.2910 exploreP:0.8331\n",
      "Episode:4 meanR:-106.9894 R:-102.1186 gloss:-50.6607 dloss:1.1674 exploreP:0.8235\n",
      "Episode:5 meanR:-108.9545 R:-118.7805 gloss:-51.7753 dloss:65.9912 exploreP:0.8184\n",
      "Episode:6 meanR:-110.4548 R:-119.4565 gloss:-56.4578 dloss:124.1139 exploreP:0.8129\n",
      "Episode:7 meanR:-109.6728 R:-104.1988 gloss:-65.2943 dloss:98.1782 exploreP:0.8047\n",
      "Episode:8 meanR:-109.0496 R:-104.0641 gloss:-71.7397 dloss:87.5518 exploreP:0.8001\n",
      "Episode:9 meanR:-109.7636 R:-116.1894 gloss:-79.6815 dloss:121.2024 exploreP:0.7953\n",
      "Episode:10 meanR:-117.7094 R:-197.1678 gloss:-98.3485 dloss:8.8714 exploreP:0.6967\n",
      "Episode:11 meanR:-117.6167 R:-116.5972 gloss:-106.9560 dloss:59.8286 exploreP:0.6916\n",
      "Episode:12 meanR:-116.4215 R:-102.0781 gloss:-118.8419 dloss:107.1159 exploreP:0.6873\n",
      "Episode:13 meanR:-115.2819 R:-100.4671 gloss:-122.9558 dloss:115.1684 exploreP:0.6831\n",
      "Episode:14 meanR:-116.1230 R:-127.8984 gloss:-129.3554 dloss:89.1534 exploreP:0.6755\n",
      "Episode:15 meanR:-115.3927 R:-104.4388 gloss:-133.5634 dloss:67.0650 exploreP:0.6711\n",
      "Episode:16 meanR:-114.5023 R:-100.2565 gloss:-134.3427 dloss:102.4530 exploreP:0.6664\n",
      "Episode:17 meanR:-114.5920 R:-116.1152 gloss:-133.8717 dloss:109.2055 exploreP:0.6631\n",
      "Episode:18 meanR:-114.9121 R:-120.6745 gloss:-150.0705 dloss:102.0171 exploreP:0.6572\n",
      "Episode:19 meanR:-114.3420 R:-103.5112 gloss:-167.0189 dloss:83.9414 exploreP:0.6531\n",
      "Episode:20 meanR:-113.7159 R:-101.1935 gloss:-161.4342 dloss:102.7007 exploreP:0.6489\n",
      "Episode:21 meanR:-113.8773 R:-117.2669 gloss:-173.8055 dloss:85.9225 exploreP:0.6430\n",
      "Episode:22 meanR:-113.7981 R:-112.0554 gloss:-185.1920 dloss:66.2666 exploreP:0.6357\n",
      "Episode:23 meanR:-113.3003 R:-101.8500 gloss:-175.0642 dloss:59.3477 exploreP:0.6314\n",
      "Episode:24 meanR:-113.1824 R:-110.3537 gloss:-158.8652 dloss:77.2554 exploreP:0.6246\n",
      "Episode:25 meanR:-112.7879 R:-102.9247 gloss:-186.3567 dloss:51.9030 exploreP:0.6162\n",
      "Episode:26 meanR:-112.3041 R:-99.7268 gloss:-203.8351 dloss:49.7781 exploreP:0.6120\n",
      "Episode:27 meanR:-111.9920 R:-103.5652 gloss:-214.5822 dloss:93.6796 exploreP:0.6082\n",
      "Episode:28 meanR:-112.2614 R:-119.8026 gloss:-229.0209 dloss:84.1041 exploreP:0.6031\n",
      "Episode:29 meanR:-111.9747 R:-103.6617 gloss:-232.0961 dloss:69.6291 exploreP:0.5977\n",
      "Episode:30 meanR:-111.6309 R:-101.3161 gloss:-213.6898 dloss:69.8555 exploreP:0.5932\n",
      "Episode:31 meanR:-111.3863 R:-103.8043 gloss:-210.4035 dloss:79.9276 exploreP:0.5890\n",
      "Episode:32 meanR:-111.1808 R:-104.6045 gloss:-230.4610 dloss:77.5996 exploreP:0.5842\n",
      "Episode:33 meanR:-110.9949 R:-104.8618 gloss:-237.0835 dloss:67.5816 exploreP:0.5788\n",
      "Episode:34 meanR:-110.7783 R:-103.4143 gloss:-245.5097 dloss:58.6558 exploreP:0.5724\n",
      "Episode:35 meanR:-110.6953 R:-107.7877 gloss:-205.3972 dloss:53.8125 exploreP:0.5681\n",
      "Episode:36 meanR:-110.4677 R:-102.2747 gloss:-220.2875 dloss:77.7223 exploreP:0.5642\n",
      "Episode:37 meanR:-110.3246 R:-105.0295 gloss:-281.3258 dloss:74.2392 exploreP:0.5594\n",
      "Episode:38 meanR:-110.1399 R:-103.1211 gloss:-254.9107 dloss:83.8126 exploreP:0.5570\n",
      "Episode:39 meanR:-110.0095 R:-104.9254 gloss:-252.5447 dloss:87.6051 exploreP:0.5533\n",
      "Episode:40 meanR:-109.8339 R:-102.8077 gloss:-274.8167 dloss:97.8332 exploreP:0.5503\n",
      "Episode:41 meanR:-109.6009 R:-100.0508 gloss:-271.2101 dloss:88.5275 exploreP:0.5468\n",
      "Episode:42 meanR:-109.6378 R:-111.1858 gloss:-273.7079 dloss:72.6156 exploreP:0.5416\n",
      "Episode:43 meanR:-109.4796 R:-102.6781 gloss:-280.2054 dloss:61.3999 exploreP:0.5381\n",
      "Episode:44 meanR:-109.2999 R:-101.3927 gloss:-265.0536 dloss:77.1306 exploreP:0.5343\n",
      "Episode:45 meanR:-109.6898 R:-127.2359 gloss:-268.7928 dloss:59.7939 exploreP:0.5280\n",
      "Episode:46 meanR:-109.5011 R:-100.8178 gloss:-304.3842 dloss:44.4669 exploreP:0.5247\n",
      "Episode:47 meanR:-109.8014 R:-123.9166 gloss:-312.1478 dloss:65.1401 exploreP:0.5193\n",
      "Episode:48 meanR:-109.6839 R:-104.0450 gloss:-281.7500 dloss:50.5499 exploreP:0.5150\n",
      "Episode:49 meanR:-109.6122 R:-106.0999 gloss:-322.6898 dloss:62.0956 exploreP:0.5112\n",
      "Episode:50 meanR:-109.5121 R:-104.5052 gloss:-308.7878 dloss:71.0456 exploreP:0.5080\n",
      "Episode:51 meanR:-109.4444 R:-105.9903 gloss:-285.9580 dloss:78.2360 exploreP:0.5051\n",
      "Episode:52 meanR:-109.3345 R:-103.6218 gloss:-290.3541 dloss:79.4763 exploreP:0.5022\n",
      "Episode:53 meanR:-109.1960 R:-101.8548 gloss:-289.1405 dloss:79.2558 exploreP:0.4986\n",
      "Episode:54 meanR:-109.0944 R:-103.6095 gloss:-276.9439 dloss:64.3612 exploreP:0.4948\n",
      "Episode:55 meanR:-109.5239 R:-133.1451 gloss:-306.4031 dloss:48.9366 exploreP:0.4883\n",
      "Episode:56 meanR:-109.4353 R:-104.4737 gloss:-308.4129 dloss:37.1299 exploreP:0.4824\n",
      "Episode:57 meanR:-109.3259 R:-103.0890 gloss:-309.8262 dloss:39.0971 exploreP:0.4796\n",
      "Episode:58 meanR:-109.3937 R:-113.3290 gloss:-352.6471 dloss:56.7148 exploreP:0.4740\n",
      "Episode:59 meanR:-109.3418 R:-106.2807 gloss:-387.2148 dloss:40.9265 exploreP:0.4716\n",
      "Episode:60 meanR:-109.6833 R:-130.1700 gloss:-376.4911 dloss:58.4684 exploreP:0.4661\n",
      "Episode:61 meanR:-109.5849 R:-103.5830 gloss:-311.4888 dloss:38.7765 exploreP:0.4626\n",
      "Episode:62 meanR:-109.4543 R:-101.3581 gloss:-304.4606 dloss:58.4633 exploreP:0.4592\n",
      "Episode:63 meanR:-109.7626 R:-129.1857 gloss:-343.5154 dloss:50.1610 exploreP:0.4539\n",
      "Episode:64 meanR:-109.7156 R:-106.7083 gloss:-360.4759 dloss:37.3652 exploreP:0.4497\n",
      "Episode:65 meanR:-109.6114 R:-102.8357 gloss:-340.0386 dloss:50.0415 exploreP:0.4466\n",
      "Episode:66 meanR:-109.5115 R:-102.9217 gloss:-342.2867 dloss:59.7373 exploreP:0.4435\n",
      "Episode:67 meanR:-109.4343 R:-104.2590 gloss:-329.9214 dloss:56.0414 exploreP:0.4400\n",
      "Episode:68 meanR:-109.3763 R:-105.4351 gloss:-338.7210 dloss:62.7174 exploreP:0.4377\n",
      "Episode:69 meanR:-109.2935 R:-103.5800 gloss:-378.4163 dloss:63.4891 exploreP:0.4344\n",
      "Episode:70 meanR:-109.2095 R:-103.3307 gloss:-413.3365 dloss:53.5451 exploreP:0.4312\n",
      "Episode:71 meanR:-109.1677 R:-106.1963 gloss:-390.3174 dloss:47.1169 exploreP:0.4265\n",
      "Episode:72 meanR:-109.4749 R:-131.5915 gloss:-369.3651 dloss:34.4810 exploreP:0.4211\n",
      "Episode:73 meanR:-109.4022 R:-104.0991 gloss:-361.1010 dloss:31.8394 exploreP:0.4188\n",
      "Episode:74 meanR:-109.3188 R:-103.1496 gloss:-394.4853 dloss:62.5272 exploreP:0.4160\n",
      "Episode:75 meanR:-109.2441 R:-103.6387 gloss:-454.3476 dloss:58.2943 exploreP:0.4132\n",
      "Episode:76 meanR:-109.1782 R:-104.1711 gloss:-430.9785 dloss:56.6478 exploreP:0.4105\n",
      "Episode:77 meanR:-109.0876 R:-102.1092 gloss:-392.1737 dloss:60.6988 exploreP:0.4083\n",
      "Episode:78 meanR:-108.9857 R:-101.0363 gloss:-432.4886 dloss:62.8536 exploreP:0.4061\n",
      "Episode:79 meanR:-108.9051 R:-102.5386 gloss:-453.6467 dloss:69.4361 exploreP:0.4041\n",
      "Episode:80 meanR:-108.8497 R:-104.4210 gloss:-452.8397 dloss:69.8669 exploreP:0.4019\n",
      "Episode:81 meanR:-108.8292 R:-107.1674 gloss:-415.7525 dloss:69.9023 exploreP:0.3999\n",
      "Episode:82 meanR:-108.7676 R:-103.7169 gloss:-420.5518 dloss:67.7812 exploreP:0.3977\n",
      "Episode:83 meanR:-108.7151 R:-104.3561 gloss:-371.6556 dloss:60.9377 exploreP:0.3948\n",
      "Episode:84 meanR:-108.6491 R:-103.1061 gloss:-379.7483 dloss:51.4343 exploreP:0.3923\n",
      "Episode:85 meanR:-108.6253 R:-106.6051 gloss:-427.0807 dloss:54.1702 exploreP:0.3897\n",
      "Episode:86 meanR:-108.5983 R:-106.2677 gloss:-385.9026 dloss:55.4681 exploreP:0.3879\n",
      "Episode:87 meanR:-108.5807 R:-107.0569 gloss:-351.5185 dloss:57.8626 exploreP:0.3850\n",
      "Episode:88 meanR:-108.5191 R:-103.0970 gloss:-340.5329 dloss:50.1282 exploreP:0.3827\n",
      "Episode:89 meanR:-108.4547 R:-102.7246 gloss:-377.4614 dloss:53.7520 exploreP:0.3804\n",
      "Episode:90 meanR:-108.3937 R:-102.8992 gloss:-432.2689 dloss:50.1154 exploreP:0.3775\n",
      "Episode:91 meanR:-108.5621 R:-123.8877 gloss:-451.3348 dloss:41.9286 exploreP:0.3745\n",
      "Episode:92 meanR:-108.5348 R:-106.0256 gloss:-420.9727 dloss:42.7808 exploreP:0.3720\n",
      "Episode:93 meanR:-108.4804 R:-103.4143 gloss:-438.9800 dloss:51.7538 exploreP:0.3699\n",
      "Episode:94 meanR:-108.4290 R:-103.5993 gloss:-444.3672 dloss:50.4836 exploreP:0.3673\n",
      "Episode:95 meanR:-108.3583 R:-101.6446 gloss:-399.2921 dloss:46.9486 exploreP:0.3650\n",
      "Episode:96 meanR:-108.2969 R:-102.4014 gloss:-421.1115 dloss:47.7664 exploreP:0.3626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:97 meanR:-108.2648 R:-105.1486 gloss:-477.6659 dloss:50.0663 exploreP:0.3608\n",
      "Episode:98 meanR:-108.2225 R:-104.0853 gloss:-515.7126 dloss:52.4609 exploreP:0.3585\n",
      "Episode:99 meanR:-108.2040 R:-106.3717 gloss:-493.9487 dloss:54.0818 exploreP:0.3568\n",
      "Episode:100 meanR:-108.1298 R:-104.2090 gloss:-486.7580 dloss:52.3880 exploreP:0.3543\n",
      "Episode:101 meanR:-108.0500 R:-104.3472 gloss:-509.7498 dloss:38.9849 exploreP:0.3510\n",
      "Episode:102 meanR:-107.8217 R:-103.0876 gloss:-583.7803 dloss:34.2279 exploreP:0.3485\n",
      "Episode:103 meanR:-108.0184 R:-102.6221 gloss:-549.1526 dloss:38.9625 exploreP:0.3458\n",
      "Episode:104 meanR:-108.0537 R:-105.6482 gloss:-513.7426 dloss:38.1695 exploreP:0.3432\n",
      "Episode:105 meanR:-107.9257 R:-105.9827 gloss:-552.1723 dloss:43.1513 exploreP:0.3413\n",
      "Episode:106 meanR:-107.7751 R:-104.3944 gloss:-502.9954 dloss:46.3555 exploreP:0.3395\n",
      "Episode:107 meanR:-107.7825 R:-104.9390 gloss:-428.5504 dloss:52.7492 exploreP:0.3378\n",
      "Episode:108 meanR:-107.7791 R:-103.7242 gloss:-426.3477 dloss:54.1785 exploreP:0.3360\n",
      "Episode:109 meanR:-107.6419 R:-102.4696 gloss:-479.4658 dloss:50.3400 exploreP:0.3337\n",
      "Episode:110 meanR:-106.7434 R:-107.3161 gloss:-508.6779 dloss:34.8980 exploreP:0.3305\n",
      "Episode:111 meanR:-106.6563 R:-107.8889 gloss:-489.8658 dloss:35.6532 exploreP:0.3290\n",
      "Episode:112 meanR:-106.6598 R:-102.4223 gloss:-454.0951 dloss:43.6434 exploreP:0.3268\n",
      "Episode:113 meanR:-106.7002 R:-104.5065 gloss:-516.5928 dloss:46.9382 exploreP:0.3251\n",
      "Episode:114 meanR:-106.4972 R:-107.6026 gloss:-487.2188 dloss:45.0296 exploreP:0.3235\n",
      "Episode:115 meanR:-106.5146 R:-106.1771 gloss:-374.7418 dloss:50.0319 exploreP:0.3215\n",
      "Episode:116 meanR:-106.5517 R:-103.9680 gloss:-469.3456 dloss:46.1221 exploreP:0.3196\n",
      "Episode:117 meanR:-106.4216 R:-103.1082 gloss:-576.8483 dloss:41.9982 exploreP:0.3181\n",
      "Episode:118 meanR:-106.2320 R:-101.7160 gloss:-552.4750 dloss:46.8444 exploreP:0.3164\n",
      "Episode:119 meanR:-106.2346 R:-103.7719 gloss:-590.3964 dloss:50.1086 exploreP:0.3148\n",
      "Episode:120 meanR:-106.2498 R:-102.7116 gloss:-649.9848 dloss:47.8415 exploreP:0.3131\n",
      "Episode:121 meanR:-106.1486 R:-107.1454 gloss:-637.9799 dloss:47.6693 exploreP:0.3116\n",
      "Episode:122 meanR:-106.0732 R:-104.5184 gloss:-604.2587 dloss:46.6712 exploreP:0.3098\n",
      "Episode:123 meanR:-106.1158 R:-106.1021 gloss:-612.3431 dloss:35.6577 exploreP:0.3068\n",
      "Episode:124 meanR:-106.0533 R:-104.1070 gloss:-527.3072 dloss:30.1256 exploreP:0.3054\n",
      "Episode:125 meanR:-106.0631 R:-103.9043 gloss:-448.4147 dloss:38.5073 exploreP:0.3033\n",
      "Episode:126 meanR:-106.1415 R:-107.5700 gloss:-409.2417 dloss:41.7530 exploreP:0.3019\n",
      "Episode:127 meanR:-106.1407 R:-103.4795 gloss:-406.0969 dloss:39.6038 exploreP:0.3000\n",
      "Episode:128 meanR:-105.9863 R:-104.3607 gloss:-416.2002 dloss:41.6403 exploreP:0.2982\n",
      "Episode:129 meanR:-106.0066 R:-105.6968 gloss:-466.5003 dloss:37.5028 exploreP:0.2966\n",
      "Episode:130 meanR:-106.0285 R:-103.5100 gloss:-577.8522 dloss:39.3483 exploreP:0.2948\n",
      "Episode:131 meanR:-106.0635 R:-107.2977 gloss:-630.4121 dloss:34.7351 exploreP:0.2926\n",
      "Episode:132 meanR:-106.0617 R:-104.4257 gloss:-605.5443 dloss:32.0106 exploreP:0.2908\n",
      "Episode:133 meanR:-106.0604 R:-104.7362 gloss:-533.4821 dloss:35.9115 exploreP:0.2891\n",
      "Episode:134 meanR:-106.0965 R:-107.0197 gloss:-502.8593 dloss:36.3590 exploreP:0.2876\n",
      "Episode:135 meanR:-106.0599 R:-104.1296 gloss:-513.1926 dloss:39.5598 exploreP:0.2859\n",
      "Episode:136 meanR:-106.0830 R:-104.5881 gloss:-532.6866 dloss:34.1605 exploreP:0.2837\n",
      "Episode:137 meanR:-106.0703 R:-103.7523 gloss:-556.0997 dloss:30.7436 exploreP:0.2821\n",
      "Episode:138 meanR:-106.1002 R:-106.1189 gloss:-545.2836 dloss:34.5586 exploreP:0.2809\n",
      "Episode:139 meanR:-106.0803 R:-102.9295 gloss:-492.8657 dloss:41.1335 exploreP:0.2793\n",
      "Episode:140 meanR:-106.0818 R:-102.9632 gloss:-454.1486 dloss:37.8569 exploreP:0.2774\n",
      "Episode:141 meanR:-106.1346 R:-105.3232 gloss:-483.6703 dloss:30.4588 exploreP:0.2756\n",
      "Episode:142 meanR:-106.0702 R:-104.7447 gloss:-462.4406 dloss:26.4481 exploreP:0.2729\n",
      "Episode:143 meanR:-106.0741 R:-103.0761 gloss:-476.7140 dloss:23.0839 exploreP:0.2710\n",
      "Episode:144 meanR:-106.0831 R:-102.2889 gloss:-481.3372 dloss:32.2061 exploreP:0.2695\n",
      "Episode:145 meanR:-105.8489 R:-103.8186 gloss:-558.1473 dloss:32.8080 exploreP:0.2679\n",
      "Episode:146 meanR:-105.8731 R:-103.2315 gloss:-574.8847 dloss:34.3226 exploreP:0.2663\n",
      "Episode:147 meanR:-105.7342 R:-110.0312 gloss:-492.7870 dloss:22.5540 exploreP:0.2628\n",
      "Episode:148 meanR:-105.7498 R:-105.6024 gloss:-427.4020 dloss:16.3348 exploreP:0.2601\n",
      "Episode:149 meanR:-105.7208 R:-103.2035 gloss:-473.5410 dloss:21.4253 exploreP:0.2585\n",
      "Episode:150 meanR:-105.7068 R:-103.1090 gloss:-429.0222 dloss:32.4762 exploreP:0.2569\n",
      "Episode:151 meanR:-105.9977 R:-135.0704 gloss:-394.2969 dloss:21.4999 exploreP:0.2535\n",
      "Episode:152 meanR:-105.9916 R:-103.0207 gloss:-418.4185 dloss:16.1542 exploreP:0.2518\n",
      "Episode:153 meanR:-106.3215 R:-134.8406 gloss:-408.1132 dloss:21.7117 exploreP:0.2486\n",
      "Episode:154 meanR:-106.3309 R:-104.5525 gloss:-414.4714 dloss:15.9999 exploreP:0.2471\n",
      "Episode:155 meanR:-106.0220 R:-102.2504 gloss:-364.6120 dloss:30.9760 exploreP:0.2455\n",
      "Episode:156 meanR:-106.0762 R:-109.8992 gloss:-488.3192 dloss:27.0490 exploreP:0.2436\n",
      "Episode:157 meanR:-106.0598 R:-101.4423 gloss:-572.6768 dloss:28.4792 exploreP:0.2423\n",
      "Episode:158 meanR:-105.9542 R:-102.7692 gloss:-453.2296 dloss:30.7240 exploreP:0.2410\n",
      "Episode:159 meanR:-105.9208 R:-102.9400 gloss:-438.6099 dloss:35.1354 exploreP:0.2399\n",
      "Episode:160 meanR:-105.6683 R:-104.9228 gloss:-482.9521 dloss:36.2840 exploreP:0.2387\n",
      "Episode:161 meanR:-105.6932 R:-106.0756 gloss:-553.7374 dloss:37.7616 exploreP:0.2376\n",
      "Episode:162 meanR:-105.7435 R:-106.3895 gloss:-548.0081 dloss:38.6332 exploreP:0.2366\n",
      "Episode:163 meanR:-105.4881 R:-103.6380 gloss:-426.4464 dloss:35.4626 exploreP:0.2350\n",
      "Episode:164 meanR:-105.4435 R:-102.2543 gloss:-595.7785 dloss:27.6227 exploreP:0.2334\n",
      "Episode:165 meanR:-105.4439 R:-102.8732 gloss:-652.3605 dloss:26.7819 exploreP:0.2321\n",
      "Episode:166 meanR:-105.4572 R:-104.2499 gloss:-542.3213 dloss:27.7585 exploreP:0.2306\n",
      "Episode:167 meanR:-105.4886 R:-107.3972 gloss:-666.0628 dloss:27.7813 exploreP:0.2292\n",
      "Episode:168 meanR:-105.4607 R:-102.6470 gloss:-664.3903 dloss:27.5586 exploreP:0.2279\n",
      "Episode:169 meanR:-105.4592 R:-103.4361 gloss:-530.8442 dloss:28.5547 exploreP:0.2265\n",
      "Episode:170 meanR:-105.5040 R:-107.8048 gloss:-389.3686 dloss:27.9505 exploreP:0.2255\n",
      "Episode:171 meanR:-105.4765 R:-103.4503 gloss:-359.2195 dloss:29.1830 exploreP:0.2239\n",
      "Episode:172 meanR:-105.1891 R:-102.8457 gloss:-477.2462 dloss:26.7461 exploreP:0.2226\n",
      "Episode:173 meanR:-105.1919 R:-104.3805 gloss:-565.9797 dloss:26.4301 exploreP:0.2215\n",
      "Episode:174 meanR:-105.2059 R:-104.5505 gloss:-522.7747 dloss:28.3786 exploreP:0.2202\n",
      "Episode:175 meanR:-105.2138 R:-104.4312 gloss:-480.7641 dloss:28.5238 exploreP:0.2190\n",
      "Episode:176 meanR:-105.2384 R:-106.6352 gloss:-553.2595 dloss:27.9603 exploreP:0.2179\n",
      "Episode:177 meanR:-105.2826 R:-106.5250 gloss:-578.8481 dloss:28.5219 exploreP:0.2165\n",
      "Episode:178 meanR:-105.3416 R:-106.9327 gloss:-655.8906 dloss:27.4030 exploreP:0.2155\n",
      "Episode:179 meanR:-105.3546 R:-103.8426 gloss:-654.1339 dloss:27.3125 exploreP:0.2144\n",
      "Episode:180 meanR:-105.3504 R:-103.9973 gloss:-564.4755 dloss:29.4467 exploreP:0.2132\n",
      "Episode:181 meanR:-105.3112 R:-103.2460 gloss:-525.7793 dloss:27.1852 exploreP:0.2120\n",
      "Episode:182 meanR:-105.3096 R:-103.5580 gloss:-411.6914 dloss:25.7069 exploreP:0.2108\n",
      "Episode:183 meanR:-105.3077 R:-104.1653 gloss:-467.4796 dloss:24.9938 exploreP:0.2096\n",
      "Episode:184 meanR:-105.3263 R:-104.9740 gloss:-678.7316 dloss:25.5236 exploreP:0.2085\n",
      "Episode:185 meanR:-105.3282 R:-106.7926 gloss:-686.7068 dloss:26.6741 exploreP:0.2075\n",
      "Episode:186 meanR:-105.5910 R:-132.5493 gloss:-529.1487 dloss:21.6199 exploreP:0.2052\n",
      "Episode:187 meanR:-105.5567 R:-103.6254 gloss:-438.3295 dloss:13.0599 exploreP:0.2040\n",
      "Episode:188 meanR:-105.5847 R:-105.8949 gloss:-472.3118 dloss:23.1326 exploreP:0.2030\n",
      "Episode:189 meanR:-105.5862 R:-102.8714 gloss:-460.4042 dloss:25.7690 exploreP:0.2018\n",
      "Episode:190 meanR:-105.5943 R:-103.7115 gloss:-630.3298 dloss:21.9972 exploreP:0.2003\n",
      "Episode:191 meanR:-105.6945 R:-133.9067 gloss:-706.7102 dloss:14.0372 exploreP:0.1976\n",
      "Episode:192 meanR:-105.7271 R:-109.2923 gloss:-469.8586 dloss:11.7058 exploreP:0.1960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:193 meanR:-105.7539 R:-106.0866 gloss:-485.5628 dloss:18.7874 exploreP:0.1948\n",
      "Episode:194 meanR:-105.8159 R:-109.8037 gloss:-508.4550 dloss:14.7893 exploreP:0.1920\n",
      "Episode:195 meanR:-105.8294 R:-102.9940 gloss:-497.5061 dloss:12.3442 exploreP:0.1909\n",
      "Episode:196 meanR:-105.8340 R:-102.8584 gloss:-630.1274 dloss:23.3650 exploreP:0.1896\n",
      "Episode:197 meanR:-105.8033 R:-102.0765 gloss:-804.0572 dloss:23.9118 exploreP:0.1886\n",
      "Episode:198 meanR:-105.8061 R:-104.3712 gloss:-710.0206 dloss:24.3398 exploreP:0.1878\n",
      "Episode:199 meanR:-105.7917 R:-104.9258 gloss:-542.6937 dloss:30.3940 exploreP:0.1870\n",
      "Episode:200 meanR:-105.7700 R:-102.0448 gloss:-542.0499 dloss:30.6882 exploreP:0.1860\n",
      "Episode:201 meanR:-105.7735 R:-104.6969 gloss:-569.0645 dloss:29.3688 exploreP:0.1851\n",
      "Episode:202 meanR:-105.7669 R:-102.4317 gloss:-561.8984 dloss:28.2193 exploreP:0.1842\n",
      "Episode:203 meanR:-105.7812 R:-104.0487 gloss:-507.2881 dloss:27.2342 exploreP:0.1831\n",
      "Episode:204 meanR:-105.7598 R:-103.5052 gloss:-477.6814 dloss:24.4655 exploreP:0.1822\n",
      "Episode:205 meanR:-105.7646 R:-106.4616 gloss:-443.3115 dloss:24.4550 exploreP:0.1814\n",
      "Episode:206 meanR:-105.7568 R:-103.6186 gloss:-570.6664 dloss:27.8469 exploreP:0.1805\n",
      "Episode:207 meanR:-105.7356 R:-102.8168 gloss:-629.0478 dloss:26.8130 exploreP:0.1794\n",
      "Episode:208 meanR:-106.0138 R:-131.5461 gloss:-592.9821 dloss:18.0453 exploreP:0.1775\n",
      "Episode:209 meanR:-106.0168 R:-102.7716 gloss:-576.1754 dloss:12.7302 exploreP:0.1763\n",
      "Episode:210 meanR:-105.9702 R:-102.6530 gloss:-460.1090 dloss:21.5162 exploreP:0.1753\n",
      "Episode:211 meanR:-105.9167 R:-102.5349 gloss:-430.6736 dloss:21.5568 exploreP:0.1742\n",
      "Episode:212 meanR:-105.9528 R:-106.0406 gloss:-447.8629 dloss:21.9095 exploreP:0.1733\n",
      "Episode:213 meanR:-106.2121 R:-130.4273 gloss:-528.8486 dloss:18.8182 exploreP:0.1716\n",
      "Episode:214 meanR:-106.1922 R:-105.6175 gloss:-664.6145 dloss:16.2661 exploreP:0.1708\n",
      "Episode:215 meanR:-106.1957 R:-106.5289 gloss:-630.3093 dloss:21.4262 exploreP:0.1698\n",
      "Episode:216 meanR:-106.1865 R:-103.0423 gloss:-540.7642 dloss:21.7308 exploreP:0.1686\n",
      "Episode:217 meanR:-106.2007 R:-104.5353 gloss:-531.9551 dloss:19.9961 exploreP:0.1677\n",
      "Episode:218 meanR:-106.2567 R:-107.3126 gloss:-492.8200 dloss:18.7063 exploreP:0.1663\n",
      "Episode:219 meanR:-106.2489 R:-102.9883 gloss:-388.9985 dloss:15.7619 exploreP:0.1650\n",
      "Episode:220 meanR:-106.2866 R:-106.4875 gloss:-443.1950 dloss:17.2198 exploreP:0.1637\n",
      "Episode:221 meanR:-106.2533 R:-103.8145 gloss:-570.2639 dloss:18.2010 exploreP:0.1627\n",
      "Episode:222 meanR:-106.2391 R:-103.0986 gloss:-631.5042 dloss:20.9099 exploreP:0.1617\n",
      "Episode:223 meanR:-106.2214 R:-104.3343 gloss:-634.1781 dloss:21.8491 exploreP:0.1608\n",
      "Episode:224 meanR:-106.2220 R:-104.1644 gloss:-557.0923 dloss:22.2158 exploreP:0.1600\n",
      "Episode:225 meanR:-106.2445 R:-106.1520 gloss:-521.4338 dloss:24.1642 exploreP:0.1589\n",
      "Episode:226 meanR:-106.2003 R:-103.1531 gloss:-475.5546 dloss:20.6389 exploreP:0.1580\n",
      "Episode:227 meanR:-106.1910 R:-102.5468 gloss:-464.0685 dloss:21.5337 exploreP:0.1570\n",
      "Episode:228 meanR:-106.1820 R:-103.4662 gloss:-459.6896 dloss:19.8219 exploreP:0.1559\n",
      "Episode:229 meanR:-106.1692 R:-104.4142 gloss:-395.0883 dloss:21.0697 exploreP:0.1551\n",
      "Episode:230 meanR:-106.1648 R:-103.0683 gloss:-372.8580 dloss:21.9964 exploreP:0.1541\n",
      "Episode:231 meanR:-106.1312 R:-103.9362 gloss:-467.8221 dloss:19.8577 exploreP:0.1529\n",
      "Episode:232 meanR:-106.1160 R:-102.9081 gloss:-494.6187 dloss:17.1042 exploreP:0.1518\n",
      "Episode:233 meanR:-106.1106 R:-104.1938 gloss:-532.1263 dloss:18.4211 exploreP:0.1507\n",
      "Episode:234 meanR:-106.0719 R:-103.1469 gloss:-584.5194 dloss:18.1549 exploreP:0.1496\n",
      "Episode:235 meanR:-106.0870 R:-105.6450 gloss:-567.1317 dloss:22.9823 exploreP:0.1489\n",
      "Episode:236 meanR:-106.1005 R:-105.9398 gloss:-610.1697 dloss:22.9075 exploreP:0.1482\n",
      "Episode:237 meanR:-106.0840 R:-102.1017 gloss:-720.7747 dloss:28.3754 exploreP:0.1475\n",
      "Episode:238 meanR:-106.0560 R:-103.3176 gloss:-651.0143 dloss:26.5975 exploreP:0.1467\n",
      "Episode:239 meanR:-106.0804 R:-105.3642 gloss:-512.1422 dloss:25.4564 exploreP:0.1460\n",
      "Episode:240 meanR:-106.1014 R:-105.0657 gloss:-519.9205 dloss:26.4728 exploreP:0.1454\n",
      "Episode:241 meanR:-106.0649 R:-101.6719 gloss:-498.8123 dloss:26.1855 exploreP:0.1446\n",
      "Episode:242 meanR:-106.0458 R:-102.8330 gloss:-482.6240 dloss:25.3894 exploreP:0.1438\n",
      "Episode:243 meanR:-106.0470 R:-103.2018 gloss:-377.5978 dloss:23.4959 exploreP:0.1432\n",
      "Episode:244 meanR:-106.0534 R:-102.9260 gloss:-343.5695 dloss:24.3459 exploreP:0.1424\n",
      "Episode:245 meanR:-106.3462 R:-133.1018 gloss:-532.8861 dloss:17.0165 exploreP:0.1407\n",
      "Episode:246 meanR:-106.3736 R:-105.9743 gloss:-405.4688 dloss:10.8179 exploreP:0.1401\n",
      "Episode:247 meanR:-106.3354 R:-106.2035 gloss:-329.5879 dloss:20.8789 exploreP:0.1394\n",
      "Episode:248 meanR:-106.3385 R:-105.9192 gloss:-343.0693 dloss:26.5407 exploreP:0.1388\n",
      "Episode:249 meanR:-106.6005 R:-129.4016 gloss:-414.4442 dloss:19.6823 exploreP:0.1374\n",
      "Episode:250 meanR:-106.5911 R:-102.1692 gloss:-609.5693 dloss:12.4972 exploreP:0.1364\n",
      "Episode:251 meanR:-106.2713 R:-103.0884 gloss:-458.7385 dloss:17.6859 exploreP:0.1356\n",
      "Episode:252 meanR:-106.5461 R:-130.5015 gloss:-502.3513 dloss:15.9784 exploreP:0.1342\n",
      "Episode:253 meanR:-106.2739 R:-107.6251 gloss:-540.6218 dloss:13.5871 exploreP:0.1336\n",
      "Episode:254 meanR:-106.2734 R:-104.5015 gloss:-504.1758 dloss:20.5586 exploreP:0.1329\n",
      "Episode:255 meanR:-106.2906 R:-103.9615 gloss:-611.9030 dloss:24.0285 exploreP:0.1321\n",
      "Episode:256 meanR:-106.2563 R:-106.4763 gloss:-608.3513 dloss:21.3470 exploreP:0.1315\n",
      "Episode:257 meanR:-106.2886 R:-104.6668 gloss:-403.7948 dloss:20.8939 exploreP:0.1305\n",
      "Episode:258 meanR:-106.3267 R:-106.5779 gloss:-243.7532 dloss:20.1097 exploreP:0.1300\n",
      "Episode:259 meanR:-106.6204 R:-132.3194 gloss:-316.3999 dloss:17.0174 exploreP:0.1286\n",
      "Episode:260 meanR:-106.7084 R:-113.7188 gloss:-419.7739 dloss:9.2080 exploreP:0.1268\n",
      "Episode:261 meanR:-106.6910 R:-104.3394 gloss:-511.3212 dloss:10.8276 exploreP:0.1261\n",
      "Episode:262 meanR:-106.9153 R:-128.8116 gloss:-589.6325 dloss:17.8441 exploreP:0.1249\n",
      "Episode:263 meanR:-106.9201 R:-104.1256 gloss:-620.8961 dloss:15.3324 exploreP:0.1242\n",
      "Episode:264 meanR:-106.9220 R:-102.4421 gloss:-570.7210 dloss:18.6625 exploreP:0.1233\n",
      "Episode:265 meanR:-106.9416 R:-104.8292 gloss:-422.6349 dloss:18.8618 exploreP:0.1226\n",
      "Episode:266 meanR:-106.9528 R:-105.3710 gloss:-475.5327 dloss:21.7225 exploreP:0.1221\n",
      "Episode:267 meanR:-106.9315 R:-105.2731 gloss:-582.9561 dloss:25.9693 exploreP:0.1215\n",
      "Episode:268 meanR:-107.0045 R:-109.9440 gloss:-515.5291 dloss:19.2307 exploreP:0.1201\n",
      "Episode:269 meanR:-107.0376 R:-106.7471 gloss:-587.4596 dloss:12.1344 exploreP:0.1196\n",
      "Episode:270 meanR:-107.0186 R:-105.9062 gloss:-539.9348 dloss:19.1563 exploreP:0.1185\n",
      "Episode:271 meanR:-107.0527 R:-106.8609 gloss:-369.7296 dloss:16.5570 exploreP:0.1180\n",
      "Episode:272 meanR:-107.0823 R:-105.8038 gloss:-273.0062 dloss:21.8053 exploreP:0.1174\n",
      "Episode:273 meanR:-107.3720 R:-133.3489 gloss:-364.3284 dloss:18.5361 exploreP:0.1160\n",
      "Episode:274 meanR:-107.3968 R:-107.0247 gloss:-425.0508 dloss:12.1021 exploreP:0.1156\n",
      "Episode:275 meanR:-107.3823 R:-102.9811 gloss:-362.5609 dloss:21.3850 exploreP:0.1148\n",
      "Episode:276 meanR:-107.3415 R:-102.5614 gloss:-278.8697 dloss:21.4243 exploreP:0.1141\n",
      "Episode:277 meanR:-107.3010 R:-102.4728 gloss:-246.3371 dloss:21.3577 exploreP:0.1136\n",
      "Episode:278 meanR:-107.2539 R:-102.2220 gloss:-280.9983 dloss:23.2801 exploreP:0.1130\n",
      "Episode:279 meanR:-107.2336 R:-101.8130 gloss:-324.5932 dloss:25.0188 exploreP:0.1125\n",
      "Episode:280 meanR:-107.2141 R:-102.0486 gloss:-378.9103 dloss:24.6227 exploreP:0.1119\n",
      "Episode:281 meanR:-107.2087 R:-102.7007 gloss:-415.2034 dloss:23.9899 exploreP:0.1114\n",
      "Episode:282 meanR:-107.2096 R:-103.6484 gloss:-431.0216 dloss:24.3251 exploreP:0.1109\n",
      "Episode:283 meanR:-107.1920 R:-102.4053 gloss:-339.0871 dloss:25.2633 exploreP:0.1103\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model2.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        state = env.reset() # env first state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: state.reshape([1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "                #action = np.argmax(action_logits) # For discrete action space\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # exploit\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            g_loss, d_loss, _, _ = sess.run(fetches=[model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs,\n",
    "                                    model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                                    model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "            gloss_batch.append(g_loss)\n",
    "            dloss_batch.append(d_loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) <= -200:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-bipedal-seq.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(11):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            env.render()\n",
    "            action_logits, g_initial_state, d_initial_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state, \n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "# Closing the env\n",
    "print('total_reward: {}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
