{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "for _ in range(100):\n",
    "    #env.render()\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200000             # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 MeanR:27.0000 R: 27.0 Loss: 0.9930 ExploreP: 0.9973\n",
      "Episode: 2 MeanR:22.5000 R: 18.0 Loss: 0.9942 ExploreP: 0.9956\n",
      "Episode: 3 MeanR:21.6667 R: 20.0 Loss: 1.0328 ExploreP: 0.9936\n",
      "Episode: 4 MeanR:23.2500 R: 28.0 Loss: 0.9970 ExploreP: 0.9908\n",
      "Episode: 5 MeanR:21.0000 R: 12.0 Loss: 0.9940 ExploreP: 0.9897\n",
      "Episode: 6 MeanR:19.8333 R: 14.0 Loss: 0.9815 ExploreP: 0.9883\n",
      "Episode: 7 MeanR:21.0000 R: 28.0 Loss: 1.0247 ExploreP: 0.9856\n",
      "Episode: 8 MeanR:20.0000 R: 13.0 Loss: 1.0561 ExploreP: 0.9843\n",
      "Episode: 9 MeanR:19.1111 R: 12.0 Loss: 1.0187 ExploreP: 0.9831\n",
      "Episode: 10 MeanR:18.9000 R: 17.0 Loss: 0.9490 ExploreP: 0.9815\n",
      "Episode: 11 MeanR:18.7273 R: 17.0 Loss: 1.1009 ExploreP: 0.9798\n",
      "Episode: 12 MeanR:19.5000 R: 28.0 Loss: 1.0319 ExploreP: 0.9771\n",
      "Episode: 13 MeanR:19.0769 R: 14.0 Loss: 1.0199 ExploreP: 0.9757\n",
      "Episode: 14 MeanR:18.8571 R: 16.0 Loss: 0.9594 ExploreP: 0.9742\n",
      "Episode: 15 MeanR:18.2000 R: 9.0 Loss: 1.2634 ExploreP: 0.9733\n",
      "Episode: 16 MeanR:18.6250 R: 25.0 Loss: 1.1009 ExploreP: 0.9709\n",
      "Episode: 17 MeanR:18.7059 R: 20.0 Loss: 1.0383 ExploreP: 0.9690\n",
      "Episode: 18 MeanR:19.0556 R: 25.0 Loss: 1.1076 ExploreP: 0.9666\n",
      "Episode: 19 MeanR:18.7895 R: 14.0 Loss: 1.1429 ExploreP: 0.9653\n",
      "Episode: 20 MeanR:18.6500 R: 16.0 Loss: 1.0895 ExploreP: 0.9638\n",
      "Episode: 21 MeanR:19.1429 R: 29.0 Loss: 0.9559 ExploreP: 0.9610\n",
      "Episode: 22 MeanR:19.3636 R: 24.0 Loss: 1.0703 ExploreP: 0.9587\n",
      "Episode: 23 MeanR:19.6957 R: 27.0 Loss: 1.1837 ExploreP: 0.9562\n",
      "Episode: 24 MeanR:19.6667 R: 19.0 Loss: 1.2809 ExploreP: 0.9544\n",
      "Episode: 25 MeanR:19.9600 R: 27.0 Loss: 1.0444 ExploreP: 0.9518\n",
      "Episode: 26 MeanR:20.7692 R: 41.0 Loss: 1.5113 ExploreP: 0.9480\n",
      "Episode: 27 MeanR:20.4815 R: 13.0 Loss: 1.3346 ExploreP: 0.9467\n",
      "Episode: 28 MeanR:20.2500 R: 14.0 Loss: 1.5638 ExploreP: 0.9454\n",
      "Episode: 29 MeanR:19.9655 R: 12.0 Loss: 1.0820 ExploreP: 0.9443\n",
      "Episode: 30 MeanR:20.0333 R: 22.0 Loss: 1.1697 ExploreP: 0.9423\n",
      "Episode: 31 MeanR:19.9677 R: 18.0 Loss: 1.3523 ExploreP: 0.9406\n",
      "Episode: 32 MeanR:19.9375 R: 19.0 Loss: 1.4066 ExploreP: 0.9388\n",
      "Episode: 33 MeanR:19.8485 R: 17.0 Loss: 1.4880 ExploreP: 0.9372\n",
      "Episode: 34 MeanR:19.8529 R: 20.0 Loss: 4.7726 ExploreP: 0.9354\n",
      "Episode: 35 MeanR:20.0286 R: 26.0 Loss: 1.0474 ExploreP: 0.9330\n",
      "Episode: 36 MeanR:20.1389 R: 24.0 Loss: 2.0734 ExploreP: 0.9308\n",
      "Episode: 37 MeanR:20.8649 R: 47.0 Loss: 1.3590 ExploreP: 0.9264\n",
      "Episode: 38 MeanR:20.6316 R: 12.0 Loss: 1.7127 ExploreP: 0.9253\n",
      "Episode: 39 MeanR:20.5385 R: 17.0 Loss: 13.2389 ExploreP: 0.9238\n",
      "Episode: 40 MeanR:20.8500 R: 33.0 Loss: 0.9466 ExploreP: 0.9208\n",
      "Episode: 41 MeanR:21.0244 R: 28.0 Loss: 2.2864 ExploreP: 0.9182\n",
      "Episode: 42 MeanR:20.8571 R: 14.0 Loss: 1.0889 ExploreP: 0.9170\n",
      "Episode: 43 MeanR:20.7442 R: 16.0 Loss: 1.1632 ExploreP: 0.9155\n",
      "Episode: 44 MeanR:20.5455 R: 12.0 Loss: 1.6220 ExploreP: 0.9144\n",
      "Episode: 45 MeanR:20.9111 R: 37.0 Loss: 1.7051 ExploreP: 0.9111\n",
      "Episode: 46 MeanR:20.6739 R: 10.0 Loss: 1.1704 ExploreP: 0.9102\n",
      "Episode: 47 MeanR:21.4894 R: 59.0 Loss: 1.1364 ExploreP: 0.9049\n",
      "Episode: 48 MeanR:21.3958 R: 17.0 Loss: 2.4368 ExploreP: 0.9034\n",
      "Episode: 49 MeanR:21.6122 R: 32.0 Loss: 2.0883 ExploreP: 0.9005\n",
      "Episode: 50 MeanR:21.5200 R: 17.0 Loss: 0.9489 ExploreP: 0.8990\n",
      "Episode: 51 MeanR:21.3922 R: 15.0 Loss: 0.8454 ExploreP: 0.8977\n",
      "Episode: 52 MeanR:21.4231 R: 23.0 Loss: 2.2334 ExploreP: 0.8956\n",
      "Episode: 53 MeanR:21.3585 R: 18.0 Loss: 69.4107 ExploreP: 0.8940\n",
      "Episode: 54 MeanR:21.3704 R: 22.0 Loss: 1.7136 ExploreP: 0.8921\n",
      "Episode: 55 MeanR:21.5091 R: 29.0 Loss: 1.1674 ExploreP: 0.8895\n",
      "Episode: 56 MeanR:21.3393 R: 12.0 Loss: 1.3831 ExploreP: 0.8885\n",
      "Episode: 57 MeanR:21.3333 R: 21.0 Loss: 50.6305 ExploreP: 0.8866\n",
      "Episode: 58 MeanR:21.4828 R: 30.0 Loss: 0.9519 ExploreP: 0.8840\n",
      "Episode: 59 MeanR:21.4237 R: 18.0 Loss: 0.9477 ExploreP: 0.8824\n",
      "Episode: 60 MeanR:21.2833 R: 13.0 Loss: 2.4097 ExploreP: 0.8813\n",
      "Episode: 61 MeanR:21.1639 R: 14.0 Loss: 0.9605 ExploreP: 0.8801\n",
      "Episode: 62 MeanR:20.9677 R: 9.0 Loss: 1.4603 ExploreP: 0.8793\n",
      "Episode: 63 MeanR:21.0317 R: 25.0 Loss: 37.1262 ExploreP: 0.8771\n",
      "Episode: 64 MeanR:21.0156 R: 20.0 Loss: 2.8551 ExploreP: 0.8754\n",
      "Episode: 65 MeanR:21.0462 R: 23.0 Loss: 4.3048 ExploreP: 0.8734\n",
      "Episode: 66 MeanR:21.1212 R: 26.0 Loss: 1.9685 ExploreP: 0.8712\n",
      "Episode: 67 MeanR:21.0299 R: 15.0 Loss: 1.0537 ExploreP: 0.8699\n",
      "Episode: 68 MeanR:21.0735 R: 24.0 Loss: 1.1277 ExploreP: 0.8678\n",
      "Episode: 69 MeanR:21.4493 R: 47.0 Loss: 0.9566 ExploreP: 0.8638\n",
      "Episode: 70 MeanR:21.4571 R: 22.0 Loss: 3.7741 ExploreP: 0.8619\n",
      "Episode: 71 MeanR:21.3662 R: 15.0 Loss: 86.7720 ExploreP: 0.8607\n",
      "Episode: 72 MeanR:21.4583 R: 28.0 Loss: 1.5373 ExploreP: 0.8583\n",
      "Episode: 73 MeanR:21.4658 R: 22.0 Loss: 0.8336 ExploreP: 0.8564\n",
      "Episode: 74 MeanR:21.5270 R: 26.0 Loss: 1.0383 ExploreP: 0.8542\n",
      "Episode: 75 MeanR:22.2267 R: 74.0 Loss: 2.3970 ExploreP: 0.8480\n",
      "Episode: 76 MeanR:22.1842 R: 19.0 Loss: 1.3825 ExploreP: 0.8464\n",
      "Episode: 77 MeanR:22.2727 R: 29.0 Loss: 1.2432 ExploreP: 0.8440\n",
      "Episode: 78 MeanR:22.2179 R: 18.0 Loss: 1.5822 ExploreP: 0.8425\n",
      "Episode: 79 MeanR:22.1139 R: 14.0 Loss: 1.5805 ExploreP: 0.8413\n",
      "Episode: 80 MeanR:22.1000 R: 21.0 Loss: 1.0773 ExploreP: 0.8396\n",
      "Episode: 81 MeanR:21.9259 R: 8.0 Loss: 1.4786 ExploreP: 0.8389\n",
      "Episode: 82 MeanR:21.8659 R: 17.0 Loss: 1.6376 ExploreP: 0.8375\n",
      "Episode: 83 MeanR:21.7711 R: 14.0 Loss: 74.5246 ExploreP: 0.8363\n",
      "Episode: 84 MeanR:21.9762 R: 39.0 Loss: 1.0453 ExploreP: 0.8331\n",
      "Episode: 85 MeanR:21.8824 R: 14.0 Loss: 3.5926 ExploreP: 0.8320\n",
      "Episode: 86 MeanR:21.9767 R: 30.0 Loss: 1.5596 ExploreP: 0.8295\n",
      "Episode: 87 MeanR:22.0345 R: 27.0 Loss: 87.2577 ExploreP: 0.8273\n",
      "Episode: 88 MeanR:22.0455 R: 23.0 Loss: 2.0400 ExploreP: 0.8254\n",
      "Episode: 89 MeanR:21.9213 R: 11.0 Loss: 1.4869 ExploreP: 0.8245\n",
      "Episode: 90 MeanR:21.8111 R: 12.0 Loss: 1.7286 ExploreP: 0.8235\n",
      "Episode: 91 MeanR:21.7692 R: 18.0 Loss: 1.7125 ExploreP: 0.8221\n",
      "Episode: 92 MeanR:21.6739 R: 13.0 Loss: 1.6173 ExploreP: 0.8210\n",
      "Episode: 93 MeanR:21.6237 R: 17.0 Loss: 3.0835 ExploreP: 0.8197\n",
      "Episode: 94 MeanR:21.4894 R: 9.0 Loss: 109.7496 ExploreP: 0.8189\n",
      "Episode: 95 MeanR:21.4947 R: 22.0 Loss: 2.4656 ExploreP: 0.8171\n",
      "Episode: 96 MeanR:21.3646 R: 9.0 Loss: 1.9617 ExploreP: 0.8164\n",
      "Episode: 97 MeanR:21.3402 R: 19.0 Loss: 1.1159 ExploreP: 0.8149\n",
      "Episode: 98 MeanR:21.3776 R: 25.0 Loss: 1.4278 ExploreP: 0.8129\n",
      "Episode: 99 MeanR:21.3636 R: 20.0 Loss: 1.6716 ExploreP: 0.8113\n",
      "Episode: 100 MeanR:21.5200 R: 37.0 Loss: 1.8133 ExploreP: 0.8083\n",
      "Episode: 101 MeanR:21.3600 R: 11.0 Loss: 1.4275 ExploreP: 0.8074\n",
      "Episode: 102 MeanR:21.4600 R: 28.0 Loss: 1.7968 ExploreP: 0.8052\n",
      "Episode: 103 MeanR:21.3600 R: 10.0 Loss: 2.2984 ExploreP: 0.8044\n",
      "Episode: 104 MeanR:21.2400 R: 16.0 Loss: 2.3301 ExploreP: 0.8031\n",
      "Episode: 105 MeanR:21.3300 R: 21.0 Loss: 0.9224 ExploreP: 0.8015\n",
      "Episode: 106 MeanR:21.5100 R: 32.0 Loss: 116.1031 ExploreP: 0.7990\n",
      "Episode: 107 MeanR:21.4400 R: 21.0 Loss: 0.8698 ExploreP: 0.7973\n",
      "Episode: 108 MeanR:21.5100 R: 20.0 Loss: 0.5440 ExploreP: 0.7957\n",
      "Episode: 109 MeanR:21.7200 R: 33.0 Loss: 0.9919 ExploreP: 0.7931\n",
      "Episode: 110 MeanR:22.2400 R: 69.0 Loss: 0.9213 ExploreP: 0.7877\n",
      "Episode: 111 MeanR:22.2600 R: 19.0 Loss: 241.5992 ExploreP: 0.7863\n",
      "Episode: 112 MeanR:22.1300 R: 15.0 Loss: 0.8903 ExploreP: 0.7851\n",
      "Episode: 113 MeanR:22.1300 R: 14.0 Loss: 0.5046 ExploreP: 0.7840\n",
      "Episode: 114 MeanR:22.2000 R: 23.0 Loss: 0.4152 ExploreP: 0.7822\n",
      "Episode: 115 MeanR:22.3400 R: 23.0 Loss: 0.8671 ExploreP: 0.7805\n",
      "Episode: 116 MeanR:22.2300 R: 14.0 Loss: 0.5761 ExploreP: 0.7794\n",
      "Episode: 117 MeanR:22.2400 R: 21.0 Loss: 327.8706 ExploreP: 0.7778\n",
      "Episode: 118 MeanR:22.1300 R: 14.0 Loss: 0.3859 ExploreP: 0.7767\n",
      "Episode: 119 MeanR:22.3700 R: 38.0 Loss: 0.3682 ExploreP: 0.7738\n",
      "Episode: 120 MeanR:22.3300 R: 12.0 Loss: 107.2944 ExploreP: 0.7729\n",
      "Episode: 121 MeanR:22.2000 R: 16.0 Loss: 0.4194 ExploreP: 0.7717\n",
      "Episode: 122 MeanR:22.1100 R: 15.0 Loss: 0.5950 ExploreP: 0.7705\n",
      "Episode: 123 MeanR:21.9500 R: 11.0 Loss: 1.5109 ExploreP: 0.7697\n",
      "Episode: 124 MeanR:21.8900 R: 13.0 Loss: 0.5873 ExploreP: 0.7687\n",
      "Episode: 125 MeanR:21.8400 R: 22.0 Loss: 0.3223 ExploreP: 0.7670\n",
      "Episode: 126 MeanR:21.5900 R: 16.0 Loss: 186.8989 ExploreP: 0.7658\n",
      "Episode: 127 MeanR:21.6900 R: 23.0 Loss: 0.9143 ExploreP: 0.7641\n",
      "Episode: 128 MeanR:21.7200 R: 17.0 Loss: 0.5063 ExploreP: 0.7628\n",
      "Episode: 129 MeanR:21.7900 R: 19.0 Loss: 149.2511 ExploreP: 0.7614\n",
      "Episode: 130 MeanR:21.8200 R: 25.0 Loss: 0.5160 ExploreP: 0.7595\n",
      "Episode: 131 MeanR:21.8600 R: 22.0 Loss: 0.7952 ExploreP: 0.7579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 132 MeanR:21.8400 R: 17.0 Loss: 0.3855 ExploreP: 0.7566\n",
      "Episode: 133 MeanR:21.8000 R: 13.0 Loss: 129.5027 ExploreP: 0.7556\n",
      "Episode: 134 MeanR:21.8400 R: 24.0 Loss: 0.8497 ExploreP: 0.7538\n",
      "Episode: 135 MeanR:21.7900 R: 21.0 Loss: 312.6091 ExploreP: 0.7523\n",
      "Episode: 136 MeanR:21.7300 R: 18.0 Loss: 0.3413 ExploreP: 0.7509\n",
      "Episode: 137 MeanR:21.5000 R: 24.0 Loss: 0.8498 ExploreP: 0.7492\n",
      "Episode: 138 MeanR:21.7100 R: 33.0 Loss: 0.6209 ExploreP: 0.7467\n",
      "Episode: 139 MeanR:21.7900 R: 25.0 Loss: 0.7856 ExploreP: 0.7449\n",
      "Episode: 140 MeanR:21.6000 R: 14.0 Loss: 0.7668 ExploreP: 0.7439\n",
      "Episode: 141 MeanR:21.6200 R: 30.0 Loss: 132.8302 ExploreP: 0.7417\n",
      "Episode: 142 MeanR:21.5700 R: 9.0 Loss: 109.7969 ExploreP: 0.7410\n",
      "Episode: 143 MeanR:21.5900 R: 18.0 Loss: 0.7070 ExploreP: 0.7397\n",
      "Episode: 144 MeanR:21.6600 R: 19.0 Loss: 125.3307 ExploreP: 0.7383\n",
      "Episode: 145 MeanR:21.5100 R: 22.0 Loss: 0.7793 ExploreP: 0.7367\n",
      "Episode: 146 MeanR:21.6500 R: 24.0 Loss: 1.2483 ExploreP: 0.7350\n",
      "Episode: 147 MeanR:21.1700 R: 11.0 Loss: 181.3290 ExploreP: 0.7342\n",
      "Episode: 148 MeanR:21.1800 R: 18.0 Loss: 1.0937 ExploreP: 0.7329\n",
      "Episode: 149 MeanR:21.1000 R: 24.0 Loss: 1.3612 ExploreP: 0.7311\n",
      "Episode: 150 MeanR:21.1500 R: 22.0 Loss: 0.7268 ExploreP: 0.7295\n",
      "Episode: 151 MeanR:21.1300 R: 13.0 Loss: 2.0141 ExploreP: 0.7286\n",
      "Episode: 152 MeanR:21.0300 R: 13.0 Loss: 1.4223 ExploreP: 0.7277\n",
      "Episode: 153 MeanR:21.0000 R: 15.0 Loss: 1.3140 ExploreP: 0.7266\n",
      "Episode: 154 MeanR:20.9300 R: 15.0 Loss: 0.7028 ExploreP: 0.7255\n",
      "Episode: 155 MeanR:20.9800 R: 34.0 Loss: 140.4753 ExploreP: 0.7231\n",
      "Episode: 156 MeanR:20.9900 R: 13.0 Loss: 495.0257 ExploreP: 0.7222\n",
      "Episode: 157 MeanR:20.8900 R: 11.0 Loss: 0.8911 ExploreP: 0.7214\n",
      "Episode: 158 MeanR:20.8000 R: 21.0 Loss: 81.0518 ExploreP: 0.7199\n",
      "Episode: 159 MeanR:20.8000 R: 18.0 Loss: 0.9017 ExploreP: 0.7186\n",
      "Episode: 160 MeanR:20.8500 R: 18.0 Loss: 1.1473 ExploreP: 0.7173\n",
      "Episode: 161 MeanR:20.9700 R: 26.0 Loss: 0.8393 ExploreP: 0.7155\n",
      "Episode: 162 MeanR:21.3300 R: 45.0 Loss: 172.3507 ExploreP: 0.7123\n",
      "Episode: 163 MeanR:21.4700 R: 39.0 Loss: 163.6680 ExploreP: 0.7096\n",
      "Episode: 164 MeanR:21.4000 R: 13.0 Loss: 1.0527 ExploreP: 0.7087\n",
      "Episode: 165 MeanR:21.3800 R: 21.0 Loss: 281.5731 ExploreP: 0.7072\n",
      "Episode: 166 MeanR:21.2900 R: 17.0 Loss: 0.9755 ExploreP: 0.7060\n",
      "Episode: 167 MeanR:21.2800 R: 14.0 Loss: 0.8039 ExploreP: 0.7051\n",
      "Episode: 168 MeanR:21.1700 R: 13.0 Loss: 109.4391 ExploreP: 0.7042\n",
      "Episode: 169 MeanR:20.8000 R: 10.0 Loss: 124.3599 ExploreP: 0.7035\n",
      "Episode: 170 MeanR:21.0700 R: 49.0 Loss: 0.8764 ExploreP: 0.7001\n",
      "Episode: 171 MeanR:21.0400 R: 12.0 Loss: 1.2125 ExploreP: 0.6993\n",
      "Episode: 172 MeanR:20.8500 R: 9.0 Loss: 0.8315 ExploreP: 0.6986\n",
      "Episode: 173 MeanR:20.7600 R: 13.0 Loss: 0.7491 ExploreP: 0.6977\n",
      "Episode: 174 MeanR:20.7600 R: 26.0 Loss: 126.6433 ExploreP: 0.6960\n",
      "Episode: 175 MeanR:20.1300 R: 11.0 Loss: 0.8944 ExploreP: 0.6952\n",
      "Episode: 176 MeanR:20.1000 R: 16.0 Loss: 103.3027 ExploreP: 0.6941\n",
      "Episode: 177 MeanR:19.9000 R: 9.0 Loss: 0.5941 ExploreP: 0.6935\n",
      "Episode: 178 MeanR:19.8200 R: 10.0 Loss: 1.0952 ExploreP: 0.6928\n",
      "Episode: 179 MeanR:19.8400 R: 16.0 Loss: 0.8657 ExploreP: 0.6917\n",
      "Episode: 180 MeanR:19.8000 R: 17.0 Loss: 0.8891 ExploreP: 0.6906\n",
      "Episode: 181 MeanR:19.9500 R: 23.0 Loss: 0.8635 ExploreP: 0.6890\n",
      "Episode: 182 MeanR:19.8600 R: 8.0 Loss: 0.7020 ExploreP: 0.6884\n",
      "Episode: 183 MeanR:19.8300 R: 11.0 Loss: 1.2295 ExploreP: 0.6877\n",
      "Episode: 184 MeanR:19.6300 R: 19.0 Loss: 0.7542 ExploreP: 0.6864\n",
      "Episode: 185 MeanR:19.5800 R: 9.0 Loss: 103.6208 ExploreP: 0.6858\n",
      "Episode: 186 MeanR:19.6300 R: 35.0 Loss: 126.6208 ExploreP: 0.6834\n",
      "Episode: 187 MeanR:19.4900 R: 13.0 Loss: 1.3115 ExploreP: 0.6826\n",
      "Episode: 188 MeanR:19.3800 R: 12.0 Loss: 0.8530 ExploreP: 0.6818\n",
      "Episode: 189 MeanR:19.4000 R: 13.0 Loss: 1.2373 ExploreP: 0.6809\n",
      "Episode: 190 MeanR:19.3700 R: 9.0 Loss: 0.8526 ExploreP: 0.6803\n",
      "Episode: 191 MeanR:19.3800 R: 19.0 Loss: 0.8042 ExploreP: 0.6790\n",
      "Episode: 192 MeanR:19.3600 R: 11.0 Loss: 158.9233 ExploreP: 0.6783\n",
      "Episode: 193 MeanR:19.2800 R: 9.0 Loss: 117.1780 ExploreP: 0.6777\n",
      "Episode: 194 MeanR:19.3600 R: 17.0 Loss: 257.1404 ExploreP: 0.6765\n",
      "Episode: 195 MeanR:19.3200 R: 18.0 Loss: 0.8799 ExploreP: 0.6753\n",
      "Episode: 196 MeanR:19.3600 R: 13.0 Loss: 1.0262 ExploreP: 0.6745\n",
      "Episode: 197 MeanR:19.2600 R: 9.0 Loss: 1.4239 ExploreP: 0.6739\n",
      "Episode: 198 MeanR:19.1500 R: 14.0 Loss: 1.4271 ExploreP: 0.6730\n",
      "Episode: 199 MeanR:19.0600 R: 11.0 Loss: 0.9930 ExploreP: 0.6722\n",
      "Episode: 200 MeanR:18.8300 R: 14.0 Loss: 0.8408 ExploreP: 0.6713\n",
      "Episode: 201 MeanR:18.8100 R: 9.0 Loss: 443.7148 ExploreP: 0.6707\n",
      "Episode: 202 MeanR:18.7800 R: 25.0 Loss: 0.7295 ExploreP: 0.6691\n",
      "Episode: 203 MeanR:18.8100 R: 13.0 Loss: 0.5148 ExploreP: 0.6682\n",
      "Episode: 204 MeanR:18.9200 R: 27.0 Loss: 0.8453 ExploreP: 0.6664\n",
      "Episode: 205 MeanR:18.8500 R: 14.0 Loss: 0.8391 ExploreP: 0.6655\n",
      "Episode: 206 MeanR:18.7000 R: 17.0 Loss: 0.4118 ExploreP: 0.6644\n",
      "Episode: 207 MeanR:18.7400 R: 25.0 Loss: 0.3810 ExploreP: 0.6628\n",
      "Episode: 208 MeanR:18.8000 R: 26.0 Loss: 0.4700 ExploreP: 0.6611\n",
      "Episode: 209 MeanR:18.8400 R: 37.0 Loss: 0.5059 ExploreP: 0.6587\n",
      "Episode: 210 MeanR:18.3000 R: 15.0 Loss: 1.0212 ExploreP: 0.6577\n",
      "Episode: 211 MeanR:18.3000 R: 19.0 Loss: 0.5067 ExploreP: 0.6565\n",
      "Episode: 212 MeanR:18.3200 R: 17.0 Loss: 0.7234 ExploreP: 0.6554\n",
      "Episode: 213 MeanR:18.3700 R: 19.0 Loss: 1.0269 ExploreP: 0.6541\n",
      "Episode: 214 MeanR:18.2700 R: 13.0 Loss: 259.9517 ExploreP: 0.6533\n",
      "Episode: 215 MeanR:18.1700 R: 13.0 Loss: 118.4513 ExploreP: 0.6525\n",
      "Episode: 216 MeanR:18.2700 R: 24.0 Loss: 1.1014 ExploreP: 0.6509\n",
      "Episode: 217 MeanR:18.1800 R: 12.0 Loss: 77.2033 ExploreP: 0.6502\n",
      "Episode: 218 MeanR:18.1900 R: 15.0 Loss: 112.5221 ExploreP: 0.6492\n",
      "Episode: 219 MeanR:17.9300 R: 12.0 Loss: 123.4240 ExploreP: 0.6484\n",
      "Episode: 220 MeanR:17.9700 R: 16.0 Loss: 0.6542 ExploreP: 0.6474\n",
      "Episode: 221 MeanR:17.9400 R: 13.0 Loss: 0.6714 ExploreP: 0.6466\n",
      "Episode: 222 MeanR:18.1200 R: 33.0 Loss: 0.8808 ExploreP: 0.6445\n",
      "Episode: 223 MeanR:18.2000 R: 19.0 Loss: 77.2566 ExploreP: 0.6433\n",
      "Episode: 224 MeanR:18.1500 R: 8.0 Loss: 96.5053 ExploreP: 0.6428\n",
      "Episode: 225 MeanR:18.2500 R: 32.0 Loss: 91.4491 ExploreP: 0.6407\n",
      "Episode: 226 MeanR:18.1800 R: 9.0 Loss: 91.4929 ExploreP: 0.6402\n",
      "Episode: 227 MeanR:18.0300 R: 8.0 Loss: 84.2381 ExploreP: 0.6397\n",
      "Episode: 228 MeanR:18.1700 R: 31.0 Loss: 0.8476 ExploreP: 0.6377\n",
      "Episode: 229 MeanR:18.1700 R: 19.0 Loss: 92.7789 ExploreP: 0.6365\n",
      "Episode: 230 MeanR:18.0100 R: 9.0 Loss: 101.4108 ExploreP: 0.6360\n",
      "Episode: 231 MeanR:18.0000 R: 21.0 Loss: 1.4376 ExploreP: 0.6347\n",
      "Episode: 232 MeanR:17.9600 R: 13.0 Loss: 1.0933 ExploreP: 0.6338\n",
      "Episode: 233 MeanR:18.0700 R: 24.0 Loss: 1.2105 ExploreP: 0.6324\n",
      "Episode: 234 MeanR:17.9600 R: 13.0 Loss: 1.7646 ExploreP: 0.6315\n",
      "Episode: 235 MeanR:17.9700 R: 22.0 Loss: 1.5742 ExploreP: 0.6302\n",
      "Episode: 236 MeanR:18.2300 R: 44.0 Loss: 160.3553 ExploreP: 0.6275\n",
      "Episode: 237 MeanR:18.0900 R: 10.0 Loss: 1.1615 ExploreP: 0.6268\n",
      "Episode: 238 MeanR:18.0700 R: 31.0 Loss: 211.5574 ExploreP: 0.6249\n",
      "Episode: 239 MeanR:17.9700 R: 15.0 Loss: 1.1685 ExploreP: 0.6240\n",
      "Episode: 240 MeanR:18.0600 R: 23.0 Loss: 1.3384 ExploreP: 0.6226\n",
      "Episode: 241 MeanR:17.8900 R: 13.0 Loss: 2.6641 ExploreP: 0.6218\n",
      "Episode: 242 MeanR:17.8900 R: 9.0 Loss: 1.1576 ExploreP: 0.6212\n",
      "Episode: 243 MeanR:17.9300 R: 22.0 Loss: 1.1910 ExploreP: 0.6199\n",
      "Episode: 244 MeanR:17.8400 R: 10.0 Loss: 109.4588 ExploreP: 0.6193\n",
      "Episode: 245 MeanR:17.7200 R: 10.0 Loss: 207.3405 ExploreP: 0.6187\n",
      "Episode: 246 MeanR:17.6900 R: 21.0 Loss: 1.3105 ExploreP: 0.6174\n",
      "Episode: 247 MeanR:17.7000 R: 12.0 Loss: 1.0813 ExploreP: 0.6167\n",
      "Episode: 248 MeanR:17.8700 R: 35.0 Loss: 74.8056 ExploreP: 0.6146\n",
      "Episode: 249 MeanR:17.7500 R: 12.0 Loss: 81.7503 ExploreP: 0.6138\n",
      "Episode: 250 MeanR:17.8400 R: 31.0 Loss: 69.7563 ExploreP: 0.6120\n",
      "Episode: 251 MeanR:18.1600 R: 45.0 Loss: 0.9107 ExploreP: 0.6093\n",
      "Episode: 252 MeanR:18.2200 R: 19.0 Loss: 92.4069 ExploreP: 0.6081\n",
      "Episode: 253 MeanR:18.2200 R: 15.0 Loss: 0.5175 ExploreP: 0.6072\n",
      "Episode: 254 MeanR:18.2200 R: 15.0 Loss: 0.7856 ExploreP: 0.6063\n",
      "Episode: 255 MeanR:17.9700 R: 9.0 Loss: 0.7370 ExploreP: 0.6058\n",
      "Episode: 256 MeanR:17.9400 R: 10.0 Loss: 0.8238 ExploreP: 0.6052\n",
      "Episode: 257 MeanR:17.9400 R: 11.0 Loss: 0.8730 ExploreP: 0.6046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 258 MeanR:17.8300 R: 10.0 Loss: 94.2040 ExploreP: 0.6040\n",
      "Episode: 259 MeanR:17.8200 R: 17.0 Loss: 82.3912 ExploreP: 0.6029\n",
      "Episode: 260 MeanR:17.8200 R: 18.0 Loss: 78.8140 ExploreP: 0.6019\n",
      "Episode: 261 MeanR:17.6600 R: 10.0 Loss: 64.3849 ExploreP: 0.6013\n",
      "Episode: 262 MeanR:17.3200 R: 11.0 Loss: 0.8759 ExploreP: 0.6006\n",
      "Episode: 263 MeanR:17.0200 R: 9.0 Loss: 0.7232 ExploreP: 0.6001\n",
      "Episode: 264 MeanR:17.0100 R: 12.0 Loss: 92.0887 ExploreP: 0.5994\n",
      "Episode: 265 MeanR:17.0300 R: 23.0 Loss: 69.5312 ExploreP: 0.5980\n",
      "Episode: 266 MeanR:16.9800 R: 12.0 Loss: 0.8240 ExploreP: 0.5973\n",
      "Episode: 267 MeanR:16.9300 R: 9.0 Loss: 0.6940 ExploreP: 0.5968\n",
      "Episode: 268 MeanR:16.9500 R: 15.0 Loss: 0.8239 ExploreP: 0.5959\n",
      "Episode: 269 MeanR:16.9600 R: 11.0 Loss: 0.6401 ExploreP: 0.5953\n",
      "Episode: 270 MeanR:16.7600 R: 29.0 Loss: 91.1557 ExploreP: 0.5936\n",
      "Episode: 271 MeanR:16.7900 R: 15.0 Loss: 100.7286 ExploreP: 0.5927\n",
      "Episode: 272 MeanR:16.8000 R: 10.0 Loss: 80.1571 ExploreP: 0.5921\n",
      "Episode: 273 MeanR:16.8200 R: 15.0 Loss: 62.3401 ExploreP: 0.5913\n",
      "Episode: 274 MeanR:16.7200 R: 16.0 Loss: 0.6533 ExploreP: 0.5903\n",
      "Episode: 275 MeanR:16.8000 R: 19.0 Loss: 0.5619 ExploreP: 0.5892\n",
      "Episode: 276 MeanR:16.7500 R: 11.0 Loss: 0.6576 ExploreP: 0.5886\n",
      "Episode: 277 MeanR:16.8500 R: 19.0 Loss: 0.5975 ExploreP: 0.5875\n",
      "Episode: 278 MeanR:16.8900 R: 14.0 Loss: 0.7332 ExploreP: 0.5867\n",
      "Episode: 279 MeanR:17.0000 R: 27.0 Loss: 0.7749 ExploreP: 0.5851\n",
      "Episode: 280 MeanR:16.9600 R: 13.0 Loss: 0.7752 ExploreP: 0.5844\n",
      "Episode: 281 MeanR:16.8600 R: 13.0 Loss: 98.4937 ExploreP: 0.5836\n",
      "Episode: 282 MeanR:16.9900 R: 21.0 Loss: 1.0210 ExploreP: 0.5824\n",
      "Episode: 283 MeanR:17.0200 R: 14.0 Loss: 0.6083 ExploreP: 0.5816\n",
      "Episode: 284 MeanR:16.9100 R: 8.0 Loss: 0.6179 ExploreP: 0.5812\n",
      "Episode: 285 MeanR:16.9100 R: 9.0 Loss: 0.7296 ExploreP: 0.5807\n",
      "Episode: 286 MeanR:16.7000 R: 14.0 Loss: 69.0739 ExploreP: 0.5799\n",
      "Episode: 287 MeanR:16.6400 R: 7.0 Loss: 0.8537 ExploreP: 0.5795\n",
      "Episode: 288 MeanR:16.6200 R: 10.0 Loss: 370.0086 ExploreP: 0.5789\n",
      "Episode: 289 MeanR:16.6300 R: 14.0 Loss: 0.5310 ExploreP: 0.5781\n",
      "Episode: 290 MeanR:16.6900 R: 15.0 Loss: 85.2603 ExploreP: 0.5773\n",
      "Episode: 291 MeanR:16.6100 R: 11.0 Loss: 142.3974 ExploreP: 0.5766\n",
      "Episode: 292 MeanR:16.6100 R: 11.0 Loss: 150.6553 ExploreP: 0.5760\n",
      "Episode: 293 MeanR:16.6500 R: 13.0 Loss: 127.6225 ExploreP: 0.5753\n",
      "Episode: 294 MeanR:16.5900 R: 11.0 Loss: 47.4825 ExploreP: 0.5746\n",
      "Episode: 295 MeanR:17.1200 R: 71.0 Loss: 182.0941 ExploreP: 0.5707\n",
      "Episode: 296 MeanR:17.1100 R: 12.0 Loss: 219.2283 ExploreP: 0.5700\n",
      "Episode: 297 MeanR:17.1600 R: 14.0 Loss: 117.2708 ExploreP: 0.5692\n",
      "Episode: 298 MeanR:17.1400 R: 12.0 Loss: 77.7085 ExploreP: 0.5685\n",
      "Episode: 299 MeanR:17.2300 R: 20.0 Loss: 0.6318 ExploreP: 0.5674\n",
      "Episode: 300 MeanR:17.3100 R: 22.0 Loss: 88.9794 ExploreP: 0.5662\n",
      "Episode: 301 MeanR:17.6800 R: 46.0 Loss: 0.7694 ExploreP: 0.5636\n",
      "Episode: 302 MeanR:17.6600 R: 23.0 Loss: 225.8512 ExploreP: 0.5624\n",
      "Episode: 303 MeanR:17.8100 R: 28.0 Loss: 0.4692 ExploreP: 0.5608\n",
      "Episode: 304 MeanR:17.9600 R: 42.0 Loss: 61.6524 ExploreP: 0.5585\n",
      "Episode: 305 MeanR:18.4400 R: 62.0 Loss: 118.0699 ExploreP: 0.5551\n",
      "Episode: 306 MeanR:18.4100 R: 14.0 Loss: 0.5460 ExploreP: 0.5544\n",
      "Episode: 307 MeanR:18.2400 R: 8.0 Loss: 44.8181 ExploreP: 0.5539\n",
      "Episode: 308 MeanR:18.1100 R: 13.0 Loss: 86.4775 ExploreP: 0.5532\n",
      "Episode: 309 MeanR:18.0000 R: 26.0 Loss: 53.7911 ExploreP: 0.5518\n",
      "Episode: 310 MeanR:18.0900 R: 24.0 Loss: 100.1457 ExploreP: 0.5505\n",
      "Episode: 311 MeanR:18.3000 R: 40.0 Loss: 0.5362 ExploreP: 0.5483\n",
      "Episode: 312 MeanR:18.4600 R: 33.0 Loss: 0.5063 ExploreP: 0.5466\n",
      "Episode: 313 MeanR:18.4400 R: 17.0 Loss: 21.2907 ExploreP: 0.5457\n",
      "Episode: 314 MeanR:18.4600 R: 15.0 Loss: 0.5065 ExploreP: 0.5449\n",
      "Episode: 315 MeanR:18.6100 R: 28.0 Loss: 0.4784 ExploreP: 0.5434\n",
      "Episode: 316 MeanR:18.5900 R: 22.0 Loss: 0.5199 ExploreP: 0.5422\n",
      "Episode: 317 MeanR:18.7000 R: 23.0 Loss: 0.8086 ExploreP: 0.5410\n",
      "Episode: 318 MeanR:18.8000 R: 25.0 Loss: 29.0066 ExploreP: 0.5396\n",
      "Episode: 319 MeanR:18.8300 R: 15.0 Loss: 43.7875 ExploreP: 0.5389\n",
      "Episode: 320 MeanR:19.1600 R: 49.0 Loss: 0.8887 ExploreP: 0.5363\n",
      "Episode: 321 MeanR:19.1300 R: 10.0 Loss: 0.5014 ExploreP: 0.5357\n",
      "Episode: 322 MeanR:18.8900 R: 9.0 Loss: 0.6278 ExploreP: 0.5353\n",
      "Episode: 323 MeanR:18.8000 R: 10.0 Loss: 0.8071 ExploreP: 0.5347\n",
      "Episode: 324 MeanR:18.8400 R: 12.0 Loss: 0.6693 ExploreP: 0.5341\n",
      "Episode: 325 MeanR:18.7300 R: 21.0 Loss: 37.2475 ExploreP: 0.5330\n",
      "Episode: 326 MeanR:18.7700 R: 13.0 Loss: 0.8048 ExploreP: 0.5323\n",
      "Episode: 327 MeanR:18.7700 R: 8.0 Loss: 85.6874 ExploreP: 0.5319\n",
      "Episode: 328 MeanR:18.6100 R: 15.0 Loss: 0.4170 ExploreP: 0.5311\n",
      "Episode: 329 MeanR:18.5800 R: 16.0 Loss: 47.6298 ExploreP: 0.5303\n",
      "Episode: 330 MeanR:18.7100 R: 22.0 Loss: 0.5547 ExploreP: 0.5292\n",
      "Episode: 331 MeanR:18.9600 R: 46.0 Loss: 36.2605 ExploreP: 0.5268\n",
      "Episode: 332 MeanR:19.1100 R: 28.0 Loss: 31.4303 ExploreP: 0.5253\n",
      "Episode: 333 MeanR:19.0900 R: 22.0 Loss: 79.9051 ExploreP: 0.5242\n",
      "Episode: 334 MeanR:19.4200 R: 46.0 Loss: 35.7985 ExploreP: 0.5218\n",
      "Episode: 335 MeanR:19.9100 R: 71.0 Loss: 59.0998 ExploreP: 0.5182\n",
      "Episode: 336 MeanR:19.5500 R: 8.0 Loss: 0.6033 ExploreP: 0.5178\n",
      "Episode: 337 MeanR:19.9200 R: 47.0 Loss: 1.2027 ExploreP: 0.5154\n",
      "Episode: 338 MeanR:19.8100 R: 20.0 Loss: 70.6219 ExploreP: 0.5144\n",
      "Episode: 339 MeanR:19.8000 R: 14.0 Loss: 42.4147 ExploreP: 0.5137\n",
      "Episode: 340 MeanR:20.0700 R: 50.0 Loss: 0.6739 ExploreP: 0.5112\n",
      "Episode: 341 MeanR:20.3600 R: 42.0 Loss: 59.2915 ExploreP: 0.5091\n",
      "Episode: 342 MeanR:20.6200 R: 35.0 Loss: 1.0871 ExploreP: 0.5074\n",
      "Episode: 343 MeanR:20.5300 R: 13.0 Loss: 97.8348 ExploreP: 0.5067\n",
      "Episode: 344 MeanR:20.6800 R: 25.0 Loss: 0.7483 ExploreP: 0.5055\n",
      "Episode: 345 MeanR:20.7800 R: 20.0 Loss: 27.6406 ExploreP: 0.5045\n",
      "Episode: 346 MeanR:20.6700 R: 10.0 Loss: 135.0100 ExploreP: 0.5040\n",
      "Episode: 347 MeanR:20.6300 R: 8.0 Loss: 37.5154 ExploreP: 0.5036\n",
      "Episode: 348 MeanR:20.5200 R: 24.0 Loss: 79.9519 ExploreP: 0.5024\n",
      "Episode: 349 MeanR:20.7800 R: 38.0 Loss: 67.1229 ExploreP: 0.5005\n",
      "Episode: 350 MeanR:20.9200 R: 45.0 Loss: 1.0190 ExploreP: 0.4983\n",
      "Episode: 351 MeanR:21.0600 R: 59.0 Loss: 0.8454 ExploreP: 0.4955\n",
      "Episode: 352 MeanR:21.0900 R: 22.0 Loss: 25.0982 ExploreP: 0.4944\n",
      "Episode: 353 MeanR:21.1900 R: 25.0 Loss: 27.4697 ExploreP: 0.4932\n",
      "Episode: 354 MeanR:21.2200 R: 18.0 Loss: 0.8105 ExploreP: 0.4923\n",
      "Episode: 355 MeanR:21.9200 R: 79.0 Loss: 56.6640 ExploreP: 0.4885\n",
      "Episode: 356 MeanR:22.1400 R: 32.0 Loss: 23.9799 ExploreP: 0.4870\n",
      "Episode: 357 MeanR:22.5700 R: 54.0 Loss: 25.3724 ExploreP: 0.4844\n",
      "Episode: 358 MeanR:22.7500 R: 28.0 Loss: 0.9130 ExploreP: 0.4831\n",
      "Episode: 359 MeanR:22.8800 R: 30.0 Loss: 0.6636 ExploreP: 0.4817\n",
      "Episode: 360 MeanR:23.0900 R: 39.0 Loss: 0.3418 ExploreP: 0.4798\n",
      "Episode: 361 MeanR:23.2600 R: 27.0 Loss: 1.0814 ExploreP: 0.4786\n",
      "Episode: 362 MeanR:23.4400 R: 29.0 Loss: 19.7236 ExploreP: 0.4772\n",
      "Episode: 363 MeanR:23.6700 R: 32.0 Loss: 31.1856 ExploreP: 0.4757\n",
      "Episode: 364 MeanR:24.0100 R: 46.0 Loss: 26.5740 ExploreP: 0.4736\n",
      "Episode: 365 MeanR:24.1000 R: 32.0 Loss: 0.9772 ExploreP: 0.4721\n",
      "Episode: 366 MeanR:24.8500 R: 87.0 Loss: 0.6214 ExploreP: 0.4681\n",
      "Episode: 367 MeanR:25.0000 R: 24.0 Loss: 105.9586 ExploreP: 0.4670\n",
      "Episode: 368 MeanR:25.2400 R: 39.0 Loss: 1.1730 ExploreP: 0.4652\n",
      "Episode: 369 MeanR:25.5100 R: 38.0 Loss: 38.7941 ExploreP: 0.4635\n",
      "Episode: 370 MeanR:25.8100 R: 59.0 Loss: 0.8657 ExploreP: 0.4608\n",
      "Episode: 371 MeanR:25.9900 R: 33.0 Loss: 22.9335 ExploreP: 0.4594\n",
      "Episode: 372 MeanR:26.1300 R: 24.0 Loss: 0.8726 ExploreP: 0.4583\n",
      "Episode: 373 MeanR:26.3000 R: 32.0 Loss: 21.9987 ExploreP: 0.4568\n",
      "Episode: 374 MeanR:26.7400 R: 60.0 Loss: 18.3430 ExploreP: 0.4542\n",
      "Episode: 375 MeanR:26.9000 R: 35.0 Loss: 1.0113 ExploreP: 0.4526\n",
      "Episode: 376 MeanR:27.4100 R: 62.0 Loss: 1.0024 ExploreP: 0.4499\n",
      "Episode: 377 MeanR:27.6700 R: 45.0 Loss: 33.3793 ExploreP: 0.4479\n",
      "Episode: 378 MeanR:27.7700 R: 24.0 Loss: 0.5082 ExploreP: 0.4469\n",
      "Episode: 379 MeanR:28.0800 R: 58.0 Loss: 1.0421 ExploreP: 0.4443\n",
      "Episode: 380 MeanR:28.5800 R: 63.0 Loss: 29.0600 ExploreP: 0.4416\n",
      "Episode: 381 MeanR:29.1500 R: 70.0 Loss: 1.1212 ExploreP: 0.4386\n",
      "Episode: 382 MeanR:29.3100 R: 37.0 Loss: 0.6295 ExploreP: 0.4370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 383 MeanR:29.8400 R: 67.0 Loss: 33.5193 ExploreP: 0.4342\n",
      "Episode: 384 MeanR:30.5600 R: 80.0 Loss: 0.8762 ExploreP: 0.4308\n",
      "Episode: 385 MeanR:30.9200 R: 45.0 Loss: 1.1541 ExploreP: 0.4289\n",
      "Episode: 386 MeanR:31.2200 R: 44.0 Loss: 23.4301 ExploreP: 0.4270\n",
      "Episode: 387 MeanR:31.4300 R: 28.0 Loss: 19.3918 ExploreP: 0.4259\n",
      "Episode: 388 MeanR:32.8000 R: 147.0 Loss: 0.6043 ExploreP: 0.4198\n",
      "Episode: 389 MeanR:33.4200 R: 76.0 Loss: 1.1663 ExploreP: 0.4167\n",
      "Episode: 390 MeanR:33.8500 R: 58.0 Loss: 1.3162 ExploreP: 0.4144\n",
      "Episode: 391 MeanR:34.1900 R: 45.0 Loss: 17.7218 ExploreP: 0.4125\n",
      "Episode: 392 MeanR:34.5000 R: 42.0 Loss: 21.3257 ExploreP: 0.4109\n",
      "Episode: 393 MeanR:34.8600 R: 49.0 Loss: 60.1616 ExploreP: 0.4089\n",
      "Episode: 394 MeanR:35.3700 R: 62.0 Loss: 0.9830 ExploreP: 0.4064\n",
      "Episode: 395 MeanR:34.9700 R: 31.0 Loss: 1.3066 ExploreP: 0.4052\n",
      "Episode: 396 MeanR:35.3100 R: 46.0 Loss: 1.3904 ExploreP: 0.4034\n",
      "Episode: 397 MeanR:35.5000 R: 33.0 Loss: 1.5367 ExploreP: 0.4021\n",
      "Episode: 398 MeanR:35.6700 R: 29.0 Loss: 60.5606 ExploreP: 0.4010\n",
      "Episode: 399 MeanR:36.3600 R: 89.0 Loss: 0.9436 ExploreP: 0.3975\n",
      "Episode: 400 MeanR:36.6200 R: 48.0 Loss: 1.1701 ExploreP: 0.3956\n",
      "Episode: 401 MeanR:36.6400 R: 48.0 Loss: 9.3141 ExploreP: 0.3938\n",
      "Episode: 402 MeanR:36.8900 R: 48.0 Loss: 1.3922 ExploreP: 0.3920\n",
      "Episode: 403 MeanR:37.6800 R: 107.0 Loss: 1.1164 ExploreP: 0.3879\n",
      "Episode: 404 MeanR:38.2100 R: 95.0 Loss: 36.4231 ExploreP: 0.3843\n",
      "Episode: 405 MeanR:38.8700 R: 128.0 Loss: 13.7439 ExploreP: 0.3796\n",
      "Episode: 406 MeanR:39.4200 R: 69.0 Loss: 1.1250 ExploreP: 0.3770\n",
      "Episode: 407 MeanR:39.8800 R: 54.0 Loss: 1.5144 ExploreP: 0.3750\n",
      "Episode: 408 MeanR:40.2500 R: 50.0 Loss: 29.1185 ExploreP: 0.3732\n",
      "Episode: 409 MeanR:40.7000 R: 71.0 Loss: 0.5346 ExploreP: 0.3706\n",
      "Episode: 410 MeanR:40.8800 R: 42.0 Loss: 25.6478 ExploreP: 0.3691\n",
      "Episode: 411 MeanR:40.9300 R: 45.0 Loss: 15.2404 ExploreP: 0.3675\n",
      "Episode: 412 MeanR:40.9200 R: 32.0 Loss: 29.2313 ExploreP: 0.3664\n",
      "Episode: 413 MeanR:41.0500 R: 30.0 Loss: 20.0605 ExploreP: 0.3653\n",
      "Episode: 414 MeanR:41.3300 R: 43.0 Loss: 26.4089 ExploreP: 0.3638\n",
      "Episode: 415 MeanR:41.8500 R: 80.0 Loss: 23.0309 ExploreP: 0.3610\n",
      "Episode: 416 MeanR:42.2600 R: 63.0 Loss: 1.5831 ExploreP: 0.3588\n",
      "Episode: 417 MeanR:42.4700 R: 44.0 Loss: 1.5353 ExploreP: 0.3572\n",
      "Episode: 418 MeanR:42.8600 R: 64.0 Loss: 21.1074 ExploreP: 0.3550\n",
      "Episode: 419 MeanR:42.9600 R: 25.0 Loss: 1.0197 ExploreP: 0.3542\n",
      "Episode: 420 MeanR:43.2200 R: 75.0 Loss: 16.7695 ExploreP: 0.3516\n",
      "Episode: 421 MeanR:43.5300 R: 41.0 Loss: 31.5716 ExploreP: 0.3502\n",
      "Episode: 422 MeanR:44.1800 R: 74.0 Loss: 1.0762 ExploreP: 0.3477\n",
      "Episode: 423 MeanR:44.4500 R: 37.0 Loss: 1.3638 ExploreP: 0.3464\n",
      "Episode: 424 MeanR:45.3900 R: 106.0 Loss: 1.1077 ExploreP: 0.3429\n",
      "Episode: 425 MeanR:46.1800 R: 100.0 Loss: 1.1058 ExploreP: 0.3396\n",
      "Episode: 426 MeanR:46.4800 R: 43.0 Loss: 1.0537 ExploreP: 0.3382\n",
      "Episode: 427 MeanR:47.4900 R: 109.0 Loss: 1.0356 ExploreP: 0.3346\n",
      "Episode: 428 MeanR:48.0900 R: 75.0 Loss: 7.7365 ExploreP: 0.3322\n",
      "Episode: 429 MeanR:48.5900 R: 66.0 Loss: 13.1331 ExploreP: 0.3301\n",
      "Episode: 430 MeanR:49.2100 R: 84.0 Loss: 1.7954 ExploreP: 0.3274\n",
      "Episode: 431 MeanR:49.2100 R: 46.0 Loss: 14.6573 ExploreP: 0.3259\n",
      "Episode: 432 MeanR:49.4600 R: 53.0 Loss: 1.4023 ExploreP: 0.3243\n",
      "Episode: 433 MeanR:49.6400 R: 40.0 Loss: 45.9961 ExploreP: 0.3230\n",
      "Episode: 434 MeanR:49.5300 R: 35.0 Loss: 37.7680 ExploreP: 0.3219\n",
      "Episode: 435 MeanR:49.3200 R: 50.0 Loss: 2.0140 ExploreP: 0.3204\n",
      "Episode: 436 MeanR:49.6600 R: 42.0 Loss: 0.8434 ExploreP: 0.3191\n",
      "Episode: 437 MeanR:49.9200 R: 73.0 Loss: 39.1246 ExploreP: 0.3168\n",
      "Episode: 438 MeanR:50.2600 R: 54.0 Loss: 93.6146 ExploreP: 0.3152\n",
      "Episode: 439 MeanR:50.9600 R: 84.0 Loss: 1.3025 ExploreP: 0.3126\n",
      "Episode: 440 MeanR:50.8100 R: 35.0 Loss: 2.4940 ExploreP: 0.3115\n",
      "Episode: 441 MeanR:51.1600 R: 77.0 Loss: 1.4890 ExploreP: 0.3092\n",
      "Episode: 442 MeanR:51.8400 R: 103.0 Loss: 16.6911 ExploreP: 0.3062\n",
      "Episode: 443 MeanR:52.4800 R: 77.0 Loss: 1.3451 ExploreP: 0.3039\n",
      "Episode: 444 MeanR:52.8200 R: 59.0 Loss: 35.8576 ExploreP: 0.3022\n",
      "Episode: 445 MeanR:52.9500 R: 33.0 Loss: 66.4675 ExploreP: 0.3012\n",
      "Episode: 446 MeanR:53.8600 R: 101.0 Loss: 1.9900 ExploreP: 0.2983\n",
      "Episode: 447 MeanR:55.3700 R: 159.0 Loss: 1.2667 ExploreP: 0.2937\n",
      "Episode: 448 MeanR:56.1600 R: 103.0 Loss: 21.2820 ExploreP: 0.2908\n",
      "Episode: 449 MeanR:57.9500 R: 217.0 Loss: 0.5249 ExploreP: 0.2848\n",
      "Episode: 450 MeanR:58.4100 R: 91.0 Loss: 1.2327 ExploreP: 0.2823\n",
      "Episode: 451 MeanR:58.5400 R: 72.0 Loss: 1.0951 ExploreP: 0.2803\n",
      "Episode: 452 MeanR:59.4900 R: 117.0 Loss: 25.1297 ExploreP: 0.2772\n",
      "Episode: 453 MeanR:60.4000 R: 116.0 Loss: 19.0159 ExploreP: 0.2741\n",
      "Episode: 454 MeanR:61.3600 R: 114.0 Loss: 2.1995 ExploreP: 0.2711\n",
      "Episode: 455 MeanR:62.8500 R: 228.0 Loss: 1.4467 ExploreP: 0.2652\n",
      "Episode: 456 MeanR:63.0500 R: 52.0 Loss: 2.0366 ExploreP: 0.2639\n",
      "Episode: 457 MeanR:64.3500 R: 184.0 Loss: 3.4531 ExploreP: 0.2593\n",
      "Episode: 458 MeanR:65.7600 R: 169.0 Loss: 1.2156 ExploreP: 0.2551\n",
      "Episode: 459 MeanR:66.5000 R: 104.0 Loss: 1.4055 ExploreP: 0.2526\n",
      "Episode: 460 MeanR:67.7300 R: 162.0 Loss: 173.1809 ExploreP: 0.2487\n",
      "Episode: 461 MeanR:68.6000 R: 114.0 Loss: 1.1998 ExploreP: 0.2460\n",
      "Episode: 462 MeanR:69.6200 R: 131.0 Loss: 1.7761 ExploreP: 0.2429\n",
      "Episode: 463 MeanR:70.2100 R: 91.0 Loss: 27.5927 ExploreP: 0.2408\n",
      "Episode: 464 MeanR:71.3600 R: 161.0 Loss: 1.6439 ExploreP: 0.2371\n",
      "Episode: 465 MeanR:71.6300 R: 59.0 Loss: 2.1731 ExploreP: 0.2358\n",
      "Episode: 466 MeanR:72.0700 R: 131.0 Loss: 0.9677 ExploreP: 0.2328\n",
      "Episode: 467 MeanR:73.3900 R: 156.0 Loss: 2.4861 ExploreP: 0.2294\n",
      "Episode: 468 MeanR:74.7700 R: 177.0 Loss: 55.9359 ExploreP: 0.2255\n",
      "Episode: 469 MeanR:74.6100 R: 22.0 Loss: 1.3169 ExploreP: 0.2251\n",
      "Episode: 470 MeanR:75.7800 R: 176.0 Loss: 0.9579 ExploreP: 0.2213\n",
      "Episode: 471 MeanR:77.1800 R: 173.0 Loss: 21.8702 ExploreP: 0.2177\n",
      "Episode: 472 MeanR:77.9000 R: 96.0 Loss: 0.6784 ExploreP: 0.2157\n",
      "Episode: 473 MeanR:78.3000 R: 72.0 Loss: 2.1893 ExploreP: 0.2142\n",
      "Episode: 474 MeanR:79.0300 R: 133.0 Loss: 25.3539 ExploreP: 0.2115\n",
      "Episode: 475 MeanR:80.1300 R: 145.0 Loss: 1.1613 ExploreP: 0.2086\n",
      "Episode: 476 MeanR:80.7700 R: 126.0 Loss: 1.9938 ExploreP: 0.2061\n",
      "Episode: 477 MeanR:81.8600 R: 154.0 Loss: 0.9917 ExploreP: 0.2031\n",
      "Episode: 478 MeanR:82.2900 R: 67.0 Loss: 0.9481 ExploreP: 0.2018\n",
      "Episode: 479 MeanR:83.0100 R: 130.0 Loss: 1.1661 ExploreP: 0.1994\n",
      "Episode: 480 MeanR:83.9600 R: 158.0 Loss: 1.6806 ExploreP: 0.1964\n",
      "Episode: 481 MeanR:84.8400 R: 158.0 Loss: 1.0451 ExploreP: 0.1935\n",
      "Episode: 482 MeanR:85.5500 R: 108.0 Loss: 1.1926 ExploreP: 0.1915\n",
      "Episode: 483 MeanR:86.4200 R: 154.0 Loss: 1.1643 ExploreP: 0.1887\n",
      "Episode: 484 MeanR:86.2900 R: 67.0 Loss: 1.1820 ExploreP: 0.1875\n",
      "Episode: 485 MeanR:86.6100 R: 77.0 Loss: 0.7234 ExploreP: 0.1862\n",
      "Episode: 486 MeanR:88.0400 R: 187.0 Loss: 1.7055 ExploreP: 0.1829\n",
      "Episode: 487 MeanR:88.4000 R: 64.0 Loss: 37.7927 ExploreP: 0.1818\n",
      "Episode: 488 MeanR:87.5200 R: 59.0 Loss: 1.9578 ExploreP: 0.1808\n",
      "Episode: 489 MeanR:87.3100 R: 55.0 Loss: 0.8315 ExploreP: 0.1799\n",
      "Episode: 490 MeanR:88.0700 R: 134.0 Loss: 0.9696 ExploreP: 0.1776\n",
      "Episode: 491 MeanR:88.4500 R: 83.0 Loss: 1.0109 ExploreP: 0.1762\n",
      "Episode: 492 MeanR:88.4800 R: 45.0 Loss: 1.1157 ExploreP: 0.1755\n",
      "Episode: 493 MeanR:88.3400 R: 35.0 Loss: 1.2185 ExploreP: 0.1749\n",
      "Episode: 494 MeanR:88.0000 R: 28.0 Loss: 1.4443 ExploreP: 0.1744\n",
      "Episode: 495 MeanR:88.6500 R: 96.0 Loss: 1.3498 ExploreP: 0.1729\n",
      "Episode: 496 MeanR:88.7500 R: 56.0 Loss: 2.6865 ExploreP: 0.1720\n",
      "Episode: 497 MeanR:88.9300 R: 51.0 Loss: 517.0322 ExploreP: 0.1711\n",
      "Episode: 498 MeanR:89.1600 R: 52.0 Loss: 1.2849 ExploreP: 0.1703\n",
      "Episode: 499 MeanR:88.8100 R: 54.0 Loss: 43.4544 ExploreP: 0.1694\n",
      "Episode: 500 MeanR:88.8800 R: 55.0 Loss: 2.2348 ExploreP: 0.1686\n",
      "Episode: 501 MeanR:89.0800 R: 68.0 Loss: 1.9733 ExploreP: 0.1675\n",
      "Episode: 502 MeanR:89.2400 R: 64.0 Loss: 1.4668 ExploreP: 0.1665\n",
      "Episode: 503 MeanR:89.4200 R: 125.0 Loss: 248.8232 ExploreP: 0.1645\n",
      "Episode: 504 MeanR:89.0100 R: 54.0 Loss: 40.6614 ExploreP: 0.1637\n",
      "Episode: 505 MeanR:88.3000 R: 57.0 Loss: 1.6460 ExploreP: 0.1628\n",
      "Episode: 506 MeanR:88.2200 R: 61.0 Loss: 1.7168 ExploreP: 0.1619\n",
      "Episode: 507 MeanR:88.9300 R: 125.0 Loss: 1.8188 ExploreP: 0.1600\n",
      "Episode: 508 MeanR:88.9800 R: 55.0 Loss: 1.3834 ExploreP: 0.1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 509 MeanR:88.9200 R: 65.0 Loss: 1.4836 ExploreP: 0.1582\n",
      "Episode: 510 MeanR:89.2800 R: 78.0 Loss: 1.5441 ExploreP: 0.1571\n",
      "Episode: 511 MeanR:89.8300 R: 100.0 Loss: 193.2191 ExploreP: 0.1556\n",
      "Episode: 512 MeanR:90.2800 R: 77.0 Loss: 1.5069 ExploreP: 0.1545\n",
      "Episode: 513 MeanR:90.9500 R: 97.0 Loss: 271.0303 ExploreP: 0.1531\n",
      "Episode: 514 MeanR:91.8400 R: 132.0 Loss: 2.1094 ExploreP: 0.1512\n",
      "Episode: 515 MeanR:92.0000 R: 96.0 Loss: 1.4067 ExploreP: 0.1499\n",
      "Episode: 516 MeanR:92.6000 R: 123.0 Loss: 1.4237 ExploreP: 0.1482\n",
      "Episode: 517 MeanR:93.3600 R: 120.0 Loss: 1.1398 ExploreP: 0.1465\n",
      "Episode: 518 MeanR:93.8900 R: 117.0 Loss: 0.9592 ExploreP: 0.1449\n",
      "Episode: 519 MeanR:94.3600 R: 72.0 Loss: 1.1694 ExploreP: 0.1440\n",
      "Episode: 520 MeanR:94.0700 R: 46.0 Loss: 1.2497 ExploreP: 0.1433\n",
      "Episode: 521 MeanR:94.5800 R: 92.0 Loss: 1.2919 ExploreP: 0.1421\n",
      "Episode: 522 MeanR:95.0800 R: 124.0 Loss: 3.1754 ExploreP: 0.1405\n",
      "Episode: 523 MeanR:95.4200 R: 71.0 Loss: 0.6230 ExploreP: 0.1396\n",
      "Episode: 524 MeanR:96.4100 R: 205.0 Loss: 1.3230 ExploreP: 0.1369\n",
      "Episode: 525 MeanR:97.0400 R: 163.0 Loss: 1.0359 ExploreP: 0.1349\n",
      "Episode: 526 MeanR:97.4800 R: 87.0 Loss: 101.8111 ExploreP: 0.1338\n",
      "Episode: 527 MeanR:99.1200 R: 273.0 Loss: 243.8085 ExploreP: 0.1305\n",
      "Episode: 528 MeanR:100.5700 R: 220.0 Loss: 88.5056 ExploreP: 0.1278\n",
      "Episode: 529 MeanR:101.6300 R: 172.0 Loss: 84.7927 ExploreP: 0.1258\n",
      "Episode: 530 MeanR:105.4800 R: 469.0 Loss: 1.1155 ExploreP: 0.1205\n",
      "Episode: 531 MeanR:107.3900 R: 237.0 Loss: 0.8675 ExploreP: 0.1179\n",
      "Episode: 532 MeanR:108.3800 R: 152.0 Loss: 1.1059 ExploreP: 0.1163\n",
      "Episode: 533 MeanR:110.4500 R: 247.0 Loss: 1.2443 ExploreP: 0.1137\n",
      "Episode: 534 MeanR:111.7800 R: 168.0 Loss: 0.7392 ExploreP: 0.1120\n",
      "Episode: 535 MeanR:113.8300 R: 255.0 Loss: 0.7627 ExploreP: 0.1094\n",
      "Episode: 536 MeanR:115.1500 R: 174.0 Loss: 1.1997 ExploreP: 0.1077\n",
      "Episode: 537 MeanR:115.5000 R: 108.0 Loss: 0.7906 ExploreP: 0.1067\n",
      "Episode: 538 MeanR:116.4800 R: 152.0 Loss: 0.5894 ExploreP: 0.1052\n",
      "Episode: 539 MeanR:118.1300 R: 249.0 Loss: 0.6191 ExploreP: 0.1029\n",
      "Episode: 540 MeanR:120.6300 R: 285.0 Loss: 0.8947 ExploreP: 0.1003\n",
      "Episode: 541 MeanR:121.2400 R: 138.0 Loss: 0.7294 ExploreP: 0.0990\n",
      "Episode: 542 MeanR:122.2500 R: 204.0 Loss: 0.3528 ExploreP: 0.0972\n",
      "Episode: 543 MeanR:123.4200 R: 194.0 Loss: 0.7866 ExploreP: 0.0955\n",
      "Episode: 544 MeanR:124.9600 R: 213.0 Loss: 171.8685 ExploreP: 0.0937\n",
      "Episode: 545 MeanR:126.0500 R: 142.0 Loss: 155.4583 ExploreP: 0.0926\n",
      "Episode: 546 MeanR:126.4000 R: 136.0 Loss: 161.5926 ExploreP: 0.0914\n",
      "Episode: 547 MeanR:126.7700 R: 196.0 Loss: 0.5739 ExploreP: 0.0899\n",
      "Episode: 548 MeanR:127.9300 R: 219.0 Loss: 0.3508 ExploreP: 0.0881\n",
      "Episode: 549 MeanR:127.4800 R: 172.0 Loss: 0.4607 ExploreP: 0.0868\n",
      "Episode: 550 MeanR:128.0300 R: 146.0 Loss: 0.3212 ExploreP: 0.0857\n",
      "Episode: 551 MeanR:128.9000 R: 159.0 Loss: 147.4243 ExploreP: 0.0845\n",
      "Episode: 552 MeanR:129.2500 R: 152.0 Loss: 0.9293 ExploreP: 0.0834\n",
      "Episode: 553 MeanR:129.7300 R: 164.0 Loss: 0.7641 ExploreP: 0.0822\n",
      "Episode: 554 MeanR:130.2600 R: 167.0 Loss: 0.4374 ExploreP: 0.0810\n",
      "Episode: 555 MeanR:129.7100 R: 173.0 Loss: 0.7031 ExploreP: 0.0798\n",
      "Episode: 556 MeanR:130.9300 R: 174.0 Loss: 0.5724 ExploreP: 0.0786\n",
      "Episode: 557 MeanR:130.4800 R: 139.0 Loss: 0.4257 ExploreP: 0.0776\n",
      "Episode: 558 MeanR:130.0500 R: 126.0 Loss: 0.3362 ExploreP: 0.0768\n",
      "Episode: 559 MeanR:130.4200 R: 141.0 Loss: 141.9478 ExploreP: 0.0758\n",
      "Episode: 560 MeanR:130.4500 R: 165.0 Loss: 0.3671 ExploreP: 0.0748\n",
      "Episode: 561 MeanR:130.8200 R: 151.0 Loss: 0.5373 ExploreP: 0.0738\n",
      "Episode: 562 MeanR:130.5900 R: 108.0 Loss: 0.4437 ExploreP: 0.0731\n",
      "Episode: 563 MeanR:131.0900 R: 141.0 Loss: 0.4597 ExploreP: 0.0722\n",
      "Episode: 564 MeanR:130.7000 R: 122.0 Loss: 0.4112 ExploreP: 0.0715\n",
      "Episode: 565 MeanR:131.3100 R: 120.0 Loss: 0.5030 ExploreP: 0.0707\n",
      "Episode: 566 MeanR:131.2700 R: 127.0 Loss: 0.7328 ExploreP: 0.0700\n",
      "Episode: 567 MeanR:130.8500 R: 114.0 Loss: 0.7848 ExploreP: 0.0693\n",
      "Episode: 568 MeanR:129.9300 R: 85.0 Loss: 0.2499 ExploreP: 0.0688\n",
      "Episode: 569 MeanR:130.6000 R: 89.0 Loss: 0.5816 ExploreP: 0.0683\n",
      "Episode: 570 MeanR:129.9800 R: 114.0 Loss: 0.4069 ExploreP: 0.0676\n",
      "Episode: 571 MeanR:129.2500 R: 100.0 Loss: 0.6263 ExploreP: 0.0670\n",
      "Episode: 572 MeanR:128.9800 R: 69.0 Loss: 0.8405 ExploreP: 0.0666\n",
      "Episode: 573 MeanR:129.1800 R: 92.0 Loss: 0.4446 ExploreP: 0.0661\n",
      "Episode: 574 MeanR:128.6700 R: 82.0 Loss: 71.0167 ExploreP: 0.0657\n",
      "Episode: 575 MeanR:128.1100 R: 89.0 Loss: 0.8006 ExploreP: 0.0652\n",
      "Episode: 576 MeanR:127.7300 R: 88.0 Loss: 0.5089 ExploreP: 0.0647\n",
      "Episode: 577 MeanR:127.3200 R: 113.0 Loss: 0.6192 ExploreP: 0.0641\n",
      "Episode: 578 MeanR:127.9100 R: 126.0 Loss: 0.5549 ExploreP: 0.0634\n",
      "Episode: 579 MeanR:127.6700 R: 106.0 Loss: 0.4633 ExploreP: 0.0628\n",
      "Episode: 580 MeanR:127.2400 R: 115.0 Loss: 0.1814 ExploreP: 0.0622\n",
      "Episode: 581 MeanR:126.8200 R: 116.0 Loss: 0.3331 ExploreP: 0.0616\n",
      "Episode: 582 MeanR:126.8100 R: 107.0 Loss: 0.2838 ExploreP: 0.0611\n",
      "Episode: 583 MeanR:126.4800 R: 121.0 Loss: 0.7780 ExploreP: 0.0605\n",
      "Episode: 584 MeanR:126.7500 R: 94.0 Loss: 0.3531 ExploreP: 0.0600\n",
      "Episode: 585 MeanR:127.2600 R: 128.0 Loss: 0.3416 ExploreP: 0.0593\n",
      "Episode: 586 MeanR:126.4800 R: 109.0 Loss: 0.3336 ExploreP: 0.0588\n",
      "Episode: 587 MeanR:127.0500 R: 121.0 Loss: 0.5574 ExploreP: 0.0582\n",
      "Episode: 588 MeanR:127.8400 R: 138.0 Loss: 0.1751 ExploreP: 0.0576\n",
      "Episode: 589 MeanR:128.3100 R: 102.0 Loss: 29.6473 ExploreP: 0.0571\n",
      "Episode: 590 MeanR:128.0800 R: 111.0 Loss: 0.6447 ExploreP: 0.0566\n",
      "Episode: 591 MeanR:128.3000 R: 105.0 Loss: 0.2484 ExploreP: 0.0561\n",
      "Episode: 592 MeanR:129.0200 R: 117.0 Loss: 0.4350 ExploreP: 0.0555\n",
      "Episode: 593 MeanR:129.6300 R: 96.0 Loss: 0.1637 ExploreP: 0.0551\n",
      "Episode: 594 MeanR:130.5800 R: 123.0 Loss: 0.3252 ExploreP: 0.0546\n",
      "Episode: 595 MeanR:130.7900 R: 117.0 Loss: 0.5631 ExploreP: 0.0540\n",
      "Episode: 596 MeanR:131.4000 R: 117.0 Loss: 0.5470 ExploreP: 0.0535\n",
      "Episode: 597 MeanR:132.1200 R: 123.0 Loss: 0.4594 ExploreP: 0.0530\n",
      "Episode: 598 MeanR:132.7300 R: 113.0 Loss: 0.7167 ExploreP: 0.0525\n",
      "Episode: 599 MeanR:133.5700 R: 138.0 Loss: 0.1610 ExploreP: 0.0519\n",
      "Episode: 600 MeanR:134.4100 R: 139.0 Loss: 0.3816 ExploreP: 0.0513\n",
      "Episode: 601 MeanR:135.0500 R: 132.0 Loss: 0.3106 ExploreP: 0.0508\n",
      "Episode: 602 MeanR:135.7600 R: 135.0 Loss: 14.3665 ExploreP: 0.0503\n",
      "Episode: 603 MeanR:136.2600 R: 175.0 Loss: 0.2656 ExploreP: 0.0496\n",
      "Episode: 604 MeanR:137.4000 R: 168.0 Loss: 0.3431 ExploreP: 0.0489\n",
      "Episode: 605 MeanR:138.4500 R: 162.0 Loss: 0.2739 ExploreP: 0.0483\n",
      "Episode: 606 MeanR:139.6300 R: 179.0 Loss: 0.2663 ExploreP: 0.0476\n",
      "Episode: 607 MeanR:140.3800 R: 200.0 Loss: 0.3614 ExploreP: 0.0469\n",
      "Episode: 608 MeanR:141.7200 R: 189.0 Loss: 0.3729 ExploreP: 0.0462\n",
      "Episode: 609 MeanR:142.5400 R: 147.0 Loss: 0.8200 ExploreP: 0.0456\n",
      "Episode: 610 MeanR:144.3100 R: 255.0 Loss: 0.1465 ExploreP: 0.0447\n",
      "Episode: 611 MeanR:145.1400 R: 183.0 Loss: 0.1688 ExploreP: 0.0441\n",
      "Episode: 612 MeanR:147.3500 R: 298.0 Loss: 0.3230 ExploreP: 0.0431\n",
      "Episode: 613 MeanR:149.8500 R: 347.0 Loss: 0.1331 ExploreP: 0.0420\n",
      "Episode: 614 MeanR:151.5500 R: 302.0 Loss: 0.1168 ExploreP: 0.0410\n",
      "Episode: 615 MeanR:153.5300 R: 294.0 Loss: 0.1399 ExploreP: 0.0401\n",
      "Episode: 616 MeanR:156.6600 R: 436.0 Loss: 0.2209 ExploreP: 0.0388\n",
      "Episode: 617 MeanR:159.1100 R: 365.0 Loss: 0.1150 ExploreP: 0.0378\n",
      "Episode: 618 MeanR:162.9300 R: 499.0 Loss: 0.1138 ExploreP: 0.0365\n",
      "Episode: 619 MeanR:167.2000 R: 499.0 Loss: 0.1290 ExploreP: 0.0352\n",
      "Episode: 620 MeanR:171.7300 R: 499.0 Loss: 0.3066 ExploreP: 0.0339\n",
      "Episode: 621 MeanR:172.7500 R: 194.0 Loss: 0.0659 ExploreP: 0.0335\n",
      "Episode: 622 MeanR:174.9600 R: 345.0 Loss: 0.1085 ExploreP: 0.0327\n",
      "Episode: 623 MeanR:174.7900 R: 54.0 Loss: 0.1880 ExploreP: 0.0326\n",
      "Episode: 624 MeanR:173.2100 R: 47.0 Loss: 0.3889 ExploreP: 0.0325\n",
      "Episode: 625 MeanR:172.0200 R: 44.0 Loss: 0.2480 ExploreP: 0.0324\n",
      "Episode: 626 MeanR:171.6400 R: 49.0 Loss: 0.0890 ExploreP: 0.0322\n",
      "Episode: 627 MeanR:169.6400 R: 73.0 Loss: 525.7285 ExploreP: 0.0321\n",
      "Episode: 628 MeanR:167.9700 R: 53.0 Loss: 0.1501 ExploreP: 0.0320\n",
      "Episode: 629 MeanR:171.2400 R: 499.0 Loss: 429.5057 ExploreP: 0.0309\n",
      "Episode: 630 MeanR:171.5400 R: 499.0 Loss: 0.2148 ExploreP: 0.0299\n",
      "Episode: 631 MeanR:170.3900 R: 122.0 Loss: 0.2040 ExploreP: 0.0296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 632 MeanR:173.8600 R: 499.0 Loss: 0.1508 ExploreP: 0.0287\n",
      "Episode: 633 MeanR:176.3800 R: 499.0 Loss: 16.0527 ExploreP: 0.0278\n",
      "Episode: 634 MeanR:179.1000 R: 440.0 Loss: 0.1946 ExploreP: 0.0270\n",
      "Episode: 635 MeanR:180.7600 R: 421.0 Loss: 0.1334 ExploreP: 0.0263\n",
      "Episode: 636 MeanR:179.5000 R: 48.0 Loss: 0.2612 ExploreP: 0.0262\n",
      "Episode: 637 MeanR:183.4100 R: 499.0 Loss: 0.1369 ExploreP: 0.0254\n",
      "Episode: 638 MeanR:185.8000 R: 391.0 Loss: 0.1428 ExploreP: 0.0249\n",
      "Episode: 639 MeanR:186.9600 R: 365.0 Loss: 0.2293 ExploreP: 0.0243\n",
      "Episode: 640 MeanR:188.8800 R: 477.0 Loss: 0.2404 ExploreP: 0.0237\n",
      "Episode: 641 MeanR:192.4200 R: 492.0 Loss: 0.2242 ExploreP: 0.0230\n",
      "Episode: 642 MeanR:195.3700 R: 499.0 Loss: 0.2125 ExploreP: 0.0224\n",
      "Episode: 643 MeanR:198.4200 R: 499.0 Loss: 0.1096 ExploreP: 0.0218\n",
      "Episode: 644 MeanR:201.2800 R: 499.0 Loss: 201.3148 ExploreP: 0.0212\n",
      "Episode: 645 MeanR:204.8500 R: 499.0 Loss: 0.2160 ExploreP: 0.0206\n",
      "Episode: 646 MeanR:208.4800 R: 499.0 Loss: 0.1629 ExploreP: 0.0201\n",
      "Episode: 647 MeanR:211.5100 R: 499.0 Loss: 0.0840 ExploreP: 0.0196\n",
      "Episode: 648 MeanR:214.3100 R: 499.0 Loss: 0.1333 ExploreP: 0.0192\n",
      "Episode: 649 MeanR:217.5800 R: 499.0 Loss: 0.2150 ExploreP: 0.0187\n",
      "Episode: 650 MeanR:221.1100 R: 499.0 Loss: 0.0931 ExploreP: 0.0183\n",
      "Episode: 651 MeanR:224.5100 R: 499.0 Loss: 0.1659 ExploreP: 0.0179\n",
      "Episode: 652 MeanR:227.9800 R: 499.0 Loss: 0.1810 ExploreP: 0.0175\n",
      "Episode: 653 MeanR:231.3300 R: 499.0 Loss: 0.0571 ExploreP: 0.0171\n",
      "Episode: 654 MeanR:234.6500 R: 499.0 Loss: 0.2483 ExploreP: 0.0168\n",
      "Episode: 655 MeanR:237.9100 R: 499.0 Loss: 0.0970 ExploreP: 0.0165\n",
      "Episode: 656 MeanR:241.1600 R: 499.0 Loss: 0.1616 ExploreP: 0.0161\n",
      "Episode: 657 MeanR:244.7600 R: 499.0 Loss: 0.2182 ExploreP: 0.0158\n",
      "Episode: 658 MeanR:246.4900 R: 299.0 Loss: 0.1292 ExploreP: 0.0157\n",
      "Episode: 659 MeanR:248.0700 R: 299.0 Loss: 217.1434 ExploreP: 0.0155\n",
      "Episode: 660 MeanR:249.0000 R: 258.0 Loss: 0.1686 ExploreP: 0.0154\n",
      "Episode: 661 MeanR:250.2100 R: 272.0 Loss: 0.2199 ExploreP: 0.0152\n",
      "Episode: 662 MeanR:252.7800 R: 365.0 Loss: 0.0687 ExploreP: 0.0150\n",
      "Episode: 663 MeanR:254.3100 R: 294.0 Loss: 0.1768 ExploreP: 0.0149\n",
      "Episode: 664 MeanR:257.2700 R: 418.0 Loss: 0.0999 ExploreP: 0.0147\n",
      "Episode: 665 MeanR:260.8300 R: 476.0 Loss: 0.0686 ExploreP: 0.0145\n",
      "Episode: 666 MeanR:264.5500 R: 499.0 Loss: 0.1678 ExploreP: 0.0143\n",
      "Episode: 667 MeanR:268.4000 R: 499.0 Loss: 0.2106 ExploreP: 0.0140\n",
      "Episode: 668 MeanR:272.5400 R: 499.0 Loss: 0.1498 ExploreP: 0.0139\n",
      "Episode: 669 MeanR:276.6400 R: 499.0 Loss: 0.2037 ExploreP: 0.0137\n",
      "Episode: 670 MeanR:279.3400 R: 384.0 Loss: 0.5471 ExploreP: 0.0135\n",
      "Episode: 671 MeanR:278.7900 R: 45.0 Loss: 0.2532 ExploreP: 0.0135\n",
      "Episode: 672 MeanR:278.6600 R: 56.0 Loss: 0.3541 ExploreP: 0.0135\n",
      "Episode: 673 MeanR:278.0400 R: 30.0 Loss: 0.1780 ExploreP: 0.0135\n",
      "Episode: 674 MeanR:277.9300 R: 71.0 Loss: 0.1960 ExploreP: 0.0135\n",
      "Episode: 675 MeanR:277.6100 R: 57.0 Loss: 0.2166 ExploreP: 0.0134\n",
      "Episode: 676 MeanR:281.7200 R: 499.0 Loss: 0.1417 ExploreP: 0.0133\n",
      "Episode: 677 MeanR:282.1600 R: 157.0 Loss: 0.2136 ExploreP: 0.0132\n",
      "Episode: 678 MeanR:282.8800 R: 198.0 Loss: 0.1347 ExploreP: 0.0132\n",
      "Episode: 679 MeanR:282.9100 R: 109.0 Loss: 0.1165 ExploreP: 0.0131\n",
      "Episode: 680 MeanR:283.6000 R: 184.0 Loss: 0.2306 ExploreP: 0.0131\n",
      "Episode: 681 MeanR:283.9300 R: 149.0 Loss: 0.2074 ExploreP: 0.0130\n",
      "Episode: 682 MeanR:283.0100 R: 15.0 Loss: 0.1524 ExploreP: 0.0130\n",
      "Episode: 683 MeanR:281.9900 R: 19.0 Loss: 0.2208 ExploreP: 0.0130\n",
      "Episode: 684 MeanR:281.2200 R: 17.0 Loss: 0.2795 ExploreP: 0.0130\n",
      "Episode: 685 MeanR:280.0900 R: 15.0 Loss: 0.1271 ExploreP: 0.0130\n",
      "Episode: 686 MeanR:279.1200 R: 12.0 Loss: 0.2962 ExploreP: 0.0130\n",
      "Episode: 687 MeanR:278.0300 R: 12.0 Loss: 0.4854 ExploreP: 0.0130\n",
      "Episode: 688 MeanR:276.7500 R: 10.0 Loss: 0.3036 ExploreP: 0.0130\n",
      "Episode: 689 MeanR:275.8400 R: 11.0 Loss: 0.4853 ExploreP: 0.0130\n",
      "Episode: 690 MeanR:274.8400 R: 11.0 Loss: 0.2418 ExploreP: 0.0130\n",
      "Episode: 691 MeanR:273.9200 R: 13.0 Loss: 0.2628 ExploreP: 0.0130\n",
      "Episode: 692 MeanR:272.8700 R: 12.0 Loss: 0.5717 ExploreP: 0.0130\n",
      "Episode: 693 MeanR:272.0100 R: 10.0 Loss: 0.4965 ExploreP: 0.0130\n",
      "Episode: 694 MeanR:270.8800 R: 10.0 Loss: 512.7281 ExploreP: 0.0130\n",
      "Episode: 695 MeanR:269.8100 R: 10.0 Loss: 0.3936 ExploreP: 0.0130\n",
      "Episode: 696 MeanR:268.7400 R: 10.0 Loss: 0.9042 ExploreP: 0.0130\n",
      "Episode: 697 MeanR:267.6200 R: 11.0 Loss: 310.0113 ExploreP: 0.0130\n",
      "Episode: 698 MeanR:266.6300 R: 14.0 Loss: 0.2882 ExploreP: 0.0130\n",
      "Episode: 699 MeanR:270.2400 R: 499.0 Loss: 0.5456 ExploreP: 0.0128\n",
      "Episode: 700 MeanR:270.2400 R: 139.0 Loss: 0.9114 ExploreP: 0.0128\n",
      "Episode: 701 MeanR:269.6000 R: 68.0 Loss: 1.3797 ExploreP: 0.0128\n",
      "Episode: 702 MeanR:268.7700 R: 52.0 Loss: 0.9206 ExploreP: 0.0127\n",
      "Episode: 703 MeanR:267.5500 R: 53.0 Loss: 322.8008 ExploreP: 0.0127\n",
      "Episode: 704 MeanR:266.1200 R: 25.0 Loss: 1.1638 ExploreP: 0.0127\n",
      "Episode: 705 MeanR:264.7300 R: 23.0 Loss: 1.5544 ExploreP: 0.0127\n",
      "Episode: 706 MeanR:263.1600 R: 22.0 Loss: 1.3854 ExploreP: 0.0127\n",
      "Episode: 707 MeanR:261.4700 R: 31.0 Loss: 0.9784 ExploreP: 0.0127\n",
      "Episode: 708 MeanR:259.7400 R: 16.0 Loss: 2.1960 ExploreP: 0.0127\n",
      "Episode: 709 MeanR:258.5200 R: 25.0 Loss: 1.8429 ExploreP: 0.0127\n",
      "Episode: 710 MeanR:256.1500 R: 18.0 Loss: 456.0286 ExploreP: 0.0127\n",
      "Episode: 711 MeanR:254.5700 R: 25.0 Loss: 1.3440 ExploreP: 0.0127\n",
      "Episode: 712 MeanR:251.7300 R: 14.0 Loss: 2.2186 ExploreP: 0.0127\n",
      "Episode: 713 MeanR:248.4200 R: 16.0 Loss: 2.4641 ExploreP: 0.0127\n",
      "Episode: 714 MeanR:245.5100 R: 11.0 Loss: 2.3869 ExploreP: 0.0127\n",
      "Episode: 715 MeanR:242.7000 R: 13.0 Loss: 2.5360 ExploreP: 0.0127\n",
      "Episode: 716 MeanR:238.4800 R: 14.0 Loss: 1.9934 ExploreP: 0.0127\n",
      "Episode: 717 MeanR:234.9800 R: 15.0 Loss: 3.4955 ExploreP: 0.0127\n",
      "Episode: 718 MeanR:230.1100 R: 12.0 Loss: 2.8129 ExploreP: 0.0126\n",
      "Episode: 719 MeanR:225.2300 R: 11.0 Loss: 3.3888 ExploreP: 0.0126\n",
      "Episode: 720 MeanR:220.3400 R: 10.0 Loss: 2.8233 ExploreP: 0.0126\n",
      "Episode: 721 MeanR:218.5100 R: 11.0 Loss: 2.9013 ExploreP: 0.0126\n",
      "Episode: 722 MeanR:215.1700 R: 11.0 Loss: 4.5846 ExploreP: 0.0126\n",
      "Episode: 723 MeanR:214.7100 R: 8.0 Loss: 3.3834 ExploreP: 0.0126\n",
      "Episode: 724 MeanR:214.3200 R: 8.0 Loss: 3.5642 ExploreP: 0.0126\n",
      "Episode: 725 MeanR:213.9800 R: 10.0 Loss: 2.9396 ExploreP: 0.0126\n",
      "Episode: 726 MeanR:213.6300 R: 14.0 Loss: 1.8954 ExploreP: 0.0126\n",
      "Episode: 727 MeanR:212.9900 R: 9.0 Loss: 2.4357 ExploreP: 0.0126\n",
      "Episode: 728 MeanR:212.5800 R: 12.0 Loss: 1.3535 ExploreP: 0.0126\n",
      "Episode: 729 MeanR:207.7300 R: 14.0 Loss: 1.8685 ExploreP: 0.0126\n",
      "Episode: 730 MeanR:202.8600 R: 12.0 Loss: 736.1375 ExploreP: 0.0126\n",
      "Episode: 731 MeanR:201.7500 R: 11.0 Loss: 1.2109 ExploreP: 0.0126\n",
      "Episode: 732 MeanR:196.9300 R: 17.0 Loss: 2.5402 ExploreP: 0.0126\n",
      "Episode: 733 MeanR:192.0800 R: 14.0 Loss: 1.6918 ExploreP: 0.0126\n",
      "Episode: 734 MeanR:187.8000 R: 12.0 Loss: 2.7509 ExploreP: 0.0126\n",
      "Episode: 735 MeanR:183.7000 R: 11.0 Loss: 3.4196 ExploreP: 0.0126\n",
      "Episode: 736 MeanR:183.3400 R: 12.0 Loss: 729.5777 ExploreP: 0.0126\n",
      "Episode: 737 MeanR:178.4700 R: 12.0 Loss: 455.7389 ExploreP: 0.0126\n",
      "Episode: 738 MeanR:174.7800 R: 22.0 Loss: 0.8750 ExploreP: 0.0126\n",
      "Episode: 739 MeanR:171.3700 R: 24.0 Loss: 1.7231 ExploreP: 0.0126\n",
      "Episode: 740 MeanR:166.7900 R: 19.0 Loss: 2.5299 ExploreP: 0.0126\n",
      "Episode: 741 MeanR:162.0500 R: 18.0 Loss: 2.1003 ExploreP: 0.0126\n",
      "Episode: 742 MeanR:157.1800 R: 12.0 Loss: 2.3869 ExploreP: 0.0126\n",
      "Episode: 743 MeanR:152.4100 R: 22.0 Loss: 1.0917 ExploreP: 0.0126\n",
      "Episode: 744 MeanR:147.7400 R: 32.0 Loss: 1.6587 ExploreP: 0.0126\n",
      "Episode: 745 MeanR:143.0900 R: 34.0 Loss: 2.9848 ExploreP: 0.0125\n",
      "Episode: 746 MeanR:143.0900 R: 499.0 Loss: 2.3505 ExploreP: 0.0124\n",
      "Episode: 747 MeanR:143.0900 R: 499.0 Loss: 1.3617 ExploreP: 0.0123\n",
      "Episode: 748 MeanR:143.0900 R: 499.0 Loss: 2.7341 ExploreP: 0.0122\n",
      "Episode: 749 MeanR:143.0900 R: 499.0 Loss: 3.5130 ExploreP: 0.0121\n",
      "Episode: 750 MeanR:143.0900 R: 499.0 Loss: 2.1805 ExploreP: 0.0120\n",
      "Episode: 751 MeanR:143.0900 R: 499.0 Loss: 0.8883 ExploreP: 0.0119\n",
      "Episode: 752 MeanR:143.0900 R: 499.0 Loss: 2.5923 ExploreP: 0.0118\n",
      "Episode: 753 MeanR:143.0900 R: 499.0 Loss: 417.5966 ExploreP: 0.0117\n",
      "Episode: 754 MeanR:143.0900 R: 499.0 Loss: 3.7852 ExploreP: 0.0116\n",
      "Episode: 755 MeanR:143.0900 R: 499.0 Loss: 3.7819 ExploreP: 0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 756 MeanR:143.0900 R: 499.0 Loss: 4.9975 ExploreP: 0.0115\n",
      "Episode: 757 MeanR:143.0900 R: 499.0 Loss: 1.4637 ExploreP: 0.0114\n",
      "Episode: 758 MeanR:145.0900 R: 499.0 Loss: 3.0991 ExploreP: 0.0113\n",
      "Episode: 759 MeanR:147.0900 R: 499.0 Loss: 5.9471 ExploreP: 0.0113\n",
      "Episode: 760 MeanR:149.5000 R: 499.0 Loss: 0.2735 ExploreP: 0.0112\n",
      "Episode: 761 MeanR:151.7700 R: 499.0 Loss: 17.1714 ExploreP: 0.0111\n",
      "Episode: 762 MeanR:153.1100 R: 499.0 Loss: 60.3738 ExploreP: 0.0111\n",
      "Episode: 763 MeanR:155.1600 R: 499.0 Loss: 1.0307 ExploreP: 0.0110\n",
      "Episode: 764 MeanR:155.9700 R: 499.0 Loss: 1.3360 ExploreP: 0.0110\n",
      "Episode: 765 MeanR:156.2000 R: 499.0 Loss: 0.3392 ExploreP: 0.0109\n",
      "Episode: 766 MeanR:156.2000 R: 499.0 Loss: 0.2394 ExploreP: 0.0109\n",
      "Episode: 767 MeanR:156.2000 R: 499.0 Loss: 0.4845 ExploreP: 0.0108\n",
      "Episode: 768 MeanR:156.2000 R: 499.0 Loss: 0.6176 ExploreP: 0.0108\n",
      "Episode: 769 MeanR:156.2000 R: 499.0 Loss: 0.1947 ExploreP: 0.0108\n",
      "Episode: 770 MeanR:157.3500 R: 499.0 Loss: 0.3356 ExploreP: 0.0107\n",
      "Episode: 771 MeanR:161.8900 R: 499.0 Loss: 0.0740 ExploreP: 0.0107\n",
      "Episode: 772 MeanR:166.3200 R: 499.0 Loss: 0.1139 ExploreP: 0.0107\n",
      "Episode: 773 MeanR:171.0100 R: 499.0 Loss: 0.1586 ExploreP: 0.0106\n",
      "Episode: 774 MeanR:175.2900 R: 499.0 Loss: 0.1815 ExploreP: 0.0106\n",
      "Episode: 775 MeanR:179.7100 R: 499.0 Loss: 0.3251 ExploreP: 0.0106\n",
      "Episode: 776 MeanR:179.7100 R: 499.0 Loss: 0.3713 ExploreP: 0.0105\n",
      "Episode: 777 MeanR:183.1300 R: 499.0 Loss: 0.1284 ExploreP: 0.0105\n",
      "Episode: 778 MeanR:186.1400 R: 499.0 Loss: 0.1951 ExploreP: 0.0105\n",
      "Episode: 779 MeanR:190.0400 R: 499.0 Loss: 0.0926 ExploreP: 0.0105\n",
      "Episode: 780 MeanR:193.1900 R: 499.0 Loss: 0.2391 ExploreP: 0.0104\n",
      "Episode: 781 MeanR:196.3300 R: 463.0 Loss: 270.3869 ExploreP: 0.0104\n",
      "Episode: 782 MeanR:200.4400 R: 426.0 Loss: 0.1754 ExploreP: 0.0104\n",
      "Episode: 783 MeanR:204.3000 R: 405.0 Loss: 0.3604 ExploreP: 0.0104\n",
      "Episode: 784 MeanR:208.5400 R: 441.0 Loss: 0.3868 ExploreP: 0.0104\n",
      "Episode: 785 MeanR:212.5400 R: 415.0 Loss: 0.2219 ExploreP: 0.0104\n",
      "Episode: 786 MeanR:215.8500 R: 343.0 Loss: 0.1587 ExploreP: 0.0103\n",
      "Episode: 787 MeanR:219.6000 R: 387.0 Loss: 0.4963 ExploreP: 0.0103\n",
      "Episode: 788 MeanR:222.9900 R: 349.0 Loss: 0.1273 ExploreP: 0.0103\n",
      "Episode: 789 MeanR:227.7300 R: 485.0 Loss: 0.4886 ExploreP: 0.0103\n",
      "Episode: 790 MeanR:231.0300 R: 341.0 Loss: 0.2745 ExploreP: 0.0103\n",
      "Episode: 791 MeanR:234.7500 R: 385.0 Loss: 0.3507 ExploreP: 0.0103\n",
      "Episode: 792 MeanR:237.9500 R: 332.0 Loss: 0.0920 ExploreP: 0.0103\n",
      "Episode: 793 MeanR:241.6300 R: 378.0 Loss: 0.2649 ExploreP: 0.0103\n",
      "Episode: 794 MeanR:245.6800 R: 415.0 Loss: 0.0958 ExploreP: 0.0103\n",
      "Episode: 795 MeanR:249.8100 R: 423.0 Loss: 0.2340 ExploreP: 0.0102\n",
      "Episode: 796 MeanR:254.7000 R: 499.0 Loss: 0.2003 ExploreP: 0.0102\n",
      "Episode: 797 MeanR:259.5800 R: 499.0 Loss: 0.2441 ExploreP: 0.0102\n",
      "Episode: 798 MeanR:264.4300 R: 499.0 Loss: 0.1234 ExploreP: 0.0102\n",
      "Episode: 799 MeanR:264.4300 R: 499.0 Loss: 0.2629 ExploreP: 0.0102\n",
      "Episode: 800 MeanR:268.0300 R: 499.0 Loss: 0.1532 ExploreP: 0.0102\n",
      "Episode: 801 MeanR:272.2800 R: 493.0 Loss: 0.1297 ExploreP: 0.0102\n",
      "Episode: 802 MeanR:276.7500 R: 499.0 Loss: 0.2970 ExploreP: 0.0102\n",
      "Episode: 803 MeanR:281.2100 R: 499.0 Loss: 0.1960 ExploreP: 0.0102\n",
      "Episode: 804 MeanR:285.9500 R: 499.0 Loss: 0.0956 ExploreP: 0.0102\n",
      "Episode: 805 MeanR:290.2900 R: 457.0 Loss: 0.2027 ExploreP: 0.0101\n",
      "Episode: 806 MeanR:295.0600 R: 499.0 Loss: 0.1818 ExploreP: 0.0101\n",
      "Episode: 807 MeanR:299.7400 R: 499.0 Loss: 0.1993 ExploreP: 0.0101\n",
      "Episode: 808 MeanR:304.5700 R: 499.0 Loss: 0.4566 ExploreP: 0.0101\n",
      "Episode: 809 MeanR:309.3100 R: 499.0 Loss: 0.2501 ExploreP: 0.0101\n",
      "Episode: 810 MeanR:314.1200 R: 499.0 Loss: 0.2235 ExploreP: 0.0101\n",
      "Episode: 811 MeanR:318.8600 R: 499.0 Loss: 0.1503 ExploreP: 0.0101\n",
      "Episode: 812 MeanR:323.7100 R: 499.0 Loss: 0.1427 ExploreP: 0.0101\n",
      "Episode: 813 MeanR:328.5400 R: 499.0 Loss: 0.1537 ExploreP: 0.0101\n",
      "Episode: 814 MeanR:333.4200 R: 499.0 Loss: 0.1267 ExploreP: 0.0101\n",
      "Episode: 815 MeanR:338.2800 R: 499.0 Loss: 0.0684 ExploreP: 0.0101\n",
      "Episode: 816 MeanR:343.1300 R: 499.0 Loss: 0.0770 ExploreP: 0.0101\n",
      "Episode: 817 MeanR:347.9700 R: 499.0 Loss: 0.1007 ExploreP: 0.0101\n",
      "Episode: 818 MeanR:352.8400 R: 499.0 Loss: 0.0563 ExploreP: 0.0101\n",
      "Episode: 819 MeanR:357.7200 R: 499.0 Loss: 0.0676 ExploreP: 0.0101\n",
      "Episode: 820 MeanR:362.6100 R: 499.0 Loss: 0.0654 ExploreP: 0.0101\n",
      "Episode: 821 MeanR:367.4900 R: 499.0 Loss: 0.0573 ExploreP: 0.0101\n",
      "Episode: 822 MeanR:372.3700 R: 499.0 Loss: 0.0714 ExploreP: 0.0101\n",
      "Episode: 823 MeanR:377.2800 R: 499.0 Loss: 0.0957 ExploreP: 0.0101\n",
      "Episode: 824 MeanR:382.1900 R: 499.0 Loss: 0.1614 ExploreP: 0.0101\n",
      "Episode: 825 MeanR:387.0800 R: 499.0 Loss: 0.0568 ExploreP: 0.0101\n",
      "Episode: 826 MeanR:391.9300 R: 499.0 Loss: 0.1041 ExploreP: 0.0101\n",
      "Episode: 827 MeanR:396.8300 R: 499.0 Loss: 0.1129 ExploreP: 0.0100\n",
      "Episode: 828 MeanR:401.7000 R: 499.0 Loss: 0.0634 ExploreP: 0.0100\n",
      "Episode: 829 MeanR:406.5500 R: 499.0 Loss: 0.1450 ExploreP: 0.0100\n",
      "Episode: 830 MeanR:411.4200 R: 499.0 Loss: 0.1746 ExploreP: 0.0100\n",
      "Episode: 831 MeanR:416.3000 R: 499.0 Loss: 0.0977 ExploreP: 0.0100\n",
      "Episode: 832 MeanR:421.1200 R: 499.0 Loss: 0.0479 ExploreP: 0.0100\n",
      "Episode: 833 MeanR:425.9700 R: 499.0 Loss: 0.2338 ExploreP: 0.0100\n",
      "Episode: 834 MeanR:430.8400 R: 499.0 Loss: 0.1856 ExploreP: 0.0100\n",
      "Episode: 835 MeanR:435.7200 R: 499.0 Loss: 0.0887 ExploreP: 0.0100\n",
      "Episode: 836 MeanR:440.5900 R: 499.0 Loss: 0.1409 ExploreP: 0.0100\n",
      "Episode: 837 MeanR:445.4600 R: 499.0 Loss: 0.1660 ExploreP: 0.0100\n",
      "Episode: 838 MeanR:450.2300 R: 499.0 Loss: 0.0569 ExploreP: 0.0100\n",
      "Episode: 839 MeanR:454.9800 R: 499.0 Loss: 0.1088 ExploreP: 0.0100\n",
      "Episode: 840 MeanR:459.7800 R: 499.0 Loss: 0.1756 ExploreP: 0.0100\n",
      "Episode: 841 MeanR:464.5900 R: 499.0 Loss: 0.0853 ExploreP: 0.0100\n",
      "Episode: 842 MeanR:469.4600 R: 499.0 Loss: 0.0411 ExploreP: 0.0100\n",
      "Episode: 843 MeanR:474.2300 R: 499.0 Loss: 0.2198 ExploreP: 0.0100\n",
      "Episode: 844 MeanR:478.9000 R: 499.0 Loss: 0.0706 ExploreP: 0.0100\n",
      "Episode: 845 MeanR:483.5500 R: 499.0 Loss: 0.1634 ExploreP: 0.0100\n",
      "Episode: 846 MeanR:483.5500 R: 499.0 Loss: 0.0618 ExploreP: 0.0100\n",
      "Episode: 847 MeanR:483.3700 R: 481.0 Loss: 0.1673 ExploreP: 0.0100\n",
      "Episode: 848 MeanR:483.3700 R: 499.0 Loss: 0.2948 ExploreP: 0.0100\n",
      "Episode: 849 MeanR:483.3700 R: 499.0 Loss: 0.1290 ExploreP: 0.0100\n",
      "Episode: 850 MeanR:483.3700 R: 499.0 Loss: 0.1115 ExploreP: 0.0100\n",
      "Episode: 851 MeanR:483.3700 R: 499.0 Loss: 0.1700 ExploreP: 0.0100\n",
      "Episode: 852 MeanR:483.3700 R: 499.0 Loss: 0.0518 ExploreP: 0.0100\n",
      "Episode: 853 MeanR:483.3700 R: 499.0 Loss: 0.1074 ExploreP: 0.0100\n",
      "Episode: 854 MeanR:483.3700 R: 499.0 Loss: 0.0535 ExploreP: 0.0100\n",
      "Episode: 855 MeanR:483.3700 R: 499.0 Loss: 0.1487 ExploreP: 0.0100\n",
      "Episode: 856 MeanR:483.3700 R: 499.0 Loss: 0.1061 ExploreP: 0.0100\n",
      "Episode: 857 MeanR:483.3700 R: 499.0 Loss: 0.0761 ExploreP: 0.0100\n",
      "Episode: 858 MeanR:483.3700 R: 499.0 Loss: 0.0949 ExploreP: 0.0100\n",
      "Episode: 859 MeanR:483.2700 R: 489.0 Loss: 0.0702 ExploreP: 0.0100\n",
      "Episode: 860 MeanR:483.2700 R: 499.0 Loss: 0.0544 ExploreP: 0.0100\n",
      "Episode: 861 MeanR:483.2700 R: 499.0 Loss: 0.0917 ExploreP: 0.0100\n",
      "Episode: 862 MeanR:483.2700 R: 499.0 Loss: 0.0875 ExploreP: 0.0100\n",
      "Episode: 863 MeanR:483.2700 R: 499.0 Loss: 0.1241 ExploreP: 0.0100\n",
      "Episode: 864 MeanR:483.2700 R: 499.0 Loss: 0.2842 ExploreP: 0.0100\n",
      "Episode: 865 MeanR:483.2700 R: 499.0 Loss: 0.1144 ExploreP: 0.0100\n",
      "Episode: 866 MeanR:483.2700 R: 499.0 Loss: 0.1017 ExploreP: 0.0100\n",
      "Episode: 867 MeanR:483.2700 R: 499.0 Loss: 0.1035 ExploreP: 0.0100\n",
      "Episode: 868 MeanR:483.2700 R: 499.0 Loss: 0.2620 ExploreP: 0.0100\n",
      "Episode: 869 MeanR:483.2700 R: 499.0 Loss: 0.1748 ExploreP: 0.0100\n",
      "Episode: 870 MeanR:483.2700 R: 499.0 Loss: 0.1320 ExploreP: 0.0100\n",
      "Episode: 871 MeanR:483.2700 R: 499.0 Loss: 0.1547 ExploreP: 0.0100\n",
      "Episode: 872 MeanR:483.2700 R: 499.0 Loss: 0.1386 ExploreP: 0.0100\n",
      "Episode: 873 MeanR:483.2700 R: 499.0 Loss: 0.1139 ExploreP: 0.0100\n",
      "Episode: 874 MeanR:483.2700 R: 499.0 Loss: 0.1252 ExploreP: 0.0100\n",
      "Episode: 875 MeanR:483.2700 R: 499.0 Loss: 0.1136 ExploreP: 0.0100\n",
      "Episode: 876 MeanR:483.2700 R: 499.0 Loss: 0.2260 ExploreP: 0.0100\n",
      "Episode: 877 MeanR:483.2700 R: 499.0 Loss: 0.0411 ExploreP: 0.0100\n",
      "Episode: 878 MeanR:483.2700 R: 499.0 Loss: 0.0602 ExploreP: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 879 MeanR:483.2700 R: 499.0 Loss: 0.0876 ExploreP: 0.0100\n",
      "Episode: 880 MeanR:483.2700 R: 499.0 Loss: 0.0630 ExploreP: 0.0100\n",
      "Episode: 881 MeanR:483.6300 R: 499.0 Loss: 181.9871 ExploreP: 0.0100\n",
      "Episode: 882 MeanR:484.3600 R: 499.0 Loss: 0.1228 ExploreP: 0.0100\n",
      "Episode: 883 MeanR:485.3000 R: 499.0 Loss: 0.1448 ExploreP: 0.0100\n",
      "Episode: 884 MeanR:485.8800 R: 499.0 Loss: 0.0707 ExploreP: 0.0100\n",
      "Episode: 885 MeanR:486.7200 R: 499.0 Loss: 0.0574 ExploreP: 0.0100\n",
      "Episode: 886 MeanR:488.2800 R: 499.0 Loss: 0.0490 ExploreP: 0.0100\n",
      "Episode: 887 MeanR:489.4000 R: 499.0 Loss: 0.0422 ExploreP: 0.0100\n",
      "Episode: 888 MeanR:490.9000 R: 499.0 Loss: 0.1167 ExploreP: 0.0100\n",
      "Episode: 889 MeanR:491.0400 R: 499.0 Loss: 0.1066 ExploreP: 0.0100\n",
      "Episode: 890 MeanR:492.6200 R: 499.0 Loss: 0.0861 ExploreP: 0.0100\n",
      "Episode: 891 MeanR:493.7600 R: 499.0 Loss: 193.7801 ExploreP: 0.0100\n",
      "Episode: 892 MeanR:495.4300 R: 499.0 Loss: 0.0728 ExploreP: 0.0100\n",
      "Episode: 893 MeanR:496.6400 R: 499.0 Loss: 0.0753 ExploreP: 0.0100\n",
      "Episode: 894 MeanR:497.4800 R: 499.0 Loss: 0.0617 ExploreP: 0.0100\n",
      "Episode: 895 MeanR:498.2400 R: 499.0 Loss: 0.0701 ExploreP: 0.0100\n",
      "Episode: 896 MeanR:498.2400 R: 499.0 Loss: 226.4591 ExploreP: 0.0100\n",
      "Episode: 897 MeanR:498.2400 R: 499.0 Loss: 0.0789 ExploreP: 0.0100\n",
      "Episode: 898 MeanR:498.2400 R: 499.0 Loss: 0.0426 ExploreP: 0.0100\n",
      "Episode: 899 MeanR:498.2400 R: 499.0 Loss: 0.1358 ExploreP: 0.0100\n",
      "Episode: 900 MeanR:498.2400 R: 499.0 Loss: 0.0484 ExploreP: 0.0100\n",
      "Episode: 901 MeanR:498.3000 R: 499.0 Loss: 0.1558 ExploreP: 0.0100\n",
      "Episode: 902 MeanR:498.3000 R: 499.0 Loss: 0.0678 ExploreP: 0.0100\n",
      "Episode: 903 MeanR:498.3000 R: 499.0 Loss: 0.1012 ExploreP: 0.0100\n",
      "Episode: 904 MeanR:498.3000 R: 499.0 Loss: 0.0656 ExploreP: 0.0100\n",
      "Episode: 905 MeanR:498.7200 R: 499.0 Loss: 0.1359 ExploreP: 0.0100\n",
      "Episode: 906 MeanR:498.7200 R: 499.0 Loss: 0.1219 ExploreP: 0.0100\n",
      "Episode: 907 MeanR:498.7200 R: 499.0 Loss: 0.1485 ExploreP: 0.0100\n",
      "Episode: 908 MeanR:498.7200 R: 499.0 Loss: 0.2004 ExploreP: 0.0100\n",
      "Episode: 909 MeanR:498.7200 R: 499.0 Loss: 0.0827 ExploreP: 0.0100\n",
      "Episode: 910 MeanR:498.7200 R: 499.0 Loss: 0.0460 ExploreP: 0.0100\n",
      "Episode: 911 MeanR:498.7200 R: 499.0 Loss: 0.0408 ExploreP: 0.0100\n",
      "Episode: 912 MeanR:498.7200 R: 499.0 Loss: 0.1021 ExploreP: 0.0100\n",
      "Episode: 913 MeanR:498.7200 R: 499.0 Loss: 0.0588 ExploreP: 0.0100\n",
      "Episode: 914 MeanR:498.7200 R: 499.0 Loss: 0.0889 ExploreP: 0.0100\n",
      "Episode: 915 MeanR:498.7200 R: 499.0 Loss: 226.9071 ExploreP: 0.0100\n",
      "Episode: 916 MeanR:498.7200 R: 499.0 Loss: 0.0903 ExploreP: 0.0100\n",
      "Episode: 917 MeanR:498.7200 R: 499.0 Loss: 0.0450 ExploreP: 0.0100\n",
      "Episode: 918 MeanR:498.7200 R: 499.0 Loss: 0.0987 ExploreP: 0.0100\n",
      "Episode: 919 MeanR:498.7200 R: 499.0 Loss: 0.0852 ExploreP: 0.0100\n",
      "Episode: 920 MeanR:498.7200 R: 499.0 Loss: 0.0201 ExploreP: 0.0100\n",
      "Episode: 921 MeanR:498.7200 R: 499.0 Loss: 0.0851 ExploreP: 0.0100\n",
      "Episode: 922 MeanR:498.7200 R: 499.0 Loss: 0.0798 ExploreP: 0.0100\n",
      "Episode: 923 MeanR:498.7200 R: 499.0 Loss: 0.1231 ExploreP: 0.0100\n",
      "Episode: 924 MeanR:498.7200 R: 499.0 Loss: 0.1008 ExploreP: 0.0100\n",
      "Episode: 925 MeanR:498.7200 R: 499.0 Loss: 0.0572 ExploreP: 0.0100\n",
      "Episode: 926 MeanR:498.7200 R: 499.0 Loss: 0.0999 ExploreP: 0.0100\n",
      "Episode: 927 MeanR:498.7200 R: 499.0 Loss: 0.0572 ExploreP: 0.0100\n",
      "Episode: 928 MeanR:498.7200 R: 499.0 Loss: 0.1049 ExploreP: 0.0100\n",
      "Episode: 929 MeanR:498.7200 R: 499.0 Loss: 0.0620 ExploreP: 0.0100\n",
      "Episode: 930 MeanR:498.7200 R: 499.0 Loss: 0.0633 ExploreP: 0.0100\n",
      "Episode: 931 MeanR:498.7200 R: 499.0 Loss: 0.0478 ExploreP: 0.0100\n",
      "Episode: 932 MeanR:498.7200 R: 499.0 Loss: 0.0506 ExploreP: 0.0100\n",
      "Episode: 933 MeanR:498.7200 R: 499.0 Loss: 0.1730 ExploreP: 0.0100\n",
      "Episode: 934 MeanR:498.7200 R: 499.0 Loss: 0.0502 ExploreP: 0.0100\n",
      "Episode: 935 MeanR:498.7200 R: 499.0 Loss: 0.0819 ExploreP: 0.0100\n",
      "Episode: 936 MeanR:498.7200 R: 499.0 Loss: 0.1371 ExploreP: 0.0100\n",
      "Episode: 937 MeanR:498.7200 R: 499.0 Loss: 0.0510 ExploreP: 0.0100\n",
      "Episode: 938 MeanR:498.7200 R: 499.0 Loss: 0.1658 ExploreP: 0.0100\n",
      "Episode: 939 MeanR:493.8300 R: 10.0 Loss: 0.1005 ExploreP: 0.0100\n",
      "Episode: 940 MeanR:488.9400 R: 10.0 Loss: 0.1183 ExploreP: 0.0100\n",
      "Episode: 941 MeanR:488.9400 R: 499.0 Loss: 0.2284 ExploreP: 0.0100\n",
      "Episode: 942 MeanR:488.9400 R: 499.0 Loss: 0.0462 ExploreP: 0.0100\n",
      "Episode: 943 MeanR:488.9400 R: 499.0 Loss: 0.1597 ExploreP: 0.0100\n",
      "Episode: 944 MeanR:488.9400 R: 499.0 Loss: 0.1134 ExploreP: 0.0100\n",
      "Episode: 945 MeanR:488.9400 R: 499.0 Loss: 0.3171 ExploreP: 0.0100\n",
      "Episode: 946 MeanR:488.9400 R: 499.0 Loss: 0.2123 ExploreP: 0.0100\n",
      "Episode: 947 MeanR:489.1200 R: 499.0 Loss: 0.3227 ExploreP: 0.0100\n",
      "Episode: 948 MeanR:489.1200 R: 499.0 Loss: 0.7007 ExploreP: 0.0100\n",
      "Episode: 949 MeanR:489.1200 R: 499.0 Loss: 0.3018 ExploreP: 0.0100\n",
      "Episode: 950 MeanR:489.1200 R: 499.0 Loss: 0.5826 ExploreP: 0.0100\n",
      "Episode: 951 MeanR:489.1200 R: 499.0 Loss: 0.4548 ExploreP: 0.0100\n",
      "Episode: 952 MeanR:489.1200 R: 499.0 Loss: 1.0908 ExploreP: 0.0100\n",
      "Episode: 953 MeanR:489.1200 R: 499.0 Loss: 2.9113 ExploreP: 0.0100\n",
      "Episode: 954 MeanR:488.3800 R: 425.0 Loss: 4.9861 ExploreP: 0.0100\n",
      "Episode: 955 MeanR:483.4900 R: 10.0 Loss: 5.1534 ExploreP: 0.0100\n",
      "Episode: 956 MeanR:481.7100 R: 321.0 Loss: 4.5956 ExploreP: 0.0100\n",
      "Episode: 957 MeanR:476.8200 R: 10.0 Loss: 3.7037 ExploreP: 0.0100\n",
      "Episode: 958 MeanR:474.8300 R: 300.0 Loss: 4.7271 ExploreP: 0.0100\n",
      "Episode: 959 MeanR:470.0300 R: 9.0 Loss: 5.4238 ExploreP: 0.0100\n",
      "Episode: 960 MeanR:465.1500 R: 11.0 Loss: 5.3140 ExploreP: 0.0100\n",
      "Episode: 961 MeanR:460.2800 R: 12.0 Loss: 3.3993 ExploreP: 0.0100\n",
      "Episode: 962 MeanR:457.7800 R: 249.0 Loss: 11.9830 ExploreP: 0.0100\n",
      "Episode: 963 MeanR:452.8800 R: 9.0 Loss: 3.7052 ExploreP: 0.0100\n",
      "Episode: 964 MeanR:447.9800 R: 9.0 Loss: 3.3694 ExploreP: 0.0100\n",
      "Episode: 965 MeanR:443.0700 R: 8.0 Loss: 12.4317 ExploreP: 0.0100\n",
      "Episode: 966 MeanR:438.2000 R: 12.0 Loss: 9.4352 ExploreP: 0.0100\n",
      "Episode: 967 MeanR:433.3000 R: 9.0 Loss: 3.8393 ExploreP: 0.0100\n",
      "Episode: 968 MeanR:428.4300 R: 12.0 Loss: 9.0476 ExploreP: 0.0100\n",
      "Episode: 969 MeanR:423.5200 R: 8.0 Loss: 13.3267 ExploreP: 0.0100\n",
      "Episode: 970 MeanR:418.6200 R: 9.0 Loss: 7.4891 ExploreP: 0.0100\n",
      "Episode: 971 MeanR:413.7200 R: 9.0 Loss: 9515.6221 ExploreP: 0.0100\n",
      "Episode: 972 MeanR:408.8500 R: 12.0 Loss: 4.9603 ExploreP: 0.0100\n",
      "Episode: 973 MeanR:403.9600 R: 10.0 Loss: 4.8777 ExploreP: 0.0100\n",
      "Episode: 974 MeanR:403.9600 R: 499.0 Loss: 3.1298 ExploreP: 0.0100\n",
      "Episode: 975 MeanR:401.8400 R: 287.0 Loss: 1752.3904 ExploreP: 0.0100\n",
      "Episode: 976 MeanR:398.5300 R: 168.0 Loss: 5.3407 ExploreP: 0.0100\n",
      "Episode: 977 MeanR:393.6700 R: 13.0 Loss: 2.7837 ExploreP: 0.0100\n",
      "Episode: 978 MeanR:388.7700 R: 9.0 Loss: 3.9165 ExploreP: 0.0100\n",
      "Episode: 979 MeanR:383.8800 R: 10.0 Loss: 5.9052 ExploreP: 0.0100\n",
      "Episode: 980 MeanR:378.9700 R: 8.0 Loss: 5.6650 ExploreP: 0.0100\n",
      "Episode: 981 MeanR:374.0900 R: 11.0 Loss: 4.5029 ExploreP: 0.0100\n",
      "Episode: 982 MeanR:369.1800 R: 8.0 Loss: 5.0995 ExploreP: 0.0100\n",
      "Episode: 983 MeanR:364.2900 R: 10.0 Loss: 3.4357 ExploreP: 0.0100\n",
      "Episode: 984 MeanR:359.3900 R: 9.0 Loss: 5.5930 ExploreP: 0.0100\n",
      "Episode: 985 MeanR:354.5200 R: 12.0 Loss: 2.5882 ExploreP: 0.0100\n",
      "Episode: 986 MeanR:349.6400 R: 11.0 Loss: 1545.9460 ExploreP: 0.0100\n",
      "Episode: 987 MeanR:344.7300 R: 8.0 Loss: 7.4107 ExploreP: 0.0100\n",
      "Episode: 988 MeanR:339.8400 R: 10.0 Loss: 6.6431 ExploreP: 0.0100\n",
      "Episode: 989 MeanR:334.9400 R: 9.0 Loss: 8.4471 ExploreP: 0.0100\n",
      "Episode: 990 MeanR:330.0600 R: 11.0 Loss: 4.0958 ExploreP: 0.0100\n",
      "Episode: 991 MeanR:325.1600 R: 9.0 Loss: 3.3635 ExploreP: 0.0100\n",
      "Episode: 992 MeanR:320.2500 R: 8.0 Loss: 4.1416 ExploreP: 0.0100\n",
      "Episode: 993 MeanR:315.3400 R: 8.0 Loss: 10.9052 ExploreP: 0.0100\n",
      "Episode: 994 MeanR:310.4500 R: 10.0 Loss: 2.5660 ExploreP: 0.0100\n",
      "Episode: 995 MeanR:305.5500 R: 9.0 Loss: 2.4362 ExploreP: 0.0100\n",
      "Episode: 996 MeanR:300.6600 R: 10.0 Loss: 6.3383 ExploreP: 0.0100\n",
      "Episode: 997 MeanR:295.7700 R: 10.0 Loss: 4.2710 ExploreP: 0.0100\n",
      "Episode: 998 MeanR:290.8800 R: 10.0 Loss: 5.5752 ExploreP: 0.0100\n",
      "Episode: 999 MeanR:285.9700 R: 8.0 Loss: 3222.0210 ExploreP: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    episode_reward = deque(maxlen=100) # running average    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        # After each episode\n",
    "        rewards_list.append((ep, total_reward))\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'MeanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'Loss:{:.4f}'.format(loss),\n",
    "              'ExploreP:{:.4f}'.format(explore_p))\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # save the model at the end of all training episodes\n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cartpole.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for ep in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "            # Get action from Q-network\n",
    "            feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
