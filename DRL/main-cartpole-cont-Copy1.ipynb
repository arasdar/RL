{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.11.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.598382748165233 -2.4623703992637225\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # targetQs/nextQs\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossQ = tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), # ==  self.rates\n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 R:17.0000 rate:0.0340 gloss:0.5156 dlossA:0.5944 dlossQ:1.0084 exploreP:0.9983\n",
      "Episode:1 meanR:16.5000 R:16.0000 rate:0.0320 gloss:0.5319 dlossA:0.5954 dlossQ:1.0005 exploreP:0.9967\n",
      "Episode:2 meanR:14.6667 R:11.0000 rate:0.0220 gloss:0.4916 dlossA:0.5883 dlossQ:1.0120 exploreP:0.9957\n",
      "Episode:3 meanR:17.0000 R:24.0000 rate:0.0480 gloss:0.5030 dlossA:0.5884 dlossQ:1.0604 exploreP:0.9933\n",
      "Episode:4 meanR:20.8000 R:36.0000 rate:0.0720 gloss:0.5255 dlossA:0.5933 dlossQ:1.0234 exploreP:0.9898\n",
      "Episode:5 meanR:21.5000 R:25.0000 rate:0.0500 gloss:0.5298 dlossA:0.5870 dlossQ:1.0553 exploreP:0.9873\n",
      "Episode:6 meanR:21.7143 R:23.0000 rate:0.0460 gloss:0.5142 dlossA:0.5804 dlossQ:1.0952 exploreP:0.9851\n",
      "Episode:7 meanR:21.3750 R:19.0000 rate:0.0380 gloss:0.5133 dlossA:0.5803 dlossQ:1.3128 exploreP:0.9832\n",
      "Episode:8 meanR:23.2222 R:38.0000 rate:0.0760 gloss:0.5373 dlossA:0.5984 dlossQ:1.0660 exploreP:0.9795\n",
      "Episode:9 meanR:22.6000 R:17.0000 rate:0.0340 gloss:0.5541 dlossA:0.6006 dlossQ:1.0906 exploreP:0.9779\n",
      "Episode:10 meanR:21.9091 R:15.0000 rate:0.0300 gloss:0.5091 dlossA:0.5803 dlossQ:1.0639 exploreP:0.9764\n",
      "Episode:11 meanR:22.0000 R:23.0000 rate:0.0460 gloss:0.5510 dlossA:0.5953 dlossQ:1.1237 exploreP:0.9742\n",
      "Episode:12 meanR:21.9231 R:21.0000 rate:0.0420 gloss:0.5430 dlossA:0.5898 dlossQ:1.1112 exploreP:0.9722\n",
      "Episode:13 meanR:21.9286 R:22.0000 rate:0.0440 gloss:0.4806 dlossA:0.5669 dlossQ:1.2731 exploreP:0.9701\n",
      "Episode:14 meanR:22.3333 R:28.0000 rate:0.0560 gloss:0.5377 dlossA:0.5946 dlossQ:1.0982 exploreP:0.9674\n",
      "Episode:15 meanR:21.9375 R:16.0000 rate:0.0320 gloss:0.5045 dlossA:0.5854 dlossQ:1.1243 exploreP:0.9659\n",
      "Episode:16 meanR:21.7647 R:19.0000 rate:0.0380 gloss:0.5104 dlossA:0.5786 dlossQ:1.1191 exploreP:0.9640\n",
      "Episode:17 meanR:23.0556 R:45.0000 rate:0.0900 gloss:0.5351 dlossA:0.5965 dlossQ:1.1250 exploreP:0.9598\n",
      "Episode:18 meanR:22.4737 R:12.0000 rate:0.0240 gloss:0.4713 dlossA:0.5663 dlossQ:1.2737 exploreP:0.9586\n",
      "Episode:19 meanR:21.8000 R:9.0000 rate:0.0180 gloss:0.5277 dlossA:0.5956 dlossQ:1.1409 exploreP:0.9578\n",
      "Episode:20 meanR:21.6667 R:19.0000 rate:0.0380 gloss:0.5274 dlossA:0.5852 dlossQ:1.1365 exploreP:0.9560\n",
      "Episode:21 meanR:22.0455 R:30.0000 rate:0.0600 gloss:0.5688 dlossA:0.6097 dlossQ:1.1305 exploreP:0.9531\n",
      "Episode:22 meanR:22.0435 R:22.0000 rate:0.0440 gloss:0.5563 dlossA:0.5960 dlossQ:1.1957 exploreP:0.9511\n",
      "Episode:23 meanR:21.7917 R:16.0000 rate:0.0320 gloss:0.5650 dlossA:0.6116 dlossQ:1.1074 exploreP:0.9496\n",
      "Episode:24 meanR:21.5600 R:16.0000 rate:0.0320 gloss:0.5380 dlossA:0.5979 dlossQ:1.1537 exploreP:0.9481\n",
      "Episode:25 meanR:21.5769 R:22.0000 rate:0.0440 gloss:0.5348 dlossA:0.5930 dlossQ:1.1278 exploreP:0.9460\n",
      "Episode:26 meanR:21.1852 R:11.0000 rate:0.0220 gloss:0.5571 dlossA:0.6003 dlossQ:1.1372 exploreP:0.9450\n",
      "Episode:27 meanR:21.1786 R:21.0000 rate:0.0420 gloss:0.5426 dlossA:0.5981 dlossQ:1.1345 exploreP:0.9430\n",
      "Episode:28 meanR:21.6207 R:34.0000 rate:0.0680 gloss:0.5203 dlossA:0.5822 dlossQ:1.1345 exploreP:0.9398\n",
      "Episode:29 meanR:21.4333 R:16.0000 rate:0.0320 gloss:0.5501 dlossA:0.5946 dlossQ:1.1315 exploreP:0.9383\n",
      "Episode:30 meanR:21.5161 R:24.0000 rate:0.0480 gloss:0.5744 dlossA:0.5748 dlossQ:1.3279 exploreP:0.9361\n",
      "Episode:31 meanR:21.3125 R:15.0000 rate:0.0300 gloss:0.5011 dlossA:0.5672 dlossQ:1.2010 exploreP:0.9347\n",
      "Episode:32 meanR:21.1818 R:17.0000 rate:0.0340 gloss:0.5389 dlossA:0.5840 dlossQ:1.2622 exploreP:0.9332\n",
      "Episode:33 meanR:21.0882 R:18.0000 rate:0.0360 gloss:0.5592 dlossA:0.6034 dlossQ:1.1542 exploreP:0.9315\n",
      "Episode:34 meanR:21.3143 R:29.0000 rate:0.0580 gloss:0.5037 dlossA:0.5723 dlossQ:1.2357 exploreP:0.9288\n",
      "Episode:35 meanR:21.9167 R:43.0000 rate:0.0860 gloss:0.5191 dlossA:0.5886 dlossQ:1.1908 exploreP:0.9249\n",
      "Episode:36 meanR:22.1351 R:30.0000 rate:0.0600 gloss:0.5260 dlossA:0.5818 dlossQ:1.2402 exploreP:0.9222\n",
      "Episode:37 meanR:22.1579 R:23.0000 rate:0.0460 gloss:0.5365 dlossA:0.5926 dlossQ:1.1656 exploreP:0.9201\n",
      "Episode:38 meanR:22.7949 R:47.0000 rate:0.0940 gloss:0.5449 dlossA:0.5985 dlossQ:1.1483 exploreP:0.9158\n",
      "Episode:39 meanR:22.5750 R:14.0000 rate:0.0280 gloss:0.4959 dlossA:0.5688 dlossQ:1.2458 exploreP:0.9145\n",
      "Episode:40 meanR:22.3902 R:15.0000 rate:0.0300 gloss:0.5267 dlossA:0.5842 dlossQ:1.1693 exploreP:0.9132\n",
      "Episode:41 meanR:22.2381 R:16.0000 rate:0.0320 gloss:0.4997 dlossA:0.5831 dlossQ:1.1908 exploreP:0.9117\n",
      "Episode:42 meanR:22.4419 R:31.0000 rate:0.0620 gloss:0.5387 dlossA:0.5927 dlossQ:1.1619 exploreP:0.9089\n",
      "Episode:43 meanR:22.2955 R:16.0000 rate:0.0320 gloss:0.5334 dlossA:0.5917 dlossQ:1.1656 exploreP:0.9075\n",
      "Episode:44 meanR:22.0000 R:9.0000 rate:0.0180 gloss:0.5587 dlossA:0.6064 dlossQ:1.1559 exploreP:0.9067\n",
      "Episode:45 meanR:22.8043 R:59.0000 rate:0.1180 gloss:0.5305 dlossA:0.5890 dlossQ:1.1733 exploreP:0.9014\n",
      "Episode:46 meanR:23.0213 R:33.0000 rate:0.0660 gloss:0.5561 dlossA:0.6038 dlossQ:1.1323 exploreP:0.8985\n",
      "Episode:47 meanR:22.8542 R:15.0000 rate:0.0300 gloss:0.5395 dlossA:0.5957 dlossQ:1.1632 exploreP:0.8971\n",
      "Episode:48 meanR:22.7143 R:16.0000 rate:0.0320 gloss:0.5507 dlossA:0.5970 dlossQ:1.1522 exploreP:0.8957\n",
      "Episode:49 meanR:22.7000 R:22.0000 rate:0.0440 gloss:0.5138 dlossA:0.5824 dlossQ:1.1846 exploreP:0.8938\n",
      "Episode:50 meanR:22.7255 R:24.0000 rate:0.0480 gloss:0.5429 dlossA:0.5947 dlossQ:1.1584 exploreP:0.8917\n",
      "Episode:51 meanR:22.9615 R:35.0000 rate:0.0700 gloss:0.5453 dlossA:0.5978 dlossQ:1.1749 exploreP:0.8886\n",
      "Episode:52 meanR:22.8868 R:19.0000 rate:0.0380 gloss:0.5052 dlossA:0.5789 dlossQ:1.1864 exploreP:0.8869\n",
      "Episode:53 meanR:22.7778 R:17.0000 rate:0.0340 gloss:0.5497 dlossA:0.6037 dlossQ:1.1144 exploreP:0.8854\n",
      "Episode:54 meanR:23.0182 R:36.0000 rate:0.0720 gloss:0.5493 dlossA:0.6013 dlossQ:1.1604 exploreP:0.8823\n",
      "Episode:55 meanR:23.0714 R:26.0000 rate:0.0520 gloss:0.5111 dlossA:0.5851 dlossQ:1.1644 exploreP:0.8800\n",
      "Episode:56 meanR:23.0175 R:20.0000 rate:0.0400 gloss:0.5739 dlossA:0.5837 dlossQ:1.2759 exploreP:0.8783\n",
      "Episode:57 meanR:23.0862 R:27.0000 rate:0.0540 gloss:0.5438 dlossA:0.5974 dlossQ:1.1750 exploreP:0.8759\n",
      "Episode:58 meanR:23.0169 R:19.0000 rate:0.0380 gloss:0.5206 dlossA:0.5813 dlossQ:1.2153 exploreP:0.8743\n",
      "Episode:59 meanR:23.0500 R:25.0000 rate:0.0500 gloss:0.5197 dlossA:0.5816 dlossQ:1.2049 exploreP:0.8721\n",
      "Episode:60 meanR:23.2459 R:35.0000 rate:0.0700 gloss:0.5355 dlossA:0.5897 dlossQ:1.1698 exploreP:0.8691\n",
      "Episode:61 meanR:23.2258 R:22.0000 rate:0.0440 gloss:0.5345 dlossA:0.5865 dlossQ:1.2418 exploreP:0.8672\n",
      "Episode:62 meanR:23.2857 R:27.0000 rate:0.0540 gloss:0.5270 dlossA:0.5811 dlossQ:1.2006 exploreP:0.8649\n",
      "Episode:63 meanR:23.2656 R:22.0000 rate:0.0440 gloss:0.5350 dlossA:0.5909 dlossQ:1.1839 exploreP:0.8630\n",
      "Episode:64 meanR:23.1231 R:14.0000 rate:0.0280 gloss:0.5286 dlossA:0.5883 dlossQ:1.1770 exploreP:0.8618\n",
      "Episode:65 meanR:23.0455 R:18.0000 rate:0.0360 gloss:0.5709 dlossA:0.6080 dlossQ:1.1394 exploreP:0.8603\n",
      "Episode:66 meanR:23.4776 R:52.0000 rate:0.1040 gloss:0.5315 dlossA:0.5934 dlossQ:1.1792 exploreP:0.8559\n",
      "Episode:67 meanR:23.7353 R:41.0000 rate:0.0820 gloss:0.5544 dlossA:0.5918 dlossQ:1.1994 exploreP:0.8524\n",
      "Episode:68 meanR:23.6522 R:18.0000 rate:0.0360 gloss:0.5428 dlossA:0.5909 dlossQ:1.1797 exploreP:0.8509\n",
      "Episode:69 meanR:23.5143 R:14.0000 rate:0.0280 gloss:0.5325 dlossA:0.5896 dlossQ:1.1647 exploreP:0.8498\n",
      "Episode:70 meanR:23.6056 R:30.0000 rate:0.0600 gloss:0.5437 dlossA:0.5922 dlossQ:1.1735 exploreP:0.8472\n",
      "Episode:71 meanR:23.5417 R:19.0000 rate:0.0380 gloss:0.5098 dlossA:0.5723 dlossQ:1.2201 exploreP:0.8456\n",
      "Episode:72 meanR:24.0685 R:62.0000 rate:0.1240 gloss:0.5274 dlossA:0.5863 dlossQ:1.1857 exploreP:0.8405\n",
      "Episode:73 meanR:23.8919 R:11.0000 rate:0.0220 gloss:0.5143 dlossA:0.5825 dlossQ:1.1853 exploreP:0.8396\n",
      "Episode:74 meanR:23.8667 R:22.0000 rate:0.0440 gloss:0.5523 dlossA:0.5995 dlossQ:1.1618 exploreP:0.8377\n",
      "Episode:75 meanR:24.6053 R:80.0000 rate:0.1600 gloss:0.5339 dlossA:0.5914 dlossQ:1.1992 exploreP:0.8311\n",
      "Episode:76 meanR:24.6234 R:26.0000 rate:0.0520 gloss:0.5395 dlossA:0.6011 dlossQ:1.1476 exploreP:0.8290\n",
      "Episode:77 meanR:24.5128 R:16.0000 rate:0.0320 gloss:0.5283 dlossA:0.5891 dlossQ:1.1913 exploreP:0.8277\n",
      "Episode:78 meanR:25.0000 R:63.0000 rate:0.1260 gloss:0.5483 dlossA:0.5951 dlossQ:1.1790 exploreP:0.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:24.8375 R:12.0000 rate:0.0240 gloss:0.4904 dlossA:0.5904 dlossQ:1.1639 exploreP:0.8216\n",
      "Episode:80 meanR:24.7901 R:21.0000 rate:0.0420 gloss:0.5839 dlossA:0.5865 dlossQ:1.3526 exploreP:0.8199\n",
      "Episode:81 meanR:24.6098 R:10.0000 rate:0.0200 gloss:0.5016 dlossA:0.5647 dlossQ:1.2515 exploreP:0.8191\n",
      "Episode:82 meanR:24.5060 R:16.0000 rate:0.0320 gloss:0.5582 dlossA:0.6112 dlossQ:1.1511 exploreP:0.8178\n",
      "Episode:83 meanR:24.5595 R:29.0000 rate:0.0580 gloss:0.5373 dlossA:0.5895 dlossQ:1.1786 exploreP:0.8155\n",
      "Episode:84 meanR:24.6000 R:28.0000 rate:0.0560 gloss:0.5486 dlossA:0.5973 dlossQ:1.2826 exploreP:0.8132\n",
      "Episode:85 meanR:24.4535 R:12.0000 rate:0.0240 gloss:0.4561 dlossA:0.5532 dlossQ:1.2828 exploreP:0.8122\n",
      "Episode:86 meanR:24.3678 R:17.0000 rate:0.0340 gloss:0.5340 dlossA:0.5958 dlossQ:1.1662 exploreP:0.8109\n",
      "Episode:87 meanR:24.3977 R:27.0000 rate:0.0540 gloss:0.5246 dlossA:0.5880 dlossQ:1.1912 exploreP:0.8087\n",
      "Episode:88 meanR:24.3708 R:22.0000 rate:0.0440 gloss:0.5305 dlossA:0.5816 dlossQ:1.2131 exploreP:0.8070\n",
      "Episode:89 meanR:24.3889 R:26.0000 rate:0.0520 gloss:0.5367 dlossA:0.5885 dlossQ:1.1709 exploreP:0.8049\n",
      "Episode:90 meanR:24.3516 R:21.0000 rate:0.0420 gloss:0.5527 dlossA:0.6008 dlossQ:1.1726 exploreP:0.8032\n",
      "Episode:91 meanR:24.2065 R:11.0000 rate:0.0220 gloss:0.5646 dlossA:0.6019 dlossQ:1.1393 exploreP:0.8024\n",
      "Episode:92 meanR:24.3441 R:37.0000 rate:0.0740 gloss:0.5415 dlossA:0.5923 dlossQ:1.1858 exploreP:0.7994\n",
      "Episode:93 meanR:24.3830 R:28.0000 rate:0.0560 gloss:0.5320 dlossA:0.5942 dlossQ:1.1535 exploreP:0.7972\n",
      "Episode:94 meanR:24.7053 R:55.0000 rate:0.1100 gloss:0.5277 dlossA:0.5858 dlossQ:1.1890 exploreP:0.7929\n",
      "Episode:95 meanR:24.6042 R:15.0000 rate:0.0300 gloss:0.5618 dlossA:0.6045 dlossQ:1.1690 exploreP:0.7917\n",
      "Episode:96 meanR:24.9381 R:57.0000 rate:0.1140 gloss:0.5564 dlossA:0.6003 dlossQ:1.1546 exploreP:0.7873\n",
      "Episode:97 meanR:24.9286 R:24.0000 rate:0.0480 gloss:0.5214 dlossA:0.5859 dlossQ:1.2043 exploreP:0.7854\n",
      "Episode:98 meanR:26.2323 R:154.0000 rate:0.3080 gloss:0.5241 dlossA:0.5862 dlossQ:1.2171 exploreP:0.7736\n",
      "Episode:99 meanR:26.4500 R:48.0000 rate:0.0960 gloss:0.5193 dlossA:0.5866 dlossQ:1.2529 exploreP:0.7699\n",
      "Episode:100 meanR:26.5000 R:22.0000 rate:0.0440 gloss:0.5558 dlossA:0.6008 dlossQ:1.1710 exploreP:0.7682\n",
      "Episode:101 meanR:26.6800 R:34.0000 rate:0.0680 gloss:0.5537 dlossA:0.5986 dlossQ:1.1856 exploreP:0.7657\n",
      "Episode:102 meanR:26.7900 R:22.0000 rate:0.0440 gloss:0.5537 dlossA:0.6012 dlossQ:1.1978 exploreP:0.7640\n",
      "Episode:103 meanR:26.7800 R:23.0000 rate:0.0460 gloss:0.5114 dlossA:0.5819 dlossQ:1.2041 exploreP:0.7623\n",
      "Episode:104 meanR:26.8200 R:40.0000 rate:0.0800 gloss:0.5465 dlossA:0.5942 dlossQ:1.2316 exploreP:0.7593\n",
      "Episode:105 meanR:26.7200 R:15.0000 rate:0.0300 gloss:0.5010 dlossA:0.5815 dlossQ:1.2304 exploreP:0.7582\n",
      "Episode:106 meanR:27.0200 R:53.0000 rate:0.1060 gloss:0.5361 dlossA:0.5903 dlossQ:1.2219 exploreP:0.7542\n",
      "Episode:107 meanR:27.0500 R:22.0000 rate:0.0440 gloss:0.5646 dlossA:0.6034 dlossQ:1.1610 exploreP:0.7526\n",
      "Episode:108 meanR:26.8000 R:13.0000 rate:0.0260 gloss:0.5649 dlossA:0.6078 dlossQ:1.1664 exploreP:0.7516\n",
      "Episode:109 meanR:27.0800 R:45.0000 rate:0.0900 gloss:0.5369 dlossA:0.5909 dlossQ:1.1966 exploreP:0.7483\n",
      "Episode:110 meanR:27.7300 R:80.0000 rate:0.1600 gloss:0.5079 dlossA:0.5770 dlossQ:1.2324 exploreP:0.7424\n",
      "Episode:111 meanR:27.6200 R:12.0000 rate:0.0240 gloss:0.5521 dlossA:0.5959 dlossQ:1.1780 exploreP:0.7415\n",
      "Episode:112 meanR:27.7000 R:29.0000 rate:0.0580 gloss:0.5205 dlossA:0.5756 dlossQ:1.2484 exploreP:0.7394\n",
      "Episode:113 meanR:27.7400 R:26.0000 rate:0.0520 gloss:0.5441 dlossA:0.5962 dlossQ:1.1928 exploreP:0.7375\n",
      "Episode:114 meanR:28.0500 R:59.0000 rate:0.1180 gloss:0.5210 dlossA:0.5799 dlossQ:1.2327 exploreP:0.7332\n",
      "Episode:115 meanR:28.1400 R:25.0000 rate:0.0500 gloss:0.5241 dlossA:0.5789 dlossQ:1.2502 exploreP:0.7314\n",
      "Episode:116 meanR:28.2000 R:25.0000 rate:0.0500 gloss:0.5079 dlossA:0.5668 dlossQ:1.2751 exploreP:0.7296\n",
      "Episode:117 meanR:28.0500 R:30.0000 rate:0.0600 gloss:0.5295 dlossA:0.5878 dlossQ:1.2066 exploreP:0.7275\n",
      "Episode:118 meanR:28.0700 R:14.0000 rate:0.0280 gloss:0.5354 dlossA:0.5925 dlossQ:1.2023 exploreP:0.7264\n",
      "Episode:119 meanR:28.4700 R:49.0000 rate:0.0980 gloss:0.5433 dlossA:0.5931 dlossQ:1.1972 exploreP:0.7229\n",
      "Episode:120 meanR:28.6200 R:34.0000 rate:0.0680 gloss:0.5109 dlossA:0.5813 dlossQ:1.2555 exploreP:0.7205\n",
      "Episode:121 meanR:28.8600 R:54.0000 rate:0.1080 gloss:0.5502 dlossA:0.5960 dlossQ:1.2017 exploreP:0.7167\n",
      "Episode:122 meanR:29.0500 R:41.0000 rate:0.0820 gloss:0.5386 dlossA:0.5981 dlossQ:1.1933 exploreP:0.7138\n",
      "Episode:123 meanR:29.2000 R:31.0000 rate:0.0620 gloss:0.5419 dlossA:0.5985 dlossQ:1.1761 exploreP:0.7116\n",
      "Episode:124 meanR:29.1600 R:12.0000 rate:0.0240 gloss:0.5746 dlossA:0.6160 dlossQ:1.1310 exploreP:0.7108\n",
      "Episode:125 meanR:29.1700 R:23.0000 rate:0.0460 gloss:0.5178 dlossA:0.5894 dlossQ:1.2575 exploreP:0.7092\n",
      "Episode:126 meanR:29.4900 R:43.0000 rate:0.0860 gloss:0.5327 dlossA:0.5921 dlossQ:1.1921 exploreP:0.7062\n",
      "Episode:127 meanR:29.7600 R:48.0000 rate:0.0960 gloss:0.5483 dlossA:0.5988 dlossQ:1.1979 exploreP:0.7028\n",
      "Episode:128 meanR:30.2700 R:85.0000 rate:0.1700 gloss:0.5394 dlossA:0.5963 dlossQ:1.1820 exploreP:0.6970\n",
      "Episode:129 meanR:30.2200 R:11.0000 rate:0.0220 gloss:0.4531 dlossA:0.5477 dlossQ:1.2856 exploreP:0.6962\n",
      "Episode:130 meanR:30.4000 R:42.0000 rate:0.0840 gloss:0.5310 dlossA:0.5865 dlossQ:1.2140 exploreP:0.6933\n",
      "Episode:131 meanR:30.5600 R:31.0000 rate:0.0620 gloss:0.5540 dlossA:0.6054 dlossQ:1.1739 exploreP:0.6912\n",
      "Episode:132 meanR:30.5800 R:19.0000 rate:0.0380 gloss:0.5191 dlossA:0.5760 dlossQ:1.2173 exploreP:0.6899\n",
      "Episode:133 meanR:30.6300 R:23.0000 rate:0.0460 gloss:0.5439 dlossA:0.5882 dlossQ:1.2101 exploreP:0.6884\n",
      "Episode:134 meanR:31.3000 R:96.0000 rate:0.1920 gloss:0.5191 dlossA:0.5861 dlossQ:1.2212 exploreP:0.6819\n",
      "Episode:135 meanR:31.0000 R:13.0000 rate:0.0260 gloss:0.5406 dlossA:0.5916 dlossQ:1.1954 exploreP:0.6810\n",
      "Episode:136 meanR:31.3000 R:60.0000 rate:0.1200 gloss:0.5083 dlossA:0.5814 dlossQ:1.2305 exploreP:0.6770\n",
      "Episode:137 meanR:31.4200 R:35.0000 rate:0.0700 gloss:0.5440 dlossA:0.5996 dlossQ:1.1679 exploreP:0.6747\n",
      "Episode:138 meanR:31.2700 R:32.0000 rate:0.0640 gloss:0.4989 dlossA:0.5786 dlossQ:1.2480 exploreP:0.6726\n",
      "Episode:139 meanR:31.6900 R:56.0000 rate:0.1120 gloss:0.5256 dlossA:0.5896 dlossQ:1.2086 exploreP:0.6689\n",
      "Episode:140 meanR:31.8000 R:26.0000 rate:0.0520 gloss:0.5212 dlossA:0.5896 dlossQ:1.2052 exploreP:0.6671\n",
      "Episode:141 meanR:32.3100 R:67.0000 rate:0.1340 gloss:0.5227 dlossA:0.5849 dlossQ:1.2205 exploreP:0.6628\n",
      "Episode:142 meanR:32.4800 R:48.0000 rate:0.0960 gloss:0.5356 dlossA:0.5899 dlossQ:1.2252 exploreP:0.6596\n",
      "Episode:143 meanR:32.7700 R:45.0000 rate:0.0900 gloss:0.5386 dlossA:0.5962 dlossQ:1.1840 exploreP:0.6567\n",
      "Episode:144 meanR:33.0000 R:32.0000 rate:0.0640 gloss:0.5467 dlossA:0.5983 dlossQ:1.1744 exploreP:0.6546\n",
      "Episode:145 meanR:32.6600 R:25.0000 rate:0.0500 gloss:0.5421 dlossA:0.5949 dlossQ:1.1985 exploreP:0.6530\n",
      "Episode:146 meanR:33.2200 R:89.0000 rate:0.1780 gloss:0.5345 dlossA:0.5971 dlossQ:1.2244 exploreP:0.6473\n",
      "Episode:147 meanR:34.1800 R:111.0000 rate:0.2220 gloss:0.5320 dlossA:0.5917 dlossQ:1.2464 exploreP:0.6403\n",
      "Episode:148 meanR:34.3900 R:37.0000 rate:0.0740 gloss:0.5352 dlossA:0.5907 dlossQ:1.1946 exploreP:0.6380\n",
      "Episode:149 meanR:35.0500 R:88.0000 rate:0.1760 gloss:0.5344 dlossA:0.5833 dlossQ:1.2868 exploreP:0.6325\n",
      "Episode:150 meanR:35.7100 R:90.0000 rate:0.1800 gloss:0.5143 dlossA:0.5882 dlossQ:1.2059 exploreP:0.6269\n",
      "Episode:151 meanR:35.6100 R:25.0000 rate:0.0500 gloss:0.5311 dlossA:0.5956 dlossQ:1.1959 exploreP:0.6254\n",
      "Episode:152 meanR:35.7700 R:35.0000 rate:0.0700 gloss:0.4902 dlossA:0.5776 dlossQ:1.3014 exploreP:0.6232\n",
      "Episode:153 meanR:36.2300 R:63.0000 rate:0.1260 gloss:0.5163 dlossA:0.5817 dlossQ:1.2258 exploreP:0.6194\n",
      "Episode:154 meanR:36.1400 R:27.0000 rate:0.0540 gloss:0.5094 dlossA:0.5959 dlossQ:1.2183 exploreP:0.6177\n",
      "Episode:155 meanR:36.0800 R:20.0000 rate:0.0400 gloss:0.5200 dlossA:0.5946 dlossQ:1.2287 exploreP:0.6165\n",
      "Episode:156 meanR:36.1600 R:28.0000 rate:0.0560 gloss:0.5183 dlossA:0.5814 dlossQ:1.2508 exploreP:0.6148\n",
      "Episode:157 meanR:36.0600 R:17.0000 rate:0.0340 gloss:0.5054 dlossA:0.5843 dlossQ:1.2312 exploreP:0.6138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:36.6700 R:80.0000 rate:0.1600 gloss:0.5483 dlossA:0.5987 dlossQ:1.2060 exploreP:0.6090\n",
      "Episode:159 meanR:36.5900 R:17.0000 rate:0.0340 gloss:0.5358 dlossA:0.5867 dlossQ:1.2269 exploreP:0.6079\n",
      "Episode:160 meanR:36.5400 R:30.0000 rate:0.0600 gloss:0.4980 dlossA:0.5777 dlossQ:1.2415 exploreP:0.6062\n",
      "Episode:161 meanR:36.7300 R:41.0000 rate:0.0820 gloss:0.5383 dlossA:0.5944 dlossQ:1.1931 exploreP:0.6037\n",
      "Episode:162 meanR:37.0500 R:59.0000 rate:0.1180 gloss:0.5347 dlossA:0.5944 dlossQ:1.1912 exploreP:0.6002\n",
      "Episode:163 meanR:37.2200 R:39.0000 rate:0.0780 gloss:0.5480 dlossA:0.5962 dlossQ:1.2035 exploreP:0.5979\n",
      "Episode:164 meanR:37.3600 R:28.0000 rate:0.0560 gloss:0.5246 dlossA:0.5910 dlossQ:1.2028 exploreP:0.5963\n",
      "Episode:165 meanR:37.3600 R:18.0000 rate:0.0360 gloss:0.5626 dlossA:0.5933 dlossQ:1.3393 exploreP:0.5952\n",
      "Episode:166 meanR:37.1600 R:32.0000 rate:0.0640 gloss:0.5468 dlossA:0.5951 dlossQ:1.1993 exploreP:0.5934\n",
      "Episode:167 meanR:37.1300 R:38.0000 rate:0.0760 gloss:0.5464 dlossA:0.6017 dlossQ:1.1814 exploreP:0.5911\n",
      "Episode:168 meanR:37.2900 R:34.0000 rate:0.0680 gloss:0.5137 dlossA:0.5863 dlossQ:1.2911 exploreP:0.5892\n",
      "Episode:169 meanR:37.6400 R:49.0000 rate:0.0980 gloss:0.5091 dlossA:0.5803 dlossQ:1.2395 exploreP:0.5863\n",
      "Episode:170 meanR:37.7700 R:43.0000 rate:0.0860 gloss:0.5508 dlossA:0.6044 dlossQ:1.1817 exploreP:0.5839\n",
      "Episode:171 meanR:38.1100 R:53.0000 rate:0.1060 gloss:0.5190 dlossA:0.5761 dlossQ:1.2891 exploreP:0.5808\n",
      "Episode:172 meanR:37.9100 R:42.0000 rate:0.0840 gloss:0.5375 dlossA:0.5961 dlossQ:1.2391 exploreP:0.5784\n",
      "Episode:173 meanR:37.9200 R:12.0000 rate:0.0240 gloss:0.5411 dlossA:0.5967 dlossQ:1.1812 exploreP:0.5778\n",
      "Episode:174 meanR:38.3100 R:61.0000 rate:0.1220 gloss:0.5379 dlossA:0.5840 dlossQ:1.3630 exploreP:0.5743\n",
      "Episode:175 meanR:38.0500 R:54.0000 rate:0.1080 gloss:0.5476 dlossA:0.5996 dlossQ:1.1803 exploreP:0.5713\n",
      "Episode:176 meanR:37.9300 R:14.0000 rate:0.0280 gloss:0.5146 dlossA:0.5811 dlossQ:1.3640 exploreP:0.5705\n",
      "Episode:177 meanR:38.7400 R:97.0000 rate:0.1940 gloss:0.5302 dlossA:0.5896 dlossQ:1.2183 exploreP:0.5651\n",
      "Episode:178 meanR:40.0500 R:194.0000 rate:0.3880 gloss:0.5203 dlossA:0.5868 dlossQ:1.2288 exploreP:0.5544\n",
      "Episode:179 meanR:40.5100 R:58.0000 rate:0.1160 gloss:0.5260 dlossA:0.5910 dlossQ:1.2126 exploreP:0.5513\n",
      "Episode:180 meanR:40.4700 R:17.0000 rate:0.0340 gloss:0.4764 dlossA:0.5722 dlossQ:1.3297 exploreP:0.5503\n",
      "Episode:181 meanR:41.7300 R:136.0000 rate:0.2720 gloss:0.5274 dlossA:0.5878 dlossQ:1.2184 exploreP:0.5430\n",
      "Episode:182 meanR:42.0200 R:45.0000 rate:0.0900 gloss:0.5191 dlossA:0.5806 dlossQ:1.2721 exploreP:0.5407\n",
      "Episode:183 meanR:42.5200 R:79.0000 rate:0.1580 gloss:0.5170 dlossA:0.5771 dlossQ:1.2809 exploreP:0.5365\n",
      "Episode:184 meanR:42.6600 R:42.0000 rate:0.0840 gloss:0.5228 dlossA:0.5891 dlossQ:1.2265 exploreP:0.5343\n",
      "Episode:185 meanR:43.1700 R:63.0000 rate:0.1260 gloss:0.5004 dlossA:0.5781 dlossQ:1.2542 exploreP:0.5310\n",
      "Episode:186 meanR:43.5700 R:57.0000 rate:0.1140 gloss:0.5251 dlossA:0.5980 dlossQ:1.2081 exploreP:0.5280\n",
      "Episode:187 meanR:43.4800 R:18.0000 rate:0.0360 gloss:0.5361 dlossA:0.5929 dlossQ:1.1885 exploreP:0.5271\n",
      "Episode:188 meanR:44.2300 R:97.0000 rate:0.1940 gloss:0.5254 dlossA:0.5856 dlossQ:1.2238 exploreP:0.5221\n",
      "Episode:189 meanR:44.4600 R:49.0000 rate:0.0980 gloss:0.5133 dlossA:0.5864 dlossQ:1.2583 exploreP:0.5196\n",
      "Episode:190 meanR:44.9400 R:69.0000 rate:0.1380 gloss:0.4985 dlossA:0.5777 dlossQ:1.2553 exploreP:0.5161\n",
      "Episode:191 meanR:45.2200 R:39.0000 rate:0.0780 gloss:0.5077 dlossA:0.5909 dlossQ:1.2454 exploreP:0.5141\n",
      "Episode:192 meanR:45.1500 R:30.0000 rate:0.0600 gloss:0.5308 dlossA:0.5944 dlossQ:1.2134 exploreP:0.5126\n",
      "Episode:193 meanR:45.7200 R:85.0000 rate:0.1700 gloss:0.5250 dlossA:0.5879 dlossQ:1.2438 exploreP:0.5084\n",
      "Episode:194 meanR:45.6000 R:43.0000 rate:0.0860 gloss:0.4828 dlossA:0.5737 dlossQ:1.2794 exploreP:0.5062\n",
      "Episode:195 meanR:45.9400 R:49.0000 rate:0.0980 gloss:0.5445 dlossA:0.5977 dlossQ:1.2119 exploreP:0.5038\n",
      "Episode:196 meanR:45.9000 R:53.0000 rate:0.1060 gloss:0.5274 dlossA:0.5893 dlossQ:1.2899 exploreP:0.5012\n",
      "Episode:197 meanR:46.2400 R:58.0000 rate:0.1160 gloss:0.5111 dlossA:0.5734 dlossQ:1.2547 exploreP:0.4983\n",
      "Episode:198 meanR:45.0800 R:38.0000 rate:0.0760 gloss:0.5252 dlossA:0.5932 dlossQ:1.2299 exploreP:0.4965\n",
      "Episode:199 meanR:45.2500 R:65.0000 rate:0.1300 gloss:0.5105 dlossA:0.5768 dlossQ:1.2582 exploreP:0.4933\n",
      "Episode:200 meanR:45.4800 R:45.0000 rate:0.0900 gloss:0.5353 dlossA:0.5803 dlossQ:1.3346 exploreP:0.4912\n",
      "Episode:201 meanR:45.4600 R:32.0000 rate:0.0640 gloss:0.4969 dlossA:0.5946 dlossQ:1.2548 exploreP:0.4896\n",
      "Episode:202 meanR:46.4000 R:116.0000 rate:0.2320 gloss:0.5224 dlossA:0.5883 dlossQ:1.2176 exploreP:0.4841\n",
      "Episode:203 meanR:46.6400 R:47.0000 rate:0.0940 gloss:0.5167 dlossA:0.5881 dlossQ:1.2336 exploreP:0.4819\n",
      "Episode:204 meanR:46.7100 R:47.0000 rate:0.0940 gloss:0.5037 dlossA:0.5850 dlossQ:1.2440 exploreP:0.4797\n",
      "Episode:205 meanR:47.0900 R:53.0000 rate:0.1060 gloss:0.5305 dlossA:0.5974 dlossQ:1.2021 exploreP:0.4772\n",
      "Episode:206 meanR:46.9100 R:35.0000 rate:0.0700 gloss:0.4969 dlossA:0.5721 dlossQ:1.2587 exploreP:0.4755\n",
      "Episode:207 meanR:47.0700 R:38.0000 rate:0.0760 gloss:0.5208 dlossA:0.5899 dlossQ:1.2086 exploreP:0.4738\n",
      "Episode:208 meanR:47.1600 R:22.0000 rate:0.0440 gloss:0.5437 dlossA:0.5969 dlossQ:1.1922 exploreP:0.4728\n",
      "Episode:209 meanR:47.2700 R:56.0000 rate:0.1120 gloss:0.4923 dlossA:0.5763 dlossQ:1.2779 exploreP:0.4702\n",
      "Episode:210 meanR:47.0400 R:57.0000 rate:0.1140 gloss:0.5125 dlossA:0.5906 dlossQ:1.2498 exploreP:0.4676\n",
      "Episode:211 meanR:47.4100 R:49.0000 rate:0.0980 gloss:0.5313 dlossA:0.5903 dlossQ:1.2263 exploreP:0.4653\n",
      "Episode:212 meanR:47.5100 R:39.0000 rate:0.0780 gloss:0.5218 dlossA:0.5869 dlossQ:1.2256 exploreP:0.4635\n",
      "Episode:213 meanR:48.4200 R:117.0000 rate:0.2340 gloss:0.5218 dlossA:0.5813 dlossQ:1.3176 exploreP:0.4583\n",
      "Episode:214 meanR:48.2700 R:44.0000 rate:0.0880 gloss:0.5116 dlossA:0.5833 dlossQ:1.2429 exploreP:0.4563\n",
      "Episode:215 meanR:48.3800 R:36.0000 rate:0.0720 gloss:0.5296 dlossA:0.5883 dlossQ:1.2193 exploreP:0.4547\n",
      "Episode:216 meanR:48.9300 R:80.0000 rate:0.1600 gloss:0.5213 dlossA:0.5826 dlossQ:1.2395 exploreP:0.4512\n",
      "Episode:217 meanR:49.6300 R:100.0000 rate:0.2000 gloss:0.5100 dlossA:0.5792 dlossQ:1.3116 exploreP:0.4468\n",
      "Episode:218 meanR:50.2800 R:79.0000 rate:0.1580 gloss:0.5220 dlossA:0.5855 dlossQ:1.2290 exploreP:0.4433\n",
      "Episode:219 meanR:50.7100 R:92.0000 rate:0.1840 gloss:0.5031 dlossA:0.5789 dlossQ:1.2453 exploreP:0.4394\n",
      "Episode:220 meanR:50.7400 R:37.0000 rate:0.0740 gloss:0.5064 dlossA:0.5804 dlossQ:1.2605 exploreP:0.4378\n",
      "Episode:221 meanR:51.0300 R:83.0000 rate:0.1660 gloss:0.4940 dlossA:0.5752 dlossQ:1.2882 exploreP:0.4342\n",
      "Episode:222 meanR:51.9400 R:132.0000 rate:0.2640 gloss:0.5243 dlossA:0.5880 dlossQ:1.2728 exploreP:0.4287\n",
      "Episode:223 meanR:51.8400 R:21.0000 rate:0.0420 gloss:0.5243 dlossA:0.6003 dlossQ:1.2165 exploreP:0.4278\n",
      "Episode:224 meanR:52.3100 R:59.0000 rate:0.1180 gloss:0.4963 dlossA:0.5776 dlossQ:1.2780 exploreP:0.4253\n",
      "Episode:225 meanR:52.2900 R:21.0000 rate:0.0420 gloss:0.5277 dlossA:0.5950 dlossQ:1.1953 exploreP:0.4245\n",
      "Episode:226 meanR:52.8000 R:94.0000 rate:0.1880 gloss:0.5030 dlossA:0.5816 dlossQ:1.2742 exploreP:0.4206\n",
      "Episode:227 meanR:52.8200 R:50.0000 rate:0.1000 gloss:0.5209 dlossA:0.5790 dlossQ:1.2724 exploreP:0.4185\n",
      "Episode:228 meanR:52.6300 R:66.0000 rate:0.1320 gloss:0.5160 dlossA:0.5839 dlossQ:1.2588 exploreP:0.4159\n",
      "Episode:229 meanR:53.3400 R:82.0000 rate:0.1640 gloss:0.5151 dlossA:0.5862 dlossQ:1.2736 exploreP:0.4125\n",
      "Episode:230 meanR:53.7000 R:78.0000 rate:0.1560 gloss:0.4996 dlossA:0.5757 dlossQ:1.3069 exploreP:0.4094\n",
      "Episode:231 meanR:54.1700 R:78.0000 rate:0.1560 gloss:0.5194 dlossA:0.5878 dlossQ:1.3609 exploreP:0.4063\n",
      "Episode:232 meanR:54.3400 R:36.0000 rate:0.0720 gloss:0.5097 dlossA:0.5844 dlossQ:1.2639 exploreP:0.4049\n",
      "Episode:233 meanR:54.8800 R:77.0000 rate:0.1540 gloss:0.5067 dlossA:0.5824 dlossQ:1.2728 exploreP:0.4019\n",
      "Episode:234 meanR:54.4100 R:49.0000 rate:0.0980 gloss:0.5126 dlossA:0.5888 dlossQ:1.2359 exploreP:0.3999\n",
      "Episode:235 meanR:54.5300 R:25.0000 rate:0.0500 gloss:0.5075 dlossA:0.5924 dlossQ:1.2017 exploreP:0.3990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:54.4400 R:51.0000 rate:0.1020 gloss:0.5306 dlossA:0.5922 dlossQ:1.2632 exploreP:0.3970\n",
      "Episode:237 meanR:54.7900 R:70.0000 rate:0.1400 gloss:0.5006 dlossA:0.5827 dlossQ:1.2453 exploreP:0.3943\n",
      "Episode:238 meanR:55.5700 R:110.0000 rate:0.2200 gloss:0.5042 dlossA:0.5744 dlossQ:1.3344 exploreP:0.3901\n",
      "Episode:239 meanR:56.1100 R:110.0000 rate:0.2200 gloss:0.4963 dlossA:0.5802 dlossQ:1.2781 exploreP:0.3859\n",
      "Episode:240 meanR:56.6100 R:76.0000 rate:0.1520 gloss:0.4956 dlossA:0.5742 dlossQ:1.3149 exploreP:0.3831\n",
      "Episode:241 meanR:56.4500 R:51.0000 rate:0.1020 gloss:0.5242 dlossA:0.5909 dlossQ:1.2435 exploreP:0.3812\n",
      "Episode:242 meanR:56.5500 R:58.0000 rate:0.1160 gloss:0.4955 dlossA:0.5803 dlossQ:1.2630 exploreP:0.3790\n",
      "Episode:243 meanR:56.6900 R:59.0000 rate:0.1180 gloss:0.4952 dlossA:0.5783 dlossQ:1.3448 exploreP:0.3769\n",
      "Episode:244 meanR:57.7200 R:135.0000 rate:0.2700 gloss:0.4965 dlossA:0.5827 dlossQ:1.2620 exploreP:0.3719\n",
      "Episode:245 meanR:58.1600 R:69.0000 rate:0.1380 gloss:0.4817 dlossA:0.5732 dlossQ:1.3364 exploreP:0.3695\n",
      "Episode:246 meanR:57.7900 R:52.0000 rate:0.1040 gloss:0.4707 dlossA:0.5706 dlossQ:1.2992 exploreP:0.3676\n",
      "Episode:247 meanR:57.9200 R:124.0000 rate:0.2480 gloss:0.5104 dlossA:0.5830 dlossQ:1.2576 exploreP:0.3632\n",
      "Episode:248 meanR:58.4600 R:91.0000 rate:0.1820 gloss:0.5184 dlossA:0.5959 dlossQ:1.2689 exploreP:0.3600\n",
      "Episode:249 meanR:58.4900 R:91.0000 rate:0.1820 gloss:0.5048 dlossA:0.5827 dlossQ:1.2680 exploreP:0.3568\n",
      "Episode:250 meanR:58.5300 R:94.0000 rate:0.1880 gloss:0.5014 dlossA:0.5813 dlossQ:1.2867 exploreP:0.3536\n",
      "Episode:251 meanR:58.7400 R:46.0000 rate:0.0920 gloss:0.4812 dlossA:0.5574 dlossQ:1.3984 exploreP:0.3520\n",
      "Episode:252 meanR:59.1800 R:79.0000 rate:0.1580 gloss:0.5140 dlossA:0.5882 dlossQ:1.2544 exploreP:0.3493\n",
      "Episode:253 meanR:59.3300 R:78.0000 rate:0.1560 gloss:0.5177 dlossA:0.5918 dlossQ:1.2727 exploreP:0.3467\n",
      "Episode:254 meanR:60.1600 R:110.0000 rate:0.2200 gloss:0.4934 dlossA:0.5834 dlossQ:1.3238 exploreP:0.3430\n",
      "Episode:255 meanR:60.5300 R:57.0000 rate:0.1140 gloss:0.5258 dlossA:0.5961 dlossQ:1.2594 exploreP:0.3411\n",
      "Episode:256 meanR:60.7200 R:47.0000 rate:0.0940 gloss:0.5057 dlossA:0.5766 dlossQ:1.3245 exploreP:0.3395\n",
      "Episode:257 meanR:61.1800 R:63.0000 rate:0.1260 gloss:0.5322 dlossA:0.5960 dlossQ:1.2099 exploreP:0.3375\n",
      "Episode:258 meanR:61.6200 R:124.0000 rate:0.2480 gloss:0.5048 dlossA:0.5771 dlossQ:1.3838 exploreP:0.3334\n",
      "Episode:259 meanR:63.6300 R:218.0000 rate:0.4360 gloss:0.5105 dlossA:0.5854 dlossQ:1.2773 exploreP:0.3265\n",
      "Episode:260 meanR:65.7000 R:237.0000 rate:0.4740 gloss:0.5107 dlossA:0.5887 dlossQ:1.2894 exploreP:0.3191\n",
      "Episode:261 meanR:65.9900 R:70.0000 rate:0.1400 gloss:0.4979 dlossA:0.5785 dlossQ:1.2844 exploreP:0.3169\n",
      "Episode:262 meanR:66.3400 R:94.0000 rate:0.1880 gloss:0.5021 dlossA:0.5827 dlossQ:1.3087 exploreP:0.3140\n",
      "Episode:263 meanR:67.0400 R:109.0000 rate:0.2180 gloss:0.4961 dlossA:0.5789 dlossQ:1.2735 exploreP:0.3107\n",
      "Episode:264 meanR:68.6800 R:192.0000 rate:0.3840 gloss:0.4924 dlossA:0.5754 dlossQ:1.3075 exploreP:0.3050\n",
      "Episode:265 meanR:69.4000 R:90.0000 rate:0.1800 gloss:0.5071 dlossA:0.5873 dlossQ:1.2952 exploreP:0.3024\n",
      "Episode:266 meanR:70.8500 R:177.0000 rate:0.3540 gloss:0.4808 dlossA:0.5755 dlossQ:1.3257 exploreP:0.2972\n",
      "Episode:267 meanR:70.8900 R:42.0000 rate:0.0840 gloss:0.4944 dlossA:0.5767 dlossQ:1.3424 exploreP:0.2960\n",
      "Episode:268 meanR:71.1600 R:61.0000 rate:0.1220 gloss:0.4908 dlossA:0.5851 dlossQ:1.3511 exploreP:0.2943\n",
      "Episode:269 meanR:71.1800 R:51.0000 rate:0.1020 gloss:0.4997 dlossA:0.5834 dlossQ:1.2723 exploreP:0.2928\n",
      "Episode:270 meanR:70.9500 R:20.0000 rate:0.0400 gloss:0.4864 dlossA:0.5972 dlossQ:1.4414 exploreP:0.2923\n",
      "Episode:271 meanR:71.0100 R:59.0000 rate:0.1180 gloss:0.5141 dlossA:0.5878 dlossQ:1.2875 exploreP:0.2906\n",
      "Episode:272 meanR:71.3500 R:76.0000 rate:0.1520 gloss:0.4889 dlossA:0.5825 dlossQ:1.2917 exploreP:0.2885\n",
      "Episode:273 meanR:72.3200 R:109.0000 rate:0.2180 gloss:0.4899 dlossA:0.5759 dlossQ:1.3205 exploreP:0.2855\n",
      "Episode:274 meanR:73.9600 R:225.0000 rate:0.4500 gloss:0.4960 dlossA:0.5868 dlossQ:1.2876 exploreP:0.2793\n",
      "Episode:275 meanR:75.0500 R:163.0000 rate:0.3260 gloss:0.5040 dlossA:0.5832 dlossQ:1.2892 exploreP:0.2750\n",
      "Episode:276 meanR:75.5000 R:59.0000 rate:0.1180 gloss:0.4762 dlossA:0.5699 dlossQ:1.3603 exploreP:0.2734\n",
      "Episode:277 meanR:76.6500 R:212.0000 rate:0.4240 gloss:0.5031 dlossA:0.5874 dlossQ:1.3040 exploreP:0.2679\n",
      "Episode:278 meanR:75.2800 R:57.0000 rate:0.1140 gloss:0.4989 dlossA:0.5783 dlossQ:1.3351 exploreP:0.2664\n",
      "Episode:279 meanR:75.8300 R:113.0000 rate:0.2260 gloss:0.4937 dlossA:0.5780 dlossQ:1.3174 exploreP:0.2636\n",
      "Episode:280 meanR:76.2800 R:62.0000 rate:0.1240 gloss:0.4922 dlossA:0.5819 dlossQ:1.3013 exploreP:0.2620\n",
      "Episode:281 meanR:76.1200 R:120.0000 rate:0.2400 gloss:0.4933 dlossA:0.5915 dlossQ:1.3231 exploreP:0.2590\n",
      "Episode:282 meanR:76.8900 R:122.0000 rate:0.2440 gloss:0.5154 dlossA:0.5914 dlossQ:1.3114 exploreP:0.2560\n",
      "Episode:283 meanR:77.4600 R:136.0000 rate:0.2720 gloss:0.5112 dlossA:0.5828 dlossQ:1.3481 exploreP:0.2526\n",
      "Episode:284 meanR:78.8500 R:181.0000 rate:0.3620 gloss:0.5016 dlossA:0.5860 dlossQ:1.2852 exploreP:0.2483\n",
      "Episode:285 meanR:78.7600 R:54.0000 rate:0.1080 gloss:0.4853 dlossA:0.5828 dlossQ:1.3210 exploreP:0.2470\n",
      "Episode:286 meanR:78.6000 R:41.0000 rate:0.0820 gloss:0.4808 dlossA:0.5729 dlossQ:1.3585 exploreP:0.2460\n",
      "Episode:287 meanR:79.3400 R:92.0000 rate:0.1840 gloss:0.4934 dlossA:0.5885 dlossQ:1.3053 exploreP:0.2439\n",
      "Episode:288 meanR:79.2100 R:84.0000 rate:0.1680 gloss:0.4913 dlossA:0.5895 dlossQ:1.3678 exploreP:0.2419\n",
      "Episode:289 meanR:79.3500 R:63.0000 rate:0.1260 gloss:0.5257 dlossA:0.5954 dlossQ:1.2896 exploreP:0.2405\n",
      "Episode:290 meanR:80.1600 R:150.0000 rate:0.3000 gloss:0.5067 dlossA:0.5932 dlossQ:1.3317 exploreP:0.2370\n",
      "Episode:291 meanR:80.4200 R:65.0000 rate:0.1300 gloss:0.4982 dlossA:0.5999 dlossQ:1.3238 exploreP:0.2356\n",
      "Episode:292 meanR:81.2000 R:108.0000 rate:0.2160 gloss:0.4923 dlossA:0.5939 dlossQ:1.3151 exploreP:0.2331\n",
      "Episode:293 meanR:82.1200 R:177.0000 rate:0.3540 gloss:0.5110 dlossA:0.5929 dlossQ:1.3451 exploreP:0.2292\n",
      "Episode:294 meanR:82.8500 R:116.0000 rate:0.2320 gloss:0.5021 dlossA:0.5906 dlossQ:1.3770 exploreP:0.2267\n",
      "Episode:295 meanR:82.9900 R:63.0000 rate:0.1260 gloss:0.5141 dlossA:0.5892 dlossQ:1.3591 exploreP:0.2253\n",
      "Episode:296 meanR:83.1600 R:70.0000 rate:0.1400 gloss:0.5409 dlossA:0.6048 dlossQ:1.2911 exploreP:0.2238\n",
      "Episode:297 meanR:83.2100 R:63.0000 rate:0.1260 gloss:0.5046 dlossA:0.5940 dlossQ:1.3491 exploreP:0.2225\n",
      "Episode:298 meanR:83.9600 R:113.0000 rate:0.2260 gloss:0.4983 dlossA:0.5800 dlossQ:1.3811 exploreP:0.2201\n",
      "Episode:299 meanR:83.8800 R:57.0000 rate:0.1140 gloss:0.5249 dlossA:0.5973 dlossQ:1.2887 exploreP:0.2189\n",
      "Episode:300 meanR:84.2100 R:78.0000 rate:0.1560 gloss:0.5049 dlossA:0.5962 dlossQ:1.2945 exploreP:0.2173\n",
      "Episode:301 meanR:85.4600 R:157.0000 rate:0.3140 gloss:0.5152 dlossA:0.5880 dlossQ:1.3194 exploreP:0.2141\n",
      "Episode:302 meanR:85.0900 R:79.0000 rate:0.1580 gloss:0.4898 dlossA:0.5885 dlossQ:1.3812 exploreP:0.2125\n",
      "Episode:303 meanR:85.2900 R:67.0000 rate:0.1340 gloss:0.4929 dlossA:0.5899 dlossQ:1.3200 exploreP:0.2111\n",
      "Episode:304 meanR:85.4200 R:60.0000 rate:0.1200 gloss:0.4958 dlossA:0.5905 dlossQ:1.3157 exploreP:0.2099\n",
      "Episode:305 meanR:85.5400 R:65.0000 rate:0.1300 gloss:0.5076 dlossA:0.5838 dlossQ:1.4933 exploreP:0.2086\n",
      "Episode:306 meanR:85.9400 R:75.0000 rate:0.1500 gloss:0.5042 dlossA:0.5866 dlossQ:1.3168 exploreP:0.2071\n",
      "Episode:307 meanR:85.9900 R:43.0000 rate:0.0860 gloss:0.4967 dlossA:0.5821 dlossQ:1.4236 exploreP:0.2063\n",
      "Episode:308 meanR:86.5100 R:74.0000 rate:0.1480 gloss:0.5084 dlossA:0.6024 dlossQ:1.2918 exploreP:0.2048\n",
      "Episode:309 meanR:87.1700 R:122.0000 rate:0.2440 gloss:0.5395 dlossA:0.6029 dlossQ:1.3868 exploreP:0.2025\n",
      "Episode:310 meanR:87.2400 R:64.0000 rate:0.1280 gloss:0.5423 dlossA:0.6094 dlossQ:1.2644 exploreP:0.2012\n",
      "Episode:311 meanR:87.4100 R:66.0000 rate:0.1320 gloss:0.5018 dlossA:0.5894 dlossQ:1.3461 exploreP:0.2000\n",
      "Episode:312 meanR:87.5300 R:51.0000 rate:0.1020 gloss:0.5019 dlossA:0.5845 dlossQ:1.3426 exploreP:0.1990\n",
      "Episode:313 meanR:87.2800 R:92.0000 rate:0.1840 gloss:0.5067 dlossA:0.5928 dlossQ:1.3318 exploreP:0.1973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:314 meanR:87.3700 R:53.0000 rate:0.1060 gloss:0.5273 dlossA:0.6018 dlossQ:1.2854 exploreP:0.1963\n",
      "Episode:315 meanR:88.0900 R:108.0000 rate:0.2160 gloss:0.5000 dlossA:0.5864 dlossQ:1.3456 exploreP:0.1943\n",
      "Episode:316 meanR:87.8900 R:60.0000 rate:0.1200 gloss:0.5058 dlossA:0.5936 dlossQ:1.3315 exploreP:0.1932\n",
      "Episode:317 meanR:87.9900 R:110.0000 rate:0.2200 gloss:0.5020 dlossA:0.5904 dlossQ:1.3281 exploreP:0.1912\n",
      "Episode:318 meanR:88.1400 R:94.0000 rate:0.1880 gloss:0.4883 dlossA:0.5865 dlossQ:1.3707 exploreP:0.1895\n",
      "Episode:319 meanR:87.8100 R:59.0000 rate:0.1180 gloss:0.5011 dlossA:0.6041 dlossQ:1.3095 exploreP:0.1884\n",
      "Episode:320 meanR:88.3200 R:88.0000 rate:0.1760 gloss:0.5090 dlossA:0.6008 dlossQ:1.3165 exploreP:0.1869\n",
      "Episode:321 meanR:88.3500 R:86.0000 rate:0.1720 gloss:0.5201 dlossA:0.6029 dlossQ:1.3150 exploreP:0.1854\n",
      "Episode:322 meanR:88.0200 R:99.0000 rate:0.1980 gloss:0.5072 dlossA:0.5996 dlossQ:1.3049 exploreP:0.1836\n",
      "Episode:323 meanR:88.3400 R:53.0000 rate:0.1060 gloss:0.5154 dlossA:0.5994 dlossQ:1.3223 exploreP:0.1827\n",
      "Episode:324 meanR:88.9200 R:117.0000 rate:0.2340 gloss:0.5024 dlossA:0.5925 dlossQ:1.3528 exploreP:0.1807\n",
      "Episode:325 meanR:89.9800 R:127.0000 rate:0.2540 gloss:0.5117 dlossA:0.5946 dlossQ:1.3133 exploreP:0.1785\n",
      "Episode:326 meanR:90.1000 R:106.0000 rate:0.2120 gloss:0.5012 dlossA:0.5925 dlossQ:1.3429 exploreP:0.1768\n",
      "Episode:327 meanR:90.6500 R:105.0000 rate:0.2100 gloss:0.5060 dlossA:0.6000 dlossQ:1.3123 exploreP:0.1750\n",
      "Episode:328 meanR:91.1000 R:111.0000 rate:0.2220 gloss:0.4928 dlossA:0.5895 dlossQ:1.3802 exploreP:0.1732\n",
      "Episode:329 meanR:91.5000 R:122.0000 rate:0.2440 gloss:0.5040 dlossA:0.5925 dlossQ:1.3903 exploreP:0.1712\n",
      "Episode:330 meanR:91.7700 R:105.0000 rate:0.2100 gloss:0.5056 dlossA:0.5950 dlossQ:1.3382 exploreP:0.1695\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        idx_arr = np.arange(memory_size// batch_size)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            percentage = 0.9\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            states = states[rates >= (np.max(rates)*percentage)]\n",
    "            actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "            next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "            dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "            rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eps, arr = np.array(episode_rewards_list).T\n",
    "# smoothed_arr = running_mean(arr, 10)\n",
    "# plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "# plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
