{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for BipedalWalker\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.11.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('BipedalWalker-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(24,), Box(4,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: 0.99992806 -0.9998551\n",
      "actions: 2.253050168355306 -2.222696304321289\n",
      "rewards: 2.253050168355306 -2.222696304321289\n"
     ]
    }
   ],
   "source": [
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1.], dtype=float32),\n",
       " array([-1., -1., -1., -1.], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high, env.action_space.low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_shape], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    training = tf.placeholder(tf.bool, [], name='training')\n",
    "    return states, actions, targetQs, rewards, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        predictions = tf.nn.tanh(logits) # [-1, +1]\n",
    "\n",
    "        # return actions logits\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DDPG\n",
    "# def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "#     actions_preds = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "#     gQs = discriminator(actions=actions_preds, hidden_size=hidden_size, states=states) # nextQs/targetQs\n",
    "#     gloss = -tf.reduce_mean(gQs)\n",
    "#     dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "#     targetQs = tf.reshape(targetQs, shape=[-1, 1]) # gQs\n",
    "#     dloss = tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "#     rates = tf.reshape(rates, shape=[-1, 1]) # [-1,+1]\n",
    "#     dloss += tf.reduce_mean(tf.square(tf.nn.tanh(dQs) - rates)) # DQN\n",
    "#     return actions_preds, gQs, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adverseial Q-learning\n",
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rewards, training):\n",
    "    actions_preds = generator(states=states, hidden_size=hidden_size, action_size=action_size, training=training)\n",
    "    gQs = discriminator(actions=actions_preds, hidden_size=hidden_size, states=states, training=training) #nextQs\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, training=training, reuse=True)#Qs\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1]) # gQs\n",
    "    gloss = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dloss = tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dloss += tf.reduce_mean(tf.square(dQs - rewards)) # DQN\n",
    "    return actions_preds, gQs, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rewards, self.training = model_input(\n",
    "            state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_preds, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            rewards=self.rewards, training=self.training) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, \n",
    "                                           d_loss=self.d_loss,\n",
    "                                           g_learning_rate=g_learning_rate, \n",
    "                                           d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(24,), Box(4,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 24\n",
    "action_size = 4\n",
    "hidden_size = 24*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size == one episode size is 1000/int(1e3) steps\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1.], dtype=float32),\n",
       " array([-1., -1., -1., -1.], dtype=float32),\n",
       " (4,),\n",
       " (-inf, inf),\n",
       " Box(4,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high, env.action_space.low, env.action_space.shape, \\\n",
    "env.reward_range, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: 0.99992806 -0.9998551\n",
      "actions: 2.253050168355306 -2.222696304321289\n",
      "rewards: 2.253050168355306 -2.222696304321289\n"
     ]
    }
   ],
   "source": [
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.01599\n",
      "Progress: 0.01939\n",
      "Progress: 0.02001\n",
      "Progress: 0.02127\n",
      "Progress: 0.02207\n",
      "Progress: 0.03807\n",
      "Progress: 0.03923\n",
      "Progress: 0.05523\n",
      "Progress: 0.05646\n",
      "Progress: 0.05746\n",
      "Progress: 0.05838\n",
      "Progress: 0.05911\n",
      "Progress: 0.05992\n",
      "Progress: 0.07592\n",
      "Progress: 0.07651\n",
      "Progress: 0.09251\n",
      "Progress: 0.09308\n",
      "Progress: 0.09393\n",
      "Progress: 0.09488\n",
      "Progress: 0.11088\n",
      "Progress: 0.11204\n",
      "Progress: 0.12804\n",
      "Progress: 0.12857\n",
      "Progress: 0.12929\n",
      "Progress: 0.14529\n",
      "Progress: 0.14582\n",
      "Progress: 0.14684\n",
      "Progress: 0.16284\n",
      "Progress: 0.17884\n",
      "Progress: 0.17987\n",
      "Progress: 0.18092\n",
      "Progress: 0.18146\n",
      "Progress: 0.18221\n",
      "Progress: 0.18293\n",
      "Progress: 0.18375\n",
      "Progress: 0.18456\n",
      "Progress: 0.18524\n",
      "Progress: 0.18593\n",
      "Progress: 0.20193\n",
      "Progress: 0.21793\n",
      "Progress: 0.21854\n",
      "Progress: 0.21926\n",
      "Progress: 0.23526\n",
      "Progress: 0.23599\n",
      "Progress: 0.23719\n",
      "Progress: 0.23806\n",
      "Progress: 0.23891\n",
      "Progress: 0.25491\n",
      "Progress: 0.25597\n",
      "Progress: 0.25699\n",
      "Progress: 0.25795\n",
      "Progress: 0.25854\n",
      "Progress: 0.25964\n",
      "Progress: 0.26024\n",
      "Progress: 0.26098\n",
      "Progress: 0.26149\n",
      "Progress: 0.26225\n",
      "Progress: 0.26307\n",
      "Progress: 0.27907\n",
      "Progress: 0.2797\n",
      "Progress: 0.2957\n",
      "Progress: 0.2965\n",
      "Progress: 0.3125\n",
      "Progress: 0.3285\n",
      "Progress: 0.3445\n",
      "Progress: 0.3605\n",
      "Progress: 0.36143\n",
      "Progress: 0.36216\n",
      "Progress: 0.36295\n",
      "Progress: 0.36429\n",
      "Progress: 0.38029\n",
      "Progress: 0.39629\n",
      "Progress: 0.39741\n",
      "Progress: 0.39818\n",
      "Progress: 0.39916\n",
      "Progress: 0.40028\n",
      "Progress: 0.41628\n",
      "Progress: 0.41732\n",
      "Progress: 0.41815\n",
      "Progress: 0.41916\n",
      "Progress: 0.43516\n",
      "Progress: 0.43593\n",
      "Progress: 0.45193\n",
      "Progress: 0.45245\n",
      "Progress: 0.46845\n",
      "Progress: 0.46907\n",
      "Progress: 0.4697\n",
      "Progress: 0.4857\n",
      "Progress: 0.5017\n",
      "Progress: 0.5177\n",
      "Progress: 0.5337\n",
      "Progress: 0.53463\n",
      "Progress: 0.55063\n",
      "Progress: 0.56663\n",
      "Progress: 0.56733\n",
      "Progress: 0.56779\n",
      "Progress: 0.56836\n",
      "Progress: 0.58436\n",
      "Progress: 0.60036\n",
      "Progress: 0.61636\n",
      "Progress: 0.61693\n",
      "Progress: 0.61757\n",
      "Progress: 0.61855\n",
      "Progress: 0.61925\n",
      "Progress: 0.63525\n",
      "Progress: 0.63631\n",
      "Progress: 0.65231\n",
      "Progress: 0.65278\n",
      "Progress: 0.65354\n",
      "Progress: 0.66954\n",
      "Progress: 0.68554\n",
      "Progress: 0.68627\n",
      "Progress: 0.70227\n",
      "Progress: 0.70315\n",
      "Progress: 0.70376\n",
      "Progress: 0.70442\n",
      "Progress: 0.72042\n",
      "Progress: 0.72092\n",
      "Progress: 0.72147\n",
      "Progress: 0.73747\n",
      "Progress: 0.73812\n",
      "Progress: 0.73884\n",
      "Progress: 0.73956\n",
      "Progress: 0.74031\n",
      "Progress: 0.74131\n",
      "Progress: 0.74197\n",
      "Progress: 0.74319\n",
      "Progress: 0.744\n",
      "Progress: 0.76\n",
      "Progress: 0.776\n",
      "Progress: 0.792\n",
      "Progress: 0.808\n",
      "Progress: 0.80889\n",
      "Progress: 0.82489\n",
      "Progress: 0.82597\n",
      "Progress: 0.82679\n",
      "Progress: 0.82792\n",
      "Progress: 0.84392\n",
      "Progress: 0.84475\n",
      "Progress: 0.86075\n",
      "Progress: 0.87675\n",
      "Progress: 0.87756\n",
      "Progress: 0.87819\n",
      "Progress: 0.87892\n",
      "Progress: 0.87964\n",
      "Progress: 0.89564\n",
      "Progress: 0.89685\n",
      "Progress: 0.89753\n",
      "Progress: 0.89843\n",
      "Progress: 0.91443\n",
      "Progress: 0.93043\n",
      "Progress: 0.9315\n",
      "Progress: 0.93244\n",
      "Progress: 0.94844\n",
      "Progress: 0.96444\n",
      "Progress: 0.96542\n",
      "Progress: 0.96601\n",
      "Progress: 0.96667\n",
      "Progress: 0.98267\n",
      "Progress: 0.99867\n",
      "Progress: 0.9994\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each_step in range(memory_size):\n",
    "    action = env.action_space.sample() # randomness\n",
    "    action = np.clip(action, -1, 1) # clipped: [-1, +1]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rate = -1 # success rate: [-1, +1]\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory updated\n",
    "    total_reward += reward # max reward 300\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        print('Progress:', each_step/memory_size)\n",
    "        state = env.reset()\n",
    "        # Best 100-episode average reward was 220.62 Â± 0.69. \n",
    "        # (BipedalWalker-v2 is considered \"solved\" \n",
    "        #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "        rate = total_reward/300\n",
    "        rate = np.clip(rate, -1, 1) \n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-121.5387 R:-121.5387 gloss:15.0686 dloss:30.9303\n",
      "Episode:1 meanR:-112.5238 R:-103.5088 gloss:10.5824 dloss:21.4712\n",
      "Episode:2 meanR:-110.7832 R:-107.3021 gloss:14.3477 dloss:29.0787\n",
      "Episode:3 meanR:-109.5724 R:-105.9397 gloss:11.9405 dloss:24.1308\n",
      "Episode:4 meanR:-116.9562 R:-146.4917 gloss:10.7301 dloss:21.7678\n",
      "Episode:5 meanR:-116.0979 R:-111.8065 gloss:8.9250 dloss:18.1928\n",
      "Episode:6 meanR:-114.9556 R:-108.1016 gloss:6.2803 dloss:12.7687\n",
      "Episode:7 meanR:-113.9792 R:-107.1444 gloss:12.3230 dloss:24.9873\n",
      "Episode:8 meanR:-120.4068 R:-171.8278 gloss:10.3868 dloss:21.1026\n",
      "Episode:9 meanR:-118.8849 R:-105.1880 gloss:8.8043 dloss:17.9101\n",
      "Episode:10 meanR:-117.4303 R:-102.8840 gloss:14.8935 dloss:30.2764\n",
      "Episode:11 meanR:-116.5814 R:-107.2432 gloss:9.6568 dloss:19.6784\n",
      "Episode:12 meanR:-115.4829 R:-102.3006 gloss:13.1001 dloss:26.6146\n",
      "Episode:13 meanR:-114.7152 R:-104.7349 gloss:10.8661 dloss:22.1534\n",
      "Episode:14 meanR:-114.0127 R:-104.1782 gloss:5.7906 dloss:11.7912\n",
      "Episode:15 meanR:-113.3667 R:-103.6769 gloss:9.7176 dloss:19.7474\n",
      "Episode:16 meanR:-112.7342 R:-102.6140 gloss:16.3367 dloss:33.2388\n",
      "Episode:17 meanR:-112.1476 R:-102.1764 gloss:6.6809 dloss:13.6642\n",
      "Episode:18 meanR:-111.6550 R:-102.7874 gloss:16.6909 dloss:34.0768\n",
      "Episode:19 meanR:-111.0863 R:-100.2804 gloss:19.1316 dloss:39.2015\n",
      "Episode:20 meanR:-110.6644 R:-102.2267 gloss:9.9526 dloss:20.4037\n",
      "Episode:21 meanR:-110.3463 R:-103.6674 gloss:15.9888 dloss:32.6842\n",
      "Episode:22 meanR:-110.0269 R:-102.9980 gloss:11.9934 dloss:24.5079\n",
      "Episode:23 meanR:-109.7264 R:-102.8166 gloss:7.6950 dloss:15.7820\n",
      "Episode:24 meanR:-109.3876 R:-101.2547 gloss:4.3044 dloss:8.7682\n",
      "Episode:25 meanR:-109.0953 R:-101.7878 gloss:15.9733 dloss:32.6108\n",
      "Episode:26 meanR:-108.8956 R:-103.7045 gloss:26.1482 dloss:53.6732\n",
      "Episode:27 meanR:-108.6718 R:-102.6278 gloss:6.8198 dloss:14.0920\n",
      "Episode:28 meanR:-108.4975 R:-103.6178 gloss:14.1166 dloss:28.9591\n",
      "Episode:29 meanR:-108.2942 R:-102.3987 gloss:8.7241 dloss:17.8548\n",
      "Episode:30 meanR:-108.1522 R:-103.8927 gloss:10.7634 dloss:22.2256\n",
      "Episode:31 meanR:-108.0100 R:-103.6017 gloss:9.5748 dloss:19.6699\n",
      "Episode:32 meanR:-107.9082 R:-104.6497 gloss:18.9985 dloss:39.1658\n",
      "Episode:33 meanR:-107.7404 R:-102.2050 gloss:11.8217 dloss:24.4759\n",
      "Episode:34 meanR:-107.6159 R:-103.3820 gloss:20.2727 dloss:41.8697\n",
      "Episode:35 meanR:-107.4896 R:-103.0707 gloss:13.5196 dloss:27.9913\n",
      "Episode:36 meanR:-107.4150 R:-104.7264 gloss:16.6082 dloss:34.3814\n",
      "Episode:37 meanR:-107.3261 R:-104.0372 gloss:9.2108 dloss:19.0398\n",
      "Episode:38 meanR:-107.1292 R:-99.6483 gloss:8.2119 dloss:16.9606\n",
      "Episode:39 meanR:-107.0264 R:-103.0153 gloss:9.5319 dloss:19.6828\n",
      "Episode:40 meanR:-106.8412 R:-99.4352 gloss:8.2123 dloss:16.9853\n",
      "Episode:41 meanR:-106.6901 R:-100.4960 gloss:9.0365 dloss:18.7140\n",
      "Episode:42 meanR:-106.5072 R:-98.8224 gloss:6.5897 dloss:13.6393\n",
      "Episode:43 meanR:-106.3640 R:-100.2069 gloss:10.2116 dloss:21.0369\n",
      "Episode:44 meanR:-106.2043 R:-99.1785 gloss:12.7364 dloss:26.3370\n",
      "Episode:45 meanR:-106.0587 R:-99.5047 gloss:6.8436 dloss:14.1089\n",
      "Episode:46 meanR:-105.9027 R:-98.7268 gloss:15.5193 dloss:32.0566\n",
      "Episode:47 meanR:-105.7594 R:-99.0270 gloss:12.0727 dloss:24.9813\n",
      "Episode:48 meanR:-105.6354 R:-99.6810 gloss:13.6535 dloss:28.0604\n",
      "Episode:49 meanR:-105.5026 R:-98.9979 gloss:12.7188 dloss:26.3215\n",
      "Episode:50 meanR:-105.3627 R:-98.3658 gloss:14.4187 dloss:29.8830\n",
      "Episode:51 meanR:-105.2290 R:-98.4092 gloss:7.7804 dloss:16.1862\n",
      "Episode:52 meanR:-105.1024 R:-98.5204 gloss:15.1401 dloss:31.5430\n",
      "Episode:53 meanR:-104.9904 R:-99.0557 gloss:9.6536 dloss:20.0436\n",
      "Episode:54 meanR:-104.8991 R:-99.9709 gloss:10.1815 dloss:21.1736\n",
      "Episode:55 meanR:-104.7851 R:-98.5149 gloss:9.3851 dloss:19.6428\n",
      "Episode:56 meanR:-104.6715 R:-98.3064 gloss:10.6235 dloss:22.1788\n",
      "Episode:57 meanR:-104.5659 R:-98.5477 gloss:14.6442 dloss:30.5982\n",
      "Episode:58 meanR:-104.4569 R:-98.1340 gloss:12.2605 dloss:25.6346\n",
      "Episode:59 meanR:-104.3642 R:-98.8956 gloss:11.5950 dloss:24.2405\n",
      "Episode:60 meanR:-104.2838 R:-99.4574 gloss:16.5970 dloss:34.7336\n",
      "Episode:61 meanR:-104.2015 R:-99.1862 gloss:16.0765 dloss:33.9487\n",
      "Episode:62 meanR:-104.2453 R:-106.9609 gloss:12.3728 dloss:26.2036\n",
      "Episode:63 meanR:-104.2573 R:-105.0089 gloss:14.2532 dloss:29.8476\n",
      "Episode:64 meanR:-104.2203 R:-101.8572 gloss:9.2212 dloss:19.6331\n",
      "Episode:65 meanR:-104.3893 R:-115.3729 gloss:12.4447 dloss:26.0305\n",
      "Episode:66 meanR:-104.4096 R:-105.7472 gloss:15.0679 dloss:31.9488\n",
      "Episode:67 meanR:-104.5548 R:-114.2867 gloss:12.1305 dloss:25.6023\n",
      "Episode:68 meanR:-104.5724 R:-105.7647 gloss:8.5960 dloss:18.1350\n",
      "Episode:69 meanR:-104.7313 R:-115.6996 gloss:10.2423 dloss:21.5943\n",
      "Episode:70 meanR:-104.9132 R:-117.6423 gloss:18.4721 dloss:39.2537\n",
      "Episode:71 meanR:-105.0811 R:-117.0047 gloss:14.3757 dloss:30.7284\n",
      "Episode:72 meanR:-105.1629 R:-111.0517 gloss:16.8982 dloss:36.1353\n",
      "Episode:73 meanR:-105.2078 R:-108.4856 gloss:8.1236 dloss:17.5478\n",
      "Episode:74 meanR:-105.2424 R:-107.8032 gloss:15.1193 dloss:32.7230\n",
      "Episode:75 meanR:-105.3132 R:-110.6255 gloss:19.1679 dloss:41.6584\n",
      "Episode:76 meanR:-105.3150 R:-105.4490 gloss:16.8151 dloss:36.2922\n",
      "Episode:77 meanR:-105.3332 R:-106.7335 gloss:17.2270 dloss:36.5339\n",
      "Episode:78 meanR:-105.3571 R:-107.2186 gloss:21.3019 dloss:45.9720\n",
      "Episode:79 meanR:-105.6967 R:-132.5316 gloss:13.4626 dloss:29.1890\n",
      "Episode:80 meanR:-105.8823 R:-120.7258 gloss:15.9689 dloss:34.6545\n",
      "Episode:81 meanR:-105.8830 R:-105.9389 gloss:15.5722 dloss:33.7320\n",
      "Episode:82 meanR:-106.2741 R:-138.3496 gloss:12.3078 dloss:26.6783\n",
      "Episode:83 meanR:-106.2813 R:-106.8791 gloss:17.4587 dloss:37.4534\n",
      "Episode:84 meanR:-106.2770 R:-105.9169 gloss:20.2631 dloss:43.8812\n",
      "Episode:85 meanR:-106.2437 R:-103.4113 gloss:19.5836 dloss:42.9455\n",
      "Episode:86 meanR:-106.2447 R:-106.3303 gloss:10.7358 dloss:23.1842\n",
      "Episode:87 meanR:-106.2418 R:-105.9857 gloss:17.4715 dloss:38.0815\n",
      "Episode:88 meanR:-106.2390 R:-105.9906 gloss:11.6964 dloss:25.3658\n",
      "Episode:89 meanR:-106.2366 R:-106.0275 gloss:12.2516 dloss:26.7450\n",
      "Episode:90 meanR:-106.2303 R:-105.6586 gloss:5.5759 dloss:12.0649\n",
      "Episode:91 meanR:-106.2277 R:-105.9936 gloss:18.9531 dloss:41.3188\n",
      "Episode:92 meanR:-106.2213 R:-105.6383 gloss:16.6748 dloss:36.2140\n",
      "Episode:93 meanR:-106.2047 R:-104.6601 gloss:17.7256 dloss:38.9330\n",
      "Episode:94 meanR:-106.2049 R:-106.2202 gloss:10.4917 dloss:23.2566\n",
      "Episode:95 meanR:-106.1789 R:-103.7113 gloss:8.7817 dloss:18.9426\n",
      "Episode:96 meanR:-106.1587 R:-104.2173 gloss:9.4445 dloss:21.1764\n",
      "Episode:97 meanR:-106.1380 R:-104.1326 gloss:19.5738 dloss:42.5109\n",
      "Episode:98 meanR:-106.5416 R:-146.0939 gloss:18.8914 dloss:41.7243\n",
      "Episode:99 meanR:-106.9607 R:-148.4529 gloss:17.7910 dloss:39.2695\n",
      "Episode:100 meanR:-107.1534 R:-140.8079 gloss:19.9850 dloss:45.0992\n",
      "Episode:101 meanR:-107.1754 R:-105.7072 gloss:12.7967 dloss:28.4808\n",
      "Episode:102 meanR:-107.1711 R:-106.8730 gloss:16.6967 dloss:37.3677\n",
      "Episode:103 meanR:-107.5008 R:-138.9061 gloss:18.8818 dloss:42.5756\n",
      "Episode:104 meanR:-108.1561 R:-212.0242 gloss:16.9211 dloss:38.1272\n",
      "Episode:105 meanR:-108.4696 R:-143.1546 gloss:15.8547 dloss:36.3507\n",
      "Episode:106 meanR:-108.8213 R:-143.2771 gloss:15.0765 dloss:34.0311\n",
      "Episode:107 meanR:-109.1124 R:-136.2543 gloss:20.3946 dloss:46.3725\n",
      "Episode:108 meanR:-108.4401 R:-104.5941 gloss:20.3420 dloss:46.4623\n",
      "Episode:109 meanR:-108.7154 R:-132.7186 gloss:15.8726 dloss:36.2256\n",
      "Episode:110 meanR:-109.1110 R:-142.4425 gloss:13.6090 dloss:31.2659\n",
      "Episode:111 meanR:-109.3544 R:-131.5867 gloss:14.9073 dloss:34.3612\n",
      "Episode:112 meanR:-109.7427 R:-141.1256 gloss:16.0710 dloss:36.9234\n",
      "Episode:113 meanR:-110.1295 R:-143.4213 gloss:20.3692 dloss:47.1208\n",
      "Episode:114 meanR:-110.4331 R:-134.5389 gloss:21.4900 dloss:50.1136\n",
      "Episode:115 meanR:-110.7732 R:-137.6847 gloss:14.2618 dloss:33.3274\n",
      "Episode:116 meanR:-111.1177 R:-137.0615 gloss:19.3195 dloss:45.0562\n",
      "Episode:117 meanR:-111.4759 R:-137.9963 gloss:20.8531 dloss:49.2252\n",
      "Episode:118 meanR:-111.8278 R:-137.9810 gloss:12.0953 dloss:28.5123\n",
      "Episode:119 meanR:-112.2597 R:-143.4633 gloss:17.7727 dloss:41.4331\n",
      "Episode:120 meanR:-112.6601 R:-142.2689 gloss:22.2725 dloss:52.5477\n",
      "Episode:121 meanR:-112.7008 R:-107.7362 gloss:10.0957 dloss:24.2894\n",
      "Episode:122 meanR:-112.7735 R:-110.2679 gloss:16.3050 dloss:38.8662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:123 meanR:-113.0862 R:-134.0903 gloss:18.5711 dloss:43.8839\n",
      "Episode:124 meanR:-113.4373 R:-136.3668 gloss:23.3709 dloss:55.5921\n",
      "Episode:125 meanR:-113.7528 R:-133.3325 gloss:10.7791 dloss:26.0888\n",
      "Episode:126 meanR:-114.0398 R:-132.4066 gloss:27.2516 dloss:64.7860\n",
      "Episode:127 meanR:-114.3406 R:-132.7044 gloss:22.0260 dloss:53.0313\n",
      "Episode:128 meanR:-114.3854 R:-108.1004 gloss:26.0584 dloss:63.4180\n",
      "Episode:129 meanR:-114.6957 R:-133.4283 gloss:12.2271 dloss:30.0559\n",
      "Episode:130 meanR:-115.0187 R:-136.1895 gloss:12.1583 dloss:29.3942\n",
      "Episode:131 meanR:-115.3080 R:-132.5337 gloss:23.4739 dloss:55.9023\n",
      "Episode:132 meanR:-115.5839 R:-132.2448 gloss:20.1185 dloss:48.8712\n",
      "Episode:133 meanR:-115.9085 R:-134.6576 gloss:17.6676 dloss:43.2666\n",
      "Episode:134 meanR:-116.1947 R:-132.0057 gloss:14.2514 dloss:34.4620\n",
      "Episode:135 meanR:-116.4841 R:-132.0100 gloss:13.1595 dloss:31.7312\n",
      "Episode:136 meanR:-116.7493 R:-131.2472 gloss:15.1882 dloss:36.2729\n",
      "Episode:137 meanR:-117.0121 R:-130.3144 gloss:21.6011 dloss:52.4940\n",
      "Episode:138 meanR:-117.0995 R:-108.3951 gloss:5.4368 dloss:13.5369\n",
      "Episode:139 meanR:-117.3408 R:-127.1432 gloss:12.1644 dloss:30.2691\n",
      "Episode:140 meanR:-117.6620 R:-131.5503 gloss:26.7296 dloss:64.4704\n",
      "Episode:141 meanR:-117.9849 R:-132.7859 gloss:21.3030 dloss:52.4148\n",
      "Episode:142 meanR:-118.2252 R:-122.8579 gloss:19.3090 dloss:48.3569\n",
      "Episode:143 meanR:-118.5478 R:-132.4610 gloss:13.5745 dloss:34.1429\n",
      "Episode:144 meanR:-118.9235 R:-136.7520 gloss:18.3441 dloss:45.5588\n",
      "Episode:145 meanR:-119.2626 R:-133.4169 gloss:15.7080 dloss:38.2423\n",
      "Episode:146 meanR:-119.6129 R:-133.7570 gloss:11.7303 dloss:29.3578\n",
      "Episode:147 meanR:-119.9474 R:-132.4756 gloss:19.2146 dloss:46.6592\n",
      "Episode:148 meanR:-120.2776 R:-132.6996 gloss:16.7699 dloss:42.3571\n",
      "Episode:149 meanR:-120.5807 R:-129.3114 gloss:17.9034 dloss:44.5267\n",
      "Episode:150 meanR:-120.8950 R:-129.7958 gloss:21.9523 dloss:55.2854\n",
      "Episode:151 meanR:-121.1605 R:-124.9551 gloss:15.4268 dloss:38.1230\n",
      "Episode:152 meanR:-121.5013 R:-132.6045 gloss:16.6281 dloss:41.2409\n",
      "Episode:153 meanR:-121.7769 R:-126.6114 gloss:22.3830 dloss:55.7125\n",
      "Episode:154 meanR:-122.0307 R:-125.3511 gloss:16.9185 dloss:41.7566\n",
      "Episode:155 meanR:-122.2731 R:-122.7614 gloss:13.4170 dloss:34.0825\n",
      "Episode:156 meanR:-122.6208 R:-133.0674 gloss:13.9213 dloss:34.8373\n",
      "Episode:157 meanR:-122.8587 R:-122.3426 gloss:18.2026 dloss:46.8876\n",
      "Episode:158 meanR:-123.1731 R:-129.5698 gloss:9.7283 dloss:24.8642\n",
      "Episode:159 meanR:-123.5158 R:-133.1675 gloss:16.9464 dloss:43.5970\n",
      "Episode:160 meanR:-123.7154 R:-119.4242 gloss:20.7538 dloss:51.8916\n",
      "Episode:161 meanR:-123.9749 R:-125.1351 gloss:18.1229 dloss:46.1688\n",
      "Episode:162 meanR:-124.1570 R:-125.1653 gloss:17.9622 dloss:46.2696\n",
      "Episode:163 meanR:-124.3571 R:-125.0210 gloss:12.9649 dloss:32.0019\n",
      "Episode:164 meanR:-124.5887 R:-125.0193 gloss:17.8642 dloss:45.5534\n",
      "Episode:165 meanR:-124.6945 R:-125.9491 gloss:23.7345 dloss:60.4990\n",
      "Episode:166 meanR:-124.9069 R:-126.9847 gloss:16.0315 dloss:42.4522\n",
      "Episode:167 meanR:-124.9481 R:-118.4097 gloss:9.2582 dloss:24.0264\n",
      "Episode:168 meanR:-125.1981 R:-130.7671 gloss:19.2136 dloss:48.4589\n",
      "Episode:169 meanR:-125.2750 R:-123.3844 gloss:20.7026 dloss:53.4862\n",
      "Episode:170 meanR:-125.3979 R:-129.9320 gloss:17.6029 dloss:45.0878\n",
      "Episode:171 meanR:-125.4777 R:-124.9840 gloss:23.2647 dloss:60.0288\n",
      "Episode:172 meanR:-125.5815 R:-121.4368 gloss:17.7403 dloss:46.1492\n",
      "Episode:173 meanR:-125.8040 R:-130.7355 gloss:15.7139 dloss:40.7124\n",
      "Episode:174 meanR:-126.0665 R:-134.0562 gloss:22.7742 dloss:58.3190\n",
      "Episode:175 meanR:-126.2892 R:-132.8942 gloss:22.3782 dloss:58.1965\n",
      "Episode:176 meanR:-126.5203 R:-128.5566 gloss:14.9279 dloss:39.7602\n",
      "Episode:177 meanR:-126.5402 R:-108.7243 gloss:18.0745 dloss:48.9428\n",
      "Episode:178 meanR:-126.5497 R:-108.1663 gloss:12.7395 dloss:34.3799\n",
      "Episode:179 meanR:-126.4266 R:-120.2242 gloss:18.9130 dloss:50.8495\n",
      "Episode:180 meanR:-126.4252 R:-120.5837 gloss:15.1945 dloss:39.9063\n",
      "Episode:181 meanR:-126.6112 R:-124.5349 gloss:20.5323 dloss:54.7810\n",
      "Episode:182 meanR:-126.4555 R:-122.7833 gloss:18.8821 dloss:49.8515\n",
      "Episode:183 meanR:-126.7532 R:-136.6467 gloss:19.6730 dloss:53.0833\n",
      "Episode:184 meanR:-126.9022 R:-120.8224 gloss:14.6921 dloss:39.8134\n",
      "Episode:185 meanR:-127.0727 R:-120.4592 gloss:18.4405 dloss:49.8126\n",
      "Episode:186 meanR:-127.2398 R:-123.0416 gloss:21.1610 dloss:57.2812\n",
      "Episode:187 meanR:-127.3766 R:-119.6642 gloss:13.5371 dloss:36.5811\n",
      "Episode:188 meanR:-127.5079 R:-119.1197 gloss:28.7428 dloss:78.1813\n",
      "Episode:189 meanR:-127.6944 R:-124.6796 gloss:20.6667 dloss:56.0499\n",
      "Episode:190 meanR:-127.7774 R:-113.9590 gloss:24.8136 dloss:66.3598\n",
      "Episode:191 meanR:-127.9205 R:-120.3000 gloss:17.1566 dloss:47.9374\n",
      "Episode:192 meanR:-128.0012 R:-113.7100 gloss:7.8787 dloss:23.0682\n",
      "Episode:193 meanR:-128.1244 R:-116.9826 gloss:26.6685 dloss:71.1932\n",
      "Episode:194 meanR:-128.2493 R:-118.7041 gloss:14.7328 dloss:40.7430\n",
      "Episode:195 meanR:-128.4322 R:-122.0038 gloss:21.5178 dloss:58.2451\n",
      "Episode:196 meanR:-128.7250 R:-133.5011 gloss:16.7792 dloss:46.1334\n",
      "Episode:197 meanR:-128.9268 R:-124.3072 gloss:15.2351 dloss:41.6519\n",
      "Episode:198 meanR:-128.6867 R:-122.0916 gloss:21.5605 dloss:59.6048\n",
      "Episode:199 meanR:-128.4408 R:-123.8614 gloss:22.3925 dloss:61.2106\n",
      "Episode:200 meanR:-128.2803 R:-124.7545 gloss:23.4830 dloss:65.7247\n",
      "Episode:201 meanR:-128.4837 R:-126.0468 gloss:14.5265 dloss:40.9960\n",
      "Episode:202 meanR:-128.2851 R:-87.0092 gloss:18.4676 dloss:53.6802\n",
      "Episode:203 meanR:-127.8934 R:-99.7403 gloss:14.6612 dloss:44.6890\n",
      "Episode:204 meanR:-126.7716 R:-99.8489 gloss:15.5915 dloss:47.7215\n",
      "Episode:205 meanR:-126.3443 R:-100.4196 gloss:16.3479 dloss:49.7493\n",
      "Episode:206 meanR:-125.9041 R:-99.2560 gloss:18.0206 dloss:53.1809\n",
      "Episode:207 meanR:-125.5478 R:-100.6255 gloss:15.6877 dloss:47.4842\n",
      "Episode:208 meanR:-125.4906 R:-98.8752 gloss:14.5590 dloss:45.1040\n",
      "Episode:209 meanR:-125.1592 R:-99.5742 gloss:19.2965 dloss:57.5958\n",
      "Episode:210 meanR:-124.9896 R:-125.4875 gloss:16.7396 dloss:51.8625\n",
      "Episode:211 meanR:-124.9351 R:-126.1332 gloss:20.9253 dloss:64.8870\n",
      "Episode:212 meanR:-124.4865 R:-96.2664 gloss:12.8005 dloss:42.2297\n",
      "Episode:213 meanR:-124.3124 R:-126.0085 gloss:16.2720 dloss:50.4336\n",
      "Episode:214 meanR:-124.2323 R:-126.5340 gloss:20.7212 dloss:65.4627\n",
      "Episode:215 meanR:-123.8057 R:-95.0232 gloss:20.1431 dloss:63.8919\n",
      "Episode:216 meanR:-123.3569 R:-92.1855 gloss:15.2781 dloss:49.5978\n",
      "Episode:217 meanR:-122.9472 R:-97.0228 gloss:14.3660 dloss:45.0695\n",
      "Episode:218 meanR:-122.5481 R:-98.0682 gloss:14.5567 dloss:47.6231\n",
      "Episode:219 meanR:-122.1702 R:-105.6735 gloss:14.3158 dloss:48.1052\n",
      "Episode:220 meanR:-121.7717 R:-102.4260 gloss:15.1930 dloss:51.3415\n",
      "Episode:221 meanR:-121.6650 R:-97.0626 gloss:15.1699 dloss:50.4268\n",
      "Episode:222 meanR:-121.5922 R:-102.9830 gloss:12.0940 dloss:42.8249\n",
      "Episode:223 meanR:-121.2340 R:-98.2753 gloss:17.6366 dloss:61.1845\n",
      "Episode:224 meanR:-120.9046 R:-103.4247 gloss:19.3808 dloss:62.9500\n",
      "Episode:225 meanR:-120.5584 R:-98.7094 gloss:13.6682 dloss:46.7332\n",
      "Episode:226 meanR:-120.2596 R:-102.5297 gloss:17.5924 dloss:60.1648\n",
      "Episode:227 meanR:-119.9871 R:-105.4531 gloss:11.3487 dloss:39.9716\n",
      "Episode:228 meanR:-120.1749 R:-126.8779 gloss:15.3830 dloss:49.0869\n",
      "Episode:229 meanR:-119.8725 R:-103.1909 gloss:16.1540 dloss:52.6646\n",
      "Episode:230 meanR:-119.5468 R:-103.6223 gloss:20.5240 dloss:67.0134\n",
      "Episode:231 meanR:-119.2764 R:-105.4966 gloss:15.4656 dloss:54.8595\n",
      "Episode:232 meanR:-119.0073 R:-105.3308 gloss:19.9411 dloss:66.8011\n",
      "Episode:233 meanR:-118.6916 R:-103.0842 gloss:12.0675 dloss:42.8970\n",
      "Episode:234 meanR:-118.4092 R:-103.7688 gloss:15.1280 dloss:49.4240\n",
      "Episode:235 meanR:-118.1339 R:-104.4778 gloss:17.4351 dloss:59.1336\n",
      "Episode:236 meanR:-117.8039 R:-98.2507 gloss:16.1974 dloss:56.4326\n",
      "Episode:237 meanR:-117.5187 R:-101.7925 gloss:17.9654 dloss:60.1261\n",
      "Episode:238 meanR:-117.4193 R:-98.4543 gloss:12.7627 dloss:45.9490\n",
      "Episode:239 meanR:-117.1836 R:-103.5718 gloss:15.4968 dloss:55.0719\n",
      "Episode:240 meanR:-116.8404 R:-97.2339 gloss:12.7482 dloss:45.0416\n",
      "Episode:241 meanR:-116.5423 R:-102.9715 gloss:17.2845 dloss:59.1433\n",
      "Episode:242 meanR:-116.3436 R:-102.9899 gloss:20.8232 dloss:69.8012\n",
      "Episode:243 meanR:-116.0525 R:-103.3560 gloss:15.7397 dloss:55.1450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:244 meanR:-115.6661 R:-98.1094 gloss:16.1383 dloss:55.3515\n",
      "Episode:245 meanR:-115.3117 R:-97.9777 gloss:17.5843 dloss:59.8073\n",
      "Episode:246 meanR:-114.9600 R:-98.5891 gloss:14.4877 dloss:51.1116\n",
      "Episode:247 meanR:-114.6449 R:-100.9594 gloss:20.7569 dloss:74.1512\n",
      "Episode:248 meanR:-114.3674 R:-104.9520 gloss:12.7569 dloss:44.5850\n",
      "Episode:249 meanR:-114.0626 R:-98.8324 gloss:18.6215 dloss:64.5589\n",
      "Episode:250 meanR:-113.7985 R:-103.3901 gloss:13.2598 dloss:49.6411\n",
      "Episode:251 meanR:-113.5598 R:-101.0813 gloss:12.7820 dloss:47.0969\n",
      "Episode:252 meanR:-113.2736 R:-103.9842 gloss:23.5848 dloss:81.1778\n",
      "Episode:253 meanR:-113.0481 R:-104.0563 gloss:11.5423 dloss:44.1997\n",
      "Episode:254 meanR:-112.8378 R:-104.3241 gloss:14.9650 dloss:52.9399\n",
      "Episode:255 meanR:-112.5809 R:-97.0733 gloss:14.3504 dloss:49.8211\n",
      "Episode:256 meanR:-112.2016 R:-95.1385 gloss:19.0912 dloss:64.4395\n",
      "Episode:257 meanR:-112.0162 R:-103.8047 gloss:17.8694 dloss:62.0498\n",
      "Episode:258 meanR:-111.6769 R:-95.6320 gloss:15.4165 dloss:56.6731\n",
      "Episode:259 meanR:-111.3399 R:-99.4696 gloss:17.2711 dloss:61.9212\n",
      "Episode:260 meanR:-111.1123 R:-96.6681 gloss:16.8139 dloss:59.3682\n",
      "Episode:261 meanR:-110.8561 R:-99.5162 gloss:19.2947 dloss:65.9068\n",
      "Episode:262 meanR:-110.6218 R:-101.7371 gloss:13.8177 dloss:48.6288\n",
      "Episode:263 meanR:-110.3741 R:-100.2466 gloss:16.2755 dloss:61.9669\n",
      "Episode:264 meanR:-110.1197 R:-99.5756 gloss:19.5256 dloss:66.7751\n",
      "Episode:265 meanR:-109.8573 R:-99.7168 gloss:15.3954 dloss:55.1146\n",
      "Episode:266 meanR:-109.5848 R:-99.7332 gloss:15.5456 dloss:57.3982\n",
      "Episode:267 meanR:-109.3989 R:-99.8125 gloss:18.2468 dloss:67.1879\n",
      "Episode:268 meanR:-109.0974 R:-100.6171 gloss:17.2571 dloss:67.4440\n",
      "Episode:269 meanR:-108.8583 R:-99.4780 gloss:19.8041 dloss:70.6942\n",
      "Episode:270 meanR:-108.5645 R:-100.5493 gloss:12.6919 dloss:49.1804\n",
      "Episode:271 meanR:-108.3463 R:-103.1664 gloss:14.3247 dloss:57.0509\n",
      "Episode:272 meanR:-108.1269 R:-99.5008 gloss:13.9376 dloss:52.1743\n",
      "Episode:273 meanR:-107.8554 R:-103.5805 gloss:22.5461 dloss:78.9329\n",
      "Episode:274 meanR:-107.5201 R:-100.5317 gloss:15.6691 dloss:58.7737\n",
      "Episode:275 meanR:-107.2072 R:-101.6036 gloss:19.8450 dloss:70.3190\n",
      "Episode:276 meanR:-106.9212 R:-99.9495 gloss:15.0778 dloss:56.5467\n",
      "Episode:277 meanR:-106.8512 R:-101.7253 gloss:16.2155 dloss:65.2353\n",
      "Episode:278 meanR:-106.7752 R:-100.5709 gloss:15.9631 dloss:59.1143\n",
      "Episode:279 meanR:-106.5936 R:-102.0601 gloss:11.8661 dloss:49.9087\n",
      "Episode:280 meanR:-106.3926 R:-100.4857 gloss:13.6966 dloss:51.0264\n",
      "Episode:281 meanR:-106.1831 R:-103.5854 gloss:18.7195 dloss:69.4824\n",
      "Episode:282 meanR:-105.9933 R:-103.8002 gloss:16.3679 dloss:59.8707\n",
      "Episode:283 meanR:-105.6469 R:-102.0078 gloss:19.2876 dloss:69.2456\n",
      "Episode:284 meanR:-105.4437 R:-100.5072 gloss:23.5248 dloss:84.9893\n",
      "Episode:285 meanR:-105.2759 R:-103.6792 gloss:15.0911 dloss:55.8179\n",
      "Episode:286 meanR:-105.0649 R:-101.9363 gloss:19.0309 dloss:69.1880\n",
      "Episode:287 meanR:-104.8899 R:-102.1641 gloss:14.4593 dloss:55.5655\n",
      "Episode:288 meanR:-104.7048 R:-100.6176 gloss:17.9655 dloss:71.9799\n",
      "Episode:289 meanR:-104.4570 R:-99.8929 gloss:16.6573 dloss:59.4797\n",
      "Episode:290 meanR:-104.3187 R:-100.1319 gloss:13.3979 dloss:53.5365\n",
      "Episode:291 meanR:-104.1499 R:-103.4226 gloss:21.7994 dloss:78.4511\n",
      "Episode:292 meanR:-104.0569 R:-104.4043 gloss:9.8259 dloss:38.1270\n",
      "Episode:293 meanR:-103.9137 R:-102.6601 gloss:19.7547 dloss:70.8855\n",
      "Episode:294 meanR:-103.7361 R:-100.9502 gloss:14.0354 dloss:49.4492\n",
      "Episode:295 meanR:-103.5622 R:-104.6166 gloss:27.1695 dloss:92.8923\n",
      "Episode:296 meanR:-103.2710 R:-104.3819 gloss:19.3111 dloss:66.8220\n",
      "Episode:297 meanR:-103.0663 R:-103.8293 gloss:16.8040 dloss:65.8171\n",
      "Episode:298 meanR:-102.8815 R:-103.6188 gloss:22.3950 dloss:82.6006\n",
      "Episode:299 meanR:-102.6984 R:-105.5456 gloss:21.0919 dloss:74.1741\n",
      "Episode:300 meanR:-102.4735 R:-102.2707 gloss:15.6971 dloss:62.0040\n",
      "Episode:301 meanR:-102.2144 R:-100.1312 gloss:21.9759 dloss:77.5561\n",
      "Episode:302 meanR:-102.3529 R:-100.8564 gloss:14.0863 dloss:52.8983\n",
      "Episode:303 meanR:-102.3720 R:-101.6551 gloss:15.5830 dloss:59.5820\n",
      "Episode:304 meanR:-102.3910 R:-101.7452 gloss:16.0061 dloss:61.1957\n",
      "Episode:305 meanR:-102.4048 R:-101.8031 gloss:15.5682 dloss:54.5305\n",
      "Episode:306 meanR:-102.4215 R:-100.9282 gloss:19.3987 dloss:68.6946\n",
      "Episode:307 meanR:-102.4371 R:-102.1826 gloss:16.8818 dloss:62.9683\n",
      "Episode:308 meanR:-102.4565 R:-100.8137 gloss:13.5421 dloss:51.7731\n",
      "Episode:309 meanR:-102.4734 R:-101.2693 gloss:16.7861 dloss:60.6960\n",
      "Episode:310 meanR:-102.2388 R:-102.0195 gloss:21.1562 dloss:75.9959\n",
      "Episode:311 meanR:-101.9919 R:-101.4463 gloss:11.6026 dloss:43.9572\n",
      "Episode:312 meanR:-102.0821 R:-105.2913 gloss:16.0920 dloss:63.7412\n",
      "Episode:313 meanR:-101.8433 R:-102.1296 gloss:16.1339 dloss:61.5503\n",
      "Episode:314 meanR:-101.6386 R:-106.0554 gloss:15.2134 dloss:59.1931\n",
      "Episode:315 meanR:-101.7505 R:-106.2172 gloss:19.2173 dloss:66.7191\n",
      "Episode:316 meanR:-101.8375 R:-100.8810 gloss:22.4848 dloss:81.1071\n",
      "Episode:317 meanR:-101.8809 R:-101.3721 gloss:20.5326 dloss:80.6882\n",
      "Episode:318 meanR:-101.9042 R:-100.3889 gloss:19.2181 dloss:73.8391\n",
      "Episode:319 meanR:-101.8529 R:-100.5434 gloss:17.1418 dloss:62.3486\n",
      "Episode:320 meanR:-101.8408 R:-101.2197 gloss:13.0507 dloss:52.5188\n",
      "Episode:321 meanR:-101.8801 R:-100.9892 gloss:16.1888 dloss:63.9394\n",
      "Episode:322 meanR:-101.8599 R:-100.9712 gloss:22.8190 dloss:85.5961\n",
      "Episode:323 meanR:-101.8864 R:-100.9204 gloss:9.3075 dloss:41.9852\n",
      "Episode:324 meanR:-101.8600 R:-100.7823 gloss:10.5774 dloss:40.9083\n",
      "Episode:325 meanR:-101.8772 R:-100.4371 gloss:15.1292 dloss:53.6602\n",
      "Episode:326 meanR:-101.8675 R:-101.5555 gloss:16.7948 dloss:59.3223\n",
      "Episode:327 meanR:-101.8268 R:-101.3879 gloss:27.2209 dloss:95.4137\n",
      "Episode:328 meanR:-101.5693 R:-101.1202 gloss:11.6016 dloss:44.6706\n",
      "Episode:329 meanR:-101.5487 R:-101.1357 gloss:8.4809 dloss:33.2389\n",
      "Episode:330 meanR:-101.5269 R:-101.4372 gloss:14.5856 dloss:54.6755\n",
      "Episode:331 meanR:-101.5005 R:-102.8606 gloss:18.6215 dloss:68.9912\n",
      "Episode:332 meanR:-101.4931 R:-104.5888 gloss:19.7681 dloss:75.5347\n",
      "Episode:333 meanR:-101.4757 R:-101.3491 gloss:14.7431 dloss:54.3582\n",
      "Episode:334 meanR:-101.4938 R:-105.5782 gloss:17.6608 dloss:61.2310\n",
      "Episode:335 meanR:-101.5138 R:-106.4729 gloss:18.6438 dloss:69.3560\n",
      "Episode:336 meanR:-101.5865 R:-105.5231 gloss:11.8764 dloss:46.1189\n",
      "Episode:337 meanR:-101.6311 R:-106.2565 gloss:9.9907 dloss:39.4912\n",
      "Episode:338 meanR:-101.6586 R:-101.2004 gloss:14.6964 dloss:57.7172\n",
      "Episode:339 meanR:-101.6906 R:-106.7702 gloss:17.5133 dloss:68.2220\n",
      "Episode:340 meanR:-101.7411 R:-102.2851 gloss:17.1408 dloss:64.5304\n",
      "Episode:341 meanR:-101.7394 R:-102.8057 gloss:19.8209 dloss:72.5364\n",
      "Episode:342 meanR:-101.7336 R:-102.4045 gloss:18.3404 dloss:70.0056\n",
      "Episode:343 meanR:-101.7685 R:-106.8485 gloss:18.3544 dloss:69.9458\n",
      "Episode:344 meanR:-101.8463 R:-105.8926 gloss:12.6720 dloss:50.9415\n",
      "Episode:345 meanR:-101.9235 R:-105.6901 gloss:13.2374 dloss:53.0501\n",
      "Episode:346 meanR:-101.9949 R:-105.7311 gloss:20.8863 dloss:81.2525\n",
      "Episode:347 meanR:-102.0453 R:-106.0023 gloss:22.2246 dloss:86.0405\n",
      "Episode:348 meanR:-102.0056 R:-100.9751 gloss:16.9657 dloss:69.0550\n",
      "Episode:349 meanR:-102.0863 R:-106.9052 gloss:19.3964 dloss:73.3786\n",
      "Episode:350 meanR:-102.1154 R:-106.3068 gloss:17.8927 dloss:70.4319\n",
      "Episode:351 meanR:-102.1180 R:-101.3383 gloss:20.4836 dloss:80.2568\n",
      "Episode:352 meanR:-102.1377 R:-105.9547 gloss:22.8437 dloss:87.4151\n",
      "Episode:353 meanR:-102.1057 R:-100.8535 gloss:16.9009 dloss:66.8276\n",
      "Episode:354 meanR:-102.1197 R:-105.7232 gloss:21.6335 dloss:85.3495\n",
      "Episode:355 meanR:-102.1613 R:-101.2316 gloss:14.5110 dloss:59.4117\n",
      "Episode:356 meanR:-102.2719 R:-106.1979 gloss:19.8442 dloss:75.5822\n",
      "Episode:357 meanR:-102.2384 R:-100.4550 gloss:16.9666 dloss:67.1298\n",
      "Episode:358 meanR:-102.2874 R:-100.5371 gloss:17.5803 dloss:73.2399\n",
      "Episode:359 meanR:-102.2995 R:-100.6820 gloss:22.1046 dloss:85.0409\n",
      "Episode:360 meanR:-102.3431 R:-101.0270 gloss:21.4921 dloss:85.9829\n",
      "Episode:361 meanR:-102.3571 R:-100.9093 gloss:23.0518 dloss:90.8948\n",
      "Episode:362 meanR:-102.3701 R:-103.0383 gloss:24.0780 dloss:90.5036\n",
      "Episode:363 meanR:-102.3848 R:-101.7206 gloss:16.7980 dloss:65.2406\n",
      "Episode:364 meanR:-102.3995 R:-101.0457 gloss:19.6523 dloss:74.4864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:365 meanR:-102.4089 R:-100.6547 gloss:12.8687 dloss:52.1486\n",
      "Episode:366 meanR:-102.4207 R:-100.9175 gloss:23.6589 dloss:93.3959\n",
      "Episode:367 meanR:-102.4461 R:-102.3459 gloss:23.5891 dloss:90.9168\n",
      "Episode:368 meanR:-102.4496 R:-100.9734 gloss:19.3322 dloss:79.0731\n",
      "Episode:369 meanR:-102.4792 R:-102.4399 gloss:17.7022 dloss:74.3754\n",
      "Episode:370 meanR:-102.4832 R:-100.9421 gloss:19.8399 dloss:75.9188\n",
      "Episode:371 meanR:-102.5122 R:-106.0679 gloss:21.8593 dloss:85.1624\n",
      "Episode:372 meanR:-102.5269 R:-100.9744 gloss:19.3674 dloss:79.6624\n",
      "Episode:373 meanR:-102.5188 R:-102.7655 gloss:11.2538 dloss:47.9685\n",
      "Episode:374 meanR:-102.5802 R:-106.6720 gloss:19.4381 dloss:79.5245\n",
      "Episode:375 meanR:-102.6273 R:-106.3118 gloss:17.9349 dloss:80.3664\n",
      "Episode:376 meanR:-102.6957 R:-106.7977 gloss:20.8926 dloss:80.3797\n",
      "Episode:377 meanR:-102.6908 R:-101.2354 gloss:20.4885 dloss:80.1452\n",
      "Episode:378 meanR:-102.7480 R:-106.2860 gloss:16.6891 dloss:76.3107\n",
      "Episode:379 meanR:-102.7911 R:-106.3752 gloss:13.9555 dloss:55.0219\n",
      "Episode:380 meanR:-102.8006 R:-101.4356 gloss:16.8585 dloss:68.2642\n",
      "Episode:381 meanR:-102.8258 R:-106.1035 gloss:31.1249 dloss:112.1303\n",
      "Episode:382 meanR:-102.8014 R:-101.3587 gloss:22.8196 dloss:94.1686\n",
      "Episode:383 meanR:-102.8533 R:-107.1997 gloss:14.3084 dloss:66.5493\n",
      "Episode:384 meanR:-102.9092 R:-106.0916 gloss:20.8286 dloss:89.0329\n",
      "Episode:385 meanR:-102.8850 R:-101.2574 gloss:17.0579 dloss:73.8645\n",
      "Episode:386 meanR:-102.8802 R:-101.4607 gloss:14.3306 dloss:59.5467\n",
      "Episode:387 meanR:-102.8719 R:-101.3380 gloss:15.6982 dloss:62.3341\n",
      "Episode:388 meanR:-102.9244 R:-105.8623 gloss:16.7770 dloss:67.4707\n",
      "Episode:389 meanR:-102.9865 R:-106.1090 gloss:22.2264 dloss:86.2822\n",
      "Episode:390 meanR:-103.0475 R:-106.2321 gloss:15.8783 dloss:61.9578\n",
      "Episode:391 meanR:-103.0255 R:-101.2151 gloss:16.2307 dloss:68.2822\n",
      "Episode:392 meanR:-102.9962 R:-101.4817 gloss:18.5799 dloss:76.0323\n",
      "Episode:393 meanR:-102.9825 R:-101.2885 gloss:17.6380 dloss:69.6409\n",
      "Episode:394 meanR:-103.0348 R:-106.1820 gloss:16.9351 dloss:68.3859\n",
      "Episode:395 meanR:-103.0450 R:-105.6286 gloss:25.2112 dloss:95.0470\n",
      "Episode:396 meanR:-103.0127 R:-101.1556 gloss:23.2383 dloss:97.9668\n",
      "Episode:397 meanR:-102.9837 R:-100.9300 gloss:19.3165 dloss:76.2803\n",
      "Episode:398 meanR:-102.9543 R:-100.6789 gloss:26.2220 dloss:101.7916\n",
      "Episode:399 meanR:-102.9103 R:-101.1454 gloss:14.7797 dloss:67.0454\n",
      "Episode:400 meanR:-102.8936 R:-100.5986 gloss:16.3333 dloss:73.2243\n",
      "Episode:401 meanR:-102.8993 R:-100.7069 gloss:17.7147 dloss:70.0306\n",
      "Episode:402 meanR:-102.8999 R:-100.9113 gloss:12.5654 dloss:52.2594\n",
      "Episode:403 meanR:-102.8900 R:-100.6630 gloss:19.4463 dloss:78.2775\n",
      "Episode:404 meanR:-102.8867 R:-101.4184 gloss:25.2494 dloss:102.4053\n",
      "Episode:405 meanR:-102.8746 R:-100.5877 gloss:9.1958 dloss:43.6532\n",
      "Episode:406 meanR:-102.8760 R:-101.0719 gloss:23.9042 dloss:92.6041\n",
      "Episode:407 meanR:-102.8611 R:-100.6984 gloss:19.0232 dloss:79.0531\n",
      "Episode:408 meanR:-102.8764 R:-102.3378 gloss:20.9339 dloss:85.7683\n",
      "Episode:409 meanR:-102.8746 R:-101.0918 gloss:16.4582 dloss:68.8593\n",
      "Episode:410 meanR:-102.8745 R:-102.0048 gloss:22.5701 dloss:90.5640\n",
      "Episode:411 meanR:-102.8828 R:-102.2777 gloss:18.3085 dloss:72.8576\n",
      "Episode:412 meanR:-102.8387 R:-100.8783 gloss:23.0916 dloss:98.2230\n",
      "Episode:413 meanR:-102.8293 R:-101.1901 gloss:14.3878 dloss:60.0650\n",
      "Episode:414 meanR:-102.7758 R:-100.7083 gloss:21.0603 dloss:87.6990\n",
      "Episode:415 meanR:-102.7365 R:-102.2862 gloss:12.6059 dloss:58.1053\n",
      "Episode:416 meanR:-102.7390 R:-101.1322 gloss:20.7526 dloss:89.5170\n",
      "Episode:417 meanR:-102.7419 R:-101.6625 gloss:23.7639 dloss:92.4556\n",
      "Episode:418 meanR:-102.7449 R:-100.6856 gloss:16.3660 dloss:72.5101\n",
      "Episode:419 meanR:-102.7496 R:-101.0210 gloss:24.2747 dloss:94.2014\n",
      "Episode:420 meanR:-102.7532 R:-101.5759 gloss:20.4266 dloss:84.8833\n",
      "Episode:421 meanR:-103.1166 R:-137.3316 gloss:23.6833 dloss:94.7162\n",
      "Episode:422 meanR:-103.1216 R:-101.4655 gloss:20.2935 dloss:86.4740\n",
      "Episode:423 meanR:-103.1257 R:-101.3380 gloss:16.4579 dloss:76.7386\n",
      "Episode:424 meanR:-103.1298 R:-101.1837 gloss:16.1905 dloss:73.0899\n",
      "Episode:425 meanR:-103.4855 R:-136.0109 gloss:18.7133 dloss:78.0930\n",
      "Episode:426 meanR:-103.4854 R:-101.5464 gloss:27.1179 dloss:111.0397\n",
      "Episode:427 meanR:-103.5380 R:-106.6441 gloss:18.9886 dloss:85.2618\n",
      "Episode:428 meanR:-103.5469 R:-102.0112 gloss:17.8566 dloss:76.9549\n",
      "Episode:429 meanR:-103.5477 R:-101.2181 gloss:21.9238 dloss:89.3100\n",
      "Episode:430 meanR:-103.5427 R:-100.9421 gloss:20.4067 dloss:86.1620\n",
      "Episode:431 meanR:-103.5309 R:-101.6786 gloss:23.6111 dloss:94.5033\n",
      "Episode:432 meanR:-103.5030 R:-101.7942 gloss:24.4970 dloss:98.2960\n",
      "Episode:433 meanR:-103.5124 R:-102.2900 gloss:17.7492 dloss:76.3658\n",
      "Episode:434 meanR:-103.5440 R:-108.7433 gloss:13.3332 dloss:65.6840\n",
      "Episode:435 meanR:-103.4990 R:-101.9735 gloss:17.6132 dloss:72.6103\n",
      "Episode:436 meanR:-103.5145 R:-107.0698 gloss:26.3181 dloss:104.5049\n",
      "Episode:437 meanR:-103.4511 R:-99.9176 gloss:18.1053 dloss:76.3444\n",
      "Episode:438 meanR:-103.4605 R:-102.1398 gloss:16.3092 dloss:78.6804\n",
      "Episode:439 meanR:-103.4041 R:-101.1319 gloss:28.9569 dloss:112.9109\n",
      "Episode:440 meanR:-103.3897 R:-100.8408 gloss:14.2136 dloss:64.5333\n",
      "Episode:441 meanR:-103.3764 R:-101.4718 gloss:16.0454 dloss:72.6215\n",
      "Episode:442 meanR:-103.3762 R:-102.3933 gloss:12.7066 dloss:57.1410\n",
      "Episode:443 meanR:-103.3128 R:-100.5090 gloss:17.3938 dloss:73.7172\n",
      "Episode:444 meanR:-103.2682 R:-101.4270 gloss:28.3057 dloss:114.7824\n",
      "Episode:445 meanR:-103.2466 R:-103.5325 gloss:15.0365 dloss:71.9074\n",
      "Episode:446 meanR:-103.1969 R:-100.7628 gloss:17.1619 dloss:75.3671\n",
      "Episode:447 meanR:-103.1659 R:-102.9002 gloss:30.9163 dloss:124.0949\n",
      "Episode:448 meanR:-103.1891 R:-103.2909 gloss:25.5286 dloss:113.9551\n",
      "Episode:449 meanR:-103.1304 R:-101.0403 gloss:21.2008 dloss:91.1483\n",
      "Episode:450 meanR:-103.0734 R:-100.6020 gloss:23.8950 dloss:93.6513\n",
      "Episode:451 meanR:-103.0820 R:-102.2036 gloss:24.1166 dloss:97.9255\n",
      "Episode:452 meanR:-103.0299 R:-100.7395 gloss:29.2286 dloss:117.3416\n",
      "Episode:453 meanR:-103.0374 R:-101.6038 gloss:16.5996 dloss:76.3854\n",
      "Episode:454 meanR:-102.9970 R:-101.6896 gloss:17.3922 dloss:80.5752\n",
      "Episode:455 meanR:-103.0060 R:-102.1287 gloss:18.5918 dloss:77.6685\n",
      "Episode:456 meanR:-102.9736 R:-102.9597 gloss:23.4067 dloss:102.3376\n",
      "Episode:457 meanR:-102.9882 R:-101.9083 gloss:17.4318 dloss:79.3625\n",
      "Episode:458 meanR:-102.9945 R:-101.1687 gloss:23.1319 dloss:94.1729\n",
      "Episode:459 meanR:-102.9903 R:-100.2690 gloss:20.2438 dloss:87.5617\n",
      "Episode:460 meanR:-102.9946 R:-101.4480 gloss:17.4852 dloss:75.6613\n",
      "Episode:461 meanR:-103.0047 R:-101.9251 gloss:23.6980 dloss:97.6432\n",
      "Episode:462 meanR:-102.9959 R:-102.1538 gloss:21.5639 dloss:91.1226\n",
      "Episode:463 meanR:-102.9801 R:-100.1419 gloss:21.4829 dloss:87.6865\n",
      "Episode:464 meanR:-102.9725 R:-100.2839 gloss:21.4267 dloss:87.6393\n",
      "Episode:465 meanR:-102.9700 R:-100.4048 gloss:22.3838 dloss:92.7013\n",
      "Episode:466 meanR:-103.0147 R:-105.3948 gloss:28.1217 dloss:105.2431\n",
      "Episode:467 meanR:-103.0073 R:-101.6033 gloss:22.0638 dloss:95.2037\n",
      "Episode:468 meanR:-103.0601 R:-106.2528 gloss:14.3375 dloss:68.9114\n",
      "Episode:469 meanR:-103.0556 R:-101.9894 gloss:18.0237 dloss:77.6377\n",
      "Episode:470 meanR:-103.0906 R:-104.4442 gloss:29.2277 dloss:114.8907\n",
      "Episode:471 meanR:-103.0642 R:-103.4285 gloss:20.1370 dloss:81.5743\n",
      "Episode:472 meanR:-103.0744 R:-101.9912 gloss:22.5993 dloss:95.7463\n",
      "Episode:473 meanR:-103.0522 R:-100.5434 gloss:24.2666 dloss:110.5937\n",
      "Episode:474 meanR:-102.9908 R:-100.5315 gloss:20.1098 dloss:82.5933\n",
      "Episode:475 meanR:-102.9322 R:-100.4499 gloss:22.1057 dloss:100.6799\n",
      "Episode:476 meanR:-102.8801 R:-101.5879 gloss:19.1004 dloss:86.8428\n",
      "Episode:477 meanR:-102.8697 R:-100.2032 gloss:18.5039 dloss:87.9110\n",
      "Episode:478 meanR:-102.8127 R:-100.5857 gloss:19.8092 dloss:90.2376\n",
      "Episode:479 meanR:-102.7603 R:-101.1355 gloss:26.4184 dloss:115.5980\n",
      "Episode:480 meanR:-102.7480 R:-100.2001 gloss:24.3041 dloss:105.2119\n",
      "Episode:481 meanR:-102.6908 R:-100.3898 gloss:15.2430 dloss:70.4920\n",
      "Episode:482 meanR:-102.6934 R:-101.6130 gloss:23.7152 dloss:102.8332\n",
      "Episode:483 meanR:-102.6247 R:-100.3288 gloss:30.1900 dloss:122.0685\n",
      "Episode:484 meanR:-102.5739 R:-101.0156 gloss:19.2232 dloss:82.5886\n",
      "Episode:485 meanR:-102.5671 R:-100.5718 gloss:17.3903 dloss:69.1320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:486 meanR:-102.5569 R:-100.4406 gloss:19.9031 dloss:90.7032\n",
      "Episode:487 meanR:-102.5828 R:-103.9304 gloss:20.1763 dloss:82.9073\n",
      "Episode:488 meanR:-102.5245 R:-100.0372 gloss:25.6371 dloss:111.4434\n",
      "Episode:489 meanR:-102.4635 R:-100.0066 gloss:13.5566 dloss:63.9469\n",
      "Episode:490 meanR:-102.4169 R:-101.5687 gloss:29.3322 dloss:119.7818\n",
      "Episode:491 meanR:-102.4269 R:-102.2182 gloss:19.5041 dloss:80.7162\n",
      "Episode:492 meanR:-102.4208 R:-100.8687 gloss:26.3634 dloss:112.1206\n",
      "Episode:493 meanR:-102.4237 R:-101.5783 gloss:20.6156 dloss:87.7947\n",
      "Episode:494 meanR:-102.4264 R:-106.4553 gloss:27.5549 dloss:107.2141\n",
      "Episode:495 meanR:-102.3747 R:-100.4580 gloss:17.8665 dloss:82.2806\n",
      "Episode:496 meanR:-102.3735 R:-101.0353 gloss:28.3105 dloss:116.0817\n",
      "Episode:497 meanR:-102.4289 R:-106.4704 gloss:19.2517 dloss:92.6679\n",
      "Episode:498 meanR:-102.4548 R:-103.2671 gloss:27.3159 dloss:118.7784\n",
      "Episode:499 meanR:-102.4686 R:-102.5261 gloss:25.2461 dloss:105.8193\n",
      "Episode:500 meanR:-102.8718 R:-140.9148 gloss:22.7698 dloss:100.5978\n",
      "Episode:501 meanR:-102.9108 R:-104.6099 gloss:21.3111 dloss:99.1425\n",
      "Episode:502 meanR:-102.9346 R:-103.2887 gloss:16.9266 dloss:77.5199\n",
      "Episode:503 meanR:-102.9450 R:-101.7120 gloss:17.8480 dloss:78.6193\n",
      "Episode:504 meanR:-102.9503 R:-101.9391 gloss:19.8574 dloss:89.4296\n",
      "Episode:505 meanR:-102.9705 R:-102.6086 gloss:23.0617 dloss:103.0574\n",
      "Episode:506 meanR:-102.9779 R:-101.8195 gloss:26.3638 dloss:108.7896\n",
      "Episode:507 meanR:-102.9945 R:-102.3502 gloss:22.1509 dloss:97.9293\n",
      "Episode:508 meanR:-102.9801 R:-100.9064 gloss:22.0937 dloss:100.5321\n",
      "Episode:509 meanR:-103.0238 R:-105.4548 gloss:18.1643 dloss:81.3690\n",
      "Episode:510 meanR:-103.0138 R:-101.0121 gloss:18.5964 dloss:82.2348\n",
      "Episode:511 meanR:-103.0504 R:-105.9307 gloss:24.5681 dloss:99.0029\n",
      "Episode:512 meanR:-103.0535 R:-101.1921 gloss:16.8424 dloss:78.0055\n",
      "Episode:513 meanR:-103.0529 R:-101.1315 gloss:21.8745 dloss:100.6157\n",
      "Episode:514 meanR:-103.1204 R:-107.4583 gloss:18.8012 dloss:85.5765\n",
      "Episode:515 meanR:-103.1127 R:-101.5175 gloss:25.9664 dloss:112.1606\n",
      "Episode:516 meanR:-103.1230 R:-102.1619 gloss:32.0916 dloss:131.0541\n",
      "Episode:517 meanR:-103.1078 R:-100.1403 gloss:32.5796 dloss:138.7220\n",
      "Episode:518 meanR:-103.1060 R:-100.5005 gloss:23.1600 dloss:100.6034\n",
      "Episode:519 meanR:-103.1028 R:-100.7060 gloss:27.3062 dloss:125.8650\n",
      "Episode:520 meanR:-103.0976 R:-101.0566 gloss:21.6917 dloss:102.3114\n",
      "Episode:521 meanR:-102.7692 R:-104.4890 gloss:28.2446 dloss:121.5092\n",
      "Episode:522 meanR:-102.7638 R:-100.9234 gloss:23.5503 dloss:102.7988\n",
      "Episode:523 meanR:-102.7621 R:-101.1749 gloss:20.8550 dloss:88.6296\n",
      "Episode:524 meanR:-102.7548 R:-100.4458 gloss:18.8784 dloss:92.3913\n",
      "Episode:525 meanR:-102.4014 R:-100.6753 gloss:27.3566 dloss:114.3195\n",
      "Episode:526 meanR:-102.3893 R:-100.3393 gloss:19.7439 dloss:97.7547\n",
      "Episode:527 meanR:-102.3581 R:-103.5242 gloss:21.1019 dloss:96.0676\n",
      "Episode:528 meanR:-102.3725 R:-103.4450 gloss:21.2983 dloss:101.0156\n",
      "Episode:529 meanR:-102.3640 R:-100.3735 gloss:12.8650 dloss:65.0044\n",
      "Episode:530 meanR:-102.3604 R:-100.5831 gloss:19.7154 dloss:85.5892\n",
      "Episode:531 meanR:-102.3467 R:-100.3046 gloss:21.7806 dloss:101.0569\n",
      "Episode:532 meanR:-102.3380 R:-100.9275 gloss:29.3586 dloss:116.5563\n",
      "Episode:533 meanR:-102.3208 R:-100.5624 gloss:21.6690 dloss:99.7320\n",
      "Episode:534 meanR:-102.2480 R:-101.4685 gloss:23.4793 dloss:96.8002\n",
      "Episode:535 meanR:-102.2864 R:-105.8163 gloss:25.5654 dloss:99.8604\n",
      "Episode:536 meanR:-102.2895 R:-107.3741 gloss:18.5054 dloss:78.5268\n",
      "Episode:537 meanR:-102.3365 R:-104.6206 gloss:25.2663 dloss:104.9680\n",
      "Episode:538 meanR:-102.3785 R:-106.3392 gloss:21.2546 dloss:95.1874\n",
      "Episode:539 meanR:-102.3782 R:-101.1042 gloss:20.3345 dloss:91.5061\n",
      "Episode:540 meanR:-102.4263 R:-105.6436 gloss:24.0958 dloss:107.4003\n",
      "Episode:541 meanR:-102.4501 R:-103.8600 gloss:24.5518 dloss:105.8112\n",
      "Episode:542 meanR:-102.4398 R:-101.3615 gloss:27.0178 dloss:121.1391\n",
      "Episode:543 meanR:-102.4916 R:-105.6866 gloss:21.6895 dloss:106.7140\n",
      "Episode:544 meanR:-102.5560 R:-107.8682 gloss:21.7747 dloss:98.4903\n",
      "Episode:545 meanR:-102.5340 R:-101.3338 gloss:24.6593 dloss:104.7182\n",
      "Episode:546 meanR:-102.5413 R:-101.4885 gloss:24.6462 dloss:109.4741\n",
      "Episode:547 meanR:-102.5760 R:-106.3740 gloss:21.8421 dloss:99.5640\n",
      "Episode:548 meanR:-102.6014 R:-105.8305 gloss:29.3550 dloss:124.1196\n",
      "Episode:549 meanR:-102.6489 R:-105.7868 gloss:16.3536 dloss:80.1885\n",
      "Episode:550 meanR:-102.6551 R:-101.2240 gloss:29.3085 dloss:124.0520\n",
      "Episode:551 meanR:-102.7056 R:-107.2507 gloss:31.7727 dloss:131.2627\n",
      "Episode:552 meanR:-102.7163 R:-101.8095 gloss:18.1875 dloss:84.2356\n",
      "Episode:553 meanR:-102.7274 R:-102.7206 gloss:22.3215 dloss:102.3449\n",
      "Episode:554 meanR:-102.7585 R:-104.7921 gloss:22.8158 dloss:106.4630\n",
      "Episode:555 meanR:-102.7951 R:-105.7949 gloss:17.7669 dloss:76.3441\n",
      "Episode:556 meanR:-102.8275 R:-106.1933 gloss:11.7705 dloss:58.6438\n",
      "Episode:557 meanR:-102.8215 R:-101.3092 gloss:20.6094 dloss:81.9785\n",
      "Episode:558 meanR:-102.8223 R:-101.2529 gloss:26.5556 dloss:111.6320\n",
      "Episode:559 meanR:-102.8551 R:-103.5515 gloss:17.9953 dloss:75.4312\n",
      "Episode:560 meanR:-102.8506 R:-100.9997 gloss:26.3352 dloss:107.4955\n",
      "Episode:561 meanR:-102.8350 R:-100.3628 gloss:27.6688 dloss:121.7095\n",
      "Episode:562 meanR:-102.8203 R:-100.6813 gloss:23.9220 dloss:111.2130\n",
      "Episode:563 meanR:-102.8242 R:-100.5289 gloss:26.9343 dloss:113.0502\n",
      "Episode:564 meanR:-102.8318 R:-101.0485 gloss:22.9927 dloss:99.0530\n",
      "Episode:565 meanR:-102.8349 R:-100.7105 gloss:26.5851 dloss:115.6957\n",
      "Episode:566 meanR:-102.7977 R:-101.6742 gloss:27.3581 dloss:112.0824\n",
      "Episode:567 meanR:-102.7851 R:-100.3417 gloss:26.5262 dloss:114.2848\n",
      "Episode:568 meanR:-102.7310 R:-100.8520 gloss:26.4070 dloss:127.2430\n",
      "Episode:569 meanR:-102.7433 R:-103.2175 gloss:19.3714 dloss:93.4843\n",
      "Episode:570 meanR:-102.7040 R:-100.5102 gloss:30.1752 dloss:133.3006\n",
      "Episode:571 meanR:-102.6761 R:-100.6424 gloss:28.8186 dloss:122.3233\n",
      "Episode:572 meanR:-102.6671 R:-101.0852 gloss:31.8003 dloss:143.3695\n",
      "Episode:573 meanR:-102.6721 R:-101.0419 gloss:24.5593 dloss:112.8116\n",
      "Episode:574 meanR:-102.6816 R:-101.4835 gloss:29.1122 dloss:127.9609\n",
      "Episode:575 meanR:-102.6887 R:-101.1637 gloss:20.4166 dloss:98.0345\n",
      "Episode:576 meanR:-102.7065 R:-103.3697 gloss:19.0104 dloss:94.3275\n",
      "Episode:577 meanR:-102.7671 R:-106.2567 gloss:18.2077 dloss:88.8930\n",
      "Episode:578 meanR:-102.7780 R:-101.6836 gloss:31.4198 dloss:127.2299\n",
      "Episode:579 meanR:-102.7748 R:-100.8125 gloss:31.4036 dloss:136.0541\n",
      "Episode:580 meanR:-102.8265 R:-105.3655 gloss:35.4147 dloss:152.7004\n",
      "Episode:581 meanR:-102.8826 R:-106.0030 gloss:20.2075 dloss:98.6813\n",
      "Episode:582 meanR:-102.9245 R:-105.8046 gloss:26.1770 dloss:113.4678\n",
      "Episode:583 meanR:-102.9766 R:-105.5363 gloss:21.9014 dloss:101.6846\n",
      "Episode:584 meanR:-102.9944 R:-102.7942 gloss:23.1496 dloss:110.7374\n",
      "Episode:585 meanR:-103.0006 R:-101.1959 gloss:20.3140 dloss:93.3447\n",
      "Episode:586 meanR:-103.0551 R:-105.8867 gloss:19.6359 dloss:97.1754\n",
      "Episode:587 meanR:-103.0625 R:-104.6683 gloss:22.5633 dloss:103.3830\n",
      "Episode:588 meanR:-103.1162 R:-105.4131 gloss:28.8558 dloss:120.5313\n",
      "Episode:589 meanR:-103.1892 R:-107.3049 gloss:17.6166 dloss:86.1395\n",
      "Episode:590 meanR:-103.2255 R:-105.1945 gloss:22.3017 dloss:99.4191\n",
      "Episode:591 meanR:-103.2144 R:-101.1135 gloss:24.7101 dloss:115.0048\n",
      "Episode:592 meanR:-103.2437 R:-103.7987 gloss:20.7845 dloss:98.3725\n",
      "Episode:593 meanR:-103.2390 R:-101.1089 gloss:26.5905 dloss:108.9191\n",
      "Episode:594 meanR:-103.2321 R:-105.7606 gloss:17.9205 dloss:89.5136\n",
      "Episode:595 meanR:-103.2900 R:-106.2471 gloss:23.2075 dloss:102.6728\n",
      "Episode:596 meanR:-103.3076 R:-102.8020 gloss:27.1880 dloss:117.4470\n",
      "Episode:597 meanR:-103.2681 R:-102.5180 gloss:27.0394 dloss:110.2892\n",
      "Episode:598 meanR:-103.2418 R:-100.6417 gloss:25.9694 dloss:109.3284\n",
      "Episode:599 meanR:-103.2350 R:-101.8457 gloss:28.2667 dloss:122.7336\n",
      "Episode:600 meanR:-102.8356 R:-100.9659 gloss:32.3104 dloss:134.3681\n",
      "Episode:601 meanR:-102.8233 R:-103.3894 gloss:23.2236 dloss:97.6859\n",
      "Episode:602 meanR:-102.7966 R:-100.6179 gloss:19.1323 dloss:96.9200\n",
      "Episode:603 meanR:-102.7803 R:-100.0821 gloss:21.7066 dloss:107.3366\n",
      "Episode:604 meanR:-102.8095 R:-104.8532 gloss:24.2764 dloss:115.1934\n",
      "Episode:605 meanR:-102.8098 R:-102.6417 gloss:32.5743 dloss:128.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:606 meanR:-102.7942 R:-100.2577 gloss:27.1440 dloss:118.9712\n",
      "Episode:607 meanR:-102.7769 R:-100.6226 gloss:21.0995 dloss:95.6794\n",
      "Episode:608 meanR:-102.7671 R:-99.9217 gloss:26.0762 dloss:113.1711\n",
      "Episode:609 meanR:-102.7222 R:-100.9697 gloss:24.8782 dloss:117.0200\n",
      "Episode:610 meanR:-102.7124 R:-100.0271 gloss:20.0421 dloss:98.0119\n",
      "Episode:611 meanR:-102.6542 R:-100.1166 gloss:28.6987 dloss:124.4914\n",
      "Episode:612 meanR:-102.6416 R:-99.9263 gloss:21.1263 dloss:103.7091\n",
      "Episode:613 meanR:-102.6298 R:-99.9563 gloss:28.1885 dloss:127.5518\n",
      "Episode:614 meanR:-102.5729 R:-101.7682 gloss:26.9786 dloss:124.6219\n",
      "Episode:615 meanR:-102.5553 R:-99.7531 gloss:19.0525 dloss:90.1292\n",
      "Episode:616 meanR:-102.5334 R:-99.9775 gloss:25.0351 dloss:107.6707\n",
      "Episode:617 meanR:-102.5906 R:-105.8595 gloss:23.5040 dloss:107.2979\n",
      "Episode:618 meanR:-102.5969 R:-101.1313 gloss:24.1181 dloss:110.4191\n",
      "Episode:619 meanR:-102.5980 R:-100.8106 gloss:21.4004 dloss:101.2084\n",
      "Episode:620 meanR:-102.5967 R:-100.9337 gloss:26.3446 dloss:113.7345\n",
      "Episode:621 meanR:-102.6222 R:-107.0302 gloss:20.6710 dloss:89.5552\n",
      "Episode:622 meanR:-102.6743 R:-106.1367 gloss:32.3649 dloss:131.4692\n",
      "Episode:623 meanR:-102.7290 R:-106.6447 gloss:23.5324 dloss:115.4287\n",
      "Episode:624 meanR:-102.7508 R:-102.6245 gloss:29.6210 dloss:132.5320\n",
      "Episode:625 meanR:-102.7993 R:-105.5304 gloss:25.9609 dloss:117.8852\n",
      "Episode:626 meanR:-102.8009 R:-100.4938 gloss:23.4006 dloss:105.9740\n",
      "Episode:627 meanR:-102.8316 R:-106.5989 gloss:25.6606 dloss:116.7012\n",
      "Episode:628 meanR:-102.8282 R:-103.0987 gloss:24.4173 dloss:116.4284\n",
      "Episode:629 meanR:-102.8856 R:-106.1193 gloss:34.4174 dloss:145.8356\n",
      "Episode:630 meanR:-102.9382 R:-105.8434 gloss:21.5962 dloss:99.1793\n",
      "Episode:631 meanR:-102.9641 R:-102.8950 gloss:23.7462 dloss:105.9554\n",
      "Episode:632 meanR:-103.0159 R:-106.1040 gloss:28.5785 dloss:127.4285\n",
      "Episode:633 meanR:-103.0464 R:-103.6127 gloss:19.8607 dloss:93.2342\n",
      "Episode:634 meanR:-103.4741 R:-144.2346 gloss:24.8049 dloss:111.4584\n",
      "Episode:635 meanR:-103.4185 R:-100.2624 gloss:23.2547 dloss:108.8229\n",
      "Episode:636 meanR:-103.3458 R:-100.1071 gloss:26.4321 dloss:115.9550\n",
      "Episode:637 meanR:-103.2956 R:-99.5930 gloss:30.9916 dloss:131.9307\n",
      "Episode:638 meanR:-103.2434 R:-101.1228 gloss:21.8754 dloss:101.1124\n",
      "Episode:639 meanR:-103.2275 R:-99.5114 gloss:25.9672 dloss:111.6231\n",
      "Episode:640 meanR:-103.2092 R:-103.8150 gloss:30.7717 dloss:137.7561\n",
      "Episode:641 meanR:-103.2032 R:-103.2606 gloss:26.4991 dloss:128.8444\n",
      "Episode:642 meanR:-103.2094 R:-101.9832 gloss:21.7435 dloss:104.5096\n",
      "Episode:643 meanR:-103.1601 R:-100.7558 gloss:27.0236 dloss:122.4226\n",
      "Episode:644 meanR:-103.0943 R:-101.2868 gloss:24.2338 dloss:115.4382\n",
      "Episode:645 meanR:-103.0773 R:-99.6372 gloss:27.2512 dloss:117.3193\n",
      "Episode:646 meanR:-103.0625 R:-100.0020 gloss:23.5622 dloss:109.6756\n",
      "Episode:647 meanR:-102.9954 R:-99.6708 gloss:32.9517 dloss:133.9299\n",
      "Episode:648 meanR:-102.9441 R:-100.6930 gloss:25.6409 dloss:118.0582\n",
      "Episode:649 meanR:-102.9215 R:-103.5271 gloss:25.1665 dloss:116.1679\n",
      "Episode:650 meanR:-102.9384 R:-102.9137 gloss:21.8230 dloss:116.3107\n",
      "Episode:651 meanR:-102.9016 R:-103.5778 gloss:32.6164 dloss:143.5171\n",
      "Episode:652 meanR:-102.8914 R:-100.7919 gloss:35.8230 dloss:156.8640\n",
      "Episode:653 meanR:-102.8695 R:-100.5284 gloss:27.2737 dloss:127.1984\n",
      "Episode:654 meanR:-102.8351 R:-101.3494 gloss:26.4260 dloss:125.5740\n",
      "Episode:655 meanR:-102.7883 R:-101.1156 gloss:22.9538 dloss:114.9492\n",
      "Episode:656 meanR:-102.7618 R:-103.5403 gloss:25.6282 dloss:116.8120\n",
      "Episode:657 meanR:-102.7571 R:-100.8411 gloss:28.4558 dloss:139.4336\n",
      "Episode:658 meanR:-102.8247 R:-108.0150 gloss:39.0474 dloss:177.3758\n",
      "Episode:659 meanR:-102.8191 R:-102.9946 gloss:24.9210 dloss:113.1671\n",
      "Episode:660 meanR:-102.8887 R:-107.9586 gloss:25.2616 dloss:115.1846\n",
      "Episode:661 meanR:-102.8956 R:-101.0486 gloss:21.1521 dloss:99.0316\n",
      "Episode:662 meanR:-102.9529 R:-106.4073 gloss:29.4593 dloss:128.1016\n",
      "Episode:663 meanR:-102.9807 R:-103.3107 gloss:28.3688 dloss:136.8546\n",
      "Episode:664 meanR:-103.0013 R:-103.1096 gloss:20.7821 dloss:100.9609\n",
      "Episode:665 meanR:-103.0254 R:-103.1271 gloss:29.3364 dloss:141.0237\n",
      "Episode:666 meanR:-103.0237 R:-101.5024 gloss:29.8904 dloss:135.8565\n",
      "Episode:667 meanR:-103.0095 R:-98.9152 gloss:25.2801 dloss:121.6091\n",
      "Episode:668 meanR:-103.0614 R:-106.0483 gloss:18.8755 dloss:104.9158\n",
      "Episode:669 meanR:-103.1104 R:-108.1180 gloss:27.3153 dloss:118.8383\n",
      "Episode:670 meanR:-103.1387 R:-103.3366 gloss:25.0790 dloss:117.2974\n",
      "Episode:671 meanR:-103.1651 R:-103.2864 gloss:26.2512 dloss:122.8884\n",
      "Episode:672 meanR:-103.1838 R:-102.9556 gloss:31.0761 dloss:132.7247\n",
      "Episode:673 meanR:-103.2194 R:-104.5931 gloss:27.7526 dloss:131.4457\n",
      "Episode:674 meanR:-103.2390 R:-103.4469 gloss:30.3600 dloss:134.2600\n",
      "Episode:675 meanR:-103.3092 R:-108.1808 gloss:35.1463 dloss:157.9962\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model2.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        num_step = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_preds = sess.run(model.actions_preds, feed_dict={model.states: state.reshape([1, -1]), \n",
    "                                                                    model.training: False})\n",
    "            noise = np.random.normal(loc=0, scale=0.1, size=action_size) # randomness\n",
    "            action = action_preds.reshape([-1]) + noise\n",
    "            #print(action.shape, action_logits.shape, noise.shape)\n",
    "            action = np.clip(action, -1, 1) # clipped\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rate = -1 # success rate: -1 to +1\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # memory updated\n",
    "            total_reward += reward # max reward 300\n",
    "            state = next_state\n",
    "            \n",
    "            if done is True:\n",
    "                # Best 100-episode average reward was 220.62 Â± 0.69. \n",
    "                # (BipedalWalker-v2 is considered \"solved\" \n",
    "                #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "                rate = total_reward/300\n",
    "                rate = np.clip(rate, -1, 1)\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1:\n",
    "                        memory.buffer[-1-idx][-1] = rate\n",
    "                        \n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            #rates = np.array([each[5] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states, \n",
    "                                                                   model.training: False})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # discrete DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # continuous DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.targetQs: targetQs, \n",
    "                                                       model.rewards: rewards, \n",
    "                                                       model.training: True})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        # Did not solve the environment. \n",
    "        # Best 100-episode average reward was 220.62 Â± 0.69. \n",
    "        # (BipedalWalker-v2 is considered \"solved\" \n",
    "        #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "        if np.mean(episode_reward) >= 300:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: -103.43347330792993\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_preds = sess.run(model.actions_preds, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.reshape(action_preds, [-1]) # For continuous action space\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "# End the env                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
