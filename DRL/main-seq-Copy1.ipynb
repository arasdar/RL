{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.5946314602374336 -2.7744319439056646\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 0.7310585786300049)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size) # hidden size\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Generator/Controller: Generating/prediting the actions\n",
    "# def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('generator', reuse=reuse):\n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return actions logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "# def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('discriminator', reuse=reuse):\n",
    "#         # Fusion/merge states and actions/ SA/ SM\n",
    "#         x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return rewards logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, \n",
    "                                              cell=cell, initial_state=g_initial_state, lstm_size=hidden_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs_logits, d_final_state = discriminator(states=states, actions=actions_logits, \n",
    "                                             cell=cell, initial_state=d_initial_state, lstm_size=hidden_size)\n",
    "    d_loss = tf.reduce_mean(tf.square(tf.reshape(Qs_logits, [-1]) - targetQs))\n",
    "    # d_lossQ_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Qs_logits, [-1]),\n",
    "    #                                                                       labels=tf.nn.sigmoid(targetQs)))\n",
    "    #d_loss = d_lossQ_sigm + d_lossQ\n",
    "\n",
    "    return actions_logits, Qs_logits, g_final_state, d_final_state, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1111, 4) actions:(1111,)\n",
      "action size:2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 128               # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n",
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.0029129 , -0.00483114, -0.01933036,  0.0234201 ]),\n",
       " 0,\n",
       " array([ 0.00281628, -0.19967062, -0.01886196,  0.30994194]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:50.0000 rate:0.1000 gloss:1.1889 dloss:1.0007\n",
      "Episode:1 meanR:35.5000 rate:0.0420 gloss:1.2996 dloss:1.0501\n",
      "Episode:2 meanR:37.0000 rate:0.0800 gloss:1.9988 dloss:1.2875\n",
      "Episode:3 meanR:34.5000 rate:0.0540 gloss:1.7074 dloss:1.7590\n",
      "Episode:4 meanR:38.2000 rate:0.1060 gloss:1.0673 dloss:2.4366\n",
      "Episode:5 meanR:38.5000 rate:0.0800 gloss:0.7713 dloss:4.0421\n",
      "Episode:6 meanR:37.2857 rate:0.0600 gloss:2.2937 dloss:2.1546\n",
      "Episode:7 meanR:38.5000 rate:0.0940 gloss:0.9863 dloss:3.5994\n",
      "Episode:8 meanR:39.4444 rate:0.0940 gloss:0.1859 dloss:3.4468\n",
      "Episode:9 meanR:40.2000 rate:0.0940 gloss:-14.5617 dloss:4.5379\n",
      "Episode:10 meanR:39.7273 rate:0.0700 gloss:1.6908 dloss:6.8889\n",
      "Episode:11 meanR:38.1667 rate:0.0420 gloss:13.8402 dloss:5.0318\n",
      "Episode:12 meanR:37.0769 rate:0.0480 gloss:10.7555 dloss:3.4873\n",
      "Episode:13 meanR:42.7143 rate:0.2320 gloss:1.7979 dloss:4.0999\n",
      "Episode:14 meanR:42.1333 rate:0.0680 gloss:2.2200 dloss:7.4854\n",
      "Episode:15 meanR:42.0000 rate:0.0800 gloss:2.7607 dloss:9.2070\n",
      "Episode:16 meanR:42.7647 rate:0.1100 gloss:1.9210 dloss:6.2255\n",
      "Episode:17 meanR:42.6111 rate:0.0800 gloss:1.4272 dloss:7.4197\n",
      "Episode:18 meanR:43.5789 rate:0.1220 gloss:0.7826 dloss:7.4512\n",
      "Episode:19 meanR:44.8500 rate:0.1380 gloss:0.5234 dloss:3.5838\n",
      "Episode:20 meanR:47.5714 rate:0.2040 gloss:0.5304 dloss:10.7466\n",
      "Episode:21 meanR:49.7273 rate:0.1900 gloss:0.2808 dloss:20.0036\n",
      "Episode:22 meanR:51.8696 rate:0.1980 gloss:0.5748 dloss:14.2819\n",
      "Episode:23 meanR:53.1250 rate:0.1640 gloss:0.7105 dloss:3.3358\n",
      "Episode:24 meanR:52.4400 rate:0.0720 gloss:0.1622 dloss:25.2106\n",
      "Episode:25 meanR:54.3846 rate:0.2060 gloss:0.0692 dloss:17.9650\n",
      "Episode:26 meanR:53.9630 rate:0.0860 gloss:0.0551 dloss:23.9505\n",
      "Episode:27 meanR:53.9643 rate:0.1080 gloss:0.3521 dloss:36.1621\n",
      "Episode:28 meanR:53.9655 rate:0.1080 gloss:0.1242 dloss:37.7638\n",
      "Episode:29 meanR:55.2000 rate:0.1820 gloss:0.0840 dloss:12.0164\n",
      "Episode:30 meanR:56.7742 rate:0.2080 gloss:0.0518 dloss:4.3151\n",
      "Episode:31 meanR:58.8125 rate:0.2440 gloss:0.1194 dloss:12.0014\n",
      "Episode:32 meanR:60.2424 rate:0.2120 gloss:0.1991 dloss:8.3567\n",
      "Episode:33 meanR:60.1176 rate:0.1120 gloss:0.0727 dloss:37.1607\n",
      "Episode:34 meanR:61.2286 rate:0.1980 gloss:0.0625 dloss:10.9325\n",
      "Episode:35 meanR:61.3056 rate:0.1280 gloss:0.1422 dloss:15.1178\n",
      "Episode:36 meanR:62.0270 rate:0.1760 gloss:0.1700 dloss:10.0544\n",
      "Episode:37 meanR:61.9211 rate:0.1160 gloss:0.3530 dloss:15.3141\n",
      "Episode:38 meanR:63.0769 rate:0.2140 gloss:0.1096 dloss:15.7558\n",
      "Episode:39 meanR:63.9250 rate:0.1940 gloss:0.3516 dloss:20.3589\n",
      "Episode:40 meanR:65.8049 rate:0.2820 gloss:0.0927 dloss:23.4254\n",
      "Episode:41 meanR:66.3571 rate:0.1780 gloss:0.0966 dloss:19.6786\n",
      "Episode:42 meanR:67.4419 rate:0.2260 gloss:0.0717 dloss:19.2622\n",
      "Episode:43 meanR:69.5682 rate:0.3220 gloss:0.3444 dloss:4.5288\n",
      "Episode:44 meanR:69.9333 rate:0.1720 gloss:0.1326 dloss:10.9710\n",
      "Episode:45 meanR:71.5217 rate:0.2860 gloss:0.0382 dloss:41.3970\n",
      "Episode:46 meanR:72.1064 rate:0.1980 gloss:0.0330 dloss:33.2437\n",
      "Episode:47 meanR:72.7083 rate:0.2020 gloss:0.0123 dloss:25.4930\n",
      "Episode:48 meanR:73.8571 rate:0.2580 gloss:0.1036 dloss:30.5413\n",
      "Episode:49 meanR:74.5000 rate:0.2120 gloss:0.0236 dloss:31.9788\n",
      "Episode:50 meanR:74.8235 rate:0.1820 gloss:0.0155 dloss:17.8279\n",
      "Episode:51 meanR:75.3654 rate:0.2060 gloss:0.0619 dloss:14.8639\n",
      "Episode:52 meanR:76.0943 rate:0.2280 gloss:0.0390 dloss:6.2403\n",
      "Episode:53 meanR:76.7407 rate:0.2220 gloss:0.0659 dloss:6.4422\n",
      "Episode:54 meanR:77.3273 rate:0.2180 gloss:0.0665 dloss:5.5410\n",
      "Episode:55 meanR:77.5000 rate:0.1740 gloss:0.0657 dloss:6.2960\n",
      "Episode:56 meanR:77.4035 rate:0.1440 gloss:0.2094 dloss:18.0927\n",
      "Episode:57 meanR:78.2759 rate:0.2560 gloss:0.0831 dloss:12.9449\n",
      "Episode:58 meanR:78.1525 rate:0.1420 gloss:0.3379 dloss:8.8186\n",
      "Episode:59 meanR:78.3667 rate:0.1820 gloss:0.0690 dloss:44.1457\n",
      "Episode:60 meanR:78.8361 rate:0.2140 gloss:0.0225 dloss:16.2934\n",
      "Episode:61 meanR:79.6452 rate:0.2580 gloss:0.0283 dloss:8.1853\n",
      "Episode:62 meanR:80.0159 rate:0.2060 gloss:0.0292 dloss:24.8804\n",
      "Episode:63 meanR:80.0156 rate:0.1600 gloss:0.0165 dloss:10.7074\n",
      "Episode:64 meanR:80.2923 rate:0.1960 gloss:0.0055 dloss:6.5038\n",
      "Episode:65 meanR:80.3333 rate:0.1660 gloss:0.0189 dloss:6.8810\n",
      "Episode:66 meanR:80.5672 rate:0.1920 gloss:0.0520 dloss:5.6144\n",
      "Episode:67 meanR:80.8824 rate:0.2040 gloss:0.0113 dloss:7.0337\n",
      "Episode:68 meanR:81.3043 rate:0.2200 gloss:0.0187 dloss:4.9904\n",
      "Episode:69 meanR:81.6857 rate:0.2160 gloss:0.0521 dloss:4.2465\n",
      "Episode:70 meanR:81.6479 rate:0.1580 gloss:0.0149 dloss:4.4413\n",
      "Episode:71 meanR:81.8889 rate:0.1980 gloss:0.0066 dloss:3.3811\n",
      "Episode:72 meanR:82.1644 rate:0.2040 gloss:0.0305 dloss:3.3429\n",
      "Episode:73 meanR:82.7297 rate:0.2480 gloss:0.0084 dloss:5.7550\n",
      "Episode:74 meanR:83.4267 rate:0.2700 gloss:0.0016 dloss:7.4739\n",
      "Episode:75 meanR:83.8421 rate:0.2300 gloss:0.0662 dloss:4.1639\n",
      "Episode:76 meanR:83.7792 rate:0.1580 gloss:0.0288 dloss:10.2085\n",
      "Episode:77 meanR:84.2179 rate:0.2360 gloss:0.0267 dloss:6.1954\n",
      "Episode:78 meanR:84.3544 rate:0.1900 gloss:0.0356 dloss:5.2993\n",
      "Episode:79 meanR:84.5000 rate:0.1920 gloss:0.1412 dloss:13.4333\n",
      "Episode:80 meanR:84.5802 rate:0.1820 gloss:0.0094 dloss:4.5894\n",
      "Episode:81 meanR:84.7927 rate:0.2040 gloss:0.0089 dloss:4.0605\n",
      "Episode:82 meanR:85.0120 rate:0.2060 gloss:0.0125 dloss:8.7083\n",
      "Episode:83 meanR:85.4286 rate:0.2400 gloss:0.0298 dloss:5.2938\n",
      "Episode:84 meanR:85.4353 rate:0.1720 gloss:0.0138 dloss:5.7917\n",
      "Episode:85 meanR:84.9651 rate:0.0900 gloss:0.0555 dloss:25.2490\n",
      "Episode:86 meanR:84.9195 rate:0.1620 gloss:0.0056 dloss:11.5708\n",
      "Episode:87 meanR:84.9318 rate:0.1720 gloss:0.0094 dloss:11.2142\n",
      "Episode:88 meanR:84.5169 rate:0.0960 gloss:0.0075 dloss:8.1682\n",
      "Episode:89 meanR:84.5222 rate:0.1700 gloss:0.0026 dloss:4.4661\n",
      "Episode:90 meanR:84.2637 rate:0.1220 gloss:0.0058 dloss:3.9813\n",
      "Episode:91 meanR:83.9239 rate:0.1060 gloss:0.0185 dloss:7.1317\n",
      "Episode:92 meanR:84.1183 rate:0.2040 gloss:0.0098 dloss:5.7539\n",
      "Episode:93 meanR:84.5532 rate:0.2500 gloss:0.0023 dloss:5.6426\n",
      "Episode:94 meanR:84.1053 rate:0.0840 gloss:0.0057 dloss:2.3896\n",
      "Episode:95 meanR:83.6250 rate:0.0760 gloss:0.0144 dloss:3.5278\n",
      "Episode:96 meanR:83.3299 rate:0.1100 gloss:0.0749 dloss:4.3863\n",
      "Episode:97 meanR:83.0306 rate:0.1080 gloss:0.0322 dloss:4.2073\n",
      "Episode:98 meanR:82.6465 rate:0.0900 gloss:0.0129 dloss:2.4642\n",
      "Episode:99 meanR:82.2600 rate:0.0880 gloss:0.1533 dloss:2.1998\n",
      "Episode:100 meanR:82.7500 rate:0.1980 gloss:0.0165 dloss:4.8525\n",
      "Episode:101 meanR:83.8000 rate:0.2520 gloss:0.0511 dloss:5.0975\n",
      "Episode:102 meanR:83.8000 rate:0.0800 gloss:0.0228 dloss:5.2259\n",
      "Episode:103 meanR:84.9100 rate:0.2760 gloss:0.0354 dloss:3.4963\n",
      "Episode:104 meanR:84.9400 rate:0.1120 gloss:-1.0840 dloss:7.2798\n",
      "Episode:105 meanR:85.5200 rate:0.1960 gloss:1.7530 dloss:6.6040\n",
      "Episode:106 meanR:86.0900 rate:0.1740 gloss:0.0204 dloss:4.8598\n",
      "Episode:107 meanR:86.6900 rate:0.2140 gloss:0.0027 dloss:3.7951\n",
      "Episode:108 meanR:86.7000 rate:0.0960 gloss:0.0023 dloss:4.9972\n",
      "Episode:109 meanR:86.9900 rate:0.1520 gloss:0.0040 dloss:1.9263\n",
      "Episode:110 meanR:87.8000 rate:0.2320 gloss:0.0233 dloss:4.1352\n",
      "Episode:111 meanR:88.0900 rate:0.1000 gloss:0.0094 dloss:7.0054\n",
      "Episode:112 meanR:89.0000 rate:0.2300 gloss:0.0051 dloss:7.3058\n",
      "Episode:113 meanR:88.4000 rate:0.1120 gloss:0.0024 dloss:7.9320\n",
      "Episode:114 meanR:89.2500 rate:0.2380 gloss:0.0133 dloss:3.7148\n",
      "Episode:115 meanR:89.4500 rate:0.1200 gloss:0.0037 dloss:5.7570\n",
      "Episode:116 meanR:90.1600 rate:0.2520 gloss:0.0057 dloss:3.4340\n",
      "Episode:117 meanR:90.8800 rate:0.2240 gloss:0.0176 dloss:4.1037\n",
      "Episode:118 meanR:91.0300 rate:0.1520 gloss:0.0033 dloss:3.3974\n",
      "Episode:119 meanR:91.3400 rate:0.2000 gloss:0.0105 dloss:5.0597\n",
      "Episode:120 meanR:91.3100 rate:0.1980 gloss:0.1547 dloss:3.0581\n",
      "Episode:121 meanR:91.1000 rate:0.1480 gloss:0.0067 dloss:12.2317\n",
      "Episode:122 meanR:90.6900 rate:0.1160 gloss:0.0013 dloss:10.3101\n",
      "Episode:123 meanR:90.4700 rate:0.1200 gloss:0.0027 dloss:10.1511\n",
      "Episode:124 meanR:90.6700 rate:0.1120 gloss:0.0082 dloss:5.5705\n",
      "Episode:125 meanR:90.1400 rate:0.1000 gloss:0.0044 dloss:5.0740\n",
      "Episode:126 meanR:90.1000 rate:0.0780 gloss:0.0070 dloss:6.1975\n",
      "Episode:127 meanR:90.2900 rate:0.1460 gloss:0.0008 dloss:7.5211\n",
      "Episode:128 meanR:90.4000 rate:0.1300 gloss:0.0081 dloss:16.4013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:129 meanR:90.0000 rate:0.1020 gloss:0.0094 dloss:10.9600\n",
      "Episode:130 meanR:89.5500 rate:0.1180 gloss:0.0067 dloss:8.5929\n",
      "Episode:131 meanR:89.1300 rate:0.1600 gloss:0.0005 dloss:7.9039\n",
      "Episode:132 meanR:88.6800 rate:0.1220 gloss:0.0221 dloss:11.0543\n",
      "Episode:133 meanR:88.5700 rate:0.0900 gloss:0.0173 dloss:12.3298\n",
      "Episode:134 meanR:88.0600 rate:0.0960 gloss:0.0134 dloss:11.6484\n",
      "Episode:135 meanR:87.9800 rate:0.1120 gloss:0.0205 dloss:7.2780\n",
      "Episode:136 meanR:87.5900 rate:0.0980 gloss:0.0039 dloss:3.8217\n",
      "Episode:137 meanR:87.4400 rate:0.0860 gloss:0.0052 dloss:2.3314\n",
      "Episode:138 meanR:86.9500 rate:0.1160 gloss:0.0110 dloss:5.8355\n",
      "Episode:139 meanR:86.6000 rate:0.1240 gloss:0.0037 dloss:5.3905\n",
      "Episode:140 meanR:85.6500 rate:0.0920 gloss:0.0017 dloss:4.9117\n",
      "Episode:141 meanR:85.3500 rate:0.1180 gloss:0.0017 dloss:8.7412\n",
      "Episode:142 meanR:84.9400 rate:0.1440 gloss:0.0005 dloss:7.6760\n",
      "Episode:143 meanR:83.9700 rate:0.1280 gloss:0.0003 dloss:4.5228\n",
      "Episode:144 meanR:84.1800 rate:0.2140 gloss:0.0898 dloss:4.0169\n",
      "Episode:145 meanR:83.4700 rate:0.1440 gloss:0.0058 dloss:5.0242\n",
      "Episode:146 meanR:83.1900 rate:0.1420 gloss:0.0344 dloss:11.4556\n",
      "Episode:147 meanR:82.9500 rate:0.1540 gloss:0.0163 dloss:5.6142\n",
      "Episode:148 meanR:82.4200 rate:0.1520 gloss:0.0147 dloss:6.7630\n",
      "Episode:149 meanR:81.8800 rate:0.1040 gloss:0.0134 dloss:8.0353\n",
      "Episode:150 meanR:81.6700 rate:0.1400 gloss:0.0026 dloss:5.2944\n",
      "Episode:151 meanR:81.1800 rate:0.1080 gloss:0.0036 dloss:4.0518\n",
      "Episode:152 meanR:80.6500 rate:0.1220 gloss:0.0343 dloss:8.6834\n",
      "Episode:153 meanR:80.3000 rate:0.1520 gloss:0.0048 dloss:3.3276\n",
      "Episode:154 meanR:79.8200 rate:0.1220 gloss:0.0067 dloss:2.8423\n",
      "Episode:155 meanR:79.6800 rate:0.1460 gloss:0.0066 dloss:6.2269\n",
      "Episode:156 meanR:79.5500 rate:0.1180 gloss:0.0066 dloss:6.7557\n",
      "Episode:157 meanR:78.9500 rate:0.1360 gloss:0.0017 dloss:9.8292\n",
      "Episode:158 meanR:78.8500 rate:0.1220 gloss:0.0012 dloss:7.7127\n",
      "Episode:159 meanR:78.4300 rate:0.0980 gloss:0.0053 dloss:6.9589\n",
      "Episode:160 meanR:78.4900 rate:0.2260 gloss:0.0078 dloss:2.2521\n",
      "Episode:161 meanR:77.7500 rate:0.1100 gloss:0.0012 dloss:6.3108\n",
      "Episode:162 meanR:77.1600 rate:0.0880 gloss:0.0012 dloss:8.5821\n",
      "Episode:163 meanR:77.1600 rate:0.1600 gloss:0.0017 dloss:6.0834\n",
      "Episode:164 meanR:76.7200 rate:0.1080 gloss:0.0018 dloss:13.4848\n",
      "Episode:165 meanR:76.2900 rate:0.0800 gloss:0.0041 dloss:18.7340\n",
      "Episode:166 meanR:75.9600 rate:0.1260 gloss:0.0027 dloss:14.6313\n",
      "Episode:167 meanR:75.8000 rate:0.1720 gloss:0.0036 dloss:6.8928\n",
      "Episode:168 meanR:75.2600 rate:0.1120 gloss:0.0014 dloss:4.8492\n",
      "Episode:169 meanR:74.8200 rate:0.1280 gloss:0.0010 dloss:4.1418\n",
      "Episode:170 meanR:74.4700 rate:0.0880 gloss:0.0005 dloss:9.2350\n",
      "Episode:171 meanR:74.3300 rate:0.1700 gloss:0.0132 dloss:15.8706\n",
      "Episode:172 meanR:74.4100 rate:0.2200 gloss:0.0058 dloss:10.0264\n",
      "Episode:173 meanR:74.5900 rate:0.2840 gloss:0.0076 dloss:7.1970\n",
      "Episode:174 meanR:73.7800 rate:0.1080 gloss:0.0014 dloss:6.6681\n",
      "Episode:175 meanR:73.2300 rate:0.1200 gloss:0.0302 dloss:8.8666\n",
      "Episode:176 meanR:73.6500 rate:0.2420 gloss:0.0037 dloss:14.6883\n",
      "Episode:177 meanR:73.0300 rate:0.1120 gloss:0.0017 dloss:7.2799\n",
      "Episode:178 meanR:72.6900 rate:0.1220 gloss:0.0014 dloss:7.1753\n",
      "Episode:179 meanR:72.5600 rate:0.1660 gloss:0.0011 dloss:8.8727\n",
      "Episode:180 meanR:72.6800 rate:0.2060 gloss:0.0023 dloss:12.8660\n",
      "Episode:181 meanR:72.3400 rate:0.1360 gloss:0.0006 dloss:9.0157\n",
      "Episode:182 meanR:72.1700 rate:0.1720 gloss:0.0045 dloss:6.3307\n",
      "Episode:183 meanR:71.5300 rate:0.1120 gloss:0.0053 dloss:2.9597\n",
      "Episode:184 meanR:71.6200 rate:0.1900 gloss:0.0009 dloss:11.3452\n",
      "Episode:185 meanR:72.3300 rate:0.2320 gloss:0.0023 dloss:15.0253\n",
      "Episode:186 meanR:72.0600 rate:0.1080 gloss:0.0011 dloss:5.9604\n",
      "Episode:187 meanR:72.6500 rate:0.2900 gloss:0.0008 dloss:3.2847\n",
      "Episode:188 meanR:72.9200 rate:0.1500 gloss:0.0009 dloss:10.4186\n",
      "Episode:189 meanR:72.9900 rate:0.1840 gloss:0.0002 dloss:7.8754\n",
      "Episode:190 meanR:73.2500 rate:0.1740 gloss:0.0005 dloss:7.3325\n",
      "Episode:191 meanR:73.1000 rate:0.0760 gloss:0.0001 dloss:7.2499\n",
      "Episode:192 meanR:72.6800 rate:0.1200 gloss:0.0001 dloss:6.3166\n",
      "Episode:193 meanR:71.8700 rate:0.0880 gloss:0.0010 dloss:2.4387\n",
      "Episode:194 meanR:71.8600 rate:0.0820 gloss:0.0056 dloss:1.9934\n",
      "Episode:195 meanR:72.1200 rate:0.1280 gloss:0.0024 dloss:3.3035\n",
      "Episode:196 meanR:72.2100 rate:0.1280 gloss:0.0040 dloss:3.1964\n",
      "Episode:197 meanR:72.2200 rate:0.1100 gloss:0.0156 dloss:16.9333\n",
      "Episode:198 meanR:72.4900 rate:0.1440 gloss:0.0007 dloss:5.1116\n",
      "Episode:199 meanR:72.7500 rate:0.1400 gloss:0.0050 dloss:3.8223\n",
      "Episode:200 meanR:72.1400 rate:0.0760 gloss:0.0095 dloss:3.6125\n",
      "Episode:201 meanR:71.2700 rate:0.0780 gloss:0.0018 dloss:3.2553\n",
      "Episode:202 meanR:72.0400 rate:0.2340 gloss:0.0011 dloss:5.5557\n",
      "Episode:203 meanR:71.4500 rate:0.1580 gloss:0.0005 dloss:2.9457\n",
      "Episode:204 meanR:71.5500 rate:0.1320 gloss:0.0017 dloss:3.7193\n",
      "Episode:205 meanR:71.0300 rate:0.0920 gloss:0.0009 dloss:5.3909\n",
      "Episode:206 meanR:70.6800 rate:0.1040 gloss:0.0030 dloss:1.6231\n",
      "Episode:207 meanR:70.1800 rate:0.1140 gloss:0.0004 dloss:4.7096\n",
      "Episode:208 meanR:70.8300 rate:0.2260 gloss:0.0002 dloss:7.7634\n",
      "Episode:209 meanR:70.4800 rate:0.0820 gloss:0.0005 dloss:8.9670\n",
      "Episode:210 meanR:69.8000 rate:0.0960 gloss:0.0009 dloss:9.0591\n",
      "Episode:211 meanR:70.4000 rate:0.2200 gloss:0.0001 dloss:3.6641\n",
      "Episode:212 meanR:69.6000 rate:0.0700 gloss:0.0001 dloss:5.4066\n",
      "Episode:213 meanR:69.5600 rate:0.1040 gloss:-0.0005 dloss:10.5299\n",
      "Episode:214 meanR:70.4000 rate:0.4060 gloss:0.0540 dloss:5.6957\n",
      "Episode:215 meanR:70.9300 rate:0.2260 gloss:-2.0533 dloss:154.4088\n",
      "Episode:216 meanR:69.9300 rate:0.0520 gloss:-411.1732 dloss:7.2807\n",
      "Episode:217 meanR:68.9100 rate:0.0200 gloss:-363.7042 dloss:18.5246\n",
      "Episode:218 meanR:68.2300 rate:0.0160 gloss:-113.4282 dloss:29.6299\n",
      "Episode:219 meanR:67.3100 rate:0.0160 gloss:71.7012 dloss:20.1169\n",
      "Episode:220 meanR:66.4100 rate:0.0180 gloss:-7.8613 dloss:0.1133\n",
      "Episode:221 meanR:65.7700 rate:0.0200 gloss:-350.7459 dloss:11.4414\n",
      "Episode:222 meanR:65.2800 rate:0.0180 gloss:-306.8423 dloss:34.7835\n",
      "Episode:223 meanR:64.7700 rate:0.0180 gloss:-314.6155 dloss:17.6327\n",
      "Episode:224 meanR:64.3100 rate:0.0200 gloss:-333.2527 dloss:26.0664\n",
      "Episode:225 meanR:63.9100 rate:0.0200 gloss:-227.7525 dloss:30.7429\n",
      "Episode:226 meanR:63.6100 rate:0.0180 gloss:-282.5168 dloss:52.8734\n",
      "Episode:227 meanR:62.9800 rate:0.0200 gloss:-101.8234 dloss:20.4644\n",
      "Episode:228 meanR:62.4100 rate:0.0160 gloss:-152.3749 dloss:85.1734\n",
      "Episode:229 meanR:62.0100 rate:0.0220 gloss:-30.4433 dloss:65.3955\n",
      "Episode:230 meanR:61.5200 rate:0.0200 gloss:0.0000 dloss:27.0712\n",
      "Episode:231 meanR:60.8100 rate:0.0180 gloss:0.0000 dloss:15.6095\n",
      "Episode:232 meanR:60.2800 rate:0.0160 gloss:0.0000 dloss:13.2014\n",
      "Episode:233 meanR:59.9100 rate:0.0160 gloss:0.0000 dloss:12.6995\n",
      "Episode:234 meanR:59.5200 rate:0.0180 gloss:0.0000 dloss:11.6656\n",
      "Episode:235 meanR:59.0500 rate:0.0180 gloss:0.0000 dloss:10.2367\n",
      "Episode:236 meanR:58.6500 rate:0.0180 gloss:0.0000 dloss:9.8816\n",
      "Episode:237 meanR:58.3000 rate:0.0160 gloss:0.0000 dloss:7.4983\n",
      "Episode:238 meanR:57.8200 rate:0.0200 gloss:0.0000 dloss:7.3157\n",
      "Episode:239 meanR:57.3000 rate:0.0200 gloss:0.0000 dloss:7.1926\n",
      "Episode:240 meanR:56.9400 rate:0.0200 gloss:0.0000 dloss:7.8152\n",
      "Episode:241 meanR:56.4500 rate:0.0200 gloss:0.0000 dloss:7.6272\n",
      "Episode:242 meanR:55.8200 rate:0.0180 gloss:0.0000 dloss:6.7847\n",
      "Episode:243 meanR:55.2800 rate:0.0200 gloss:0.0000 dloss:7.1812\n",
      "Episode:244 meanR:54.3000 rate:0.0180 gloss:0.0000 dloss:7.2628\n",
      "Episode:245 meanR:53.6900 rate:0.0220 gloss:0.0000 dloss:7.0416\n",
      "Episode:246 meanR:53.0600 rate:0.0160 gloss:0.0000 dloss:6.8277\n",
      "Episode:247 meanR:52.3700 rate:0.0160 gloss:0.0000 dloss:6.5485\n",
      "Episode:248 meanR:51.7100 rate:0.0200 gloss:0.0000 dloss:6.3650\n",
      "Episode:249 meanR:51.2800 rate:0.0180 gloss:0.0000 dloss:6.1337\n",
      "Episode:250 meanR:50.6800 rate:0.0200 gloss:0.0000 dloss:5.8742\n",
      "Episode:251 meanR:50.2300 rate:0.0180 gloss:0.0000 dloss:5.5863\n",
      "Episode:252 meanR:49.7200 rate:0.0200 gloss:0.0000 dloss:5.4032\n",
      "Episode:253 meanR:49.0600 rate:0.0200 gloss:0.0000 dloss:5.2461\n",
      "Episode:254 meanR:48.5400 rate:0.0180 gloss:0.0000 dloss:5.2385\n",
      "Episode:255 meanR:47.9100 rate:0.0200 gloss:0.0000 dloss:5.3473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:256 meanR:47.4200 rate:0.0200 gloss:0.0000 dloss:5.0811\n",
      "Episode:257 meanR:46.8400 rate:0.0200 gloss:0.0000 dloss:4.7868\n",
      "Episode:258 meanR:46.3200 rate:0.0180 gloss:0.0000 dloss:4.6030\n",
      "Episode:259 meanR:45.9200 rate:0.0180 gloss:0.0000 dloss:5.2286\n",
      "Episode:260 meanR:44.8800 rate:0.0180 gloss:0.0000 dloss:4.9717\n",
      "Episode:261 meanR:44.4200 rate:0.0180 gloss:0.0000 dloss:4.5539\n",
      "Episode:262 meanR:44.0800 rate:0.0200 gloss:0.0000 dloss:4.3657\n",
      "Episode:263 meanR:43.3700 rate:0.0180 gloss:0.0000 dloss:4.0544\n",
      "Episode:264 meanR:42.9300 rate:0.0200 gloss:0.0000 dloss:3.8939\n",
      "Episode:265 meanR:42.6100 rate:0.0160 gloss:0.0000 dloss:3.6479\n",
      "Episode:266 meanR:42.0600 rate:0.0160 gloss:0.0000 dloss:3.5703\n",
      "Episode:267 meanR:41.2900 rate:0.0180 gloss:0.0000 dloss:3.5509\n",
      "Episode:268 meanR:40.8200 rate:0.0180 gloss:0.0000 dloss:3.3464\n",
      "Episode:269 meanR:40.2700 rate:0.0180 gloss:0.0000 dloss:3.3325\n",
      "Episode:270 meanR:39.9100 rate:0.0160 gloss:0.0000 dloss:3.3913\n",
      "Episode:271 meanR:39.1600 rate:0.0200 gloss:0.0000 dloss:3.5120\n",
      "Episode:272 meanR:38.1600 rate:0.0200 gloss:0.0000 dloss:3.5427\n",
      "Episode:273 meanR:36.8400 rate:0.0200 gloss:0.0000 dloss:3.0855\n",
      "Episode:274 meanR:36.4000 rate:0.0200 gloss:0.0000 dloss:2.8820\n",
      "Episode:275 meanR:35.9100 rate:0.0220 gloss:0.0000 dloss:2.6896\n",
      "Episode:276 meanR:34.7900 rate:0.0180 gloss:0.0000 dloss:2.6739\n",
      "Episode:277 meanR:34.3300 rate:0.0200 gloss:0.0000 dloss:2.5861\n",
      "Episode:278 meanR:33.8100 rate:0.0180 gloss:0.0000 dloss:2.7204\n",
      "Episode:279 meanR:33.0800 rate:0.0200 gloss:0.0000 dloss:2.5730\n",
      "Episode:280 meanR:32.1400 rate:0.0180 gloss:0.0000 dloss:2.3874\n",
      "Episode:281 meanR:31.5400 rate:0.0160 gloss:0.0000 dloss:2.3239\n",
      "Episode:282 meanR:30.7700 rate:0.0180 gloss:0.0000 dloss:2.2643\n",
      "Episode:283 meanR:30.2900 rate:0.0160 gloss:0.0000 dloss:2.3299\n",
      "Episode:284 meanR:29.4400 rate:0.0200 gloss:0.0000 dloss:2.1557\n",
      "Episode:285 meanR:28.3600 rate:0.0160 gloss:0.0000 dloss:2.1496\n",
      "Episode:286 meanR:27.9200 rate:0.0200 gloss:0.0000 dloss:2.1199\n",
      "Episode:287 meanR:26.5600 rate:0.0180 gloss:0.0000 dloss:2.1206\n",
      "Episode:288 meanR:25.9100 rate:0.0200 gloss:0.0000 dloss:2.1879\n",
      "Episode:289 meanR:25.0900 rate:0.0200 gloss:0.0000 dloss:2.5179\n",
      "Episode:290 meanR:24.3100 rate:0.0180 gloss:0.0000 dloss:2.3654\n",
      "Episode:291 meanR:24.0200 rate:0.0180 gloss:0.0000 dloss:1.9769\n",
      "Episode:292 meanR:23.5200 rate:0.0200 gloss:0.0000 dloss:1.9200\n",
      "Episode:293 meanR:23.1700 rate:0.0180 gloss:0.0000 dloss:2.2489\n",
      "Episode:294 meanR:22.8500 rate:0.0180 gloss:0.0000 dloss:1.9846\n",
      "Episode:295 meanR:22.3000 rate:0.0180 gloss:0.0000 dloss:1.7968\n",
      "Episode:296 meanR:21.7400 rate:0.0160 gloss:0.0000 dloss:1.7348\n",
      "Episode:297 meanR:21.2900 rate:0.0200 gloss:0.0000 dloss:1.5381\n",
      "Episode:298 meanR:20.6700 rate:0.0200 gloss:0.0000 dloss:1.5515\n",
      "Episode:299 meanR:20.0600 rate:0.0180 gloss:0.0000 dloss:1.5548\n",
      "Episode:300 meanR:19.7800 rate:0.0200 gloss:0.0000 dloss:1.4325\n",
      "Episode:301 meanR:19.4800 rate:0.0180 gloss:0.0000 dloss:1.4186\n",
      "Episode:302 meanR:18.4100 rate:0.0200 gloss:0.0000 dloss:1.3871\n",
      "Episode:303 meanR:17.7100 rate:0.0180 gloss:0.0000 dloss:1.3235\n",
      "Episode:304 meanR:17.1500 rate:0.0200 gloss:0.0000 dloss:1.2918\n",
      "Episode:305 meanR:16.7800 rate:0.0180 gloss:0.0000 dloss:1.2551\n",
      "Episode:306 meanR:16.3600 rate:0.0200 gloss:0.0000 dloss:1.4471\n",
      "Episode:307 meanR:15.8800 rate:0.0180 gloss:0.0000 dloss:1.3099\n",
      "Episode:308 meanR:14.8500 rate:0.0200 gloss:0.0000 dloss:1.1543\n",
      "Episode:309 meanR:14.5400 rate:0.0200 gloss:0.0000 dloss:1.1722\n",
      "Episode:310 meanR:14.1600 rate:0.0200 gloss:0.0000 dloss:1.0569\n",
      "Episode:311 meanR:13.1500 rate:0.0180 gloss:0.0000 dloss:1.0388\n",
      "Episode:312 meanR:12.9000 rate:0.0200 gloss:0.0000 dloss:1.0340\n",
      "Episode:313 meanR:12.4700 rate:0.0180 gloss:0.0000 dloss:0.9933\n",
      "Episode:314 meanR:10.5400 rate:0.0200 gloss:0.0000 dloss:1.1695\n",
      "Episode:315 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.0621\n",
      "Episode:316 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:0.9812\n",
      "Episode:317 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.8824\n",
      "Episode:318 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.8939\n",
      "Episode:319 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.8430\n",
      "Episode:320 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:0.8312\n",
      "Episode:321 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:0.8153\n",
      "Episode:322 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.8273\n",
      "Episode:323 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:0.7671\n",
      "Episode:324 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.8748\n",
      "Episode:325 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.8347\n",
      "Episode:326 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.7680\n",
      "Episode:327 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.6761\n",
      "Episode:328 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:0.8960\n",
      "Episode:329 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.8805\n",
      "Episode:330 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.7281\n",
      "Episode:331 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.7061\n",
      "Episode:332 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.5520\n",
      "Episode:333 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.5165\n",
      "Episode:334 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.6476\n",
      "Episode:335 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:0.4939\n",
      "Episode:336 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:0.6215\n",
      "Episode:337 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:0.7035\n",
      "Episode:338 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.6261\n",
      "Episode:339 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:0.5591\n",
      "Episode:340 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.4314\n",
      "Episode:341 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:0.3857\n",
      "Episode:342 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:0.5624\n",
      "Episode:343 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:0.3894\n",
      "Episode:344 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:0.3127\n",
      "Episode:345 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:0.5046\n",
      "Episode:346 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:0.3059\n",
      "Episode:347 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.5385\n",
      "Episode:348 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.3790\n",
      "Episode:349 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.2872\n",
      "Episode:350 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.7015\n",
      "Episode:351 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.5863\n",
      "Episode:352 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.4455\n",
      "Episode:353 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.2214\n",
      "Episode:354 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.5140\n",
      "Episode:355 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:0.3673\n",
      "Episode:356 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:0.1766\n",
      "Episode:357 meanR:9.2900 rate:0.0160 gloss:0.0000 dloss:0.3519\n",
      "Episode:358 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:0.2291\n",
      "Episode:359 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:0.1268\n",
      "Episode:360 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.1986\n",
      "Episode:361 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.1014\n",
      "Episode:362 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.2074\n",
      "Episode:363 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.3596\n",
      "Episode:364 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.1569\n",
      "Episode:365 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.3755\n",
      "Episode:366 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:0.1596\n",
      "Episode:367 meanR:9.3800 rate:0.0220 gloss:0.0000 dloss:0.3819\n",
      "Episode:368 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1903\n",
      "Episode:369 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.2619\n",
      "Episode:370 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.1209\n",
      "Episode:371 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.1117\n",
      "Episode:372 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.2367\n",
      "Episode:373 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:0.1634\n",
      "Episode:374 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1427\n",
      "Episode:375 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.0847\n",
      "Episode:376 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.0928\n",
      "Episode:377 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:0.0767\n",
      "Episode:378 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:0.0970\n",
      "Episode:379 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:0.1656\n",
      "Episode:380 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:0.3846\n",
      "Episode:381 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.0723\n",
      "Episode:382 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1083\n",
      "Episode:383 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:0.0888\n",
      "Episode:384 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.1351\n",
      "Episode:385 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:0.0958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:386 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1453\n",
      "Episode:387 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:0.1305\n",
      "Episode:388 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:0.1334\n",
      "Episode:389 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.0920\n",
      "Episode:390 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.1502\n",
      "Episode:391 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.1824\n",
      "Episode:392 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:0.3044\n",
      "Episode:393 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.0953\n",
      "Episode:394 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.1574\n",
      "Episode:395 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.1314\n",
      "Episode:396 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:0.0753\n",
      "Episode:397 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.0772\n",
      "Episode:398 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.1752\n",
      "Episode:399 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.2303\n",
      "Episode:400 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.1478\n",
      "Episode:401 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:0.1909\n",
      "Episode:402 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.1971\n",
      "Episode:403 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.1629\n",
      "Episode:404 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.1396\n",
      "Episode:405 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1164\n",
      "Episode:406 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:0.1273\n",
      "Episode:407 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:0.1632\n",
      "Episode:408 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.2933\n",
      "Episode:409 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.1841\n",
      "Episode:410 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.3213\n",
      "Episode:411 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.2796\n",
      "Episode:412 meanR:9.3800 rate:0.0220 gloss:0.0000 dloss:0.2248\n",
      "Episode:413 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.1898\n",
      "Episode:414 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:0.2033\n",
      "Episode:415 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.2129\n",
      "Episode:416 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.2447\n",
      "Episode:417 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:0.3676\n",
      "Episode:418 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:0.5997\n",
      "Episode:419 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:0.4412\n",
      "Episode:420 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:0.6194\n",
      "Episode:421 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.0490\n",
      "Episode:422 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.6983\n",
      "Episode:423 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:2.0269\n",
      "Episode:424 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.4619\n",
      "Episode:425 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.4821\n",
      "Episode:426 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.1431\n",
      "Episode:427 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.0051\n",
      "Episode:428 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.9886\n",
      "Episode:429 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.3362\n",
      "Episode:430 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.5699\n",
      "Episode:431 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.2887\n",
      "Episode:432 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.3460\n",
      "Episode:433 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:1.4703\n",
      "Episode:434 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.5196\n",
      "Episode:435 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.5309\n",
      "Episode:436 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.4914\n",
      "Episode:437 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.5416\n",
      "Episode:438 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.4192\n",
      "Episode:439 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5011\n",
      "Episode:440 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.4502\n",
      "Episode:441 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.4517\n",
      "Episode:442 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.4826\n",
      "Episode:443 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.4567\n",
      "Episode:444 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.5185\n",
      "Episode:445 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5468\n",
      "Episode:446 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.6134\n",
      "Episode:447 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:1.3992\n",
      "Episode:448 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.5442\n",
      "Episode:449 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.5013\n",
      "Episode:450 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.5205\n",
      "Episode:451 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4203\n",
      "Episode:452 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4360\n",
      "Episode:453 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4753\n",
      "Episode:454 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.4935\n",
      "Episode:455 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:1.5592\n",
      "Episode:456 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.5098\n",
      "Episode:457 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.5490\n",
      "Episode:458 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4395\n",
      "Episode:459 meanR:9.3300 rate:0.0220 gloss:0.0000 dloss:1.3180\n",
      "Episode:460 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.4177\n",
      "Episode:461 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.5599\n",
      "Episode:462 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.4859\n",
      "Episode:463 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.5385\n",
      "Episode:464 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4889\n",
      "Episode:465 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.5366\n",
      "Episode:466 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4858\n",
      "Episode:467 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3313\n",
      "Episode:468 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.3945\n",
      "Episode:469 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.4362\n",
      "Episode:470 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4579\n",
      "Episode:471 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.4099\n",
      "Episode:472 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.2245\n",
      "Episode:473 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.2455\n",
      "Episode:474 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4656\n",
      "Episode:475 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4371\n",
      "Episode:476 meanR:9.3000 rate:0.0220 gloss:0.0000 dloss:1.5056\n",
      "Episode:477 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4509\n",
      "Episode:478 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3459\n",
      "Episode:479 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.4548\n",
      "Episode:480 meanR:9.3100 rate:0.0220 gloss:0.0000 dloss:1.3987\n",
      "Episode:481 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.3524\n",
      "Episode:482 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.3651\n",
      "Episode:483 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3975\n",
      "Episode:484 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.4818\n",
      "Episode:485 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.4686\n",
      "Episode:486 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4166\n",
      "Episode:487 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.3983\n",
      "Episode:488 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:1.4150\n",
      "Episode:489 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.2639\n",
      "Episode:490 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3003\n",
      "Episode:491 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3871\n",
      "Episode:492 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.2153\n",
      "Episode:493 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.1104\n",
      "Episode:494 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:1.0237\n",
      "Episode:495 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.2079\n",
      "Episode:496 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.4949\n",
      "Episode:497 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4979\n",
      "Episode:498 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3007\n",
      "Episode:499 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.4261\n",
      "Episode:500 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.4088\n",
      "Episode:501 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.3850\n",
      "Episode:502 meanR:9.2900 rate:0.0160 gloss:0.0000 dloss:1.4175\n",
      "Episode:503 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4766\n",
      "Episode:504 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.4052\n",
      "Episode:505 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:1.4609\n",
      "Episode:506 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:1.4335\n",
      "Episode:507 meanR:9.2400 rate:0.0160 gloss:0.0000 dloss:1.3559\n",
      "Episode:508 meanR:9.2300 rate:0.0180 gloss:0.0000 dloss:1.2692\n",
      "Episode:509 meanR:9.2300 rate:0.0200 gloss:0.0000 dloss:1.3240\n",
      "Episode:510 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:1.3349\n",
      "Episode:511 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:1.4016\n",
      "Episode:512 meanR:9.2300 rate:0.0180 gloss:0.0000 dloss:1.3732\n",
      "Episode:513 meanR:9.2400 rate:0.0220 gloss:0.0000 dloss:1.4429\n",
      "Episode:514 meanR:9.2300 rate:0.0160 gloss:0.0000 dloss:1.2433\n",
      "Episode:515 meanR:9.2400 rate:0.0180 gloss:0.0000 dloss:1.3433\n",
      "Episode:516 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.3335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:517 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.1963\n",
      "Episode:518 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.1663\n",
      "Episode:519 meanR:9.2700 rate:0.0160 gloss:0.0000 dloss:1.2581\n",
      "Episode:520 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.2947\n",
      "Episode:521 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.3663\n",
      "Episode:522 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.3952\n",
      "Episode:523 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.4859\n",
      "Episode:524 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:1.4436\n",
      "Episode:525 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.3865\n",
      "Episode:526 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:1.2913\n",
      "Episode:527 meanR:9.2700 rate:0.0160 gloss:0.0000 dloss:1.3231\n",
      "Episode:528 meanR:9.2900 rate:0.0220 gloss:0.0000 dloss:1.3959\n",
      "Episode:529 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.3987\n",
      "Episode:530 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.2577\n",
      "Episode:531 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.2908\n",
      "Episode:532 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.2538\n",
      "Episode:533 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.3204\n",
      "Episode:534 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.3453\n",
      "Episode:535 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.2735\n",
      "Episode:536 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3525\n",
      "Episode:537 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3688\n",
      "Episode:538 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.4535\n",
      "Episode:539 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4187\n",
      "Episode:540 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3671\n",
      "Episode:541 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3106\n",
      "Episode:542 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3089\n",
      "Episode:543 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4545\n",
      "Episode:544 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.2970\n",
      "Episode:545 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.3606\n",
      "Episode:546 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.4607\n",
      "Episode:547 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.3429\n",
      "Episode:548 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3974\n",
      "Episode:549 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.3393\n",
      "Episode:550 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3746\n",
      "Episode:551 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4320\n",
      "Episode:552 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.3995\n",
      "Episode:553 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.3724\n",
      "Episode:554 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.3746\n",
      "Episode:555 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4531\n",
      "Episode:556 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.3912\n",
      "Episode:557 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.2261\n",
      "Episode:558 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.3176\n",
      "Episode:559 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:1.2200\n",
      "Episode:560 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3263\n",
      "Episode:561 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4389\n",
      "Episode:562 meanR:9.2900 rate:0.0160 gloss:0.0000 dloss:1.3672\n",
      "Episode:563 meanR:9.2700 rate:0.0160 gloss:0.0000 dloss:1.4492\n",
      "Episode:564 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.2637\n",
      "Episode:565 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:1.4127\n",
      "Episode:566 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:1.4153\n",
      "Episode:567 meanR:9.2400 rate:0.0160 gloss:0.0000 dloss:1.4609\n",
      "Episode:568 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:1.4898\n",
      "Episode:569 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:1.3197\n",
      "Episode:570 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.3441\n",
      "Episode:571 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.3243\n",
      "Episode:572 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:1.2959\n",
      "Episode:573 meanR:9.2400 rate:0.0180 gloss:0.0000 dloss:1.2621\n",
      "Episode:574 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:1.3730\n",
      "Episode:575 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:1.3789\n",
      "Episode:576 meanR:9.2400 rate:0.0200 gloss:0.0000 dloss:1.3639\n",
      "Episode:577 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:1.3728\n",
      "Episode:578 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.0875\n",
      "Episode:579 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.2554\n",
      "Episode:580 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.2891\n",
      "Episode:581 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.3291\n",
      "Episode:582 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.3258\n",
      "Episode:583 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:1.4378\n",
      "Episode:584 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.4001\n",
      "Episode:585 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:1.3305\n",
      "Episode:586 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:1.4033\n",
      "Episode:587 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:1.4533\n",
      "Episode:588 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:1.3587\n",
      "Episode:589 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:1.4681\n",
      "Episode:590 meanR:9.2700 rate:0.0160 gloss:0.0000 dloss:1.4848\n",
      "Episode:591 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:1.4848\n",
      "Episode:592 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:1.4802\n",
      "Episode:593 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4540\n",
      "Episode:594 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.3831\n",
      "Episode:595 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.4470\n",
      "Episode:596 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.4350\n",
      "Episode:597 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4618\n",
      "Episode:598 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.2457\n",
      "Episode:599 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3597\n",
      "Episode:600 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:1.4874\n",
      "Episode:601 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.3729\n",
      "Episode:602 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.4119\n",
      "Episode:603 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.2939\n",
      "Episode:604 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.4236\n",
      "Episode:605 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3988\n",
      "Episode:606 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4880\n",
      "Episode:607 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.4113\n",
      "Episode:608 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.3108\n",
      "Episode:609 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.3456\n",
      "Episode:610 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.3642\n",
      "Episode:611 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.3204\n",
      "Episode:612 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.3690\n",
      "Episode:613 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.2341\n",
      "Episode:614 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.2882\n",
      "Episode:615 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.3075\n",
      "Episode:616 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.2902\n",
      "Episode:617 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.3091\n",
      "Episode:618 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:1.3193\n",
      "Episode:619 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4194\n",
      "Episode:620 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4303\n",
      "Episode:621 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.3103\n",
      "Episode:622 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.4649\n",
      "Episode:623 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4157\n",
      "Episode:624 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.3355\n",
      "Episode:625 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.3382\n",
      "Episode:626 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.4026\n",
      "Episode:627 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.4137\n",
      "Episode:628 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4807\n",
      "Episode:629 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.1135\n",
      "Episode:630 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:1.1732\n",
      "Episode:631 meanR:9.2900 rate:0.0160 gloss:0.0000 dloss:1.3739\n",
      "Episode:632 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.3449\n",
      "Episode:633 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.4469\n",
      "Episode:634 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.3043\n",
      "Episode:635 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.4383\n",
      "Episode:636 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.4385\n",
      "Episode:637 meanR:9.2800 rate:0.0160 gloss:0.0000 dloss:1.4801\n",
      "Episode:638 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.2643\n",
      "Episode:639 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.3496\n",
      "Episode:640 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.4026\n",
      "Episode:641 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:1.4868\n",
      "Episode:642 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.4664\n",
      "Episode:643 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:1.2783\n",
      "Episode:644 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:1.3590\n",
      "Episode:645 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.4202\n",
      "Episode:646 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4878\n",
      "Episode:647 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:648 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.2954\n",
      "Episode:649 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.4238\n",
      "Episode:650 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:1.3507\n",
      "Episode:651 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.4796\n",
      "Episode:652 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.3911\n",
      "Episode:653 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4590\n",
      "Episode:654 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4616\n",
      "Episode:655 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:1.4422\n",
      "Episode:656 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3082\n",
      "Episode:657 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:1.5117\n",
      "Episode:658 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:1.3379\n",
      "Episode:659 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.4419\n",
      "Episode:660 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.4453\n",
      "Episode:661 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.3437\n",
      "Episode:662 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.4087\n",
      "Episode:663 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.3524\n",
      "Episode:664 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:1.4214\n",
      "Episode:665 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:1.4536\n",
      "Episode:666 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:1.5734\n",
      "Episode:667 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5296\n",
      "Episode:668 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5201\n",
      "Episode:669 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5050\n",
      "Episode:670 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.4648\n",
      "Episode:671 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.4643\n",
      "Episode:672 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.4445\n",
      "Episode:673 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.5127\n",
      "Episode:674 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5585\n",
      "Episode:675 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5746\n",
      "Episode:676 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.5324\n",
      "Episode:677 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.2495\n",
      "Episode:678 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.3699\n",
      "Episode:679 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.4386\n",
      "Episode:680 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.5543\n",
      "Episode:681 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.5387\n",
      "Episode:682 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.5716\n",
      "Episode:683 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.5520\n",
      "Episode:684 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.4281\n",
      "Episode:685 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:1.4136\n",
      "Episode:686 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:1.4713\n",
      "Episode:687 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.5015\n",
      "Episode:688 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.5473\n",
      "Episode:689 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:1.5252\n",
      "Episode:690 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.5736\n",
      "Episode:691 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.4958\n",
      "Episode:692 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.4630\n",
      "Episode:693 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5369\n",
      "Episode:694 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5151\n",
      "Episode:695 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.3727\n",
      "Episode:696 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.4899\n",
      "Episode:697 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5736\n",
      "Episode:698 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:1.3863\n",
      "Episode:699 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.5447\n",
      "Episode:700 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.6287\n",
      "Episode:701 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5953\n",
      "Episode:702 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.4792\n",
      "Episode:703 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.5055\n",
      "Episode:704 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.5184\n",
      "Episode:705 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5562\n",
      "Episode:706 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.4837\n",
      "Episode:707 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.4942\n",
      "Episode:708 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.4352\n",
      "Episode:709 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.5053\n",
      "Episode:710 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.4913\n",
      "Episode:711 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:1.4168\n",
      "Episode:712 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5689\n",
      "Episode:713 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:1.5983\n",
      "Episode:714 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.5847\n",
      "Episode:715 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.5255\n",
      "Episode:716 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5943\n",
      "Episode:717 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.6322\n",
      "Episode:718 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.6519\n",
      "Episode:719 meanR:9.4100 rate:0.0220 gloss:0.0000 dloss:1.6148\n",
      "Episode:720 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.5061\n",
      "Episode:721 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5548\n",
      "Episode:722 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.5237\n",
      "Episode:723 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.5810\n",
      "Episode:724 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.3607\n",
      "Episode:725 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:1.4944\n",
      "Episode:726 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4266\n",
      "Episode:727 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.5376\n",
      "Episode:728 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.5083\n",
      "Episode:729 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:1.5261\n",
      "Episode:730 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.5007\n",
      "Episode:731 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:1.5349\n",
      "Episode:732 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.5517\n",
      "Episode:733 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4786\n",
      "Episode:734 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.2717\n",
      "Episode:735 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.6554\n",
      "Episode:736 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:1.5241\n",
      "Episode:737 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.4293\n",
      "Episode:738 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5067\n",
      "Episode:739 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.3863\n",
      "Episode:740 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.3661\n",
      "Episode:741 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.4436\n",
      "Episode:742 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.3155\n",
      "Episode:743 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.2840\n",
      "Episode:744 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.3497\n",
      "Episode:745 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.4321\n",
      "Episode:746 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.3691\n",
      "Episode:747 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.4062\n",
      "Episode:748 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5062\n",
      "Episode:749 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.3952\n",
      "Episode:750 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.5110\n",
      "Episode:751 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.4917\n",
      "Episode:752 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4101\n",
      "Episode:753 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.5176\n",
      "Episode:754 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.4673\n",
      "Episode:755 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5376\n",
      "Episode:756 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.4534\n",
      "Episode:757 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:1.4841\n",
      "Episode:758 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.5142\n",
      "Episode:759 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.4846\n",
      "Episode:760 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.5051\n",
      "Episode:761 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.4948\n",
      "Episode:762 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5599\n",
      "Episode:763 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.5878\n",
      "Episode:764 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.5256\n",
      "Episode:765 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.4318\n",
      "Episode:766 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.4578\n",
      "Episode:767 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5381\n",
      "Episode:768 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4992\n",
      "Episode:769 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.4003\n",
      "Episode:770 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.4538\n",
      "Episode:771 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.5891\n",
      "Episode:772 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5525\n",
      "Episode:773 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.5857\n",
      "Episode:774 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.5944\n",
      "Episode:775 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.5587\n",
      "Episode:776 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.5520\n",
      "Episode:777 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.5576\n",
      "Episode:778 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.4622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:779 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.4995\n",
      "Episode:780 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4626\n",
      "Episode:781 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.5155\n",
      "Episode:782 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5611\n",
      "Episode:783 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5519\n",
      "Episode:784 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.4782\n",
      "Episode:785 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.4981\n",
      "Episode:786 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.5238\n",
      "Episode:787 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.5904\n",
      "Episode:788 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5422\n",
      "Episode:789 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5379\n",
      "Episode:790 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:1.6298\n",
      "Episode:791 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.4623\n",
      "Episode:792 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:1.5118\n",
      "Episode:793 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:1.5302\n",
      "Episode:794 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.4892\n",
      "Episode:795 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.4269\n",
      "Episode:796 meanR:9.3800 rate:0.0220 gloss:0.0000 dloss:1.4901\n",
      "Episode:797 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.4818\n",
      "Episode:798 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:1.5463\n",
      "Episode:799 meanR:9.3900 rate:0.0220 gloss:0.0000 dloss:1.4321\n",
      "Episode:800 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.2816\n",
      "Episode:801 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:1.3631\n",
      "Episode:802 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.4498\n",
      "Episode:803 meanR:9.4000 rate:0.0220 gloss:0.0000 dloss:1.3259\n",
      "Episode:804 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.3833\n",
      "Episode:805 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.4534\n",
      "Episode:806 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5496\n",
      "Episode:807 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:1.5944\n",
      "Episode:808 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:1.5773\n",
      "Episode:809 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.2929\n",
      "Episode:810 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.5173\n",
      "Episode:811 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:1.4933\n",
      "Episode:812 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.4544\n",
      "Episode:813 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.7335\n",
      "Episode:814 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:1.5504\n",
      "Episode:815 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5607\n",
      "Episode:816 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5229\n",
      "Episode:817 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:1.6394\n",
      "Episode:818 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:1.6000\n",
      "Episode:819 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.6259\n",
      "Episode:820 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5710\n",
      "Episode:821 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5826\n",
      "Episode:822 meanR:9.4100 rate:0.0160 gloss:0.0000 dloss:1.5783\n",
      "Episode:823 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.5400\n",
      "Episode:824 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:1.5216\n",
      "Episode:825 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.5072\n",
      "Episode:826 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.4898\n",
      "Episode:827 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:1.5386\n",
      "Episode:828 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:1.5594\n",
      "Episode:829 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:1.6227\n",
      "Episode:830 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:1.6037\n",
      "Episode:831 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:1.5923\n",
      "Episode:832 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:1.5611\n",
      "Episode:833 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:1.6447\n",
      "Episode:834 meanR:9.4800 rate:0.0220 gloss:0.0000 dloss:1.6395\n",
      "Episode:835 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:1.3382\n",
      "Episode:836 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:1.5241\n",
      "Episode:837 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:1.4480\n",
      "Episode:838 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:1.5409\n",
      "Episode:839 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.5579\n",
      "Episode:840 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.6097\n",
      "Episode:841 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:1.6518\n",
      "Episode:842 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:1.6588\n",
      "Episode:843 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.6821\n",
      "Episode:844 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.5984\n",
      "Episode:845 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.6204\n",
      "Episode:846 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.5674\n",
      "Episode:847 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.2949\n",
      "Episode:848 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.6084\n",
      "Episode:849 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:0.4060\n",
      "Episode:850 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.4356\n",
      "Episode:851 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.4665\n",
      "Episode:852 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.5472\n",
      "Episode:853 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:0.5520\n",
      "Episode:854 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:0.5360\n",
      "Episode:855 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.7351\n",
      "Episode:856 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.6718\n",
      "Episode:857 meanR:9.5300 rate:0.0200 gloss:0.0000 dloss:0.7576\n",
      "Episode:858 meanR:9.5200 rate:0.0180 gloss:0.0000 dloss:1.3578\n",
      "Episode:859 meanR:9.5300 rate:0.0200 gloss:0.0000 dloss:1.3701\n",
      "Episode:860 meanR:9.5300 rate:0.0180 gloss:0.0000 dloss:1.4765\n",
      "Episode:861 meanR:9.5200 rate:0.0180 gloss:0.0000 dloss:1.4811\n",
      "Episode:862 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:1.4251\n",
      "Episode:863 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.4661\n",
      "Episode:864 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.5017\n",
      "Episode:865 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:1.4930\n",
      "Episode:866 meanR:9.5000 rate:0.0220 gloss:0.0000 dloss:1.5037\n",
      "Episode:867 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:1.4484\n",
      "Episode:868 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.4627\n",
      "Episode:869 meanR:9.4900 rate:0.0160 gloss:0.0000 dloss:1.4786\n",
      "Episode:870 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.4840\n",
      "Episode:871 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:1.5845\n",
      "Episode:872 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.4666\n",
      "Episode:873 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.3510\n",
      "Episode:874 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.3971\n",
      "Episode:875 meanR:9.5200 rate:0.0200 gloss:0.0000 dloss:1.3908\n",
      "Episode:876 meanR:9.5200 rate:0.0200 gloss:0.0000 dloss:1.3531\n",
      "Episode:877 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:1.3727\n",
      "Episode:878 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:1.3910\n",
      "Episode:879 meanR:9.5000 rate:0.0160 gloss:0.0000 dloss:1.0340\n",
      "Episode:880 meanR:9.5200 rate:0.0220 gloss:0.0000 dloss:0.3066\n",
      "Episode:881 meanR:9.5000 rate:0.0160 gloss:0.0000 dloss:0.1717\n",
      "Episode:882 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.1233\n",
      "Episode:883 meanR:9.5000 rate:0.0160 gloss:0.0000 dloss:0.1827\n",
      "Episode:884 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:0.2680\n",
      "Episode:885 meanR:9.5100 rate:0.0220 gloss:0.0000 dloss:0.2337\n",
      "Episode:886 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:0.3161\n",
      "Episode:887 meanR:9.5300 rate:0.0220 gloss:0.0000 dloss:0.6483\n",
      "Episode:888 meanR:9.5300 rate:0.0180 gloss:0.0000 dloss:0.6319\n",
      "Episode:889 meanR:9.5400 rate:0.0200 gloss:0.0000 dloss:0.8777\n",
      "Episode:890 meanR:9.5600 rate:0.0200 gloss:0.0000 dloss:1.2966\n",
      "Episode:891 meanR:9.5600 rate:0.0200 gloss:0.0000 dloss:1.3071\n",
      "Episode:892 meanR:9.5600 rate:0.0180 gloss:0.0000 dloss:1.3243\n",
      "Episode:893 meanR:9.5800 rate:0.0200 gloss:0.0000 dloss:1.0927\n",
      "Episode:894 meanR:9.5800 rate:0.0200 gloss:0.0000 dloss:0.7124\n",
      "Episode:895 meanR:9.5800 rate:0.0200 gloss:0.0000 dloss:0.9786\n",
      "Episode:896 meanR:9.5600 rate:0.0180 gloss:0.0000 dloss:1.3373\n",
      "Episode:897 meanR:9.5600 rate:0.0200 gloss:0.0000 dloss:0.9236\n",
      "Episode:898 meanR:9.5700 rate:0.0180 gloss:0.0000 dloss:0.7892\n",
      "Episode:899 meanR:9.5400 rate:0.0160 gloss:0.0000 dloss:0.4202\n",
      "Episode:900 meanR:9.5400 rate:0.0200 gloss:0.0000 dloss:0.0751\n",
      "Episode:901 meanR:9.5300 rate:0.0180 gloss:0.0000 dloss:0.0617\n",
      "Episode:902 meanR:9.5400 rate:0.0200 gloss:0.0000 dloss:0.0619\n",
      "Episode:903 meanR:9.5100 rate:0.0160 gloss:0.0000 dloss:0.0411\n",
      "Episode:904 meanR:9.5200 rate:0.0200 gloss:0.0000 dloss:0.0389\n",
      "Episode:905 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:0.0305\n",
      "Episode:906 meanR:9.5200 rate:0.0200 gloss:0.0000 dloss:0.0277\n",
      "Episode:907 meanR:9.5100 rate:0.0160 gloss:0.0000 dloss:0.0415\n",
      "Episode:908 meanR:9.5200 rate:0.0200 gloss:0.0000 dloss:0.0382\n",
      "Episode:909 meanR:9.5100 rate:0.0180 gloss:0.0000 dloss:0.0952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:910 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:0.0609\n",
      "Episode:911 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.0854\n",
      "Episode:912 meanR:9.4900 rate:0.0160 gloss:0.0000 dloss:0.0921\n",
      "Episode:913 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:0.0721\n",
      "Episode:914 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:0.0782\n",
      "Episode:915 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:0.0618\n",
      "Episode:916 meanR:9.4700 rate:0.0180 gloss:0.0000 dloss:0.0758\n",
      "Episode:917 meanR:9.4700 rate:0.0180 gloss:0.0000 dloss:0.0681\n",
      "Episode:918 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:0.0646\n",
      "Episode:919 meanR:9.4700 rate:0.0200 gloss:0.0000 dloss:0.0744\n",
      "Episode:920 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:0.0683\n",
      "Episode:921 meanR:9.4500 rate:0.0180 gloss:0.0000 dloss:0.0564\n",
      "Episode:922 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:0.0704\n",
      "Episode:923 meanR:9.4700 rate:0.0200 gloss:0.0000 dloss:0.0514\n",
      "Episode:924 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:0.0849\n",
      "Episode:925 meanR:9.5000 rate:0.0220 gloss:0.0000 dloss:0.0565\n",
      "Episode:926 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.0313\n",
      "Episode:927 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.0216\n",
      "Episode:928 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.0151\n",
      "Episode:929 meanR:9.5100 rate:0.0200 gloss:0.0000 dloss:0.0097\n",
      "Episode:930 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:0.0095\n",
      "Episode:931 meanR:9.5000 rate:0.0180 gloss:0.0000 dloss:0.0114\n",
      "Episode:932 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:0.1061\n",
      "Episode:933 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:0.0202\n",
      "Episode:934 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:0.0725\n",
      "Episode:935 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:0.0653\n",
      "Episode:936 meanR:9.4900 rate:0.0180 gloss:0.0000 dloss:0.1249\n",
      "Episode:937 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.1588\n",
      "Episode:938 meanR:9.5000 rate:0.0200 gloss:0.0000 dloss:0.5385\n",
      "Episode:939 meanR:9.4800 rate:0.0160 gloss:0.0000 dloss:1.7959\n",
      "Episode:940 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:2.1424\n",
      "Episode:941 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:3.0549\n",
      "Episode:942 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:3.0065\n",
      "Episode:943 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:3.0160\n",
      "Episode:944 meanR:9.4900 rate:0.0200 gloss:0.0000 dloss:3.0842\n",
      "Episode:945 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:3.0441\n",
      "Episode:946 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:3.1001\n",
      "Episode:947 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:3.1459\n",
      "Episode:948 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:3.3693\n",
      "Episode:949 meanR:9.4800 rate:0.0180 gloss:0.0000 dloss:3.3188\n",
      "Episode:950 meanR:9.4700 rate:0.0180 gloss:0.0000 dloss:3.3669\n",
      "Episode:951 meanR:9.4700 rate:0.0200 gloss:0.0000 dloss:3.5280\n",
      "Episode:952 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:3.5538\n",
      "Episode:953 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:3.5703\n",
      "Episode:954 meanR:9.4700 rate:0.0200 gloss:0.0000 dloss:4.2390\n",
      "Episode:955 meanR:9.4500 rate:0.0160 gloss:0.0000 dloss:4.1957\n",
      "Episode:956 meanR:9.4400 rate:0.0180 gloss:0.0000 dloss:4.1569\n",
      "Episode:957 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:4.2380\n",
      "Episode:958 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:4.2012\n",
      "Episode:959 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:4.3454\n",
      "Episode:960 meanR:9.4500 rate:0.0180 gloss:0.0000 dloss:4.3642\n",
      "Episode:961 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:4.3322\n",
      "Episode:962 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:4.3254\n",
      "Episode:963 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:4.4237\n",
      "Episode:964 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:4.5414\n",
      "Episode:965 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:4.4915\n",
      "Episode:966 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:4.4287\n",
      "Episode:967 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:4.4751\n",
      "Episode:968 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:4.3964\n",
      "Episode:969 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:4.6561\n",
      "Episode:970 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:4.5989\n",
      "Episode:971 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:4.5388\n",
      "Episode:972 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:5.0076\n",
      "Episode:973 meanR:9.4800 rate:0.0200 gloss:0.0000 dloss:5.1550\n",
      "Episode:974 meanR:9.4700 rate:0.0180 gloss:0.0000 dloss:5.2100\n",
      "Episode:975 meanR:9.4700 rate:0.0200 gloss:0.0000 dloss:5.1799\n",
      "Episode:976 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:5.2459\n",
      "Episode:977 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:5.3042\n",
      "Episode:978 meanR:9.4500 rate:0.0180 gloss:0.0000 dloss:5.3486\n",
      "Episode:979 meanR:9.4600 rate:0.0180 gloss:0.0000 dloss:5.3177\n",
      "Episode:980 meanR:9.4400 rate:0.0180 gloss:0.0000 dloss:5.4459\n",
      "Episode:981 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:5.5018\n",
      "Episode:982 meanR:9.4500 rate:0.0180 gloss:0.0000 dloss:5.5347\n",
      "Episode:983 meanR:9.4500 rate:0.0160 gloss:0.0000 dloss:5.5532\n",
      "Episode:984 meanR:9.4600 rate:0.0200 gloss:0.0000 dloss:5.4983\n",
      "Episode:985 meanR:9.4400 rate:0.0180 gloss:0.0000 dloss:5.3186\n",
      "Episode:986 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:5.3746\n",
      "Episode:987 meanR:9.4200 rate:0.0160 gloss:0.0000 dloss:5.3315\n",
      "Episode:988 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:5.8913\n",
      "Episode:989 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:5.6815\n",
      "Episode:990 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:5.3849\n",
      "Episode:991 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:5.7472\n",
      "Episode:992 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:5.6372\n",
      "Episode:993 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:5.8353\n",
      "Episode:994 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:6.0165\n",
      "Episode:995 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:6.0445\n",
      "Episode:996 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:5.9170\n",
      "Episode:997 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:5.9579\n",
      "Episode:998 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:5.8833\n",
      "Episode:999 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:6.4880\n",
      "Episode:1000 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:6.2706\n",
      "Episode:1001 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.3457\n",
      "Episode:1002 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:7.7895\n",
      "Episode:1003 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.8873\n",
      "Episode:1004 meanR:9.4000 rate:0.0220 gloss:0.0000 dloss:8.0056\n",
      "Episode:1005 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.0489\n",
      "Episode:1006 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:8.3756\n",
      "Episode:1007 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:8.4052\n",
      "Episode:1008 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:8.5335\n",
      "Episode:1009 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.3856\n",
      "Episode:1010 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.5098\n",
      "Episode:1011 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:8.3343\n",
      "Episode:1012 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.5098\n",
      "Episode:1013 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.0287\n",
      "Episode:1014 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.5059\n",
      "Episode:1015 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:8.1042\n",
      "Episode:1016 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:8.2797\n",
      "Episode:1017 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:8.3458\n",
      "Episode:1018 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:5.8037\n",
      "Episode:1019 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:2.3439\n",
      "Episode:1020 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:0.5836\n",
      "Episode:1021 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.4120\n",
      "Episode:1022 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:0.3609\n",
      "Episode:1023 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:0.4159\n",
      "Episode:1024 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:0.5058\n",
      "Episode:1025 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.6642\n",
      "Episode:1026 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.6343\n",
      "Episode:1027 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:0.6888\n",
      "Episode:1028 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:0.7386\n",
      "Episode:1029 meanR:9.3600 rate:0.0220 gloss:0.0000 dloss:0.7530\n",
      "Episode:1030 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:0.8471\n",
      "Episode:1031 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:0.7965\n",
      "Episode:1032 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.0064\n",
      "Episode:1033 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:1.2428\n",
      "Episode:1034 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.9194\n",
      "Episode:1035 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.4650\n",
      "Episode:1036 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:0.8181\n",
      "Episode:1037 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.1009\n",
      "Episode:1038 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:0.7997\n",
      "Episode:1039 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.7817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1040 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:2.3662\n",
      "Episode:1041 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:3.0878\n",
      "Episode:1042 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:4.1870\n",
      "Episode:1043 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:4.7223\n",
      "Episode:1044 meanR:9.3400 rate:0.0220 gloss:0.0000 dloss:4.6748\n",
      "Episode:1045 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:5.1460\n",
      "Episode:1046 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:3.7156\n",
      "Episode:1047 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:3.3249\n",
      "Episode:1048 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:3.7410\n",
      "Episode:1049 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:3.3688\n",
      "Episode:1050 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:3.1476\n",
      "Episode:1051 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:3.0425\n",
      "Episode:1052 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:0.7532\n",
      "Episode:1053 meanR:9.3900 rate:0.0220 gloss:0.0000 dloss:0.7724\n",
      "Episode:1054 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.7251\n",
      "Episode:1055 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.6523\n",
      "Episode:1056 meanR:9.4300 rate:0.0220 gloss:0.0000 dloss:0.5484\n",
      "Episode:1057 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:0.7841\n",
      "Episode:1058 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:0.7505\n",
      "Episode:1059 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:0.7026\n",
      "Episode:1060 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:0.7257\n",
      "Episode:1061 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:0.7257\n",
      "Episode:1062 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:0.6791\n",
      "Episode:1063 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:0.6809\n",
      "Episode:1064 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:0.6513\n",
      "Episode:1065 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:0.6881\n",
      "Episode:1066 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:0.6385\n",
      "Episode:1067 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:0.6886\n",
      "Episode:1068 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:0.6545\n",
      "Episode:1069 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:0.6389\n",
      "Episode:1070 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.8047\n",
      "Episode:1071 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:0.7999\n",
      "Episode:1072 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:0.7319\n",
      "Episode:1073 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.7499\n",
      "Episode:1074 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.8100\n",
      "Episode:1075 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:0.7515\n",
      "Episode:1076 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:0.8449\n",
      "Episode:1077 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:0.8153\n",
      "Episode:1078 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:0.8469\n",
      "Episode:1079 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:0.8718\n",
      "Episode:1080 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:0.8776\n",
      "Episode:1081 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.8358\n",
      "Episode:1082 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.8066\n",
      "Episode:1083 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.7913\n",
      "Episode:1084 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:0.7739\n",
      "Episode:1085 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.7410\n",
      "Episode:1086 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.9599\n",
      "Episode:1087 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.9240\n",
      "Episode:1088 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:0.9429\n",
      "Episode:1089 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.9982\n",
      "Episode:1090 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.0844\n",
      "Episode:1091 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:0.9929\n",
      "Episode:1092 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:0.9594\n",
      "Episode:1093 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.8964\n",
      "Episode:1094 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:0.9863\n",
      "Episode:1095 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:0.9804\n",
      "Episode:1096 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:0.9287\n",
      "Episode:1097 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:0.9291\n",
      "Episode:1098 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:0.9665\n",
      "Episode:1099 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:1.0400\n",
      "Episode:1100 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:0.9755\n",
      "Episode:1101 meanR:9.3200 rate:0.0220 gloss:0.0000 dloss:0.9486\n",
      "Episode:1102 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.0260\n",
      "Episode:1103 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.0105\n",
      "Episode:1104 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:1.0407\n",
      "Episode:1105 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.1366\n",
      "Episode:1106 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.1201\n",
      "Episode:1107 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:1.1264\n",
      "Episode:1108 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.1741\n",
      "Episode:1109 meanR:9.3600 rate:0.0220 gloss:0.0000 dloss:1.1144\n",
      "Episode:1110 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:1.2080\n",
      "Episode:1111 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.2320\n",
      "Episode:1112 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.1806\n",
      "Episode:1113 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:1.1895\n",
      "Episode:1114 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:1.1416\n",
      "Episode:1115 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.2448\n",
      "Episode:1116 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.2167\n",
      "Episode:1117 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.2088\n",
      "Episode:1118 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.2558\n",
      "Episode:1119 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.2265\n",
      "Episode:1120 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:1.2459\n",
      "Episode:1121 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:1.2020\n",
      "Episode:1122 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:1.1653\n",
      "Episode:1123 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:1.8124\n",
      "Episode:1124 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:1.4132\n",
      "Episode:1125 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.5230\n",
      "Episode:1126 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:1.6737\n",
      "Episode:1127 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:1.8195\n",
      "Episode:1128 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:1.9257\n",
      "Episode:1129 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:2.0685\n",
      "Episode:1130 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:2.1138\n",
      "Episode:1131 meanR:9.3700 rate:0.0220 gloss:0.0000 dloss:2.2127\n",
      "Episode:1132 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:2.3217\n",
      "Episode:1133 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:2.3880\n",
      "Episode:1134 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:2.4707\n",
      "Episode:1135 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:2.6331\n",
      "Episode:1136 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:2.7466\n",
      "Episode:1137 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:2.7984\n",
      "Episode:1138 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:2.9429\n",
      "Episode:1139 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:2.9930\n",
      "Episode:1140 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:3.2360\n",
      "Episode:1141 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:3.1436\n",
      "Episode:1142 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:3.1699\n",
      "Episode:1143 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:3.4637\n",
      "Episode:1144 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:3.2156\n",
      "Episode:1145 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:3.5077\n",
      "Episode:1146 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:3.8908\n",
      "Episode:1147 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:4.0271\n",
      "Episode:1148 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:4.1510\n",
      "Episode:1149 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:4.2670\n",
      "Episode:1150 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:4.4462\n",
      "Episode:1151 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:4.5579\n",
      "Episode:1152 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:4.3835\n",
      "Episode:1153 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.5148\n",
      "Episode:1154 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:4.5919\n",
      "Episode:1155 meanR:9.2400 rate:0.0180 gloss:0.0000 dloss:4.6715\n",
      "Episode:1156 meanR:9.2200 rate:0.0180 gloss:0.0000 dloss:5.0557\n",
      "Episode:1157 meanR:9.2000 rate:0.0160 gloss:0.0000 dloss:5.2764\n",
      "Episode:1158 meanR:9.2000 rate:0.0180 gloss:0.0000 dloss:5.2346\n",
      "Episode:1159 meanR:9.2000 rate:0.0180 gloss:0.0000 dloss:5.4369\n",
      "Episode:1160 meanR:9.2100 rate:0.0200 gloss:0.0000 dloss:5.1568\n",
      "Episode:1161 meanR:9.2300 rate:0.0200 gloss:0.0000 dloss:5.2517\n",
      "Episode:1162 meanR:9.2200 rate:0.0180 gloss:0.0000 dloss:5.1422\n",
      "Episode:1163 meanR:9.2200 rate:0.0180 gloss:0.0000 dloss:5.3981\n",
      "Episode:1164 meanR:9.2000 rate:0.0160 gloss:0.0000 dloss:5.3038\n",
      "Episode:1165 meanR:9.2100 rate:0.0200 gloss:0.0000 dloss:5.5378\n",
      "Episode:1166 meanR:9.2000 rate:0.0160 gloss:0.0000 dloss:5.6130\n",
      "Episode:1167 meanR:9.2100 rate:0.0200 gloss:0.0000 dloss:5.6870\n",
      "Episode:1168 meanR:9.2100 rate:0.0200 gloss:0.0000 dloss:5.4279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1169 meanR:9.2100 rate:0.0180 gloss:0.0000 dloss:5.4490\n",
      "Episode:1170 meanR:9.1900 rate:0.0160 gloss:0.0000 dloss:5.8019\n",
      "Episode:1171 meanR:9.1800 rate:0.0180 gloss:0.0000 dloss:5.4627\n",
      "Episode:1172 meanR:9.2000 rate:0.0200 gloss:0.0000 dloss:5.4283\n",
      "Episode:1173 meanR:9.2100 rate:0.0200 gloss:0.0000 dloss:5.1383\n",
      "Episode:1174 meanR:9.2100 rate:0.0180 gloss:0.0000 dloss:5.3684\n",
      "Episode:1175 meanR:9.2000 rate:0.0180 gloss:0.0000 dloss:5.3963\n",
      "Episode:1176 meanR:9.2000 rate:0.0180 gloss:0.0000 dloss:5.3668\n",
      "Episode:1177 meanR:9.2100 rate:0.0180 gloss:0.0000 dloss:5.4246\n",
      "Episode:1178 meanR:9.2300 rate:0.0200 gloss:0.0000 dloss:5.3412\n",
      "Episode:1179 meanR:9.2600 rate:0.0220 gloss:0.0000 dloss:5.4448\n",
      "Episode:1180 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:5.0851\n",
      "Episode:1181 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:5.3569\n",
      "Episode:1182 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:5.3794\n",
      "Episode:1183 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:5.4096\n",
      "Episode:1184 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:5.4258\n",
      "Episode:1185 meanR:9.2400 rate:0.0160 gloss:0.0000 dloss:5.4129\n",
      "Episode:1186 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:5.3692\n",
      "Episode:1187 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:5.4434\n",
      "Episode:1188 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:5.4041\n",
      "Episode:1189 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:5.3518\n",
      "Episode:1190 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:5.1993\n",
      "Episode:1191 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:4.7414\n",
      "Episode:1192 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.4993\n",
      "Episode:1193 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.8223\n",
      "Episode:1194 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.7168\n",
      "Episode:1195 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.7611\n",
      "Episode:1196 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.7901\n",
      "Episode:1197 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:4.9194\n",
      "Episode:1198 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:5.0653\n",
      "Episode:1199 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:4.9439\n",
      "Episode:1200 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:4.9332\n",
      "Episode:1201 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:5.0207\n",
      "Episode:1202 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:5.0791\n",
      "Episode:1203 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:5.0718\n",
      "Episode:1204 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:5.1580\n",
      "Episode:1205 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:5.1295\n",
      "Episode:1206 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:5.1260\n",
      "Episode:1207 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.6255\n",
      "Episode:1208 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:5.0203\n",
      "Episode:1209 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.9895\n",
      "Episode:1210 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:5.0061\n",
      "Episode:1211 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:4.9940\n",
      "Episode:1212 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:5.1092\n",
      "Episode:1213 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:5.0964\n",
      "Episode:1214 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:5.0833\n",
      "Episode:1215 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:5.1994\n",
      "Episode:1216 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:5.0513\n",
      "Episode:1217 meanR:9.2800 rate:0.0160 gloss:0.0000 dloss:4.8691\n",
      "Episode:1218 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:4.9648\n",
      "Episode:1219 meanR:9.2600 rate:0.0160 gloss:0.0000 dloss:4.9630\n",
      "Episode:1220 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:4.8989\n",
      "Episode:1221 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.7931\n",
      "Episode:1222 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:4.3844\n",
      "Episode:1223 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.7688\n",
      "Episode:1224 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:4.6060\n",
      "Episode:1225 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:4.6438\n",
      "Episode:1226 meanR:9.2400 rate:0.0160 gloss:0.0000 dloss:4.6713\n",
      "Episode:1227 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:4.7882\n",
      "Episode:1228 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.6855\n",
      "Episode:1229 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:4.6977\n",
      "Episode:1230 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:4.7563\n",
      "Episode:1231 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:4.6045\n",
      "Episode:1232 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:4.8813\n",
      "Episode:1233 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:4.8864\n",
      "Episode:1234 meanR:9.2400 rate:0.0160 gloss:0.0000 dloss:4.9030\n",
      "Episode:1235 meanR:9.2300 rate:0.0180 gloss:0.0000 dloss:4.9431\n",
      "Episode:1236 meanR:9.2200 rate:0.0180 gloss:0.0000 dloss:5.0034\n",
      "Episode:1237 meanR:9.2400 rate:0.0200 gloss:0.0000 dloss:5.0173\n",
      "Episode:1238 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:5.0016\n",
      "Episode:1239 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:5.0633\n",
      "Episode:1240 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:4.9221\n",
      "Episode:1241 meanR:9.3000 rate:0.0220 gloss:0.0000 dloss:4.8406\n",
      "Episode:1242 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:4.8026\n",
      "Episode:1243 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:4.7481\n",
      "Episode:1244 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:4.3133\n",
      "Episode:1245 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:4.5966\n",
      "Episode:1246 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:4.5124\n",
      "Episode:1247 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:4.4659\n",
      "Episode:1248 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:4.4480\n",
      "Episode:1249 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:4.3936\n",
      "Episode:1250 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:4.2087\n",
      "Episode:1251 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:4.1353\n",
      "Episode:1252 meanR:9.3300 rate:0.0220 gloss:0.0000 dloss:3.6143\n",
      "Episode:1253 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:3.5612\n",
      "Episode:1254 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:3.0634\n",
      "Episode:1255 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:3.1038\n",
      "Episode:1256 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:2.8368\n",
      "Episode:1257 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:2.7342\n",
      "Episode:1258 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:2.4111\n",
      "Episode:1259 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:2.2717\n",
      "Episode:1260 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:2.1598\n",
      "Episode:1261 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:2.7600\n",
      "Episode:1262 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:3.7336\n",
      "Episode:1263 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:4.4652\n",
      "Episode:1264 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:3.8713\n",
      "Episode:1265 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:3.8179\n",
      "Episode:1266 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:3.6964\n",
      "Episode:1267 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:3.8800\n",
      "Episode:1268 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:3.7060\n",
      "Episode:1269 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:3.8832\n",
      "Episode:1270 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:4.2665\n",
      "Episode:1271 meanR:9.3700 rate:0.0220 gloss:0.0000 dloss:4.8107\n",
      "Episode:1272 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:5.3104\n",
      "Episode:1273 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:5.6737\n",
      "Episode:1274 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:5.8767\n",
      "Episode:1275 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:5.7913\n",
      "Episode:1276 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:5.9995\n",
      "Episode:1277 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:6.1850\n",
      "Episode:1278 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:6.4014\n",
      "Episode:1279 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:6.5271\n",
      "Episode:1280 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:6.6472\n",
      "Episode:1281 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:6.9707\n",
      "Episode:1282 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:7.1849\n",
      "Episode:1283 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:6.8945\n",
      "Episode:1284 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:6.2762\n",
      "Episode:1285 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:6.8537\n",
      "Episode:1286 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:6.9001\n",
      "Episode:1287 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:7.1784\n",
      "Episode:1288 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:7.3800\n",
      "Episode:1289 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:7.2536\n",
      "Episode:1290 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:7.9372\n",
      "Episode:1291 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:8.1196\n",
      "Episode:1292 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:8.4184\n",
      "Episode:1293 meanR:9.2900 rate:0.0160 gloss:0.0000 dloss:8.8934\n",
      "Episode:1294 meanR:9.2700 rate:0.0160 gloss:0.0000 dloss:8.8573\n",
      "Episode:1295 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:9.3314\n",
      "Episode:1296 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:9.6521\n",
      "Episode:1297 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:9.5755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1298 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:9.9294\n",
      "Episode:1299 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:10.6975\n",
      "Episode:1300 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:10.6659\n",
      "Episode:1301 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:10.6660\n",
      "Episode:1302 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:10.2760\n",
      "Episode:1303 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:10.3730\n",
      "Episode:1304 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:10.9089\n",
      "Episode:1305 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:10.8583\n",
      "Episode:1306 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:11.0687\n",
      "Episode:1307 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:11.2750\n",
      "Episode:1308 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:11.5911\n",
      "Episode:1309 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:11.6650\n",
      "Episode:1310 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:11.0324\n",
      "Episode:1311 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:11.6184\n",
      "Episode:1312 meanR:9.2800 rate:0.0180 gloss:0.0000 dloss:12.1056\n",
      "Episode:1313 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:12.7291\n",
      "Episode:1314 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:13.2389\n",
      "Episode:1315 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:13.4263\n",
      "Episode:1316 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:13.4351\n",
      "Episode:1317 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:13.8916\n",
      "Episode:1318 meanR:9.3200 rate:0.0220 gloss:0.0000 dloss:15.1127\n",
      "Episode:1319 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:15.7467\n",
      "Episode:1320 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:15.9900\n",
      "Episode:1321 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:16.0383\n",
      "Episode:1322 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:16.0709\n",
      "Episode:1323 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:16.2331\n",
      "Episode:1324 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:15.9801\n",
      "Episode:1325 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:15.7737\n",
      "Episode:1326 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:15.8024\n",
      "Episode:1327 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:15.1559\n",
      "Episode:1328 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:14.5921\n",
      "Episode:1329 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:14.2660\n",
      "Episode:1330 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:12.9127\n",
      "Episode:1331 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:11.5621\n",
      "Episode:1332 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:11.0009\n",
      "Episode:1333 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:10.4612\n",
      "Episode:1334 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:10.9815\n",
      "Episode:1335 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:10.9896\n",
      "Episode:1336 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:11.3473\n",
      "Episode:1337 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:11.6797\n",
      "Episode:1338 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:11.7884\n",
      "Episode:1339 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:11.7383\n",
      "Episode:1340 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:12.0433\n",
      "Episode:1341 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:11.6017\n",
      "Episode:1342 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:11.9247\n",
      "Episode:1343 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:12.3398\n",
      "Episode:1344 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:12.0967\n",
      "Episode:1345 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:12.1846\n",
      "Episode:1346 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:11.8890\n",
      "Episode:1347 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:11.5568\n",
      "Episode:1348 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:11.5404\n",
      "Episode:1349 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:11.1691\n",
      "Episode:1350 meanR:9.3000 rate:0.0180 gloss:0.0000 dloss:11.1236\n",
      "Episode:1351 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:11.2886\n",
      "Episode:1352 meanR:9.3000 rate:0.0200 gloss:0.0000 dloss:10.9677\n",
      "Episode:1353 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:11.0901\n",
      "Episode:1354 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:10.0445\n",
      "Episode:1355 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:9.9844\n",
      "Episode:1356 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:10.1096\n",
      "Episode:1357 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:9.7340\n",
      "Episode:1358 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:9.2069\n",
      "Episode:1359 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:9.0625\n",
      "Episode:1360 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:8.8849\n",
      "Episode:1361 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:8.7171\n",
      "Episode:1362 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:8.6974\n",
      "Episode:1363 meanR:9.3500 rate:0.0220 gloss:0.0000 dloss:8.5373\n",
      "Episode:1364 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:9.2375\n",
      "Episode:1365 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:9.6003\n",
      "Episode:1366 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:9.3569\n",
      "Episode:1367 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:9.4828\n",
      "Episode:1368 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:9.7181\n",
      "Episode:1369 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:10.2056\n",
      "Episode:1370 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:9.8236\n",
      "Episode:1371 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:9.9474\n",
      "Episode:1372 meanR:9.3000 rate:0.0160 gloss:0.0000 dloss:9.8038\n",
      "Episode:1373 meanR:9.3300 rate:0.0220 gloss:0.0000 dloss:9.7896\n",
      "Episode:1374 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:10.2439\n",
      "Episode:1375 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:9.7433\n",
      "Episode:1376 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:9.1749\n",
      "Episode:1377 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:9.0615\n",
      "Episode:1378 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:10.1667\n",
      "Episode:1379 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:11.0297\n",
      "Episode:1380 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:12.0242\n",
      "Episode:1381 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:12.7902\n",
      "Episode:1382 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:15.4502\n",
      "Episode:1383 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:18.6317\n",
      "Episode:1384 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:17.5583\n",
      "Episode:1385 meanR:9.3200 rate:0.0180 gloss:0.0000 dloss:19.8037\n",
      "Episode:1386 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:20.6648\n",
      "Episode:1387 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:20.0451\n",
      "Episode:1388 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:16.4718\n",
      "Episode:1389 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:16.8709\n",
      "Episode:1390 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:16.7545\n",
      "Episode:1391 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:8.4613\n",
      "Episode:1392 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:5.9918\n",
      "Episode:1393 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:5.7742\n",
      "Episode:1394 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:6.2448\n",
      "Episode:1395 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:6.0541\n",
      "Episode:1396 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:6.7596\n",
      "Episode:1397 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:5.2138\n",
      "Episode:1398 meanR:9.4200 rate:0.0220 gloss:0.0000 dloss:11.0921\n",
      "Episode:1399 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:8.4071\n",
      "Episode:1400 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:5.8543\n",
      "Episode:1401 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:5.6052\n",
      "Episode:1402 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:5.6407\n",
      "Episode:1403 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:5.8728\n",
      "Episode:1404 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:5.9809\n",
      "Episode:1405 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:6.1672\n",
      "Episode:1406 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.3236\n",
      "Episode:1407 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.2317\n",
      "Episode:1408 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:6.2927\n",
      "Episode:1409 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.3638\n",
      "Episode:1410 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.5049\n",
      "Episode:1411 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.7725\n",
      "Episode:1412 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.2935\n",
      "Episode:1413 meanR:9.3800 rate:0.0220 gloss:0.0000 dloss:6.6437\n",
      "Episode:1414 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:6.6767\n",
      "Episode:1415 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:6.7035\n",
      "Episode:1416 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:6.7242\n",
      "Episode:1417 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:6.7396\n",
      "Episode:1418 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:6.7604\n",
      "Episode:1419 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:6.7794\n",
      "Episode:1420 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:6.8040\n",
      "Episode:1421 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:6.8321\n",
      "Episode:1422 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:6.8617\n",
      "Episode:1423 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:6.8947\n",
      "Episode:1424 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:6.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1425 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.0276\n",
      "Episode:1426 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.3893\n",
      "Episode:1427 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.1289\n",
      "Episode:1428 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.2078\n",
      "Episode:1429 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.2028\n",
      "Episode:1430 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.2071\n",
      "Episode:1431 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.2076\n",
      "Episode:1432 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.2085\n",
      "Episode:1433 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:7.2080\n",
      "Episode:1434 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.2060\n",
      "Episode:1435 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.2047\n",
      "Episode:1436 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.2162\n",
      "Episode:1437 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.2130\n",
      "Episode:1438 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.2172\n",
      "Episode:1439 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.2190\n",
      "Episode:1440 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.2210\n",
      "Episode:1441 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.2277\n",
      "Episode:1442 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.2335\n",
      "Episode:1443 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.2431\n",
      "Episode:1444 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.2675\n",
      "Episode:1445 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.2692\n",
      "Episode:1446 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2818\n",
      "Episode:1447 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2902\n",
      "Episode:1448 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2941\n",
      "Episode:1449 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2930\n",
      "Episode:1450 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.2895\n",
      "Episode:1451 meanR:9.4300 rate:0.0220 gloss:0.0000 dloss:7.2857\n",
      "Episode:1452 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.2897\n",
      "Episode:1453 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.3029\n",
      "Episode:1454 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:7.3039\n",
      "Episode:1455 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:7.3171\n",
      "Episode:1456 meanR:9.4500 rate:0.0200 gloss:0.0000 dloss:7.3172\n",
      "Episode:1457 meanR:9.4300 rate:0.0160 gloss:0.0000 dloss:7.3246\n",
      "Episode:1458 meanR:9.4100 rate:0.0160 gloss:0.0000 dloss:7.3217\n",
      "Episode:1459 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.3146\n",
      "Episode:1460 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.3064\n",
      "Episode:1461 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.3010\n",
      "Episode:1462 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.3045\n",
      "Episode:1463 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.3746\n",
      "Episode:1464 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.5607\n",
      "Episode:1465 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.3646\n",
      "Episode:1466 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.3918\n",
      "Episode:1467 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.3779\n",
      "Episode:1468 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:7.3588\n",
      "Episode:1469 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.7853\n",
      "Episode:1470 meanR:9.4100 rate:0.0220 gloss:0.0000 dloss:7.2781\n",
      "Episode:1471 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2439\n",
      "Episode:1472 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:7.2255\n",
      "Episode:1473 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:7.2011\n",
      "Episode:1474 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.1851\n",
      "Episode:1475 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.1735\n",
      "Episode:1476 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.1669\n",
      "Episode:1477 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.1790\n",
      "Episode:1478 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2768\n",
      "Episode:1479 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2202\n",
      "Episode:1480 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2506\n",
      "Episode:1481 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.2968\n",
      "Episode:1482 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.3803\n",
      "Episode:1483 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:7.9711\n",
      "Episode:1484 meanR:9.4300 rate:0.0180 gloss:0.0000 dloss:7.5205\n",
      "Episode:1485 meanR:9.4300 rate:0.0180 gloss:0.0000 dloss:7.6134\n",
      "Episode:1486 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:7.6138\n",
      "Episode:1487 meanR:9.4400 rate:0.0180 gloss:0.0000 dloss:7.5989\n",
      "Episode:1488 meanR:9.4300 rate:0.0180 gloss:0.0000 dloss:7.5844\n",
      "Episode:1489 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.6123\n",
      "Episode:1490 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:7.9503\n",
      "Episode:1491 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.6319\n",
      "Episode:1492 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.6615\n",
      "Episode:1493 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.6411\n",
      "Episode:1494 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.6091\n",
      "Episode:1495 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.5669\n",
      "Episode:1496 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:7.5527\n",
      "Episode:1497 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.5057\n",
      "Episode:1498 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.4765\n",
      "Episode:1499 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.4980\n",
      "Episode:1500 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.4456\n",
      "Episode:1501 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.4413\n",
      "Episode:1502 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.4504\n",
      "Episode:1503 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.5938\n",
      "Episode:1504 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.4716\n",
      "Episode:1505 meanR:9.4300 rate:0.0200 gloss:0.0000 dloss:7.4789\n",
      "Episode:1506 meanR:9.4400 rate:0.0200 gloss:0.0000 dloss:7.4722\n",
      "Episode:1507 meanR:9.4300 rate:0.0160 gloss:0.0000 dloss:7.4997\n",
      "Episode:1508 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.5884\n",
      "Episode:1509 meanR:9.4100 rate:0.0160 gloss:0.0000 dloss:7.4889\n",
      "Episode:1510 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.5261\n",
      "Episode:1511 meanR:9.4100 rate:0.0160 gloss:0.0000 dloss:7.4933\n",
      "Episode:1512 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.4426\n",
      "Episode:1513 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.4358\n",
      "Episode:1514 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.3698\n",
      "Episode:1515 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.3238\n",
      "Episode:1516 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:7.2906\n",
      "Episode:1517 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:7.4106\n",
      "Episode:1518 meanR:9.4200 rate:0.0180 gloss:0.0000 dloss:7.2592\n",
      "Episode:1519 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:7.2682\n",
      "Episode:1520 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.2165\n",
      "Episode:1521 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:7.1395\n",
      "Episode:1522 meanR:9.4100 rate:0.0220 gloss:0.0000 dloss:6.8948\n",
      "Episode:1523 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:3.1746\n",
      "Episode:1524 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:3.6608\n",
      "Episode:1525 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:4.2508\n",
      "Episode:1526 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:4.8509\n",
      "Episode:1527 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:4.8472\n",
      "Episode:1528 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:4.8702\n",
      "Episode:1529 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:4.6895\n",
      "Episode:1530 meanR:9.4100 rate:0.0160 gloss:0.0000 dloss:4.5098\n",
      "Episode:1531 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:4.5731\n",
      "Episode:1532 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:4.6905\n",
      "Episode:1533 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:4.8839\n",
      "Episode:1534 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:5.1805\n",
      "Episode:1535 meanR:9.4100 rate:0.0220 gloss:0.0000 dloss:6.0254\n",
      "Episode:1536 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:5.5174\n",
      "Episode:1537 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:5.7302\n",
      "Episode:1538 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:5.9670\n",
      "Episode:1539 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:6.4571\n",
      "Episode:1540 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:6.3278\n",
      "Episode:1541 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:6.5111\n",
      "Episode:1542 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:6.6408\n",
      "Episode:1543 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:6.8386\n",
      "Episode:1544 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:7.0712\n",
      "Episode:1545 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.0896\n",
      "Episode:1546 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.2944\n",
      "Episode:1547 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.5580\n",
      "Episode:1548 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:8.3084\n",
      "Episode:1549 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.5668\n",
      "Episode:1550 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.5558\n",
      "Episode:1551 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:7.5442\n",
      "Episode:1552 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.5856\n",
      "Episode:1553 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:7.4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1554 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.4485\n",
      "Episode:1555 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:7.3785\n",
      "Episode:1556 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:7.7988\n",
      "Episode:1557 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:7.7128\n",
      "Episode:1558 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:7.1900\n",
      "Episode:1559 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:7.1713\n",
      "Episode:1560 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.2043\n",
      "Episode:1561 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.1167\n",
      "Episode:1562 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.0804\n",
      "Episode:1563 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.0665\n",
      "Episode:1564 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.0837\n",
      "Episode:1565 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.1472\n",
      "Episode:1566 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.1186\n",
      "Episode:1567 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.1462\n",
      "Episode:1568 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.1813\n",
      "Episode:1569 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:7.2156\n",
      "Episode:1570 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.2517\n",
      "Episode:1571 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.3244\n",
      "Episode:1572 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.3778\n",
      "Episode:1573 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:7.4514\n",
      "Episode:1574 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.7537\n",
      "Episode:1575 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.5392\n",
      "Episode:1576 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.5873\n",
      "Episode:1577 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.8176\n",
      "Episode:1578 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.6122\n",
      "Episode:1579 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.6213\n",
      "Episode:1580 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.6187\n",
      "Episode:1581 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.5507\n",
      "Episode:1582 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.4666\n",
      "Episode:1583 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:7.3999\n",
      "Episode:1584 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:7.8211\n",
      "Episode:1585 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.7649\n",
      "Episode:1586 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:7.6704\n",
      "Episode:1587 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:7.1773\n",
      "Episode:1588 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.2480\n",
      "Episode:1589 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:7.1489\n",
      "Episode:1590 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.2360\n",
      "Episode:1591 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.2221\n",
      "Episode:1592 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.1827\n",
      "Episode:1593 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:7.6621\n",
      "Episode:1594 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:7.6321\n",
      "Episode:1595 meanR:9.3400 rate:0.0220 gloss:0.0000 dloss:7.1757\n",
      "Episode:1596 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:7.1720\n",
      "Episode:1597 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:7.2458\n",
      "Episode:1598 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:7.4266\n",
      "Episode:1599 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:7.5952\n",
      "Episode:1600 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:7.9030\n",
      "Episode:1601 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:8.1762\n",
      "Episode:1602 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:8.8913\n",
      "Episode:1603 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:8.9996\n",
      "Episode:1604 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:9.9604\n",
      "Episode:1605 meanR:9.3700 rate:0.0220 gloss:0.0000 dloss:10.2282\n",
      "Episode:1606 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:10.4247\n",
      "Episode:1607 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:9.8551\n",
      "Episode:1608 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:10.7114\n",
      "Episode:1609 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:11.4030\n",
      "Episode:1610 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:11.6583\n",
      "Episode:1611 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:11.7514\n",
      "Episode:1612 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:12.0425\n",
      "Episode:1613 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:12.4163\n",
      "Episode:1614 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:12.7028\n",
      "Episode:1615 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:13.0132\n",
      "Episode:1616 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:13.3721\n",
      "Episode:1617 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:13.8493\n",
      "Episode:1618 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:14.0343\n",
      "Episode:1619 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:14.8048\n",
      "Episode:1620 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:14.8865\n",
      "Episode:1621 meanR:9.4200 rate:0.0200 gloss:0.0000 dloss:15.1052\n",
      "Episode:1622 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:15.2774\n",
      "Episode:1623 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:15.2639\n",
      "Episode:1624 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:15.5834\n",
      "Episode:1625 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:15.8124\n",
      "Episode:1626 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:16.0657\n",
      "Episode:1627 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:16.2991\n",
      "Episode:1628 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:16.4701\n",
      "Episode:1629 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:16.6550\n",
      "Episode:1630 meanR:9.4100 rate:0.0180 gloss:0.0000 dloss:16.8812\n",
      "Episode:1631 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:16.6169\n",
      "Episode:1632 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:16.8451\n",
      "Episode:1633 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:16.9305\n",
      "Episode:1634 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:17.0792\n",
      "Episode:1635 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:18.4824\n",
      "Episode:1636 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:17.1849\n",
      "Episode:1637 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:15.5709\n",
      "Episode:1638 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:14.9387\n",
      "Episode:1639 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:14.3322\n",
      "Episode:1640 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:14.1739\n",
      "Episode:1641 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:14.9164\n",
      "Episode:1642 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:13.6780\n",
      "Episode:1643 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:12.9480\n",
      "Episode:1644 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:12.2015\n",
      "Episode:1645 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:10.3906\n",
      "Episode:1646 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:10.6813\n",
      "Episode:1647 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:10.5551\n",
      "Episode:1648 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:10.4688\n",
      "Episode:1649 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:10.8810\n",
      "Episode:1650 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:10.5687\n",
      "Episode:1651 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:10.8230\n",
      "Episode:1652 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:10.5332\n",
      "Episode:1653 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:10.5560\n",
      "Episode:1654 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:10.6037\n",
      "Episode:1655 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:10.9145\n",
      "Episode:1656 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:11.3661\n",
      "Episode:1657 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:12.3831\n",
      "Episode:1658 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:12.6470\n",
      "Episode:1659 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:13.1991\n",
      "Episode:1660 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:13.8160\n",
      "Episode:1661 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:14.3565\n",
      "Episode:1662 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:14.7494\n",
      "Episode:1663 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:14.7555\n",
      "Episode:1664 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:14.7405\n",
      "Episode:1665 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:15.0559\n",
      "Episode:1666 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:15.0455\n",
      "Episode:1667 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:15.1792\n",
      "Episode:1668 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:15.0179\n",
      "Episode:1669 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:15.1959\n",
      "Episode:1670 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:15.3694\n",
      "Episode:1671 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:15.2969\n",
      "Episode:1672 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:15.1701\n",
      "Episode:1673 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:15.5235\n",
      "Episode:1674 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:15.5172\n",
      "Episode:1675 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:15.7377\n",
      "Episode:1676 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:15.6324\n",
      "Episode:1677 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:15.8487\n",
      "Episode:1678 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:17.0399\n",
      "Episode:1679 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:17.1381\n",
      "Episode:1680 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:17.1364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1681 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:15.6069\n",
      "Episode:1682 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:14.1269\n",
      "Episode:1683 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:13.8003\n",
      "Episode:1684 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:13.2397\n",
      "Episode:1685 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:12.5294\n",
      "Episode:1686 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:11.9275\n",
      "Episode:1687 meanR:9.3500 rate:0.0160 gloss:0.0000 dloss:11.4321\n",
      "Episode:1688 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:10.8445\n",
      "Episode:1689 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:10.3464\n",
      "Episode:1690 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:9.9328\n",
      "Episode:1691 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:9.5186\n",
      "Episode:1692 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:9.1061\n",
      "Episode:1693 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:8.8482\n",
      "Episode:1694 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:9.1096\n",
      "Episode:1695 meanR:9.3500 rate:0.0180 gloss:0.0000 dloss:8.4553\n",
      "Episode:1696 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:8.3246\n",
      "Episode:1697 meanR:9.3500 rate:0.0200 gloss:0.0000 dloss:7.8084\n",
      "Episode:1698 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:7.4486\n",
      "Episode:1699 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:7.2840\n",
      "Episode:1700 meanR:9.3200 rate:0.0160 gloss:0.0000 dloss:7.3218\n",
      "Episode:1701 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:7.4378\n",
      "Episode:1702 meanR:9.3100 rate:0.0160 gloss:0.0000 dloss:7.2670\n",
      "Episode:1703 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:7.3299\n",
      "Episode:1704 meanR:9.3100 rate:0.0180 gloss:0.0000 dloss:6.9262\n",
      "Episode:1705 meanR:9.2900 rate:0.0180 gloss:0.0000 dloss:7.7132\n",
      "Episode:1706 meanR:9.2800 rate:0.0160 gloss:0.0000 dloss:7.6072\n",
      "Episode:1707 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:7.4232\n",
      "Episode:1708 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:7.1977\n",
      "Episode:1709 meanR:9.2700 rate:0.0180 gloss:0.0000 dloss:6.9956\n",
      "Episode:1710 meanR:9.2600 rate:0.0180 gloss:0.0000 dloss:7.2456\n",
      "Episode:1711 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:6.9279\n",
      "Episode:1712 meanR:9.2500 rate:0.0180 gloss:0.0000 dloss:6.8137\n",
      "Episode:1713 meanR:9.2600 rate:0.0200 gloss:0.0000 dloss:6.7300\n",
      "Episode:1714 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:7.0279\n",
      "Episode:1715 meanR:9.2500 rate:0.0160 gloss:0.0000 dloss:6.9743\n",
      "Episode:1716 meanR:9.2500 rate:0.0200 gloss:0.0000 dloss:6.8602\n",
      "Episode:1717 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:7.0964\n",
      "Episode:1718 meanR:9.2700 rate:0.0200 gloss:0.0000 dloss:7.0711\n",
      "Episode:1719 meanR:9.2800 rate:0.0200 gloss:0.0000 dloss:7.0987\n",
      "Episode:1720 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:7.2506\n",
      "Episode:1721 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:7.3911\n",
      "Episode:1722 meanR:9.2900 rate:0.0200 gloss:0.0000 dloss:7.3561\n",
      "Episode:1723 meanR:9.3100 rate:0.0200 gloss:0.0000 dloss:7.4198\n",
      "Episode:1724 meanR:9.3200 rate:0.0200 gloss:0.0000 dloss:7.5800\n",
      "Episode:1725 meanR:9.3400 rate:0.0220 gloss:0.0000 dloss:7.1726\n",
      "Episode:1726 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:7.4810\n",
      "Episode:1727 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:7.5713\n",
      "Episode:1728 meanR:9.3300 rate:0.0180 gloss:0.0000 dloss:8.0798\n",
      "Episode:1729 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:8.9079\n",
      "Episode:1730 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:9.6130\n",
      "Episode:1731 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:10.1271\n",
      "Episode:1732 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:9.9384\n",
      "Episode:1733 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:9.8248\n",
      "Episode:1734 meanR:9.4100 rate:0.0200 gloss:0.0000 dloss:9.9833\n",
      "Episode:1735 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:9.7722\n",
      "Episode:1736 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:10.5571\n",
      "Episode:1737 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:11.3492\n",
      "Episode:1738 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:10.1842\n",
      "Episode:1739 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:8.5590\n",
      "Episode:1740 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:8.5556\n",
      "Episode:1741 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:8.4915\n",
      "Episode:1742 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:4.4478\n",
      "Episode:1743 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:2.5490\n",
      "Episode:1744 meanR:9.4000 rate:0.0160 gloss:0.0000 dloss:2.5632\n",
      "Episode:1745 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:3.3307\n",
      "Episode:1746 meanR:9.4000 rate:0.0180 gloss:0.0000 dloss:3.3767\n",
      "Episode:1747 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:3.9052\n",
      "Episode:1748 meanR:9.3900 rate:0.0160 gloss:0.0000 dloss:4.1045\n",
      "Episode:1749 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:4.3343\n",
      "Episode:1750 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:4.5918\n",
      "Episode:1751 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:4.8591\n",
      "Episode:1752 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:5.1436\n",
      "Episode:1753 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:5.3422\n",
      "Episode:1754 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:5.5440\n",
      "Episode:1755 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:5.7339\n",
      "Episode:1756 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:6.3186\n",
      "Episode:1757 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:6.4622\n",
      "Episode:1758 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:6.2390\n",
      "Episode:1759 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:6.4352\n",
      "Episode:1760 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:7.0476\n",
      "Episode:1761 meanR:9.3400 rate:0.0160 gloss:0.0000 dloss:7.2704\n",
      "Episode:1762 meanR:9.3500 rate:0.0220 gloss:0.0000 dloss:7.2582\n",
      "Episode:1763 meanR:9.3300 rate:0.0160 gloss:0.0000 dloss:8.0992\n",
      "Episode:1764 meanR:9.3300 rate:0.0200 gloss:0.0000 dloss:9.2129\n",
      "Episode:1765 meanR:9.3400 rate:0.0200 gloss:0.0000 dloss:9.1128\n",
      "Episode:1766 meanR:9.3400 rate:0.0180 gloss:0.0000 dloss:9.2511\n",
      "Episode:1767 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:9.4400\n",
      "Episode:1768 meanR:9.3600 rate:0.0200 gloss:0.0000 dloss:9.3386\n",
      "Episode:1769 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:9.2694\n",
      "Episode:1770 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:8.8511\n",
      "Episode:1771 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:8.6973\n",
      "Episode:1772 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:9.1224\n",
      "Episode:1773 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:9.0605\n",
      "Episode:1774 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:8.7513\n",
      "Episode:1775 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:8.2828\n",
      "Episode:1776 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:8.1103\n",
      "Episode:1777 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.8636\n",
      "Episode:1778 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.6164\n",
      "Episode:1779 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.8335\n",
      "Episode:1780 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.8341\n",
      "Episode:1781 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.7079\n",
      "Episode:1782 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.8041\n",
      "Episode:1783 meanR:9.3700 rate:0.0160 gloss:0.0000 dloss:7.8113\n",
      "Episode:1784 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.8315\n",
      "Episode:1785 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.8455\n",
      "Episode:1786 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.8011\n",
      "Episode:1787 meanR:9.4000 rate:0.0200 gloss:0.0000 dloss:7.8077\n",
      "Episode:1788 meanR:9.3900 rate:0.0180 gloss:0.0000 dloss:7.8364\n",
      "Episode:1789 meanR:9.3800 rate:0.0180 gloss:0.0000 dloss:7.8223\n",
      "Episode:1790 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.8229\n",
      "Episode:1791 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.7801\n",
      "Episode:1792 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.7401\n",
      "Episode:1793 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:7.7046\n",
      "Episode:1794 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.6512\n",
      "Episode:1795 meanR:9.3900 rate:0.0200 gloss:0.0000 dloss:7.5577\n",
      "Episode:1796 meanR:9.3800 rate:0.0160 gloss:0.0000 dloss:7.5458\n",
      "Episode:1797 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.4665\n",
      "Episode:1798 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.2255\n",
      "Episode:1799 meanR:9.3600 rate:0.0180 gloss:0.0000 dloss:7.3031\n",
      "Episode:1800 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:7.1704\n",
      "Episode:1801 meanR:9.3600 rate:0.0160 gloss:0.0000 dloss:7.5797\n",
      "Episode:1802 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.5509\n",
      "Episode:1803 meanR:9.3700 rate:0.0200 gloss:0.0000 dloss:7.0585\n",
      "Episode:1804 meanR:9.3700 rate:0.0180 gloss:0.0000 dloss:7.0374\n",
      "Episode:1805 meanR:9.3800 rate:0.0200 gloss:0.0000 dloss:6.9041\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        state = env.reset() # env first state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            g_loss, d_loss, _, _ = sess.run(\n",
    "                fetches=[model.g_loss, model.d_loss, model.g_opt, model.d_opt], \n",
    "                feed_dict = {model.states: states, \n",
    "                             model.actions: actions,\n",
    "                             model.targetQs: targetQs,\n",
    "                             model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                             model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        # Episode total reward and success rate/prob\n",
    "        episode_reward.append(total_reward) # stopping criteria\n",
    "        rate = total_reward/ 500 # success is 500 points: 0-1\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-seq-Copy1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossR_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(100):\n",
    "    #while True:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        #for _ in range(111111111111111111):\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Print and break condition\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "        # if total_reward == 500:\n",
    "        #     break\n",
    "                \n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
