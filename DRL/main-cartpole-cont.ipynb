{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.5148954174114357 -2.870291843392152\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), # ==  self.rates\n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = memory.buffer\n",
    "# percentage = 0.9\n",
    "# for idx in range(memory_size// batch_size):\n",
    "#     #batch, rates_ = memory.sample(batch_size=batch_size)\n",
    "#     rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "#     states = np.array([each[0] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     actions = np.array([each[1] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     next_states = np.array([each[2] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     rewards = np.array([each[3] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     dones = np.array([each[4] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     maxrates = rates[rates >= (np.max(rates)*percentage)]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, maxrates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = memory.buffer\n",
    "# percentage = 0.9\n",
    "# for idx in range(memory_size// batch_size):\n",
    "#     states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "#     states = states[rates >= 0]\n",
    "#     actions = actions[rates >= 0]\n",
    "#     next_states = next_states[rates >= 0]\n",
    "#     rewards = rewards[rates >= 0]\n",
    "#     dones = dones[rates >= 0]\n",
    "#     rates = rates[rates >= 0]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(193, 4) (193,) (193, 4) (193,) (193,) (193,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(346, 4) (346,) (346, 4) (346,) (346,) (346,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(89, 4) (89,) (89, 4) (89,) (89,) (89,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(156, 4) (156,) (156, 4) (156,) (156,) (156,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(91, 4) (91,) (91, 4) (91,) (91,) (91,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(142, 4) (142,) (142, 4) (142,) (142,) (142,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(99, 4) (99,) (99, 4) (99,) (99,) (99,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(167, 4) (167,) (167, 4) (167,) (167,) (167,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(107, 4) (107,) (107, 4) (107,) (107,) (107,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(72, 4) (72,) (72, 4) (72,) (72,) (72,)\n"
     ]
    }
   ],
   "source": [
    "batch = memory.buffer\n",
    "percentage = 0.9\n",
    "# statesL, actionsL, next_statesL, rewardsL, donesL, ratesL = [], [], [], [], [], []\n",
    "for idx in range(memory_size// batch_size):\n",
    "    states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "    print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "    states = states[rates >= (np.max(rates)*percentage)]\n",
    "    actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "    next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "    rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "    dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "    rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "    print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "    # statesL.append(states)\n",
    "    # actionsL.append(actions)\n",
    "    # next_statesL.append(next_states)\n",
    "    # rewardsL.append(rewards)\n",
    "    # donesL.append(dones)\n",
    "    # ratesL.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 4), (72,), (72, 4), (72,), (72,), (72,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:12.0000 R:12.0 rate:0.024 gloss:0.6205 dlossA:0.6656 dlossQ:1.1547 exploreP:0.9988\n",
      "Episode:1 meanR:18.5000 R:25.0 rate:0.05 gloss:0.6211 dlossA:0.6628 dlossQ:1.1682 exploreP:0.9963\n",
      "Episode:2 meanR:21.0000 R:26.0 rate:0.052 gloss:0.6172 dlossA:0.6609 dlossQ:1.1629 exploreP:0.9938\n",
      "Episode:3 meanR:20.0000 R:17.0 rate:0.034 gloss:0.6149 dlossA:0.6591 dlossQ:1.1679 exploreP:0.9921\n",
      "Episode:4 meanR:18.0000 R:10.0 rate:0.02 gloss:0.6140 dlossA:0.6640 dlossQ:1.1479 exploreP:0.9911\n",
      "Episode:5 meanR:21.8333 R:41.0 rate:0.082 gloss:0.6167 dlossA:0.6620 dlossQ:1.1639 exploreP:0.9871\n",
      "Episode:6 meanR:20.7143 R:14.0 rate:0.028 gloss:0.6138 dlossA:0.6607 dlossQ:1.1630 exploreP:0.9857\n",
      "Episode:7 meanR:19.7500 R:13.0 rate:0.026 gloss:0.6163 dlossA:0.6606 dlossQ:1.1733 exploreP:0.9845\n",
      "Episode:8 meanR:19.7778 R:20.0 rate:0.04 gloss:0.6167 dlossA:0.6601 dlossQ:1.1772 exploreP:0.9825\n",
      "Episode:9 meanR:20.0000 R:22.0 rate:0.044 gloss:0.6185 dlossA:0.6601 dlossQ:1.1817 exploreP:0.9804\n",
      "Episode:10 meanR:20.8182 R:29.0 rate:0.058 gloss:0.6133 dlossA:0.6613 dlossQ:1.1630 exploreP:0.9776\n",
      "Episode:11 meanR:24.3333 R:63.0 rate:0.126 gloss:0.6087 dlossA:0.6604 dlossQ:1.1546 exploreP:0.9715\n",
      "Episode:12 meanR:23.7692 R:17.0 rate:0.034 gloss:0.6102 dlossA:0.6594 dlossQ:1.1677 exploreP:0.9699\n",
      "Episode:13 meanR:22.9286 R:12.0 rate:0.024 gloss:0.6058 dlossA:0.6619 dlossQ:1.1409 exploreP:0.9687\n",
      "Episode:14 meanR:23.2667 R:28.0 rate:0.056 gloss:0.6121 dlossA:0.6599 dlossQ:1.1686 exploreP:0.9660\n",
      "Episode:15 meanR:23.5000 R:27.0 rate:0.054 gloss:0.6051 dlossA:0.6603 dlossQ:1.1446 exploreP:0.9635\n",
      "Episode:16 meanR:23.0588 R:16.0 rate:0.032 gloss:0.6064 dlossA:0.6611 dlossQ:1.1438 exploreP:0.9619\n",
      "Episode:17 meanR:23.5000 R:31.0 rate:0.062 gloss:0.6137 dlossA:0.6604 dlossQ:1.1720 exploreP:0.9590\n",
      "Episode:18 meanR:23.6316 R:26.0 rate:0.052 gloss:0.6085 dlossA:0.6612 dlossQ:1.1519 exploreP:0.9565\n",
      "Episode:19 meanR:25.0500 R:52.0 rate:0.104 gloss:0.6099 dlossA:0.6614 dlossQ:1.1584 exploreP:0.9516\n",
      "Episode:20 meanR:25.1905 R:28.0 rate:0.056 gloss:0.6051 dlossA:0.6621 dlossQ:1.1411 exploreP:0.9490\n",
      "Episode:21 meanR:25.0909 R:23.0 rate:0.046 gloss:0.6086 dlossA:0.6597 dlossQ:1.1618 exploreP:0.9468\n",
      "Episode:22 meanR:25.7826 R:41.0 rate:0.082 gloss:0.6088 dlossA:0.6612 dlossQ:1.1581 exploreP:0.9430\n",
      "Episode:23 meanR:25.4583 R:18.0 rate:0.036 gloss:0.6052 dlossA:0.6595 dlossQ:1.1528 exploreP:0.9413\n",
      "Episode:24 meanR:26.7200 R:57.0 rate:0.114 gloss:0.6117 dlossA:0.6612 dlossQ:1.1673 exploreP:0.9360\n",
      "Episode:25 meanR:27.4615 R:46.0 rate:0.092 gloss:0.6077 dlossA:0.6610 dlossQ:1.1573 exploreP:0.9318\n",
      "Episode:26 meanR:26.9259 R:13.0 rate:0.026 gloss:0.6167 dlossA:0.6597 dlossQ:1.1996 exploreP:0.9306\n",
      "Episode:27 meanR:27.4643 R:42.0 rate:0.084 gloss:0.6079 dlossA:0.6598 dlossQ:1.1652 exploreP:0.9267\n",
      "Episode:28 meanR:26.9655 R:13.0 rate:0.026 gloss:0.6079 dlossA:0.6624 dlossQ:1.1561 exploreP:0.9255\n",
      "Episode:29 meanR:27.0000 R:28.0 rate:0.056 gloss:0.6037 dlossA:0.6601 dlossQ:1.1486 exploreP:0.9230\n",
      "Episode:30 meanR:26.6774 R:17.0 rate:0.034 gloss:0.6044 dlossA:0.6612 dlossQ:1.1476 exploreP:0.9214\n",
      "Episode:31 meanR:26.3438 R:16.0 rate:0.032 gloss:0.6047 dlossA:0.6609 dlossQ:1.1494 exploreP:0.9200\n",
      "Episode:32 meanR:27.1515 R:53.0 rate:0.106 gloss:0.6077 dlossA:0.6601 dlossQ:1.1687 exploreP:0.9152\n",
      "Episode:33 meanR:26.8235 R:16.0 rate:0.032 gloss:0.6085 dlossA:0.6593 dlossQ:1.1781 exploreP:0.9137\n",
      "Episode:34 meanR:26.4571 R:14.0 rate:0.028 gloss:0.6111 dlossA:0.6598 dlossQ:1.1879 exploreP:0.9124\n",
      "Episode:35 meanR:26.6667 R:34.0 rate:0.068 gloss:0.6015 dlossA:0.6608 dlossQ:1.1432 exploreP:0.9094\n",
      "Episode:36 meanR:26.5405 R:22.0 rate:0.044 gloss:0.6056 dlossA:0.6612 dlossQ:1.1614 exploreP:0.9074\n",
      "Episode:37 meanR:26.1579 R:12.0 rate:0.024 gloss:0.5994 dlossA:0.6600 dlossQ:1.1398 exploreP:0.9063\n",
      "Episode:38 meanR:26.6923 R:47.0 rate:0.094 gloss:0.6029 dlossA:0.6605 dlossQ:1.1506 exploreP:0.9021\n",
      "Episode:39 meanR:26.6000 R:23.0 rate:0.046 gloss:0.6043 dlossA:0.6628 dlossQ:1.1406 exploreP:0.9001\n",
      "Episode:40 meanR:26.8049 R:35.0 rate:0.07 gloss:0.6033 dlossA:0.6613 dlossQ:1.1506 exploreP:0.8970\n",
      "Episode:41 meanR:27.1905 R:43.0 rate:0.086 gloss:0.6015 dlossA:0.6598 dlossQ:1.1557 exploreP:0.8932\n",
      "Episode:42 meanR:27.3953 R:36.0 rate:0.072 gloss:0.6011 dlossA:0.6616 dlossQ:1.1423 exploreP:0.8900\n",
      "Episode:43 meanR:27.1818 R:18.0 rate:0.036 gloss:0.6064 dlossA:0.6624 dlossQ:1.1575 exploreP:0.8884\n",
      "Episode:44 meanR:27.1556 R:26.0 rate:0.052 gloss:0.6026 dlossA:0.6604 dlossQ:1.1589 exploreP:0.8861\n",
      "Episode:45 meanR:27.3478 R:36.0 rate:0.072 gloss:0.6055 dlossA:0.6612 dlossQ:1.1738 exploreP:0.8830\n",
      "Episode:46 meanR:27.0851 R:15.0 rate:0.03 gloss:0.6006 dlossA:0.6624 dlossQ:1.1448 exploreP:0.8817\n",
      "Episode:47 meanR:27.1250 R:29.0 rate:0.058 gloss:0.6016 dlossA:0.6605 dlossQ:1.1585 exploreP:0.8791\n",
      "Episode:48 meanR:27.2041 R:31.0 rate:0.062 gloss:0.6012 dlossA:0.6609 dlossQ:1.1509 exploreP:0.8765\n",
      "Episode:49 meanR:26.8800 R:11.0 rate:0.022 gloss:0.6029 dlossA:0.6606 dlossQ:1.1618 exploreP:0.8755\n",
      "Episode:50 meanR:28.5490 R:112.0 rate:0.224 gloss:0.6018 dlossA:0.6610 dlossQ:1.1551 exploreP:0.8659\n",
      "Episode:51 meanR:28.5769 R:30.0 rate:0.06 gloss:0.6031 dlossA:0.6621 dlossQ:1.1591 exploreP:0.8633\n",
      "Episode:52 meanR:28.3019 R:14.0 rate:0.028 gloss:0.6052 dlossA:0.6617 dlossQ:1.1597 exploreP:0.8621\n",
      "Episode:53 meanR:28.6111 R:45.0 rate:0.09 gloss:0.6026 dlossA:0.6611 dlossQ:1.1590 exploreP:0.8583\n",
      "Episode:54 meanR:29.1273 R:57.0 rate:0.114 gloss:0.6015 dlossA:0.6615 dlossQ:1.1605 exploreP:0.8535\n",
      "Episode:55 meanR:29.5000 R:50.0 rate:0.1 gloss:0.6011 dlossA:0.6630 dlossQ:1.1501 exploreP:0.8492\n",
      "Episode:56 meanR:29.3158 R:19.0 rate:0.038 gloss:0.6006 dlossA:0.6623 dlossQ:1.1647 exploreP:0.8477\n",
      "Episode:57 meanR:29.6552 R:49.0 rate:0.098 gloss:0.6018 dlossA:0.6633 dlossQ:1.1495 exploreP:0.8436\n",
      "Episode:58 meanR:29.9492 R:47.0 rate:0.094 gloss:0.5999 dlossA:0.6621 dlossQ:1.1497 exploreP:0.8397\n",
      "Episode:59 meanR:29.7167 R:16.0 rate:0.032 gloss:0.5947 dlossA:0.6623 dlossQ:1.1516 exploreP:0.8383\n",
      "Episode:60 meanR:29.6393 R:25.0 rate:0.05 gloss:0.5975 dlossA:0.6620 dlossQ:1.1537 exploreP:0.8363\n",
      "Episode:61 meanR:29.5968 R:27.0 rate:0.054 gloss:0.6033 dlossA:0.6632 dlossQ:1.1507 exploreP:0.8340\n",
      "Episode:62 meanR:29.3810 R:16.0 rate:0.032 gloss:0.6001 dlossA:0.6635 dlossQ:1.1464 exploreP:0.8327\n",
      "Episode:63 meanR:29.8906 R:62.0 rate:0.124 gloss:0.5982 dlossA:0.6628 dlossQ:1.1476 exploreP:0.8276\n",
      "Episode:64 meanR:29.5692 R:9.0 rate:0.018 gloss:0.5920 dlossA:0.6617 dlossQ:1.1518 exploreP:0.8269\n",
      "Episode:65 meanR:29.4091 R:19.0 rate:0.038 gloss:0.5964 dlossA:0.6607 dlossQ:1.1446 exploreP:0.8253\n",
      "Episode:66 meanR:29.2985 R:22.0 rate:0.044 gloss:0.6050 dlossA:0.6664 dlossQ:1.1437 exploreP:0.8235\n",
      "Episode:67 meanR:29.2206 R:24.0 rate:0.048 gloss:0.5979 dlossA:0.6637 dlossQ:1.1472 exploreP:0.8216\n",
      "Episode:68 meanR:29.0000 R:14.0 rate:0.028 gloss:0.5928 dlossA:0.6620 dlossQ:1.1471 exploreP:0.8205\n",
      "Episode:69 meanR:29.1857 R:42.0 rate:0.084 gloss:0.5962 dlossA:0.6616 dlossQ:1.1545 exploreP:0.8171\n",
      "Episode:70 meanR:29.4930 R:51.0 rate:0.102 gloss:0.5942 dlossA:0.6613 dlossQ:1.1427 exploreP:0.8130\n",
      "Episode:71 meanR:29.2917 R:15.0 rate:0.03 gloss:0.5996 dlossA:0.6645 dlossQ:1.1381 exploreP:0.8118\n",
      "Episode:72 meanR:29.2740 R:28.0 rate:0.056 gloss:0.5943 dlossA:0.6625 dlossQ:1.1453 exploreP:0.8095\n",
      "Episode:73 meanR:29.5270 R:48.0 rate:0.096 gloss:0.6001 dlossA:0.6644 dlossQ:1.1369 exploreP:0.8057\n",
      "Episode:74 meanR:29.3600 R:17.0 rate:0.034 gloss:0.5993 dlossA:0.6626 dlossQ:1.1501 exploreP:0.8043\n",
      "Episode:75 meanR:29.1711 R:15.0 rate:0.03 gloss:0.6051 dlossA:0.6695 dlossQ:1.1310 exploreP:0.8031\n",
      "Episode:76 meanR:29.2987 R:39.0 rate:0.078 gloss:0.5975 dlossA:0.6661 dlossQ:1.1317 exploreP:0.8001\n",
      "Episode:77 meanR:29.3718 R:35.0 rate:0.07 gloss:0.5965 dlossA:0.6632 dlossQ:1.1421 exploreP:0.7973\n",
      "Episode:78 meanR:29.3797 R:30.0 rate:0.06 gloss:0.6001 dlossA:0.6662 dlossQ:1.1377 exploreP:0.7949\n",
      "Episode:79 meanR:29.3875 R:30.0 rate:0.06 gloss:0.6002 dlossA:0.6651 dlossQ:1.1420 exploreP:0.7926\n",
      "Episode:80 meanR:30.2469 R:99.0 rate:0.198 gloss:0.5973 dlossA:0.6640 dlossQ:1.1436 exploreP:0.7849\n",
      "Episode:81 meanR:30.1463 R:22.0 rate:0.044 gloss:0.5935 dlossA:0.6651 dlossQ:1.1396 exploreP:0.7832\n",
      "Episode:82 meanR:30.0120 R:19.0 rate:0.038 gloss:0.6009 dlossA:0.6665 dlossQ:1.1454 exploreP:0.7817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:83 meanR:30.3095 R:55.0 rate:0.11 gloss:0.6023 dlossA:0.6675 dlossQ:1.1330 exploreP:0.7775\n",
      "Episode:84 meanR:30.1529 R:17.0 rate:0.034 gloss:0.6012 dlossA:0.6681 dlossQ:1.1254 exploreP:0.7762\n",
      "Episode:85 meanR:30.0233 R:19.0 rate:0.038 gloss:0.6072 dlossA:0.6696 dlossQ:1.1331 exploreP:0.7747\n",
      "Episode:86 meanR:30.3333 R:57.0 rate:0.114 gloss:0.6009 dlossA:0.6658 dlossQ:1.1301 exploreP:0.7704\n",
      "Episode:87 meanR:30.3523 R:32.0 rate:0.064 gloss:0.6036 dlossA:0.6658 dlossQ:1.1353 exploreP:0.7679\n",
      "Episode:88 meanR:30.2472 R:21.0 rate:0.042 gloss:0.5946 dlossA:0.6639 dlossQ:1.1455 exploreP:0.7664\n",
      "Episode:89 meanR:31.3222 R:127.0 rate:0.254 gloss:0.5986 dlossA:0.6649 dlossQ:1.1351 exploreP:0.7568\n",
      "Episode:90 meanR:32.0549 R:98.0 rate:0.196 gloss:0.6007 dlossA:0.6655 dlossQ:1.1379 exploreP:0.7495\n",
      "Episode:91 meanR:31.8043 R:9.0 rate:0.018 gloss:0.5885 dlossA:0.6603 dlossQ:1.1481 exploreP:0.7489\n",
      "Episode:92 meanR:31.6989 R:22.0 rate:0.044 gloss:0.6008 dlossA:0.6667 dlossQ:1.1352 exploreP:0.7472\n",
      "Episode:93 meanR:31.9255 R:53.0 rate:0.106 gloss:0.5980 dlossA:0.6659 dlossQ:1.1412 exploreP:0.7433\n",
      "Episode:94 meanR:32.9158 R:126.0 rate:0.252 gloss:0.5970 dlossA:0.6633 dlossQ:1.1416 exploreP:0.7342\n",
      "Episode:95 meanR:32.8750 R:29.0 rate:0.058 gloss:0.5973 dlossA:0.6644 dlossQ:1.1485 exploreP:0.7321\n",
      "Episode:96 meanR:32.8763 R:33.0 rate:0.066 gloss:0.5999 dlossA:0.6645 dlossQ:1.1324 exploreP:0.7297\n",
      "Episode:97 meanR:33.5408 R:98.0 rate:0.196 gloss:0.5964 dlossA:0.6633 dlossQ:1.1484 exploreP:0.7227\n",
      "Episode:98 meanR:34.0000 R:79.0 rate:0.158 gloss:0.5946 dlossA:0.6639 dlossQ:1.1507 exploreP:0.7171\n",
      "Episode:99 meanR:34.0300 R:37.0 rate:0.074 gloss:0.6029 dlossA:0.6654 dlossQ:1.1443 exploreP:0.7144\n",
      "Episode:100 meanR:34.2800 R:37.0 rate:0.074 gloss:0.5905 dlossA:0.6616 dlossQ:1.1527 exploreP:0.7118\n",
      "Episode:101 meanR:34.8500 R:82.0 rate:0.164 gloss:0.5984 dlossA:0.6636 dlossQ:1.1430 exploreP:0.7061\n",
      "Episode:102 meanR:34.7100 R:12.0 rate:0.024 gloss:0.6089 dlossA:0.6665 dlossQ:1.1506 exploreP:0.7053\n",
      "Episode:103 meanR:34.7200 R:18.0 rate:0.036 gloss:0.5982 dlossA:0.6636 dlossQ:1.1422 exploreP:0.7040\n",
      "Episode:104 meanR:34.9300 R:31.0 rate:0.062 gloss:0.5964 dlossA:0.6634 dlossQ:1.1608 exploreP:0.7019\n",
      "Episode:105 meanR:34.7900 R:27.0 rate:0.054 gloss:0.5888 dlossA:0.6621 dlossQ:1.1453 exploreP:0.7000\n",
      "Episode:106 meanR:34.8200 R:17.0 rate:0.034 gloss:0.5911 dlossA:0.6636 dlossQ:1.1470 exploreP:0.6988\n",
      "Episode:107 meanR:34.9600 R:27.0 rate:0.054 gloss:0.5961 dlossA:0.6624 dlossQ:1.1480 exploreP:0.6970\n",
      "Episode:108 meanR:35.0600 R:30.0 rate:0.06 gloss:0.5927 dlossA:0.6651 dlossQ:1.1379 exploreP:0.6949\n",
      "Episode:109 meanR:34.9400 R:10.0 rate:0.02 gloss:0.5949 dlossA:0.6661 dlossQ:1.1366 exploreP:0.6942\n",
      "Episode:110 meanR:35.2200 R:57.0 rate:0.114 gloss:0.5951 dlossA:0.6643 dlossQ:1.1443 exploreP:0.6903\n",
      "Episode:111 meanR:35.2500 R:66.0 rate:0.132 gloss:0.5963 dlossA:0.6646 dlossQ:1.1465 exploreP:0.6859\n",
      "Episode:112 meanR:35.3700 R:29.0 rate:0.058 gloss:0.5929 dlossA:0.6636 dlossQ:1.1402 exploreP:0.6839\n",
      "Episode:113 meanR:36.3200 R:107.0 rate:0.214 gloss:0.5971 dlossA:0.6651 dlossQ:1.1379 exploreP:0.6767\n",
      "Episode:114 meanR:36.7400 R:70.0 rate:0.14 gloss:0.5977 dlossA:0.6628 dlossQ:1.1549 exploreP:0.6721\n",
      "Episode:115 meanR:36.9400 R:47.0 rate:0.094 gloss:0.6010 dlossA:0.6657 dlossQ:1.1416 exploreP:0.6690\n",
      "Episode:116 meanR:36.9500 R:17.0 rate:0.034 gloss:0.5955 dlossA:0.6632 dlossQ:1.1559 exploreP:0.6679\n",
      "Episode:117 meanR:37.5200 R:88.0 rate:0.176 gloss:0.5997 dlossA:0.6642 dlossQ:1.1466 exploreP:0.6621\n",
      "Episode:118 meanR:37.6600 R:40.0 rate:0.08 gloss:0.6006 dlossA:0.6643 dlossQ:1.1459 exploreP:0.6595\n",
      "Episode:119 meanR:37.3000 R:16.0 rate:0.032 gloss:0.5892 dlossA:0.6629 dlossQ:1.1353 exploreP:0.6585\n",
      "Episode:120 meanR:37.8300 R:81.0 rate:0.162 gloss:0.6030 dlossA:0.6624 dlossQ:1.1574 exploreP:0.6532\n",
      "Episode:121 meanR:38.1300 R:53.0 rate:0.106 gloss:0.5970 dlossA:0.6648 dlossQ:1.1427 exploreP:0.6498\n",
      "Episode:122 meanR:38.0400 R:32.0 rate:0.064 gloss:0.6000 dlossA:0.6652 dlossQ:1.1470 exploreP:0.6478\n",
      "Episode:123 meanR:38.6300 R:77.0 rate:0.154 gloss:0.5949 dlossA:0.6640 dlossQ:1.1469 exploreP:0.6429\n",
      "Episode:124 meanR:38.2800 R:22.0 rate:0.044 gloss:0.6010 dlossA:0.6653 dlossQ:1.1427 exploreP:0.6415\n",
      "Episode:125 meanR:38.2700 R:45.0 rate:0.09 gloss:0.6001 dlossA:0.6648 dlossQ:1.1395 exploreP:0.6387\n",
      "Episode:126 meanR:39.5700 R:143.0 rate:0.286 gloss:0.5961 dlossA:0.6630 dlossQ:1.1432 exploreP:0.6297\n",
      "Episode:127 meanR:39.9500 R:80.0 rate:0.16 gloss:0.5979 dlossA:0.6648 dlossQ:1.1396 exploreP:0.6248\n",
      "Episode:128 meanR:40.1900 R:37.0 rate:0.074 gloss:0.6023 dlossA:0.6642 dlossQ:1.1357 exploreP:0.6225\n",
      "Episode:129 meanR:41.6100 R:170.0 rate:0.34 gloss:0.5994 dlossA:0.6645 dlossQ:1.1367 exploreP:0.6122\n",
      "Episode:130 meanR:42.2500 R:81.0 rate:0.162 gloss:0.5922 dlossA:0.6639 dlossQ:1.1522 exploreP:0.6074\n",
      "Episode:131 meanR:42.4400 R:35.0 rate:0.07 gloss:0.5949 dlossA:0.6620 dlossQ:1.1367 exploreP:0.6053\n",
      "Episode:132 meanR:43.3600 R:145.0 rate:0.29 gloss:0.5975 dlossA:0.6645 dlossQ:1.1417 exploreP:0.5967\n",
      "Episode:133 meanR:44.0500 R:85.0 rate:0.17 gloss:0.5996 dlossA:0.6681 dlossQ:1.1406 exploreP:0.5917\n",
      "Episode:134 meanR:44.1400 R:23.0 rate:0.046 gloss:0.6045 dlossA:0.6653 dlossQ:1.1273 exploreP:0.5904\n",
      "Episode:135 meanR:45.1500 R:135.0 rate:0.27 gloss:0.6004 dlossA:0.6673 dlossQ:1.1397 exploreP:0.5826\n",
      "Episode:136 meanR:45.7600 R:83.0 rate:0.166 gloss:0.6022 dlossA:0.6674 dlossQ:1.1322 exploreP:0.5779\n",
      "Episode:137 meanR:46.6500 R:101.0 rate:0.202 gloss:0.5991 dlossA:0.6671 dlossQ:1.1338 exploreP:0.5722\n",
      "Episode:138 meanR:46.5100 R:33.0 rate:0.066 gloss:0.5990 dlossA:0.6603 dlossQ:1.1473 exploreP:0.5703\n",
      "Episode:139 meanR:46.4500 R:17.0 rate:0.034 gloss:0.6003 dlossA:0.6685 dlossQ:1.1282 exploreP:0.5694\n",
      "Episode:140 meanR:46.7100 R:61.0 rate:0.122 gloss:0.6008 dlossA:0.6678 dlossQ:1.1258 exploreP:0.5660\n",
      "Episode:141 meanR:47.2400 R:96.0 rate:0.192 gloss:0.6036 dlossA:0.6687 dlossQ:1.1271 exploreP:0.5607\n",
      "Episode:142 meanR:47.3500 R:47.0 rate:0.094 gloss:0.5975 dlossA:0.6682 dlossQ:1.1346 exploreP:0.5581\n",
      "Episode:143 meanR:49.5000 R:233.0 rate:0.466 gloss:0.6015 dlossA:0.6681 dlossQ:1.1249 exploreP:0.5454\n",
      "Episode:144 meanR:52.9500 R:371.0 rate:0.742 gloss:0.6045 dlossA:0.6706 dlossQ:1.1143 exploreP:0.5259\n",
      "Episode:145 meanR:54.8900 R:230.0 rate:0.46 gloss:0.6049 dlossA:0.6759 dlossQ:1.0964 exploreP:0.5142\n",
      "Episode:146 meanR:55.1000 R:36.0 rate:0.072 gloss:0.6065 dlossA:0.6787 dlossQ:1.0958 exploreP:0.5124\n",
      "Episode:147 meanR:56.7600 R:195.0 rate:0.39 gloss:0.6176 dlossA:0.6840 dlossQ:1.0722 exploreP:0.5027\n",
      "Episode:148 meanR:56.8800 R:43.0 rate:0.086 gloss:0.6192 dlossA:0.6837 dlossQ:1.0736 exploreP:0.5006\n",
      "Episode:149 meanR:58.2900 R:152.0 rate:0.304 gloss:0.6228 dlossA:0.6866 dlossQ:1.0635 exploreP:0.4932\n",
      "Episode:150 meanR:58.3500 R:118.0 rate:0.236 gloss:0.6276 dlossA:0.6893 dlossQ:1.0584 exploreP:0.4875\n",
      "Episode:151 meanR:58.4000 R:35.0 rate:0.07 gloss:0.6360 dlossA:0.6947 dlossQ:1.0413 exploreP:0.4859\n",
      "Episode:152 meanR:59.5600 R:130.0 rate:0.26 gloss:0.6338 dlossA:0.6920 dlossQ:1.0549 exploreP:0.4797\n",
      "Episode:153 meanR:59.2300 R:12.0 rate:0.024 gloss:0.6350 dlossA:0.6906 dlossQ:1.0848 exploreP:0.4791\n",
      "Episode:154 meanR:59.4200 R:76.0 rate:0.152 gloss:0.6386 dlossA:0.6940 dlossQ:1.0536 exploreP:0.4756\n",
      "Episode:155 meanR:60.3800 R:146.0 rate:0.292 gloss:0.6441 dlossA:0.6980 dlossQ:1.0377 exploreP:0.4688\n",
      "Episode:156 meanR:60.4700 R:28.0 rate:0.056 gloss:0.6520 dlossA:0.7040 dlossQ:1.0247 exploreP:0.4676\n",
      "Episode:157 meanR:61.6500 R:167.0 rate:0.334 gloss:0.6515 dlossA:0.7029 dlossQ:1.0288 exploreP:0.4600\n",
      "Episode:158 meanR:61.4400 R:26.0 rate:0.052 gloss:0.6432 dlossA:0.7119 dlossQ:0.9960 exploreP:0.4588\n",
      "Episode:159 meanR:64.4600 R:318.0 rate:0.636 gloss:0.6665 dlossA:0.7066 dlossQ:1.0273 exploreP:0.4448\n",
      "Episode:160 meanR:65.5300 R:132.0 rate:0.264 gloss:0.6694 dlossA:0.7109 dlossQ:1.0212 exploreP:0.4391\n",
      "Episode:161 meanR:66.0300 R:77.0 rate:0.154 gloss:0.6777 dlossA:0.7158 dlossQ:1.0116 exploreP:0.4358\n",
      "Episode:162 meanR:66.7300 R:86.0 rate:0.172 gloss:0.6795 dlossA:0.7169 dlossQ:1.0064 exploreP:0.4321\n",
      "Episode:163 meanR:67.6100 R:150.0 rate:0.3 gloss:0.6829 dlossA:0.7164 dlossQ:1.0024 exploreP:0.4258\n",
      "Episode:164 meanR:67.8900 R:37.0 rate:0.074 gloss:0.6949 dlossA:0.7293 dlossQ:0.9841 exploreP:0.4243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:165 meanR:70.1400 R:244.0 rate:0.488 gloss:0.6950 dlossA:0.7252 dlossQ:1.0001 exploreP:0.4143\n",
      "Episode:166 meanR:71.5100 R:159.0 rate:0.318 gloss:0.7054 dlossA:0.7256 dlossQ:0.9923 exploreP:0.4079\n",
      "Episode:167 meanR:74.8800 R:361.0 rate:0.722 gloss:0.7167 dlossA:0.7350 dlossQ:0.9822 exploreP:0.3938\n",
      "Episode:168 meanR:76.9600 R:222.0 rate:0.444 gloss:0.7267 dlossA:0.7435 dlossQ:0.9803 exploreP:0.3854\n",
      "Episode:169 meanR:79.7600 R:322.0 rate:0.644 gloss:0.7309 dlossA:0.7456 dlossQ:0.9890 exploreP:0.3735\n",
      "Episode:170 meanR:80.5600 R:131.0 rate:0.262 gloss:0.7356 dlossA:0.7420 dlossQ:0.9841 exploreP:0.3688\n",
      "Episode:171 meanR:82.8000 R:239.0 rate:0.478 gloss:0.7454 dlossA:0.7540 dlossQ:0.9696 exploreP:0.3603\n",
      "Episode:172 meanR:86.6000 R:408.0 rate:0.816 gloss:0.7547 dlossA:0.7618 dlossQ:0.9618 exploreP:0.3463\n",
      "Episode:173 meanR:90.3100 R:419.0 rate:0.838 gloss:0.7645 dlossA:0.7687 dlossQ:0.9558 exploreP:0.3325\n",
      "Episode:174 meanR:92.6900 R:255.0 rate:0.51 gloss:0.7721 dlossA:0.7640 dlossQ:0.9588 exploreP:0.3244\n",
      "Episode:175 meanR:94.2900 R:175.0 rate:0.35 gloss:0.7855 dlossA:0.7726 dlossQ:0.9503 exploreP:0.3189\n",
      "Episode:176 meanR:95.5300 R:163.0 rate:0.326 gloss:0.7954 dlossA:0.7807 dlossQ:0.9408 exploreP:0.3139\n",
      "Episode:177 meanR:99.5100 R:433.0 rate:0.866 gloss:0.8043 dlossA:0.7854 dlossQ:0.9360 exploreP:0.3011\n",
      "Episode:178 meanR:104.2100 R:500.0 rate:1.0 gloss:0.8202 dlossA:0.7944 dlossQ:0.9305 exploreP:0.2869\n",
      "Episode:179 meanR:108.7200 R:481.0 rate:0.962 gloss:0.8420 dlossA:0.8046 dlossQ:0.9313 exploreP:0.2739\n",
      "Episode:180 meanR:109.0100 R:128.0 rate:0.256 gloss:0.8558 dlossA:0.8048 dlossQ:0.9165 exploreP:0.2705\n",
      "Episode:181 meanR:113.7900 R:500.0 rate:1.0 gloss:0.8726 dlossA:0.8263 dlossQ:0.9077 exploreP:0.2578\n",
      "Episode:182 meanR:116.8800 R:328.0 rate:0.656 gloss:0.8911 dlossA:0.8245 dlossQ:0.8875 exploreP:0.2498\n",
      "Episode:183 meanR:118.7800 R:245.0 rate:0.49 gloss:0.9071 dlossA:0.8505 dlossQ:0.8859 exploreP:0.2440\n",
      "Episode:184 meanR:123.6100 R:500.0 rate:1.0 gloss:0.9237 dlossA:0.8510 dlossQ:0.8755 exploreP:0.2326\n",
      "Episode:185 meanR:126.8100 R:339.0 rate:0.678 gloss:0.9491 dlossA:0.8538 dlossQ:0.8635 exploreP:0.2252\n",
      "Episode:186 meanR:126.4900 R:25.0 rate:0.05 gloss:0.9705 dlossA:0.8830 dlossQ:0.8641 exploreP:0.2246\n",
      "Episode:187 meanR:127.8500 R:168.0 rate:0.336 gloss:0.9614 dlossA:0.8808 dlossQ:0.8581 exploreP:0.2211\n",
      "Episode:188 meanR:131.5400 R:390.0 rate:0.78 gloss:0.9794 dlossA:0.8975 dlossQ:0.8597 exploreP:0.2130\n",
      "Episode:189 meanR:135.2700 R:500.0 rate:1.0 gloss:0.9951 dlossA:0.8756 dlossQ:0.8423 exploreP:0.2031\n",
      "Episode:190 meanR:139.0000 R:471.0 rate:0.942 gloss:1.0344 dlossA:0.8863 dlossQ:0.8251 exploreP:0.1942\n",
      "Episode:191 meanR:143.2200 R:431.0 rate:0.862 gloss:1.0731 dlossA:0.9216 dlossQ:0.8130 exploreP:0.1864\n",
      "Episode:192 meanR:146.0300 R:303.0 rate:0.606 gloss:1.1035 dlossA:0.9286 dlossQ:0.8073 exploreP:0.1812\n",
      "Episode:193 meanR:150.5000 R:500.0 rate:1.0 gloss:1.1299 dlossA:0.9581 dlossQ:0.8130 exploreP:0.1728\n",
      "Episode:194 meanR:154.2400 R:500.0 rate:1.0 gloss:1.1677 dlossA:0.9825 dlossQ:0.8036 exploreP:0.1649\n",
      "Episode:195 meanR:156.2700 R:232.0 rate:0.464 gloss:1.1938 dlossA:1.0150 dlossQ:0.7992 exploreP:0.1613\n",
      "Episode:196 meanR:160.9400 R:500.0 rate:1.0 gloss:1.2251 dlossA:1.0284 dlossQ:0.8021 exploreP:0.1539\n",
      "Episode:197 meanR:161.4000 R:144.0 rate:0.288 gloss:1.2499 dlossA:1.0661 dlossQ:0.7987 exploreP:0.1519\n",
      "Episode:198 meanR:165.6100 R:500.0 rate:1.0 gloss:1.2696 dlossA:1.0476 dlossQ:0.8041 exploreP:0.1450\n",
      "Episode:199 meanR:167.4700 R:223.0 rate:0.446 gloss:1.2980 dlossA:1.0815 dlossQ:0.8055 exploreP:0.1420\n",
      "Episode:200 meanR:172.1000 R:500.0 rate:1.0 gloss:1.3197 dlossA:1.0943 dlossQ:0.8057 exploreP:0.1356\n",
      "Episode:201 meanR:176.2800 R:500.0 rate:1.0 gloss:1.3564 dlossA:1.1041 dlossQ:0.8038 exploreP:0.1294\n",
      "Episode:202 meanR:179.2700 R:311.0 rate:0.622 gloss:1.3974 dlossA:1.1572 dlossQ:0.8054 exploreP:0.1258\n",
      "Episode:203 meanR:183.9000 R:481.0 rate:0.962 gloss:1.4350 dlossA:1.1870 dlossQ:0.8058 exploreP:0.1203\n",
      "Episode:204 meanR:186.5200 R:293.0 rate:0.586 gloss:1.4674 dlossA:1.1859 dlossQ:0.8022 exploreP:0.1171\n",
      "Episode:205 meanR:190.9500 R:470.0 rate:0.94 gloss:1.5172 dlossA:1.2086 dlossQ:0.7935 exploreP:0.1122\n",
      "Episode:206 meanR:195.7800 R:500.0 rate:1.0 gloss:1.5765 dlossA:1.2736 dlossQ:0.7850 exploreP:0.1072\n",
      "Episode:207 meanR:197.1400 R:163.0 rate:0.326 gloss:1.6058 dlossA:1.2630 dlossQ:0.7811 exploreP:0.1057\n",
      "Episode:208 meanR:201.8400 R:500.0 rate:1.0 gloss:1.6507 dlossA:1.2974 dlossQ:0.7776 exploreP:0.1010\n",
      "Episode:209 meanR:206.7400 R:500.0 rate:1.0 gloss:1.6905 dlossA:1.3659 dlossQ:0.7752 exploreP:0.0966\n",
      "Episode:210 meanR:211.1700 R:500.0 rate:1.0 gloss:1.7401 dlossA:1.3123 dlossQ:0.7666 exploreP:0.0923\n",
      "Episode:211 meanR:215.5100 R:500.0 rate:1.0 gloss:1.8246 dlossA:1.4063 dlossQ:0.7606 exploreP:0.0883\n",
      "Episode:212 meanR:220.2200 R:500.0 rate:1.0 gloss:1.8913 dlossA:1.4453 dlossQ:0.7594 exploreP:0.0845\n",
      "Episode:213 meanR:224.1500 R:500.0 rate:1.0 gloss:1.9452 dlossA:1.4875 dlossQ:0.7571 exploreP:0.0809\n",
      "Episode:214 meanR:228.4500 R:500.0 rate:1.0 gloss:2.0225 dlossA:1.5287 dlossQ:0.7519 exploreP:0.0774\n",
      "Episode:215 meanR:232.9800 R:500.0 rate:1.0 gloss:2.0868 dlossA:1.5692 dlossQ:0.7517 exploreP:0.0741\n",
      "Episode:216 meanR:234.4900 R:168.0 rate:0.336 gloss:2.1265 dlossA:1.6757 dlossQ:0.7573 exploreP:0.0731\n",
      "Episode:217 meanR:238.6100 R:500.0 rate:1.0 gloss:2.1730 dlossA:1.7130 dlossQ:0.7576 exploreP:0.0700\n",
      "Episode:218 meanR:243.2100 R:500.0 rate:1.0 gloss:2.2212 dlossA:1.7319 dlossQ:0.7553 exploreP:0.0671\n",
      "Episode:219 meanR:248.0500 R:500.0 rate:1.0 gloss:2.2901 dlossA:1.7535 dlossQ:0.7551 exploreP:0.0643\n",
      "Episode:220 meanR:251.5600 R:432.0 rate:0.864 gloss:2.3637 dlossA:1.8560 dlossQ:0.7590 exploreP:0.0620\n",
      "Episode:221 meanR:256.0300 R:500.0 rate:1.0 gloss:2.4517 dlossA:1.9134 dlossQ:0.7691 exploreP:0.0594\n",
      "Episode:222 meanR:260.4500 R:474.0 rate:0.948 gloss:2.5074 dlossA:2.0311 dlossQ:0.7719 exploreP:0.0572\n",
      "Episode:223 meanR:264.6800 R:500.0 rate:1.0 gloss:2.5222 dlossA:2.0117 dlossQ:0.7743 exploreP:0.0549\n",
      "Episode:224 meanR:266.0600 R:160.0 rate:0.32 gloss:2.6117 dlossA:2.1177 dlossQ:0.7937 exploreP:0.0541\n",
      "Episode:225 meanR:270.6100 R:500.0 rate:1.0 gloss:2.6411 dlossA:2.0909 dlossQ:0.7945 exploreP:0.0520\n",
      "Episode:226 meanR:273.7000 R:452.0 rate:0.904 gloss:2.7717 dlossA:2.1835 dlossQ:0.8062 exploreP:0.0501\n",
      "Episode:227 meanR:277.9000 R:500.0 rate:1.0 gloss:2.8805 dlossA:2.2750 dlossQ:0.8209 exploreP:0.0482\n",
      "Episode:228 meanR:279.0000 R:147.0 rate:0.294 gloss:2.9876 dlossA:2.2449 dlossQ:0.8221 exploreP:0.0476\n",
      "Episode:229 meanR:282.3000 R:500.0 rate:1.0 gloss:3.1150 dlossA:2.4467 dlossQ:0.8523 exploreP:0.0458\n",
      "Episode:230 meanR:286.4900 R:500.0 rate:1.0 gloss:3.1932 dlossA:2.4401 dlossQ:0.8566 exploreP:0.0440\n",
      "Episode:231 meanR:291.1400 R:500.0 rate:1.0 gloss:3.3893 dlossA:2.5792 dlossQ:0.9158 exploreP:0.0424\n",
      "Episode:232 meanR:294.6900 R:500.0 rate:1.0 gloss:3.4802 dlossA:2.7997 dlossQ:0.9209 exploreP:0.0408\n",
      "Episode:233 meanR:298.8400 R:500.0 rate:1.0 gloss:3.6349 dlossA:2.9608 dlossQ:0.9480 exploreP:0.0393\n",
      "Episode:234 meanR:303.6100 R:500.0 rate:1.0 gloss:3.7605 dlossA:3.0168 dlossQ:0.9836 exploreP:0.0379\n",
      "Episode:235 meanR:305.4700 R:321.0 rate:0.642 gloss:3.8581 dlossA:2.9928 dlossQ:1.0027 exploreP:0.0370\n",
      "Episode:236 meanR:309.6400 R:500.0 rate:1.0 gloss:4.0058 dlossA:2.9364 dlossQ:1.0060 exploreP:0.0357\n",
      "Episode:237 meanR:313.6300 R:500.0 rate:1.0 gloss:4.1065 dlossA:3.1055 dlossQ:0.9817 exploreP:0.0344\n",
      "Episode:238 meanR:318.3000 R:500.0 rate:1.0 gloss:4.3287 dlossA:3.3215 dlossQ:1.0618 exploreP:0.0332\n",
      "Episode:239 meanR:323.1300 R:500.0 rate:1.0 gloss:4.4527 dlossA:3.3956 dlossQ:1.1429 exploreP:0.0321\n",
      "Episode:240 meanR:327.5200 R:500.0 rate:1.0 gloss:4.6313 dlossA:3.5225 dlossQ:1.1698 exploreP:0.0310\n",
      "Episode:241 meanR:331.5600 R:500.0 rate:1.0 gloss:4.8300 dlossA:3.8825 dlossQ:1.2251 exploreP:0.0300\n",
      "Episode:242 meanR:336.0900 R:500.0 rate:1.0 gloss:4.9672 dlossA:3.6382 dlossQ:1.2246 exploreP:0.0290\n",
      "Episode:243 meanR:338.7600 R:500.0 rate:1.0 gloss:5.3152 dlossA:4.3134 dlossQ:1.3757 exploreP:0.0281\n",
      "Episode:244 meanR:339.9500 R:490.0 rate:0.98 gloss:5.4322 dlossA:4.1821 dlossQ:1.4071 exploreP:0.0272\n",
      "Episode:245 meanR:342.6500 R:500.0 rate:1.0 gloss:5.5915 dlossA:4.4459 dlossQ:1.3984 exploreP:0.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:246 meanR:347.2900 R:500.0 rate:1.0 gloss:5.8539 dlossA:4.7761 dlossQ:1.5203 exploreP:0.0256\n",
      "Episode:247 meanR:348.3700 R:303.0 rate:0.606 gloss:6.0015 dlossA:4.7427 dlossQ:1.4859 exploreP:0.0251\n",
      "Episode:248 meanR:352.9400 R:500.0 rate:1.0 gloss:6.1646 dlossA:4.8433 dlossQ:1.5560 exploreP:0.0244\n",
      "Episode:249 meanR:356.4200 R:500.0 rate:1.0 gloss:6.6527 dlossA:4.9133 dlossQ:1.6127 exploreP:0.0237\n",
      "Episode:250 meanR:360.2400 R:500.0 rate:1.0 gloss:7.1341 dlossA:5.3243 dlossQ:1.8980 exploreP:0.0230\n",
      "Episode:251 meanR:364.8900 R:500.0 rate:1.0 gloss:7.3818 dlossA:5.9522 dlossQ:2.0194 exploreP:0.0224\n",
      "Episode:252 meanR:368.5900 R:500.0 rate:1.0 gloss:7.7569 dlossA:5.8168 dlossQ:2.3246 exploreP:0.0218\n",
      "Episode:253 meanR:373.4700 R:500.0 rate:1.0 gloss:7.8622 dlossA:6.2360 dlossQ:2.3337 exploreP:0.0212\n",
      "Episode:254 meanR:377.7100 R:500.0 rate:1.0 gloss:8.3947 dlossA:5.9505 dlossQ:2.3900 exploreP:0.0207\n",
      "Episode:255 meanR:381.2500 R:500.0 rate:1.0 gloss:8.9207 dlossA:6.6697 dlossQ:2.9710 exploreP:0.0201\n",
      "Episode:256 meanR:385.9700 R:500.0 rate:1.0 gloss:9.1327 dlossA:6.6499 dlossQ:2.7154 exploreP:0.0196\n",
      "Episode:257 meanR:389.3000 R:500.0 rate:1.0 gloss:9.6881 dlossA:7.2447 dlossQ:3.0051 exploreP:0.0192\n",
      "Episode:258 meanR:392.2500 R:321.0 rate:0.642 gloss:9.7717 dlossA:7.1089 dlossQ:2.9069 exploreP:0.0189\n",
      "Episode:259 meanR:394.0700 R:500.0 rate:1.0 gloss:10.1134 dlossA:7.6524 dlossQ:3.3255 exploreP:0.0184\n",
      "Episode:260 meanR:396.5500 R:380.0 rate:0.76 gloss:10.5271 dlossA:8.0355 dlossQ:3.5039 exploreP:0.0181\n",
      "Episode:261 meanR:400.1700 R:439.0 rate:0.878 gloss:11.0207 dlossA:8.3508 dlossQ:3.6727 exploreP:0.0178\n",
      "Episode:262 meanR:404.3100 R:500.0 rate:1.0 gloss:11.4074 dlossA:8.0841 dlossQ:3.5472 exploreP:0.0174\n",
      "Episode:263 meanR:407.8100 R:500.0 rate:1.0 gloss:12.0827 dlossA:9.0506 dlossQ:3.8259 exploreP:0.0170\n",
      "Episode:264 meanR:412.4400 R:500.0 rate:1.0 gloss:12.6455 dlossA:8.9498 dlossQ:4.4438 exploreP:0.0167\n",
      "Episode:265 meanR:415.0000 R:500.0 rate:1.0 gloss:12.9113 dlossA:9.2563 dlossQ:4.3468 exploreP:0.0164\n",
      "Episode:266 meanR:418.4100 R:500.0 rate:1.0 gloss:13.3846 dlossA:9.8783 dlossQ:4.4649 exploreP:0.0161\n",
      "Episode:267 meanR:417.6100 R:281.0 rate:0.562 gloss:13.5821 dlossA:9.2642 dlossQ:4.4725 exploreP:0.0159\n",
      "Episode:268 meanR:420.3900 R:500.0 rate:1.0 gloss:14.3949 dlossA:10.1989 dlossQ:5.1798 exploreP:0.0156\n",
      "Episode:269 meanR:422.1700 R:500.0 rate:1.0 gloss:14.7188 dlossA:10.3318 dlossQ:5.0720 exploreP:0.0153\n",
      "Episode:270 meanR:425.8600 R:500.0 rate:1.0 gloss:15.8134 dlossA:11.0464 dlossQ:5.3642 exploreP:0.0151\n",
      "Episode:271 meanR:428.4700 R:500.0 rate:1.0 gloss:17.0037 dlossA:12.4052 dlossQ:7.6340 exploreP:0.0148\n",
      "Episode:272 meanR:429.3900 R:500.0 rate:1.0 gloss:17.3760 dlossA:11.8917 dlossQ:8.0136 exploreP:0.0146\n",
      "Episode:273 meanR:430.2000 R:500.0 rate:1.0 gloss:17.5423 dlossA:12.1827 dlossQ:7.3071 exploreP:0.0144\n",
      "Episode:274 meanR:432.6500 R:500.0 rate:1.0 gloss:17.8437 dlossA:12.4684 dlossQ:7.7724 exploreP:0.0142\n",
      "Episode:275 meanR:435.9000 R:500.0 rate:1.0 gloss:18.8585 dlossA:12.8175 dlossQ:8.7879 exploreP:0.0140\n",
      "Episode:276 meanR:437.0700 R:280.0 rate:0.56 gloss:19.5171 dlossA:14.4635 dlossQ:10.0948 exploreP:0.0138\n",
      "Episode:277 meanR:437.7400 R:500.0 rate:1.0 gloss:19.4794 dlossA:14.2750 dlossQ:10.0220 exploreP:0.0137\n",
      "Episode:278 meanR:437.7400 R:500.0 rate:1.0 gloss:19.2479 dlossA:13.1810 dlossQ:8.1991 exploreP:0.0135\n",
      "Episode:279 meanR:437.9300 R:500.0 rate:1.0 gloss:19.2184 dlossA:12.7789 dlossQ:8.2516 exploreP:0.0133\n",
      "Episode:280 meanR:441.6500 R:500.0 rate:1.0 gloss:19.7178 dlossA:12.8689 dlossQ:9.1522 exploreP:0.0131\n",
      "Episode:281 meanR:440.1600 R:351.0 rate:0.702 gloss:20.2132 dlossA:13.2129 dlossQ:8.6901 exploreP:0.0130\n",
      "Episode:282 meanR:441.8800 R:500.0 rate:1.0 gloss:20.4117 dlossA:13.6094 dlossQ:9.2316 exploreP:0.0129\n",
      "Episode:283 meanR:444.4300 R:500.0 rate:1.0 gloss:21.0576 dlossA:14.3824 dlossQ:9.9973 exploreP:0.0127\n",
      "Episode:284 meanR:444.4300 R:500.0 rate:1.0 gloss:21.2859 dlossA:14.3077 dlossQ:9.7336 exploreP:0.0126\n",
      "Episode:285 meanR:446.0400 R:500.0 rate:1.0 gloss:21.7273 dlossA:14.3610 dlossQ:10.6378 exploreP:0.0125\n",
      "Episode:286 meanR:450.7900 R:500.0 rate:1.0 gloss:21.9338 dlossA:14.6947 dlossQ:11.5450 exploreP:0.0124\n",
      "Episode:287 meanR:454.1100 R:500.0 rate:1.0 gloss:22.1877 dlossA:14.8501 dlossQ:10.6833 exploreP:0.0123\n",
      "Episode:288 meanR:455.2100 R:500.0 rate:1.0 gloss:22.3380 dlossA:14.8489 dlossQ:11.0824 exploreP:0.0121\n",
      "Episode:289 meanR:455.2100 R:500.0 rate:1.0 gloss:23.1187 dlossA:15.9172 dlossQ:12.0685 exploreP:0.0120\n",
      "Episode:290 meanR:455.5000 R:500.0 rate:1.0 gloss:22.6503 dlossA:14.2668 dlossQ:11.1230 exploreP:0.0119\n",
      "Episode:291 meanR:455.3600 R:417.0 rate:0.834 gloss:23.6871 dlossA:14.1828 dlossQ:10.9900 exploreP:0.0119\n",
      "Episode:292 meanR:455.9700 R:364.0 rate:0.728 gloss:23.8660 dlossA:13.4610 dlossQ:11.1626 exploreP:0.0118\n",
      "Episode:293 meanR:455.9700 R:500.0 rate:1.0 gloss:24.7072 dlossA:15.5919 dlossQ:12.9831 exploreP:0.0117\n",
      "Episode:294 meanR:455.9700 R:500.0 rate:1.0 gloss:23.9596 dlossA:14.4057 dlossQ:17.8106 exploreP:0.0116\n",
      "Episode:295 meanR:458.6500 R:500.0 rate:1.0 gloss:22.6672 dlossA:14.3266 dlossQ:9.8086 exploreP:0.0115\n",
      "Episode:296 meanR:458.6500 R:500.0 rate:1.0 gloss:21.4063 dlossA:12.9274 dlossQ:10.7362 exploreP:0.0115\n",
      "Episode:297 meanR:462.2100 R:500.0 rate:1.0 gloss:20.9419 dlossA:14.0980 dlossQ:10.9699 exploreP:0.0114\n",
      "Episode:298 meanR:462.2100 R:500.0 rate:1.0 gloss:19.6504 dlossA:13.3422 dlossQ:9.3832 exploreP:0.0113\n",
      "Episode:299 meanR:463.4000 R:342.0 rate:0.684 gloss:18.8649 dlossA:11.1682 dlossQ:8.0605 exploreP:0.0113\n",
      "Episode:300 meanR:463.4000 R:500.0 rate:1.0 gloss:17.9088 dlossA:10.8150 dlossQ:7.8927 exploreP:0.0112\n",
      "Episode:301 meanR:463.4000 R:500.0 rate:1.0 gloss:17.4276 dlossA:10.2110 dlossQ:7.3357 exploreP:0.0112\n",
      "Episode:302 meanR:465.2900 R:500.0 rate:1.0 gloss:17.2747 dlossA:9.8262 dlossQ:6.6683 exploreP:0.0111\n",
      "Episode:303 meanR:465.4800 R:500.0 rate:1.0 gloss:17.7246 dlossA:10.2016 dlossQ:7.0901 exploreP:0.0110\n",
      "Episode:304 meanR:467.5500 R:500.0 rate:1.0 gloss:18.2155 dlossA:11.1175 dlossQ:7.5589 exploreP:0.0110\n",
      "Episode:305 meanR:467.8500 R:500.0 rate:1.0 gloss:18.4978 dlossA:11.0084 dlossQ:7.7068 exploreP:0.0109\n",
      "Episode:306 meanR:467.8500 R:500.0 rate:1.0 gloss:19.2287 dlossA:11.2336 dlossQ:8.4037 exploreP:0.0109\n",
      "Episode:307 meanR:469.0000 R:278.0 rate:0.556 gloss:20.5016 dlossA:14.1137 dlossQ:9.8962 exploreP:0.0109\n",
      "Episode:308 meanR:469.0000 R:500.0 rate:1.0 gloss:19.5291 dlossA:11.5061 dlossQ:8.5714 exploreP:0.0108\n",
      "Episode:309 meanR:469.0000 R:500.0 rate:1.0 gloss:19.9285 dlossA:11.8876 dlossQ:9.1944 exploreP:0.0108\n",
      "Episode:310 meanR:469.0000 R:500.0 rate:1.0 gloss:20.8261 dlossA:12.2104 dlossQ:9.5391 exploreP:0.0108\n",
      "Episode:311 meanR:469.0000 R:500.0 rate:1.0 gloss:21.1473 dlossA:11.9213 dlossQ:9.6384 exploreP:0.0107\n",
      "Episode:312 meanR:469.0000 R:500.0 rate:1.0 gloss:21.6024 dlossA:10.8257 dlossQ:8.9602 exploreP:0.0107\n",
      "Episode:313 meanR:469.0000 R:500.0 rate:1.0 gloss:22.9492 dlossA:12.5232 dlossQ:10.4775 exploreP:0.0107\n",
      "Episode:314 meanR:469.0000 R:500.0 rate:1.0 gloss:22.6866 dlossA:11.5507 dlossQ:10.0127 exploreP:0.0106\n",
      "Episode:315 meanR:469.0000 R:500.0 rate:1.0 gloss:22.4665 dlossA:11.0673 dlossQ:10.5673 exploreP:0.0106\n",
      "Episode:316 meanR:472.3200 R:500.0 rate:1.0 gloss:22.4369 dlossA:11.5591 dlossQ:8.6098 exploreP:0.0106\n",
      "Episode:317 meanR:472.3200 R:500.0 rate:1.0 gloss:22.6405 dlossA:11.7313 dlossQ:8.9295 exploreP:0.0105\n",
      "Episode:318 meanR:472.3200 R:500.0 rate:1.0 gloss:22.0098 dlossA:11.4340 dlossQ:9.7986 exploreP:0.0105\n",
      "Episode:319 meanR:472.3200 R:500.0 rate:1.0 gloss:22.0871 dlossA:11.5966 dlossQ:9.5188 exploreP:0.0105\n",
      "Episode:320 meanR:473.0000 R:500.0 rate:1.0 gloss:22.0155 dlossA:9.6734 dlossQ:9.4331 exploreP:0.0105\n",
      "Episode:321 meanR:473.0000 R:500.0 rate:1.0 gloss:21.1681 dlossA:8.4010 dlossQ:9.1550 exploreP:0.0104\n",
      "Episode:322 meanR:473.2600 R:500.0 rate:1.0 gloss:21.7129 dlossA:10.2411 dlossQ:10.6291 exploreP:0.0104\n",
      "Episode:323 meanR:473.2600 R:500.0 rate:1.0 gloss:20.9382 dlossA:8.4495 dlossQ:9.4766 exploreP:0.0104\n",
      "Episode:324 meanR:476.6600 R:500.0 rate:1.0 gloss:21.6240 dlossA:10.1389 dlossQ:10.0760 exploreP:0.0104\n",
      "Episode:325 meanR:476.6600 R:500.0 rate:1.0 gloss:21.7787 dlossA:10.0776 dlossQ:9.5997 exploreP:0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:326 meanR:477.1400 R:500.0 rate:1.0 gloss:21.2632 dlossA:8.3736 dlossQ:9.6350 exploreP:0.0103\n",
      "Episode:327 meanR:477.1400 R:500.0 rate:1.0 gloss:21.7871 dlossA:10.1387 dlossQ:10.4156 exploreP:0.0103\n",
      "Episode:328 meanR:480.6700 R:500.0 rate:1.0 gloss:21.5989 dlossA:10.3015 dlossQ:10.6448 exploreP:0.0103\n",
      "Episode:329 meanR:480.6700 R:500.0 rate:1.0 gloss:21.4148 dlossA:11.2631 dlossQ:11.1940 exploreP:0.0103\n",
      "Episode:330 meanR:480.6700 R:500.0 rate:1.0 gloss:21.2416 dlossA:9.4805 dlossQ:8.9223 exploreP:0.0103\n",
      "Episode:331 meanR:480.6700 R:500.0 rate:1.0 gloss:21.4556 dlossA:9.0298 dlossQ:9.4877 exploreP:0.0103\n",
      "Episode:332 meanR:480.6700 R:500.0 rate:1.0 gloss:21.9906 dlossA:8.2837 dlossQ:9.0323 exploreP:0.0103\n",
      "Episode:333 meanR:480.6700 R:500.0 rate:1.0 gloss:22.6517 dlossA:7.7198 dlossQ:9.1126 exploreP:0.0102\n",
      "Episode:334 meanR:480.6700 R:500.0 rate:1.0 gloss:23.9301 dlossA:9.3846 dlossQ:9.9356 exploreP:0.0102\n",
      "Episode:335 meanR:482.4600 R:500.0 rate:1.0 gloss:23.3297 dlossA:8.7661 dlossQ:14.3712 exploreP:0.0102\n",
      "Episode:336 meanR:482.4600 R:500.0 rate:1.0 gloss:23.4249 dlossA:8.6098 dlossQ:9.5738 exploreP:0.0102\n",
      "Episode:337 meanR:482.4600 R:500.0 rate:1.0 gloss:22.5999 dlossA:8.2654 dlossQ:9.6064 exploreP:0.0102\n",
      "Episode:338 meanR:482.4600 R:500.0 rate:1.0 gloss:22.0331 dlossA:8.4301 dlossQ:9.9353 exploreP:0.0102\n",
      "Episode:339 meanR:482.4600 R:500.0 rate:1.0 gloss:21.5013 dlossA:6.5797 dlossQ:8.5612 exploreP:0.0102\n",
      "Episode:340 meanR:482.4600 R:500.0 rate:1.0 gloss:21.4723 dlossA:5.5392 dlossQ:9.5842 exploreP:0.0102\n",
      "Episode:341 meanR:482.4600 R:500.0 rate:1.0 gloss:21.3656 dlossA:6.5757 dlossQ:11.8498 exploreP:0.0102\n",
      "Episode:342 meanR:482.4600 R:500.0 rate:1.0 gloss:20.6938 dlossA:5.1795 dlossQ:10.3832 exploreP:0.0102\n",
      "Episode:343 meanR:482.4600 R:500.0 rate:1.0 gloss:20.5309 dlossA:5.3845 dlossQ:11.0294 exploreP:0.0101\n",
      "Episode:344 meanR:482.5600 R:500.0 rate:1.0 gloss:21.0203 dlossA:6.0479 dlossQ:11.6653 exploreP:0.0101\n",
      "Episode:345 meanR:482.5600 R:500.0 rate:1.0 gloss:20.7878 dlossA:6.0269 dlossQ:11.1302 exploreP:0.0101\n",
      "Episode:346 meanR:482.5600 R:500.0 rate:1.0 gloss:20.6002 dlossA:5.0357 dlossQ:10.2802 exploreP:0.0101\n",
      "Episode:347 meanR:484.5300 R:500.0 rate:1.0 gloss:20.8709 dlossA:6.0864 dlossQ:11.3527 exploreP:0.0101\n",
      "Episode:348 meanR:484.5300 R:500.0 rate:1.0 gloss:20.6882 dlossA:6.3848 dlossQ:11.2867 exploreP:0.0101\n",
      "Episode:349 meanR:484.5300 R:500.0 rate:1.0 gloss:20.4912 dlossA:5.8485 dlossQ:10.8649 exploreP:0.0101\n",
      "Episode:350 meanR:484.5300 R:500.0 rate:1.0 gloss:20.5781 dlossA:5.8851 dlossQ:10.3394 exploreP:0.0101\n",
      "Episode:351 meanR:484.5300 R:500.0 rate:1.0 gloss:20.6364 dlossA:4.4168 dlossQ:10.2387 exploreP:0.0101\n",
      "Episode:352 meanR:484.5300 R:500.0 rate:1.0 gloss:22.4356 dlossA:4.9891 dlossQ:10.6714 exploreP:0.0101\n",
      "Episode:353 meanR:484.5300 R:500.0 rate:1.0 gloss:22.7565 dlossA:4.0718 dlossQ:11.3899 exploreP:0.0101\n",
      "Episode:354 meanR:484.5300 R:500.0 rate:1.0 gloss:23.6265 dlossA:5.8712 dlossQ:12.2326 exploreP:0.0101\n",
      "Episode:355 meanR:484.5300 R:500.0 rate:1.0 gloss:22.5587 dlossA:4.7348 dlossQ:11.3530 exploreP:0.0101\n",
      "Episode:356 meanR:484.5300 R:500.0 rate:1.0 gloss:22.2075 dlossA:3.9114 dlossQ:10.3353 exploreP:0.0101\n",
      "Episode:357 meanR:484.5300 R:500.0 rate:1.0 gloss:23.8850 dlossA:4.8088 dlossQ:11.2752 exploreP:0.0101\n",
      "Episode:358 meanR:486.3200 R:500.0 rate:1.0 gloss:24.0585 dlossA:5.5243 dlossQ:11.5842 exploreP:0.0101\n",
      "Episode:359 meanR:486.3200 R:500.0 rate:1.0 gloss:23.8598 dlossA:3.9918 dlossQ:9.6532 exploreP:0.0101\n",
      "Episode:360 meanR:487.5200 R:500.0 rate:1.0 gloss:24.2252 dlossA:1.8649 dlossQ:10.5629 exploreP:0.0101\n",
      "Episode:361 meanR:488.1300 R:500.0 rate:1.0 gloss:25.3876 dlossA:2.1337 dlossQ:12.0718 exploreP:0.0101\n",
      "Episode:362 meanR:488.1300 R:500.0 rate:1.0 gloss:26.5129 dlossA:2.5831 dlossQ:13.1280 exploreP:0.0101\n",
      "Episode:363 meanR:488.1300 R:500.0 rate:1.0 gloss:27.3831 dlossA:2.4024 dlossQ:13.7163 exploreP:0.0101\n",
      "Episode:364 meanR:488.1300 R:500.0 rate:1.0 gloss:27.2447 dlossA:2.1626 dlossQ:14.4935 exploreP:0.0101\n",
      "Episode:365 meanR:488.1300 R:500.0 rate:1.0 gloss:27.8724 dlossA:2.4688 dlossQ:14.7369 exploreP:0.0100\n",
      "Episode:366 meanR:488.1300 R:500.0 rate:1.0 gloss:27.0964 dlossA:2.1834 dlossQ:15.0305 exploreP:0.0100\n",
      "Episode:367 meanR:490.3200 R:500.0 rate:1.0 gloss:26.4958 dlossA:2.1377 dlossQ:15.0041 exploreP:0.0100\n",
      "Episode:368 meanR:490.3200 R:500.0 rate:1.0 gloss:27.4227 dlossA:2.3207 dlossQ:14.7002 exploreP:0.0100\n",
      "Episode:369 meanR:490.3200 R:500.0 rate:1.0 gloss:26.7850 dlossA:1.7130 dlossQ:14.3891 exploreP:0.0100\n",
      "Episode:370 meanR:490.3200 R:500.0 rate:1.0 gloss:26.0454 dlossA:1.3489 dlossQ:14.8448 exploreP:0.0100\n",
      "Episode:371 meanR:490.3200 R:500.0 rate:1.0 gloss:26.6033 dlossA:1.5929 dlossQ:14.5769 exploreP:0.0100\n",
      "Episode:372 meanR:490.3200 R:500.0 rate:1.0 gloss:26.3053 dlossA:0.1569 dlossQ:14.6181 exploreP:0.0100\n",
      "Episode:373 meanR:490.3200 R:500.0 rate:1.0 gloss:26.0718 dlossA:0.2716 dlossQ:15.5388 exploreP:0.0100\n",
      "Episode:374 meanR:490.3200 R:500.0 rate:1.0 gloss:25.1190 dlossA:0.2683 dlossQ:15.8085 exploreP:0.0100\n",
      "Episode:375 meanR:490.3200 R:500.0 rate:1.0 gloss:25.0951 dlossA:0.2575 dlossQ:15.7512 exploreP:0.0100\n",
      "Episode:376 meanR:492.5200 R:500.0 rate:1.0 gloss:24.4228 dlossA:0.2364 dlossQ:15.9774 exploreP:0.0100\n",
      "Episode:377 meanR:492.5200 R:500.0 rate:1.0 gloss:24.1717 dlossA:0.2118 dlossQ:16.1670 exploreP:0.0100\n",
      "Episode:378 meanR:492.5200 R:500.0 rate:1.0 gloss:23.6350 dlossA:0.1916 dlossQ:16.2154 exploreP:0.0100\n",
      "Episode:379 meanR:492.5200 R:500.0 rate:1.0 gloss:23.6956 dlossA:0.1991 dlossQ:15.8635 exploreP:0.0100\n",
      "Episode:380 meanR:492.5200 R:500.0 rate:1.0 gloss:22.7382 dlossA:0.2020 dlossQ:16.1951 exploreP:0.0100\n",
      "Episode:381 meanR:494.0100 R:500.0 rate:1.0 gloss:22.3878 dlossA:0.2012 dlossQ:15.9030 exploreP:0.0100\n",
      "Episode:382 meanR:494.0100 R:500.0 rate:1.0 gloss:22.3941 dlossA:0.2118 dlossQ:15.5659 exploreP:0.0100\n",
      "Episode:383 meanR:494.0100 R:500.0 rate:1.0 gloss:21.6692 dlossA:0.1770 dlossQ:16.0011 exploreP:0.0100\n",
      "Episode:384 meanR:494.0100 R:500.0 rate:1.0 gloss:21.6150 dlossA:0.1617 dlossQ:16.2988 exploreP:0.0100\n",
      "Episode:385 meanR:494.0100 R:500.0 rate:1.0 gloss:21.6135 dlossA:0.1773 dlossQ:15.9734 exploreP:0.0100\n",
      "Episode:386 meanR:494.0100 R:500.0 rate:1.0 gloss:21.1127 dlossA:0.1448 dlossQ:16.1713 exploreP:0.0100\n",
      "Episode:387 meanR:494.0100 R:500.0 rate:1.0 gloss:20.8865 dlossA:0.1407 dlossQ:16.0634 exploreP:0.0100\n",
      "Episode:388 meanR:494.0100 R:500.0 rate:1.0 gloss:20.4950 dlossA:0.1373 dlossQ:15.8920 exploreP:0.0100\n",
      "Episode:389 meanR:494.0100 R:500.0 rate:1.0 gloss:20.5083 dlossA:0.1551 dlossQ:15.1245 exploreP:0.0100\n",
      "Episode:390 meanR:494.0100 R:500.0 rate:1.0 gloss:19.8517 dlossA:0.1244 dlossQ:15.0107 exploreP:0.0100\n",
      "Episode:391 meanR:494.7800 R:494.0 rate:0.988 gloss:19.6873 dlossA:0.1456 dlossQ:14.5836 exploreP:0.0100\n",
      "Episode:392 meanR:496.1400 R:500.0 rate:1.0 gloss:19.4089 dlossA:0.1476 dlossQ:14.3286 exploreP:0.0100\n",
      "Episode:393 meanR:496.1400 R:500.0 rate:1.0 gloss:19.4813 dlossA:0.1723 dlossQ:14.0072 exploreP:0.0100\n",
      "Episode:394 meanR:496.1400 R:500.0 rate:1.0 gloss:19.1533 dlossA:0.1305 dlossQ:14.3495 exploreP:0.0100\n",
      "Episode:395 meanR:496.1400 R:500.0 rate:1.0 gloss:19.2620 dlossA:0.1315 dlossQ:14.3133 exploreP:0.0100\n",
      "Episode:396 meanR:496.1400 R:500.0 rate:1.0 gloss:19.2998 dlossA:0.1413 dlossQ:14.1881 exploreP:0.0100\n",
      "Episode:397 meanR:496.1400 R:500.0 rate:1.0 gloss:19.3153 dlossA:0.1535 dlossQ:14.0763 exploreP:0.0100\n",
      "Episode:398 meanR:496.1400 R:500.0 rate:1.0 gloss:19.1978 dlossA:0.1485 dlossQ:13.8574 exploreP:0.0100\n",
      "Episode:399 meanR:497.7200 R:500.0 rate:1.0 gloss:18.9667 dlossA:0.1408 dlossQ:13.9700 exploreP:0.0100\n",
      "Episode:400 meanR:497.7200 R:500.0 rate:1.0 gloss:18.8043 dlossA:0.1277 dlossQ:13.9388 exploreP:0.0100\n",
      "Episode:401 meanR:497.7200 R:500.0 rate:1.0 gloss:18.9160 dlossA:0.1093 dlossQ:14.0802 exploreP:0.0100\n",
      "Episode:402 meanR:497.7200 R:500.0 rate:1.0 gloss:18.7352 dlossA:0.1010 dlossQ:14.0402 exploreP:0.0100\n",
      "Episode:403 meanR:497.7200 R:500.0 rate:1.0 gloss:18.7906 dlossA:0.1135 dlossQ:13.9225 exploreP:0.0100\n",
      "Episode:404 meanR:497.7200 R:500.0 rate:1.0 gloss:18.4629 dlossA:0.0983 dlossQ:13.8983 exploreP:0.0100\n",
      "Episode:405 meanR:497.7200 R:500.0 rate:1.0 gloss:18.4724 dlossA:0.1050 dlossQ:13.7721 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:406 meanR:497.7200 R:500.0 rate:1.0 gloss:18.1891 dlossA:0.1040 dlossQ:13.8333 exploreP:0.0100\n",
      "Episode:407 meanR:499.9400 R:500.0 rate:1.0 gloss:18.3220 dlossA:0.1149 dlossQ:13.5511 exploreP:0.0100\n",
      "Episode:408 meanR:499.9400 R:500.0 rate:1.0 gloss:18.1628 dlossA:0.1062 dlossQ:13.5563 exploreP:0.0100\n",
      "Episode:409 meanR:499.9400 R:500.0 rate:1.0 gloss:18.0998 dlossA:0.1051 dlossQ:13.5836 exploreP:0.0100\n",
      "Episode:410 meanR:499.9400 R:500.0 rate:1.0 gloss:18.1215 dlossA:0.1080 dlossQ:13.5215 exploreP:0.0100\n",
      "Episode:411 meanR:499.9400 R:500.0 rate:1.0 gloss:18.0674 dlossA:0.0922 dlossQ:13.6502 exploreP:0.0100\n",
      "Episode:412 meanR:499.9400 R:500.0 rate:1.0 gloss:18.0427 dlossA:0.0778 dlossQ:13.8113 exploreP:0.0100\n",
      "Episode:413 meanR:499.9400 R:500.0 rate:1.0 gloss:17.9599 dlossA:0.0704 dlossQ:13.7685 exploreP:0.0100\n",
      "Episode:414 meanR:499.9400 R:500.0 rate:1.0 gloss:18.0572 dlossA:0.0804 dlossQ:13.6102 exploreP:0.0100\n",
      "Episode:415 meanR:499.9400 R:500.0 rate:1.0 gloss:17.8131 dlossA:0.0745 dlossQ:13.6301 exploreP:0.0100\n",
      "Episode:416 meanR:499.9400 R:500.0 rate:1.0 gloss:17.7312 dlossA:0.0798 dlossQ:13.5393 exploreP:0.0100\n",
      "Episode:417 meanR:499.9400 R:500.0 rate:1.0 gloss:17.6761 dlossA:0.0845 dlossQ:13.4939 exploreP:0.0100\n",
      "Episode:418 meanR:499.9400 R:500.0 rate:1.0 gloss:17.3362 dlossA:0.0617 dlossQ:13.6366 exploreP:0.0100\n",
      "Episode:419 meanR:499.9400 R:500.0 rate:1.0 gloss:17.2332 dlossA:0.0694 dlossQ:13.5403 exploreP:0.0100\n",
      "Episode:420 meanR:499.9400 R:500.0 rate:1.0 gloss:17.2289 dlossA:0.0685 dlossQ:13.4810 exploreP:0.0100\n",
      "Episode:421 meanR:499.9400 R:500.0 rate:1.0 gloss:17.1844 dlossA:0.0808 dlossQ:13.4832 exploreP:0.0100\n",
      "Episode:422 meanR:499.9400 R:500.0 rate:1.0 gloss:17.0432 dlossA:0.0749 dlossQ:13.4189 exploreP:0.0100\n",
      "Episode:423 meanR:499.9400 R:500.0 rate:1.0 gloss:16.9890 dlossA:0.0743 dlossQ:13.4059 exploreP:0.0100\n",
      "Episode:424 meanR:499.9400 R:500.0 rate:1.0 gloss:16.9692 dlossA:0.0796 dlossQ:13.4165 exploreP:0.0100\n",
      "Episode:425 meanR:499.9400 R:500.0 rate:1.0 gloss:16.8695 dlossA:0.0756 dlossQ:13.4335 exploreP:0.0100\n",
      "Episode:426 meanR:499.9400 R:500.0 rate:1.0 gloss:16.9340 dlossA:0.0818 dlossQ:13.4607 exploreP:0.0100\n",
      "Episode:427 meanR:499.9400 R:500.0 rate:1.0 gloss:16.7169 dlossA:0.0666 dlossQ:13.4124 exploreP:0.0100\n",
      "Episode:428 meanR:499.9400 R:500.0 rate:1.0 gloss:16.6225 dlossA:0.0456 dlossQ:13.4892 exploreP:0.0100\n",
      "Episode:429 meanR:499.9400 R:500.0 rate:1.0 gloss:16.6725 dlossA:0.0482 dlossQ:13.4901 exploreP:0.0100\n",
      "Episode:430 meanR:499.9400 R:500.0 rate:1.0 gloss:16.4908 dlossA:0.0394 dlossQ:13.4627 exploreP:0.0100\n",
      "Episode:431 meanR:499.9400 R:500.0 rate:1.0 gloss:16.6484 dlossA:0.0230 dlossQ:13.5075 exploreP:0.0100\n",
      "Episode:432 meanR:499.9400 R:500.0 rate:1.0 gloss:16.4297 dlossA:0.0106 dlossQ:13.7182 exploreP:0.0100\n",
      "Episode:433 meanR:499.9400 R:500.0 rate:1.0 gloss:16.5929 dlossA:0.0155 dlossQ:13.7591 exploreP:0.0100\n",
      "Episode:434 meanR:499.9400 R:500.0 rate:1.0 gloss:16.1999 dlossA:0.0129 dlossQ:13.7207 exploreP:0.0100\n",
      "Episode:435 meanR:499.9400 R:500.0 rate:1.0 gloss:16.2010 dlossA:0.0133 dlossQ:13.7296 exploreP:0.0100\n",
      "Episode:436 meanR:499.9400 R:500.0 rate:1.0 gloss:16.2427 dlossA:0.0139 dlossQ:13.6816 exploreP:0.0100\n",
      "Episode:437 meanR:499.9400 R:500.0 rate:1.0 gloss:16.2020 dlossA:0.0132 dlossQ:13.5160 exploreP:0.0100\n",
      "Episode:438 meanR:499.9400 R:500.0 rate:1.0 gloss:15.9426 dlossA:0.0130 dlossQ:13.5898 exploreP:0.0100\n",
      "Episode:439 meanR:499.9400 R:500.0 rate:1.0 gloss:15.8451 dlossA:0.0118 dlossQ:13.4304 exploreP:0.0100\n",
      "Episode:440 meanR:499.9400 R:500.0 rate:1.0 gloss:15.9413 dlossA:0.0144 dlossQ:13.3910 exploreP:0.0100\n",
      "Episode:441 meanR:499.9400 R:500.0 rate:1.0 gloss:15.7683 dlossA:0.0127 dlossQ:13.3669 exploreP:0.0100\n",
      "Episode:442 meanR:499.9400 R:500.0 rate:1.0 gloss:15.6404 dlossA:0.0141 dlossQ:13.3276 exploreP:0.0100\n",
      "Episode:443 meanR:499.9400 R:500.0 rate:1.0 gloss:15.6147 dlossA:0.0134 dlossQ:13.3727 exploreP:0.0100\n",
      "Episode:444 meanR:499.9400 R:500.0 rate:1.0 gloss:15.6562 dlossA:0.0136 dlossQ:13.4205 exploreP:0.0100\n",
      "Episode:445 meanR:499.9400 R:500.0 rate:1.0 gloss:15.6455 dlossA:0.0155 dlossQ:13.4516 exploreP:0.0100\n",
      "Episode:446 meanR:499.9400 R:500.0 rate:1.0 gloss:15.4802 dlossA:0.0138 dlossQ:13.3757 exploreP:0.0100\n",
      "Episode:447 meanR:499.9400 R:500.0 rate:1.0 gloss:15.5407 dlossA:0.0143 dlossQ:13.3916 exploreP:0.0100\n",
      "Episode:448 meanR:499.9400 R:500.0 rate:1.0 gloss:15.3638 dlossA:0.0070 dlossQ:13.4728 exploreP:0.0100\n",
      "Episode:449 meanR:499.9400 R:500.0 rate:1.0 gloss:15.5576 dlossA:0.0060 dlossQ:13.5580 exploreP:0.0100\n",
      "Episode:450 meanR:499.9400 R:500.0 rate:1.0 gloss:15.2695 dlossA:0.0050 dlossQ:13.4976 exploreP:0.0100\n",
      "Episode:451 meanR:499.9400 R:500.0 rate:1.0 gloss:15.2718 dlossA:0.0048 dlossQ:13.4845 exploreP:0.0100\n",
      "Episode:452 meanR:499.9400 R:500.0 rate:1.0 gloss:15.1430 dlossA:0.0052 dlossQ:13.5448 exploreP:0.0100\n",
      "Episode:453 meanR:499.9400 R:500.0 rate:1.0 gloss:15.1290 dlossA:0.0057 dlossQ:13.5634 exploreP:0.0100\n",
      "Episode:454 meanR:499.9400 R:500.0 rate:1.0 gloss:14.9344 dlossA:0.0062 dlossQ:13.5933 exploreP:0.0100\n",
      "Episode:455 meanR:499.9400 R:500.0 rate:1.0 gloss:14.9597 dlossA:0.0066 dlossQ:13.5429 exploreP:0.0100\n",
      "Episode:456 meanR:499.9400 R:500.0 rate:1.0 gloss:14.8832 dlossA:0.0051 dlossQ:13.4624 exploreP:0.0100\n",
      "Episode:457 meanR:499.9400 R:500.0 rate:1.0 gloss:14.8861 dlossA:0.0047 dlossQ:13.4355 exploreP:0.0100\n",
      "Episode:458 meanR:499.9400 R:500.0 rate:1.0 gloss:14.8010 dlossA:0.0053 dlossQ:13.4564 exploreP:0.0100\n",
      "Episode:459 meanR:499.9400 R:500.0 rate:1.0 gloss:14.7637 dlossA:0.0063 dlossQ:13.4309 exploreP:0.0100\n",
      "Episode:460 meanR:499.9400 R:500.0 rate:1.0 gloss:14.7113 dlossA:0.0047 dlossQ:13.3926 exploreP:0.0100\n",
      "Episode:461 meanR:499.9400 R:500.0 rate:1.0 gloss:14.5784 dlossA:0.0052 dlossQ:13.3843 exploreP:0.0100\n",
      "Episode:462 meanR:499.9400 R:500.0 rate:1.0 gloss:14.6375 dlossA:0.0050 dlossQ:13.3614 exploreP:0.0100\n",
      "Episode:463 meanR:499.9400 R:500.0 rate:1.0 gloss:14.4974 dlossA:0.0046 dlossQ:13.3630 exploreP:0.0100\n",
      "Episode:464 meanR:499.9400 R:500.0 rate:1.0 gloss:14.5260 dlossA:0.0056 dlossQ:13.4457 exploreP:0.0100\n",
      "Episode:465 meanR:499.9400 R:500.0 rate:1.0 gloss:14.3739 dlossA:0.0056 dlossQ:13.4416 exploreP:0.0100\n",
      "Episode:466 meanR:499.9400 R:500.0 rate:1.0 gloss:14.3435 dlossA:0.0048 dlossQ:13.4030 exploreP:0.0100\n",
      "Episode:467 meanR:499.9400 R:500.0 rate:1.0 gloss:14.1607 dlossA:0.0059 dlossQ:13.4156 exploreP:0.0100\n",
      "Episode:468 meanR:499.9400 R:500.0 rate:1.0 gloss:14.1937 dlossA:0.0053 dlossQ:13.3859 exploreP:0.0100\n",
      "Episode:469 meanR:499.9400 R:500.0 rate:1.0 gloss:13.9601 dlossA:0.0048 dlossQ:13.3274 exploreP:0.0100\n",
      "Episode:470 meanR:499.9400 R:500.0 rate:1.0 gloss:13.9690 dlossA:0.0045 dlossQ:13.3388 exploreP:0.0100\n",
      "Episode:471 meanR:499.9400 R:500.0 rate:1.0 gloss:13.8346 dlossA:0.0052 dlossQ:13.3192 exploreP:0.0100\n",
      "Episode:472 meanR:499.9400 R:500.0 rate:1.0 gloss:13.9014 dlossA:0.0051 dlossQ:13.3785 exploreP:0.0100\n",
      "Episode:473 meanR:499.9400 R:500.0 rate:1.0 gloss:13.8015 dlossA:0.0055 dlossQ:13.3320 exploreP:0.0100\n",
      "Episode:474 meanR:499.9400 R:500.0 rate:1.0 gloss:13.9799 dlossA:0.0054 dlossQ:13.3307 exploreP:0.0100\n",
      "Episode:475 meanR:499.9400 R:500.0 rate:1.0 gloss:13.6956 dlossA:0.0051 dlossQ:13.2733 exploreP:0.0100\n",
      "Episode:476 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5544 dlossA:0.0059 dlossQ:13.2807 exploreP:0.0100\n",
      "Episode:477 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5033 dlossA:0.0045 dlossQ:13.2343 exploreP:0.0100\n",
      "Episode:478 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5196 dlossA:0.0066 dlossQ:13.2411 exploreP:0.0100\n",
      "Episode:479 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5878 dlossA:0.0047 dlossQ:13.2149 exploreP:0.0100\n",
      "Episode:480 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5715 dlossA:0.0054 dlossQ:13.2090 exploreP:0.0100\n",
      "Episode:481 meanR:499.9400 R:500.0 rate:1.0 gloss:13.5082 dlossA:0.0045 dlossQ:13.1802 exploreP:0.0100\n",
      "Episode:482 meanR:499.9400 R:500.0 rate:1.0 gloss:13.3305 dlossA:0.0053 dlossQ:13.1581 exploreP:0.0100\n",
      "Episode:483 meanR:499.9400 R:500.0 rate:1.0 gloss:13.1553 dlossA:0.0065 dlossQ:13.1827 exploreP:0.0100\n",
      "Episode:484 meanR:499.9400 R:500.0 rate:1.0 gloss:13.1901 dlossA:0.0051 dlossQ:13.1165 exploreP:0.0100\n",
      "Episode:485 meanR:499.9400 R:500.0 rate:1.0 gloss:13.3234 dlossA:0.0038 dlossQ:13.1283 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:486 meanR:499.9400 R:500.0 rate:1.0 gloss:13.0452 dlossA:0.0054 dlossQ:13.1474 exploreP:0.0100\n",
      "Episode:487 meanR:499.9400 R:500.0 rate:1.0 gloss:12.8715 dlossA:0.0047 dlossQ:13.1084 exploreP:0.0100\n",
      "Episode:488 meanR:499.9400 R:500.0 rate:1.0 gloss:13.0321 dlossA:0.0048 dlossQ:13.0862 exploreP:0.0100\n",
      "Episode:489 meanR:499.9400 R:500.0 rate:1.0 gloss:12.8710 dlossA:0.0052 dlossQ:13.0736 exploreP:0.0100\n",
      "Episode:490 meanR:499.9400 R:500.0 rate:1.0 gloss:12.6206 dlossA:0.0047 dlossQ:13.0525 exploreP:0.0100\n",
      "Episode:491 meanR:500.0000 R:500.0 rate:1.0 gloss:12.5268 dlossA:0.0051 dlossQ:13.1171 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        idx_arr = np.arange(memory_size// batch_size)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            percentage = 0.9\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            states = states[rates >= (np.max(rates)*percentage)]\n",
    "            actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "            next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "            dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "            rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
