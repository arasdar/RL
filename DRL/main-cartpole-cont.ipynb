{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.5148954174114357 -2.870291843392152\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), # ==  self.rates\n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = memory.buffer\n",
    "# percentage = 0.9\n",
    "# for idx in range(memory_size// batch_size):\n",
    "#     #batch, rates_ = memory.sample(batch_size=batch_size)\n",
    "#     rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "#     states = np.array([each[0] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     actions = np.array([each[1] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     next_states = np.array([each[2] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     rewards = np.array([each[3] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     dones = np.array([each[4] for each in batch])[rates >= (np.max(rates)*percentage)]\n",
    "#     maxrates = rates[rates >= (np.max(rates)*percentage)]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, maxrates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = memory.buffer\n",
    "# percentage = 0.9\n",
    "# for idx in range(memory_size// batch_size):\n",
    "#     states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "#     rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "#     states = states[rates >= 0]\n",
    "#     actions = actions[rates >= 0]\n",
    "#     next_states = next_states[rates >= 0]\n",
    "#     rewards = rewards[rates >= 0]\n",
    "#     dones = dones[rates >= 0]\n",
    "#     rates = rates[rates >= 0]\n",
    "#     print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(193, 4) (193,) (193, 4) (193,) (193,) (193,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(346, 4) (346,) (346, 4) (346,) (346,) (346,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(89, 4) (89,) (89, 4) (89,) (89,) (89,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(156, 4) (156,) (156, 4) (156,) (156,) (156,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(91, 4) (91,) (91, 4) (91,) (91,) (91,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(142, 4) (142,) (142, 4) (142,) (142,) (142,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(99, 4) (99,) (99, 4) (99,) (99,) (99,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(167, 4) (167,) (167, 4) (167,) (167,) (167,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(107, 4) (107,) (107, 4) (107,) (107,) (107,)\n",
      "(10000, 4) (10000,) (10000, 4) (10000,) (10000,) (10000,)\n",
      "(72, 4) (72,) (72, 4) (72,) (72,) (72,)\n"
     ]
    }
   ],
   "source": [
    "batch = memory.buffer\n",
    "percentage = 0.9\n",
    "# statesL, actionsL, next_statesL, rewardsL, donesL, ratesL = [], [], [], [], [], []\n",
    "for idx in range(memory_size// batch_size):\n",
    "    states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "    rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "    print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "    states = states[rates >= (np.max(rates)*percentage)]\n",
    "    actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "    next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "    rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "    dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "    rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "    print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "    # statesL.append(states)\n",
    "    # actionsL.append(actions)\n",
    "    # next_statesL.append(next_states)\n",
    "    # rewardsL.append(rewards)\n",
    "    # donesL.append(dones)\n",
    "    # ratesL.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 4), (72,), (72, 4), (72,), (72,), (72,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:12.0000 R:12.0 rate:0.024 gloss:0.6205 dlossA:0.6656 dlossQ:1.1547 exploreP:0.9988\n",
      "Episode:1 meanR:18.5000 R:25.0 rate:0.05 gloss:0.6211 dlossA:0.6628 dlossQ:1.1682 exploreP:0.9963\n",
      "Episode:2 meanR:21.0000 R:26.0 rate:0.052 gloss:0.6172 dlossA:0.6609 dlossQ:1.1629 exploreP:0.9938\n",
      "Episode:3 meanR:20.0000 R:17.0 rate:0.034 gloss:0.6149 dlossA:0.6591 dlossQ:1.1679 exploreP:0.9921\n",
      "Episode:4 meanR:18.0000 R:10.0 rate:0.02 gloss:0.6140 dlossA:0.6640 dlossQ:1.1479 exploreP:0.9911\n",
      "Episode:5 meanR:21.8333 R:41.0 rate:0.082 gloss:0.6167 dlossA:0.6620 dlossQ:1.1639 exploreP:0.9871\n",
      "Episode:6 meanR:20.7143 R:14.0 rate:0.028 gloss:0.6138 dlossA:0.6607 dlossQ:1.1630 exploreP:0.9857\n",
      "Episode:7 meanR:19.7500 R:13.0 rate:0.026 gloss:0.6163 dlossA:0.6606 dlossQ:1.1733 exploreP:0.9845\n",
      "Episode:8 meanR:19.7778 R:20.0 rate:0.04 gloss:0.6167 dlossA:0.6601 dlossQ:1.1772 exploreP:0.9825\n",
      "Episode:9 meanR:20.0000 R:22.0 rate:0.044 gloss:0.6185 dlossA:0.6601 dlossQ:1.1817 exploreP:0.9804\n",
      "Episode:10 meanR:20.8182 R:29.0 rate:0.058 gloss:0.6133 dlossA:0.6613 dlossQ:1.1630 exploreP:0.9776\n",
      "Episode:11 meanR:24.3333 R:63.0 rate:0.126 gloss:0.6087 dlossA:0.6604 dlossQ:1.1546 exploreP:0.9715\n",
      "Episode:12 meanR:23.7692 R:17.0 rate:0.034 gloss:0.6102 dlossA:0.6594 dlossQ:1.1677 exploreP:0.9699\n",
      "Episode:13 meanR:22.9286 R:12.0 rate:0.024 gloss:0.6058 dlossA:0.6619 dlossQ:1.1409 exploreP:0.9687\n",
      "Episode:14 meanR:23.2667 R:28.0 rate:0.056 gloss:0.6121 dlossA:0.6599 dlossQ:1.1686 exploreP:0.9660\n",
      "Episode:15 meanR:23.5000 R:27.0 rate:0.054 gloss:0.6051 dlossA:0.6603 dlossQ:1.1446 exploreP:0.9635\n",
      "Episode:16 meanR:23.0588 R:16.0 rate:0.032 gloss:0.6064 dlossA:0.6611 dlossQ:1.1438 exploreP:0.9619\n",
      "Episode:17 meanR:23.5000 R:31.0 rate:0.062 gloss:0.6137 dlossA:0.6604 dlossQ:1.1720 exploreP:0.9590\n",
      "Episode:18 meanR:23.6316 R:26.0 rate:0.052 gloss:0.6085 dlossA:0.6612 dlossQ:1.1519 exploreP:0.9565\n",
      "Episode:19 meanR:25.0500 R:52.0 rate:0.104 gloss:0.6099 dlossA:0.6614 dlossQ:1.1584 exploreP:0.9516\n",
      "Episode:20 meanR:25.1905 R:28.0 rate:0.056 gloss:0.6051 dlossA:0.6621 dlossQ:1.1411 exploreP:0.9490\n",
      "Episode:21 meanR:25.0909 R:23.0 rate:0.046 gloss:0.6086 dlossA:0.6597 dlossQ:1.1618 exploreP:0.9468\n",
      "Episode:22 meanR:25.7826 R:41.0 rate:0.082 gloss:0.6088 dlossA:0.6612 dlossQ:1.1581 exploreP:0.9430\n",
      "Episode:23 meanR:25.4583 R:18.0 rate:0.036 gloss:0.6052 dlossA:0.6595 dlossQ:1.1528 exploreP:0.9413\n",
      "Episode:24 meanR:26.7200 R:57.0 rate:0.114 gloss:0.6117 dlossA:0.6612 dlossQ:1.1673 exploreP:0.9360\n",
      "Episode:25 meanR:27.4615 R:46.0 rate:0.092 gloss:0.6077 dlossA:0.6610 dlossQ:1.1573 exploreP:0.9318\n",
      "Episode:26 meanR:26.9259 R:13.0 rate:0.026 gloss:0.6167 dlossA:0.6597 dlossQ:1.1996 exploreP:0.9306\n",
      "Episode:27 meanR:27.4643 R:42.0 rate:0.084 gloss:0.6079 dlossA:0.6598 dlossQ:1.1652 exploreP:0.9267\n",
      "Episode:28 meanR:26.9655 R:13.0 rate:0.026 gloss:0.6079 dlossA:0.6624 dlossQ:1.1561 exploreP:0.9255\n",
      "Episode:29 meanR:27.0000 R:28.0 rate:0.056 gloss:0.6037 dlossA:0.6601 dlossQ:1.1486 exploreP:0.9230\n",
      "Episode:30 meanR:26.6774 R:17.0 rate:0.034 gloss:0.6044 dlossA:0.6612 dlossQ:1.1476 exploreP:0.9214\n",
      "Episode:31 meanR:26.3438 R:16.0 rate:0.032 gloss:0.6047 dlossA:0.6609 dlossQ:1.1494 exploreP:0.9200\n",
      "Episode:32 meanR:27.1515 R:53.0 rate:0.106 gloss:0.6077 dlossA:0.6601 dlossQ:1.1687 exploreP:0.9152\n",
      "Episode:33 meanR:26.8235 R:16.0 rate:0.032 gloss:0.6085 dlossA:0.6593 dlossQ:1.1781 exploreP:0.9137\n",
      "Episode:34 meanR:26.4571 R:14.0 rate:0.028 gloss:0.6111 dlossA:0.6598 dlossQ:1.1879 exploreP:0.9124\n",
      "Episode:35 meanR:26.6667 R:34.0 rate:0.068 gloss:0.6015 dlossA:0.6608 dlossQ:1.1432 exploreP:0.9094\n",
      "Episode:36 meanR:26.5405 R:22.0 rate:0.044 gloss:0.6056 dlossA:0.6612 dlossQ:1.1614 exploreP:0.9074\n",
      "Episode:37 meanR:26.1579 R:12.0 rate:0.024 gloss:0.5994 dlossA:0.6600 dlossQ:1.1398 exploreP:0.9063\n",
      "Episode:38 meanR:26.6923 R:47.0 rate:0.094 gloss:0.6029 dlossA:0.6605 dlossQ:1.1506 exploreP:0.9021\n",
      "Episode:39 meanR:26.6000 R:23.0 rate:0.046 gloss:0.6043 dlossA:0.6628 dlossQ:1.1406 exploreP:0.9001\n",
      "Episode:40 meanR:26.8049 R:35.0 rate:0.07 gloss:0.6033 dlossA:0.6613 dlossQ:1.1506 exploreP:0.8970\n",
      "Episode:41 meanR:27.1905 R:43.0 rate:0.086 gloss:0.6015 dlossA:0.6598 dlossQ:1.1557 exploreP:0.8932\n",
      "Episode:42 meanR:27.3953 R:36.0 rate:0.072 gloss:0.6011 dlossA:0.6616 dlossQ:1.1423 exploreP:0.8900\n",
      "Episode:43 meanR:27.1818 R:18.0 rate:0.036 gloss:0.6064 dlossA:0.6624 dlossQ:1.1575 exploreP:0.8884\n",
      "Episode:44 meanR:27.1556 R:26.0 rate:0.052 gloss:0.6026 dlossA:0.6604 dlossQ:1.1589 exploreP:0.8861\n",
      "Episode:45 meanR:27.3478 R:36.0 rate:0.072 gloss:0.6055 dlossA:0.6612 dlossQ:1.1738 exploreP:0.8830\n",
      "Episode:46 meanR:27.0851 R:15.0 rate:0.03 gloss:0.6006 dlossA:0.6624 dlossQ:1.1448 exploreP:0.8817\n",
      "Episode:47 meanR:27.1250 R:29.0 rate:0.058 gloss:0.6016 dlossA:0.6605 dlossQ:1.1585 exploreP:0.8791\n",
      "Episode:48 meanR:27.2041 R:31.0 rate:0.062 gloss:0.6012 dlossA:0.6609 dlossQ:1.1509 exploreP:0.8765\n",
      "Episode:49 meanR:26.8800 R:11.0 rate:0.022 gloss:0.6029 dlossA:0.6606 dlossQ:1.1618 exploreP:0.8755\n",
      "Episode:50 meanR:28.5490 R:112.0 rate:0.224 gloss:0.6018 dlossA:0.6610 dlossQ:1.1551 exploreP:0.8659\n",
      "Episode:51 meanR:28.5769 R:30.0 rate:0.06 gloss:0.6031 dlossA:0.6621 dlossQ:1.1591 exploreP:0.8633\n",
      "Episode:52 meanR:28.3019 R:14.0 rate:0.028 gloss:0.6052 dlossA:0.6617 dlossQ:1.1597 exploreP:0.8621\n",
      "Episode:53 meanR:28.6111 R:45.0 rate:0.09 gloss:0.6026 dlossA:0.6611 dlossQ:1.1590 exploreP:0.8583\n",
      "Episode:54 meanR:29.1273 R:57.0 rate:0.114 gloss:0.6015 dlossA:0.6615 dlossQ:1.1605 exploreP:0.8535\n",
      "Episode:55 meanR:29.5000 R:50.0 rate:0.1 gloss:0.6011 dlossA:0.6630 dlossQ:1.1501 exploreP:0.8492\n",
      "Episode:56 meanR:29.3158 R:19.0 rate:0.038 gloss:0.6006 dlossA:0.6623 dlossQ:1.1647 exploreP:0.8477\n",
      "Episode:57 meanR:29.6552 R:49.0 rate:0.098 gloss:0.6018 dlossA:0.6633 dlossQ:1.1495 exploreP:0.8436\n",
      "Episode:58 meanR:29.9492 R:47.0 rate:0.094 gloss:0.5999 dlossA:0.6621 dlossQ:1.1497 exploreP:0.8397\n",
      "Episode:59 meanR:29.7167 R:16.0 rate:0.032 gloss:0.5947 dlossA:0.6623 dlossQ:1.1516 exploreP:0.8383\n",
      "Episode:60 meanR:29.6393 R:25.0 rate:0.05 gloss:0.5975 dlossA:0.6620 dlossQ:1.1537 exploreP:0.8363\n",
      "Episode:61 meanR:29.5968 R:27.0 rate:0.054 gloss:0.6033 dlossA:0.6632 dlossQ:1.1507 exploreP:0.8340\n",
      "Episode:62 meanR:29.3810 R:16.0 rate:0.032 gloss:0.6001 dlossA:0.6635 dlossQ:1.1464 exploreP:0.8327\n",
      "Episode:63 meanR:29.8906 R:62.0 rate:0.124 gloss:0.5982 dlossA:0.6628 dlossQ:1.1476 exploreP:0.8276\n",
      "Episode:64 meanR:29.5692 R:9.0 rate:0.018 gloss:0.5920 dlossA:0.6617 dlossQ:1.1518 exploreP:0.8269\n",
      "Episode:65 meanR:29.4091 R:19.0 rate:0.038 gloss:0.5964 dlossA:0.6607 dlossQ:1.1446 exploreP:0.8253\n",
      "Episode:66 meanR:29.2985 R:22.0 rate:0.044 gloss:0.6050 dlossA:0.6664 dlossQ:1.1437 exploreP:0.8235\n",
      "Episode:67 meanR:29.2206 R:24.0 rate:0.048 gloss:0.5979 dlossA:0.6637 dlossQ:1.1472 exploreP:0.8216\n",
      "Episode:68 meanR:29.0000 R:14.0 rate:0.028 gloss:0.5928 dlossA:0.6620 dlossQ:1.1471 exploreP:0.8205\n",
      "Episode:69 meanR:29.1857 R:42.0 rate:0.084 gloss:0.5962 dlossA:0.6616 dlossQ:1.1545 exploreP:0.8171\n",
      "Episode:70 meanR:29.4930 R:51.0 rate:0.102 gloss:0.5942 dlossA:0.6613 dlossQ:1.1427 exploreP:0.8130\n",
      "Episode:71 meanR:29.2917 R:15.0 rate:0.03 gloss:0.5996 dlossA:0.6645 dlossQ:1.1381 exploreP:0.8118\n",
      "Episode:72 meanR:29.2740 R:28.0 rate:0.056 gloss:0.5943 dlossA:0.6625 dlossQ:1.1453 exploreP:0.8095\n",
      "Episode:73 meanR:29.5270 R:48.0 rate:0.096 gloss:0.6001 dlossA:0.6644 dlossQ:1.1369 exploreP:0.8057\n",
      "Episode:74 meanR:29.3600 R:17.0 rate:0.034 gloss:0.5993 dlossA:0.6626 dlossQ:1.1501 exploreP:0.8043\n",
      "Episode:75 meanR:29.1711 R:15.0 rate:0.03 gloss:0.6051 dlossA:0.6695 dlossQ:1.1310 exploreP:0.8031\n",
      "Episode:76 meanR:29.2987 R:39.0 rate:0.078 gloss:0.5975 dlossA:0.6661 dlossQ:1.1317 exploreP:0.8001\n",
      "Episode:77 meanR:29.3718 R:35.0 rate:0.07 gloss:0.5965 dlossA:0.6632 dlossQ:1.1421 exploreP:0.7973\n",
      "Episode:78 meanR:29.3797 R:30.0 rate:0.06 gloss:0.6001 dlossA:0.6662 dlossQ:1.1377 exploreP:0.7949\n",
      "Episode:79 meanR:29.3875 R:30.0 rate:0.06 gloss:0.6002 dlossA:0.6651 dlossQ:1.1420 exploreP:0.7926\n",
      "Episode:80 meanR:30.2469 R:99.0 rate:0.198 gloss:0.5973 dlossA:0.6640 dlossQ:1.1436 exploreP:0.7849\n",
      "Episode:81 meanR:30.1463 R:22.0 rate:0.044 gloss:0.5935 dlossA:0.6651 dlossQ:1.1396 exploreP:0.7832\n",
      "Episode:82 meanR:30.0120 R:19.0 rate:0.038 gloss:0.6009 dlossA:0.6665 dlossQ:1.1454 exploreP:0.7817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:83 meanR:30.3095 R:55.0 rate:0.11 gloss:0.6023 dlossA:0.6675 dlossQ:1.1330 exploreP:0.7775\n",
      "Episode:84 meanR:30.1529 R:17.0 rate:0.034 gloss:0.6012 dlossA:0.6681 dlossQ:1.1254 exploreP:0.7762\n",
      "Episode:85 meanR:30.0233 R:19.0 rate:0.038 gloss:0.6072 dlossA:0.6696 dlossQ:1.1331 exploreP:0.7747\n",
      "Episode:86 meanR:30.3333 R:57.0 rate:0.114 gloss:0.6009 dlossA:0.6658 dlossQ:1.1301 exploreP:0.7704\n",
      "Episode:87 meanR:30.3523 R:32.0 rate:0.064 gloss:0.6036 dlossA:0.6658 dlossQ:1.1353 exploreP:0.7679\n",
      "Episode:88 meanR:30.2472 R:21.0 rate:0.042 gloss:0.5946 dlossA:0.6639 dlossQ:1.1455 exploreP:0.7664\n",
      "Episode:89 meanR:31.3222 R:127.0 rate:0.254 gloss:0.5986 dlossA:0.6649 dlossQ:1.1351 exploreP:0.7568\n",
      "Episode:90 meanR:32.0549 R:98.0 rate:0.196 gloss:0.6007 dlossA:0.6655 dlossQ:1.1379 exploreP:0.7495\n",
      "Episode:91 meanR:31.8043 R:9.0 rate:0.018 gloss:0.5885 dlossA:0.6603 dlossQ:1.1481 exploreP:0.7489\n",
      "Episode:92 meanR:31.6989 R:22.0 rate:0.044 gloss:0.6008 dlossA:0.6667 dlossQ:1.1352 exploreP:0.7472\n",
      "Episode:93 meanR:31.9255 R:53.0 rate:0.106 gloss:0.5980 dlossA:0.6659 dlossQ:1.1412 exploreP:0.7433\n",
      "Episode:94 meanR:32.9158 R:126.0 rate:0.252 gloss:0.5970 dlossA:0.6633 dlossQ:1.1416 exploreP:0.7342\n",
      "Episode:95 meanR:32.8750 R:29.0 rate:0.058 gloss:0.5973 dlossA:0.6644 dlossQ:1.1485 exploreP:0.7321\n",
      "Episode:96 meanR:32.8763 R:33.0 rate:0.066 gloss:0.5999 dlossA:0.6645 dlossQ:1.1324 exploreP:0.7297\n",
      "Episode:97 meanR:33.5408 R:98.0 rate:0.196 gloss:0.5964 dlossA:0.6633 dlossQ:1.1484 exploreP:0.7227\n",
      "Episode:98 meanR:34.0000 R:79.0 rate:0.158 gloss:0.5946 dlossA:0.6639 dlossQ:1.1507 exploreP:0.7171\n",
      "Episode:99 meanR:34.0300 R:37.0 rate:0.074 gloss:0.6029 dlossA:0.6654 dlossQ:1.1443 exploreP:0.7144\n",
      "Episode:100 meanR:34.2800 R:37.0 rate:0.074 gloss:0.5905 dlossA:0.6616 dlossQ:1.1527 exploreP:0.7118\n",
      "Episode:101 meanR:34.8500 R:82.0 rate:0.164 gloss:0.5984 dlossA:0.6636 dlossQ:1.1430 exploreP:0.7061\n",
      "Episode:102 meanR:34.7100 R:12.0 rate:0.024 gloss:0.6089 dlossA:0.6665 dlossQ:1.1506 exploreP:0.7053\n",
      "Episode:103 meanR:34.7200 R:18.0 rate:0.036 gloss:0.5982 dlossA:0.6636 dlossQ:1.1422 exploreP:0.7040\n",
      "Episode:104 meanR:34.9300 R:31.0 rate:0.062 gloss:0.5964 dlossA:0.6634 dlossQ:1.1608 exploreP:0.7019\n",
      "Episode:105 meanR:34.7900 R:27.0 rate:0.054 gloss:0.5888 dlossA:0.6621 dlossQ:1.1453 exploreP:0.7000\n",
      "Episode:106 meanR:34.8200 R:17.0 rate:0.034 gloss:0.5911 dlossA:0.6636 dlossQ:1.1470 exploreP:0.6988\n",
      "Episode:107 meanR:34.9600 R:27.0 rate:0.054 gloss:0.5961 dlossA:0.6624 dlossQ:1.1480 exploreP:0.6970\n",
      "Episode:108 meanR:35.0600 R:30.0 rate:0.06 gloss:0.5927 dlossA:0.6651 dlossQ:1.1379 exploreP:0.6949\n",
      "Episode:109 meanR:34.9400 R:10.0 rate:0.02 gloss:0.5949 dlossA:0.6661 dlossQ:1.1366 exploreP:0.6942\n",
      "Episode:110 meanR:35.2200 R:57.0 rate:0.114 gloss:0.5951 dlossA:0.6643 dlossQ:1.1443 exploreP:0.6903\n",
      "Episode:111 meanR:35.2500 R:66.0 rate:0.132 gloss:0.5963 dlossA:0.6646 dlossQ:1.1465 exploreP:0.6859\n",
      "Episode:112 meanR:35.3700 R:29.0 rate:0.058 gloss:0.5929 dlossA:0.6636 dlossQ:1.1402 exploreP:0.6839\n",
      "Episode:113 meanR:36.3200 R:107.0 rate:0.214 gloss:0.5971 dlossA:0.6651 dlossQ:1.1379 exploreP:0.6767\n",
      "Episode:114 meanR:36.7400 R:70.0 rate:0.14 gloss:0.5977 dlossA:0.6628 dlossQ:1.1549 exploreP:0.6721\n",
      "Episode:115 meanR:36.9400 R:47.0 rate:0.094 gloss:0.6010 dlossA:0.6657 dlossQ:1.1416 exploreP:0.6690\n",
      "Episode:116 meanR:36.9500 R:17.0 rate:0.034 gloss:0.5955 dlossA:0.6632 dlossQ:1.1559 exploreP:0.6679\n",
      "Episode:117 meanR:37.5200 R:88.0 rate:0.176 gloss:0.5997 dlossA:0.6642 dlossQ:1.1466 exploreP:0.6621\n",
      "Episode:118 meanR:37.6600 R:40.0 rate:0.08 gloss:0.6006 dlossA:0.6643 dlossQ:1.1459 exploreP:0.6595\n",
      "Episode:119 meanR:37.3000 R:16.0 rate:0.032 gloss:0.5892 dlossA:0.6629 dlossQ:1.1353 exploreP:0.6585\n",
      "Episode:120 meanR:37.8300 R:81.0 rate:0.162 gloss:0.6030 dlossA:0.6624 dlossQ:1.1574 exploreP:0.6532\n",
      "Episode:121 meanR:38.1300 R:53.0 rate:0.106 gloss:0.5970 dlossA:0.6648 dlossQ:1.1427 exploreP:0.6498\n",
      "Episode:122 meanR:38.0400 R:32.0 rate:0.064 gloss:0.6000 dlossA:0.6652 dlossQ:1.1470 exploreP:0.6478\n",
      "Episode:123 meanR:38.6300 R:77.0 rate:0.154 gloss:0.5949 dlossA:0.6640 dlossQ:1.1469 exploreP:0.6429\n",
      "Episode:124 meanR:38.2800 R:22.0 rate:0.044 gloss:0.6010 dlossA:0.6653 dlossQ:1.1427 exploreP:0.6415\n",
      "Episode:125 meanR:38.2700 R:45.0 rate:0.09 gloss:0.6001 dlossA:0.6648 dlossQ:1.1395 exploreP:0.6387\n",
      "Episode:126 meanR:39.5700 R:143.0 rate:0.286 gloss:0.5961 dlossA:0.6630 dlossQ:1.1432 exploreP:0.6297\n",
      "Episode:127 meanR:39.9500 R:80.0 rate:0.16 gloss:0.5979 dlossA:0.6648 dlossQ:1.1396 exploreP:0.6248\n",
      "Episode:128 meanR:40.1900 R:37.0 rate:0.074 gloss:0.6023 dlossA:0.6642 dlossQ:1.1357 exploreP:0.6225\n",
      "Episode:129 meanR:41.6100 R:170.0 rate:0.34 gloss:0.5994 dlossA:0.6645 dlossQ:1.1367 exploreP:0.6122\n",
      "Episode:130 meanR:42.2500 R:81.0 rate:0.162 gloss:0.5922 dlossA:0.6639 dlossQ:1.1522 exploreP:0.6074\n",
      "Episode:131 meanR:42.4400 R:35.0 rate:0.07 gloss:0.5949 dlossA:0.6620 dlossQ:1.1367 exploreP:0.6053\n",
      "Episode:132 meanR:43.3600 R:145.0 rate:0.29 gloss:0.5975 dlossA:0.6645 dlossQ:1.1417 exploreP:0.5967\n",
      "Episode:133 meanR:44.0500 R:85.0 rate:0.17 gloss:0.5996 dlossA:0.6681 dlossQ:1.1406 exploreP:0.5917\n",
      "Episode:134 meanR:44.1400 R:23.0 rate:0.046 gloss:0.6045 dlossA:0.6653 dlossQ:1.1273 exploreP:0.5904\n",
      "Episode:135 meanR:45.1500 R:135.0 rate:0.27 gloss:0.6004 dlossA:0.6673 dlossQ:1.1397 exploreP:0.5826\n",
      "Episode:136 meanR:45.7600 R:83.0 rate:0.166 gloss:0.6022 dlossA:0.6674 dlossQ:1.1322 exploreP:0.5779\n",
      "Episode:137 meanR:46.6500 R:101.0 rate:0.202 gloss:0.5991 dlossA:0.6671 dlossQ:1.1338 exploreP:0.5722\n",
      "Episode:138 meanR:46.5100 R:33.0 rate:0.066 gloss:0.5990 dlossA:0.6603 dlossQ:1.1473 exploreP:0.5703\n",
      "Episode:139 meanR:46.4500 R:17.0 rate:0.034 gloss:0.6003 dlossA:0.6685 dlossQ:1.1282 exploreP:0.5694\n",
      "Episode:140 meanR:46.7100 R:61.0 rate:0.122 gloss:0.6008 dlossA:0.6678 dlossQ:1.1258 exploreP:0.5660\n",
      "Episode:141 meanR:47.2400 R:96.0 rate:0.192 gloss:0.6036 dlossA:0.6687 dlossQ:1.1271 exploreP:0.5607\n",
      "Episode:142 meanR:47.3500 R:47.0 rate:0.094 gloss:0.5975 dlossA:0.6682 dlossQ:1.1346 exploreP:0.5581\n",
      "Episode:143 meanR:49.5000 R:233.0 rate:0.466 gloss:0.6015 dlossA:0.6681 dlossQ:1.1249 exploreP:0.5454\n",
      "Episode:144 meanR:52.9500 R:371.0 rate:0.742 gloss:0.6045 dlossA:0.6706 dlossQ:1.1143 exploreP:0.5259\n",
      "Episode:145 meanR:54.8900 R:230.0 rate:0.46 gloss:0.6049 dlossA:0.6759 dlossQ:1.0964 exploreP:0.5142\n",
      "Episode:146 meanR:55.1000 R:36.0 rate:0.072 gloss:0.6065 dlossA:0.6787 dlossQ:1.0958 exploreP:0.5124\n",
      "Episode:147 meanR:56.7600 R:195.0 rate:0.39 gloss:0.6176 dlossA:0.6840 dlossQ:1.0722 exploreP:0.5027\n",
      "Episode:148 meanR:56.8800 R:43.0 rate:0.086 gloss:0.6192 dlossA:0.6837 dlossQ:1.0736 exploreP:0.5006\n",
      "Episode:149 meanR:58.2900 R:152.0 rate:0.304 gloss:0.6228 dlossA:0.6866 dlossQ:1.0635 exploreP:0.4932\n",
      "Episode:150 meanR:58.3500 R:118.0 rate:0.236 gloss:0.6276 dlossA:0.6893 dlossQ:1.0584 exploreP:0.4875\n",
      "Episode:151 meanR:58.4000 R:35.0 rate:0.07 gloss:0.6360 dlossA:0.6947 dlossQ:1.0413 exploreP:0.4859\n",
      "Episode:152 meanR:59.5600 R:130.0 rate:0.26 gloss:0.6338 dlossA:0.6920 dlossQ:1.0549 exploreP:0.4797\n",
      "Episode:153 meanR:59.2300 R:12.0 rate:0.024 gloss:0.6350 dlossA:0.6906 dlossQ:1.0848 exploreP:0.4791\n",
      "Episode:154 meanR:59.4200 R:76.0 rate:0.152 gloss:0.6386 dlossA:0.6940 dlossQ:1.0536 exploreP:0.4756\n",
      "Episode:155 meanR:60.3800 R:146.0 rate:0.292 gloss:0.6441 dlossA:0.6980 dlossQ:1.0377 exploreP:0.4688\n",
      "Episode:156 meanR:60.4700 R:28.0 rate:0.056 gloss:0.6520 dlossA:0.7040 dlossQ:1.0247 exploreP:0.4676\n",
      "Episode:157 meanR:61.6500 R:167.0 rate:0.334 gloss:0.6515 dlossA:0.7029 dlossQ:1.0288 exploreP:0.4600\n",
      "Episode:158 meanR:61.4400 R:26.0 rate:0.052 gloss:0.6432 dlossA:0.7119 dlossQ:0.9960 exploreP:0.4588\n",
      "Episode:159 meanR:64.4600 R:318.0 rate:0.636 gloss:0.6665 dlossA:0.7066 dlossQ:1.0273 exploreP:0.4448\n",
      "Episode:160 meanR:65.5300 R:132.0 rate:0.264 gloss:0.6694 dlossA:0.7109 dlossQ:1.0212 exploreP:0.4391\n",
      "Episode:161 meanR:66.0300 R:77.0 rate:0.154 gloss:0.6777 dlossA:0.7158 dlossQ:1.0116 exploreP:0.4358\n",
      "Episode:162 meanR:66.7300 R:86.0 rate:0.172 gloss:0.6795 dlossA:0.7169 dlossQ:1.0064 exploreP:0.4321\n",
      "Episode:163 meanR:67.6100 R:150.0 rate:0.3 gloss:0.6829 dlossA:0.7164 dlossQ:1.0024 exploreP:0.4258\n",
      "Episode:164 meanR:67.8900 R:37.0 rate:0.074 gloss:0.6949 dlossA:0.7293 dlossQ:0.9841 exploreP:0.4243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:165 meanR:70.1400 R:244.0 rate:0.488 gloss:0.6950 dlossA:0.7252 dlossQ:1.0001 exploreP:0.4143\n",
      "Episode:166 meanR:71.5100 R:159.0 rate:0.318 gloss:0.7054 dlossA:0.7256 dlossQ:0.9923 exploreP:0.4079\n",
      "Episode:167 meanR:74.8800 R:361.0 rate:0.722 gloss:0.7167 dlossA:0.7350 dlossQ:0.9822 exploreP:0.3938\n",
      "Episode:168 meanR:76.9600 R:222.0 rate:0.444 gloss:0.7267 dlossA:0.7435 dlossQ:0.9803 exploreP:0.3854\n",
      "Episode:169 meanR:79.7600 R:322.0 rate:0.644 gloss:0.7309 dlossA:0.7456 dlossQ:0.9890 exploreP:0.3735\n",
      "Episode:170 meanR:80.5600 R:131.0 rate:0.262 gloss:0.7356 dlossA:0.7420 dlossQ:0.9841 exploreP:0.3688\n",
      "Episode:171 meanR:82.8000 R:239.0 rate:0.478 gloss:0.7454 dlossA:0.7540 dlossQ:0.9696 exploreP:0.3603\n",
      "Episode:172 meanR:86.6000 R:408.0 rate:0.816 gloss:0.7547 dlossA:0.7618 dlossQ:0.9618 exploreP:0.3463\n",
      "Episode:173 meanR:90.3100 R:419.0 rate:0.838 gloss:0.7645 dlossA:0.7687 dlossQ:0.9558 exploreP:0.3325\n",
      "Episode:174 meanR:92.6900 R:255.0 rate:0.51 gloss:0.7721 dlossA:0.7640 dlossQ:0.9588 exploreP:0.3244\n",
      "Episode:175 meanR:94.2900 R:175.0 rate:0.35 gloss:0.7855 dlossA:0.7726 dlossQ:0.9503 exploreP:0.3189\n",
      "Episode:176 meanR:95.5300 R:163.0 rate:0.326 gloss:0.7954 dlossA:0.7807 dlossQ:0.9408 exploreP:0.3139\n",
      "Episode:177 meanR:99.5100 R:433.0 rate:0.866 gloss:0.8043 dlossA:0.7854 dlossQ:0.9360 exploreP:0.3011\n",
      "Episode:178 meanR:104.2100 R:500.0 rate:1.0 gloss:0.8202 dlossA:0.7944 dlossQ:0.9305 exploreP:0.2869\n",
      "Episode:179 meanR:108.7200 R:481.0 rate:0.962 gloss:0.8420 dlossA:0.8046 dlossQ:0.9313 exploreP:0.2739\n",
      "Episode:180 meanR:109.0100 R:128.0 rate:0.256 gloss:0.8558 dlossA:0.8048 dlossQ:0.9165 exploreP:0.2705\n",
      "Episode:181 meanR:113.7900 R:500.0 rate:1.0 gloss:0.8726 dlossA:0.8263 dlossQ:0.9077 exploreP:0.2578\n",
      "Episode:182 meanR:116.8800 R:328.0 rate:0.656 gloss:0.8911 dlossA:0.8245 dlossQ:0.8875 exploreP:0.2498\n",
      "Episode:183 meanR:118.7800 R:245.0 rate:0.49 gloss:0.9071 dlossA:0.8505 dlossQ:0.8859 exploreP:0.2440\n",
      "Episode:184 meanR:123.6100 R:500.0 rate:1.0 gloss:0.9237 dlossA:0.8510 dlossQ:0.8755 exploreP:0.2326\n",
      "Episode:185 meanR:126.8100 R:339.0 rate:0.678 gloss:0.9491 dlossA:0.8538 dlossQ:0.8635 exploreP:0.2252\n",
      "Episode:186 meanR:126.4900 R:25.0 rate:0.05 gloss:0.9705 dlossA:0.8830 dlossQ:0.8641 exploreP:0.2246\n",
      "Episode:187 meanR:127.8500 R:168.0 rate:0.336 gloss:0.9614 dlossA:0.8808 dlossQ:0.8581 exploreP:0.2211\n",
      "Episode:188 meanR:131.5400 R:390.0 rate:0.78 gloss:0.9794 dlossA:0.8975 dlossQ:0.8597 exploreP:0.2130\n",
      "Episode:189 meanR:135.2700 R:500.0 rate:1.0 gloss:0.9951 dlossA:0.8756 dlossQ:0.8423 exploreP:0.2031\n",
      "Episode:190 meanR:139.0000 R:471.0 rate:0.942 gloss:1.0344 dlossA:0.8863 dlossQ:0.8251 exploreP:0.1942\n",
      "Episode:191 meanR:143.2200 R:431.0 rate:0.862 gloss:1.0731 dlossA:0.9216 dlossQ:0.8130 exploreP:0.1864\n",
      "Episode:192 meanR:146.0300 R:303.0 rate:0.606 gloss:1.1035 dlossA:0.9286 dlossQ:0.8073 exploreP:0.1812\n",
      "Episode:193 meanR:150.5000 R:500.0 rate:1.0 gloss:1.1299 dlossA:0.9581 dlossQ:0.8130 exploreP:0.1728\n",
      "Episode:194 meanR:154.2400 R:500.0 rate:1.0 gloss:1.1677 dlossA:0.9825 dlossQ:0.8036 exploreP:0.1649\n",
      "Episode:195 meanR:156.2700 R:232.0 rate:0.464 gloss:1.1938 dlossA:1.0150 dlossQ:0.7992 exploreP:0.1613\n",
      "Episode:196 meanR:160.9400 R:500.0 rate:1.0 gloss:1.2251 dlossA:1.0284 dlossQ:0.8021 exploreP:0.1539\n",
      "Episode:197 meanR:161.4000 R:144.0 rate:0.288 gloss:1.2499 dlossA:1.0661 dlossQ:0.7987 exploreP:0.1519\n",
      "Episode:198 meanR:165.6100 R:500.0 rate:1.0 gloss:1.2696 dlossA:1.0476 dlossQ:0.8041 exploreP:0.1450\n",
      "Episode:199 meanR:167.4700 R:223.0 rate:0.446 gloss:1.2980 dlossA:1.0815 dlossQ:0.8055 exploreP:0.1420\n",
      "Episode:200 meanR:172.1000 R:500.0 rate:1.0 gloss:1.3197 dlossA:1.0943 dlossQ:0.8057 exploreP:0.1356\n",
      "Episode:201 meanR:176.2800 R:500.0 rate:1.0 gloss:1.3564 dlossA:1.1041 dlossQ:0.8038 exploreP:0.1294\n",
      "Episode:202 meanR:179.2700 R:311.0 rate:0.622 gloss:1.3974 dlossA:1.1572 dlossQ:0.8054 exploreP:0.1258\n",
      "Episode:203 meanR:183.9000 R:481.0 rate:0.962 gloss:1.4350 dlossA:1.1870 dlossQ:0.8058 exploreP:0.1203\n",
      "Episode:204 meanR:186.5200 R:293.0 rate:0.586 gloss:1.4674 dlossA:1.1859 dlossQ:0.8022 exploreP:0.1171\n",
      "Episode:205 meanR:190.9500 R:470.0 rate:0.94 gloss:1.5172 dlossA:1.2086 dlossQ:0.7935 exploreP:0.1122\n",
      "Episode:206 meanR:195.7800 R:500.0 rate:1.0 gloss:1.5765 dlossA:1.2736 dlossQ:0.7850 exploreP:0.1072\n",
      "Episode:207 meanR:197.1400 R:163.0 rate:0.326 gloss:1.6058 dlossA:1.2630 dlossQ:0.7811 exploreP:0.1057\n",
      "Episode:208 meanR:201.8400 R:500.0 rate:1.0 gloss:1.6507 dlossA:1.2974 dlossQ:0.7776 exploreP:0.1010\n",
      "Episode:209 meanR:206.7400 R:500.0 rate:1.0 gloss:1.6905 dlossA:1.3659 dlossQ:0.7752 exploreP:0.0966\n",
      "Episode:210 meanR:211.1700 R:500.0 rate:1.0 gloss:1.7401 dlossA:1.3123 dlossQ:0.7666 exploreP:0.0923\n",
      "Episode:211 meanR:215.5100 R:500.0 rate:1.0 gloss:1.8246 dlossA:1.4063 dlossQ:0.7606 exploreP:0.0883\n",
      "Episode:212 meanR:220.2200 R:500.0 rate:1.0 gloss:1.8913 dlossA:1.4453 dlossQ:0.7594 exploreP:0.0845\n",
      "Episode:213 meanR:224.1500 R:500.0 rate:1.0 gloss:1.9452 dlossA:1.4875 dlossQ:0.7571 exploreP:0.0809\n",
      "Episode:214 meanR:228.4500 R:500.0 rate:1.0 gloss:2.0225 dlossA:1.5287 dlossQ:0.7519 exploreP:0.0774\n",
      "Episode:215 meanR:232.9800 R:500.0 rate:1.0 gloss:2.0868 dlossA:1.5692 dlossQ:0.7517 exploreP:0.0741\n",
      "Episode:216 meanR:234.4900 R:168.0 rate:0.336 gloss:2.1265 dlossA:1.6757 dlossQ:0.7573 exploreP:0.0731\n",
      "Episode:217 meanR:238.6100 R:500.0 rate:1.0 gloss:2.1730 dlossA:1.7130 dlossQ:0.7576 exploreP:0.0700\n",
      "Episode:218 meanR:243.2100 R:500.0 rate:1.0 gloss:2.2212 dlossA:1.7319 dlossQ:0.7553 exploreP:0.0671\n",
      "Episode:219 meanR:248.0500 R:500.0 rate:1.0 gloss:2.2901 dlossA:1.7535 dlossQ:0.7551 exploreP:0.0643\n",
      "Episode:220 meanR:251.5600 R:432.0 rate:0.864 gloss:2.3637 dlossA:1.8560 dlossQ:0.7590 exploreP:0.0620\n",
      "Episode:221 meanR:256.0300 R:500.0 rate:1.0 gloss:2.4517 dlossA:1.9134 dlossQ:0.7691 exploreP:0.0594\n",
      "Episode:222 meanR:260.4500 R:474.0 rate:0.948 gloss:2.5074 dlossA:2.0311 dlossQ:0.7719 exploreP:0.0572\n",
      "Episode:223 meanR:264.6800 R:500.0 rate:1.0 gloss:2.5222 dlossA:2.0117 dlossQ:0.7743 exploreP:0.0549\n",
      "Episode:224 meanR:266.0600 R:160.0 rate:0.32 gloss:2.6117 dlossA:2.1177 dlossQ:0.7937 exploreP:0.0541\n",
      "Episode:225 meanR:270.6100 R:500.0 rate:1.0 gloss:2.6411 dlossA:2.0909 dlossQ:0.7945 exploreP:0.0520\n",
      "Episode:226 meanR:273.7000 R:452.0 rate:0.904 gloss:2.7717 dlossA:2.1835 dlossQ:0.8062 exploreP:0.0501\n",
      "Episode:227 meanR:277.9000 R:500.0 rate:1.0 gloss:2.8805 dlossA:2.2750 dlossQ:0.8209 exploreP:0.0482\n",
      "Episode:228 meanR:279.0000 R:147.0 rate:0.294 gloss:2.9876 dlossA:2.2449 dlossQ:0.8221 exploreP:0.0476\n",
      "Episode:229 meanR:282.3000 R:500.0 rate:1.0 gloss:3.1150 dlossA:2.4467 dlossQ:0.8523 exploreP:0.0458\n",
      "Episode:230 meanR:286.4900 R:500.0 rate:1.0 gloss:3.1932 dlossA:2.4401 dlossQ:0.8566 exploreP:0.0440\n",
      "Episode:231 meanR:291.1400 R:500.0 rate:1.0 gloss:3.3893 dlossA:2.5792 dlossQ:0.9158 exploreP:0.0424\n",
      "Episode:232 meanR:294.6900 R:500.0 rate:1.0 gloss:3.4802 dlossA:2.7997 dlossQ:0.9209 exploreP:0.0408\n",
      "Episode:233 meanR:298.8400 R:500.0 rate:1.0 gloss:3.6349 dlossA:2.9608 dlossQ:0.9480 exploreP:0.0393\n",
      "Episode:234 meanR:303.6100 R:500.0 rate:1.0 gloss:3.7605 dlossA:3.0168 dlossQ:0.9836 exploreP:0.0379\n",
      "Episode:235 meanR:305.4700 R:321.0 rate:0.642 gloss:3.8581 dlossA:2.9928 dlossQ:1.0027 exploreP:0.0370\n",
      "Episode:236 meanR:309.6400 R:500.0 rate:1.0 gloss:4.0058 dlossA:2.9364 dlossQ:1.0060 exploreP:0.0357\n",
      "Episode:237 meanR:313.6300 R:500.0 rate:1.0 gloss:4.1065 dlossA:3.1055 dlossQ:0.9817 exploreP:0.0344\n",
      "Episode:238 meanR:318.3000 R:500.0 rate:1.0 gloss:4.3287 dlossA:3.3215 dlossQ:1.0618 exploreP:0.0332\n",
      "Episode:239 meanR:323.1300 R:500.0 rate:1.0 gloss:4.4527 dlossA:3.3956 dlossQ:1.1429 exploreP:0.0321\n",
      "Episode:240 meanR:327.5200 R:500.0 rate:1.0 gloss:4.6313 dlossA:3.5225 dlossQ:1.1698 exploreP:0.0310\n",
      "Episode:241 meanR:331.5600 R:500.0 rate:1.0 gloss:4.8300 dlossA:3.8825 dlossQ:1.2251 exploreP:0.0300\n",
      "Episode:242 meanR:336.0900 R:500.0 rate:1.0 gloss:4.9672 dlossA:3.6382 dlossQ:1.2246 exploreP:0.0290\n",
      "Episode:243 meanR:338.7600 R:500.0 rate:1.0 gloss:5.3152 dlossA:4.3134 dlossQ:1.3757 exploreP:0.0281\n",
      "Episode:244 meanR:339.9500 R:490.0 rate:0.98 gloss:5.4322 dlossA:4.1821 dlossQ:1.4071 exploreP:0.0272\n",
      "Episode:245 meanR:342.6500 R:500.0 rate:1.0 gloss:5.5915 dlossA:4.4459 dlossQ:1.3984 exploreP:0.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:246 meanR:347.2900 R:500.0 rate:1.0 gloss:5.8539 dlossA:4.7761 dlossQ:1.5203 exploreP:0.0256\n",
      "Episode:247 meanR:348.3700 R:303.0 rate:0.606 gloss:6.0015 dlossA:4.7427 dlossQ:1.4859 exploreP:0.0251\n",
      "Episode:248 meanR:352.9400 R:500.0 rate:1.0 gloss:6.1646 dlossA:4.8433 dlossQ:1.5560 exploreP:0.0244\n",
      "Episode:249 meanR:356.4200 R:500.0 rate:1.0 gloss:6.6527 dlossA:4.9133 dlossQ:1.6127 exploreP:0.0237\n",
      "Episode:250 meanR:360.2400 R:500.0 rate:1.0 gloss:7.1341 dlossA:5.3243 dlossQ:1.8980 exploreP:0.0230\n",
      "Episode:251 meanR:364.8900 R:500.0 rate:1.0 gloss:7.3818 dlossA:5.9522 dlossQ:2.0194 exploreP:0.0224\n",
      "Episode:252 meanR:368.5900 R:500.0 rate:1.0 gloss:7.7569 dlossA:5.8168 dlossQ:2.3246 exploreP:0.0218\n",
      "Episode:253 meanR:373.4700 R:500.0 rate:1.0 gloss:7.8622 dlossA:6.2360 dlossQ:2.3337 exploreP:0.0212\n",
      "Episode:254 meanR:377.7100 R:500.0 rate:1.0 gloss:8.3947 dlossA:5.9505 dlossQ:2.3900 exploreP:0.0207\n",
      "Episode:255 meanR:381.2500 R:500.0 rate:1.0 gloss:8.9207 dlossA:6.6697 dlossQ:2.9710 exploreP:0.0201\n",
      "Episode:256 meanR:385.9700 R:500.0 rate:1.0 gloss:9.1327 dlossA:6.6499 dlossQ:2.7154 exploreP:0.0196\n",
      "Episode:257 meanR:389.3000 R:500.0 rate:1.0 gloss:9.6881 dlossA:7.2447 dlossQ:3.0051 exploreP:0.0192\n",
      "Episode:258 meanR:392.2500 R:321.0 rate:0.642 gloss:9.7717 dlossA:7.1089 dlossQ:2.9069 exploreP:0.0189\n",
      "Episode:259 meanR:394.0700 R:500.0 rate:1.0 gloss:10.1134 dlossA:7.6524 dlossQ:3.3255 exploreP:0.0184\n",
      "Episode:260 meanR:396.5500 R:380.0 rate:0.76 gloss:10.5271 dlossA:8.0355 dlossQ:3.5039 exploreP:0.0181\n",
      "Episode:261 meanR:400.1700 R:439.0 rate:0.878 gloss:11.0207 dlossA:8.3508 dlossQ:3.6727 exploreP:0.0178\n",
      "Episode:262 meanR:404.3100 R:500.0 rate:1.0 gloss:11.4074 dlossA:8.0841 dlossQ:3.5472 exploreP:0.0174\n",
      "Episode:263 meanR:407.8100 R:500.0 rate:1.0 gloss:12.0827 dlossA:9.0506 dlossQ:3.8259 exploreP:0.0170\n",
      "Episode:264 meanR:412.4400 R:500.0 rate:1.0 gloss:12.6455 dlossA:8.9498 dlossQ:4.4438 exploreP:0.0167\n",
      "Episode:265 meanR:415.0000 R:500.0 rate:1.0 gloss:12.9113 dlossA:9.2563 dlossQ:4.3468 exploreP:0.0164\n",
      "Episode:266 meanR:418.4100 R:500.0 rate:1.0 gloss:13.3846 dlossA:9.8783 dlossQ:4.4649 exploreP:0.0161\n",
      "Episode:267 meanR:417.6100 R:281.0 rate:0.562 gloss:13.5821 dlossA:9.2642 dlossQ:4.4725 exploreP:0.0159\n",
      "Episode:268 meanR:420.3900 R:500.0 rate:1.0 gloss:14.3949 dlossA:10.1989 dlossQ:5.1798 exploreP:0.0156\n",
      "Episode:269 meanR:422.1700 R:500.0 rate:1.0 gloss:14.7188 dlossA:10.3318 dlossQ:5.0720 exploreP:0.0153\n",
      "Episode:270 meanR:425.8600 R:500.0 rate:1.0 gloss:15.8134 dlossA:11.0464 dlossQ:5.3642 exploreP:0.0151\n",
      "Episode:271 meanR:428.4700 R:500.0 rate:1.0 gloss:17.0037 dlossA:12.4052 dlossQ:7.6340 exploreP:0.0148\n",
      "Episode:272 meanR:429.3900 R:500.0 rate:1.0 gloss:17.3760 dlossA:11.8917 dlossQ:8.0136 exploreP:0.0146\n",
      "Episode:273 meanR:430.2000 R:500.0 rate:1.0 gloss:17.5423 dlossA:12.1827 dlossQ:7.3071 exploreP:0.0144\n",
      "Episode:274 meanR:432.6500 R:500.0 rate:1.0 gloss:17.8437 dlossA:12.4684 dlossQ:7.7724 exploreP:0.0142\n",
      "Episode:275 meanR:435.9000 R:500.0 rate:1.0 gloss:18.8585 dlossA:12.8175 dlossQ:8.7879 exploreP:0.0140\n",
      "Episode:276 meanR:437.0700 R:280.0 rate:0.56 gloss:19.5171 dlossA:14.4635 dlossQ:10.0948 exploreP:0.0138\n",
      "Episode:277 meanR:437.7400 R:500.0 rate:1.0 gloss:19.4794 dlossA:14.2750 dlossQ:10.0220 exploreP:0.0137\n",
      "Episode:278 meanR:437.7400 R:500.0 rate:1.0 gloss:19.2479 dlossA:13.1810 dlossQ:8.1991 exploreP:0.0135\n",
      "Episode:279 meanR:437.9300 R:500.0 rate:1.0 gloss:19.2184 dlossA:12.7789 dlossQ:8.2516 exploreP:0.0133\n",
      "Episode:280 meanR:441.6500 R:500.0 rate:1.0 gloss:19.7178 dlossA:12.8689 dlossQ:9.1522 exploreP:0.0131\n",
      "Episode:281 meanR:440.1600 R:351.0 rate:0.702 gloss:20.2132 dlossA:13.2129 dlossQ:8.6901 exploreP:0.0130\n",
      "Episode:282 meanR:441.8800 R:500.0 rate:1.0 gloss:20.4117 dlossA:13.6094 dlossQ:9.2316 exploreP:0.0129\n",
      "Episode:283 meanR:444.4300 R:500.0 rate:1.0 gloss:21.0576 dlossA:14.3824 dlossQ:9.9973 exploreP:0.0127\n",
      "Episode:284 meanR:444.4300 R:500.0 rate:1.0 gloss:21.2859 dlossA:14.3077 dlossQ:9.7336 exploreP:0.0126\n",
      "Episode:285 meanR:446.0400 R:500.0 rate:1.0 gloss:21.7273 dlossA:14.3610 dlossQ:10.6378 exploreP:0.0125\n",
      "Episode:286 meanR:450.7900 R:500.0 rate:1.0 gloss:21.9338 dlossA:14.6947 dlossQ:11.5450 exploreP:0.0124\n",
      "Episode:287 meanR:454.1100 R:500.0 rate:1.0 gloss:22.1877 dlossA:14.8501 dlossQ:10.6833 exploreP:0.0123\n",
      "Episode:288 meanR:455.2100 R:500.0 rate:1.0 gloss:22.3380 dlossA:14.8489 dlossQ:11.0824 exploreP:0.0121\n",
      "Episode:289 meanR:455.2100 R:500.0 rate:1.0 gloss:23.1187 dlossA:15.9172 dlossQ:12.0685 exploreP:0.0120\n",
      "Episode:290 meanR:455.5000 R:500.0 rate:1.0 gloss:22.6503 dlossA:14.2668 dlossQ:11.1230 exploreP:0.0119\n",
      "Episode:291 meanR:455.3600 R:417.0 rate:0.834 gloss:23.6871 dlossA:14.1828 dlossQ:10.9900 exploreP:0.0119\n",
      "Episode:292 meanR:455.9700 R:364.0 rate:0.728 gloss:23.8660 dlossA:13.4610 dlossQ:11.1626 exploreP:0.0118\n",
      "Episode:293 meanR:455.9700 R:500.0 rate:1.0 gloss:24.7072 dlossA:15.5919 dlossQ:12.9831 exploreP:0.0117\n",
      "Episode:294 meanR:455.9700 R:500.0 rate:1.0 gloss:23.9596 dlossA:14.4057 dlossQ:17.8106 exploreP:0.0116\n",
      "Episode:295 meanR:458.6500 R:500.0 rate:1.0 gloss:22.6672 dlossA:14.3266 dlossQ:9.8086 exploreP:0.0115\n",
      "Episode:296 meanR:458.6500 R:500.0 rate:1.0 gloss:21.4063 dlossA:12.9274 dlossQ:10.7362 exploreP:0.0115\n",
      "Episode:297 meanR:462.2100 R:500.0 rate:1.0 gloss:20.9419 dlossA:14.0980 dlossQ:10.9699 exploreP:0.0114\n",
      "Episode:298 meanR:462.2100 R:500.0 rate:1.0 gloss:19.6504 dlossA:13.3422 dlossQ:9.3832 exploreP:0.0113\n",
      "Episode:299 meanR:463.4000 R:342.0 rate:0.684 gloss:18.8649 dlossA:11.1682 dlossQ:8.0605 exploreP:0.0113\n",
      "Episode:300 meanR:463.4000 R:500.0 rate:1.0 gloss:17.9088 dlossA:10.8150 dlossQ:7.8927 exploreP:0.0112\n",
      "Episode:301 meanR:463.4000 R:500.0 rate:1.0 gloss:17.4276 dlossA:10.2110 dlossQ:7.3357 exploreP:0.0112\n",
      "Episode:302 meanR:465.2900 R:500.0 rate:1.0 gloss:17.2747 dlossA:9.8262 dlossQ:6.6683 exploreP:0.0111\n",
      "Episode:303 meanR:465.4800 R:500.0 rate:1.0 gloss:17.7246 dlossA:10.2016 dlossQ:7.0901 exploreP:0.0110\n",
      "Episode:304 meanR:467.5500 R:500.0 rate:1.0 gloss:18.2155 dlossA:11.1175 dlossQ:7.5589 exploreP:0.0110\n",
      "Episode:305 meanR:467.8500 R:500.0 rate:1.0 gloss:18.4978 dlossA:11.0084 dlossQ:7.7068 exploreP:0.0109\n",
      "Episode:306 meanR:467.8500 R:500.0 rate:1.0 gloss:19.2287 dlossA:11.2336 dlossQ:8.4037 exploreP:0.0109\n",
      "Episode:307 meanR:469.0000 R:278.0 rate:0.556 gloss:20.5016 dlossA:14.1137 dlossQ:9.8962 exploreP:0.0109\n",
      "Episode:308 meanR:469.0000 R:500.0 rate:1.0 gloss:19.5291 dlossA:11.5061 dlossQ:8.5714 exploreP:0.0108\n",
      "Episode:309 meanR:469.0000 R:500.0 rate:1.0 gloss:19.9285 dlossA:11.8876 dlossQ:9.1944 exploreP:0.0108\n",
      "Episode:310 meanR:469.0000 R:500.0 rate:1.0 gloss:20.8261 dlossA:12.2104 dlossQ:9.5391 exploreP:0.0108\n",
      "Episode:311 meanR:469.0000 R:500.0 rate:1.0 gloss:21.1473 dlossA:11.9213 dlossQ:9.6384 exploreP:0.0107\n",
      "Episode:312 meanR:469.0000 R:500.0 rate:1.0 gloss:21.6024 dlossA:10.8257 dlossQ:8.9602 exploreP:0.0107\n",
      "Episode:313 meanR:469.0000 R:500.0 rate:1.0 gloss:22.9492 dlossA:12.5232 dlossQ:10.4775 exploreP:0.0107\n",
      "Episode:314 meanR:469.0000 R:500.0 rate:1.0 gloss:22.6866 dlossA:11.5507 dlossQ:10.0127 exploreP:0.0106\n",
      "Episode:315 meanR:469.0000 R:500.0 rate:1.0 gloss:22.4665 dlossA:11.0673 dlossQ:10.5673 exploreP:0.0106\n",
      "Episode:316 meanR:472.3200 R:500.0 rate:1.0 gloss:22.4369 dlossA:11.5591 dlossQ:8.6098 exploreP:0.0106\n",
      "Episode:317 meanR:472.3200 R:500.0 rate:1.0 gloss:22.6405 dlossA:11.7313 dlossQ:8.9295 exploreP:0.0105\n",
      "Episode:318 meanR:472.3200 R:500.0 rate:1.0 gloss:22.0098 dlossA:11.4340 dlossQ:9.7986 exploreP:0.0105\n",
      "Episode:319 meanR:472.3200 R:500.0 rate:1.0 gloss:22.0871 dlossA:11.5966 dlossQ:9.5188 exploreP:0.0105\n",
      "Episode:320 meanR:473.0000 R:500.0 rate:1.0 gloss:22.0155 dlossA:9.6734 dlossQ:9.4331 exploreP:0.0105\n",
      "Episode:321 meanR:473.0000 R:500.0 rate:1.0 gloss:21.1681 dlossA:8.4010 dlossQ:9.1550 exploreP:0.0104\n",
      "Episode:322 meanR:473.2600 R:500.0 rate:1.0 gloss:21.7129 dlossA:10.2411 dlossQ:10.6291 exploreP:0.0104\n",
      "Episode:323 meanR:473.2600 R:500.0 rate:1.0 gloss:20.9382 dlossA:8.4495 dlossQ:9.4766 exploreP:0.0104\n",
      "Episode:324 meanR:476.6600 R:500.0 rate:1.0 gloss:21.6240 dlossA:10.1389 dlossQ:10.0760 exploreP:0.0104\n",
      "Episode:325 meanR:476.6600 R:500.0 rate:1.0 gloss:21.7787 dlossA:10.0776 dlossQ:9.5997 exploreP:0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:326 meanR:477.1400 R:500.0 rate:1.0 gloss:21.2632 dlossA:8.3736 dlossQ:9.6350 exploreP:0.0103\n",
      "Episode:327 meanR:477.1400 R:500.0 rate:1.0 gloss:21.7871 dlossA:10.1387 dlossQ:10.4156 exploreP:0.0103\n",
      "Episode:328 meanR:480.6700 R:500.0 rate:1.0 gloss:21.5989 dlossA:10.3015 dlossQ:10.6448 exploreP:0.0103\n",
      "Episode:329 meanR:480.6700 R:500.0 rate:1.0 gloss:21.4148 dlossA:11.2631 dlossQ:11.1940 exploreP:0.0103\n",
      "Episode:330 meanR:480.6700 R:500.0 rate:1.0 gloss:21.2416 dlossA:9.4805 dlossQ:8.9223 exploreP:0.0103\n",
      "Episode:331 meanR:480.6700 R:500.0 rate:1.0 gloss:21.4556 dlossA:9.0298 dlossQ:9.4877 exploreP:0.0103\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        idx_arr = np.arange(memory_size// batch_size)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            percentage = 0.9\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            states = states[rates >= (np.max(rates)*percentage)]\n",
    "            actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "            next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "            dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "            rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
