{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.99989252 -0.01466129  0.99616522  0.08749202 -0.00902808 -0.06894136] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999987e-01  1.61953504e-04  9.99561439e-01  2.96129879e-02\n",
      "  1.52293064e-01 -4.98332031e-01] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99957239  0.02924106  0.99764688 -0.0685617   0.12956216 -0.46254085] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99808812  0.06180695  0.98358908 -0.18042316  0.18600244 -0.64044194] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99631932  0.08571942  0.96117504 -0.27593937  0.04658256 -0.32199838] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99687593  0.07898347  0.95369567 -0.30077327 -0.11309361  0.06513157] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9995646   0.02950627  0.97549525 -0.22002049 -0.3703753   0.75103438] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99938404 -0.03509343  0.99616581 -0.08748525 -0.25839445  0.55796721] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99553235 -0.09442108  0.99828586  0.05852642 -0.31936463  0.86869897] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99171048 -0.12849251  0.98469437  0.17429    -0.01373613  0.27383403] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99508316 -0.09904299  0.98654357  0.16349859  0.303432   -0.37640917] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99991257 -0.01322285  0.99944928  0.03318336  0.53484522 -0.89818106] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99702791  0.07704116  0.99427483 -0.10685297  0.34618358 -0.46655683] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99135451  0.13121067  0.98441533 -0.17585917  0.18574433 -0.2111421 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99094625  0.13425917  0.98843044 -0.15167489 -0.15457438  0.452036  ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99732163  0.07314072  0.99999318 -0.00369377 -0.44274544  0.99834903] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99998171 -0.0060489   0.98740705  0.15820025 -0.32901848  0.58762361] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99790414 -0.06470952  0.96809668  0.25057697 -0.24369533  0.33224191] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99512589 -0.09861269  0.95984281  0.28053837 -0.08902556 -0.03104437] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99511396 -0.09873305  0.97132029  0.2377749   0.08657686 -0.40440953] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99779764 -0.06633153  0.99170891  0.12850461  0.22864045 -0.68401044] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999940e-01  3.45336741e-04  9.98462006e-01 -5.54402618e-02\n",
      "  4.18771960e-01 -1.11619296e+00] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99696883  0.07780202  0.96587791 -0.2589978   0.33453531 -0.90182752] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9902898   0.13901841  0.90749613 -0.42006044  0.26424833 -0.77650049] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98424488  0.17681068  0.84736924 -0.53100411  0.10872684 -0.46329879] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98585768  0.16758468  0.83622185 -0.5483913  -0.19998051  0.2569678 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99608177  0.08843708  0.90949109 -0.41572341 -0.58163392  1.23208397] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99962222 -0.02748491  0.98471264 -0.17418675 -0.54725057  1.24271612] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9896243  -0.14367934  0.99379215  0.11125274 -0.58567097  1.55263691] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97560225 -0.21954553  0.94005188  0.34103147 -0.16525248  0.76538112] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97604841 -0.21755342  0.9046279   0.42620224  0.18629984  0.14538767] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98881568 -0.14914273  0.91851019  0.3953973   0.49469517 -0.46844374] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99906926 -0.04313493  0.95602914  0.29327169  0.54629545 -0.58610942] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99642223  0.08451471  0.99291848  0.1187977   0.70065889 -1.15044485] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.978134    0.20797565  0.9946778  -0.10303433  0.51603039 -1.01414043] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96105845  0.27634519  0.96483216 -0.26286671  0.1706894  -0.57757526] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96275236  0.27038473  0.94768914 -0.31919476 -0.23234224 -0.00377526] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97934077  0.20221686  0.95432112 -0.29878286 -0.45490213  0.20536867] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99729889  0.07345021  0.98333267 -0.18181546 -0.81665871  0.96208346] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99672052 -0.08092101  0.99994766 -0.01023117 -0.69172631  0.70872599] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97668874 -0.21466046  0.98878895  0.1493198  -0.6293482   0.8434121 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94911532 -0.31492874  0.95306223  0.30277448 -0.38622812  0.69331772] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94362192 -0.33102517  0.9403742   0.34014168  0.21930895 -0.30485724] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96576401 -0.25942219  0.96675575  0.25570161  0.51324447 -0.55587289] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99337349 -0.11493093  0.99757513  0.06959786  0.92426218 -1.27996637] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99606142  0.08866597  0.97521847 -0.22124405  1.06351468 -1.55968957] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95870489  0.28440277  0.86917586 -0.49450311  0.88491708 -1.30464878] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91531261  0.40274411  0.7720278  -0.63558876  0.35236663 -0.38033306] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91148487  0.41133361  0.78754916 -0.61625183 -0.25987288  0.6260677 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95481814  0.2971907   0.91623379 -0.40064405 -0.93828518  1.85287582] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99576055  0.09198325  0.99977269 -0.0213206  -1.10147887  1.9513068 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99345035 -0.11426465  0.94819475  0.31768965 -0.91235796  1.40165124] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96427736 -0.26489466  0.85580919  0.51729162 -0.5896787   0.75120156] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.946701   -0.32211367  0.83743574  0.54653581  0.00249727 -0.41481206] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96122718 -0.2757577   0.91566315  0.40194652  0.47178722 -1.20406466] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98695829 -0.16097622  0.98887325  0.14876055  0.67303997 -1.37665487] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99968367 -0.0251509   0.99412825 -0.10820823  0.65577428 -1.132996  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99361756  0.11280137  0.94036986 -0.34015368  0.69031891 -1.18968507] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97496125  0.22237481  0.86439055 -0.50282101  0.3971241  -0.56875117] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96510388  0.26186734  0.84478718 -0.53510245  0.00194375  0.19975129] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97480885  0.22304193  0.90104664 -0.43372221 -0.39294214  0.94056725] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99337022  0.11495912  0.97948326 -0.20152553 -0.67605442  1.46200352] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99904033 -0.04379968  0.99001766  0.14094338 -0.86874188  1.89164962] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97767387 -0.21012809  0.87122583  0.49088242 -0.76381148  1.73456449] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94484643 -0.32751371  0.6927769   0.72115197 -0.42953231  1.14003124] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93781365 -0.34713911  0.62397764  0.78144219  0.22098993 -0.22861449] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96338583 -0.26811889  0.71267911  0.70149019  0.59379339 -0.94664035] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99347615 -0.11404008  0.87777703  0.4790694   0.94582504 -1.78203726] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99477514  0.10209028  0.99807241  0.06206015  1.16304449 -2.48456023] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95235405  0.3049947   0.91910107 -0.39402185  0.85366408 -2.06728178] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91568077  0.40190638  0.76633366 -0.64244278  0.16433186 -0.8209446 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92197254  0.38725527  0.7041159  -0.71008507 -0.32034123 -0.09102202] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95809855  0.28643877  0.74139282 -0.67107129 -0.72953878  0.6137097 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99563188  0.0933657   0.88124799 -0.47265418 -1.20148693  1.77081105] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98980489 -0.14242993  0.99181356 -0.12769442 -1.10279578  1.76650038] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9479247  -0.31849454  0.98522104  0.1712878  -0.66400257  1.15443718] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92283651 -0.38519187  0.95205685  0.30592115 -0.03306928  0.20539168] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94392395 -0.33016294  0.96854605  0.24883438  0.60961872 -0.78352582] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98684677 -0.16165846  0.99991247  0.01323056  1.08987124 -1.53267628] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99700607  0.07732337  0.94782535 -0.31879006  1.2468936  -1.74029681] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95929862  0.28239363  0.83847353 -0.54494233  0.7982462  -0.71908356] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91780877  0.39702275  0.78857752 -0.61493536  0.39769425 -0.11825691] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91529962  0.40277364  0.84960614 -0.52741768 -0.33711962  1.17652991] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95785435  0.28725432  0.97195981 -0.23514702 -0.86361588  1.94324422] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99611244  0.0880909   0.98305503  0.18331067 -1.10995159  2.16081088] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99185095 -0.12740368  0.83691837  0.54732773 -0.99649834  1.69062965] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96077611 -0.27732519  0.69613543  0.71791048 -0.50866162  0.4912395 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94437277 -0.32887698  0.69716771  0.71690807 -0.02112526 -0.51121661] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95878072 -0.28414701  0.82606348  0.56357709  0.48316545 -1.47226732] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98874025 -0.14964198  0.97398472  0.2266137   0.85942608 -2.15366905] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99884761  0.0479943   0.96787281 -0.2514403   1.05949626 -2.54395867] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9726522   0.23226642  0.76349479 -0.64581399  0.75835426 -1.84430114] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94666258  0.32222658  0.59010197 -0.80732872  0.16779706 -0.51272891] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94838978  0.31710697  0.57011915 -0.82156202 -0.21978554  0.26766381] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9731839   0.23002849  0.69255582 -0.72136429 -0.67101128  1.29735753] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99868305  0.0513046   0.91014981 -0.41427928 -1.10213325  2.42834143] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98718849 -0.1595584   0.99851445  0.05448754 -0.9490308   2.25767919] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9459411  -0.32433846  0.88014961  0.47469639 -0.70013942  2.0374636 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92073762 -0.39018231  0.71513138  0.69899006  0.00478589  0.72780145] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9421982  -0.33505604  0.68710511  0.72655803  0.57327597 -0.33081475] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98154226 -0.19124536  0.77779992  0.62851197  0.8884788  -0.9730641 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999883 -0.00152839  0.90321553  0.42918727  0.97769436 -1.32445671] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98147102  0.19161065  0.98930314  0.14587428  0.91829542 -1.56598867] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93596163  0.35210202  0.98346212 -0.18111392  0.70747685 -1.63462473] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90173938  0.4322801   0.90367042 -0.42822864  0.14210587 -0.92041932] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9158939   0.40142044  0.85836495 -0.51303958 -0.47592992 -0.03295639] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96208505  0.27274961  0.88166755 -0.4718711  -0.86507441  0.48438512] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99842338  0.05613156  0.95741525 -0.28871444 -1.28774414  1.44127733] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9770304  -0.21309999  0.99877563  0.04946966 -1.35512365  1.87423524] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89677614 -0.44248453  0.91712759  0.39859376 -1.02286085  1.63301469] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82235456 -0.56897537  0.78204442  0.62322269 -0.41663113  0.95065981] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82151365 -0.57018885  0.732674    0.68057976  0.40108602 -0.19956827] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89511289 -0.44583956  0.80988419  0.58658981  1.01910899 -0.99583169] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97699927 -0.21324266  0.93336247  0.35893523  1.4006552  -1.53311278] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99740292  0.07202376  0.99865756  0.05179846  1.40015592 -1.51320667] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94999771  0.31225687  0.98098868 -0.19406496  0.99839879 -0.87644765] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89879928  0.43836042  0.96218015 -0.27241396  0.33713394  0.09702104] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88983492  0.45628261  0.97531369 -0.22082394 -0.13804421  0.42797558] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92553092  0.37867204  0.99701307 -0.07723305 -0.69624134  0.991032  ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98102498  0.19388137  0.98262624  0.18559548 -1.1891512   1.57034156] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99798359 -0.06347249  0.87241405  0.48876755 -1.33472486  1.56790977] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9548202  -0.29718409  0.73524519  0.67780123 -0.99849293  0.71451133] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89673564 -0.4425666   0.68770462  0.7259906  -0.53852522 -0.05797463] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87910524 -0.47662771  0.7727237   0.63474253  0.16321344 -1.18160546] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92767156 -0.3733972   0.95018183  0.31169616  0.9533393  -2.47104044] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98673886 -0.16231581  0.98206273 -0.18855451  1.17568036 -2.46005253] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99814882  0.06081881  0.81726734 -0.57625871  1.01097134 -1.68795797] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96931988  0.2458027   0.6215948  -0.78333895  0.82580854 -1.11443253] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9371986   0.34879618  0.55216403 -0.8337355   0.23368017  0.26940588] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93625925  0.35130986  0.65995599 -0.75130426 -0.20945844  1.07953961] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96238573  0.27168678  0.84568366 -0.5336845  -0.61017953  1.74961518] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99484885  0.10136943  0.99404517 -0.10896879 -1.07817899  2.69037937] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99479766 -0.10187062  0.92570394  0.37824889 -0.89553366  2.1527288 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96585376 -0.25908785  0.71230451  0.70187057 -0.66592592  1.67267736] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9367978  -0.34987124  0.50915044  0.86067754 -0.27247527  0.88518135] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93293264 -0.36005095  0.42991313  0.90287026  0.16255959  0.00914873] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96266728 -0.27068748  0.55327719  0.83299721  0.76662466 -1.4188313 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99758623 -0.06943857  0.84335103  0.53736304  1.24705157 -2.70209689] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9796048   0.2009339   0.99853512 -0.05410734  1.38902984 -3.34651466] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90701487  0.4210986   0.79050905 -0.61245036  0.86504501 -2.56407116] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 8.67026952e-01  4.98261241e-01  5.24090000e-01 -8.51662887e-01\n",
      " -6.93689194e-04 -1.01734012e+00] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89635972  0.44332747  0.43344036 -0.90118225 -0.60649013 -0.01611278] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96136633  0.27527219  0.53951011 -0.84197912 -1.16304345  1.20998474] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999919e-01 -4.02817002e-04  8.12080201e-01 -5.83545839e-01\n",
      " -1.58070450e+00  2.50738139e+00] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94693349 -0.32142958  0.99913154 -0.04166725 -1.6048274   3.14750936] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82288255 -0.56821149  0.84701336  0.53157161 -1.08757297  2.72113231] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74577002 -0.66620348  0.55985992  0.82858727 -0.1415243   1.40411087] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79547385 -0.60598792  0.46425198  0.88570317  0.90211754 -0.29095295] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92821201 -0.37205169  0.65173963  0.75844279  1.75183575 -1.96197339] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99966559  0.02585944  0.94754651  0.31961792  2.23620606 -3.26822162] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89264299  0.45076435  0.93546646 -0.35341547  2.0603857  -3.36099936] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70751228  0.70670105  0.63648304 -0.7712907   1.06219542 -1.77150716] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.63451372  0.7729116   0.48373816 -0.87521277 -0.08318258 -0.08171256] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71484609  0.69928182  0.56424818 -0.82560523 -0.99297018  1.02780174] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87894228  0.47692816  0.7909239  -0.61191453 -1.73509656  2.06318418] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99427394  0.10686125  0.98385846 -0.17894843 -2.07686628  2.58065315] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95413344 -0.29938165  0.94503538  0.32696808 -1.93062875  2.36841627] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80115931 -0.59845114  0.73757572  0.67526444 -1.37778779  1.62238637] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.68692674 -0.72672667  0.6188748   0.78548964 -0.3198804  -0.00378136] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72102874 -0.69290516  0.73783813  0.6749777   0.79700505 -1.61603855] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87020744 -0.49268552  0.9558527   0.29384623  1.65941174 -2.73997872] 0 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.9928094  -0.11970588  0.94500921 -0.32704372  2.18565294 -3.36771997] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95881029  0.28404721  0.62534763 -0.7803463   1.81220076 -2.12268882] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83054538  0.55695097  0.37655691 -0.92639349  1.16452121 -0.74098282] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74002845  0.67257556  0.37394921 -0.92744918  0.2778211   0.71262989] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76781498  0.64067164  0.62096428 -0.78383886 -0.70188038  2.14554234] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88860217  0.45867874  0.93120479 -0.36449643 -1.43755135  3.04658267] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98843318  0.151657    0.96701013  0.25473791 -1.70965728  3.07228514] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98559732 -0.16910919  0.70871315  0.70549675 -1.44248519  2.06283991] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91514089 -0.40313416  0.46775628  0.88385749 -0.9656109   0.9045791 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85640779 -0.51630001  0.42099333  0.90706373 -0.28597032 -0.39134646] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86082126 -0.50890741  0.57723335  0.81657925  0.37403625 -1.40668794] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9267683  -0.3756335   0.85167841  0.52406477  1.08871034 -2.58626235] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99437304 -0.10593514  0.99724715 -0.07414929  1.61735519 -3.50730953] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9763039   0.216404    0.72294128 -0.69090948  1.52980759 -3.18944811] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.892524    0.4509999   0.29314282 -0.95606866  0.93565957 -1.87742897] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83878616  0.544461    0.08621336 -0.9962767   0.13620063 -0.24004546] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86245999  0.50612525  0.17436426 -0.98468122 -0.58258191  1.12966427] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94039828  0.34007509  0.51232754 -0.85879013 -1.23520728  2.48534193] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99874897  0.05000505  0.91399242 -0.40573126 -1.67426844  3.56115559] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96367348 -0.26708319  0.96097155  0.27664723 -1.42330606  3.20381572] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88463125 -0.46629128  0.69931422  0.7148144  -0.67941019  1.87430372] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86027022 -0.50983835  0.52533845  0.85089336  0.18007981  0.33253636] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90966444 -0.41534397  0.57419041  0.81872179  0.86673317 -0.90836407] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97765038 -0.21023731  0.76867649  0.63963776  1.25665712 -1.69460369] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99785436  0.06547265  0.96129871  0.27550823  1.45339059 -2.3495875 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94301682  0.33274507  0.98189227 -0.18944017  1.21029876 -2.21026694] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8593883   0.51132353  0.830469   -0.55706484  0.71688168 -1.70568269] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81791395  0.57534056  0.66010273 -0.75117534  0.03141438 -0.85308884] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85887459  0.51218594  0.62503178 -0.78059931 -0.76904446  0.39507815] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95388323  0.30017793  0.78657227 -0.61749823 -1.51892925  1.87393063] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99977606 -0.02116212  0.97102807 -0.23896546 -1.66491447  2.24479537] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93830672 -0.34580414  0.97117332  0.23837445 -1.56894327  2.42174631] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81565915 -0.57853276  0.7830273   0.62198734 -1.01215488  1.79153581] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74438272 -0.66775322  0.61735     0.78668861 -0.11533018  0.53297932] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78856624 -0.61494982  0.63560731  0.77201253  0.79290147 -0.76636785] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91163785 -0.41099443  0.82357314  0.56721008  1.55505545 -1.98639653] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99698986 -0.07753208  0.98826276  0.15276363  1.8227103  -2.37822713] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95708487  0.2898078   0.93930483 -0.34308371  1.79794496 -2.4877107 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82168656  0.56993964  0.709376   -0.70483026  1.26302265 -1.74084739] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71744174  0.69661851  0.54191805 -0.84043133  0.35787296 -0.40770698] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72702846  0.68660733  0.56200904 -0.82713109 -0.49430953  0.64707427] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84969257  0.52727842  0.77718534 -0.62927175 -1.49425661  2.27180876] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98406447  0.17781201  0.99602342 -0.08909179 -2.18649582  3.49474787] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97032508 -0.24180414  0.84980482  0.52709749 -1.93365028  2.74362385] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83374449 -0.55215046  0.52651504  0.85016581 -1.41110904  1.78806853] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70311035 -0.71108075  0.30887036  0.95110415 -0.62180524  0.60853502] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.677139   -0.73585513  0.30589193  0.95206624  0.26797182 -0.5751346 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7804056  -0.62527362  0.54122765  0.84087611  1.23135708 -2.03466838] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94053499 -0.33969682  0.89874102  0.43847985  2.00056468 -3.33774415] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9951197   0.09867511  0.95538827 -0.29535278  2.32278729 -3.9388453 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8638696   0.50371551  0.52927349 -0.84845128  1.87393641 -3.03131804] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67837612  0.7347148   0.08428785 -0.99644145  1.06568239 -1.69248417] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59933626  0.80049738 -0.07341477 -0.99730149 -0.04382875  0.09992016] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69114318  0.72271786  0.12468824 -0.99219597 -1.15253791  1.90117053] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88544752  0.46473937  0.6200747  -0.78454277 -2.05883378  3.53027531] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99994577  0.01041382  0.99765013 -0.06851437 -2.54826959  4.56925462] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.897505   -0.44100428  0.72828869  0.68527045 -1.99788214  3.42134777] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72402315 -0.68977568  0.3062168   0.9519618  -1.01586463  1.61321932] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64064891 -0.76783395  0.11312289  0.993581   -0.11554746  0.36767326] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69835448 -0.71575206  0.18864519  0.98204531  0.88349947 -1.13664652] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8629766  -0.5052439   0.54083834  0.84112656  1.76653225 -2.67667236] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99241956 -0.12289596  0.93568739  0.35283013  2.21195726 -3.56982735] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94524934  0.32634902  0.92019654 -0.39145668  2.20962954 -3.78809642] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76409001  0.64510965  0.51221035 -0.85886004  1.4135322  -2.42747098] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.63358324  0.77367453  0.20397193 -0.97897674  0.40586984 -0.90149764] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65119275  0.75891238  0.17392493 -0.98475891 -0.63019621  0.59670424] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80296336  0.59602839  0.43262515 -0.90157389 -1.57526747  2.13494912] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96956865  0.24481958  0.85350877 -0.52107848 -2.27304396  3.53939492] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97102227 -0.23898901  0.96881207  0.24779664 -2.47175977  4.16023937] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77582148 -0.63095248  0.54163088  0.84061644 -1.84752239  3.15596962] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57412325 -0.81876889  0.0792727   0.99685297 -0.89087724  1.77486641] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.52618213 -0.8503719  -0.09365047  0.99560514  0.31408275 -0.03056329] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66283586 -0.74876474  0.06930786  0.99759532  1.3718137  -1.61737555] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88264534 -0.47003958  0.50857332  0.86101868  2.15259852 -3.01295691] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999972e-01  2.38096714e-04  9.57490399e-01  2.88465140e-01\n",
      "  2.64159338e+00 -4.25840347e+00] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87447343  0.48507341  0.85658724 -0.51600222  2.2740663  -3.79186349] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64555968  0.76370983  0.40685138 -0.91349437  1.30334772 -2.25423581] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.52441176  0.8514648   0.12453501 -0.99221521  0.18536837 -0.70068519] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57967714  0.81484625  0.11129473 -0.99378744 -0.83920238  0.57478324] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78240929  0.62276456  0.40185688 -0.91570249 -1.9340948   2.46770815] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98093982  0.19431179  0.89049229 -0.45499834 -2.75990466  4.28866503] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93908481 -0.34368551  0.92698942  0.37508749 -2.54232079  3.94541709] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70932339 -0.7048832   0.50673764  0.86210032 -1.69976978  2.50383757] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5270322  -0.84984531  0.18880343  0.9820149  -0.61540608  0.92579228] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.52723078 -0.84972213  0.18065146  0.98354718  0.61700754 -0.84253068] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71650999 -0.69757683  0.50918262  0.8606585   1.80080248 -2.70803675] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95268563 -0.30395737  0.95317708  0.30241272  2.75210889 -4.46783028] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97073564  0.24015061  0.84621916 -0.53283499  2.60108052 -3.89297184] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76734992  0.64122859  0.36762947 -0.92997235  1.86513404 -2.35051969] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57237546  0.81999167  0.0969702  -0.99528729  0.76105439 -0.46642661] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53831118  0.84274615  0.15678213 -0.98763321 -0.35954377  1.06496473] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69115311  0.72270836  0.5261239  -0.85040793 -1.57958708  2.92351202] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91867546  0.39501316  0.95215093 -0.30562821 -2.35641085  4.00917709] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99417405 -0.1077866   0.85242362  0.52285177 -2.62798921  4.2746932 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84263944 -0.5384782   0.33011004  0.94394245 -1.91121157  2.48534688] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64891986 -0.76085676  0.00286918  0.99999588 -1.01373491  0.85990577] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57057427 -0.82124601 -0.0129923   0.9999156   0.0395044  -0.69366304] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66265776 -0.74892236  0.27790971  0.9606072   1.12801    -2.26517465] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87408828 -0.4857671   0.80035742  0.59952314  2.23096273 -4.17908577] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99996431  0.00844855  0.95004632 -0.31210894  2.75793911 -5.07587474] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86642349  0.49930986  0.33078395 -0.94370651  2.27621131 -3.8885634 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.62397867  0.78144137 -0.2805471  -0.95984026  1.43280036 -2.34415657] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.46628339  0.88463541 -0.57402231 -0.81883966  0.43793532 -0.94632527] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.48072183  0.87687315 -0.61843328 -0.78583731 -0.59766807  0.39231668] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66293995  0.74867257 -0.41672152 -0.9090342  -1.60726447  1.99469033] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90064691  0.43455165  0.11719334 -0.99310912 -2.31297032  3.49031062] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99679896 -0.07994898  0.85019857 -0.52646214 -2.91463574  5.39972821] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79508146 -0.60650266  0.84191599  0.53960862 -2.5932226   5.39716625] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.50120999 -0.86532569  0.11036509  0.99389111 -1.30654309  3.47680992] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.38751461 -0.92186356 -0.39663536  0.91797625  0.02729387  1.74138853] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.50453792 -0.86338954 -0.55666062  0.83074001  1.25253241  0.0800041 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76720236 -0.64140512 -0.44184468  0.89709156  2.14385907 -1.41221989] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9784936  -0.20627716 -0.00472066  0.99998886  2.66187321 -3.09890611] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94334253  0.33182053  0.6902609   0.72356057  2.72538233 -4.45526627] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6568581   0.75401421  0.9887     -0.14990764  2.29844555 -4.83461736] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.37103358  0.92861945  0.55688356 -0.83059057  0.99806693 -3.34962145] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.32953971  0.94414172  0.11345551 -0.99354308 -0.54621365 -1.42574943] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5489403   0.83586156  0.00284545 -0.99999595 -1.87049558  0.35524888] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87262566  0.48838966  0.26712128 -0.96366292 -2.84670561  2.3323723 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99492379 -0.10063127  0.77703432 -0.62945824 -3.15111663  3.71397514] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76890763 -0.63935988  0.99702795  0.07704068 -2.63209468  3.55829406] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.43617533 -0.8998617   0.77743511  0.62896316 -1.56001142  2.36601888] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.26169767 -0.9651499   0.52694874  0.84989707 -0.29527827  0.99846195] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.31822874 -0.94801396  0.44593482  0.89506543  0.88583002 -0.09164803] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58592497 -0.81036531  0.593057    0.80516047  2.11186657 -1.6720524 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90800183 -0.4189662   0.8924039   0.45123749  2.93101511 -2.92348981] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98441518  0.17586002  0.9884156  -0.15177158  3.00678985 -3.00358361] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74986575  0.66159003  0.79078129 -0.61209881  2.34291118 -1.89775308] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.47808889  0.87831145  0.65539585 -0.75528556  1.1106513  -0.08622441] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.40064341  0.91623406  0.76211798 -0.64743817 -0.25522101  1.5829806 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.54188704  0.84045133  0.95644    -0.29192897 -1.32264615  2.45369148] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80522978  0.5929629   0.96128189  0.27556693 -2.23809453  3.15633876] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98990075  0.14176215  0.63371203  0.77356904 -2.58378414  2.69716782] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93678444 -0.349907    0.27439473  0.96161715 -2.31438798  1.28359037] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72848333 -0.68506352  0.16916163  0.98558832 -1.5766433  -0.22538405] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57982968 -0.81473771  0.38289102  0.92379352 -0.35763561 -1.98777125] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.62879338 -0.77757243  0.8173991   0.57607179  0.9578415  -3.61146939] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82939102 -0.55866854  0.97513243 -0.22162297  1.91263141 -4.54681053] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98211704 -0.18827138  0.48493139 -0.8745522   2.03745848 -3.68529364] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97574204  0.21892343 -0.14121417 -0.98997907  2.00969749 -2.72693116] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83449621  0.55101368 -0.52053697 -0.85383913  1.56122046 -1.3173001 ] 0 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.67517081  0.73766143 -0.63328999 -0.77391459  0.85366362 -0.05743997] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.61806497  0.78612702 -0.51956053 -0.85443365 -0.12603283  1.46186018] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71490139  0.69922528 -0.09359147 -0.99561069 -1.18029431  3.09500244] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91189659  0.41042004  0.67128035 -0.74120355 -2.32020219  5.22802334] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99611144 -0.08810219  0.95730897  0.28906666 -2.58112131  5.6243042 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85242076 -0.52285644  0.26071479  0.96541587 -1.92827394  4.31673174] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67652751 -0.73641736 -0.38413772  0.9232758  -0.84863666  2.32127643] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.61823645 -0.78599217 -0.66231829  0.74922258  0.08285268  0.99131117] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70675462 -0.70745877 -0.68402484  0.72945871  1.07509446 -0.69016087] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87602073 -0.48227345 -0.4720006   0.88159823  1.7017216  -1.91979725] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99371897 -0.1119045   0.05726284  0.99835914  2.16503206 -3.57060968] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93995044  0.34131097  0.80219759  0.59705864  2.38912447 -5.08018605] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70001926  0.71412396  0.91458078 -0.40440327  1.90930705 -5.12339349] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.49577452  0.86845128  0.26901938 -0.96313476  0.59239454 -3.60393763] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.52410072  0.85165629 -0.25597343 -0.96668382 -0.89355408 -1.71925475] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75915487  0.65091004 -0.38347452 -0.92355146 -2.12992597  0.36612074] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97560451  0.2195355  -0.1641186  -0.98644061 -2.63962773  1.87547761] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95425229 -0.29900261  0.31910813 -0.94771831 -2.51555985  2.90841067] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72310997 -0.69073292  0.8247746  -0.56546163 -2.00306439  3.38733001] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.46263375 -0.8865495   0.99711287  0.07593364 -1.20657004  3.23883558] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.35363846 -0.93538219  0.81705068  0.57656586  0.04574597  2.07806829] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.49428315 -0.86930097  0.65532989  0.75534279  1.5003501   0.29650681] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81264904 -0.58275341  0.75420374  0.65664048  2.75529851 -1.70360638] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99995453  0.00953634  0.97976506  0.20015099  3.42334513 -3.24704201] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77923107  0.62673674  0.89847002 -0.43903487  3.09380441 -3.01380589] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.36209153  0.93214254  0.59821911 -0.80133258  2.07441174 -1.69284596] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08516027  0.99636727  0.43937936 -0.89830161  0.77134889 -0.22695717] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05102284  0.99869749  0.48431417 -0.87489416 -0.42669953  0.71934317] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.2740987   0.96170157  0.72268395 -0.69117863 -1.83983951  2.34482783] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70432545  0.70987722  0.99311458 -0.11714707 -3.13990927  4.02380458] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99597778  0.08960058  0.74340518  0.66884134 -3.68531464  4.09531346] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8262731  -0.56326971  0.23087806  0.97298269 -3.07338225  1.8386261 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.45800453 -0.88894986  0.10964959  0.99397031 -1.8042025  -0.57409281] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.25198413 -0.96773137  0.37915711  0.92533231 -0.38899355 -2.15523211] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.33471072 -0.94232093  0.84826483  0.52957226  1.23995061 -4.05998558] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64473716 -0.76440434  0.9438758  -0.33030059  2.24824786 -4.67766744] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93108707 -0.36479703  0.34949706 -0.93693746  2.64467309 -3.91202164] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98334236  0.18176306 -0.3269434  -0.94504392  2.83886093 -2.88938865] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75466292  0.65611271 -0.68393642 -0.72954162  2.38816534 -1.2854409 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.45271585  0.89165484 -0.74608852 -0.66584677  1.39668017  0.39805247] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.31073206  0.95049755 -0.56005695 -0.82845411  0.10812844  2.07989782] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.41469847  0.90995889 -0.02786165 -0.99961179 -1.23142961  3.61357571] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72172427  0.69218067  0.76294075 -0.64646842 -2.51796291  5.33450176] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98481239  0.17362189  0.88798725  0.45986806 -3.16111659  6.0026481 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91767262 -0.39733734  0.07780181  0.99696885 -2.59173351  4.01719554] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66213035 -0.74938869 -0.49738432  0.86753031 -1.75324879  2.00819548] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.45351338 -0.89124947 -0.68845789  0.72527631 -0.74344774  0.39774255] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.41464925 -0.90998132 -0.64742616  0.76212819  0.31908767 -0.94831061] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.56213163 -0.82704778 -0.36782187  0.92989627  1.36147745 -2.3470097 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83107345 -0.55616267  0.29309325  0.95608386  2.46630929 -4.45832269] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99968831 -0.02496562  0.97484647  0.22287746  3.04826942 -5.77488808] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85171945  0.52399808  0.64131081 -0.7672812   2.52814639 -4.8244929 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58183406  0.81330752 -0.07677811 -0.9970482   1.42891609 -2.92400145] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.44320276  0.89642139 -0.45228061 -0.89187569  0.19036759 -1.0384921 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.50989844  0.86023461 -0.49655057 -0.86800779 -0.93496732  0.53909283] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73793899  0.67486743 -0.22339411 -0.9747282  -1.97862394  2.43240428] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96627415  0.25751555  0.45096124 -0.89254353 -2.78582095  4.51870476] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94475913 -0.32776544  0.99843125 -0.0559915  -3.00201176  5.62116164] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64726916 -0.76226152  0.54791397  0.83653468 -2.17545111  4.5496    ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.3989355  -0.91697899 -0.11280226  0.99361746 -0.76204781  2.43800263] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.38500386 -0.92291496 -0.40145107  0.91588047  0.59463568  0.57716093] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58436105 -0.81149379 -0.37562286  0.92677261  1.66078009 -0.87650254] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86591366 -0.5001935  -0.03162365  0.99949985  2.50574525 -2.67279286] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99960156  0.02822628  0.62956285  0.77694956  2.94251462 -4.38516279] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82687495  0.56238583  0.99931823 -0.03691978  2.59458408 -4.54558004] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53465015  0.8450735   0.69531435 -0.71870575  1.42028003 -2.96213763] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.3950425   0.91866285  0.3230383  -0.94638589  0.14968376 -1.438383  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.47426146  0.88038405  0.16795059 -0.98579541 -1.02041827 -0.14086383] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73308411  0.68013799  0.33131937 -0.94351867 -2.22372631  1.86228432] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97318353  0.23003004  0.75688944 -0.65354295 -2.84942246  3.26568589] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93263366 -0.3608247   0.99804846  0.06244422 -3.01243945  4.1830254 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.63493431 -0.77256613  0.72335744  0.69047376 -2.02686515  2.6468013 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.37755902 -0.92598552  0.39326712  0.91942426 -0.95408982  1.40099975] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.3158457  -0.94881057  0.26563489  0.9640737   0.29813654 -0.03894904] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.47500302 -0.87998416  0.38689374  0.92212431  1.4259263  -1.26971599] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7787965  -0.62727667  0.72874337  0.6847869   2.50998356 -2.92384419] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99593038 -0.09012592  0.99998344  0.0057554   3.23539978 -4.31746394] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86253449  0.50599827  0.70528343 -0.70892544  2.82307652 -3.33405745] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53981615  0.84178295  0.2895032  -0.95717705  1.82236792 -1.54822121] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.32894761  0.94434817  0.17478115 -0.98460731  0.50715319  0.32756476] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.36374697  0.93149779  0.4046868  -0.91445536 -0.88344495  2.08882976] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.61657618  0.78729525  0.81914557 -0.57358569 -2.00674986  3.32461828] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91736081  0.39805672  0.98572104  0.16838654 -2.84668136  4.24476098] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98741049 -0.15817876  0.60180648  0.79864195 -2.71165004  3.0775747 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80384364 -0.59484066  0.21851492  0.97583361 -1.99235337  1.13555822] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59980035 -0.8001497   0.1917691   0.98144007 -0.85719862 -0.85049708] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.55261031 -0.83343977  0.48009804  0.87721484  0.28902225 -2.20588654] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70292475 -0.71126422  0.90350061  0.4285868   1.62380122 -4.02504336] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92746199 -0.37391744  0.90372788 -0.42810737  2.32281631 -4.55136214] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99537916  0.09602258  0.27349321 -0.96187393  2.3778676  -3.77754044] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86541488  0.50105596 -0.29650054 -0.95503268  1.8609999  -1.99790638] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67752694  0.73549796 -0.51864253 -0.85499119  1.10415614 -0.44478943] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57548357  0.81781334 -0.47999124 -0.87727328  0.18245979  0.89214851] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.63262936  0.77445471 -0.16267351 -0.98667995 -0.90669643  2.50016794] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83357112  0.55241216  0.51395346 -0.85781807 -2.09581511  4.5781802 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99566442  0.09301808  0.99995008  0.00999206 -2.66495811  5.51799666] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92256975 -0.38583034  0.54108235  0.84096961 -2.09205472  4.10386976] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75051947 -0.66084834 -0.05370822  0.99855667 -1.14806389  2.17014864] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64732424 -0.76221475 -0.34708718  0.93783287 -0.28779052  0.84772046] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67605074 -0.73685507 -0.36421282  0.93131575  0.66211484 -0.66352238] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82458964 -0.56573133 -0.06282568  0.99802452  1.58511616 -2.45473119] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97910056 -0.20337675  0.58004116  0.81458717  2.34880594 -4.3618519 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95245677  0.30467375  0.99781148 -0.06612305  2.63959188 -5.49830583] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71887784  0.69513642  0.47532297 -0.87981139  1.81734604 -4.34482649] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53487287  0.84493255 -0.18534866 -0.98267282  0.55832818 -2.50928192] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.54952605  0.83547658 -0.47828974 -0.8782021  -0.70714655 -0.63258274] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73979863  0.67282835 -0.4265679  -0.90445554 -1.75405368  1.2167653 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94943036  0.3139777  -0.03287108 -0.9994596  -2.36700209  2.8549909 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98511384 -0.17190321  0.61464251 -0.7888058  -2.48323988  3.99848486] 1 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.79682627 -0.60420849  0.99895816 -0.04563539 -2.15743328  4.37461155] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.55424469 -0.83235379  0.74465954  0.66744451 -1.10262537  3.22686894] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.47688554 -0.8789654   0.34293753  0.93935821  0.20776356  1.6551403 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.61809758 -0.78610138  0.18357418  0.98300586  1.45530017 -0.02370679] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87219132 -0.48916491  0.3601037   0.93291228  2.41139113 -1.81635913] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99964632  0.02659394  0.78079531  0.62478691  2.86740748 -3.34641169] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84019528  0.54228396  0.99987562 -0.01577176  2.4632076  -3.28910658] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.56915872  0.82222768  0.85286786 -0.52212681  1.39009497 -1.93529105] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.42842677  0.90357651  0.69021766 -0.72360181  0.22218753 -0.65501371] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.49526308  0.86874304  0.68966411 -0.72412942 -0.9703559   0.65806761] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73388178  0.67927721  0.85721792 -0.51495383 -2.05253152  2.01854312] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.70156099e-01  2.42481222e-01  9.99995528e-01  2.99054093e-03\n",
      " -2.85983222e+00  3.24790474e+00] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95144609 -0.30781543  0.83500926  0.55023589 -2.58529134  2.32221339] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73435567 -0.67876487  0.64013554  0.76826199 -1.67114292  0.56450681] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57363874 -0.81910841  0.69087925  0.72297016 -0.43580936 -1.2253095 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59925084 -0.80056132  0.90970009  0.41526587  0.73867693 -2.53090809] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78628885 -0.61785908  0.97829806 -0.20720257  1.81390309 -3.69834335] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96782352 -0.25162996  0.59403152 -0.80444176  2.21405153 -3.37284929] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98016597  0.1981784   0.04359258 -0.99904939  2.25168827 -2.44811878] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82594947  0.56374416 -0.26727331 -0.96362077  1.66962471 -0.67052813] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66862378  0.74360086 -0.21683867 -0.97620745  0.66541242  1.20150834] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65623263  0.75455864  0.19018663 -0.98174796 -0.52603885  2.91283504] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80407289  0.59453073  0.7970831  -0.60386963 -1.619752    4.34937854] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96971076  0.2442561   0.95831889  0.28570074 -2.12755249  4.7271695 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98799299 -0.15449871  0.4356687   0.9001071  -1.80931928  3.41843959] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8956314  -0.44479702 -0.06729808  0.99773291 -1.2220926   1.75946816] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80622483 -0.59160927 -0.25197459  0.96773385 -0.47411947  0.11606524] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79076524 -0.61211954 -0.15825725  0.98739792  0.22762447 -1.07898484] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86180883 -0.50723323  0.20299092  0.97918062  1.03785937 -2.56676737] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97202195 -0.23489006  0.78270679  0.62239062  1.88164597 -4.34851378] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98762524  0.15683234  0.97210844 -0.23453184  1.91300423 -4.42911023] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88009644  0.47479497  0.49977496 -0.86615529  1.36738752 -3.50644266] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78464342  0.61994734  0.00727856 -0.99997351  0.3740657  -1.67463743] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79173184  0.6108688  -0.17804812 -0.98402178 -0.47342652 -0.19878483] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87898347  0.47685224 -0.09879789 -0.99510752 -1.09627841  0.99003985] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97273128  0.23193504  0.20959065 -0.97778922 -1.49161878  2.08612001] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99474965 -0.1023383   0.70052667 -0.71362622 -1.83249543  3.51202926] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89624806 -0.44355316  0.99930034 -0.0374009  -1.63716026  3.85187615] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74873592 -0.66286841  0.77378772  0.63344499 -0.92537972  3.20764955] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6907563  -0.72308764  0.35690304  0.93414143  0.10583129  1.9494519 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.779541   -0.62635121  0.1379472   0.99043958  1.17151511  0.31536615] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93847317 -0.34535215  0.25967008  0.96569739  2.00289471 -1.54064062] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99553117  0.09443351  0.67382296  0.73889283  2.39384509 -3.14942896] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8554507   0.51788425  0.98517055  0.17157795  1.99815339 -3.22683729] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64761139  0.76197079  0.91695573 -0.39898896  1.14874577 -2.46122127] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.55209015  0.83378443  0.71014232 -0.70405815  0.02802431 -1.21150244] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.63124751  0.77558145  0.60840829 -0.79362419 -1.00249353 -0.12191173] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82832405  0.56024929  0.68025238 -0.73297797 -1.88324549  1.06002968] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99063122  0.13656421  0.90304467 -0.42954665 -2.60432986  2.63217442] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93114639 -0.36464559  0.99610323  0.08819503 -2.37168884  2.46931439] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71097844 -0.7032138   0.87682318  0.48081298 -1.6163389   1.55123743] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5300733  -0.84795182  0.75122456  0.66004672 -0.6769634   0.62538805] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.50192525 -0.86491101  0.73090708  0.68247699  0.35120197 -0.32198923] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65074487 -0.75929646  0.84739555  0.53096213  1.45641269 -1.58690645] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89358881 -0.44888645  0.99532148  0.09661861  2.4384301  -2.93711611] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9977408   0.06718113  0.86197576 -0.50694949  2.75448348 -3.09330265] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84055394  0.54172786  0.52322263 -0.85219603  2.20271764 -1.68119942] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59815005  0.80138413  0.34652901 -0.93803926  1.3158948  -0.28707709] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.46700337  0.88425554  0.41444691 -0.91007349  0.22001165  0.99649109] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.54591289  0.83784194  0.7201252  -0.69384414 -1.13424415  2.76967718] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78566476  0.61865248  0.99358482 -0.11308935 -2.0586384   3.64255536] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97757803  0.2105735   0.82218607  0.56921882 -2.37758433  3.31684871] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96769197 -0.25213539  0.3931452   0.9194764  -2.20627885  2.1683664 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80264749 -0.5964537   0.12560683  0.9920801  -1.56640944  0.5833512 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65474422 -0.75585051  0.17301608  0.984919   -0.56720023 -1.0593797 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65710581 -0.75379835  0.52080065  0.85367833  0.60955415 -2.67414223] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81513578 -0.57926993  0.94419411  0.32938956  1.69835436 -4.11129755] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97631003 -0.21637637  0.85942222 -0.51126652  2.16243577 -4.3292914 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98211354  0.18828968  0.29592879 -0.95521     1.85232169 -2.9067089 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86573822  0.50049709 -0.16407123 -0.98644849  1.44870299 -1.7190748 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7416852   0.67074814 -0.33083116 -0.94368996  0.6268857  -0.00640656] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71113811  0.70305234 -0.21016847 -0.97766519 -0.19696452  1.26588033] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7989103   0.60145019  0.19615423 -0.98057306 -1.14659717  2.84454573] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94372055  0.33074389  0.78367936 -0.62116557 -1.88826128  4.1253947 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99542267 -0.09557039  0.96372185  0.26690857 -2.28319469  4.96002094] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88475593 -0.46605465  0.44150471  0.89725893 -1.52754381  3.33081578] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76740759 -0.64115957 -0.02508697  0.99968527 -0.57798902  1.51286406] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74891609 -0.66266484 -0.17820163  0.98399399  0.28960007  0.0351105 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82896597 -0.55929904 -0.06380653  0.99796229  0.99742688 -1.18610159] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94669162 -0.32214125  0.31374298  0.94950795  1.62582657 -2.63661516] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99917655  0.04057367  0.82943936  0.55859677  1.99979573 -3.86027407] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90825894  0.41840852  0.9772229  -0.21221548  1.78511297 -3.94540299] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7521542   0.65898714  0.60324797 -0.79755369  1.01733889 -3.01085897] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67947092  0.73370244  0.17425633 -0.98470033  0.01708743 -1.69801583] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75619133  0.65435058  0.0201166  -0.99979764 -1.08658449  0.14184143] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9110213   0.4123593   0.17928619 -0.98379696 -1.74282616  1.44441567] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99913987  0.04146703  0.55090618 -0.83456718 -2.02452798  2.51967305] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92941066 -0.3690472   0.93462001 -0.3556479  -2.08862995  3.5708938 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73901079 -0.67369359  0.94853321  0.31667767 -1.42665964  3.09057947] 2 -1.0 True {}\n",
      "state, action, reward, done, info: [ 0.99976665 -0.02160199  0.99593943  0.09002589  0.04222117 -0.03084172] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9997466  -0.02251073  0.99423362  0.10723577 -0.05042362  0.20035448] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99990189 -0.01400722  0.99515346  0.09833405  0.13320904 -0.28650983] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999897  0.00143424  0.99776586  0.06680785  0.01724195 -0.02227393] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99997199 -0.00748453  0.99593688  0.09005403 -0.10369441  0.24967851] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99993664 -0.01125722  0.99562904  0.09339605  0.06656288 -0.21673706] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99996012 -0.0089303   0.99713542  0.07563696 -0.04416957  0.04165345] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99988938 -0.01487376  0.99719932  0.07478975 -0.01413778 -0.05082022] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999927 -0.00121182  0.99976631  0.02161764  0.14615188 -0.46994413] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99964231  0.02674425  0.99751786 -0.07041385  0.12489845 -0.43077506] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99947658  0.0323506   0.99408006 -0.10865001 -0.07072697  0.05354177] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99991618  0.01294703  0.99640494 -0.08471837 -0.11862846  0.17968852] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99991939 -0.01269701  0.99916094 -0.04095617 -0.13125057  0.24730217] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99972396 -0.02349458  0.99967986 -0.02530162  0.02570701 -0.09461994] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99986713 -0.01630081  0.99907753 -0.04294275  0.04423079 -0.07774548] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99997735 -0.00673021  0.9985013  -0.05472806  0.04918435 -0.03677199] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999696  0.00246446  0.99840309 -0.05649137  0.04086363  0.02078266] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999087 -0.00427359  0.9999289  -0.01192471 -0.10529772  0.41607421] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99994626 -0.01036714  0.99941352  0.0342435   0.04678879  0.03724134] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999855e-01  5.39322924e-04  9.99306188e-01  3.72443520e-02\n",
      "  6.01337448e-02 -6.00278467e-03] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99991961  0.01267929  0.99946574  0.03268372  0.05865975 -0.03695556] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99934476  0.03619462  0.99994428 -0.01055631  0.17027944 -0.38406661] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99796783  0.06371985  0.99686814 -0.07908165  0.09771446 -0.2861336 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99821799  0.0596728   0.9963433  -0.08544024 -0.13727863  0.22215377] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99927419  0.03809322  0.99798451 -0.06345799 -0.07382654 -0.00862178] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99983538  0.01814444  0.99845518 -0.05556306 -0.12138741  0.08316878] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99976877 -0.02150358  0.99999907  0.00136725 -0.26511002  0.47009786] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99761438 -0.06903293  0.99610765  0.088145   -0.19847037  0.37667241] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99397135 -0.10964016  0.98412467  0.17747855 -0.1980856   0.50296054] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99147589 -0.1302903   0.96989303  0.24353132 -0.0041992   0.15926486] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99230208 -0.12384093  0.96275233  0.27038483  0.06867973  0.1149943 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99586223 -0.09087587  0.96703871  0.25462942  0.25550787 -0.2703402 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99920741 -0.03980638  0.97881212  0.20476044  0.24445362 -0.22559838] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999766  0.00216372  0.98498627  0.17263269  0.16592638 -0.08957701] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [0.99972254 0.02355522 0.98493011 0.17295285 0.0438381  0.09587276] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99981583  0.01919118  0.977794    0.20956835 -0.08528166  0.26985225] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99984326  0.01770446  0.97821317  0.20760297  0.0704293  -0.28936113] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99981335  0.01932     0.98591056  0.16727334 -0.05571951 -0.11423235] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999206 -0.00398492  0.98652465  0.16361272 -0.17257621  0.07381826] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99891018 -0.04667388  0.98116895  0.1931515  -0.24478849  0.21439647] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99526732 -0.09717494  0.97037268  0.24161305 -0.24982657  0.26591191] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99331374 -0.11544613  0.97450448  0.22436803  0.06910293 -0.44253115] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99640942 -0.08466566  0.99423525  0.10722062  0.23032766 -0.72119132] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99987279 -0.01594974  0.99651487 -0.08341525  0.43719038 -1.14330333] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99857379  0.05338892  0.96645618 -0.25683156  0.23804076 -0.58036256] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99498405  0.10003368  0.93133318 -0.36416823  0.21722505 -0.52400302] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9908179   0.13520317  0.89609525 -0.4438618   0.12793995 -0.32936515] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99239011  0.12313355  0.90762604 -0.41977967 -0.24666477  0.59084238] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99852742  0.05424938  0.96488562 -0.2626704  -0.42735591  1.0476027 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.98680293e-01 -5.13582762e-02  9.99999603e-01  8.91370368e-04\n",
      " -5.99096025e-01  1.55636618e+00] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98548059 -0.16978813  0.94742781  0.31996959 -0.55642901  1.61310661] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96944181 -0.24532136  0.83421675  0.55143668 -0.1969538   0.92611237] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96992018 -0.24342318  0.77401347  0.63316913  0.21493109  0.08078693] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98426183 -0.17671632  0.79622838  0.60499617  0.45297198 -0.42571946] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99818847 -0.06016456  0.88121929  0.47270769  0.6963533  -1.11259143] 2 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.99716871  0.07519677  0.96666139  0.25605811  0.62449383 -1.16153149] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98211026  0.18830677  0.99983144  0.01835989  0.48664107 -1.18455288] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97002752  0.2429951   0.98762261 -0.15684891  0.05745984 -0.53683618] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97218171  0.23422791  0.96728908 -0.25367664 -0.1491619  -0.43727548] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98490467  0.17309763  0.9570454  -0.28993811 -0.46344838  0.05729794] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99827709  0.05867587  0.97144358 -0.23727067 -0.66410411  0.46243097] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99569868 -0.09265066  0.99598499 -0.0895204  -0.81556743  0.9873861 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96851229 -0.24896574  0.99145051  0.1304833  -0.73376792  1.15374168] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94098753 -0.33844125  0.96137543  0.27524042 -0.18179598  0.29213867] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94039604 -0.34008129  0.9523645   0.30496206  0.16528749  0.01364365] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96415532 -0.26533851  0.96846513  0.24914914  0.60231483 -0.57529413] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9915706  -0.12956752  0.99184048  0.12748512  0.75249929 -0.62267745] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99898883  0.04495912  0.99887829 -0.04735149  0.95522467 -1.07013507] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97583166  0.21852361  0.97182211 -0.23571549  0.75728917 -0.77638118] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9489563   0.31540759  0.95383797 -0.30032169  0.22809818  0.12721306] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94466885  0.32802557  0.96793795 -0.25118941 -0.09538661  0.37653548] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96671272  0.25586424  0.99575283 -0.09206686 -0.63996986  1.20385417] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99452602  0.10448923  0.98573174  0.16832393 -0.86089061  1.33834446] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99672721 -0.08083852  0.90215739  0.43140704 -0.94890356  1.35235397] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97032451 -0.24180642  0.79700097  0.60397802 -0.6497758   0.62528576] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94990971 -0.31252446  0.79456108  0.60718423 -0.07115027 -0.59324276] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96364401 -0.2671895   0.91551145  0.40229191  0.53542935 -1.76097471] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99183182 -0.12755247  0.99997235  0.00743699  0.84561309 -2.20929522] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99827988  0.0586283   0.89735814 -0.44130304  0.96149909 -2.31449854] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97775918  0.20973075  0.68785785 -0.72584543  0.53774338 -1.18727615] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95828624  0.28581022  0.55578874 -0.83132357  0.23439807 -0.48365361] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95795985  0.28690228  0.56362586 -0.8260302  -0.22338085  0.5777331 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98181358  0.18984754  0.74954451 -0.66195395 -0.76456938  1.88834679] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99978307  0.02082808  0.95482728 -0.29716135 -0.89175659  2.23757381] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98560407 -0.16906986  0.98121684  0.19290803 -0.95309023  2.59042837] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95249985 -0.30453904  0.81694741  0.57671218 -0.40743798  1.53865319] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94704579 -0.32109855  0.70089214  0.71326728  0.23149186  0.24418058] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97290977 -0.23118515  0.73540327  0.67762971  0.68497447 -0.72476816] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99866793 -0.05159806  0.88507666  0.46544528  1.09602295 -1.82961399] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98515669  0.17165753  0.99685627  0.07923114  1.08724917 -2.11070881] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93167983  0.36328046  0.94046572 -0.33988854  0.84903462 -2.03986361] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89227343  0.45149543  0.8052971  -0.59287148  0.0988616  -0.79829278] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90718492  0.42073212  0.74981567 -0.66164678 -0.43478556 -0.07845484] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96180169  0.27374717  0.82262029 -0.56859112 -1.1043687   1.23559979] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99974011  0.02279741  0.96032    -0.2789005  -1.38327837  1.90002378] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97242952 -0.23319697  0.99717302  0.07513968 -1.13332681  1.56721083] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90724227 -0.42060844  0.93076382  0.365621   -0.80693614  1.34513579] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86882363 -0.49512171  0.86621594  0.49966984 -0.01654776  0.12162402] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89356982 -0.44892424  0.88355491  0.46832757  0.53054625 -0.47054235] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9586709  -0.28451732  0.96183333  0.27363596  1.20348377 -1.58394286] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99946828 -0.03260615  0.99903361 -0.04395274  1.29282439 -1.52637497] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97947774  0.20155235  0.95747226 -0.28852535  1.00829248 -0.88183096] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92922766  0.36950771  0.90455783 -0.42635095  0.70983934 -0.5506037 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88925536  0.45741108  0.87761658 -0.47936326  0.23807819 -0.02773818] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90271391  0.43024132  0.92734652 -0.37420374 -0.5345561   1.17469345] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96003336  0.2798856   0.99672401 -0.08087795 -1.0364848   1.7797312 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99866187  0.05171532  0.95843435  0.28531316 -1.22309368  1.81620397] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9836588  -0.18004268  0.82348377  0.56733983 -1.05206788  1.24150383] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9389906  -0.3439428   0.72344249  0.69038465 -0.61529531  0.311283  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91905006 -0.39414082  0.77171321  0.6359707   0.08800191 -1.03892943] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94239656 -0.33449771  0.91364898  0.40650405  0.53441116 -1.6223586 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98189574 -0.18942219  0.99945976  0.03286636  0.92676053 -2.14530794] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99997427  0.00717305  0.92334809 -0.38396394  0.99451587 -2.01280442] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98262909  0.18558036  0.74302707 -0.66926136  0.7616936  -1.31128121] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95533487  0.29552546  0.6209517  -0.78384883  0.35179269 -0.34081597] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.952174    0.30555633  0.67147154 -0.74103034 -0.25191177  1.00267234] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97481902  0.22299748  0.8427351  -0.53832848 -0.58568616  1.61859231] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99806567  0.06216844  0.98956781 -0.14406787 -0.99785565  2.53127959] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99081502 -0.13522424  0.93683915  0.34976049 -0.91690268  2.35530622] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9602219  -0.27923809  0.72932202  0.68417059 -0.52327689  1.53812015] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94356504 -0.33118728  0.57572958  0.81764018 -0.016904    0.48437706] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95797714 -0.28684456  0.58514227  0.81093065  0.47288899 -0.59346139] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98615546 -0.16582342  0.72692341  0.68671855  0.74644662 -1.26214668] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99979836  0.02008092  0.92293333  0.38495983  1.08273543 -2.29087993] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97332773  0.22941912  0.99654468 -0.08305847  0.96848753 -2.36524497] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93220553  0.36192935  0.89256715 -0.45091449  0.38300712 -1.40263292] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.925038    0.37987458  0.79073253 -0.6121618  -0.19399435 -0.48438141] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95653721  0.29161031  0.79061129 -0.61231837 -0.72404039  0.47200682] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99374686  0.11165654  0.886703   -0.46233948 -1.07729879  1.265127  ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99174123 -0.12825492  0.98924708 -0.14625394 -1.2712011   1.98150328] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94142455 -0.33722368  0.98061281  0.19595539 -0.82836459  1.36296983] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89166785 -0.45269022  0.90552372  0.42429565 -0.39987415  0.99363423] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8795136  -0.47587375  0.83680575  0.5474999   0.14529514  0.39692116] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91590707 -0.40139039  0.82912326  0.55906585  0.66893691 -0.25322036] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97165795 -0.23639125  0.88444282  0.46664857  1.03925652 -0.78941794] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99988166 -0.01538426  0.95579571  0.29403156  1.1431806  -1.01590836] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97516682  0.22147159  0.99916124  0.04094892  1.18778311 -1.47445305] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91173145  0.41078676  0.97592024 -0.2181277   0.76600604 -1.06060955] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8698932   0.49324012  0.93528461 -0.35389644  0.13984179 -0.32847467] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89439285  0.44728227  0.9514711  -0.3077381  -0.65014491  0.80576736] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96473135  0.26323642  0.99855101 -0.05381336 -1.27873679  1.71536612] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99974413 -0.02262012  0.94558122  0.32538617 -1.53828257  2.01239563] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95362428 -0.30099955  0.78538675  0.61900538 -1.23170432  1.26075799] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87978487 -0.47537205  0.68728688  0.72638609 -0.62968325  0.16737183] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85360124 -0.52092698  0.74505936  0.66699816  0.11451011 -0.99153496] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89460979 -0.44684822  0.89763257  0.44074456  0.71206661 -1.70306368] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96177037 -0.27385717  0.99711385  0.07592086  1.09712367 -2.00932993] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99938276 -0.03512984  0.94386083 -0.33034337  1.26159867 -1.99938958] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9821066   0.18832585  0.79799428 -0.60266503  0.93699806 -1.03169718] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93992004  0.34139466  0.70121779 -0.71294713  0.62039991 -0.40208904] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91478145  0.40394914  0.71899479 -0.69501546  0.0400282   0.65837146] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93488042  0.35496282  0.86077469 -0.50898618 -0.5593414   1.65723304] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98209686  0.18837666  0.99559758 -0.0937308  -1.12740723  2.64959889] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99942637 -0.03386645  0.92059546  0.39051759 -1.04153914  2.17115324] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97624252 -0.21668074  0.72015411  0.69381414 -0.76456138  1.41542125] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95018144 -0.31169735  0.60574926  0.7956556  -0.2061539   0.10196874] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9561082  -0.29301384  0.69289623  0.72103732  0.40287427 -1.24472442] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98309277 -0.18310818  0.88012295  0.47474582  0.70321701 -1.80866307] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99983474 -0.01817939  0.99690837  0.07857294  0.90839561 -2.25308906] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98575187  0.168206    0.92398246 -0.38243484  0.90359406 -2.33386333] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95251189  0.30450138  0.7118081  -0.702374    0.4699224  -1.46671454] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94287119  0.33315749  0.5930732  -0.80514855 -0.16657675 -0.09908069] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96702593  0.25467793  0.66011883 -0.75116119 -0.63933597  0.94602759] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99671955  0.08093287  0.8593226  -0.51143394 -1.0941848   2.13691814] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98907349 -0.14742332  0.99801678 -0.06294839 -1.13292388  2.47930199] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94126201 -0.3376771   0.92126955  0.3889247  -0.77462655  2.02980772] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89909807 -0.43774727  0.73371593  0.67945635 -0.28471229  1.38758114] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90862487 -0.41761328  0.63709304  0.77078691  0.49736157 -0.05964997] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96574248 -0.25950235  0.74746438  0.66430189  1.15475684 -1.4511326 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 9.99999071e-01  1.36300670e-03  9.38552991e-01  3.45135167e-01\n",
      "  1.42620008e+00 -2.20098578e+00] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96121296  0.27580728  0.9946219  -0.10357261  1.28109672 -2.22504933] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88404373  0.4674042   0.88370455 -0.46804516  0.73905583 -1.52094042] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83955019  0.54328214  0.75039076 -0.66099448  0.12251907 -0.79498453] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87285271  0.48798376  0.74161219 -0.67082886 -0.75518239  0.66041173] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96031708  0.27891057  0.89385358 -0.44835898 -1.47396982  1.99663277] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9985147  -0.05448288  0.99967038  0.02567337 -1.81344755  2.76201438] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92966459 -0.36840704  0.87756414  0.47945926 -1.33777758  1.8510172 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82987211 -0.55795365  0.69817173  0.71593032 -0.76803717  1.07672863] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79544038 -0.60603185  0.652827    0.75750703  0.18457555 -0.46387765] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86215484 -0.50664487  0.79894633  0.60140233  0.99443836 -1.65712856] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96090736 -0.27687007  0.96903126  0.24693809  1.45628897 -2.20860621] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99976592  0.02163559  0.98261199 -0.1856709   1.48764942 -2.01731778] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95229224  0.30518764  0.84037532 -0.54200491  1.32883774 -1.73248346] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86878641  0.49518702  0.685671   -0.72791159  0.71290505 -0.65281591] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83992812  0.54269767  0.69930612 -0.71482233 -0.16770933  0.84243171] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88880824  0.4582793   0.85447884 -0.5194862  -0.78805225  1.62294569] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96308038  0.26921402  0.9853581  -0.17049755 -1.19646752  2.03768989] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99999382 -0.00351598  0.95794045  0.28696707 -1.48946112  2.44701036] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96139306 -0.27517884  0.75417194  0.656677   -1.20194521  1.7062629 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88845936 -0.4589553   0.55858064  0.82945022 -0.74253224  0.87045636] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85585966 -0.51720812  0.54196803  0.8403991   0.08733621 -0.67453233] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90458186 -0.42629996  0.75904606  0.65103692  0.93440982 -2.20001341] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98344683 -0.18119696  0.99042454  0.13805515  1.5832605  -3.38385312] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9902459   0.13933076  0.86239382 -0.50623798  1.53737562 -3.11092894] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91605114  0.40106147  0.47396155 -0.88054554  1.13782296 -2.26296834] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8411146   0.54085693  0.1770686  -0.98419851  0.43484167 -0.88874596] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82938237  0.55868138  0.11234043 -0.99366978 -0.22233998  0.23465249] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88892676  0.45804936  0.29307685 -0.95608889 -0.93386933  1.61180182] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97658702  0.21512272  0.70526314 -0.70894562 -1.62709296  3.21618001] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99033582 -0.13869017  0.99832979 -0.05777228 -1.83136109  3.88683603] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88798903 -0.45986463  0.76673102  0.6419685  -1.44780872  3.44860975] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77804207 -0.62821218  0.32213024  0.94669536 -0.54329494  1.9793174 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76274607 -0.6466981   0.05633008  0.9984122   0.2967966   0.73617865] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84758214 -0.53066422  0.05764793  0.99833697  1.11022138 -0.74364139] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95979927 -0.28068729  0.31818251  0.94802948  1.59150292 -1.89092994] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9969998   0.07740409  0.75356635  0.65737186  1.97457405 -3.33879071] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90116407  0.43347817  0.99939449  0.03479434  1.63237497 -3.27949272] 1 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.76193899  0.64764881  0.85278231 -0.52226654  0.86137464 -2.4317617 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70479502  0.70941101  0.59037258 -0.80713086 -0.03847653 -1.42242806] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77193912  0.63569646  0.44632133 -0.89487277 -0.94167004 -0.25209081] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91900621  0.39424306  0.55590719 -0.83124436 -1.84207733  1.50673921] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99977072 -0.02141291  0.8665479  -0.49909391 -2.34347267  2.98152167] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89435554 -0.44735687  0.99745495  0.07129949 -1.96813221  2.74450571] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69949381 -0.71463866  0.84105754  0.54094566 -1.28543807  2.14099528] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59261949 -0.80548255  0.66696283  0.74509099 -0.10440562  0.54033995] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66917732 -0.74310276  0.70652056  0.70769252  1.08067018 -1.08935823] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85821508 -0.51329025  0.89098029  0.45404198  1.85741884 -2.01205483] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99227596 -0.12405009  0.99976134  0.02184649  2.19341923 -2.32981471] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9530873   0.30269554  0.91275403 -0.40850959  2.02221855 -1.93201326] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80421748  0.59433513  0.78401097 -0.62074696  1.20692328 -0.5037346 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69775045  0.71634092  0.77801142 -0.62825014  0.39183893  0.40634916] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70405559  0.71014486  0.87175496 -0.48994213 -0.47458947  1.24428773] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82339511  0.5674685   0.9869148  -0.16124259 -1.35087758  2.19426204] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96276269  0.27034792  0.94850212  0.31677077 -1.86629504  2.50194741] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99553397 -0.09440401  0.73751648  0.67532914 -1.73777663  1.56847907] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91946728 -0.39316654  0.58511309  0.8109517  -1.29457139  0.42047656] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83693844 -0.54729704  0.64631787  0.76306829 -0.41649344 -1.20460798] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83304039 -0.55321217  0.86228461  0.50642398  0.33848846 -2.12162709] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90954654 -0.41560209  0.9998106  -0.0194621   1.18343952 -3.26618636] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98731879 -0.15875016  0.78309832 -0.62189791  1.42831979 -3.08380472] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99432937  0.10634423  0.38678334 -0.92217062  1.19476269 -1.88338352] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94752285  0.31968806  0.10620546 -0.99434421  0.95805994 -0.99018244] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88882143  0.45825371  0.01063846 -0.99994341  0.51872679  0.04840501] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87098372  0.49131187  0.15508832 -0.98790061 -0.16215633  1.41249076] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91855496  0.3952933   0.54724822 -0.83697036 -0.90899527  2.81758852] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98530269  0.17081745  0.94402818 -0.32986481 -1.37394297  3.61425595] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99171056 -0.12849186  0.90481169  0.42581194 -1.5128717   3.90303602] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92880794 -0.37056148  0.44494064  0.89556006 -0.9449801   2.70330557] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87337471 -0.48704889  0.03368766  0.99943241 -0.3385185   1.56312912] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87383749 -0.4862181  -0.14104499  0.99000319  0.33421756  0.20030706] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92451988 -0.38113383 -0.07430216  0.99723577  0.80950332 -0.860409  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98434571 -0.17624849  0.24514247  0.96948706  1.30470547 -2.35318964] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99512074  0.09866467  0.71834638  0.69568562  1.41097417 -3.11163411] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92578699  0.37804556  0.9981167   0.06134372  1.39578722 -3.79826861] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83136394  0.55572835  0.82783229 -0.56097566  0.54953153 -2.62361215] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81525452  0.57910281  0.52364293 -0.85193784 -0.27632356 -1.5713414 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88537926  0.46486941  0.3477502  -0.93758722 -1.03320156 -0.38623755] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9791854   0.20296784  0.43135654 -0.90218154 -1.69461937  1.26178574] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9878294  -0.15554121  0.72346628 -0.69035972 -1.8407233   2.27872543] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88271866 -0.46990188  0.95680014 -0.29074643 -1.41406491  2.24996471] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75111649 -0.66016969  0.9882464   0.15286943 -0.85022728  2.12749577] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70661129 -0.70760192  0.89880318  0.43835241  0.21561882  0.83352852] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8044027  -0.59408442  0.88744872  0.46090647  1.26141142 -0.58045851] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94796158 -0.31838475  0.95563444  0.29455529  1.79950881 -1.15955123] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99690859  0.07857018  0.99988484 -0.01517576  2.13369754 -1.8511679 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88935236  0.45722247  0.94110146 -0.33812429  1.74015546 -1.32232198] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74276196  0.66955557  0.88804778 -0.45975117  0.80389442  0.02471318] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.68761321  0.72607719  0.92087111 -0.38986716 -0.02100946  0.73588869] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76422917  0.64494478  0.99234224 -0.12351871 -1.07285976  1.98716983] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9202251   0.39138954  0.93601008  0.35197319 -1.8439282   2.71061623] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99993574  0.01133681  0.65295691  0.75739506 -1.98079694  2.14161664] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93261357 -0.36087662  0.36243259  0.93200999 -1.74976345  1.18412775] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78945317 -0.61381079  0.25436477  0.96710835 -1.110482   -0.06748742] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69985841 -0.7142816   0.3888056   0.92131982 -0.20984982 -1.34602985] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73819185 -0.67459083  0.71015029  0.70405011  0.75280635 -2.52924964] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87395527 -0.48600636  0.98317549  0.18266348  1.51322171 -3.32724595] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98931691 -0.14578081  0.85593027 -0.51709126  1.9876374  -3.7162421 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97517666  0.2214283   0.39945371 -0.91675336  1.64916558 -2.35614142] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87041571  0.49231747  0.04849383 -0.99882348  1.22030365 -1.23645464] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76782231  0.64066286 -0.07745064 -0.99699619  0.55470352 -0.01784522] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75277137  0.65828206  0.06730772 -0.99773226 -0.33691921  1.47257682] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84802495  0.52995631  0.49294165 -0.87006237 -1.25558371  3.01238348] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97685387  0.21390771  0.96142031 -0.27508359 -2.10174128  4.62837037] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97790549 -0.20904748  0.79934698  0.60086971 -2.00626692  4.28956715] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86042418 -0.50957849  0.23824345  0.97120547 -1.19924022  2.52782101] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76329368 -0.64605166 -0.13833801  0.99038507 -0.46407918  1.26812451] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75823118 -0.6519858  -0.2430896   0.97000384  0.38018117 -0.19453247] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.85371687 -0.52073747 -0.03728944  0.99930451  1.22503621 -1.89583646] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97154987 -0.23683507  0.47133268  0.8819555   1.8289141  -3.37252569] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98859005  0.15063108  0.94996968  0.31234213  1.98100358 -4.07387625] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87054841  0.49208278  0.88826103 -0.45933903  1.53037868 -3.62845384] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7532271   0.65776055  0.50696166 -0.86196861  0.47531218 -1.94582545] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76202555  0.64754696  0.31149324 -0.95024837 -0.59465715 -0.2061589 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87734215  0.47986535  0.40909596 -0.91249137 -1.40802327  1.246803  ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98892324  0.14842785  0.74235729 -0.67000422 -2.05105105  2.85496505] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95865524 -0.28457007  0.99852882 -0.05422359 -2.21690336  3.74075519] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77355216 -0.63373263  0.79066211  0.61225275 -1.65717826  3.19236411] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.60237581 -0.79821262  0.38132012  0.92444306 -0.68983001  1.97935938] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58350754 -0.81210772  0.14908661  0.98882414  0.44937992  0.43723483] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72438583 -0.68939478  0.1924301   0.98131068  1.39319563 -0.88751849] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91811922 -0.3963043   0.48442039  0.87483535  2.08271016 -2.2112623 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99905174  0.04353876  0.86815349  0.49629579  2.33717222 -3.11256161] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88606518  0.46356067  0.99528531 -0.09699049  1.93456177 -2.83452713] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69305786  0.72088196  0.81933587 -0.57331382  1.2284436  -2.18539572] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58600292  0.81030894  0.60848143 -0.79356811  0.15150689 -0.85927015] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64875949  0.76099351  0.58097597 -0.81392071 -0.93929296  0.52595646] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83108017  0.55615263  0.74228034 -0.67008947 -1.77120936  1.62218813] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98581138  0.16785686  0.95769407 -0.28778825 -2.35369642  2.68453109] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95745604 -0.28857917  0.97404615  0.22634949 -2.14465903  2.31709   ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78889643 -0.61452618  0.82337391  0.56749925 -1.47349542  1.33982028] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65051007 -0.75949763  0.73365989  0.67951686 -0.50660994  0.08736191] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.64380018 -0.76519365  0.78292724  0.62211328  0.4178954  -0.83443621] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78052356 -0.62512636  0.93800321  0.34662657  1.51375292 -2.30598707] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95848653 -0.28513782  0.97718799 -0.21237615  2.25476124 -3.20142022] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98609285  0.16619533  0.69477901 -0.71922328  2.20267764 -2.50342257] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84297076  0.53795938  0.36607613 -0.93058491  1.73602416 -1.36281714] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.68416301  0.72932913  0.28308486 -0.95909486  0.71359338  0.48072087] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6591061   0.75204997  0.51166943 -0.8591824  -0.38812338  2.01233646] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78766784  0.61610013  0.88490582 -0.46577    -1.45415517  3.42879624] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95940757  0.28202326  0.94998814  0.31228599 -2.20430155  4.34832735] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99027669 -0.13911173  0.47266508  0.88124215 -1.96749858  3.10519244] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88698002 -0.46180781  0.03650643  0.99933342 -1.39484899  1.43374638] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77152395 -0.63620028 -0.10288815  0.99469293 -0.6633608  -0.04337898] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74292512 -0.66937453  0.05582745  0.99844043  0.24453008 -1.55283464] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82641449 -0.56306224  0.47517022  0.87989389  1.0986748  -2.83414324] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96063242 -0.27782252  0.94525978  0.32631877  2.00535759 -4.49504326] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98878518  0.14934479  0.81695711 -0.57669843  2.14525677 -4.6393804 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86527682  0.50129435  0.14688575 -0.98915346  1.54642285 -3.36026928] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73772853  0.67509748 -0.3322877  -0.94317808  0.60943633 -1.53571413] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71915555  0.69484911 -0.45801287 -0.88894556 -0.33346519  0.15498019] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81375198  0.58121229 -0.294838   -0.95554725 -1.12525576  1.6135912 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94632196  0.32322553  0.17184676 -0.98512369 -1.75600264  3.11133502] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99767585 -0.06813889  0.8067692  -0.5908667  -2.15998399  4.46109141] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88264111 -0.47004753  0.95666258  0.29119876 -1.8998593   4.50011701] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72276925 -0.69108944  0.51368328  0.85797989 -0.78299323  2.76665049] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69721308 -0.71686395  0.17084681  0.9852976   0.40509616  0.92067565] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81823409 -0.57488519  0.16870671  0.9856663   1.42321399 -0.8986968 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96895435 -0.24723968  0.50559948  0.86276831  2.14795422 -2.68582226] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98084606  0.19478451  0.91408045  0.4055329   2.21585098 -3.38144166] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83068428  0.55674376  0.97560968 -0.21951252  1.62453966 -2.80164812] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66739758  0.7447016   0.75723954 -0.65313725  0.82045818 -2.01628703] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.62489418  0.78070946  0.55704772 -0.83048048 -0.26837414 -0.6514184 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73377744  0.67938992  0.54304964 -0.8397006  -1.19969625  0.49382637] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91333192  0.40721591  0.72980737 -0.68365284 -2.02147741  1.91833649] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9990419  -0.04376405  0.97411614 -0.22604811 -2.5103417   3.17614697] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86114037 -0.50836724  0.91189938  0.41041385 -2.25973046  3.09737897] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.62490931 -0.78069735  0.61232864  0.79060334 -1.31139537  1.73277039] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.51002622 -0.86015886  0.46703908  0.88423667 -0.07613997  0.01026589] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.60142329 -0.79893055  0.60847925  0.79356978  1.16943749 -1.70157131] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83197219 -0.55481734  0.9070137   0.42110112  2.15527735 -3.06556659] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9962917  -0.08603979  0.96003966 -0.27986399  2.73327665 -3.86004274] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90246086  0.43077187  0.54943486 -0.83553655  2.46551882 -2.99242065] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66246164  0.74909584  0.17998571 -0.98366922  1.49749318 -1.01802914] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5080506   0.86132722  0.13958332 -0.99021033  0.38762262  0.58996814] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53544267  0.84457158  0.37638394 -0.92646378 -0.70835114  1.86278143] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74081202  0.6717124   0.81022309 -0.58612161 -1.9620181   3.71740638] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96234175  0.27184254  0.9820834   0.18844681 -2.52090988  4.183359  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97719328 -0.21235182  0.60018106  0.79986417 -2.26519474  2.99351434] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8188545  -0.57400115  0.19295495  0.98120762 -1.64915053  1.4630577 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66457396 -0.74722249  0.08663048  0.99624051 -0.63640069 -0.38125185] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6420858  -0.76663278  0.28880045  0.95738932  0.34953739 -1.67497842] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77574269 -0.63104935  0.72640967  0.68726195  1.54654807 -3.52589804] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95871114 -0.28438171  0.99764113 -0.06864529  2.2826127  -4.51270821] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98381194  0.17920398  0.5816401  -0.81344624  2.27144533 -4.03432239] 0 -1.0 False {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.83298613  0.55329387 -0.05486805 -0.99849361  1.74045516 -2.68279691] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65788628  0.75311729 -0.41696203 -0.9089239   0.89547937 -1.08712538] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5874746   0.8092426  -0.49379173 -0.8695802  -0.00704443  0.21945108] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.66047685  0.75084641 -0.33509575 -0.94218408 -0.91858081  1.53511517] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83942421  0.54347677  0.12684836 -0.99192212 -1.80730494  3.177685  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98885506  0.14888139  0.78544023 -0.61893752 -2.38018365  4.50036435] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93801306 -0.34659992  0.94658553  0.32245284 -2.47820065  5.089886  ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73123284 -0.68212795  0.3902772   0.9206974  -1.42139066  3.22079782] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.60889531 -0.79325059 -0.05040378  0.99872892 -0.23559514  1.32215303] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65451443 -0.75604951 -0.15782579  0.98746697  0.80722791 -0.24072896] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82281753 -0.56830566  0.04845586  0.99882532  1.68316961 -1.83603018] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97958672 -0.20102201  0.54432986  0.83887126  2.28430509 -3.40437929] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96114885  0.27603058  0.97969032  0.20051653  2.42452391 -4.31127217] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75102357  0.6602754   0.80248758 -0.59666882  1.86157131 -3.84166928] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.56559441  0.82468355  0.34278438 -0.93941411  0.60403693 -1.96761367] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.57071781  0.82114626  0.13795998 -0.9904378  -0.65328259 -0.15090724] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75339501  0.65756821  0.29002887 -0.9570179  -1.77174872  1.72828373] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9601901   0.2793474   0.72049741 -0.69345763 -2.50949064  3.32685394] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97260131 -0.23247946  0.99926659 -0.03829219 -2.53491373  3.69569573] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.7810793  -0.62443184  0.82562265  0.56422269 -1.76357311  2.50493414] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.60819024 -0.79379131  0.59420369  0.8043146  -0.63549852  0.83419035] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58493623 -0.81107929  0.54539971  0.83817609  0.34864365 -0.24164753] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71048288 -0.70371448  0.67001444  0.74234807  1.28484371 -1.3299093 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90708047 -0.42095726  0.905609    0.42411359  2.11841279 -2.593845  ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99984958  0.01734406  0.99387887 -0.11047534  2.28729766 -2.69448096] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90744246  0.42017638  0.84837464 -0.52939633  1.78324439 -1.63842808] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76297483  0.64642819  0.74144967 -0.67100849  0.86439116 -0.11288666] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71931227  0.69468688  0.82406812 -0.56649071 -0.2239819   1.43399654] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80074772  0.59900174  0.97128828 -0.2379056  -1.00194073  2.12450015] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93571317  0.35276176  0.96143069  0.27504733 -1.74256496  2.92230431] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.999839   -0.01794382  0.65575375  0.75497485 -1.95370027  2.68164206] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.92517868 -0.37953182  0.26718019  0.96364659 -1.69414137  1.68994577] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78251952 -0.62262605  0.05567331  0.99844904 -1.08507903  0.44055549] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69144282 -0.72243119  0.09609369  0.99537229 -0.24039299 -0.84460884] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72331424 -0.69051901  0.3826778   0.92388187  0.69265468 -2.1158858 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.8657844  -0.5004172   0.82359069  0.5671846   1.65576217 -3.59282775] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99141979 -0.13071652  0.97927526 -0.20253386  2.14858291 -4.22663488] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9580342   0.28665391  0.53788114 -0.84302069  1.96219679 -3.55574903] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81541345  0.578879    0.01018079 -0.99994817  1.27048612 -2.0092786 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.69888036  0.7152386  -0.25956744 -0.96572498  0.50477192 -0.72326286] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6921459   0.72175761 -0.25483252 -0.9669852  -0.41292212  0.77222654] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.80539801  0.59273438  0.04943159 -0.99877751 -1.29159185  2.31230711] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95585728  0.29383135  0.62260149 -0.78253906 -2.03653314  3.89585889] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99040758 -0.13817677  0.99913934 -0.04147989 -2.20206698  4.41777233] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86733473 -0.49772529  0.73729038  0.67557598 -1.51576705  3.2094487 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74221173 -0.67016546  0.3260416   0.94535542 -0.59553647  1.73756262] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72698383 -0.68665458  0.13337785  0.99106526  0.3643839   0.24896363] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83303045 -0.55322715  0.25584762  0.96671712  1.31559621 -1.50210202] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97033947 -0.2417464   0.67050994  0.74190054  2.06144648 -3.23306176] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98053656  0.19633658  0.99573158  0.09229642  2.24377189 -3.98419627] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82639703  0.56308787  0.79879518 -0.60160307  1.65279431 -3.18544938] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.67312613  0.73952769  0.41799903 -0.90844747  0.66132157 -1.72708866] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.65324509  0.75714652  0.23297113 -0.97248365 -0.39159233 -0.23426544] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.77045298  0.63749683  0.30767173 -0.95149257 -1.25971449  1.01406396] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.93953096  0.34246397  0.64110035 -0.76745706 -2.11022352  2.79992703] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.994972   -0.10015351  0.97239671 -0.23333375 -2.28294532  3.40628482] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.87117672 -0.49096957  0.92507851  0.37977592 -1.7392341   2.6426948 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70357682 -0.7106192   0.67078752  0.74164958 -0.98256615  1.74731096] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.62589149 -0.77991015  0.47452105  0.88024415 -0.04641679  0.64947542] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.70527807 -0.70893077  0.51042191  0.85992411  1.09702984 -1.06874027] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89326781 -0.44952489  0.79801396  0.60263896  2.07665687 -2.78880024] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99990906 -0.01348568  0.99960017  0.02827549  2.33819094 -3.19365986] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.91383693  0.40608134  0.86039619 -0.50962574  1.8751958  -2.2583842 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.76040715  0.64944666  0.67146183 -0.74103914  0.96617661 -0.7048276 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.68486288  0.72867196  0.64119413 -0.76737872  0.11428845  0.30274987] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73874516  0.67398486  0.77432308 -0.63279046 -0.87233581  1.58055622] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89369188  0.4486812   0.97546086 -0.2201729  -1.8191879   2.97234683] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99815463  0.06072341  0.91938153  0.39336702 -2.11798257  3.08234199] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94293114 -0.3329878   0.59591935  0.80304429 -1.80294561  2.07435791] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81142453 -0.58445721  0.38115868  0.92450963 -0.9987107   0.38195229] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74911814 -0.66243642  0.46816646  0.88364029  0.02491118 -1.34368485] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81326647 -0.58189144  0.78183321  0.62348764  0.99264565 -2.73313298] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94588699 -0.32449624  0.99990479 -0.0137989   1.83033304 -3.9765478 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99933217  0.03654069  0.7310124  -0.68236418  1.74083699 -3.1968047 ] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94288179  0.3331275   0.31823285 -0.94801258  1.2493432  -1.71713902] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86144491  0.50785103  0.11355344 -0.99353189  0.65152911 -0.3716017 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83081283  0.55655192  0.17721428 -0.98417229 -0.09292341  1.02043984] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.88634496  0.4630255   0.52423975 -0.85157072 -0.99765765  2.72350959] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98049878  0.19652518  0.95200093 -0.30609513 -1.77808006  4.24718202] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98603686 -0.16652722  0.85401345  0.52025093 -1.73537239  4.05392823] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89346082 -0.44914114  0.3063519   0.95191833 -1.19321869  2.97585024] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81569113 -0.57848766 -0.10738729  0.99421727 -0.31968521  1.23549246] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82730534 -0.5617525  -0.18821386  0.98212807  0.51091104 -0.40964509] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.90980212 -0.41504229  0.03154766  0.99950225  1.15121351 -1.79729371] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99066039 -0.13635249  0.52673178  0.85003155  1.73522746 -3.4247248 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97235085  0.23352478  0.97424466  0.22549357  1.89708112 -4.27929693] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.83591686  0.54885609  0.81490634 -0.57959267  1.42904992 -3.92216336] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.71628227  0.69781065  0.28897016 -0.9573381   0.45004209 -2.61724659] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73093095  0.68245142 -0.0652332  -0.99787005 -0.63770233 -0.97755396] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86514317  0.50152497 -0.07736071 -0.99700317 -1.56441752  0.84428567] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98731263  0.15878842  0.23843247 -0.97115908 -2.02967446  2.30285014] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96637453 -0.25713863  0.73063974 -0.68276318 -2.09782665  3.38055001] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.79043294 -0.61254858  0.99995666 -0.0093096  -1.79735914  3.83725254] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.6124755  -0.79048957  0.80758235  0.58975482 -0.66513754  2.44608285] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.60019009 -0.79985739  0.55614729  0.83108374  0.51441543  1.02883141] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75717465 -0.65321249  0.51201964  0.85897374  1.60399673 -0.52209019] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95326312 -0.30214139  0.71601176  0.69808822  2.37193607 -2.04117669] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97775627  0.20974432  0.97453147  0.2242508   2.69561687 -3.24161948] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.75368915  0.65723106  0.9122875  -0.40955039  2.24235942 -3.01233642] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.4982301   0.86704485  0.65340743 -0.75700643  1.04127412 -1.3292149 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.4188827   0.90804035  0.55064316 -0.83474074 -0.15387428  0.03384927] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5592102   0.82902591  0.68719029 -0.72647747 -1.44712128  1.72659341] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.84170803  0.53993295  0.9565217  -0.29166117 -2.56402542  3.37681619] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99995347  0.00964652  0.91196628  0.41026517 -2.89828409  3.52386576] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86182259 -0.50720983  0.53492321  0.84490068 -2.41181547  2.16490284] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.59456751 -0.80404569  0.2727597   0.96208219 -1.55166135  0.71171035] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.43921732 -0.89838085  0.31882928  0.94781216 -0.24194094 -1.16911715] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.49990521 -0.86608012  0.63041049  0.77626195  0.92088613 -2.39078557] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.74360297 -0.66862144  0.97245513  0.23309015  2.17953351 -4.07036927] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97600705 -0.21773893  0.79513803 -0.60642848  2.80274762 -4.48151433] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.95097751  0.30926004  0.18086257 -0.98350838  2.45971174 -2.78361885] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73415943  0.67897712 -0.20325972 -0.97912486  1.79193387 -1.07107512] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.54208288  0.84032502 -0.22352728 -0.97469767  0.67382728  0.85369448] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.53948821  0.84199315  0.13615033 -0.99068819 -0.66713762  2.78355406] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.73256824  0.6806936   0.74664479 -0.66522294 -1.82384487  4.25490256] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.96109637  0.27621328  0.96151606  0.27474872 -2.70343067  5.47991684] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9705056  -0.24107857  0.29833328  0.95446176 -2.4192857   4.19749255] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78047095 -0.62519204 -0.37982994  0.92505633 -1.85560398  2.72800807] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.56679101 -0.82386161 -0.71414344  0.69999939 -1.03505963  1.34759107] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.48404262 -0.87504442 -0.77846971  0.62768218  0.07388846 -0.37328214] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58575652 -0.81048708 -0.61720161  0.78680504  1.11994309 -1.90968836] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.81036162 -0.58593007 -0.11776958  0.99304095  2.04450908 -3.60104924] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99192295 -0.12684191  0.73076589  0.68262817  2.90825884 -5.76375105] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.89276925  0.45051422  0.89121795 -0.45357532  2.79562476 -5.94167916] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.61173848  0.79106007  0.10168396 -0.99481675  1.59397639 -3.96742283] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.45829341  0.88880096 -0.46567434 -0.88495616  0.2419226  -1.95419213] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.51497434  0.8572056  -0.66976646 -0.74257181 -0.87167835 -0.54610732] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.72977299  0.68368954 -0.63075713 -0.77598031 -1.84740142  1.0571576 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.94758244  0.31951139 -0.32915242 -0.94427681 -2.34984499  2.39420881] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.98527928 -0.17095245  0.30502774 -0.95234347 -2.56230849  4.02379838] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.78614113 -0.61804703  0.93515398 -0.35424149 -2.29402721  4.7655001 ] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.51937483 -0.85454654  0.8789497   0.47691449 -1.18253972  3.60550991] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.42904353 -0.90328381  0.48022839  0.87714349  0.17860104  2.07795439] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.58589705 -0.8103855   0.27806861  0.96056122  1.61494124  0.08538174] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.86685294 -0.49856392  0.41166832  0.91133374  2.54204976 -1.51208215] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.99942688  0.03385134  0.76228795  0.64723804  2.90379581 -2.78732661] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.82528434  0.56471742  0.99657199  0.08273006  2.62761771 -3.17435087] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.50847083  0.86107922  0.89072651 -0.45453965  1.66841497 -2.23725656] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.3026791   0.95309252  0.67927318 -0.73388551  0.569583   -1.27096533] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.3152916   0.94899484  0.58305048 -0.81243593 -0.70251161  0.03924768] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.5621546   0.82703217  0.71999523 -0.69397901 -2.04048029  1.80607765] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.9002224   0.43543039  0.97450909 -0.22434803 -3.10779006  3.49168391] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.97778896 -0.20959188  0.86749235  0.49745052 -3.335874    3.6325699 ] 1 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.68981408 -0.72398656  0.46839166  0.88352094 -2.54707438  1.88908742] 0 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.36969542 -0.929153    0.31665911  0.94853941 -1.23555026 -0.18919459] 2 -1.0 False {}\n",
      "state, action, reward, done, info: [ 0.25288725 -0.96749576  0.46887362  0.88326526  0.00881773 -1.42918204] 2 -1.0 True {}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    print('state, action, reward, done, info:', \n",
    "          state, action, reward, done, info)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 0.99989252, -0.01466129,  0.99616522,  0.08749202, -0.00902808,\n",
       "         -0.06894136]),\n",
       "  0,\n",
       "  array([ 9.99999987e-01,  1.61953504e-04,  9.99561439e-01,  2.96129879e-02,\n",
       "          1.52293064e-01, -4.98332031e-01]),\n",
       "  -1.0,\n",
       "  0.0],\n",
       " (6,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "(1000,) (1000, 6) (1000,) (1000,)\n",
      "float64 float64 int64 float64\n",
      "2 0\n",
      "3\n",
      "-1.0 -1.0\n",
      "6.002648100120611 -5.941679155217321\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, targetQs, action_size, hidden_size):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    #Qs = tf.reduce_max(actions_logits, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 6) actions:(1000,)\n",
      "action size: 3\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:', np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(3), Box(6,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'), dtype('float32'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.dtype, env.observation_space.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Discrete.contains of Discrete(3)>,\n",
       " <bound method Box.contains of Box(6,)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains, env.observation_space.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Space.from_jsonable of Discrete(3)>,\n",
       " <bound method Box.from_jsonable of Box(6,)>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.from_jsonable, env.observation_space.from_jsonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
       "        -28.274334], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Discrete.sample of Discrete(3)>,\n",
       " <bound method Box.sample of Box(6,)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample, env.observation_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), (6,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 3                # action size/shape\n",
    "state_size = 6                 # state/observation size/shape\n",
    "hidden_size = 6*2              # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100               # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(1, 6), b.shape=(6, 12), m=1, n=12, k=6\n\t [[Node: generator/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_states_0_0/_1, generator/dense/kernel/read)]]\n\nCaused by op 'generator/dense/MatMul', defined at:\n  File \"/home/arasdar/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-0284fdf34c52>\", line 5, in <module>\n    model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n  File \"<ipython-input-11-446714463a1c>\", line 10, in __init__\n    states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n  File \"<ipython-input-9-5b4c2f11c043>\", line 2, in model_loss\n    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n  File \"<ipython-input-8-35814031fa47>\", line 5, in generator\n    h1 = tf.layers.dense(inputs=states, units=hidden_size)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 163, in call\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4209, in mat_mul\n    name=name)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 6), b.shape=(6, 12), m=1, n=12, k=6\n\t [[Node: generator/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_states_0_0/_1, generator/dense/kernel/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 6), b.shape=(6, 12), m=1, n=12, k=6\n\t [[Node: generator/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_states_0_0/_1, generator/dense/kernel/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-19111226f5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#     action = env.action_space.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0maction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 6), b.shape=(6, 12), m=1, n=12, k=6\n\t [[Node: generator/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_states_0_0/_1, generator/dense/kernel/read)]]\n\nCaused by op 'generator/dense/MatMul', defined at:\n  File \"/home/arasdar/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-0284fdf34c52>\", line 5, in <module>\n    model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n  File \"<ipython-input-11-446714463a1c>\", line 10, in __init__\n    states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n  File \"<ipython-input-9-5b4c2f11c043>\", line 2, in model_loss\n    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n  File \"<ipython-input-8-35814031fa47>\", line 5, in generator\n    h1 = tf.layers.dense(inputs=states, units=hidden_size)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 163, in call\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4209, in mat_mul\n    name=name)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 6), b.shape=(6, 12), m=1, n=12, k=6\n\t [[Node: generator/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_states_0_0/_1, generator/dense/kernel/read)]]\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            # if explore_p > np.random.rand():\n",
    "            #     action = env.action_space.sample()\n",
    "            # else:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "#         # Break episode/epoch loop\n",
    "#         if np.mean(episode_reward) >= 500:\n",
    "#             break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average losses')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Creating a gym env\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# A training graph session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "# Close the env at the end\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
