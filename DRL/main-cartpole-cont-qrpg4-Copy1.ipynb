{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Input layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Hidden layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        actions_logits = tf.layers.dense(inputs=nl2, units=action_size)\n",
    "        return actions_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Input layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Hidden layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        Qs_logits = tf.layers.dense(inputs=nl2, units=1)\n",
    "        return Qs_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, states, actions, next_states, rewards, dones, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    gloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    dQs = discriminator(states=states, actions=actions_labels, action_size=action_size, hidden_size=hidden_size)\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs,\n",
    "                                                                   labels=rates)) # [0, 1]\n",
    "    next_actions_logits = generator(states=next_states, hidden_size=hidden_size, action_size=action_size, \n",
    "                                    reuse=True)\n",
    "    nextQs_logits = discriminator(states=next_states, actions=next_actions_logits, action_size=action_size, \n",
    "                                  hidden_size=hidden_size, reuse=True)\n",
    "    nextQs = tf.reshape(nextQs_logits, shape=[-1])\n",
    "    targetQs = rewards + (0.99 * nextQs * (1-dones))\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    glossQ = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs,\n",
    "                                                                    labels=tf.nn.sigmoid(targetQs))) # [0, 1]\n",
    "    return actions_logits, gloss, dloss, glossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(g_loss, d_loss, g_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        g_optQ = tf.train.AdamOptimizer(g_learning_rate).minimize(g_lossQ, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    return g_opt, d_opt, g_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.g_loss, self.d_loss, self.g_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, state_size=state_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, rewards=self.rewards, \n",
    "            dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt, self.g_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                        d_loss=self.d_loss,\n",
    "                                                        g_lossQ=self.g_lossQ,\n",
    "                                                        g_learning_rate=g_learning_rate, \n",
    "                                                        d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), # ==  self.rates\n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:21.0000 R:21.0000 rate:0.0420 gloss:0.7126 dloss:0.7782 glossQ:0.6580 exploreP:0.9979\n",
      "Episode:1 meanR:19.5000 R:18.0000 rate:0.0360 gloss:0.7060 dloss:0.7665 glossQ:0.6632 exploreP:0.9961\n",
      "Episode:2 meanR:22.6667 R:29.0000 rate:0.0580 gloss:0.7073 dloss:0.7508 glossQ:0.6691 exploreP:0.9933\n",
      "Episode:3 meanR:19.5000 R:10.0000 rate:0.0200 gloss:0.7058 dloss:0.7301 glossQ:0.6789 exploreP:0.9923\n",
      "Episode:4 meanR:18.2000 R:13.0000 rate:0.0260 gloss:0.7080 dloss:0.7314 glossQ:0.6801 exploreP:0.9910\n",
      "Episode:5 meanR:20.0000 R:29.0000 rate:0.0580 gloss:0.7040 dloss:0.7293 glossQ:0.6797 exploreP:0.9882\n",
      "Episode:6 meanR:22.5714 R:38.0000 rate:0.0760 gloss:0.7006 dloss:0.7099 glossQ:0.6902 exploreP:0.9845\n",
      "Episode:7 meanR:22.5000 R:22.0000 rate:0.0440 gloss:0.6969 dloss:0.6991 glossQ:0.6951 exploreP:0.9823\n",
      "Episode:8 meanR:22.2222 R:20.0000 rate:0.0400 gloss:0.6995 dloss:0.6856 glossQ:0.7056 exploreP:0.9804\n",
      "Episode:9 meanR:22.1000 R:21.0000 rate:0.0420 gloss:0.6914 dloss:0.6824 glossQ:0.7045 exploreP:0.9784\n",
      "Episode:10 meanR:21.3636 R:14.0000 rate:0.0280 gloss:0.6982 dloss:0.6718 glossQ:0.7111 exploreP:0.9770\n",
      "Episode:11 meanR:20.8333 R:15.0000 rate:0.0300 gloss:0.6956 dloss:0.6647 glossQ:0.7172 exploreP:0.9756\n",
      "Episode:12 meanR:20.4615 R:16.0000 rate:0.0320 gloss:0.6947 dloss:0.6661 glossQ:0.7147 exploreP:0.9740\n",
      "Episode:13 meanR:20.9286 R:27.0000 rate:0.0540 gloss:0.6945 dloss:0.6542 glossQ:0.7259 exploreP:0.9714\n",
      "Episode:14 meanR:21.8667 R:35.0000 rate:0.0700 gloss:0.6931 dloss:0.6508 glossQ:0.7241 exploreP:0.9681\n",
      "Episode:15 meanR:22.0000 R:24.0000 rate:0.0480 gloss:0.6933 dloss:0.6347 glossQ:0.7347 exploreP:0.9658\n",
      "Episode:16 meanR:22.8824 R:37.0000 rate:0.0740 gloss:0.6932 dloss:0.6250 glossQ:0.7408 exploreP:0.9622\n",
      "Episode:17 meanR:22.2778 R:12.0000 rate:0.0240 gloss:0.6924 dloss:0.6195 glossQ:0.7429 exploreP:0.9611\n",
      "Episode:18 meanR:22.8947 R:34.0000 rate:0.0680 gloss:0.6924 dloss:0.6164 glossQ:0.7464 exploreP:0.9579\n",
      "Episode:19 meanR:23.3000 R:31.0000 rate:0.0620 gloss:0.6913 dloss:0.5973 glossQ:0.7632 exploreP:0.9549\n",
      "Episode:20 meanR:23.2857 R:23.0000 rate:0.0460 gloss:0.6904 dloss:0.5981 glossQ:0.7589 exploreP:0.9528\n",
      "Episode:21 meanR:23.1818 R:21.0000 rate:0.0420 gloss:0.6936 dloss:0.5827 glossQ:0.7669 exploreP:0.9508\n",
      "Episode:22 meanR:24.8696 R:62.0000 rate:0.1240 gloss:0.6911 dloss:0.5791 glossQ:0.7706 exploreP:0.9450\n",
      "Episode:23 meanR:24.4583 R:15.0000 rate:0.0300 gloss:0.6914 dloss:0.5686 glossQ:0.7753 exploreP:0.9436\n",
      "Episode:24 meanR:24.3200 R:21.0000 rate:0.0420 gloss:0.6909 dloss:0.5666 glossQ:0.7784 exploreP:0.9416\n",
      "Episode:25 meanR:23.9615 R:15.0000 rate:0.0300 gloss:0.6889 dloss:0.5651 glossQ:0.7785 exploreP:0.9402\n",
      "Episode:26 meanR:23.8889 R:22.0000 rate:0.0440 gloss:0.6894 dloss:0.5570 glossQ:0.7830 exploreP:0.9382\n",
      "Episode:27 meanR:23.8214 R:22.0000 rate:0.0440 gloss:0.6934 dloss:0.5536 glossQ:0.7851 exploreP:0.9361\n",
      "Episode:28 meanR:24.2069 R:35.0000 rate:0.0700 gloss:0.6897 dloss:0.5485 glossQ:0.7863 exploreP:0.9329\n",
      "Episode:29 meanR:24.3000 R:27.0000 rate:0.0540 gloss:0.6877 dloss:0.5377 glossQ:0.7922 exploreP:0.9304\n",
      "Episode:30 meanR:24.2581 R:23.0000 rate:0.0460 gloss:0.6924 dloss:0.5339 glossQ:0.7947 exploreP:0.9283\n",
      "Episode:31 meanR:24.3438 R:27.0000 rate:0.0540 gloss:0.6907 dloss:0.5329 glossQ:0.7971 exploreP:0.9258\n",
      "Episode:32 meanR:24.1818 R:19.0000 rate:0.0380 gloss:0.6925 dloss:0.5313 glossQ:0.7957 exploreP:0.9241\n",
      "Episode:33 meanR:24.1176 R:22.0000 rate:0.0440 gloss:0.6917 dloss:0.5300 glossQ:0.7953 exploreP:0.9221\n",
      "Episode:34 meanR:24.3143 R:31.0000 rate:0.0620 gloss:0.6895 dloss:0.5141 glossQ:0.7970 exploreP:0.9192\n",
      "Episode:35 meanR:24.3889 R:27.0000 rate:0.0540 gloss:0.6924 dloss:0.5118 glossQ:0.7941 exploreP:0.9168\n",
      "Episode:36 meanR:24.3784 R:24.0000 rate:0.0480 gloss:0.6884 dloss:0.5092 glossQ:0.7971 exploreP:0.9146\n",
      "Episode:37 meanR:24.0789 R:13.0000 rate:0.0260 gloss:0.6925 dloss:0.5066 glossQ:0.7952 exploreP:0.9134\n",
      "Episode:38 meanR:23.8718 R:16.0000 rate:0.0320 gloss:0.6880 dloss:0.5042 glossQ:0.7957 exploreP:0.9120\n",
      "Episode:39 meanR:23.7250 R:18.0000 rate:0.0360 gloss:0.6898 dloss:0.5083 glossQ:0.7932 exploreP:0.9104\n",
      "Episode:40 meanR:23.4146 R:11.0000 rate:0.0220 gloss:0.6929 dloss:0.4933 glossQ:0.7920 exploreP:0.9094\n",
      "Episode:41 meanR:23.2857 R:18.0000 rate:0.0360 gloss:0.6921 dloss:0.4982 glossQ:0.7903 exploreP:0.9078\n",
      "Episode:42 meanR:23.6744 R:40.0000 rate:0.0800 gloss:0.6894 dloss:0.4990 glossQ:0.7902 exploreP:0.9042\n",
      "Episode:43 meanR:23.4545 R:14.0000 rate:0.0280 gloss:0.6908 dloss:0.4801 glossQ:0.7866 exploreP:0.9029\n",
      "Episode:44 meanR:23.2444 R:14.0000 rate:0.0280 gloss:0.6932 dloss:0.4884 glossQ:0.7810 exploreP:0.9017\n",
      "Episode:45 meanR:23.5652 R:38.0000 rate:0.0760 gloss:0.6867 dloss:0.4938 glossQ:0.7828 exploreP:0.8983\n",
      "Episode:46 meanR:23.4894 R:20.0000 rate:0.0400 gloss:0.6897 dloss:0.4854 glossQ:0.7802 exploreP:0.8965\n",
      "Episode:47 meanR:23.4167 R:20.0000 rate:0.0400 gloss:0.6882 dloss:0.4769 glossQ:0.7767 exploreP:0.8947\n",
      "Episode:48 meanR:23.3878 R:22.0000 rate:0.0440 gloss:0.6952 dloss:0.4721 glossQ:0.7687 exploreP:0.8928\n",
      "Episode:49 meanR:23.1200 R:10.0000 rate:0.0200 gloss:0.6918 dloss:0.4738 glossQ:0.7676 exploreP:0.8919\n",
      "Episode:50 meanR:23.0000 R:17.0000 rate:0.0340 gloss:0.6929 dloss:0.4660 glossQ:0.7648 exploreP:0.8904\n",
      "Episode:51 meanR:22.9038 R:18.0000 rate:0.0360 gloss:0.6945 dloss:0.4692 glossQ:0.7584 exploreP:0.8888\n",
      "Episode:52 meanR:22.8679 R:21.0000 rate:0.0420 gloss:0.6898 dloss:0.4723 glossQ:0.7555 exploreP:0.8870\n",
      "Episode:53 meanR:22.6296 R:10.0000 rate:0.0200 gloss:0.6883 dloss:0.4516 glossQ:0.7613 exploreP:0.8861\n",
      "Episode:54 meanR:22.4182 R:11.0000 rate:0.0220 gloss:0.6885 dloss:0.4683 glossQ:0.7578 exploreP:0.8852\n",
      "Episode:55 meanR:22.8214 R:45.0000 rate:0.0900 gloss:0.6911 dloss:0.4582 glossQ:0.7431 exploreP:0.8812\n",
      "Episode:56 meanR:22.6316 R:12.0000 rate:0.0240 gloss:0.6893 dloss:0.4594 glossQ:0.7509 exploreP:0.8802\n",
      "Episode:57 meanR:22.5862 R:20.0000 rate:0.0400 gloss:0.6916 dloss:0.4727 glossQ:0.7321 exploreP:0.8784\n",
      "Episode:58 meanR:22.4576 R:15.0000 rate:0.0300 gloss:0.6943 dloss:0.4619 glossQ:0.7233 exploreP:0.8771\n",
      "Episode:59 meanR:22.2833 R:12.0000 rate:0.0240 gloss:0.6867 dloss:0.4580 glossQ:0.7194 exploreP:0.8761\n",
      "Episode:60 meanR:22.0984 R:11.0000 rate:0.0220 gloss:0.6867 dloss:0.4361 glossQ:0.7235 exploreP:0.8752\n",
      "Episode:61 meanR:22.1452 R:25.0000 rate:0.0500 gloss:0.6871 dloss:0.4469 glossQ:0.7099 exploreP:0.8730\n",
      "Episode:62 meanR:22.4603 R:42.0000 rate:0.0840 gloss:0.6925 dloss:0.4390 glossQ:0.6874 exploreP:0.8694\n",
      "Episode:63 meanR:22.3594 R:16.0000 rate:0.0320 gloss:0.6939 dloss:0.4313 glossQ:0.6697 exploreP:0.8680\n",
      "Episode:64 meanR:22.5077 R:32.0000 rate:0.0640 gloss:0.6907 dloss:0.4414 glossQ:0.6735 exploreP:0.8653\n",
      "Episode:65 meanR:22.3636 R:13.0000 rate:0.0260 gloss:0.6873 dloss:0.4442 glossQ:0.6763 exploreP:0.8641\n",
      "Episode:66 meanR:22.2090 R:12.0000 rate:0.0240 gloss:0.6984 dloss:0.4324 glossQ:0.6346 exploreP:0.8631\n",
      "Episode:67 meanR:22.9559 R:73.0000 rate:0.1460 gloss:0.6917 dloss:0.4279 glossQ:0.6324 exploreP:0.8569\n",
      "Episode:68 meanR:22.7536 R:9.0000 rate:0.0180 gloss:0.6925 dloss:0.4080 glossQ:0.6261 exploreP:0.8562\n",
      "Episode:69 meanR:22.7000 R:19.0000 rate:0.0380 gloss:0.6904 dloss:0.4242 glossQ:0.6100 exploreP:0.8546\n",
      "Episode:70 meanR:22.7042 R:23.0000 rate:0.0460 gloss:0.6902 dloss:0.4262 glossQ:0.6043 exploreP:0.8526\n",
      "Episode:71 meanR:23.0139 R:45.0000 rate:0.0900 gloss:0.6904 dloss:0.4234 glossQ:0.5763 exploreP:0.8488\n",
      "Episode:72 meanR:22.8219 R:9.0000 rate:0.0180 gloss:0.6903 dloss:0.4033 glossQ:0.5876 exploreP:0.8481\n",
      "Episode:73 meanR:22.7703 R:19.0000 rate:0.0380 gloss:0.6850 dloss:0.4202 glossQ:0.5784 exploreP:0.8465\n",
      "Episode:74 meanR:22.7200 R:19.0000 rate:0.0380 gloss:0.6856 dloss:0.4081 glossQ:0.5535 exploreP:0.8449\n",
      "Episode:75 meanR:22.6974 R:21.0000 rate:0.0420 gloss:0.6925 dloss:0.4170 glossQ:0.5320 exploreP:0.8431\n",
      "Episode:76 meanR:22.6364 R:18.0000 rate:0.0360 gloss:0.6938 dloss:0.4082 glossQ:0.5017 exploreP:0.8416\n",
      "Episode:77 meanR:22.5641 R:17.0000 rate:0.0340 gloss:0.6917 dloss:0.4089 glossQ:0.5187 exploreP:0.8402\n",
      "Episode:78 meanR:22.5949 R:25.0000 rate:0.0500 gloss:0.6915 dloss:0.4117 glossQ:0.4971 exploreP:0.8382\n",
      "Episode:79 meanR:22.7000 R:31.0000 rate:0.0620 gloss:0.6880 dloss:0.4098 glossQ:0.4872 exploreP:0.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:80 meanR:22.9630 R:44.0000 rate:0.0880 gloss:0.6920 dloss:0.4031 glossQ:0.4651 exploreP:0.8320\n",
      "Episode:81 meanR:23.4024 R:59.0000 rate:0.1180 gloss:0.6918 dloss:0.4101 glossQ:0.4360 exploreP:0.8271\n",
      "Episode:82 meanR:23.5783 R:38.0000 rate:0.0760 gloss:0.6808 dloss:0.4024 glossQ:0.4229 exploreP:0.8240\n",
      "Episode:83 meanR:23.6310 R:28.0000 rate:0.0560 gloss:0.6816 dloss:0.4197 glossQ:0.4103 exploreP:0.8218\n",
      "Episode:84 meanR:23.8353 R:41.0000 rate:0.0820 gloss:0.6833 dloss:0.3974 glossQ:0.3958 exploreP:0.8184\n",
      "Episode:85 meanR:23.9767 R:36.0000 rate:0.0720 gloss:0.6866 dloss:0.4235 glossQ:0.3776 exploreP:0.8155\n",
      "Episode:86 meanR:24.6092 R:79.0000 rate:0.1580 gloss:0.6852 dloss:0.3981 glossQ:0.3524 exploreP:0.8092\n",
      "Episode:87 meanR:24.7614 R:38.0000 rate:0.0760 gloss:0.6828 dloss:0.3905 glossQ:0.3242 exploreP:0.8062\n",
      "Episode:88 meanR:24.6517 R:15.0000 rate:0.0300 gloss:0.6847 dloss:0.3782 glossQ:0.3274 exploreP:0.8050\n",
      "Episode:89 meanR:24.5556 R:16.0000 rate:0.0320 gloss:0.6818 dloss:0.3938 glossQ:0.3133 exploreP:0.8037\n",
      "Episode:90 meanR:24.6923 R:37.0000 rate:0.0740 gloss:0.6885 dloss:0.4040 glossQ:0.3013 exploreP:0.8008\n",
      "Episode:91 meanR:25.1196 R:64.0000 rate:0.1280 gloss:0.6839 dloss:0.3945 glossQ:0.2982 exploreP:0.7957\n",
      "Episode:92 meanR:25.0538 R:19.0000 rate:0.0380 gloss:0.6853 dloss:0.3735 glossQ:0.2851 exploreP:0.7942\n",
      "Episode:93 meanR:25.0426 R:24.0000 rate:0.0480 gloss:0.6833 dloss:0.3836 glossQ:0.2810 exploreP:0.7924\n",
      "Episode:94 meanR:25.3895 R:58.0000 rate:0.1160 gloss:0.6878 dloss:0.3878 glossQ:0.2785 exploreP:0.7878\n",
      "Episode:95 meanR:25.8333 R:68.0000 rate:0.1360 gloss:0.6870 dloss:0.3894 glossQ:0.2490 exploreP:0.7826\n",
      "Episode:96 meanR:26.2990 R:71.0000 rate:0.1420 gloss:0.6913 dloss:0.3895 glossQ:0.2569 exploreP:0.7771\n",
      "Episode:97 meanR:26.2245 R:19.0000 rate:0.0380 gloss:0.6809 dloss:0.3716 glossQ:0.2454 exploreP:0.7756\n",
      "Episode:98 meanR:26.3838 R:42.0000 rate:0.0840 gloss:0.6820 dloss:0.3871 glossQ:0.2353 exploreP:0.7724\n",
      "Episode:99 meanR:26.3500 R:23.0000 rate:0.0460 gloss:0.6854 dloss:0.3957 glossQ:0.2403 exploreP:0.7707\n",
      "Episode:100 meanR:26.3000 R:16.0000 rate:0.0320 gloss:0.6930 dloss:0.3914 glossQ:0.2352 exploreP:0.7695\n",
      "Episode:101 meanR:26.3800 R:26.0000 rate:0.0520 gloss:0.6856 dloss:0.3945 glossQ:0.2204 exploreP:0.7675\n",
      "Episode:102 meanR:26.3100 R:22.0000 rate:0.0440 gloss:0.6847 dloss:0.3823 glossQ:0.2174 exploreP:0.7658\n",
      "Episode:103 meanR:26.6300 R:42.0000 rate:0.0840 gloss:0.6824 dloss:0.3892 glossQ:0.2125 exploreP:0.7627\n",
      "Episode:104 meanR:26.7800 R:28.0000 rate:0.0560 gloss:0.6882 dloss:0.3851 glossQ:0.2150 exploreP:0.7605\n",
      "Episode:105 meanR:27.0400 R:55.0000 rate:0.1100 gloss:0.6814 dloss:0.4031 glossQ:0.2071 exploreP:0.7564\n",
      "Episode:106 meanR:26.9900 R:33.0000 rate:0.0660 gloss:0.6743 dloss:0.4037 glossQ:0.2097 exploreP:0.7540\n",
      "Episode:107 meanR:27.0700 R:30.0000 rate:0.0600 gloss:0.6842 dloss:0.3841 glossQ:0.2042 exploreP:0.7517\n",
      "Episode:108 meanR:27.0900 R:22.0000 rate:0.0440 gloss:0.6792 dloss:0.4076 glossQ:0.2052 exploreP:0.7501\n",
      "Episode:109 meanR:27.2800 R:40.0000 rate:0.0800 gloss:0.6792 dloss:0.3833 glossQ:0.2014 exploreP:0.7472\n",
      "Episode:110 meanR:27.5400 R:40.0000 rate:0.0800 gloss:0.6813 dloss:0.4019 glossQ:0.2019 exploreP:0.7442\n",
      "Episode:111 meanR:27.6000 R:21.0000 rate:0.0420 gloss:0.6839 dloss:0.3788 glossQ:0.2023 exploreP:0.7427\n",
      "Episode:112 meanR:28.0600 R:62.0000 rate:0.1240 gloss:0.6864 dloss:0.3963 glossQ:0.1968 exploreP:0.7381\n",
      "Episode:113 meanR:28.0600 R:27.0000 rate:0.0540 gloss:0.6814 dloss:0.4016 glossQ:0.1901 exploreP:0.7362\n",
      "Episode:114 meanR:27.9800 R:27.0000 rate:0.0540 gloss:0.6868 dloss:0.3606 glossQ:0.1934 exploreP:0.7342\n",
      "Episode:115 meanR:28.0200 R:28.0000 rate:0.0560 gloss:0.6799 dloss:0.3733 glossQ:0.1927 exploreP:0.7322\n",
      "Episode:116 meanR:27.7900 R:14.0000 rate:0.0280 gloss:0.6787 dloss:0.3841 glossQ:0.1962 exploreP:0.7312\n",
      "Episode:117 meanR:28.0100 R:34.0000 rate:0.0680 gloss:0.6836 dloss:0.3624 glossQ:0.1947 exploreP:0.7287\n",
      "Episode:118 meanR:27.9000 R:23.0000 rate:0.0460 gloss:0.6854 dloss:0.3848 glossQ:0.1918 exploreP:0.7271\n",
      "Episode:119 meanR:27.7300 R:14.0000 rate:0.0280 gloss:0.6777 dloss:0.3793 glossQ:0.1939 exploreP:0.7261\n",
      "Episode:120 meanR:27.6900 R:19.0000 rate:0.0380 gloss:0.6875 dloss:0.3694 glossQ:0.1917 exploreP:0.7247\n",
      "Episode:121 meanR:27.7700 R:29.0000 rate:0.0580 gloss:0.6812 dloss:0.3890 glossQ:0.1816 exploreP:0.7227\n",
      "Episode:122 meanR:27.3100 R:16.0000 rate:0.0320 gloss:0.6739 dloss:0.3685 glossQ:0.1908 exploreP:0.7215\n",
      "Episode:123 meanR:27.4200 R:26.0000 rate:0.0520 gloss:0.6869 dloss:0.3743 glossQ:0.1836 exploreP:0.7197\n",
      "Episode:124 meanR:27.3900 R:18.0000 rate:0.0360 gloss:0.6829 dloss:0.3728 glossQ:0.1808 exploreP:0.7184\n",
      "Episode:125 meanR:27.5600 R:32.0000 rate:0.0640 gloss:0.6803 dloss:0.3990 glossQ:0.1741 exploreP:0.7161\n",
      "Episode:126 meanR:27.9200 R:58.0000 rate:0.1160 gloss:0.6760 dloss:0.3891 glossQ:0.2020 exploreP:0.7121\n",
      "Episode:127 meanR:27.9800 R:28.0000 rate:0.0560 gloss:0.6870 dloss:0.3943 glossQ:0.1764 exploreP:0.7101\n",
      "Episode:128 meanR:28.3300 R:70.0000 rate:0.1400 gloss:0.6841 dloss:0.3842 glossQ:0.1811 exploreP:0.7052\n",
      "Episode:129 meanR:28.3400 R:28.0000 rate:0.0560 gloss:0.6829 dloss:0.3857 glossQ:0.1782 exploreP:0.7033\n",
      "Episode:130 meanR:28.3800 R:27.0000 rate:0.0540 gloss:0.6822 dloss:0.4006 glossQ:0.1760 exploreP:0.7014\n",
      "Episode:131 meanR:28.5900 R:48.0000 rate:0.0960 gloss:0.6822 dloss:0.3966 glossQ:0.1776 exploreP:0.6981\n",
      "Episode:132 meanR:29.1600 R:76.0000 rate:0.1520 gloss:0.6781 dloss:0.3952 glossQ:0.1756 exploreP:0.6929\n",
      "Episode:133 meanR:29.2000 R:26.0000 rate:0.0520 gloss:0.6811 dloss:0.3833 glossQ:0.1740 exploreP:0.6911\n",
      "Episode:134 meanR:29.4300 R:54.0000 rate:0.1080 gloss:0.6832 dloss:0.3843 glossQ:0.1753 exploreP:0.6874\n",
      "Episode:135 meanR:29.5300 R:37.0000 rate:0.0740 gloss:0.6815 dloss:0.3842 glossQ:0.1737 exploreP:0.6849\n",
      "Episode:136 meanR:29.5300 R:24.0000 rate:0.0480 gloss:0.6920 dloss:0.3918 glossQ:0.1750 exploreP:0.6833\n",
      "Episode:137 meanR:29.5900 R:19.0000 rate:0.0380 gloss:0.6785 dloss:0.3905 glossQ:0.1776 exploreP:0.6820\n",
      "Episode:138 meanR:29.7400 R:31.0000 rate:0.0620 gloss:0.6827 dloss:0.3867 glossQ:0.1728 exploreP:0.6800\n",
      "Episode:139 meanR:29.8600 R:30.0000 rate:0.0600 gloss:0.6845 dloss:0.3834 glossQ:0.1757 exploreP:0.6779\n",
      "Episode:140 meanR:30.0800 R:33.0000 rate:0.0660 gloss:0.6803 dloss:0.3785 glossQ:0.1757 exploreP:0.6757\n",
      "Episode:141 meanR:30.2300 R:33.0000 rate:0.0660 gloss:0.6862 dloss:0.3837 glossQ:0.1795 exploreP:0.6736\n",
      "Episode:142 meanR:30.4000 R:57.0000 rate:0.1140 gloss:0.6799 dloss:0.3816 glossQ:0.1806 exploreP:0.6698\n",
      "Episode:143 meanR:30.5800 R:32.0000 rate:0.0640 gloss:0.6792 dloss:0.3903 glossQ:0.1688 exploreP:0.6677\n",
      "Episode:144 meanR:30.8500 R:41.0000 rate:0.0820 gloss:0.6868 dloss:0.3968 glossQ:0.1719 exploreP:0.6650\n",
      "Episode:145 meanR:30.7400 R:27.0000 rate:0.0540 gloss:0.6793 dloss:0.3914 glossQ:0.1753 exploreP:0.6632\n",
      "Episode:146 meanR:30.8100 R:27.0000 rate:0.0540 gloss:0.6837 dloss:0.3901 glossQ:0.1721 exploreP:0.6615\n",
      "Episode:147 meanR:30.9400 R:33.0000 rate:0.0660 gloss:0.6814 dloss:0.3982 glossQ:0.1703 exploreP:0.6593\n",
      "Episode:148 meanR:31.1300 R:41.0000 rate:0.0820 gloss:0.6817 dloss:0.3981 glossQ:0.1708 exploreP:0.6566\n",
      "Episode:149 meanR:31.7300 R:70.0000 rate:0.1400 gloss:0.6798 dloss:0.3881 glossQ:0.1709 exploreP:0.6521\n",
      "Episode:150 meanR:31.8500 R:29.0000 rate:0.0580 gloss:0.6821 dloss:0.3879 glossQ:0.1692 exploreP:0.6503\n",
      "Episode:151 meanR:32.3900 R:72.0000 rate:0.1440 gloss:0.6881 dloss:0.3789 glossQ:0.1782 exploreP:0.6457\n",
      "Episode:152 meanR:32.3400 R:16.0000 rate:0.0320 gloss:0.6858 dloss:0.4057 glossQ:0.1697 exploreP:0.6447\n",
      "Episode:153 meanR:32.8300 R:59.0000 rate:0.1180 gloss:0.6829 dloss:0.3870 glossQ:0.1718 exploreP:0.6409\n",
      "Episode:154 meanR:33.2700 R:55.0000 rate:0.1100 gloss:0.6833 dloss:0.3846 glossQ:0.1805 exploreP:0.6375\n",
      "Episode:155 meanR:32.9800 R:16.0000 rate:0.0320 gloss:0.6846 dloss:0.3727 glossQ:0.1696 exploreP:0.6365\n",
      "Episode:156 meanR:33.0100 R:15.0000 rate:0.0300 gloss:0.6832 dloss:0.3954 glossQ:0.1661 exploreP:0.6355\n",
      "Episode:157 meanR:33.0200 R:21.0000 rate:0.0420 gloss:0.6812 dloss:0.3611 glossQ:0.1704 exploreP:0.6342\n",
      "Episode:158 meanR:33.5900 R:72.0000 rate:0.1440 gloss:0.6846 dloss:0.3879 glossQ:0.1663 exploreP:0.6297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:159 meanR:33.9400 R:47.0000 rate:0.0940 gloss:0.6817 dloss:0.3897 glossQ:0.1655 exploreP:0.6268\n",
      "Episode:160 meanR:33.9900 R:16.0000 rate:0.0320 gloss:0.6733 dloss:0.4188 glossQ:0.1663 exploreP:0.6259\n",
      "Episode:161 meanR:33.9200 R:18.0000 rate:0.0360 gloss:0.6836 dloss:0.3773 glossQ:0.1686 exploreP:0.6247\n",
      "Episode:162 meanR:34.2200 R:72.0000 rate:0.1440 gloss:0.6802 dloss:0.3860 glossQ:0.1717 exploreP:0.6203\n",
      "Episode:163 meanR:34.6300 R:57.0000 rate:0.1140 gloss:0.6805 dloss:0.3788 glossQ:0.1662 exploreP:0.6169\n",
      "Episode:164 meanR:35.6200 R:131.0000 rate:0.2620 gloss:0.6834 dloss:0.3893 glossQ:0.1675 exploreP:0.6090\n",
      "Episode:165 meanR:35.8200 R:33.0000 rate:0.0660 gloss:0.6982 dloss:0.3945 glossQ:0.2114 exploreP:0.6070\n",
      "Episode:166 meanR:35.9200 R:22.0000 rate:0.0440 gloss:0.6765 dloss:0.3742 glossQ:0.1681 exploreP:0.6057\n",
      "Episode:167 meanR:35.7200 R:53.0000 rate:0.1060 gloss:0.6827 dloss:0.3963 glossQ:0.1655 exploreP:0.6025\n",
      "Episode:168 meanR:36.0200 R:39.0000 rate:0.0780 gloss:0.6811 dloss:0.3899 glossQ:0.1683 exploreP:0.6002\n",
      "Episode:169 meanR:36.0400 R:21.0000 rate:0.0420 gloss:0.6808 dloss:0.3823 glossQ:0.1669 exploreP:0.5990\n",
      "Episode:170 meanR:36.3500 R:54.0000 rate:0.1080 gloss:0.6818 dloss:0.3908 glossQ:0.1692 exploreP:0.5958\n",
      "Episode:171 meanR:36.2000 R:30.0000 rate:0.0600 gloss:0.6819 dloss:0.4101 glossQ:0.1665 exploreP:0.5941\n",
      "Episode:172 meanR:36.4300 R:32.0000 rate:0.0640 gloss:0.6806 dloss:0.3674 glossQ:0.1723 exploreP:0.5922\n",
      "Episode:173 meanR:36.7600 R:52.0000 rate:0.1040 gloss:0.6821 dloss:0.4003 glossQ:0.1733 exploreP:0.5892\n",
      "Episode:174 meanR:36.8100 R:24.0000 rate:0.0480 gloss:0.6768 dloss:0.3832 glossQ:0.1685 exploreP:0.5878\n",
      "Episode:175 meanR:37.2100 R:61.0000 rate:0.1220 gloss:0.6793 dloss:0.3950 glossQ:0.1681 exploreP:0.5843\n",
      "Episode:176 meanR:37.3500 R:32.0000 rate:0.0640 gloss:0.6840 dloss:0.3835 glossQ:0.1652 exploreP:0.5824\n",
      "Episode:177 meanR:37.7700 R:59.0000 rate:0.1180 gloss:0.6850 dloss:0.3861 glossQ:0.1677 exploreP:0.5791\n",
      "Episode:178 meanR:38.0200 R:50.0000 rate:0.1000 gloss:0.6813 dloss:0.3971 glossQ:0.1660 exploreP:0.5762\n",
      "Episode:179 meanR:38.1400 R:43.0000 rate:0.0860 gloss:0.6803 dloss:0.3783 glossQ:0.1681 exploreP:0.5738\n",
      "Episode:180 meanR:38.0400 R:34.0000 rate:0.0680 gloss:0.6839 dloss:0.3830 glossQ:0.1772 exploreP:0.5719\n",
      "Episode:181 meanR:38.0400 R:59.0000 rate:0.1180 gloss:0.6793 dloss:0.3945 glossQ:0.1656 exploreP:0.5686\n",
      "Episode:182 meanR:38.0600 R:40.0000 rate:0.0800 gloss:0.6780 dloss:0.3786 glossQ:0.1664 exploreP:0.5664\n",
      "Episode:183 meanR:37.9300 R:15.0000 rate:0.0300 gloss:0.6777 dloss:0.3712 glossQ:0.1658 exploreP:0.5655\n",
      "Episode:184 meanR:37.9200 R:40.0000 rate:0.0800 gloss:0.6778 dloss:0.3975 glossQ:0.1626 exploreP:0.5633\n",
      "Episode:185 meanR:37.8100 R:25.0000 rate:0.0500 gloss:0.6855 dloss:0.3989 glossQ:0.1660 exploreP:0.5619\n",
      "Episode:186 meanR:37.4000 R:38.0000 rate:0.0760 gloss:0.6804 dloss:0.3960 glossQ:0.1664 exploreP:0.5598\n",
      "Episode:187 meanR:37.4600 R:44.0000 rate:0.0880 gloss:0.6784 dloss:0.3948 glossQ:0.1665 exploreP:0.5574\n",
      "Episode:188 meanR:37.6300 R:32.0000 rate:0.0640 gloss:0.6758 dloss:0.3827 glossQ:0.1684 exploreP:0.5557\n",
      "Episode:189 meanR:37.8500 R:38.0000 rate:0.0760 gloss:0.6816 dloss:0.4095 glossQ:0.1654 exploreP:0.5536\n",
      "Episode:190 meanR:38.3800 R:90.0000 rate:0.1800 gloss:0.6829 dloss:0.3931 glossQ:0.1737 exploreP:0.5487\n",
      "Episode:191 meanR:38.1900 R:45.0000 rate:0.0900 gloss:0.6798 dloss:0.3880 glossQ:0.1718 exploreP:0.5463\n",
      "Episode:192 meanR:38.5000 R:50.0000 rate:0.1000 gloss:0.6790 dloss:0.3853 glossQ:0.1649 exploreP:0.5436\n",
      "Episode:193 meanR:38.5500 R:29.0000 rate:0.0580 gloss:0.6762 dloss:0.3896 glossQ:0.1695 exploreP:0.5421\n",
      "Episode:194 meanR:38.2400 R:27.0000 rate:0.0540 gloss:0.6818 dloss:0.3706 glossQ:0.2296 exploreP:0.5407\n",
      "Episode:195 meanR:38.1200 R:56.0000 rate:0.1120 gloss:0.6824 dloss:0.3799 glossQ:0.1718 exploreP:0.5377\n",
      "Episode:196 meanR:38.5700 R:116.0000 rate:0.2320 gloss:0.6796 dloss:0.3915 glossQ:0.1638 exploreP:0.5316\n",
      "Episode:197 meanR:38.6300 R:25.0000 rate:0.0500 gloss:0.6769 dloss:0.3803 glossQ:0.1776 exploreP:0.5303\n",
      "Episode:198 meanR:38.5500 R:34.0000 rate:0.0680 gloss:0.6802 dloss:0.3830 glossQ:0.1720 exploreP:0.5285\n",
      "Episode:199 meanR:38.5100 R:19.0000 rate:0.0380 gloss:0.6760 dloss:0.4178 glossQ:0.1653 exploreP:0.5275\n",
      "Episode:200 meanR:38.9200 R:57.0000 rate:0.1140 gloss:0.6806 dloss:0.3782 glossQ:0.1661 exploreP:0.5246\n",
      "Episode:201 meanR:38.8600 R:20.0000 rate:0.0400 gloss:0.6817 dloss:0.3667 glossQ:0.1678 exploreP:0.5236\n",
      "Episode:202 meanR:39.3200 R:68.0000 rate:0.1360 gloss:0.6811 dloss:0.3841 glossQ:0.1655 exploreP:0.5201\n",
      "Episode:203 meanR:39.8700 R:97.0000 rate:0.1940 gloss:0.6785 dloss:0.3868 glossQ:0.1784 exploreP:0.5152\n",
      "Episode:204 meanR:40.2000 R:61.0000 rate:0.1220 gloss:0.6776 dloss:0.3806 glossQ:0.1617 exploreP:0.5121\n",
      "Episode:205 meanR:39.8800 R:23.0000 rate:0.0460 gloss:0.6811 dloss:0.3979 glossQ:0.1607 exploreP:0.5109\n",
      "Episode:206 meanR:40.1400 R:59.0000 rate:0.1180 gloss:0.6791 dloss:0.3990 glossQ:0.1634 exploreP:0.5080\n",
      "Episode:207 meanR:40.4400 R:60.0000 rate:0.1200 gloss:0.6787 dloss:0.3989 glossQ:0.1641 exploreP:0.5050\n",
      "Episode:208 meanR:40.7900 R:57.0000 rate:0.1140 gloss:0.6760 dloss:0.3937 glossQ:0.1822 exploreP:0.5022\n",
      "Episode:209 meanR:40.7500 R:36.0000 rate:0.0720 gloss:0.6751 dloss:0.3979 glossQ:0.1673 exploreP:0.5004\n",
      "Episode:210 meanR:40.5500 R:20.0000 rate:0.0400 gloss:0.6868 dloss:0.3970 glossQ:0.1819 exploreP:0.4995\n",
      "Episode:211 meanR:40.6900 R:35.0000 rate:0.0700 gloss:0.6847 dloss:0.3824 glossQ:0.1755 exploreP:0.4978\n",
      "Episode:212 meanR:40.6300 R:56.0000 rate:0.1120 gloss:0.6826 dloss:0.3814 glossQ:0.1805 exploreP:0.4950\n",
      "Episode:213 meanR:40.6800 R:32.0000 rate:0.0640 gloss:0.6750 dloss:0.3996 glossQ:0.1639 exploreP:0.4935\n",
      "Episode:214 meanR:40.8500 R:44.0000 rate:0.0880 gloss:0.6749 dloss:0.3993 glossQ:0.1685 exploreP:0.4914\n",
      "Episode:215 meanR:40.8500 R:28.0000 rate:0.0560 gloss:0.6741 dloss:0.4055 glossQ:0.1658 exploreP:0.4900\n",
      "Episode:216 meanR:41.1200 R:41.0000 rate:0.0820 gloss:0.6888 dloss:0.4035 glossQ:0.1776 exploreP:0.4880\n",
      "Episode:217 meanR:41.1500 R:37.0000 rate:0.0740 gloss:0.6750 dloss:0.4084 glossQ:0.1682 exploreP:0.4863\n",
      "Episode:218 meanR:41.3500 R:43.0000 rate:0.0860 gloss:0.6787 dloss:0.4137 glossQ:0.1706 exploreP:0.4842\n",
      "Episode:219 meanR:41.9300 R:72.0000 rate:0.1440 gloss:0.6800 dloss:0.4061 glossQ:0.1762 exploreP:0.4808\n",
      "Episode:220 meanR:42.0800 R:34.0000 rate:0.0680 gloss:0.6832 dloss:0.3845 glossQ:0.1762 exploreP:0.4792\n",
      "Episode:221 meanR:42.9300 R:114.0000 rate:0.2280 gloss:0.6794 dloss:0.3861 glossQ:0.1699 exploreP:0.4739\n",
      "Episode:222 meanR:43.5600 R:79.0000 rate:0.1580 gloss:0.6803 dloss:0.3885 glossQ:0.1697 exploreP:0.4703\n",
      "Episode:223 meanR:44.3500 R:105.0000 rate:0.2100 gloss:0.6791 dloss:0.3885 glossQ:0.1740 exploreP:0.4655\n",
      "Episode:224 meanR:44.8000 R:63.0000 rate:0.1260 gloss:0.6779 dloss:0.4012 glossQ:0.1653 exploreP:0.4626\n",
      "Episode:225 meanR:45.2800 R:80.0000 rate:0.1600 gloss:0.6741 dloss:0.4046 glossQ:0.1674 exploreP:0.4590\n",
      "Episode:226 meanR:45.2100 R:51.0000 rate:0.1020 gloss:0.6726 dloss:0.3996 glossQ:0.1678 exploreP:0.4567\n",
      "Episode:227 meanR:46.1500 R:122.0000 rate:0.2440 gloss:0.6754 dloss:0.4033 glossQ:0.1792 exploreP:0.4513\n",
      "Episode:228 meanR:45.8300 R:38.0000 rate:0.0760 gloss:0.6796 dloss:0.3759 glossQ:0.1706 exploreP:0.4496\n",
      "Episode:229 meanR:46.7300 R:118.0000 rate:0.2360 gloss:0.6781 dloss:0.3846 glossQ:0.1689 exploreP:0.4445\n",
      "Episode:230 meanR:47.4600 R:100.0000 rate:0.2000 gloss:0.6800 dloss:0.3924 glossQ:0.1664 exploreP:0.4401\n",
      "Episode:231 meanR:47.7300 R:75.0000 rate:0.1500 gloss:0.6804 dloss:0.3875 glossQ:0.1718 exploreP:0.4369\n",
      "Episode:232 meanR:47.4700 R:50.0000 rate:0.1000 gloss:0.6773 dloss:0.3942 glossQ:0.1667 exploreP:0.4348\n",
      "Episode:233 meanR:48.0500 R:84.0000 rate:0.1680 gloss:0.6784 dloss:0.3937 glossQ:0.1648 exploreP:0.4312\n",
      "Episode:234 meanR:48.0300 R:52.0000 rate:0.1040 gloss:0.6789 dloss:0.4044 glossQ:0.1665 exploreP:0.4291\n",
      "Episode:235 meanR:48.5400 R:88.0000 rate:0.1760 gloss:0.6780 dloss:0.3885 glossQ:0.1688 exploreP:0.4254\n",
      "Episode:236 meanR:49.4900 R:119.0000 rate:0.2380 gloss:0.6781 dloss:0.3971 glossQ:0.1686 exploreP:0.4205\n",
      "Episode:237 meanR:51.0500 R:175.0000 rate:0.3500 gloss:0.6764 dloss:0.3975 glossQ:0.1673 exploreP:0.4134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:238 meanR:52.2500 R:151.0000 rate:0.3020 gloss:0.6800 dloss:0.3956 glossQ:0.1679 exploreP:0.4073\n",
      "Episode:239 meanR:52.9500 R:100.0000 rate:0.2000 gloss:0.6784 dloss:0.3867 glossQ:0.1696 exploreP:0.4034\n",
      "Episode:240 meanR:54.3400 R:172.0000 rate:0.3440 gloss:0.6778 dloss:0.3914 glossQ:0.1752 exploreP:0.3966\n",
      "Episode:241 meanR:54.7000 R:69.0000 rate:0.1380 gloss:0.6788 dloss:0.3914 glossQ:0.1656 exploreP:0.3940\n",
      "Episode:242 meanR:55.7300 R:160.0000 rate:0.3200 gloss:0.6762 dloss:0.3871 glossQ:0.1665 exploreP:0.3879\n",
      "Episode:243 meanR:57.0900 R:168.0000 rate:0.3360 gloss:0.6795 dloss:0.3940 glossQ:0.1713 exploreP:0.3816\n",
      "Episode:244 meanR:57.1400 R:46.0000 rate:0.0920 gloss:0.6785 dloss:0.3905 glossQ:0.1685 exploreP:0.3799\n",
      "Episode:245 meanR:58.1600 R:129.0000 rate:0.2580 gloss:0.6733 dloss:0.4045 glossQ:0.1681 exploreP:0.3751\n",
      "Episode:246 meanR:59.3700 R:148.0000 rate:0.2960 gloss:0.6770 dloss:0.4031 glossQ:0.1730 exploreP:0.3698\n",
      "Episode:247 meanR:59.6200 R:58.0000 rate:0.1160 gloss:0.6752 dloss:0.4042 glossQ:0.1745 exploreP:0.3677\n",
      "Episode:248 meanR:60.8200 R:161.0000 rate:0.3220 gloss:0.6751 dloss:0.4012 glossQ:0.1719 exploreP:0.3620\n",
      "Episode:249 meanR:61.8900 R:177.0000 rate:0.3540 gloss:0.6757 dloss:0.3881 glossQ:0.1698 exploreP:0.3558\n",
      "Episode:250 meanR:62.5600 R:96.0000 rate:0.1920 gloss:0.6704 dloss:0.3960 glossQ:0.1745 exploreP:0.3525\n",
      "Episode:251 meanR:63.6600 R:182.0000 rate:0.3640 gloss:0.6759 dloss:0.4074 glossQ:0.1684 exploreP:0.3463\n",
      "Episode:252 meanR:65.0600 R:156.0000 rate:0.3120 gloss:0.6759 dloss:0.3970 glossQ:0.1713 exploreP:0.3411\n",
      "Episode:253 meanR:67.1400 R:267.0000 rate:0.5340 gloss:0.6752 dloss:0.3934 glossQ:0.1690 exploreP:0.3324\n",
      "Episode:254 meanR:67.2300 R:64.0000 rate:0.1280 gloss:0.6786 dloss:0.3890 glossQ:0.1697 exploreP:0.3303\n",
      "Episode:255 meanR:68.6200 R:155.0000 rate:0.3100 gloss:0.6749 dloss:0.4033 glossQ:0.1767 exploreP:0.3254\n",
      "Episode:256 meanR:70.3300 R:186.0000 rate:0.3720 gloss:0.6735 dloss:0.4145 glossQ:0.1717 exploreP:0.3196\n",
      "Episode:257 meanR:72.3300 R:221.0000 rate:0.4420 gloss:0.6737 dloss:0.4048 glossQ:0.1765 exploreP:0.3128\n",
      "Episode:258 meanR:74.8000 R:319.0000 rate:0.6380 gloss:0.6738 dloss:0.4056 glossQ:0.1763 exploreP:0.3033\n",
      "Episode:259 meanR:75.9700 R:164.0000 rate:0.3280 gloss:0.6731 dloss:0.4044 glossQ:0.1742 exploreP:0.2986\n",
      "Episode:260 meanR:75.9800 R:17.0000 rate:0.0340 gloss:0.6696 dloss:0.4315 glossQ:0.1737 exploreP:0.2981\n",
      "Episode:261 meanR:76.9800 R:118.0000 rate:0.2360 gloss:0.6760 dloss:0.4339 glossQ:0.1786 exploreP:0.2947\n",
      "Episode:262 meanR:78.6400 R:238.0000 rate:0.4760 gloss:0.6744 dloss:0.4165 glossQ:0.1800 exploreP:0.2880\n",
      "Episode:263 meanR:80.3800 R:231.0000 rate:0.4620 gloss:0.6726 dloss:0.4247 glossQ:0.1775 exploreP:0.2816\n",
      "Episode:264 meanR:81.6700 R:260.0000 rate:0.5200 gloss:0.6724 dloss:0.4171 glossQ:0.1822 exploreP:0.2747\n",
      "Episode:265 meanR:83.1300 R:179.0000 rate:0.3580 gloss:0.6743 dloss:0.4451 glossQ:0.1833 exploreP:0.2700\n",
      "Episode:266 meanR:84.4200 R:151.0000 rate:0.3020 gloss:0.6719 dloss:0.4539 glossQ:0.1906 exploreP:0.2661\n",
      "Episode:267 meanR:86.0000 R:211.0000 rate:0.4220 gloss:0.6762 dloss:0.4120 glossQ:0.1907 exploreP:0.2607\n",
      "Episode:268 meanR:87.1900 R:158.0000 rate:0.3160 gloss:0.6742 dloss:0.4058 glossQ:0.1862 exploreP:0.2568\n",
      "Episode:269 meanR:90.1000 R:312.0000 rate:0.6240 gloss:0.6705 dloss:0.4382 glossQ:0.1857 exploreP:0.2492\n",
      "Episode:270 meanR:91.1700 R:161.0000 rate:0.3220 gloss:0.6735 dloss:0.4277 glossQ:0.1876 exploreP:0.2454\n",
      "Episode:271 meanR:95.1500 R:428.0000 rate:0.8560 gloss:0.6747 dloss:0.4201 glossQ:0.1881 exploreP:0.2355\n",
      "Episode:272 meanR:96.3100 R:148.0000 rate:0.2960 gloss:0.6697 dloss:0.4403 glossQ:0.1844 exploreP:0.2322\n",
      "Episode:273 meanR:97.7100 R:192.0000 rate:0.3840 gloss:0.6694 dloss:0.4701 glossQ:0.1895 exploreP:0.2280\n",
      "Episode:274 meanR:101.0500 R:358.0000 rate:0.7160 gloss:0.6721 dloss:0.4326 glossQ:0.1964 exploreP:0.2203\n",
      "Episode:275 meanR:103.0100 R:257.0000 rate:0.5140 gloss:0.6729 dloss:0.4731 glossQ:0.1951 exploreP:0.2150\n",
      "Episode:276 meanR:106.3400 R:365.0000 rate:0.7300 gloss:0.6685 dloss:0.4566 glossQ:0.2042 exploreP:0.2077\n",
      "Episode:277 meanR:110.7500 R:500.0000 rate:1.0000 gloss:0.6705 dloss:0.4619 glossQ:0.2026 exploreP:0.1980\n",
      "Episode:278 meanR:112.0200 R:177.0000 rate:0.3540 gloss:0.6699 dloss:0.4693 glossQ:0.2062 exploreP:0.1947\n",
      "Episode:279 meanR:113.6000 R:201.0000 rate:0.4020 gloss:0.6684 dloss:0.4419 glossQ:0.2062 exploreP:0.1910\n",
      "Episode:280 meanR:118.0200 R:476.0000 rate:0.9520 gloss:0.6707 dloss:0.4535 glossQ:0.2073 exploreP:0.1826\n",
      "Episode:281 meanR:122.4300 R:500.0000 rate:1.0000 gloss:0.6700 dloss:0.4712 glossQ:0.2113 exploreP:0.1742\n",
      "Episode:282 meanR:124.5400 R:251.0000 rate:0.5020 gloss:0.6667 dloss:0.4936 glossQ:0.2110 exploreP:0.1701\n",
      "Episode:283 meanR:128.3800 R:399.0000 rate:0.7980 gloss:0.6676 dloss:0.4796 glossQ:0.2177 exploreP:0.1639\n",
      "Episode:284 meanR:131.9900 R:401.0000 rate:0.8020 gloss:0.6687 dloss:0.5021 glossQ:0.2219 exploreP:0.1578\n",
      "Episode:285 meanR:133.8000 R:206.0000 rate:0.4120 gloss:0.6694 dloss:0.4717 glossQ:0.2238 exploreP:0.1548\n",
      "Episode:286 meanR:137.3500 R:393.0000 rate:0.7860 gloss:0.6693 dloss:0.4827 glossQ:0.2257 exploreP:0.1492\n",
      "Episode:287 meanR:141.9100 R:500.0000 rate:1.0000 gloss:0.6676 dloss:0.4999 glossQ:0.2272 exploreP:0.1424\n",
      "Episode:288 meanR:146.5900 R:500.0000 rate:1.0000 gloss:0.6656 dloss:0.5263 glossQ:0.2375 exploreP:0.1360\n",
      "Episode:289 meanR:148.7700 R:256.0000 rate:0.5120 gloss:0.6684 dloss:0.4813 glossQ:0.2375 exploreP:0.1328\n",
      "Episode:290 meanR:150.2700 R:240.0000 rate:0.4800 gloss:0.6632 dloss:0.4977 glossQ:0.2423 exploreP:0.1299\n",
      "Episode:291 meanR:152.3400 R:252.0000 rate:0.5040 gloss:0.6663 dloss:0.4922 glossQ:0.2347 exploreP:0.1269\n",
      "Episode:292 meanR:153.9400 R:210.0000 rate:0.4200 gloss:0.6696 dloss:0.4973 glossQ:0.2436 exploreP:0.1245\n",
      "Episode:293 meanR:155.5800 R:193.0000 rate:0.3860 gloss:0.6660 dloss:0.5042 glossQ:0.2375 exploreP:0.1223\n",
      "Episode:294 meanR:159.5500 R:424.0000 rate:0.8480 gloss:0.6647 dloss:0.5571 glossQ:0.2473 exploreP:0.1176\n",
      "Episode:295 meanR:162.7100 R:372.0000 rate:0.7440 gloss:0.6680 dloss:0.5307 glossQ:0.2563 exploreP:0.1137\n",
      "Episode:296 meanR:165.1500 R:360.0000 rate:0.7200 gloss:0.6651 dloss:0.5453 glossQ:0.2579 exploreP:0.1100\n",
      "Episode:297 meanR:169.9000 R:500.0000 rate:1.0000 gloss:0.6664 dloss:0.5505 glossQ:0.2692 exploreP:0.1051\n",
      "Episode:298 meanR:171.9900 R:243.0000 rate:0.4860 gloss:0.6636 dloss:0.5257 glossQ:0.2702 exploreP:0.1029\n",
      "Episode:299 meanR:175.8100 R:401.0000 rate:0.8020 gloss:0.6628 dloss:0.5417 glossQ:0.2681 exploreP:0.0992\n",
      "Episode:300 meanR:177.4600 R:222.0000 rate:0.4440 gloss:0.6638 dloss:0.5437 glossQ:0.2719 exploreP:0.0973\n",
      "Episode:301 meanR:180.8700 R:361.0000 rate:0.7220 gloss:0.6626 dloss:0.5517 glossQ:0.2725 exploreP:0.0942\n",
      "Episode:302 meanR:181.8500 R:166.0000 rate:0.3320 gloss:0.6629 dloss:0.5397 glossQ:0.2745 exploreP:0.0928\n",
      "Episode:303 meanR:182.5600 R:168.0000 rate:0.3360 gloss:0.6622 dloss:0.5251 glossQ:0.2754 exploreP:0.0914\n",
      "Episode:304 meanR:186.9500 R:500.0000 rate:1.0000 gloss:0.6615 dloss:0.5472 glossQ:0.2741 exploreP:0.0874\n",
      "Episode:305 meanR:191.7200 R:500.0000 rate:1.0000 gloss:0.6623 dloss:0.5394 glossQ:0.2759 exploreP:0.0836\n",
      "Episode:306 meanR:192.8300 R:170.0000 rate:0.3400 gloss:0.6619 dloss:0.5343 glossQ:0.2766 exploreP:0.0824\n",
      "Episode:307 meanR:194.2100 R:198.0000 rate:0.3960 gloss:0.6627 dloss:0.5385 glossQ:0.2766 exploreP:0.0810\n",
      "Episode:308 meanR:195.8800 R:224.0000 rate:0.4480 gloss:0.6625 dloss:0.5584 glossQ:0.2764 exploreP:0.0794\n",
      "Episode:309 meanR:197.8800 R:236.0000 rate:0.4720 gloss:0.6602 dloss:0.5471 glossQ:0.2808 exploreP:0.0778\n",
      "Episode:310 meanR:199.7800 R:210.0000 rate:0.4200 gloss:0.6583 dloss:0.5385 glossQ:0.2788 exploreP:0.0764\n",
      "Episode:311 meanR:203.7100 R:428.0000 rate:0.8560 gloss:0.6634 dloss:0.5660 glossQ:0.2857 exploreP:0.0736\n",
      "Episode:312 meanR:206.5400 R:339.0000 rate:0.6780 gloss:0.6631 dloss:0.5958 glossQ:0.2938 exploreP:0.0715\n",
      "Episode:313 meanR:211.2200 R:500.0000 rate:1.0000 gloss:0.6586 dloss:0.5757 glossQ:0.3026 exploreP:0.0685\n",
      "Episode:314 meanR:214.5100 R:373.0000 rate:0.7460 gloss:0.6620 dloss:0.6033 glossQ:0.3161 exploreP:0.0663\n",
      "Episode:315 meanR:219.2300 R:500.0000 rate:1.0000 gloss:0.6605 dloss:0.6020 glossQ:0.3238 exploreP:0.0636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:316 meanR:223.8200 R:500.0000 rate:1.0000 gloss:0.6587 dloss:0.5880 glossQ:0.3320 exploreP:0.0610\n",
      "Episode:317 meanR:225.7200 R:227.0000 rate:0.4540 gloss:0.6612 dloss:0.5767 glossQ:0.3341 exploreP:0.0598\n",
      "Episode:318 meanR:228.0300 R:274.0000 rate:0.5480 gloss:0.6558 dloss:0.6104 glossQ:0.3391 exploreP:0.0585\n",
      "Episode:319 meanR:232.3100 R:500.0000 rate:1.0000 gloss:0.6596 dloss:0.5895 glossQ:0.3408 exploreP:0.0561\n",
      "Episode:320 meanR:235.2400 R:327.0000 rate:0.6540 gloss:0.6614 dloss:0.5952 glossQ:0.3405 exploreP:0.0546\n",
      "Episode:321 meanR:236.5100 R:241.0000 rate:0.4820 gloss:0.6601 dloss:0.5938 glossQ:0.3405 exploreP:0.0536\n",
      "Episode:322 meanR:238.4800 R:276.0000 rate:0.5520 gloss:0.6576 dloss:0.5966 glossQ:0.3504 exploreP:0.0524\n",
      "Episode:323 meanR:240.1700 R:274.0000 rate:0.5480 gloss:0.6615 dloss:0.5838 glossQ:0.3442 exploreP:0.0512\n",
      "Episode:324 meanR:241.4000 R:186.0000 rate:0.3720 gloss:0.6546 dloss:0.6156 glossQ:0.3452 exploreP:0.0505\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "# gloss_list, dloss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111*3):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dloss_batch, glossQ_batch = [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            percentage = 0.9\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx_arr = np.arange(memory_size// batch_size)\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            states = states[rates >= (np.max(rates)*percentage)]\n",
    "            actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "            next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "            dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "            rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "            gloss, _, dloss, _ = sess.run([model.g_loss, model.g_opt, model.d_loss, model.d_opt], \n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.next_states: next_states,\n",
    "                                                       model.rewards: rewards,\n",
    "                                                       model.dones: dones, \n",
    "                                                       model.rates: rates})\n",
    "            glossQ, _ = sess.run([model.g_lossQ, model.g_optQ], feed_dict = {model.states: states, \n",
    "                                                                             model.actions: actions,\n",
    "                                                                             model.next_states: next_states,\n",
    "                                                                             model.rewards: rewards,\n",
    "                                                                             model.dones: dones, \n",
    "                                                                             model.rates: rates})\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss_batch.append(gloss)\n",
    "            glossQ_batch.append(glossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'glossQ:{:.4f}'.format(np.mean(glossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        # gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        # dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-a0b0af09d9f3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-a0b0af09d9f3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    plt.ylabel('G losses')`\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
