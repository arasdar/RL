{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.00454824 -0.20735203 -0.04576232  0.32077049] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00869528 -0.01160926 -0.03934691  0.01401429] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00892746  0.18405424 -0.03906663 -0.2908188 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00524638 -0.01048952 -0.04488301 -0.01070849] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00545617  0.18524642 -0.04509717 -0.31720789] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.00175124  0.38098072 -0.05144133 -0.62376491] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.00586837  0.57678172 -0.06391663 -0.93219498] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.01740401  0.7727052  -0.08256053 -1.24425878] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03285811  0.96878368 -0.10744571 -1.56161992] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05223378  1.16501446 -0.1386781  -1.88579697] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    labelQs = tf.placeholder(tf.float32, [None], name='labelQs')\n",
    "        \n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return actions, states, targetQs, labelQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs, labelQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    lossQtgt = tf.reduce_mean(tf.square(Qs - targetQs)) # next state, next action and nextQs\n",
    "    lossQlbl = tf.reduce_mean(tf.square(Qs - labelQs)) # current state, action, and currentQs\n",
    "    lossQtgt_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs, \n",
    "                                                                           labels=tf.nn.sigmoid(targetQs)))\n",
    "    lossQlbl_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs,\n",
    "                                                                           labels=tf.nn.sigmoid(labelQs)))\n",
    "    loss = lossQtgt + lossQlbl #+ lossQtgt_sigm + lossQlbl_sigm\n",
    "    return actions_logits, final_state, loss, lossQtgt, lossQlbl, lossQtgt_sigm, lossQlbl_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs, self.labelQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss, self.lossQtgt, self.lossQlbl, self.lossQtgt_sigm, self.lossQlbl_sigm = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, labelQs=self.labelQs, \n",
    "            cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state:', np.array(states).shape[1], \n",
    "#       'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 32                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01734429,  0.0036899 , -0.0033179 ,  0.00365944]),\n",
       " 1,\n",
       " array([ 0.01741809,  0.19885928, -0.00324471, -0.29006847]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 meanReward: 79.0000 meanLoss: 2.0324\n",
      "Episode: 1 meanReward: 60.5000 meanLoss: 3.5357\n",
      "Episode: 2 meanReward: 52.0000 meanLoss: 6.5075\n",
      "Episode: 3 meanReward: 49.2500 meanLoss: 6.7281\n",
      "Episode: 4 meanReward: 44.2000 meanLoss: 11.0835\n",
      "Episode: 5 meanReward: 39.3333 meanLoss: 18.0686\n",
      "Episode: 6 meanReward: 37.2857 meanLoss: 18.8232\n",
      "Episode: 7 meanReward: 39.6250 meanLoss: 8.5163\n",
      "Episode: 8 meanReward: 39.1111 meanLoss: 16.0877\n",
      "Episode: 9 meanReward: 38.5000 meanLoss: 16.3416\n",
      "Episode: 10 meanReward: 37.4545 meanLoss: 16.3686\n",
      "Episode: 11 meanReward: 36.3333 meanLoss: 15.9999\n",
      "Episode: 12 meanReward: 35.3846 meanLoss: 6.1176\n",
      "Episode: 13 meanReward: 34.2143 meanLoss: 5.4468\n",
      "Episode: 14 meanReward: 32.9333 meanLoss: 4.4587\n",
      "Episode: 15 meanReward: 31.6250 meanLoss: 2.4320\n",
      "Episode: 16 meanReward: 30.2941 meanLoss: 12.3756\n",
      "Episode: 17 meanReward: 29.5000 meanLoss: 13.2213\n",
      "Episode: 18 meanReward: 29.4211 meanLoss: 2.5846\n",
      "Episode: 19 meanReward: 28.4000 meanLoss: 1.3671\n",
      "Episode: 20 meanReward: 27.5714 meanLoss: 4.4858\n",
      "Episode: 21 meanReward: 26.7727 meanLoss: 5.7148\n",
      "Episode: 22 meanReward: 26.2174 meanLoss: 4.0404\n",
      "Episode: 23 meanReward: 25.9583 meanLoss: 2.2921\n",
      "Episode: 24 meanReward: 27.0800 meanLoss: 2.4490\n",
      "Episode: 25 meanReward: 29.7692 meanLoss: 8.4329\n",
      "Episode: 26 meanReward: 29.4074 meanLoss: 21.9623\n",
      "Episode: 27 meanReward: 28.7143 meanLoss: 48.7327\n",
      "Episode: 28 meanReward: 28.2414 meanLoss: 28.6087\n",
      "Episode: 29 meanReward: 29.2333 meanLoss: 10.3643\n",
      "Episode: 30 meanReward: 28.7419 meanLoss: 35.8900\n",
      "Episode: 31 meanReward: 28.3750 meanLoss: 66.7058\n",
      "Episode: 32 meanReward: 30.9688 meanLoss: 9.2540\n",
      "Episode: 33 meanReward: 30.1875 meanLoss: 47.9631\n",
      "Episode: 34 meanReward: 29.3750 meanLoss: 86.1755\n",
      "Episode: 35 meanReward: 30.3125 meanLoss: 24.1384\n",
      "Episode: 36 meanReward: 30.5000 meanLoss: 42.6198\n",
      "Episode: 37 meanReward: 30.4688 meanLoss: 57.1294\n",
      "Episode: 38 meanReward: 30.0625 meanLoss: 90.6818\n",
      "Episode: 39 meanReward: 28.7500 meanLoss: 65.4978\n",
      "Episode: 40 meanReward: 28.1562 meanLoss: 29.5105\n",
      "Episode: 41 meanReward: 27.5938 meanLoss: 12.7563\n",
      "Episode: 42 meanReward: 27.1875 meanLoss: 5.9128\n",
      "Episode: 43 meanReward: 26.8750 meanLoss: 2.6885\n",
      "Episode: 44 meanReward: 26.6562 meanLoss: 4.2775\n",
      "Episode: 45 meanReward: 26.4688 meanLoss: 3.9539\n",
      "Episode: 46 meanReward: 26.7500 meanLoss: 1.9159\n",
      "Episode: 47 meanReward: 27.6875 meanLoss: 2.0879\n",
      "Episode: 48 meanReward: 28.7812 meanLoss: 12.5922\n",
      "Episode: 49 meanReward: 28.7188 meanLoss: 40.5142\n",
      "Episode: 50 meanReward: 28.3125 meanLoss: 53.7719\n",
      "Episode: 51 meanReward: 28.7500 meanLoss: 22.5614\n",
      "Episode: 52 meanReward: 28.9375 meanLoss: 7.4934\n",
      "Episode: 53 meanReward: 29.4688 meanLoss: 3.7785\n",
      "Episode: 54 meanReward: 29.8125 meanLoss: 1.6710\n",
      "Episode: 55 meanReward: 30.4375 meanLoss: 0.9733\n",
      "Episode: 56 meanReward: 31.5938 meanLoss: 1.1865\n",
      "Episode: 57 meanReward: 29.2812 meanLoss: 18.7857\n",
      "Episode: 58 meanReward: 29.6875 meanLoss: 9.3325\n",
      "Episode: 59 meanReward: 35.4375 meanLoss: 4.7395\n",
      "Episode: 60 meanReward: 36.6562 meanLoss: 36.0316\n",
      "Episode: 61 meanReward: 35.4375 meanLoss: 29.8763\n",
      "Episode: 62 meanReward: 38.1250 meanLoss: 14.9709\n",
      "Episode: 63 meanReward: 42.4688 meanLoss: 5.5767\n",
      "Episode: 64 meanReward: 42.1250 meanLoss: 6.8859\n",
      "Episode: 65 meanReward: 44.9688 meanLoss: 17.1713\n",
      "Episode: 66 meanReward: 45.8125 meanLoss: 66.5718\n",
      "Episode: 67 meanReward: 47.9688 meanLoss: 17.8657\n",
      "Episode: 68 meanReward: 49.6875 meanLoss: 35.6873\n",
      "Episode: 69 meanReward: 52.5312 meanLoss: 17.8800\n",
      "Episode: 70 meanReward: 57.0312 meanLoss: 18.8572\n",
      "Episode: 71 meanReward: 59.2188 meanLoss: 20.2964\n",
      "Episode: 72 meanReward: 59.1250 meanLoss: 87.3658\n",
      "Episode: 73 meanReward: 68.9375 meanLoss: 11.0730\n",
      "Episode: 74 meanReward: 78.4375 meanLoss: 6.9922\n",
      "Episode: 75 meanReward: 78.9688 meanLoss: 54.7867\n",
      "Episode: 76 meanReward: 84.1250 meanLoss: 24.0236\n",
      "Episode: 77 meanReward: 88.6250 meanLoss: 1.9829\n",
      "Episode: 78 meanReward: 91.5938 meanLoss: 33.2375\n",
      "Episode: 79 meanReward: 94.8750 meanLoss: 13.5407\n",
      "Episode: 80 meanReward: 99.5625 meanLoss: 9.5784\n",
      "Episode: 81 meanReward: 101.1875 meanLoss: 97.0104\n",
      "Episode: 82 meanReward: 116.2188 meanLoss: 10.1641\n",
      "Episode: 83 meanReward: 121.9688 meanLoss: 11.7699\n",
      "Episode: 84 meanReward: 127.6562 meanLoss: 18.6033\n",
      "Episode: 85 meanReward: 136.1562 meanLoss: 20.9226\n",
      "Episode: 86 meanReward: 138.7188 meanLoss: 25.4057\n",
      "Episode: 87 meanReward: 140.8438 meanLoss: 7.6992\n",
      "Episode: 88 meanReward: 153.6250 meanLoss: 1.1306\n",
      "Episode: 89 meanReward: 156.9062 meanLoss: 60.1240\n",
      "Episode: 90 meanReward: 165.3750 meanLoss: 1.7946\n",
      "Episode: 91 meanReward: 174.9375 meanLoss: 5.8884\n",
      "Episode: 92 meanReward: 188.5625 meanLoss: 16.4481\n",
      "Episode: 93 meanReward: 192.1875 meanLoss: 26.7908\n",
      "Episode: 94 meanReward: 204.6875 meanLoss: 3.1970\n",
      "Episode: 95 meanReward: 200.5312 meanLoss: 285.6485\n",
      "Episode: 96 meanReward: 197.4062 meanLoss: 141.8421\n",
      "Episode: 97 meanReward: 199.0000 meanLoss: 17.9182\n",
      "Episode: 98 meanReward: 213.5000 meanLoss: 1.6356\n",
      "Episode: 99 meanReward: 214.3438 meanLoss: 46.8883\n",
      "Episode: 100 meanReward: 227.3125 meanLoss: 2.8628\n",
      "Episode: 101 meanReward: 239.2812 meanLoss: 15.2237\n",
      "Episode: 102 meanReward: 240.3438 meanLoss: 6.4389\n",
      "Episode: 103 meanReward: 253.3438 meanLoss: 3.7549\n",
      "Episode: 104 meanReward: 254.2188 meanLoss: 228.7094\n",
      "Episode: 105 meanReward: 244.2500 meanLoss: 180.1041\n",
      "Episode: 106 meanReward: 249.9375 meanLoss: 8.3878\n",
      "Episode: 107 meanReward: 254.2188 meanLoss: 44.2645\n",
      "Episode: 108 meanReward: 253.3438 meanLoss: 11.6317\n",
      "Episode: 109 meanReward: 264.0625 meanLoss: 2.9377\n",
      "Episode: 110 meanReward: 275.9688 meanLoss: 15.6668\n",
      "Episode: 111 meanReward: 287.0000 meanLoss: 7.9809\n",
      "Episode: 112 meanReward: 296.5625 meanLoss: 16.1720\n",
      "Episode: 113 meanReward: 308.3125 meanLoss: 19.8331\n",
      "Episode: 114 meanReward: 293.1250 meanLoss: 110.5253\n",
      "Episode: 115 meanReward: 286.9375 meanLoss: 355.8463\n",
      "Episode: 116 meanReward: 281.0312 meanLoss: 342.5157\n",
      "Episode: 117 meanReward: 280.2500 meanLoss: 16.0467\n",
      "Episode: 118 meanReward: 285.7500 meanLoss: 9.3156\n",
      "Episode: 119 meanReward: 287.2188 meanLoss: 24.6994\n",
      "Episode: 120 meanReward: 285.2812 meanLoss: 4.3840\n",
      "Episode: 121 meanReward: 296.9062 meanLoss: 2.5315\n",
      "Episode: 122 meanReward: 303.0312 meanLoss: 18.1196\n",
      "Episode: 123 meanReward: 293.2812 meanLoss: 53.1093\n",
      "Episode: 124 meanReward: 282.0312 meanLoss: 9.2902\n",
      "Episode: 125 meanReward: 281.2500 meanLoss: 6.0887\n",
      "Episode: 126 meanReward: 266.2500 meanLoss: 16.6291\n",
      "Episode: 127 meanReward: 271.2500 meanLoss: 14.7872\n",
      "Episode: 128 meanReward: 285.2812 meanLoss: 2.7411\n",
      "Episode: 129 meanReward: 283.9688 meanLoss: 76.7174\n",
      "Episode: 130 meanReward: 283.9688 meanLoss: 15.6846\n",
      "Episode: 131 meanReward: 294.3750 meanLoss: 17.1070\n",
      "Episode: 132 meanReward: 280.3438 meanLoss: 152.9575\n",
      "Episode: 133 meanReward: 265.5938 meanLoss: 40.7747\n",
      "Episode: 134 meanReward: 260.1250 meanLoss: 217.5544\n",
      "Episode: 135 meanReward: 249.1562 meanLoss: 34.6267\n",
      "Episode: 136 meanReward: 252.6562 meanLoss: 8.7403\n",
      "Episode: 137 meanReward: 261.9062 meanLoss: 4.3877\n",
      "Episode: 138 meanReward: 247.2812 meanLoss: 204.2594\n",
      "Episode: 139 meanReward: 247.0625 meanLoss: 24.9476\n",
      "Episode: 140 meanReward: 248.4688 meanLoss: 6.1857\n",
      "Episode: 141 meanReward: 233.5000 meanLoss: 278.5828\n",
      "Episode: 142 meanReward: 233.5000 meanLoss: 12.7850\n",
      "Episode: 143 meanReward: 222.7188 meanLoss: 64.3621\n",
      "Episode: 144 meanReward: 207.6875 meanLoss: 287.1746\n",
      "Episode: 145 meanReward: 195.1250 meanLoss: 203.8249\n",
      "Episode: 146 meanReward: 196.6875 meanLoss: 98.9726\n",
      "Episode: 147 meanReward: 204.0938 meanLoss: 30.2507\n",
      "Episode: 148 meanReward: 204.3750 meanLoss: 187.5133\n",
      "Episode: 149 meanReward: 211.4375 meanLoss: 15.1537\n",
      "Episode: 150 meanReward: 207.5312 meanLoss: 56.0882\n",
      "Episode: 151 meanReward: 207.2500 meanLoss: 13.9420\n",
      "Episode: 152 meanReward: 201.8750 meanLoss: 7.6475\n",
      "Episode: 153 meanReward: 201.8750 meanLoss: 7.9179\n",
      "Episode: 154 meanReward: 186.6250 meanLoss: 356.3810\n",
      "Episode: 155 meanReward: 181.9062 meanLoss: 252.5845\n",
      "Episode: 156 meanReward: 193.4688 meanLoss: 14.8244\n",
      "Episode: 157 meanReward: 194.9062 meanLoss: 41.7703\n",
      "Episode: 158 meanReward: 209.9062 meanLoss: 7.0708\n",
      "Episode: 159 meanReward: 212.4062 meanLoss: 37.1083\n",
      "Episode: 160 meanReward: 204.0625 meanLoss: 39.2455\n",
      "Episode: 161 meanReward: 208.5625 meanLoss: 8.6079\n",
      "Episode: 162 meanReward: 200.2188 meanLoss: 15.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 163 meanReward: 190.1250 meanLoss: 19.7406\n",
      "Episode: 164 meanReward: 199.2812 meanLoss: 7.0148\n",
      "Episode: 165 meanReward: 203.8438 meanLoss: 22.8519\n",
      "Episode: 166 meanReward: 215.9688 meanLoss: 4.7226\n",
      "Episode: 167 meanReward: 226.9375 meanLoss: 3.1287\n",
      "Episode: 168 meanReward: 222.7188 meanLoss: 192.2695\n",
      "Episode: 169 meanReward: 228.7812 meanLoss: 10.2643\n",
      "Episode: 170 meanReward: 237.4062 meanLoss: 29.7810\n",
      "Episode: 171 meanReward: 248.0000 meanLoss: 64.6852\n",
      "Episode: 172 meanReward: 257.4062 meanLoss: 16.4301\n",
      "Episode: 173 meanReward: 272.3750 meanLoss: 13.1412\n",
      "Episode: 174 meanReward: 268.1875 meanLoss: 28.8514\n",
      "Episode: 175 meanReward: 278.9688 meanLoss: 11.6913\n",
      "Episode: 176 meanReward: 285.6562 meanLoss: 1029.2108\n",
      "Episode: 177 meanReward: 287.6875 meanLoss: 470.1177\n",
      "Episode: 178 meanReward: 288.5625 meanLoss: 491.1910\n",
      "Episode: 179 meanReward: 281.1250 meanLoss: 519.3581\n",
      "Episode: 180 meanReward: 281.0000 meanLoss: 292.4215\n",
      "Episode: 181 meanReward: 265.7500 meanLoss: 52.6490\n",
      "Episode: 182 meanReward: 261.1250 meanLoss: 24.7062\n",
      "Episode: 183 meanReward: 256.8438 meanLoss: 10.9340\n",
      "Episode: 184 meanReward: 248.8438 meanLoss: 17.7505\n",
      "Episode: 185 meanReward: 233.5312 meanLoss: 16.3902\n",
      "Episode: 186 meanReward: 233.4062 meanLoss: 13.2593\n",
      "Episode: 187 meanReward: 232.5312 meanLoss: 12.2335\n",
      "Episode: 188 meanReward: 217.2188 meanLoss: 15.2877\n",
      "Episode: 189 meanReward: 212.6250 meanLoss: 7.0665\n",
      "Episode: 190 meanReward: 200.4375 meanLoss: 590.7756\n",
      "Episode: 191 meanReward: 192.4688 meanLoss: 735.7996\n",
      "Episode: 192 meanReward: 185.5000 meanLoss: 467.7666\n",
      "Episode: 193 meanReward: 177.6250 meanLoss: 192.0287\n",
      "Episode: 194 meanReward: 170.6250 meanLoss: 17.3630\n",
      "Episode: 195 meanReward: 165.7812 meanLoss: 4.6970\n",
      "Episode: 196 meanReward: 155.4688 meanLoss: 11.8468\n",
      "Episode: 197 meanReward: 150.7812 meanLoss: 6.0622\n",
      "Episode: 198 meanReward: 138.4688 meanLoss: 4.6173\n",
      "Episode: 199 meanReward: 123.1875 meanLoss: 4.1447\n",
      "Episode: 200 meanReward: 122.9375 meanLoss: 2.9369\n",
      "Episode: 201 meanReward: 107.7188 meanLoss: 3.8239\n",
      "Episode: 202 meanReward: 98.4062 meanLoss: 4.4992\n",
      "Episode: 203 meanReward: 83.0625 meanLoss: 7.9163\n",
      "Episode: 204 meanReward: 67.7500 meanLoss: 5.8317\n",
      "Episode: 205 meanReward: 52.4062 meanLoss: 4.8350\n",
      "Episode: 206 meanReward: 41.2500 meanLoss: 2.6316\n",
      "Episode: 207 meanReward: 26.0938 meanLoss: 3.4458\n",
      "Episode: 208 meanReward: 19.3750 meanLoss: 1.9281\n",
      "Episode: 209 meanReward: 16.5625 meanLoss: 2.5635\n",
      "Episode: 210 meanReward: 14.3750 meanLoss: 2.1726\n",
      "Episode: 211 meanReward: 14.5000 meanLoss: 2.6825\n",
      "Episode: 212 meanReward: 14.4062 meanLoss: 6.1620\n",
      "Episode: 213 meanReward: 14.3125 meanLoss: 5.6017\n",
      "Episode: 214 meanReward: 14.3438 meanLoss: 3.3044\n",
      "Episode: 215 meanReward: 14.4062 meanLoss: 2.5216\n",
      "Episode: 216 meanReward: 14.5938 meanLoss: 1.8173\n",
      "Episode: 217 meanReward: 14.7812 meanLoss: 2.1806\n",
      "Episode: 218 meanReward: 15.1562 meanLoss: 2.7472\n",
      "Episode: 219 meanReward: 15.3125 meanLoss: 2.5848\n",
      "Episode: 220 meanReward: 15.3750 meanLoss: 1.8957\n",
      "Episode: 221 meanReward: 15.5938 meanLoss: 2.4512\n",
      "Episode: 222 meanReward: 13.1250 meanLoss: 4.3616\n",
      "Episode: 223 meanReward: 13.5312 meanLoss: 12.2447\n",
      "Episode: 224 meanReward: 13.8438 meanLoss: 5.2410\n",
      "Episode: 225 meanReward: 13.8750 meanLoss: 6.1211\n",
      "Episode: 226 meanReward: 14.0938 meanLoss: 4.9193\n",
      "Episode: 227 meanReward: 13.8125 meanLoss: 4.0119\n",
      "Episode: 228 meanReward: 13.6562 meanLoss: 3.8933\n",
      "Episode: 229 meanReward: 13.5312 meanLoss: 5.6623\n",
      "Episode: 230 meanReward: 13.5312 meanLoss: 4.3142\n",
      "Episode: 231 meanReward: 13.5000 meanLoss: 3.7256\n",
      "Episode: 232 meanReward: 13.4688 meanLoss: 2.5928\n",
      "Episode: 233 meanReward: 13.3750 meanLoss: 2.0026\n",
      "Episode: 234 meanReward: 13.3750 meanLoss: 2.1632\n",
      "Episode: 235 meanReward: 13.4062 meanLoss: 1.8318\n",
      "Episode: 236 meanReward: 13.4062 meanLoss: 2.0927\n",
      "Episode: 237 meanReward: 13.4375 meanLoss: 3.0256\n",
      "Episode: 238 meanReward: 13.4375 meanLoss: 3.6194\n",
      "Episode: 239 meanReward: 13.2188 meanLoss: 3.2285\n",
      "Episode: 240 meanReward: 12.9688 meanLoss: 2.8084\n",
      "Episode: 241 meanReward: 13.2188 meanLoss: 2.2169\n",
      "Episode: 242 meanReward: 13.7812 meanLoss: 2.2007\n",
      "Episode: 243 meanReward: 13.8438 meanLoss: 2.2231\n",
      "Episode: 244 meanReward: 14.1250 meanLoss: 2.8854\n",
      "Episode: 245 meanReward: 14.5625 meanLoss: 2.7606\n",
      "Episode: 246 meanReward: 15.2500 meanLoss: 2.8249\n",
      "Episode: 247 meanReward: 15.5938 meanLoss: 3.2850\n",
      "Episode: 248 meanReward: 15.7812 meanLoss: 5.1002\n",
      "Episode: 249 meanReward: 15.8125 meanLoss: 5.1203\n",
      "Episode: 250 meanReward: 15.7812 meanLoss: 5.2937\n",
      "Episode: 251 meanReward: 15.8125 meanLoss: 3.5252\n",
      "Episode: 252 meanReward: 15.9375 meanLoss: 3.5994\n",
      "Episode: 253 meanReward: 16.0625 meanLoss: 2.4507\n",
      "Episode: 254 meanReward: 15.5938 meanLoss: 2.4122\n",
      "Episode: 255 meanReward: 15.4062 meanLoss: 2.6425\n",
      "Episode: 256 meanReward: 15.1250 meanLoss: 2.1647\n",
      "Episode: 257 meanReward: 15.4688 meanLoss: 2.4584\n",
      "Episode: 258 meanReward: 15.2812 meanLoss: 1.2765\n",
      "Episode: 259 meanReward: 15.3125 meanLoss: 3.0074\n",
      "Episode: 260 meanReward: 15.5625 meanLoss: 2.4575\n",
      "Episode: 261 meanReward: 16.0000 meanLoss: 1.6252\n",
      "Episode: 262 meanReward: 16.2500 meanLoss: 1.2419\n",
      "Episode: 263 meanReward: 16.5938 meanLoss: 1.0643\n",
      "Episode: 264 meanReward: 17.0938 meanLoss: 1.8211\n",
      "Episode: 265 meanReward: 17.1875 meanLoss: 4.2587\n",
      "Episode: 266 meanReward: 17.2188 meanLoss: 4.2152\n",
      "Episode: 267 meanReward: 17.3750 meanLoss: 3.4208\n",
      "Episode: 268 meanReward: 18.0938 meanLoss: 2.4620\n",
      "Episode: 269 meanReward: 18.5625 meanLoss: 1.2306\n",
      "Episode: 270 meanReward: 18.9375 meanLoss: 1.6516\n",
      "Episode: 271 meanReward: 19.2188 meanLoss: 2.1325\n",
      "Episode: 272 meanReward: 19.4375 meanLoss: 3.0081\n",
      "Episode: 273 meanReward: 19.2188 meanLoss: 2.5543\n",
      "Episode: 274 meanReward: 18.4688 meanLoss: 2.2607\n",
      "Episode: 275 meanReward: 18.7812 meanLoss: 1.8264\n",
      "Episode: 276 meanReward: 19.2188 meanLoss: 2.2064\n",
      "Episode: 277 meanReward: 19.0312 meanLoss: 1.9600\n",
      "Episode: 278 meanReward: 19.0938 meanLoss: 3.2881\n",
      "Episode: 279 meanReward: 19.3125 meanLoss: 2.4797\n",
      "Episode: 280 meanReward: 19.7188 meanLoss: 1.5862\n",
      "Episode: 281 meanReward: 20.3750 meanLoss: 2.1621\n",
      "Episode: 282 meanReward: 20.6562 meanLoss: 2.7022\n",
      "Episode: 283 meanReward: 20.9062 meanLoss: 2.4840\n",
      "Episode: 284 meanReward: 21.3125 meanLoss: 3.5648\n",
      "Episode: 285 meanReward: 21.2500 meanLoss: 3.7555\n",
      "Episode: 286 meanReward: 21.7812 meanLoss: 3.7384\n",
      "Episode: 287 meanReward: 22.0625 meanLoss: 1.1668\n",
      "Episode: 288 meanReward: 22.4062 meanLoss: 1.8675\n",
      "Episode: 289 meanReward: 22.5938 meanLoss: 1.6148\n",
      "Episode: 290 meanReward: 22.6250 meanLoss: 2.7424\n",
      "Episode: 291 meanReward: 22.5938 meanLoss: 3.3063\n",
      "Episode: 292 meanReward: 22.7188 meanLoss: 3.3399\n",
      "Episode: 293 meanReward: 23.1875 meanLoss: 1.4836\n",
      "Episode: 294 meanReward: 23.2188 meanLoss: 2.9630\n",
      "Episode: 295 meanReward: 22.9062 meanLoss: 2.5040\n",
      "Episode: 296 meanReward: 23.2500 meanLoss: 1.4465\n",
      "Episode: 297 meanReward: 24.0000 meanLoss: 1.4229\n",
      "Episode: 298 meanReward: 25.0000 meanLoss: 2.0537\n",
      "Episode: 299 meanReward: 25.3750 meanLoss: 2.5473\n",
      "Episode: 300 meanReward: 25.1250 meanLoss: 2.6693\n",
      "Episode: 301 meanReward: 26.1562 meanLoss: 1.8644\n",
      "Episode: 302 meanReward: 26.3125 meanLoss: 1.6417\n",
      "Episode: 303 meanReward: 27.5938 meanLoss: 1.6266\n",
      "Episode: 304 meanReward: 42.6875 meanLoss: 3.1078\n",
      "Episode: 305 meanReward: 57.8125 meanLoss: 20.3936\n",
      "Episode: 306 meanReward: 73.0625 meanLoss: 14.0739\n",
      "Episode: 307 meanReward: 76.1250 meanLoss: 58.3520\n",
      "Episode: 308 meanReward: 90.6562 meanLoss: 1.8129\n",
      "Episode: 309 meanReward: 92.7500 meanLoss: 99.4848\n",
      "Episode: 310 meanReward: 107.2812 meanLoss: 4.3103\n",
      "Episode: 311 meanReward: 109.6250 meanLoss: 89.7748\n",
      "Episode: 312 meanReward: 108.9375 meanLoss: 195.7103\n",
      "Episode: 313 meanReward: 108.1562 meanLoss: 131.9381\n",
      "Episode: 314 meanReward: 122.9062 meanLoss: 5.2899\n",
      "Episode: 315 meanReward: 137.8125 meanLoss: 14.5204\n",
      "Episode: 316 meanReward: 138.4375 meanLoss: 144.2461\n",
      "Episode: 317 meanReward: 142.6562 meanLoss: 131.0849\n",
      "Episode: 318 meanReward: 142.0312 meanLoss: 146.5189\n",
      "Episode: 319 meanReward: 141.7188 meanLoss: 261.5040\n",
      "Episode: 320 meanReward: 141.5000 meanLoss: 211.1926\n",
      "Episode: 321 meanReward: 141.4688 meanLoss: 85.6757\n",
      "Episode: 322 meanReward: 142.8438 meanLoss: 25.0353\n",
      "Episode: 323 meanReward: 145.3438 meanLoss: 6.0467\n",
      "Episode: 324 meanReward: 146.1562 meanLoss: 2.1203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 325 meanReward: 146.4062 meanLoss: 1.8729\n",
      "Episode: 326 meanReward: 147.1562 meanLoss: 1.8504\n",
      "Episode: 327 meanReward: 149.2812 meanLoss: 3.3351\n",
      "Episode: 328 meanReward: 149.9375 meanLoss: 1.9840\n",
      "Episode: 329 meanReward: 152.0938 meanLoss: 5.0065\n",
      "Episode: 330 meanReward: 166.3750 meanLoss: 4.7725\n",
      "Episode: 331 meanReward: 181.1562 meanLoss: 7.6058\n",
      "Episode: 332 meanReward: 196.0000 meanLoss: 13.8338\n",
      "Episode: 333 meanReward: 209.8125 meanLoss: 16.9710\n",
      "Episode: 334 meanReward: 224.6250 meanLoss: 18.2299\n",
      "Episode: 335 meanReward: 230.7812 meanLoss: 26.3504\n",
      "Episode: 336 meanReward: 229.6250 meanLoss: 22.3279\n",
      "Episode: 337 meanReward: 214.9375 meanLoss: 337.4884\n",
      "Episode: 338 meanReward: 214.9375 meanLoss: 9.4618\n",
      "Episode: 339 meanReward: 213.7188 meanLoss: 89.6233\n",
      "Episode: 340 meanReward: 199.8438 meanLoss: 12.6962\n",
      "Episode: 341 meanReward: 199.2188 meanLoss: 16.6122\n",
      "Episode: 342 meanReward: 199.2188 meanLoss: 3.3352\n",
      "Episode: 343 meanReward: 211.5938 meanLoss: 17.0395\n",
      "Episode: 344 meanReward: 213.5312 meanLoss: 124.7571\n",
      "Episode: 345 meanReward: 228.7500 meanLoss: 15.8460\n",
      "Episode: 346 meanReward: 225.3125 meanLoss: 26.5308\n",
      "Episode: 347 meanReward: 221.0000 meanLoss: 10.5301\n",
      "Episode: 348 meanReward: 235.0938 meanLoss: 15.8219\n",
      "Episode: 349 meanReward: 245.5312 meanLoss: 25.4550\n",
      "Episode: 350 meanReward: 251.6562 meanLoss: 38.8921\n",
      "Episode: 351 meanReward: 260.8125 meanLoss: 30.9235\n",
      "Episode: 352 meanReward: 275.9688 meanLoss: 17.1574\n",
      "Episode: 353 meanReward: 275.8125 meanLoss: 344.3158\n",
      "Episode: 354 meanReward: 274.5938 meanLoss: 314.6876\n",
      "Episode: 355 meanReward: 287.3125 meanLoss: 7.2589\n",
      "Episode: 356 meanReward: 301.4688 meanLoss: 16.3762\n",
      "Episode: 357 meanReward: 303.8750 meanLoss: 64.5468\n",
      "Episode: 358 meanReward: 307.9062 meanLoss: 38.1798\n",
      "Episode: 359 meanReward: 305.9062 meanLoss: 270.7079\n",
      "Episode: 360 meanReward: 319.7500 meanLoss: 19.6205\n",
      "Episode: 361 meanReward: 332.0625 meanLoss: 12.1747\n",
      "Episode: 362 meanReward: 316.8438 meanLoss: 82.3711\n",
      "Episode: 363 meanReward: 301.5938 meanLoss: 191.2992\n",
      "Episode: 364 meanReward: 286.2812 meanLoss: 271.3136\n",
      "Episode: 365 meanReward: 270.9688 meanLoss: 191.0527\n",
      "Episode: 366 meanReward: 255.7188 meanLoss: 113.2179\n",
      "Episode: 367 meanReward: 249.6562 meanLoss: 134.5063\n",
      "Episode: 368 meanReward: 250.8125 meanLoss: 12.8912\n",
      "Episode: 369 meanReward: 256.9062 meanLoss: 19.1311\n",
      "Episode: 370 meanReward: 250.3750 meanLoss: 20.3500\n",
      "Episode: 371 meanReward: 250.1562 meanLoss: 78.9298\n",
      "Episode: 372 meanReward: 250.3438 meanLoss: 135.1821\n",
      "Episode: 373 meanReward: 263.9688 meanLoss: 10.1479\n",
      "Episode: 374 meanReward: 258.6875 meanLoss: 25.2436\n",
      "Episode: 375 meanReward: 245.3750 meanLoss: 117.0219\n",
      "Episode: 376 meanReward: 258.6562 meanLoss: 16.3495\n",
      "Episode: 377 meanReward: 248.8125 meanLoss: 41.2938\n",
      "Episode: 378 meanReward: 252.2500 meanLoss: 16.5437\n",
      "Episode: 379 meanReward: 250.0000 meanLoss: 29.2047\n",
      "Episode: 380 meanReward: 250.0000 meanLoss: 16.7471\n",
      "Episode: 381 meanReward: 235.5000 meanLoss: 295.6552\n",
      "Episode: 382 meanReward: 244.5938 meanLoss: 12.7701\n",
      "Episode: 383 meanReward: 237.0625 meanLoss: 127.0685\n",
      "Episode: 384 meanReward: 237.0625 meanLoss: 9.6747\n",
      "Episode: 385 meanReward: 237.2500 meanLoss: 269.0383\n",
      "Episode: 386 meanReward: 252.3750 meanLoss: 18.5100\n",
      "Episode: 387 meanReward: 241.0000 meanLoss: 70.5471\n",
      "Episode: 388 meanReward: 241.0000 meanLoss: 6.7008\n",
      "Episode: 389 meanReward: 241.5625 meanLoss: 58.5628\n",
      "Episode: 390 meanReward: 238.6875 meanLoss: 40.6395\n",
      "Episode: 391 meanReward: 242.2500 meanLoss: 10.2175\n",
      "Episode: 392 meanReward: 231.4375 meanLoss: 17.4463\n",
      "Episode: 393 meanReward: 225.4688 meanLoss: 6.8502\n",
      "Episode: 394 meanReward: 240.6875 meanLoss: 6.0754\n",
      "Episode: 395 meanReward: 246.1250 meanLoss: 42.9019\n",
      "Episode: 396 meanReward: 261.4375 meanLoss: 18.8599\n",
      "Episode: 397 meanReward: 276.7500 meanLoss: 20.8158\n",
      "Episode: 398 meanReward: 280.8438 meanLoss: 64.9037\n",
      "Episode: 399 meanReward: 280.1250 meanLoss: 201.6542\n",
      "Episode: 400 meanReward: 274.9062 meanLoss: 21.5242\n",
      "Episode: 401 meanReward: 271.4688 meanLoss: 53.6094\n",
      "Episode: 402 meanReward: 264.0938 meanLoss: 127.4814\n",
      "Episode: 403 meanReward: 277.3438 meanLoss: 4.9974\n",
      "Episode: 404 meanReward: 276.5625 meanLoss: 257.8277\n",
      "Episode: 405 meanReward: 276.5625 meanLoss: 16.1030\n",
      "Episode: 406 meanReward: 278.2812 meanLoss: 23.9038\n",
      "Episode: 407 meanReward: 276.3438 meanLoss: 277.6156\n",
      "Episode: 408 meanReward: 263.1562 meanLoss: 123.7225\n",
      "Episode: 409 meanReward: 258.5312 meanLoss: 83.7738\n",
      "Episode: 410 meanReward: 245.1562 meanLoss: 78.3570\n",
      "Episode: 411 meanReward: 238.2812 meanLoss: 11.3473\n",
      "Episode: 412 meanReward: 225.1875 meanLoss: 5.4856\n",
      "Episode: 413 meanReward: 230.5625 meanLoss: 5.1731\n",
      "Episode: 414 meanReward: 220.7500 meanLoss: 6.2076\n",
      "Episode: 415 meanReward: 222.6250 meanLoss: 25.8362\n",
      "Episode: 416 meanReward: 215.0000 meanLoss: 3.8803\n",
      "Episode: 417 meanReward: 218.6562 meanLoss: 6.2742\n",
      "Episode: 418 meanReward: 210.4375 meanLoss: 7.0138\n",
      "Episode: 419 meanReward: 215.7188 meanLoss: 4.7186\n",
      "Episode: 420 meanReward: 207.3750 meanLoss: 26.2756\n",
      "Episode: 421 meanReward: 203.5000 meanLoss: 84.0805\n",
      "Episode: 422 meanReward: 201.3750 meanLoss: 296.8746\n",
      "Episode: 423 meanReward: 201.8750 meanLoss: 70.8382\n",
      "Episode: 424 meanReward: 212.6875 meanLoss: 3.5024\n",
      "Episode: 425 meanReward: 210.0625 meanLoss: 40.6577\n",
      "Episode: 426 meanReward: 203.6875 meanLoss: 4.3482\n",
      "Episode: 427 meanReward: 213.5000 meanLoss: 3.5614\n",
      "Episode: 428 meanReward: 202.0625 meanLoss: 13.8315\n",
      "Episode: 429 meanReward: 187.0938 meanLoss: 35.4858\n",
      "Episode: 430 meanReward: 183.0312 meanLoss: 274.0441\n",
      "Episode: 431 meanReward: 182.2500 meanLoss: 312.0603\n",
      "Episode: 432 meanReward: 180.5938 meanLoss: 30.7530\n",
      "Episode: 433 meanReward: 180.9062 meanLoss: 18.3666\n",
      "Episode: 434 meanReward: 185.6562 meanLoss: 11.5994\n",
      "Episode: 435 meanReward: 185.6562 meanLoss: 2.0840\n",
      "Episode: 436 meanReward: 193.8125 meanLoss: 32.1330\n",
      "Episode: 437 meanReward: 185.0938 meanLoss: 20.1884\n",
      "Episode: 438 meanReward: 180.0000 meanLoss: 12.5961\n",
      "Episode: 439 meanReward: 195.2500 meanLoss: 1.8260\n",
      "Episode: 440 meanReward: 198.8750 meanLoss: 55.7457\n",
      "Episode: 441 meanReward: 202.6562 meanLoss: 12.4951\n",
      "Episode: 442 meanReward: 215.0625 meanLoss: 3.7650\n",
      "Episode: 443 meanReward: 221.9062 meanLoss: 7.4431\n",
      "Episode: 444 meanReward: 227.5000 meanLoss: 3.5779\n",
      "Episode: 445 meanReward: 228.4688 meanLoss: 1.9625\n",
      "Episode: 446 meanReward: 238.2812 meanLoss: 1.4778\n",
      "Episode: 447 meanReward: 235.4062 meanLoss: 270.4379\n",
      "Episode: 448 meanReward: 243.0312 meanLoss: 15.2087\n",
      "Episode: 449 meanReward: 238.9688 meanLoss: 338.0910\n",
      "Episode: 450 meanReward: 237.1250 meanLoss: 51.5257\n",
      "Episode: 451 meanReward: 230.0938 meanLoss: 19.4357\n",
      "Episode: 452 meanReward: 223.5000 meanLoss: 27.5618\n",
      "Episode: 453 meanReward: 225.2188 meanLoss: 55.9439\n",
      "Episode: 454 meanReward: 227.5625 meanLoss: 8.0456\n",
      "Episode: 455 meanReward: 226.9375 meanLoss: 4.1176\n",
      "Episode: 456 meanReward: 217.6250 meanLoss: 3.9707\n",
      "Episode: 457 meanReward: 214.6875 meanLoss: 10.6680\n",
      "Episode: 458 meanReward: 211.6250 meanLoss: 4.6660\n",
      "Episode: 459 meanReward: 207.8125 meanLoss: 3.4244\n",
      "Episode: 460 meanReward: 206.6250 meanLoss: 35.2370\n",
      "Episode: 461 meanReward: 221.5938 meanLoss: 1.5045\n",
      "Episode: 462 meanReward: 223.5938 meanLoss: 119.8162\n",
      "Episode: 463 meanReward: 238.8125 meanLoss: 3.1535\n",
      "Episode: 464 meanReward: 245.3438 meanLoss: 19.4294\n",
      "Episode: 465 meanReward: 241.7812 meanLoss: 194.7121\n",
      "Episode: 466 meanReward: 239.8750 meanLoss: 34.5943\n",
      "Episode: 467 meanReward: 225.0312 meanLoss: 246.6120\n",
      "Episode: 468 meanReward: 216.3750 meanLoss: 354.7110\n",
      "Episode: 469 meanReward: 214.9062 meanLoss: 41.3658\n",
      "Episode: 470 meanReward: 210.5312 meanLoss: 106.3006\n",
      "Episode: 471 meanReward: 201.2500 meanLoss: 5.2739\n",
      "Episode: 472 meanReward: 202.0938 meanLoss: 11.7915\n",
      "Episode: 473 meanReward: 202.6875 meanLoss: 14.3888\n",
      "Episode: 474 meanReward: 190.6562 meanLoss: 35.7979\n",
      "Episode: 475 meanReward: 197.2500 meanLoss: 6.2795\n",
      "Episode: 476 meanReward: 191.1875 meanLoss: 137.4386\n",
      "Episode: 477 meanReward: 187.9375 meanLoss: 9.3071\n",
      "Episode: 478 meanReward: 172.7500 meanLoss: 71.0868\n",
      "Episode: 479 meanReward: 176.2500 meanLoss: 22.3132\n",
      "Episode: 480 meanReward: 176.2500 meanLoss: 6.1374\n",
      "Episode: 481 meanReward: 191.4375 meanLoss: 20.5850\n",
      "Episode: 482 meanReward: 201.5000 meanLoss: 19.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 483 meanReward: 204.3438 meanLoss: 42.5797\n",
      "Episode: 484 meanReward: 208.7500 meanLoss: 11.0433\n",
      "Episode: 485 meanReward: 206.8750 meanLoss: 38.7882\n",
      "Episode: 486 meanReward: 204.5312 meanLoss: 190.6188\n",
      "Episode: 487 meanReward: 200.9375 meanLoss: 232.6623\n",
      "Episode: 488 meanReward: 199.3750 meanLoss: 36.3202\n",
      "Episode: 489 meanReward: 200.6875 meanLoss: 27.4150\n",
      "Episode: 490 meanReward: 210.1250 meanLoss: 2.2740\n",
      "Episode: 491 meanReward: 204.2812 meanLoss: 49.5616\n",
      "Episode: 492 meanReward: 205.3438 meanLoss: 4.6272\n",
      "Episode: 493 meanReward: 205.3438 meanLoss: 9.3414\n",
      "Episode: 494 meanReward: 218.5625 meanLoss: 20.5735\n",
      "Episode: 495 meanReward: 218.5625 meanLoss: 23.8873\n",
      "Episode: 496 meanReward: 218.9062 meanLoss: 19.7140\n",
      "Episode: 497 meanReward: 234.1875 meanLoss: 19.1904\n",
      "Episode: 498 meanReward: 230.2500 meanLoss: 303.4623\n",
      "Episode: 499 meanReward: 229.9375 meanLoss: 488.0131\n",
      "Episode: 500 meanReward: 232.4062 meanLoss: 87.7040\n",
      "Episode: 501 meanReward: 228.5312 meanLoss: 100.9845\n",
      "Episode: 502 meanReward: 229.6875 meanLoss: 32.2839\n",
      "Episode: 503 meanReward: 227.5625 meanLoss: 16.1408\n",
      "Episode: 504 meanReward: 224.6250 meanLoss: 7.5647\n",
      "Episode: 505 meanReward: 226.8125 meanLoss: 5.9006\n",
      "Episode: 506 meanReward: 230.0938 meanLoss: 9.8572\n",
      "Episode: 507 meanReward: 218.5000 meanLoss: 27.2098\n",
      "Episode: 508 meanReward: 220.6562 meanLoss: 11.3541\n",
      "Episode: 509 meanReward: 232.4688 meanLoss: 3.9828\n",
      "Episode: 510 meanReward: 233.3125 meanLoss: 209.2316\n",
      "Episode: 511 meanReward: 236.8125 meanLoss: 16.3807\n",
      "Episode: 512 meanReward: 225.4375 meanLoss: 13.1944\n",
      "Episode: 513 meanReward: 214.6875 meanLoss: 8.9975\n",
      "Episode: 514 meanReward: 210.4688 meanLoss: 5.4591\n",
      "Episode: 515 meanReward: 207.9375 meanLoss: 37.4912\n",
      "Episode: 516 meanReward: 206.4688 meanLoss: 9.3185\n",
      "Episode: 517 meanReward: 211.4375 meanLoss: 7.4464\n",
      "Episode: 518 meanReward: 216.2812 meanLoss: 7.2568\n",
      "Episode: 519 meanReward: 219.9375 meanLoss: 17.8863\n",
      "Episode: 520 meanReward: 219.8125 meanLoss: 11.2860\n",
      "Episode: 521 meanReward: 215.0000 meanLoss: 26.8802\n",
      "Episode: 522 meanReward: 199.7500 meanLoss: 219.0611\n",
      "Episode: 523 meanReward: 194.0938 meanLoss: 345.1682\n",
      "Episode: 524 meanReward: 205.6562 meanLoss: 14.4337\n",
      "Episode: 525 meanReward: 205.6562 meanLoss: 18.7250\n",
      "Episode: 526 meanReward: 193.6250 meanLoss: 78.0919\n",
      "Episode: 527 meanReward: 190.9375 meanLoss: 9.6078\n",
      "Episode: 528 meanReward: 190.9375 meanLoss: 11.5655\n",
      "Episode: 529 meanReward: 178.0000 meanLoss: 119.0127\n",
      "Episode: 530 meanReward: 186.1562 meanLoss: 15.6497\n",
      "Episode: 531 meanReward: 201.3125 meanLoss: 13.5059\n",
      "Episode: 532 meanReward: 213.8125 meanLoss: 17.6752\n",
      "Episode: 533 meanReward: 227.8750 meanLoss: 18.8701\n",
      "Episode: 534 meanReward: 229.2188 meanLoss: 51.8481\n",
      "Episode: 535 meanReward: 240.6250 meanLoss: 9.5150\n",
      "Episode: 536 meanReward: 252.2812 meanLoss: 18.3932\n",
      "Episode: 537 meanReward: 260.1875 meanLoss: 14.2728\n",
      "Episode: 538 meanReward: 269.9062 meanLoss: 18.0444\n",
      "Episode: 539 meanReward: 270.5000 meanLoss: 60.7683\n",
      "Episode: 540 meanReward: 281.9062 meanLoss: 9.0847\n",
      "Episode: 541 meanReward: 273.6562 meanLoss: 39.4268\n",
      "Episode: 542 meanReward: 288.0000 meanLoss: 6.5776\n",
      "Episode: 543 meanReward: 295.5625 meanLoss: 18.3931\n",
      "Episode: 544 meanReward: 306.9375 meanLoss: 18.7224\n",
      "Episode: 545 meanReward: 317.5312 meanLoss: 18.7257\n",
      "Episode: 546 meanReward: 315.7188 meanLoss: 14.8648\n",
      "Episode: 547 meanReward: 319.0625 meanLoss: 20.4102\n",
      "Episode: 548 meanReward: 331.0625 meanLoss: 7.9788\n",
      "Episode: 549 meanReward: 341.3750 meanLoss: 17.4748\n",
      "Episode: 550 meanReward: 351.8125 meanLoss: 12.5156\n",
      "Episode: 551 meanReward: 363.4688 meanLoss: 16.8576\n",
      "Episode: 552 meanReward: 374.4688 meanLoss: 18.8367\n",
      "Episode: 553 meanReward: 376.4688 meanLoss: 111.9356\n",
      "Episode: 554 meanReward: 391.7188 meanLoss: 5.2100\n",
      "Episode: 555 meanReward: 403.4062 meanLoss: 19.5063\n",
      "Episode: 556 meanReward: 388.2188 meanLoss: 75.8061\n",
      "Episode: 557 meanReward: 372.9062 meanLoss: 129.8067\n",
      "Episode: 558 meanReward: 384.4375 meanLoss: 6.0650\n",
      "Episode: 559 meanReward: 377.2188 meanLoss: 29.6977\n",
      "Episode: 560 meanReward: 373.2500 meanLoss: 7.5150\n",
      "Episode: 561 meanReward: 386.1875 meanLoss: 6.3046\n",
      "Episode: 562 meanReward: 383.3438 meanLoss: 27.6224\n",
      "Episode: 563 meanReward: 374.1250 meanLoss: 9.4370\n",
      "Episode: 564 meanReward: 363.6875 meanLoss: 15.7722\n",
      "Episode: 565 meanReward: 357.6562 meanLoss: 7.2278\n",
      "Episode: 566 meanReward: 356.9062 meanLoss: 10.7554\n",
      "Episode: 567 meanReward: 345.4375 meanLoss: 10.3244\n",
      "Episode: 568 meanReward: 333.4688 meanLoss: 12.8209\n",
      "Episode: 569 meanReward: 321.8750 meanLoss: 3.5161\n",
      "Episode: 570 meanReward: 310.7188 meanLoss: 3.1612\n",
      "Episode: 571 meanReward: 311.0938 meanLoss: 2.4747\n",
      "Episode: 572 meanReward: 303.0000 meanLoss: 2.9065\n",
      "Episode: 573 meanReward: 301.3750 meanLoss: 4.7957\n",
      "Episode: 574 meanReward: 293.9688 meanLoss: 6.0858\n",
      "Episode: 575 meanReward: 293.9688 meanLoss: 5.6041\n",
      "Episode: 576 meanReward: 278.9688 meanLoss: 327.9419\n",
      "Episode: 577 meanReward: 263.8438 meanLoss: 397.8665\n",
      "Episode: 578 meanReward: 267.8750 meanLoss: 8.4942\n",
      "Episode: 579 meanReward: 268.1250 meanLoss: 4.7673\n",
      "Episode: 580 meanReward: 268.1250 meanLoss: 1.6391\n",
      "Episode: 581 meanReward: 268.1250 meanLoss: 18.1748\n",
      "Episode: 582 meanReward: 260.8750 meanLoss: 34.2737\n",
      "Episode: 583 meanReward: 260.8750 meanLoss: 3.1402\n",
      "Episode: 584 meanReward: 249.2500 meanLoss: 69.4719\n",
      "Episode: 585 meanReward: 251.2812 meanLoss: 13.9351\n",
      "Episode: 586 meanReward: 240.4375 meanLoss: 9.5613\n",
      "Episode: 587 meanReward: 233.5000 meanLoss: 3.8294\n",
      "Episode: 588 meanReward: 240.0000 meanLoss: 5.9544\n",
      "Episode: 589 meanReward: 255.3125 meanLoss: 2.4961\n",
      "Episode: 590 meanReward: 245.3438 meanLoss: 53.0718\n",
      "Episode: 591 meanReward: 248.7812 meanLoss: 4.4865\n",
      "Episode: 592 meanReward: 252.7500 meanLoss: 2.3527\n",
      "Episode: 593 meanReward: 240.3438 meanLoss: 73.1941\n",
      "Episode: 594 meanReward: 234.7812 meanLoss: 30.4513\n",
      "Episode: 595 meanReward: 240.3125 meanLoss: 11.5727\n",
      "Episode: 596 meanReward: 250.7500 meanLoss: 4.5345\n",
      "Episode: 597 meanReward: 246.9375 meanLoss: 53.7872\n",
      "Episode: 598 meanReward: 246.6562 meanLoss: 21.5538\n",
      "Episode: 599 meanReward: 243.0000 meanLoss: 68.9127\n",
      "Episode: 600 meanReward: 249.3750 meanLoss: 16.4659\n",
      "Episode: 601 meanReward: 260.9688 meanLoss: 9.0062\n",
      "Episode: 602 meanReward: 268.2500 meanLoss: 22.9518\n",
      "Episode: 603 meanReward: 267.6250 meanLoss: 14.5995\n",
      "Episode: 604 meanReward: 275.7188 meanLoss: 2.5520\n",
      "Episode: 605 meanReward: 285.5938 meanLoss: 18.8427\n",
      "Episode: 606 meanReward: 293.0000 meanLoss: 3.0935\n",
      "Episode: 607 meanReward: 283.4375 meanLoss: 53.6890\n",
      "Episode: 608 meanReward: 288.1562 meanLoss: 20.8207\n",
      "Episode: 609 meanReward: 299.4062 meanLoss: 18.6509\n",
      "Episode: 610 meanReward: 286.1875 meanLoss: 251.8948\n",
      "Episode: 611 meanReward: 280.2500 meanLoss: 285.5393\n",
      "Episode: 612 meanReward: 271.6562 meanLoss: 27.4273\n",
      "Episode: 613 meanReward: 264.3125 meanLoss: 9.7751\n",
      "Episode: 614 meanReward: 263.0000 meanLoss: 6.9137\n",
      "Episode: 615 meanReward: 255.7188 meanLoss: 10.4237\n",
      "Episode: 616 meanReward: 267.3438 meanLoss: 3.0398\n",
      "Episode: 617 meanReward: 278.3438 meanLoss: 19.6329\n",
      "Episode: 618 meanReward: 289.1875 meanLoss: 19.1525\n",
      "Episode: 619 meanReward: 299.7500 meanLoss: 16.0155\n",
      "Episode: 620 meanReward: 307.4688 meanLoss: 19.1719\n",
      "Episode: 621 meanReward: 307.4688 meanLoss: 6.6508\n",
      "Episode: 622 meanReward: 317.9375 meanLoss: 18.4041\n",
      "Episode: 623 meanReward: 324.4062 meanLoss: 16.0646\n",
      "Episode: 624 meanReward: 324.4062 meanLoss: 10.4683\n",
      "Episode: 625 meanReward: 336.8125 meanLoss: 13.2659\n",
      "Episode: 626 meanReward: 352.0625 meanLoss: 17.3707\n",
      "Episode: 627 meanReward: 355.7500 meanLoss: 19.4649\n",
      "Episode: 628 meanReward: 355.7500 meanLoss: 17.8584\n",
      "Episode: 629 meanReward: 363.0938 meanLoss: 17.7443\n",
      "Episode: 630 meanReward: 374.6562 meanLoss: 12.1759\n",
      "Episode: 631 meanReward: 382.5312 meanLoss: 36.7160\n",
      "Episode: 632 meanReward: 373.0938 meanLoss: 243.7713\n",
      "Episode: 633 meanReward: 359.9375 meanLoss: 80.3261\n",
      "Episode: 634 meanReward: 363.8125 meanLoss: 18.0477\n",
      "Episode: 635 meanReward: 375.0625 meanLoss: 16.0760\n",
      "Episode: 636 meanReward: 364.3125 meanLoss: 58.5568\n",
      "Episode: 637 meanReward: 364.3125 meanLoss: 16.0859\n",
      "Episode: 638 meanReward: 364.3125 meanLoss: 18.3680\n",
      "Episode: 639 meanReward: 373.8750 meanLoss: 16.8793\n",
      "Episode: 640 meanReward: 384.1562 meanLoss: 18.4777\n",
      "Episode: 641 meanReward: 388.1875 meanLoss: 16.8776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 642 meanReward: 390.8750 meanLoss: 89.1836\n",
      "Episode: 643 meanReward: 402.5625 meanLoss: 21.7524\n",
      "Episode: 644 meanReward: 411.1562 meanLoss: 17.2782\n",
      "Episode: 645 meanReward: 404.3750 meanLoss: 171.6169\n",
      "Episode: 646 meanReward: 412.9375 meanLoss: 12.4713\n",
      "Episode: 647 meanReward: 406.7500 meanLoss: 121.1025\n",
      "Episode: 648 meanReward: 406.7500 meanLoss: 16.2702\n",
      "Episode: 649 meanReward: 394.1562 meanLoss: 81.2229\n",
      "Episode: 650 meanReward: 394.1562 meanLoss: 15.4357\n",
      "Episode: 651 meanReward: 394.1562 meanLoss: 16.0780\n",
      "Episode: 652 meanReward: 395.1250 meanLoss: 15.5929\n",
      "Episode: 653 meanReward: 382.9688 meanLoss: 77.1804\n",
      "Episode: 654 meanReward: 382.9688 meanLoss: 13.9353\n",
      "Episode: 655 meanReward: 379.1562 meanLoss: 22.0176\n",
      "Episode: 656 meanReward: 379.1562 meanLoss: 16.5671\n",
      "Episode: 657 meanReward: 364.0312 meanLoss: 274.6700\n",
      "Episode: 658 meanReward: 349.9375 meanLoss: 200.4227\n",
      "Episode: 659 meanReward: 337.1562 meanLoss: 64.6522\n",
      "Episode: 660 meanReward: 325.5625 meanLoss: 16.7361\n",
      "Episode: 661 meanReward: 313.0625 meanLoss: 236.6796\n",
      "Episode: 662 meanReward: 297.9688 meanLoss: 341.1665\n",
      "Episode: 663 meanReward: 290.0938 meanLoss: 336.5049\n",
      "Episode: 664 meanReward: 291.4062 meanLoss: 84.6053\n",
      "Episode: 665 meanReward: 293.0312 meanLoss: 22.9536\n",
      "Episode: 666 meanReward: 293.0312 meanLoss: 12.9191\n",
      "Episode: 667 meanReward: 283.9062 meanLoss: 33.3998\n",
      "Episode: 668 meanReward: 282.8125 meanLoss: 9.6786\n",
      "Episode: 669 meanReward: 270.1562 meanLoss: 53.8370\n",
      "Episode: 670 meanReward: 258.9062 meanLoss: 2.9046\n",
      "Episode: 671 meanReward: 247.2500 meanLoss: 4.3650\n",
      "Episode: 672 meanReward: 236.2812 meanLoss: 2.6313\n",
      "Episode: 673 meanReward: 224.1562 meanLoss: 4.5292\n",
      "Episode: 674 meanReward: 232.2188 meanLoss: 2.0238\n",
      "Episode: 675 meanReward: 225.4688 meanLoss: 5.9894\n",
      "Episode: 676 meanReward: 211.9375 meanLoss: 22.3531\n",
      "Episode: 677 meanReward: 211.7812 meanLoss: 71.9784\n",
      "Episode: 678 meanReward: 210.8438 meanLoss: 9.3591\n",
      "Episode: 679 meanReward: 224.3125 meanLoss: 3.8805\n",
      "Episode: 680 meanReward: 209.7188 meanLoss: 230.1700\n",
      "Episode: 681 meanReward: 207.3125 meanLoss: 242.8045\n",
      "Episode: 682 meanReward: 192.0938 meanLoss: 388.2510\n",
      "Episode: 683 meanReward: 176.8750 meanLoss: 313.8413\n",
      "Episode: 684 meanReward: 163.2812 meanLoss: 101.2785\n",
      "Episode: 685 meanReward: 165.0625 meanLoss: 7.4715\n",
      "Episode: 686 meanReward: 155.8438 meanLoss: 5.4207\n",
      "Episode: 687 meanReward: 149.9688 meanLoss: 4.2188\n",
      "Episode: 688 meanReward: 141.6875 meanLoss: 2.6142\n",
      "Episode: 689 meanReward: 152.2500 meanLoss: 20.3249\n",
      "Episode: 690 meanReward: 152.3125 meanLoss: 145.0452\n",
      "Episode: 691 meanReward: 152.6250 meanLoss: 55.5107\n",
      "Episode: 692 meanReward: 164.2188 meanLoss: 11.1915\n",
      "Episode: 693 meanReward: 179.2188 meanLoss: 13.5041\n",
      "Episode: 694 meanReward: 194.3125 meanLoss: 13.9801\n",
      "Episode: 695 meanReward: 206.5625 meanLoss: 18.5779\n",
      "Episode: 696 meanReward: 210.3750 meanLoss: 13.8489\n",
      "Episode: 697 meanReward: 210.6562 meanLoss: 15.4980\n",
      "Episode: 698 meanReward: 202.6250 meanLoss: 4.5648\n",
      "Episode: 699 meanReward: 199.4688 meanLoss: 12.9186\n",
      "Episode: 700 meanReward: 200.9688 meanLoss: 4.6585\n",
      "Episode: 701 meanReward: 200.6250 meanLoss: 8.1864\n",
      "Episode: 702 meanReward: 197.9375 meanLoss: 61.9560\n",
      "Episode: 703 meanReward: 197.0000 meanLoss: 68.4704\n",
      "Episode: 704 meanReward: 200.2812 meanLoss: 9.2694\n",
      "Episode: 705 meanReward: 198.1250 meanLoss: 169.8611\n",
      "Episode: 706 meanReward: 192.5000 meanLoss: 25.3168\n",
      "Episode: 707 meanReward: 195.1250 meanLoss: 25.1683\n",
      "Episode: 708 meanReward: 206.0312 meanLoss: 13.1243\n",
      "Episode: 709 meanReward: 210.9062 meanLoss: 30.3803\n",
      "Episode: 710 meanReward: 210.0000 meanLoss: 8.8913\n",
      "Episode: 711 meanReward: 204.6875 meanLoss: 20.3696\n",
      "Episode: 712 meanReward: 212.7188 meanLoss: 21.1545\n",
      "Episode: 713 meanReward: 213.0312 meanLoss: 183.8217\n",
      "Episode: 714 meanReward: 217.3125 meanLoss: 37.2108\n",
      "Episode: 715 meanReward: 223.4688 meanLoss: 21.9562\n",
      "Episode: 716 meanReward: 228.2500 meanLoss: 18.1827\n",
      "Episode: 717 meanReward: 226.5938 meanLoss: 48.5033\n",
      "Episode: 718 meanReward: 222.0000 meanLoss: 66.2324\n",
      "Episode: 719 meanReward: 223.5938 meanLoss: 6.9719\n",
      "Episode: 720 meanReward: 219.9688 meanLoss: 45.6704\n",
      "Episode: 721 meanReward: 224.5312 meanLoss: 3.0101\n",
      "Episode: 722 meanReward: 238.5625 meanLoss: 18.9850\n",
      "Episode: 723 meanReward: 251.0312 meanLoss: 14.2593\n",
      "Episode: 724 meanReward: 239.9688 meanLoss: 65.4892\n",
      "Episode: 725 meanReward: 229.9375 meanLoss: 9.8948\n",
      "Episode: 726 meanReward: 218.2188 meanLoss: 27.0057\n",
      "Episode: 727 meanReward: 221.0938 meanLoss: 5.6691\n",
      "Episode: 728 meanReward: 222.7500 meanLoss: 37.1800\n",
      "Episode: 729 meanReward: 221.0625 meanLoss: 38.8901\n",
      "Episode: 730 meanReward: 229.0938 meanLoss: 3.0876\n",
      "Episode: 731 meanReward: 232.2812 meanLoss: 43.4631\n",
      "Episode: 732 meanReward: 227.4375 meanLoss: 247.9681\n",
      "Episode: 733 meanReward: 234.4688 meanLoss: 17.2960\n",
      "Episode: 734 meanReward: 237.9375 meanLoss: 21.2004\n",
      "Episode: 735 meanReward: 239.4062 meanLoss: 28.1518\n",
      "Episode: 736 meanReward: 236.2812 meanLoss: 24.1951\n",
      "Episode: 737 meanReward: 239.6250 meanLoss: 21.2814\n",
      "Episode: 738 meanReward: 249.7188 meanLoss: 6.1931\n",
      "Episode: 739 meanReward: 247.7500 meanLoss: 40.3129\n",
      "Episode: 740 meanReward: 240.0312 meanLoss: 12.8090\n",
      "Episode: 741 meanReward: 242.4375 meanLoss: 10.4727\n",
      "Episode: 742 meanReward: 244.2812 meanLoss: 8.8942\n",
      "Episode: 743 meanReward: 249.5938 meanLoss: 13.3862\n",
      "Episode: 744 meanReward: 242.2188 meanLoss: 72.5562\n",
      "Episode: 745 meanReward: 256.9062 meanLoss: 6.8098\n",
      "Episode: 746 meanReward: 267.8438 meanLoss: 10.6257\n",
      "Episode: 747 meanReward: 276.9062 meanLoss: 14.2962\n",
      "Episode: 748 meanReward: 285.7188 meanLoss: 14.4008\n",
      "Episode: 749 meanReward: 293.5938 meanLoss: 21.7423\n",
      "Episode: 750 meanReward: 294.3438 meanLoss: 124.0910\n",
      "Episode: 751 meanReward: 298.3125 meanLoss: 3.7374\n",
      "Episode: 752 meanReward: 306.6250 meanLoss: 21.1292\n",
      "Episode: 753 meanReward: 300.9688 meanLoss: 18.0264\n",
      "Episode: 754 meanReward: 287.3750 meanLoss: 108.5092\n",
      "Episode: 755 meanReward: 287.3750 meanLoss: 19.7696\n",
      "Episode: 756 meanReward: 286.8438 meanLoss: 58.4801\n",
      "Episode: 757 meanReward: 296.1250 meanLoss: 17.5174\n",
      "Episode: 758 meanReward: 292.6875 meanLoss: 371.7433\n",
      "Episode: 759 meanReward: 279.7500 meanLoss: 118.9087\n",
      "Episode: 760 meanReward: 272.8438 meanLoss: 165.1086\n",
      "Episode: 761 meanReward: 282.3750 meanLoss: 19.0313\n",
      "Episode: 762 meanReward: 274.7812 meanLoss: 20.9474\n",
      "Episode: 763 meanReward: 283.8750 meanLoss: 10.2915\n",
      "Episode: 764 meanReward: 284.0938 meanLoss: 334.9385\n",
      "Episode: 765 meanReward: 274.7812 meanLoss: 569.9923\n",
      "Episode: 766 meanReward: 269.9688 meanLoss: 383.3924\n",
      "Episode: 767 meanReward: 281.0938 meanLoss: 8.7833\n",
      "Episode: 768 meanReward: 291.9062 meanLoss: 17.8085\n",
      "Episode: 769 meanReward: 302.8438 meanLoss: 16.1434\n",
      "Episode: 770 meanReward: 302.8438 meanLoss: 15.1725\n",
      "Episode: 771 meanReward: 312.4062 meanLoss: 18.1855\n",
      "Episode: 772 meanReward: 322.7500 meanLoss: 17.9096\n",
      "Episode: 773 meanReward: 329.7500 meanLoss: 17.4394\n",
      "Episode: 774 meanReward: 329.7500 meanLoss: 17.3690\n",
      "Episode: 775 meanReward: 315.2188 meanLoss: 240.9994\n",
      "Episode: 776 meanReward: 329.1562 meanLoss: 5.2785\n",
      "Episode: 777 meanReward: 329.1562 meanLoss: 16.4406\n",
      "Episode: 778 meanReward: 314.0938 meanLoss: 266.2047\n",
      "Episode: 779 meanReward: 300.3438 meanLoss: 180.1228\n",
      "Episode: 780 meanReward: 300.3438 meanLoss: 10.9833\n",
      "Episode: 781 meanReward: 296.7812 meanLoss: 30.2030\n",
      "Episode: 782 meanReward: 298.5625 meanLoss: 51.2717\n",
      "Episode: 783 meanReward: 302.6875 meanLoss: 11.2478\n",
      "Episode: 784 meanReward: 306.2812 meanLoss: 15.2907\n",
      "Episode: 785 meanReward: 311.9375 meanLoss: 14.1832\n",
      "Episode: 786 meanReward: 325.5312 meanLoss: 15.5379\n",
      "Episode: 787 meanReward: 325.5312 meanLoss: 15.5646\n",
      "Episode: 788 meanReward: 325.2188 meanLoss: 62.7423\n",
      "Episode: 789 meanReward: 310.6875 meanLoss: 90.2893\n",
      "Episode: 790 meanReward: 325.8438 meanLoss: 5.9869\n",
      "Episode: 791 meanReward: 338.7812 meanLoss: 13.3467\n",
      "Episode: 792 meanReward: 353.9375 meanLoss: 14.4447\n",
      "Episode: 793 meanReward: 350.8125 meanLoss: 25.2330\n",
      "Episode: 794 meanReward: 345.7188 meanLoss: 24.8904\n",
      "Episode: 795 meanReward: 330.8438 meanLoss: 35.4136\n",
      "Episode: 796 meanReward: 332.7500 meanLoss: 98.8001\n",
      "Episode: 797 meanReward: 335.0312 meanLoss: 8.1123\n",
      "Episode: 798 meanReward: 337.1875 meanLoss: 9.6357\n",
      "Episode: 799 meanReward: 324.9062 meanLoss: 8.1668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800 meanReward: 324.9062 meanLoss: 5.2600\n",
      "Episode: 801 meanReward: 314.1875 meanLoss: 46.7138\n",
      "Episode: 802 meanReward: 308.3750 meanLoss: 9.9683\n",
      "Episode: 803 meanReward: 302.0312 meanLoss: 14.8077\n",
      "Episode: 804 meanReward: 302.0312 meanLoss: 17.2719\n",
      "Episode: 805 meanReward: 287.2188 meanLoss: 285.1072\n",
      "Episode: 806 meanReward: 279.5938 meanLoss: 32.7480\n",
      "Episode: 807 meanReward: 294.1250 meanLoss: 14.9810\n",
      "Episode: 808 meanReward: 284.9062 meanLoss: 39.6872\n",
      "Episode: 809 meanReward: 284.9062 meanLoss: 13.7798\n",
      "Episode: 810 meanReward: 299.9688 meanLoss: 17.1654\n",
      "Episode: 811 meanReward: 313.7188 meanLoss: 14.6134\n",
      "Episode: 812 meanReward: 313.7188 meanLoss: 18.4907\n",
      "Episode: 813 meanReward: 321.4375 meanLoss: 18.4161\n",
      "Episode: 814 meanReward: 332.7188 meanLoss: 17.7222\n",
      "Episode: 815 meanReward: 332.7188 meanLoss: 16.5103\n",
      "Episode: 816 meanReward: 326.4375 meanLoss: 28.7841\n",
      "Episode: 817 meanReward: 326.4375 meanLoss: 10.2162\n",
      "Episode: 818 meanReward: 311.5000 meanLoss: 286.5331\n",
      "Episode: 819 meanReward: 311.5000 meanLoss: 18.5058\n",
      "Episode: 820 meanReward: 311.2188 meanLoss: 68.8545\n",
      "Episode: 821 meanReward: 314.0312 meanLoss: 26.6428\n",
      "Episode: 822 meanReward: 298.8438 meanLoss: 129.4452\n",
      "Episode: 823 meanReward: 287.6250 meanLoss: 39.5158\n",
      "Episode: 824 meanReward: 287.6250 meanLoss: 8.2210\n",
      "Episode: 825 meanReward: 287.5312 meanLoss: 23.7222\n",
      "Episode: 826 meanReward: 291.9688 meanLoss: 17.2573\n",
      "Episode: 827 meanReward: 297.5000 meanLoss: 5.8318\n",
      "Episode: 828 meanReward: 310.5625 meanLoss: 16.6362\n",
      "Episode: 829 meanReward: 323.5625 meanLoss: 17.1990\n",
      "Episode: 830 meanReward: 329.4062 meanLoss: 32.8089\n",
      "Episode: 831 meanReward: 331.1250 meanLoss: 46.5603\n",
      "Episode: 832 meanReward: 321.7812 meanLoss: 65.5739\n",
      "Episode: 833 meanReward: 323.8438 meanLoss: 7.1264\n",
      "Episode: 834 meanReward: 329.6562 meanLoss: 1.3376\n",
      "Episode: 835 meanReward: 329.7500 meanLoss: 28.5444\n",
      "Episode: 836 meanReward: 329.7500 meanLoss: 1.1001\n",
      "Episode: 837 meanReward: 344.5625 meanLoss: 16.9061\n",
      "Episode: 838 meanReward: 352.1875 meanLoss: 16.3357\n",
      "Episode: 839 meanReward: 352.1875 meanLoss: 16.9777\n",
      "Episode: 840 meanReward: 361.4062 meanLoss: 16.9815\n",
      "Episode: 841 meanReward: 346.8125 meanLoss: 242.8700\n",
      "Episode: 842 meanReward: 334.6875 meanLoss: 63.8370\n",
      "Episode: 843 meanReward: 334.6875 meanLoss: 16.2553\n",
      "Episode: 844 meanReward: 319.9688 meanLoss: 254.6917\n",
      "Episode: 845 meanReward: 304.7812 meanLoss: 261.2433\n",
      "Episode: 846 meanReward: 304.7812 meanLoss: 17.1253\n",
      "Episode: 847 meanReward: 304.7812 meanLoss: 14.9065\n",
      "Episode: 848 meanReward: 311.0625 meanLoss: 14.8542\n",
      "Episode: 849 meanReward: 311.0625 meanLoss: 15.8019\n",
      "Episode: 850 meanReward: 316.2500 meanLoss: 41.1567\n",
      "Episode: 851 meanReward: 309.5312 meanLoss: 7.1920\n",
      "Episode: 852 meanReward: 321.7188 meanLoss: 14.9604\n",
      "Episode: 853 meanReward: 323.2812 meanLoss: 52.9741\n",
      "Episode: 854 meanReward: 331.0312 meanLoss: 27.3824\n",
      "Episode: 855 meanReward: 342.2500 meanLoss: 21.9534\n",
      "Episode: 856 meanReward: 327.8125 meanLoss: 194.0342\n",
      "Episode: 857 meanReward: 319.8438 meanLoss: 204.7702\n",
      "Episode: 858 meanReward: 318.0312 meanLoss: 23.9474\n",
      "Episode: 859 meanReward: 316.5312 meanLoss: 37.6562\n",
      "Episode: 860 meanReward: 304.0938 meanLoss: 53.3140\n",
      "Episode: 861 meanReward: 293.8750 meanLoss: 29.1613\n",
      "Episode: 862 meanReward: 289.0312 meanLoss: 46.1273\n",
      "Episode: 863 meanReward: 289.1562 meanLoss: 26.6719\n",
      "Episode: 864 meanReward: 287.4375 meanLoss: 29.6187\n",
      "Episode: 865 meanReward: 284.1250 meanLoss: 11.8281\n",
      "Episode: 866 meanReward: 269.0625 meanLoss: 120.1644\n",
      "Episode: 867 meanReward: 260.1250 meanLoss: 255.7092\n",
      "Episode: 868 meanReward: 247.4062 meanLoss: 68.0280\n",
      "Episode: 869 meanReward: 235.8750 meanLoss: 20.7791\n",
      "Episode: 870 meanReward: 223.9062 meanLoss: 27.9225\n",
      "Episode: 871 meanReward: 221.5938 meanLoss: 9.1157\n",
      "Episode: 872 meanReward: 221.5938 meanLoss: 9.1906\n",
      "Episode: 873 meanReward: 224.6250 meanLoss: 43.5871\n",
      "Episode: 874 meanReward: 236.7500 meanLoss: 8.0121\n",
      "Episode: 875 meanReward: 236.7500 meanLoss: 11.7964\n",
      "Episode: 876 meanReward: 239.6562 meanLoss: 57.0133\n",
      "Episode: 877 meanReward: 254.8438 meanLoss: 11.7749\n",
      "Episode: 878 meanReward: 242.8125 meanLoss: 64.4024\n",
      "Episode: 879 meanReward: 242.8125 meanLoss: 3.5134\n",
      "Episode: 880 meanReward: 231.4688 meanLoss: 58.1766\n",
      "Episode: 881 meanReward: 231.4688 meanLoss: 11.3145\n",
      "Episode: 882 meanReward: 241.2188 meanLoss: 17.3917\n",
      "Episode: 883 meanReward: 247.9375 meanLoss: 15.8568\n",
      "Episode: 884 meanReward: 233.3750 meanLoss: 224.0390\n",
      "Episode: 885 meanReward: 232.6562 meanLoss: 50.1666\n",
      "Episode: 886 meanReward: 240.0938 meanLoss: 9.3279\n",
      "Episode: 887 meanReward: 228.4375 meanLoss: 57.1222\n",
      "Episode: 888 meanReward: 231.5000 meanLoss: 39.4182\n",
      "Episode: 889 meanReward: 239.0938 meanLoss: 17.5108\n",
      "Episode: 890 meanReward: 247.8750 meanLoss: 11.4713\n",
      "Episode: 891 meanReward: 251.0938 meanLoss: 24.2207\n",
      "Episode: 892 meanReward: 251.1250 meanLoss: 58.6836\n",
      "Episode: 893 meanReward: 261.3438 meanLoss: 12.6850\n",
      "Episode: 894 meanReward: 258.6250 meanLoss: 219.3172\n",
      "Episode: 895 meanReward: 253.8750 meanLoss: 209.5723\n",
      "Episode: 896 meanReward: 264.9375 meanLoss: 9.2417\n",
      "Episode: 897 meanReward: 276.9062 meanLoss: 14.5294\n",
      "Episode: 898 meanReward: 285.4062 meanLoss: 27.1331\n",
      "Episode: 899 meanReward: 288.5938 meanLoss: 10.8148\n",
      "Episode: 900 meanReward: 289.3438 meanLoss: 53.9686\n",
      "Episode: 901 meanReward: 300.8750 meanLoss: 10.7386\n",
      "Episode: 902 meanReward: 300.6875 meanLoss: 67.3489\n",
      "Episode: 903 meanReward: 288.2812 meanLoss: 114.1489\n",
      "Episode: 904 meanReward: 287.7500 meanLoss: 5.2726\n",
      "Episode: 905 meanReward: 289.9375 meanLoss: 26.7942\n",
      "Episode: 906 meanReward: 281.0938 meanLoss: 21.0098\n",
      "Episode: 907 meanReward: 276.0312 meanLoss: 4.1885\n",
      "Episode: 908 meanReward: 276.0000 meanLoss: 42.4488\n",
      "Episode: 909 meanReward: 267.8125 meanLoss: 4.9959\n",
      "Episode: 910 meanReward: 270.9688 meanLoss: 16.1858\n",
      "Episode: 911 meanReward: 261.2500 meanLoss: 18.3460\n",
      "Episode: 912 meanReward: 263.2812 meanLoss: 9.7515\n",
      "Episode: 913 meanReward: 263.2812 meanLoss: 2.1296\n",
      "Episode: 914 meanReward: 253.0625 meanLoss: 49.8496\n",
      "Episode: 915 meanReward: 242.4062 meanLoss: 14.1189\n",
      "Episode: 916 meanReward: 246.2500 meanLoss: 5.8487\n",
      "Episode: 917 meanReward: 247.2812 meanLoss: 7.2842\n",
      "Episode: 918 meanReward: 247.2812 meanLoss: 2.8531\n",
      "Episode: 919 meanReward: 247.5625 meanLoss: 62.7787\n",
      "Episode: 920 meanReward: 247.9375 meanLoss: 8.4313\n",
      "Episode: 921 meanReward: 246.2188 meanLoss: 6.1580\n",
      "Episode: 922 meanReward: 244.1250 meanLoss: 4.5222\n",
      "Episode: 923 meanReward: 240.7812 meanLoss: 11.5031\n",
      "Episode: 924 meanReward: 253.1875 meanLoss: 3.4572\n",
      "Episode: 925 meanReward: 253.1875 meanLoss: 19.5866\n",
      "Episode: 926 meanReward: 268.0312 meanLoss: 18.2169\n",
      "Episode: 927 meanReward: 283.2188 meanLoss: 14.6502\n",
      "Episode: 928 meanReward: 277.2188 meanLoss: 30.4616\n",
      "Episode: 929 meanReward: 277.2188 meanLoss: 7.9942\n",
      "Episode: 930 meanReward: 283.7812 meanLoss: 19.0693\n",
      "Episode: 931 meanReward: 295.7812 meanLoss: 18.6923\n",
      "Episode: 932 meanReward: 307.7500 meanLoss: 19.6262\n",
      "Episode: 933 meanReward: 296.9375 meanLoss: 44.8978\n",
      "Episode: 934 meanReward: 294.0312 meanLoss: 57.3288\n",
      "Episode: 935 meanReward: 301.7188 meanLoss: 12.8637\n",
      "Episode: 936 meanReward: 292.3438 meanLoss: 14.4678\n",
      "Episode: 937 meanReward: 301.7188 meanLoss: 4.7339\n",
      "Episode: 938 meanReward: 307.1875 meanLoss: 20.5825\n",
      "Episode: 939 meanReward: 312.2500 meanLoss: 10.5360\n",
      "Episode: 940 meanReward: 324.0938 meanLoss: 17.8352\n",
      "Episode: 941 meanReward: 332.2812 meanLoss: 19.6628\n",
      "Episode: 942 meanReward: 341.1562 meanLoss: 18.7918\n",
      "Episode: 943 meanReward: 350.8750 meanLoss: 17.4294\n",
      "Episode: 944 meanReward: 360.1875 meanLoss: 18.9426\n",
      "Episode: 945 meanReward: 348.1875 meanLoss: 81.7682\n",
      "Episode: 946 meanReward: 346.5000 meanLoss: 32.1280\n",
      "Episode: 947 meanReward: 357.1562 meanLoss: 5.9388\n",
      "Episode: 948 meanReward: 353.1562 meanLoss: 289.6512\n",
      "Episode: 949 meanReward: 348.4688 meanLoss: 330.3097\n",
      "Episode: 950 meanReward: 333.1562 meanLoss: 371.9078\n",
      "Episode: 951 meanReward: 344.5312 meanLoss: 9.9348\n",
      "Episode: 952 meanReward: 355.5312 meanLoss: 10.9545\n",
      "Episode: 953 meanReward: 364.2500 meanLoss: 16.2125\n",
      "Episode: 954 meanReward: 367.6250 meanLoss: 12.7259\n",
      "Episode: 955 meanReward: 378.5938 meanLoss: 18.1824\n",
      "Episode: 956 meanReward: 378.5938 meanLoss: 18.2118\n",
      "Episode: 957 meanReward: 378.5938 meanLoss: 18.2045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 958 meanReward: 366.3438 meanLoss: 81.4580\n",
      "Episode: 959 meanReward: 366.3438 meanLoss: 15.5511\n",
      "Episode: 960 meanReward: 357.9375 meanLoss: 215.2282\n",
      "Episode: 961 meanReward: 355.3750 meanLoss: 16.2070\n",
      "Episode: 962 meanReward: 355.3750 meanLoss: 12.1968\n",
      "Episode: 963 meanReward: 349.4688 meanLoss: 26.5992\n",
      "Episode: 964 meanReward: 337.0625 meanLoss: 88.8899\n",
      "Episode: 965 meanReward: 334.6250 meanLoss: 225.4866\n",
      "Episode: 966 meanReward: 335.8125 meanLoss: 204.8745\n",
      "Episode: 967 meanReward: 328.5000 meanLoss: 360.5415\n",
      "Episode: 968 meanReward: 331.8438 meanLoss: 6.8076\n",
      "Episode: 969 meanReward: 317.2188 meanLoss: 264.8535\n",
      "Episode: 970 meanReward: 309.7812 meanLoss: 31.3922\n",
      "Episode: 971 meanReward: 297.8750 meanLoss: 53.3330\n",
      "Episode: 972 meanReward: 286.0000 meanLoss: 52.8106\n",
      "Episode: 973 meanReward: 286.0000 meanLoss: 11.3937\n",
      "Episode: 974 meanReward: 275.4062 meanLoss: 48.0631\n",
      "Episode: 975 meanReward: 268.9688 meanLoss: 18.2736\n",
      "Episode: 976 meanReward: 258.9375 meanLoss: 24.6351\n",
      "Episode: 977 meanReward: 258.7500 meanLoss: 56.9475\n",
      "Episode: 978 meanReward: 257.6250 meanLoss: 69.1876\n",
      "Episode: 979 meanReward: 247.5000 meanLoss: 24.3854\n",
      "Episode: 980 meanReward: 249.2500 meanLoss: 50.2883\n",
      "Episode: 981 meanReward: 251.2188 meanLoss: 32.7292\n",
      "Episode: 982 meanReward: 251.7812 meanLoss: 46.9692\n",
      "Episode: 983 meanReward: 238.8750 meanLoss: 31.6215\n",
      "Episode: 984 meanReward: 226.0625 meanLoss: 10.5119\n",
      "Episode: 985 meanReward: 214.0000 meanLoss: 11.6724\n",
      "Episode: 986 meanReward: 201.4062 meanLoss: 11.4456\n",
      "Episode: 987 meanReward: 189.3750 meanLoss: 9.1888\n",
      "Episode: 988 meanReward: 176.5938 meanLoss: 11.7691\n",
      "Episode: 989 meanReward: 164.4688 meanLoss: 13.2022\n",
      "Episode: 990 meanReward: 164.0938 meanLoss: 12.8379\n",
      "Episode: 991 meanReward: 151.4375 meanLoss: 12.4139\n",
      "Episode: 992 meanReward: 153.4062 meanLoss: 9.2045\n",
      "Episode: 993 meanReward: 143.4062 meanLoss: 6.5319\n",
      "Episode: 994 meanReward: 130.7188 meanLoss: 9.5463\n",
      "Episode: 995 meanReward: 124.0312 meanLoss: 9.3450\n",
      "Episode: 996 meanReward: 123.9062 meanLoss: 7.5657\n",
      "Episode: 997 meanReward: 124.5625 meanLoss: 6.1918\n",
      "Episode: 998 meanReward: 127.2812 meanLoss: 4.1406\n",
      "Episode: 999 meanReward: 129.5625 meanLoss: 7.1827\n",
      "Episode: 1000 meanReward: 124.3125 meanLoss: 5.7326\n",
      "Episode: 1001 meanReward: 128.8438 meanLoss: 4.4548\n",
      "Episode: 1002 meanReward: 127.7188 meanLoss: 3.7540\n",
      "Episode: 1003 meanReward: 129.8750 meanLoss: 3.2646\n",
      "Episode: 1004 meanReward: 129.8125 meanLoss: 6.0410\n",
      "Episode: 1005 meanReward: 120.3750 meanLoss: 5.6983\n",
      "Episode: 1006 meanReward: 126.5938 meanLoss: 6.6788\n",
      "Episode: 1007 meanReward: 122.9375 meanLoss: 14.0508\n",
      "Episode: 1008 meanReward: 123.0000 meanLoss: 5.6518\n",
      "Episode: 1009 meanReward: 128.5938 meanLoss: 11.1203\n",
      "Episode: 1010 meanReward: 132.5000 meanLoss: 17.8009\n",
      "Episode: 1011 meanReward: 130.4375 meanLoss: 24.1454\n",
      "Episode: 1012 meanReward: 143.4062 meanLoss: 8.9231\n",
      "Episode: 1013 meanReward: 152.9688 meanLoss: 12.6692\n",
      "Episode: 1014 meanReward: 167.7188 meanLoss: 9.5713\n",
      "Episode: 1015 meanReward: 169.7188 meanLoss: 49.3213\n",
      "Episode: 1016 meanReward: 182.5312 meanLoss: 9.0887\n",
      "Episode: 1017 meanReward: 194.5938 meanLoss: 11.1092\n",
      "Episode: 1018 meanReward: 198.2812 meanLoss: 31.8902\n",
      "Episode: 1019 meanReward: 210.3125 meanLoss: 9.2202\n",
      "Episode: 1020 meanReward: 211.1875 meanLoss: 57.5769\n",
      "Episode: 1021 meanReward: 210.7188 meanLoss: 38.5690\n",
      "Episode: 1022 meanReward: 210.5312 meanLoss: 40.3855\n",
      "Episode: 1023 meanReward: 210.5000 meanLoss: 47.3813\n",
      "Episode: 1024 meanReward: 222.9375 meanLoss: 2.9893\n",
      "Episode: 1025 meanReward: 223.9688 meanLoss: 55.8471\n",
      "Episode: 1026 meanReward: 224.3438 meanLoss: 38.5269\n",
      "Episode: 1027 meanReward: 236.9375 meanLoss: 8.0079\n",
      "Episode: 1028 meanReward: 243.4688 meanLoss: 24.8644\n",
      "Episode: 1029 meanReward: 256.0625 meanLoss: 1.3376\n",
      "Episode: 1030 meanReward: 264.9375 meanLoss: 20.1734\n",
      "Episode: 1031 meanReward: 277.0000 meanLoss: 1.1286\n",
      "Episode: 1032 meanReward: 285.6562 meanLoss: 21.7610\n",
      "Episode: 1033 meanReward: 295.7500 meanLoss: 2.9494\n",
      "Episode: 1034 meanReward: 307.6875 meanLoss: 5.3218\n",
      "Episode: 1035 meanReward: 317.4375 meanLoss: 19.2584\n",
      "Episode: 1036 meanReward: 329.3750 meanLoss: 18.4479\n",
      "Episode: 1037 meanReward: 327.5312 meanLoss: 57.1898\n",
      "Episode: 1038 meanReward: 331.9062 meanLoss: 15.3158\n",
      "Episode: 1039 meanReward: 327.8438 meanLoss: 193.2097\n",
      "Episode: 1040 meanReward: 328.9688 meanLoss: 16.1785\n",
      "Episode: 1041 meanReward: 320.4375 meanLoss: 233.7988\n",
      "Episode: 1042 meanReward: 314.3125 meanLoss: 460.1279\n",
      "Episode: 1043 meanReward: 311.1562 meanLoss: 519.3293\n",
      "Episode: 1044 meanReward: 295.9062 meanLoss: 352.1064\n",
      "Episode: 1045 meanReward: 299.6562 meanLoss: 16.8625\n",
      "Episode: 1046 meanReward: 299.6562 meanLoss: 17.9079\n",
      "Episode: 1047 meanReward: 310.5625 meanLoss: 17.5565\n",
      "Episode: 1048 meanReward: 310.5625 meanLoss: 15.9195\n",
      "Episode: 1049 meanReward: 310.5625 meanLoss: 16.5149\n",
      "Episode: 1050 meanReward: 319.4688 meanLoss: 16.6513\n",
      "Episode: 1051 meanReward: 319.4688 meanLoss: 16.9023\n",
      "Episode: 1052 meanReward: 331.3750 meanLoss: 17.0396\n",
      "Episode: 1053 meanReward: 343.9688 meanLoss: 16.5288\n",
      "Episode: 1054 meanReward: 356.7812 meanLoss: 15.6889\n",
      "Episode: 1055 meanReward: 369.4688 meanLoss: 16.0546\n",
      "Episode: 1056 meanReward: 357.2812 meanLoss: 61.2358\n",
      "Episode: 1057 meanReward: 353.9688 meanLoss: 162.9396\n",
      "Episode: 1058 meanReward: 357.1250 meanLoss: 14.4293\n",
      "Episode: 1059 meanReward: 357.1250 meanLoss: 10.7356\n",
      "Episode: 1060 meanReward: 363.1250 meanLoss: 15.0095\n",
      "Episode: 1061 meanReward: 363.1250 meanLoss: 17.5237\n",
      "Episode: 1062 meanReward: 365.4062 meanLoss: 17.4614\n",
      "Episode: 1063 meanReward: 365.4062 meanLoss: 15.9685\n",
      "Episode: 1064 meanReward: 356.6250 meanLoss: 71.6836\n",
      "Episode: 1065 meanReward: 348.5312 meanLoss: 11.1519\n",
      "Episode: 1066 meanReward: 342.2500 meanLoss: 12.8644\n",
      "Episode: 1067 meanReward: 329.3125 meanLoss: 43.5327\n",
      "Episode: 1068 meanReward: 318.0000 meanLoss: 18.9438\n",
      "Episode: 1069 meanReward: 320.0625 meanLoss: 24.5410\n",
      "Episode: 1070 meanReward: 305.6250 meanLoss: 165.7814\n",
      "Episode: 1071 meanReward: 312.2188 meanLoss: 10.5124\n",
      "Episode: 1072 meanReward: 308.9688 meanLoss: 22.0076\n",
      "Episode: 1073 meanReward: 311.5312 meanLoss: 31.1326\n",
      "Episode: 1074 meanReward: 314.8438 meanLoss: 34.6645\n",
      "Episode: 1075 meanReward: 318.7188 meanLoss: 23.8703\n",
      "Episode: 1076 meanReward: 323.2812 meanLoss: 12.4357\n",
      "Episode: 1077 meanReward: 311.6875 meanLoss: 17.3590\n",
      "Episode: 1078 meanReward: 299.7500 meanLoss: 14.1149\n",
      "Episode: 1079 meanReward: 288.2812 meanLoss: 5.3949\n",
      "Episode: 1080 meanReward: 277.4375 meanLoss: 4.3894\n",
      "Episode: 1081 meanReward: 266.3125 meanLoss: 3.5374\n",
      "Episode: 1082 meanReward: 253.5625 meanLoss: 6.1356\n",
      "Episode: 1083 meanReward: 240.5000 meanLoss: 14.5250\n",
      "Episode: 1084 meanReward: 226.4375 meanLoss: 13.2117\n",
      "Episode: 1085 meanReward: 226.4375 meanLoss: 1.6743\n",
      "Episode: 1086 meanReward: 223.1250 meanLoss: 7.8504\n",
      "Episode: 1087 meanReward: 223.1250 meanLoss: 7.2419\n",
      "Episode: 1088 meanReward: 235.3125 meanLoss: 19.6239\n",
      "Episode: 1089 meanReward: 250.1562 meanLoss: 21.6860\n",
      "Episode: 1090 meanReward: 259.3125 meanLoss: 15.4762\n",
      "Episode: 1091 meanReward: 259.3125 meanLoss: 12.5491\n",
      "Episode: 1092 meanReward: 259.3125 meanLoss: 19.0216\n",
      "Episode: 1093 meanReward: 259.3125 meanLoss: 17.9056\n",
      "Episode: 1094 meanReward: 259.3125 meanLoss: 11.6484\n",
      "Episode: 1095 meanReward: 244.2500 meanLoss: 275.6186\n",
      "Episode: 1096 meanReward: 241.0000 meanLoss: 349.3360\n",
      "Episode: 1097 meanReward: 249.0938 meanLoss: 4.8089\n",
      "Episode: 1098 meanReward: 255.3750 meanLoss: 15.9276\n",
      "Episode: 1099 meanReward: 268.3125 meanLoss: 17.0658\n",
      "Episode: 1100 meanReward: 266.5938 meanLoss: 95.2044\n",
      "Episode: 1101 meanReward: 275.8125 meanLoss: 3.2173\n",
      "Episode: 1102 meanReward: 290.2500 meanLoss: 18.3863\n",
      "Episode: 1103 meanReward: 297.8125 meanLoss: 17.9050\n",
      "Episode: 1104 meanReward: 309.9062 meanLoss: 10.1279\n",
      "Episode: 1105 meanReward: 317.0625 meanLoss: 28.2471\n",
      "Episode: 1106 meanReward: 313.8125 meanLoss: 122.5597\n",
      "Episode: 1107 meanReward: 314.8125 meanLoss: 28.3535\n",
      "Episode: 1108 meanReward: 325.5000 meanLoss: 2.4560\n",
      "Episode: 1109 meanReward: 337.0938 meanLoss: 19.5139\n",
      "Episode: 1110 meanReward: 349.0312 meanLoss: 17.6793\n",
      "Episode: 1111 meanReward: 360.5000 meanLoss: 19.2389\n",
      "Episode: 1112 meanReward: 371.3438 meanLoss: 19.2130\n",
      "Episode: 1113 meanReward: 382.4688 meanLoss: 15.4323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1114 meanReward: 395.2188 meanLoss: 17.2943\n",
      "Episode: 1115 meanReward: 408.2812 meanLoss: 13.6745\n",
      "Episode: 1116 meanReward: 422.3438 meanLoss: 19.0685\n",
      "Episode: 1117 meanReward: 413.7812 meanLoss: 39.8044\n",
      "Episode: 1118 meanReward: 417.0938 meanLoss: 16.7132\n",
      "Episode: 1119 meanReward: 402.0625 meanLoss: 283.2770\n",
      "Episode: 1120 meanReward: 402.0625 meanLoss: 18.3985\n",
      "Episode: 1121 meanReward: 402.0625 meanLoss: 16.7030\n",
      "Episode: 1122 meanReward: 387.2188 meanLoss: 277.1144\n",
      "Episode: 1123 meanReward: 378.6250 meanLoss: 39.7333\n",
      "Episode: 1124 meanReward: 378.6250 meanLoss: 9.4778\n",
      "Episode: 1125 meanReward: 378.6250 meanLoss: 15.6917\n",
      "Episode: 1126 meanReward: 364.3438 meanLoss: 187.7686\n",
      "Episode: 1127 meanReward: 379.4062 meanLoss: 14.4938\n",
      "Episode: 1128 meanReward: 394.5938 meanLoss: 16.0325\n",
      "Episode: 1129 meanReward: 394.5938 meanLoss: 16.5827\n",
      "Episode: 1130 meanReward: 386.8750 meanLoss: 33.6071\n",
      "Episode: 1131 meanReward: 375.7500 meanLoss: 20.0239\n",
      "Episode: 1132 meanReward: 388.7812 meanLoss: 16.2594\n",
      "Episode: 1133 meanReward: 381.0312 meanLoss: 33.3069\n",
      "Episode: 1134 meanReward: 381.0312 meanLoss: 13.5644\n",
      "Episode: 1135 meanReward: 377.0000 meanLoss: 22.5198\n",
      "Episode: 1136 meanReward: 377.0000 meanLoss: 12.7149\n",
      "Episode: 1137 meanReward: 369.4062 meanLoss: 99.7350\n",
      "Episode: 1138 meanReward: 381.5625 meanLoss: 15.2929\n",
      "Episode: 1139 meanReward: 392.0312 meanLoss: 9.0378\n",
      "Episode: 1140 meanReward: 381.7500 meanLoss: 35.1705\n",
      "Episode: 1141 meanReward: 369.5000 meanLoss: 12.1403\n",
      "Episode: 1142 meanReward: 360.5938 meanLoss: 4.2738\n",
      "Episode: 1143 meanReward: 350.9375 meanLoss: 4.9128\n",
      "Episode: 1144 meanReward: 350.9375 meanLoss: 2.0250\n",
      "Episode: 1145 meanReward: 343.6562 meanLoss: 14.9356\n",
      "Episode: 1146 meanReward: 332.0938 meanLoss: 17.5300\n",
      "Episode: 1147 meanReward: 332.0938 meanLoss: 2.9655\n",
      "Episode: 1148 meanReward: 316.8438 meanLoss: 262.1433\n",
      "Episode: 1149 meanReward: 316.2188 meanLoss: 45.5355\n",
      "Episode: 1150 meanReward: 309.0000 meanLoss: 5.6544\n",
      "Episode: 1151 meanReward: 311.6250 meanLoss: 11.6619\n",
      "Episode: 1152 meanReward: 311.6250 meanLoss: 2.5146\n",
      "Episode: 1153 meanReward: 299.8438 meanLoss: 78.2896\n",
      "Episode: 1154 meanReward: 303.7188 meanLoss: 5.0268\n",
      "Episode: 1155 meanReward: 302.4375 meanLoss: 3.2332\n",
      "Episode: 1156 meanReward: 292.0625 meanLoss: 3.6901\n",
      "Episode: 1157 meanReward: 285.5625 meanLoss: 3.2061\n",
      "Episode: 1158 meanReward: 299.8438 meanLoss: 2.3306\n",
      "Episode: 1159 meanReward: 299.8438 meanLoss: 19.1616\n",
      "Episode: 1160 meanReward: 288.2812 meanLoss: 71.3973\n",
      "Episode: 1161 meanReward: 273.1875 meanLoss: 22.9958\n",
      "Episode: 1162 meanReward: 273.5312 meanLoss: 16.3096\n",
      "Episode: 1163 meanReward: 280.3750 meanLoss: 22.7138\n",
      "Episode: 1164 meanReward: 279.2500 meanLoss: 17.3285\n",
      "Episode: 1165 meanReward: 271.9062 meanLoss: 147.4173\n",
      "Episode: 1166 meanReward: 257.1250 meanLoss: 353.1950\n",
      "Episode: 1167 meanReward: 246.4375 meanLoss: 334.7419\n",
      "Episode: 1168 meanReward: 231.5938 meanLoss: 287.0766\n",
      "Episode: 1169 meanReward: 231.7500 meanLoss: 98.7298\n",
      "Episode: 1170 meanReward: 223.4375 meanLoss: 20.8684\n",
      "Episode: 1171 meanReward: 223.4375 meanLoss: 5.0492\n",
      "Episode: 1172 meanReward: 223.1875 meanLoss: 45.4696\n",
      "Episode: 1173 meanReward: 235.4375 meanLoss: 6.8109\n",
      "Episode: 1174 meanReward: 244.3438 meanLoss: 12.0435\n",
      "Episode: 1175 meanReward: 254.0000 meanLoss: 16.6600\n",
      "Episode: 1176 meanReward: 254.0000 meanLoss: 16.5502\n",
      "Episode: 1177 meanReward: 261.2812 meanLoss: 16.6474\n",
      "Episode: 1178 meanReward: 272.8438 meanLoss: 15.7121\n",
      "Episode: 1179 meanReward: 272.8438 meanLoss: 13.9253\n",
      "Episode: 1180 meanReward: 277.8750 meanLoss: 52.4589\n",
      "Episode: 1181 meanReward: 274.7812 meanLoss: 66.7858\n",
      "Episode: 1182 meanReward: 277.4062 meanLoss: 10.1379\n",
      "Episode: 1183 meanReward: 277.6562 meanLoss: 75.4546\n",
      "Episode: 1184 meanReward: 277.6562 meanLoss: 7.9743\n",
      "Episode: 1185 meanReward: 275.0000 meanLoss: 236.6856\n",
      "Episode: 1186 meanReward: 273.0000 meanLoss: 38.0822\n",
      "Episode: 1187 meanReward: 270.9375 meanLoss: 56.2254\n",
      "Episode: 1188 meanReward: 266.0000 meanLoss: 103.1291\n",
      "Episode: 1189 meanReward: 257.1562 meanLoss: 171.1671\n",
      "Episode: 1190 meanReward: 241.8438 meanLoss: 248.0396\n",
      "Episode: 1191 meanReward: 226.6875 meanLoss: 145.8258\n",
      "Episode: 1192 meanReward: 225.1250 meanLoss: 25.5003\n",
      "Episode: 1193 meanReward: 227.7812 meanLoss: 86.0250\n",
      "Episode: 1194 meanReward: 221.6875 meanLoss: 123.2850\n",
      "Episode: 1195 meanReward: 212.5000 meanLoss: 35.1165\n",
      "Episode: 1196 meanReward: 200.0625 meanLoss: 10.1579\n",
      "Episode: 1197 meanReward: 201.9688 meanLoss: 8.0779\n",
      "Episode: 1198 meanReward: 203.2188 meanLoss: 4.1869\n",
      "Episode: 1199 meanReward: 204.8750 meanLoss: 4.6228\n",
      "Episode: 1200 meanReward: 206.8750 meanLoss: 10.8357\n",
      "Episode: 1201 meanReward: 206.7188 meanLoss: 4.3558\n",
      "Episode: 1202 meanReward: 206.0938 meanLoss: 3.3117\n",
      "Episode: 1203 meanReward: 196.9062 meanLoss: 6.9764\n",
      "Episode: 1204 meanReward: 199.0938 meanLoss: 4.0944\n",
      "Episode: 1205 meanReward: 199.0938 meanLoss: 2.7651\n",
      "Episode: 1206 meanReward: 199.0938 meanLoss: 5.9152\n",
      "Episode: 1207 meanReward: 188.5938 meanLoss: 51.9283\n",
      "Episode: 1208 meanReward: 188.5938 meanLoss: 4.5141\n",
      "Episode: 1209 meanReward: 188.5938 meanLoss: 17.4157\n",
      "Episode: 1210 meanReward: 188.5938 meanLoss: 16.9872\n",
      "Episode: 1211 meanReward: 186.9688 meanLoss: 20.1954\n",
      "Episode: 1212 meanReward: 197.1875 meanLoss: 4.6257\n",
      "Episode: 1213 meanReward: 197.9688 meanLoss: 73.6114\n",
      "Episode: 1214 meanReward: 190.9062 meanLoss: 8.8138\n",
      "Episode: 1215 meanReward: 203.0625 meanLoss: 3.1171\n",
      "Episode: 1216 meanReward: 194.7500 meanLoss: 40.8204\n",
      "Episode: 1217 meanReward: 197.1250 meanLoss: 80.7940\n",
      "Episode: 1218 meanReward: 210.0938 meanLoss: 6.4575\n",
      "Episode: 1219 meanReward: 209.6875 meanLoss: 81.6624\n",
      "Episode: 1220 meanReward: 214.8438 meanLoss: 10.2418\n",
      "Episode: 1221 meanReward: 220.5000 meanLoss: 7.1765\n",
      "Episode: 1222 meanReward: 235.8125 meanLoss: 3.7425\n",
      "Episode: 1223 meanReward: 250.9688 meanLoss: 17.9429\n",
      "Episode: 1224 meanReward: 257.9062 meanLoss: 29.4772\n",
      "Episode: 1225 meanReward: 270.3438 meanLoss: 13.8320\n",
      "Episode: 1226 meanReward: 283.8125 meanLoss: 16.6281\n",
      "Episode: 1227 meanReward: 284.9375 meanLoss: 83.7432\n",
      "Episode: 1228 meanReward: 286.2188 meanLoss: 10.1302\n",
      "Episode: 1229 meanReward: 284.4062 meanLoss: 57.6924\n",
      "Episode: 1230 meanReward: 297.9375 meanLoss: 8.1437\n",
      "Episode: 1231 meanReward: 311.0000 meanLoss: 15.9641\n",
      "Episode: 1232 meanReward: 312.6562 meanLoss: 57.9811\n",
      "Episode: 1233 meanReward: 325.6562 meanLoss: 3.8671\n",
      "Episode: 1234 meanReward: 326.7188 meanLoss: 58.1030\n",
      "Episode: 1235 meanReward: 325.4062 meanLoss: 12.8880\n",
      "Episode: 1236 meanReward: 330.2500 meanLoss: 13.5582\n",
      "Episode: 1237 meanReward: 330.2500 meanLoss: 5.1555\n",
      "Episode: 1238 meanReward: 315.3438 meanLoss: 304.4106\n",
      "Episode: 1239 meanReward: 310.8125 meanLoss: 337.4133\n",
      "Episode: 1240 meanReward: 310.8125 meanLoss: 10.6428\n",
      "Episode: 1241 meanReward: 310.8125 meanLoss: 16.6846\n",
      "Episode: 1242 meanReward: 310.8125 meanLoss: 15.1079\n",
      "Episode: 1243 meanReward: 297.5000 meanLoss: 233.9535\n",
      "Episode: 1244 meanReward: 297.5000 meanLoss: 11.6982\n",
      "Episode: 1245 meanReward: 309.0000 meanLoss: 17.1445\n",
      "Episode: 1246 meanReward: 320.6562 meanLoss: 16.9799\n",
      "Episode: 1247 meanReward: 307.3438 meanLoss: 116.7945\n",
      "Episode: 1248 meanReward: 315.6562 meanLoss: 12.2679\n",
      "Episode: 1249 meanReward: 315.4062 meanLoss: 72.2059\n",
      "Episode: 1250 meanReward: 303.0938 meanLoss: 30.6500\n",
      "Episode: 1251 meanReward: 302.7500 meanLoss: 17.5736\n",
      "Episode: 1252 meanReward: 301.0312 meanLoss: 7.7659\n",
      "Episode: 1253 meanReward: 298.3750 meanLoss: 12.0807\n",
      "Episode: 1254 meanReward: 285.8125 meanLoss: 13.5746\n",
      "Episode: 1255 meanReward: 270.7500 meanLoss: 35.9553\n",
      "Episode: 1256 meanReward: 265.8438 meanLoss: 36.7315\n",
      "Episode: 1257 meanReward: 261.5000 meanLoss: 6.8622\n",
      "Episode: 1258 meanReward: 249.9688 meanLoss: 42.4871\n",
      "Episode: 1259 meanReward: 262.3125 meanLoss: 8.3459\n",
      "Episode: 1260 meanReward: 264.7500 meanLoss: 47.2574\n",
      "Episode: 1261 meanReward: 279.7500 meanLoss: 11.8948\n",
      "Episode: 1262 meanReward: 279.7500 meanLoss: 16.6108\n",
      "Episode: 1263 meanReward: 268.0000 meanLoss: 65.1895\n",
      "Episode: 1264 meanReward: 279.1875 meanLoss: 1.8941\n",
      "Episode: 1265 meanReward: 279.1875 meanLoss: 15.9853\n",
      "Episode: 1266 meanReward: 278.5938 meanLoss: 58.8473\n",
      "Episode: 1267 meanReward: 274.0000 meanLoss: 38.8807\n",
      "Episode: 1268 meanReward: 265.0938 meanLoss: 42.1851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1269 meanReward: 254.1875 meanLoss: 5.0841\n",
      "Episode: 1270 meanReward: 263.6250 meanLoss: 10.8705\n",
      "Episode: 1271 meanReward: 278.6562 meanLoss: 14.3161\n",
      "Episode: 1272 meanReward: 265.8438 meanLoss: 86.4522\n",
      "Episode: 1273 meanReward: 254.3750 meanLoss: 21.8917\n",
      "Episode: 1274 meanReward: 254.3750 meanLoss: 5.6106\n",
      "Episode: 1275 meanReward: 269.3125 meanLoss: 13.9841\n",
      "Episode: 1276 meanReward: 254.4375 meanLoss: 265.1952\n",
      "Episode: 1277 meanReward: 245.6562 meanLoss: 32.6037\n",
      "Episode: 1278 meanReward: 245.6562 meanLoss: 7.3423\n",
      "Episode: 1279 meanReward: 254.8438 meanLoss: 8.9872\n",
      "Episode: 1280 meanReward: 254.8438 meanLoss: 5.4223\n",
      "Episode: 1281 meanReward: 256.1250 meanLoss: 59.0946\n",
      "Episode: 1282 meanReward: 268.4375 meanLoss: 4.4381\n",
      "Episode: 1283 meanReward: 281.1250 meanLoss: 16.3946\n",
      "Episode: 1284 meanReward: 280.7500 meanLoss: 80.7358\n",
      "Episode: 1285 meanReward: 282.0312 meanLoss: 45.3165\n",
      "Episode: 1286 meanReward: 282.6562 meanLoss: 58.3696\n",
      "Episode: 1287 meanReward: 289.5312 meanLoss: 20.2649\n",
      "Episode: 1288 meanReward: 300.6250 meanLoss: 10.2061\n",
      "Episode: 1289 meanReward: 300.0312 meanLoss: 17.2213\n",
      "Episode: 1290 meanReward: 311.5625 meanLoss: 17.9434\n",
      "Episode: 1291 meanReward: 303.0312 meanLoss: 31.6206\n",
      "Episode: 1292 meanReward: 305.2500 meanLoss: 20.9087\n",
      "Episode: 1293 meanReward: 305.2500 meanLoss: 16.4769\n",
      "Episode: 1294 meanReward: 290.1875 meanLoss: 276.6178\n",
      "Episode: 1295 meanReward: 289.8438 meanLoss: 84.0341\n",
      "Episode: 1296 meanReward: 284.5625 meanLoss: 15.7580\n",
      "Episode: 1297 meanReward: 273.6875 meanLoss: 26.3820\n",
      "Episode: 1298 meanReward: 277.3438 meanLoss: 7.7292\n",
      "Episode: 1299 meanReward: 292.4375 meanLoss: 3.9428\n",
      "Episode: 1300 meanReward: 294.1250 meanLoss: 52.5694\n",
      "Episode: 1301 meanReward: 305.0312 meanLoss: 3.0154\n",
      "Episode: 1302 meanReward: 310.5000 meanLoss: 16.2945\n",
      "Episode: 1303 meanReward: 300.0000 meanLoss: 49.8960\n",
      "Episode: 1304 meanReward: 312.8125 meanLoss: 2.4736\n",
      "Episode: 1305 meanReward: 324.2812 meanLoss: 16.6496\n",
      "Episode: 1306 meanReward: 324.2812 meanLoss: 16.6189\n",
      "Episode: 1307 meanReward: 324.2812 meanLoss: 16.1776\n",
      "Episode: 1308 meanReward: 339.1562 meanLoss: 15.9073\n",
      "Episode: 1309 meanReward: 336.7188 meanLoss: 60.4091\n",
      "Episode: 1310 meanReward: 329.6250 meanLoss: 21.0556\n",
      "Episode: 1311 meanReward: 318.5312 meanLoss: 264.5498\n",
      "Episode: 1312 meanReward: 303.2188 meanLoss: 381.4401\n",
      "Episode: 1313 meanReward: 314.2500 meanLoss: 9.2360\n",
      "Episode: 1314 meanReward: 314.2500 meanLoss: 15.6247\n",
      "Episode: 1315 meanReward: 314.2500 meanLoss: 15.7553\n",
      "Episode: 1316 meanReward: 326.5000 meanLoss: 16.0973\n",
      "Episode: 1317 meanReward: 322.3438 meanLoss: 276.8701\n",
      "Episode: 1318 meanReward: 319.2812 meanLoss: 433.6322\n",
      "Episode: 1319 meanReward: 314.5938 meanLoss: 101.2728\n",
      "Episode: 1320 meanReward: 314.5938 meanLoss: 8.4182\n",
      "Episode: 1321 meanReward: 319.5312 meanLoss: 13.7382\n",
      "Episode: 1322 meanReward: 307.1250 meanLoss: 73.4380\n",
      "Episode: 1323 meanReward: 303.4062 meanLoss: 61.0232\n",
      "Episode: 1324 meanReward: 307.6562 meanLoss: 8.0791\n",
      "Episode: 1325 meanReward: 307.6562 meanLoss: 7.5583\n",
      "Episode: 1326 meanReward: 322.7188 meanLoss: 16.3403\n",
      "Episode: 1327 meanReward: 334.8125 meanLoss: 16.5072\n",
      "Episode: 1328 meanReward: 340.0938 meanLoss: 10.9006\n",
      "Episode: 1329 meanReward: 350.9688 meanLoss: 17.4515\n",
      "Episode: 1330 meanReward: 354.5000 meanLoss: 23.9403\n",
      "Episode: 1331 meanReward: 346.3750 meanLoss: 29.2551\n",
      "Episode: 1332 meanReward: 351.7500 meanLoss: 21.2192\n",
      "Episode: 1333 meanReward: 351.4062 meanLoss: 12.7889\n",
      "Episode: 1334 meanReward: 351.4062 meanLoss: 14.2873\n",
      "Episode: 1335 meanReward: 361.9062 meanLoss: 13.3744\n",
      "Episode: 1336 meanReward: 348.8750 meanLoss: 105.4240\n",
      "Episode: 1337 meanReward: 340.3125 meanLoss: 34.6190\n",
      "Episode: 1338 meanReward: 327.4062 meanLoss: 75.9919\n",
      "Episode: 1339 meanReward: 327.4062 meanLoss: 10.1629\n",
      "Episode: 1340 meanReward: 316.5625 meanLoss: 56.7305\n",
      "Episode: 1341 meanReward: 327.7812 meanLoss: 11.3106\n",
      "Episode: 1342 meanReward: 322.3750 meanLoss: 87.9482\n",
      "Episode: 1343 meanReward: 334.5312 meanLoss: 15.2939\n",
      "Episode: 1344 meanReward: 335.7500 meanLoss: 32.2709\n",
      "Episode: 1345 meanReward: 321.6562 meanLoss: 157.5478\n",
      "Episode: 1346 meanReward: 309.3125 meanLoss: 46.9277\n",
      "Episode: 1347 meanReward: 295.4375 meanLoss: 9.5605\n",
      "Episode: 1348 meanReward: 282.9375 meanLoss: 27.4343\n",
      "Episode: 1349 meanReward: 286.0312 meanLoss: 6.1642\n",
      "Episode: 1350 meanReward: 288.4688 meanLoss: 5.3241\n",
      "Episode: 1351 meanReward: 288.4062 meanLoss: 7.7689\n",
      "Episode: 1352 meanReward: 276.0625 meanLoss: 7.2588\n",
      "Episode: 1353 meanReward: 263.8125 meanLoss: 7.7104\n",
      "Episode: 1354 meanReward: 264.0000 meanLoss: 5.0304\n",
      "Episode: 1355 meanReward: 264.4375 meanLoss: 4.6367\n",
      "Episode: 1356 meanReward: 255.5625 meanLoss: 4.4156\n",
      "Episode: 1357 meanReward: 243.8750 meanLoss: 3.0490\n",
      "Episode: 1358 meanReward: 232.2188 meanLoss: 3.2747\n",
      "Episode: 1359 meanReward: 221.0625 meanLoss: 4.5202\n",
      "Episode: 1360 meanReward: 208.5938 meanLoss: 6.4567\n",
      "Episode: 1361 meanReward: 197.4062 meanLoss: 6.3966\n",
      "Episode: 1362 meanReward: 190.0000 meanLoss: 6.4524\n",
      "Episode: 1363 meanReward: 188.0000 meanLoss: 5.1049\n",
      "Episode: 1364 meanReward: 182.3438 meanLoss: 5.2148\n",
      "Episode: 1365 meanReward: 171.9375 meanLoss: 5.1314\n",
      "Episode: 1366 meanReward: 160.4375 meanLoss: 7.4090\n",
      "Episode: 1367 meanReward: 160.4375 meanLoss: 2.9577\n",
      "Episode: 1368 meanReward: 158.6562 meanLoss: 179.1436\n",
      "Episode: 1369 meanReward: 155.1562 meanLoss: 54.9397\n",
      "Episode: 1370 meanReward: 164.4062 meanLoss: 5.3636\n",
      "Episode: 1371 meanReward: 152.7812 meanLoss: 28.8105\n",
      "Episode: 1372 meanReward: 163.6250 meanLoss: 4.8989\n",
      "Episode: 1373 meanReward: 152.0938 meanLoss: 66.4722\n",
      "Episode: 1374 meanReward: 164.5938 meanLoss: 3.1808\n",
      "Episode: 1375 meanReward: 167.6562 meanLoss: 16.9202\n",
      "Episode: 1376 meanReward: 181.7500 meanLoss: 17.4678\n",
      "Episode: 1377 meanReward: 183.6562 meanLoss: 71.9886\n",
      "Episode: 1378 meanReward: 196.0000 meanLoss: 2.6960\n",
      "Episode: 1379 meanReward: 195.4062 meanLoss: 211.3018\n",
      "Episode: 1380 meanReward: 192.6875 meanLoss: 231.0007\n",
      "Episode: 1381 meanReward: 189.5312 meanLoss: 360.4389\n",
      "Episode: 1382 meanReward: 194.2188 meanLoss: 22.8068\n",
      "Episode: 1383 meanReward: 194.5000 meanLoss: 16.5124\n",
      "Episode: 1384 meanReward: 192.3125 meanLoss: 62.6100\n",
      "Episode: 1385 meanReward: 192.0625 meanLoss: 38.9819\n",
      "Episode: 1386 meanReward: 194.1250 meanLoss: 14.9129\n",
      "Episode: 1387 meanReward: 193.2188 meanLoss: 20.2973\n",
      "Episode: 1388 meanReward: 194.5938 meanLoss: 7.3748\n",
      "Episode: 1389 meanReward: 194.2188 meanLoss: 10.8212\n",
      "Episode: 1390 meanReward: 193.0938 meanLoss: 9.2296\n",
      "Episode: 1391 meanReward: 191.3125 meanLoss: 5.6749\n",
      "Episode: 1392 meanReward: 190.6875 meanLoss: 6.0176\n",
      "Episode: 1393 meanReward: 188.6875 meanLoss: 4.3566\n",
      "Episode: 1394 meanReward: 187.3750 meanLoss: 3.7717\n",
      "Episode: 1395 meanReward: 184.4062 meanLoss: 3.3085\n",
      "Episode: 1396 meanReward: 183.5312 meanLoss: 4.5898\n",
      "Episode: 1397 meanReward: 181.9375 meanLoss: 7.0776\n",
      "Episode: 1398 meanReward: 181.1875 meanLoss: 8.5596\n",
      "Episode: 1399 meanReward: 176.1875 meanLoss: 4.9878\n",
      "Episode: 1400 meanReward: 191.0000 meanLoss: 5.2026\n",
      "Episode: 1401 meanReward: 191.9688 meanLoss: 53.1771\n",
      "Episode: 1402 meanReward: 195.6250 meanLoss: 3.2647\n",
      "Episode: 1403 meanReward: 207.2500 meanLoss: 15.6269\n",
      "Episode: 1404 meanReward: 207.2500 meanLoss: 16.1224\n",
      "Episode: 1405 meanReward: 218.7812 meanLoss: 15.8480\n",
      "Episode: 1406 meanReward: 206.6875 meanLoss: 65.5072\n",
      "Episode: 1407 meanReward: 193.9062 meanLoss: 15.8815\n",
      "Episode: 1408 meanReward: 181.8438 meanLoss: 7.1907\n",
      "Episode: 1409 meanReward: 182.0312 meanLoss: 4.8921\n",
      "Episode: 1410 meanReward: 169.8125 meanLoss: 7.6413\n",
      "Episode: 1411 meanReward: 171.8125 meanLoss: 10.0968\n",
      "Episode: 1412 meanReward: 174.8125 meanLoss: 4.5811\n",
      "Episode: 1413 meanReward: 178.3750 meanLoss: 8.4683\n",
      "Episode: 1414 meanReward: 186.2500 meanLoss: 2.7333\n",
      "Episode: 1415 meanReward: 186.7812 meanLoss: 70.4023\n",
      "Episode: 1416 meanReward: 188.8750 meanLoss: 6.8999\n",
      "Episode: 1417 meanReward: 188.4375 meanLoss: 6.1448\n",
      "Episode: 1418 meanReward: 186.1250 meanLoss: 4.6556\n",
      "Episode: 1419 meanReward: 186.8438 meanLoss: 3.7013\n",
      "Episode: 1420 meanReward: 186.0938 meanLoss: 1.8444\n",
      "Episode: 1421 meanReward: 186.5938 meanLoss: 2.1284\n",
      "Episode: 1422 meanReward: 189.9062 meanLoss: 6.3041\n",
      "Episode: 1423 meanReward: 202.8438 meanLoss: 1.7499\n",
      "Episode: 1424 meanReward: 204.2188 meanLoss: 65.6999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1425 meanReward: 217.4062 meanLoss: 3.8246\n",
      "Episode: 1426 meanReward: 230.4375 meanLoss: 15.7121\n",
      "Episode: 1427 meanReward: 243.5312 meanLoss: 16.2227\n",
      "Episode: 1428 meanReward: 240.9688 meanLoss: 210.4989\n",
      "Episode: 1429 meanReward: 241.0312 meanLoss: 40.4014\n",
      "Episode: 1430 meanReward: 253.2812 meanLoss: 1.5963\n",
      "Episode: 1431 meanReward: 243.5000 meanLoss: 274.9341\n",
      "Episode: 1432 meanReward: 231.5625 meanLoss: 48.6061\n",
      "Episode: 1433 meanReward: 231.0312 meanLoss: 7.0919\n",
      "Episode: 1434 meanReward: 223.0312 meanLoss: 4.6414\n",
      "Episode: 1435 meanReward: 208.3125 meanLoss: 235.5883\n",
      "Episode: 1436 meanReward: 193.0625 meanLoss: 121.0975\n",
      "Episode: 1437 meanReward: 193.0625 meanLoss: 4.7603\n",
      "Episode: 1438 meanReward: 192.5312 meanLoss: 77.4420\n",
      "Episode: 1439 meanReward: 205.3125 meanLoss: 12.9125\n",
      "Episode: 1440 meanReward: 202.9062 meanLoss: 198.8514\n",
      "Episode: 1441 meanReward: 214.9062 meanLoss: 5.8056\n",
      "Episode: 1442 meanReward: 227.1250 meanLoss: 14.5258\n",
      "Episode: 1443 meanReward: 239.5938 meanLoss: 14.4328\n",
      "Episode: 1444 meanReward: 247.2500 meanLoss: 22.6016\n",
      "Episode: 1445 meanReward: 258.9688 meanLoss: 7.9095\n",
      "Episode: 1446 meanReward: 247.5625 meanLoss: 61.7672\n",
      "Episode: 1447 meanReward: 244.9375 meanLoss: 99.6124\n",
      "Episode: 1448 meanReward: 244.3438 meanLoss: 61.4698\n",
      "Episode: 1449 meanReward: 257.2812 meanLoss: 1.3979\n",
      "Episode: 1450 meanReward: 269.7500 meanLoss: 15.3129\n",
      "Episode: 1451 meanReward: 281.7500 meanLoss: 16.0135\n",
      "Episode: 1452 meanReward: 293.3750 meanLoss: 16.6114\n",
      "Episode: 1453 meanReward: 294.5625 meanLoss: 51.4229\n",
      "Episode: 1454 meanReward: 300.1875 meanLoss: 13.7821\n",
      "Episode: 1455 meanReward: 300.1875 meanLoss: 11.1028\n",
      "Episode: 1456 meanReward: 298.7188 meanLoss: 111.7617\n",
      "Episode: 1457 meanReward: 291.6250 meanLoss: 21.9851\n",
      "Episode: 1458 meanReward: 291.6250 meanLoss: 11.7793\n",
      "Episode: 1459 meanReward: 291.6250 meanLoss: 16.0654\n",
      "Episode: 1460 meanReward: 306.0625 meanLoss: 17.1957\n",
      "Episode: 1461 meanReward: 318.3438 meanLoss: 14.2322\n",
      "Episode: 1462 meanReward: 318.3438 meanLoss: 17.0771\n",
      "Episode: 1463 meanReward: 326.8438 meanLoss: 33.5307\n",
      "Episode: 1464 meanReward: 338.7812 meanLoss: 12.3042\n",
      "Episode: 1465 meanReward: 336.7812 meanLoss: 142.8806\n",
      "Episode: 1466 meanReward: 344.7812 meanLoss: 15.7867\n",
      "Episode: 1467 meanReward: 347.7812 meanLoss: 65.0946\n",
      "Episode: 1468 meanReward: 348.4062 meanLoss: 173.5771\n",
      "Episode: 1469 meanReward: 338.3438 meanLoss: 17.9727\n",
      "Episode: 1470 meanReward: 339.3438 meanLoss: 35.4642\n",
      "Episode: 1471 meanReward: 327.7188 meanLoss: 19.5668\n",
      "Episode: 1472 meanReward: 330.5625 meanLoss: 17.1284\n",
      "Episode: 1473 meanReward: 320.5000 meanLoss: 14.0303\n",
      "Episode: 1474 meanReward: 312.8750 meanLoss: 14.4146\n",
      "Episode: 1475 meanReward: 299.4688 meanLoss: 89.3886\n",
      "Episode: 1476 meanReward: 304.0312 meanLoss: 13.2789\n",
      "Episode: 1477 meanReward: 304.0312 meanLoss: 13.9201\n",
      "Episode: 1478 meanReward: 300.7812 meanLoss: 227.7407\n",
      "Episode: 1479 meanReward: 300.3438 meanLoss: 262.1389\n",
      "Episode: 1480 meanReward: 298.2500 meanLoss: 401.1500\n",
      "Episode: 1481 meanReward: 282.9688 meanLoss: 275.1509\n",
      "Episode: 1482 meanReward: 267.6875 meanLoss: 176.1528\n",
      "Episode: 1483 meanReward: 263.0312 meanLoss: 9.7798\n",
      "Episode: 1484 meanReward: 263.0312 meanLoss: 9.6437\n",
      "Episode: 1485 meanReward: 271.7500 meanLoss: 17.3358\n",
      "Episode: 1486 meanReward: 275.5938 meanLoss: 13.7466\n",
      "Episode: 1487 meanReward: 260.5625 meanLoss: 280.3778\n",
      "Episode: 1488 meanReward: 262.6562 meanLoss: 55.3906\n",
      "Episode: 1489 meanReward: 257.2188 meanLoss: 24.6368\n",
      "Episode: 1490 meanReward: 244.6875 meanLoss: 20.9117\n",
      "Episode: 1491 meanReward: 229.9375 meanLoss: 40.5517\n",
      "Episode: 1492 meanReward: 215.1562 meanLoss: 227.0445\n",
      "Episode: 1493 meanReward: 202.1875 meanLoss: 71.5854\n",
      "Episode: 1494 meanReward: 202.1875 meanLoss: 2.1044\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver() # save the trained model\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=batch_size)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            actions_logits = sess.run(model.actions_logits, \n",
    "                                      feed_dict = {model.states: states, \n",
    "                                                   model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            labelQs = np.max(actions_logits, axis=1) # explore\n",
    "            next_actions_logits = sess.run(model.actions_logits, \n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            loss, _, lossQlbl, lossQlbl_sigm, lossQtgt, lossQtgt_sigm = sess.run([model.loss, model.opt, \n",
    "                                                                                  model.lossQlbl, \n",
    "                                                                                  model.lossQlbl_sigm, \n",
    "                                                                                  model.lossQtgt, \n",
    "                                                                                  model.lossQtgt_sigm], \n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs,\n",
    "                                                         model.labelQs: labelQs,\n",
    "                                                         model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'meanReward: {:.4f}'.format(np.mean(episode_reward)),\n",
    "              'meanLoss: {:.4f}'.format(np.mean(loss_batch)))\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        if(np.mean(episode_reward) >= 500):\n",
    "            break\n",
    "    \n",
    "    saver.save(sess, 'checkpoints/model5.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward:120.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                             model.initial_state: initial_state})\n",
    "        action = np.argmax(action_logits)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "print('total_reward:{}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
