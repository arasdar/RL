{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# for steps in range(1111111):\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         print(state.shape)\n",
    "#         break\n",
    "    \n",
    "# print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     #print(state)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# batch = []\n",
    "# while True: # infinite number of steps\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     #print(state, action, reward, done)\n",
    "#     batch.append([action, state, reward, done])\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Act', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Env', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        states_logits = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)\n",
    "        Qlogits = tf.layers.dense(inputs=nl2, units=1, trainable=False)\n",
    "        return states_logits, Qlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    ################################################ a = act(s)\n",
    "    actions_logits = Act(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ################################################ s', r = env(s, a)\n",
    "    ################################################ s', Q = env(s, a)\n",
    "    ################################################ ~s', ~Q = env(s, ~a)\n",
    "    e_next_states_logits, eQs = Env(actions=actions_labels, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size)\n",
    "    a_next_states_logits, aQs = Env(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    eloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=e_next_states_logits, \n",
    "                                                                   labels=next_states_labels))\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                    labels=next_states_labels))\n",
    "    #     eQs_logits = tf.reshape(eQs, shape=[-1])\n",
    "    eQs_logits = tf.nn.tanh(tf.reshape(eQs, shape=[-1]))\n",
    "    aQs_logits = tf.reshape(aQs, shape=[-1])\n",
    "    #     eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=eQs_logits, # GAN\n",
    "    #                                                                     labels=rates)) # 0-1 real\n",
    "    eloss = tf.reduce_mean(tf.square(eQs_logits-rates)) # [-1, +1] real\n",
    "    #################################################### s'', Q' = ~env(s', ~a')\n",
    "    next_actions_logits = Act(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    _, aQs2 = Env(actions=next_actions_logits, states=next_states, hidden_size=hidden_size, \n",
    "                  action_size=action_size, state_size=state_size, reuse=True)\n",
    "    aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                    labels=tf.zeros_like(rates))) # min\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                     labels=tf.ones_like(rates))) # max\n",
    "    ###################################################### Q(s,a)= r + Q'(s',a') # max\n",
    "    ###################################################### ~Q(s,~a)= r # min\n",
    "    ###################################################### ~Q(s,~a)= r + Q'(s',a') # max\n",
    "    targetQs = rewards + (gamma * aQs2_logits)\n",
    "    eloss += tf.reduce_mean(tf.square(eQs_logits - targetQs)) # real\n",
    "    eloss += tf.reduce_mean((aQs_logits+aQs2_logits)/2) # min\n",
    "    aloss2 += -tf.reduce_mean((aQs_logits+aQs2_logits)/2) # max\n",
    "    return actions_logits, aloss, eloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, e_loss, a_loss2, a_learning_rate, e_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('Act')]\n",
    "    e_vars = [var for var in t_vars if var.name.startswith('Env')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        e_opt = tf.train.AdamOptimizer(e_learning_rate).minimize(e_loss, var_list=e_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, e_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, e_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.e_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.e_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        e_loss=self.e_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        e_learning_rate=e_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 'continuous', 4, 'discrete')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_observation_space_type, \\\n",
    "brain.vector_action_space_size, brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "e_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 300*10 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              a_learning_rate=a_learning_rate, e_learning_rate=e_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.00299\n",
      "number of steps per episode: 300 %gone: 0.00599\n",
      "number of steps per episode: 300 %gone: 0.00899\n",
      "number of steps per episode: 300 %gone: 0.01199\n",
      "number of steps per episode: 300 %gone: 0.01499\n",
      "number of steps per episode: 300 %gone: 0.01799\n",
      "number of steps per episode: 300 %gone: 0.02099\n",
      "number of steps per episode: 300 %gone: 0.02399\n",
      "number of steps per episode: 300 %gone: 0.02699\n",
      "number of steps per episode: 300 %gone: 0.02999\n",
      "number of steps per episode: 300 %gone: 0.03299\n",
      "number of steps per episode: 300 %gone: 0.03599\n",
      "number of steps per episode: 300 %gone: 0.03899\n",
      "number of steps per episode: 300 %gone: 0.04199\n",
      "number of steps per episode: 300 %gone: 0.04499\n",
      "number of steps per episode: 300 %gone: 0.04799\n",
      "number of steps per episode: 300 %gone: 0.05099\n",
      "number of steps per episode: 300 %gone: 0.05399\n",
      "number of steps per episode: 300 %gone: 0.05699\n",
      "number of steps per episode: 300 %gone: 0.05999\n",
      "number of steps per episode: 300 %gone: 0.06299\n",
      "number of steps per episode: 300 %gone: 0.06599\n",
      "number of steps per episode: 300 %gone: 0.06899\n",
      "number of steps per episode: 300 %gone: 0.07199\n",
      "number of steps per episode: 300 %gone: 0.07499\n",
      "number of steps per episode: 300 %gone: 0.07799\n",
      "number of steps per episode: 300 %gone: 0.08099\n",
      "number of steps per episode: 300 %gone: 0.08399\n",
      "number of steps per episode: 300 %gone: 0.08699\n",
      "number of steps per episode: 300 %gone: 0.08999\n",
      "number of steps per episode: 300 %gone: 0.09299\n",
      "number of steps per episode: 300 %gone: 0.09599\n",
      "number of steps per episode: 300 %gone: 0.09899\n",
      "number of steps per episode: 300 %gone: 0.10199\n",
      "number of steps per episode: 300 %gone: 0.10499\n",
      "number of steps per episode: 300 %gone: 0.10799\n",
      "number of steps per episode: 300 %gone: 0.11099\n",
      "number of steps per episode: 300 %gone: 0.11399\n",
      "number of steps per episode: 300 %gone: 0.11699\n",
      "number of steps per episode: 300 %gone: 0.11999\n",
      "number of steps per episode: 300 %gone: 0.12299\n",
      "number of steps per episode: 300 %gone: 0.12599\n",
      "number of steps per episode: 300 %gone: 0.12899\n",
      "number of steps per episode: 300 %gone: 0.13199\n",
      "number of steps per episode: 300 %gone: 0.13499\n",
      "number of steps per episode: 300 %gone: 0.13799\n",
      "number of steps per episode: 300 %gone: 0.14099\n",
      "number of steps per episode: 300 %gone: 0.14399\n",
      "number of steps per episode: 300 %gone: 0.14699\n",
      "number of steps per episode: 300 %gone: 0.14999\n",
      "number of steps per episode: 300 %gone: 0.15299\n",
      "number of steps per episode: 300 %gone: 0.15599\n",
      "number of steps per episode: 300 %gone: 0.15899\n",
      "number of steps per episode: 300 %gone: 0.16199\n",
      "number of steps per episode: 300 %gone: 0.16499\n",
      "number of steps per episode: 300 %gone: 0.16799\n",
      "number of steps per episode: 300 %gone: 0.17099\n",
      "number of steps per episode: 300 %gone: 0.17399\n",
      "number of steps per episode: 300 %gone: 0.17699\n",
      "number of steps per episode: 300 %gone: 0.17999\n",
      "number of steps per episode: 300 %gone: 0.18299\n",
      "number of steps per episode: 300 %gone: 0.18599\n",
      "number of steps per episode: 300 %gone: 0.18899\n",
      "number of steps per episode: 300 %gone: 0.19199\n",
      "number of steps per episode: 300 %gone: 0.19499\n",
      "number of steps per episode: 300 %gone: 0.19799\n",
      "number of steps per episode: 300 %gone: 0.20099\n",
      "number of steps per episode: 300 %gone: 0.20399\n",
      "number of steps per episode: 300 %gone: 0.20699\n",
      "number of steps per episode: 300 %gone: 0.20999\n",
      "number of steps per episode: 300 %gone: 0.21299\n",
      "number of steps per episode: 300 %gone: 0.21599\n",
      "number of steps per episode: 300 %gone: 0.21899\n",
      "number of steps per episode: 300 %gone: 0.22199\n",
      "number of steps per episode: 300 %gone: 0.22499\n",
      "number of steps per episode: 300 %gone: 0.22799\n",
      "number of steps per episode: 300 %gone: 0.23099\n",
      "number of steps per episode: 300 %gone: 0.23399\n",
      "number of steps per episode: 300 %gone: 0.23699\n",
      "number of steps per episode: 300 %gone: 0.23999\n",
      "number of steps per episode: 300 %gone: 0.24299\n",
      "number of steps per episode: 300 %gone: 0.24599\n",
      "number of steps per episode: 300 %gone: 0.24899\n",
      "number of steps per episode: 300 %gone: 0.25199\n",
      "number of steps per episode: 300 %gone: 0.25499\n",
      "number of steps per episode: 300 %gone: 0.25799\n",
      "number of steps per episode: 300 %gone: 0.26099\n",
      "number of steps per episode: 300 %gone: 0.26399\n",
      "number of steps per episode: 300 %gone: 0.26699\n",
      "number of steps per episode: 300 %gone: 0.26999\n",
      "number of steps per episode: 300 %gone: 0.27299\n",
      "number of steps per episode: 300 %gone: 0.27599\n",
      "number of steps per episode: 300 %gone: 0.27899\n",
      "number of steps per episode: 300 %gone: 0.28199\n",
      "number of steps per episode: 300 %gone: 0.28499\n",
      "number of steps per episode: 300 %gone: 0.28799\n",
      "number of steps per episode: 300 %gone: 0.29099\n",
      "number of steps per episode: 300 %gone: 0.29399\n",
      "number of steps per episode: 300 %gone: 0.29699\n",
      "number of steps per episode: 300 %gone: 0.29999\n",
      "number of steps per episode: 300 %gone: 0.30299\n",
      "number of steps per episode: 300 %gone: 0.30599\n",
      "number of steps per episode: 300 %gone: 0.30899\n",
      "number of steps per episode: 300 %gone: 0.31199\n",
      "number of steps per episode: 300 %gone: 0.31499\n",
      "number of steps per episode: 300 %gone: 0.31799\n",
      "number of steps per episode: 300 %gone: 0.32099\n",
      "number of steps per episode: 300 %gone: 0.32399\n",
      "number of steps per episode: 300 %gone: 0.32699\n",
      "number of steps per episode: 300 %gone: 0.32999\n",
      "number of steps per episode: 300 %gone: 0.33299\n",
      "number of steps per episode: 300 %gone: 0.33599\n",
      "number of steps per episode: 300 %gone: 0.33899\n",
      "number of steps per episode: 300 %gone: 0.34199\n",
      "number of steps per episode: 300 %gone: 0.34499\n",
      "number of steps per episode: 300 %gone: 0.34799\n",
      "number of steps per episode: 300 %gone: 0.35099\n",
      "number of steps per episode: 300 %gone: 0.35399\n",
      "number of steps per episode: 300 %gone: 0.35699\n",
      "number of steps per episode: 300 %gone: 0.35999\n",
      "number of steps per episode: 300 %gone: 0.36299\n",
      "number of steps per episode: 300 %gone: 0.36599\n",
      "number of steps per episode: 300 %gone: 0.36899\n",
      "number of steps per episode: 300 %gone: 0.37199\n",
      "number of steps per episode: 300 %gone: 0.37499\n",
      "number of steps per episode: 300 %gone: 0.37799\n",
      "number of steps per episode: 300 %gone: 0.38099\n",
      "number of steps per episode: 300 %gone: 0.38399\n",
      "number of steps per episode: 300 %gone: 0.38699\n",
      "number of steps per episode: 300 %gone: 0.38999\n",
      "number of steps per episode: 300 %gone: 0.39299\n",
      "number of steps per episode: 300 %gone: 0.39599\n",
      "number of steps per episode: 300 %gone: 0.39899\n",
      "number of steps per episode: 300 %gone: 0.40199\n",
      "number of steps per episode: 300 %gone: 0.40499\n",
      "number of steps per episode: 300 %gone: 0.40799\n",
      "number of steps per episode: 300 %gone: 0.41099\n",
      "number of steps per episode: 300 %gone: 0.41399\n",
      "number of steps per episode: 300 %gone: 0.41699\n",
      "number of steps per episode: 300 %gone: 0.41999\n",
      "number of steps per episode: 300 %gone: 0.42299\n",
      "number of steps per episode: 300 %gone: 0.42599\n",
      "number of steps per episode: 300 %gone: 0.42899\n",
      "number of steps per episode: 300 %gone: 0.43199\n",
      "number of steps per episode: 300 %gone: 0.43499\n",
      "number of steps per episode: 300 %gone: 0.43799\n",
      "number of steps per episode: 300 %gone: 0.44099\n",
      "number of steps per episode: 300 %gone: 0.44399\n",
      "number of steps per episode: 300 %gone: 0.44699\n",
      "number of steps per episode: 300 %gone: 0.44999\n",
      "number of steps per episode: 300 %gone: 0.45299\n",
      "number of steps per episode: 300 %gone: 0.45599\n",
      "number of steps per episode: 300 %gone: 0.45899\n",
      "number of steps per episode: 300 %gone: 0.46199\n",
      "number of steps per episode: 300 %gone: 0.46499\n",
      "number of steps per episode: 300 %gone: 0.46799\n",
      "number of steps per episode: 300 %gone: 0.47099\n",
      "number of steps per episode: 300 %gone: 0.47399\n",
      "number of steps per episode: 300 %gone: 0.47699\n",
      "number of steps per episode: 300 %gone: 0.47999\n",
      "number of steps per episode: 300 %gone: 0.48299\n",
      "number of steps per episode: 300 %gone: 0.48599\n",
      "number of steps per episode: 300 %gone: 0.48899\n",
      "number of steps per episode: 300 %gone: 0.49199\n",
      "number of steps per episode: 300 %gone: 0.49499\n",
      "number of steps per episode: 300 %gone: 0.49799\n",
      "number of steps per episode: 300 %gone: 0.50099\n",
      "number of steps per episode: 300 %gone: 0.50399\n",
      "number of steps per episode: 300 %gone: 0.50699\n",
      "number of steps per episode: 300 %gone: 0.50999\n",
      "number of steps per episode: 300 %gone: 0.51299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.51599\n",
      "number of steps per episode: 300 %gone: 0.51899\n",
      "number of steps per episode: 300 %gone: 0.52199\n",
      "number of steps per episode: 300 %gone: 0.52499\n",
      "number of steps per episode: 300 %gone: 0.52799\n",
      "number of steps per episode: 300 %gone: 0.53099\n",
      "number of steps per episode: 300 %gone: 0.53399\n",
      "number of steps per episode: 300 %gone: 0.53699\n",
      "number of steps per episode: 300 %gone: 0.53999\n",
      "number of steps per episode: 300 %gone: 0.54299\n",
      "number of steps per episode: 300 %gone: 0.54599\n",
      "number of steps per episode: 300 %gone: 0.54899\n",
      "number of steps per episode: 300 %gone: 0.55199\n",
      "number of steps per episode: 300 %gone: 0.55499\n",
      "number of steps per episode: 300 %gone: 0.55799\n",
      "number of steps per episode: 300 %gone: 0.56099\n",
      "number of steps per episode: 300 %gone: 0.56399\n",
      "number of steps per episode: 300 %gone: 0.56699\n",
      "number of steps per episode: 300 %gone: 0.56999\n",
      "number of steps per episode: 300 %gone: 0.57299\n",
      "number of steps per episode: 300 %gone: 0.57599\n",
      "number of steps per episode: 300 %gone: 0.57899\n",
      "number of steps per episode: 300 %gone: 0.58199\n",
      "number of steps per episode: 300 %gone: 0.58499\n",
      "number of steps per episode: 300 %gone: 0.58799\n",
      "number of steps per episode: 300 %gone: 0.59099\n",
      "number of steps per episode: 300 %gone: 0.59399\n",
      "number of steps per episode: 300 %gone: 0.59699\n",
      "number of steps per episode: 300 %gone: 0.59999\n",
      "number of steps per episode: 300 %gone: 0.60299\n",
      "number of steps per episode: 300 %gone: 0.60599\n",
      "number of steps per episode: 300 %gone: 0.60899\n",
      "number of steps per episode: 300 %gone: 0.61199\n",
      "number of steps per episode: 300 %gone: 0.61499\n",
      "number of steps per episode: 300 %gone: 0.61799\n",
      "number of steps per episode: 300 %gone: 0.62099\n",
      "number of steps per episode: 300 %gone: 0.62399\n",
      "number of steps per episode: 300 %gone: 0.62699\n",
      "number of steps per episode: 300 %gone: 0.62999\n",
      "number of steps per episode: 300 %gone: 0.63299\n",
      "number of steps per episode: 300 %gone: 0.63599\n",
      "number of steps per episode: 300 %gone: 0.63899\n",
      "number of steps per episode: 300 %gone: 0.64199\n",
      "number of steps per episode: 300 %gone: 0.64499\n",
      "number of steps per episode: 300 %gone: 0.64799\n",
      "number of steps per episode: 300 %gone: 0.65099\n",
      "number of steps per episode: 300 %gone: 0.65399\n",
      "number of steps per episode: 300 %gone: 0.65699\n",
      "number of steps per episode: 300 %gone: 0.65999\n",
      "number of steps per episode: 300 %gone: 0.66299\n",
      "number of steps per episode: 300 %gone: 0.66599\n",
      "number of steps per episode: 300 %gone: 0.66899\n",
      "number of steps per episode: 300 %gone: 0.67199\n",
      "number of steps per episode: 300 %gone: 0.67499\n",
      "number of steps per episode: 300 %gone: 0.67799\n",
      "number of steps per episode: 300 %gone: 0.68099\n",
      "number of steps per episode: 300 %gone: 0.68399\n",
      "number of steps per episode: 300 %gone: 0.68699\n",
      "number of steps per episode: 300 %gone: 0.68999\n",
      "number of steps per episode: 300 %gone: 0.69299\n",
      "number of steps per episode: 300 %gone: 0.69599\n",
      "number of steps per episode: 300 %gone: 0.69899\n",
      "number of steps per episode: 300 %gone: 0.70199\n",
      "number of steps per episode: 300 %gone: 0.70499\n",
      "number of steps per episode: 300 %gone: 0.70799\n",
      "number of steps per episode: 300 %gone: 0.71099\n",
      "number of steps per episode: 300 %gone: 0.71399\n",
      "number of steps per episode: 300 %gone: 0.71699\n",
      "number of steps per episode: 300 %gone: 0.71999\n",
      "number of steps per episode: 300 %gone: 0.72299\n",
      "number of steps per episode: 300 %gone: 0.72599\n",
      "number of steps per episode: 300 %gone: 0.72899\n",
      "number of steps per episode: 300 %gone: 0.73199\n",
      "number of steps per episode: 300 %gone: 0.73499\n",
      "number of steps per episode: 300 %gone: 0.73799\n",
      "number of steps per episode: 300 %gone: 0.74099\n",
      "number of steps per episode: 300 %gone: 0.74399\n",
      "number of steps per episode: 300 %gone: 0.74699\n",
      "number of steps per episode: 300 %gone: 0.74999\n",
      "number of steps per episode: 300 %gone: 0.75299\n",
      "number of steps per episode: 300 %gone: 0.75599\n",
      "number of steps per episode: 300 %gone: 0.75899\n",
      "number of steps per episode: 300 %gone: 0.76199\n",
      "number of steps per episode: 300 %gone: 0.76499\n",
      "number of steps per episode: 300 %gone: 0.76799\n",
      "number of steps per episode: 300 %gone: 0.77099\n",
      "number of steps per episode: 300 %gone: 0.77399\n",
      "number of steps per episode: 300 %gone: 0.77699\n",
      "number of steps per episode: 300 %gone: 0.77999\n",
      "number of steps per episode: 300 %gone: 0.78299\n",
      "number of steps per episode: 300 %gone: 0.78599\n",
      "number of steps per episode: 300 %gone: 0.78899\n",
      "number of steps per episode: 300 %gone: 0.79199\n",
      "number of steps per episode: 300 %gone: 0.79499\n",
      "number of steps per episode: 300 %gone: 0.79799\n",
      "number of steps per episode: 300 %gone: 0.80099\n",
      "number of steps per episode: 300 %gone: 0.80399\n",
      "number of steps per episode: 300 %gone: 0.80699\n",
      "number of steps per episode: 300 %gone: 0.80999\n",
      "number of steps per episode: 300 %gone: 0.81299\n",
      "number of steps per episode: 300 %gone: 0.81599\n",
      "number of steps per episode: 300 %gone: 0.81899\n",
      "number of steps per episode: 300 %gone: 0.82199\n",
      "number of steps per episode: 300 %gone: 0.82499\n",
      "number of steps per episode: 300 %gone: 0.82799\n",
      "number of steps per episode: 300 %gone: 0.83099\n",
      "number of steps per episode: 300 %gone: 0.83399\n",
      "number of steps per episode: 300 %gone: 0.83699\n",
      "number of steps per episode: 300 %gone: 0.83999\n",
      "number of steps per episode: 300 %gone: 0.84299\n",
      "number of steps per episode: 300 %gone: 0.84599\n",
      "number of steps per episode: 300 %gone: 0.84899\n",
      "number of steps per episode: 300 %gone: 0.85199\n",
      "number of steps per episode: 300 %gone: 0.85499\n",
      "number of steps per episode: 300 %gone: 0.85799\n",
      "number of steps per episode: 300 %gone: 0.86099\n",
      "number of steps per episode: 300 %gone: 0.86399\n",
      "number of steps per episode: 300 %gone: 0.86699\n",
      "number of steps per episode: 300 %gone: 0.86999\n",
      "number of steps per episode: 300 %gone: 0.87299\n",
      "number of steps per episode: 300 %gone: 0.87599\n",
      "number of steps per episode: 300 %gone: 0.87899\n",
      "number of steps per episode: 300 %gone: 0.88199\n",
      "number of steps per episode: 300 %gone: 0.88499\n",
      "number of steps per episode: 300 %gone: 0.88799\n",
      "number of steps per episode: 300 %gone: 0.89099\n",
      "number of steps per episode: 300 %gone: 0.89399\n",
      "number of steps per episode: 300 %gone: 0.89699\n",
      "number of steps per episode: 300 %gone: 0.89999\n",
      "number of steps per episode: 300 %gone: 0.90299\n",
      "number of steps per episode: 300 %gone: 0.90599\n",
      "number of steps per episode: 300 %gone: 0.90899\n",
      "number of steps per episode: 300 %gone: 0.91199\n",
      "number of steps per episode: 300 %gone: 0.91499\n",
      "number of steps per episode: 300 %gone: 0.91799\n",
      "number of steps per episode: 300 %gone: 0.92099\n",
      "number of steps per episode: 300 %gone: 0.92399\n",
      "number of steps per episode: 300 %gone: 0.92699\n",
      "number of steps per episode: 300 %gone: 0.92999\n",
      "number of steps per episode: 300 %gone: 0.93299\n",
      "number of steps per episode: 300 %gone: 0.93599\n",
      "number of steps per episode: 300 %gone: 0.93899\n",
      "number of steps per episode: 300 %gone: 0.94199\n",
      "number of steps per episode: 300 %gone: 0.94499\n",
      "number of steps per episode: 300 %gone: 0.94799\n",
      "number of steps per episode: 300 %gone: 0.95099\n",
      "number of steps per episode: 300 %gone: 0.95399\n",
      "number of steps per episode: 300 %gone: 0.95699\n",
      "number of steps per episode: 300 %gone: 0.95999\n",
      "number of steps per episode: 300 %gone: 0.96299\n",
      "number of steps per episode: 300 %gone: 0.96599\n",
      "number of steps per episode: 300 %gone: 0.96899\n",
      "number of steps per episode: 300 %gone: 0.97199\n",
      "number of steps per episode: 300 %gone: 0.97499\n",
      "number of steps per episode: 300 %gone: 0.97799\n",
      "number of steps per episode: 300 %gone: 0.98099\n",
      "number of steps per episode: 300 %gone: 0.98399\n",
      "number of steps per episode: 300 %gone: 0.98699\n",
      "number of steps per episode: 300 %gone: 0.98999\n",
      "number of steps per episode: 300 %gone: 0.99299\n",
      "number of steps per episode: 300 %gone: 0.99599\n",
      "number of steps per episode: 300 %gone: 0.99899\n"
     ]
    }
   ],
   "source": [
    "# state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        # state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        total_reward = 0 # reset\n",
    "        print('number of steps per episode:', num_step, \n",
    "              '%gone:', each/memory_size)\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-2.0000 R:-2.0000 rate:-0.1538 aloss:1.4269 eloss:1.3796 aloss2:1.0115 exploreP:0.9707\n",
      "Episode:1 meanR:-1.0000 R:0.0000 rate:0.0000 aloss:1.4086 eloss:0.4856 aloss2:1.8774 exploreP:0.9423\n",
      "Episode:2 meanR:-0.6667 R:0.0000 rate:0.0000 aloss:1.4358 eloss:0.3060 aloss2:2.4196 exploreP:0.9148\n",
      "Episode:3 meanR:-0.2500 R:1.0000 rate:0.0769 aloss:1.4700 eloss:0.1799 aloss2:3.1380 exploreP:0.8881\n",
      "Episode:4 meanR:-0.4000 R:-1.0000 rate:-0.0769 aloss:1.4668 eloss:0.1097 aloss2:3.6037 exploreP:0.8621\n",
      "Episode:5 meanR:-0.5000 R:-1.0000 rate:-0.0769 aloss:1.4334 eloss:0.0958 aloss2:3.6693 exploreP:0.8369\n",
      "Episode:6 meanR:-0.4286 R:0.0000 rate:0.0000 aloss:1.4267 eloss:0.0900 aloss2:3.6764 exploreP:0.8125\n",
      "Episode:7 meanR:-0.3750 R:0.0000 rate:0.0000 aloss:1.4306 eloss:0.0992 aloss2:3.6844 exploreP:0.7888\n",
      "Episode:8 meanR:-0.3333 R:0.0000 rate:0.0000 aloss:1.4498 eloss:0.0887 aloss2:3.6979 exploreP:0.7657\n",
      "Episode:9 meanR:-0.4000 R:-1.0000 rate:-0.0769 aloss:1.5072 eloss:0.1008 aloss2:3.7100 exploreP:0.7434\n",
      "Episode:10 meanR:-0.1818 R:2.0000 rate:0.1538 aloss:1.4615 eloss:0.0707 aloss2:3.7169 exploreP:0.7217\n",
      "Episode:11 meanR:-0.1667 R:0.0000 rate:0.0000 aloss:1.4413 eloss:0.0812 aloss2:3.7132 exploreP:0.7007\n",
      "Episode:12 meanR:0.0000 R:2.0000 rate:0.1538 aloss:1.4443 eloss:0.0671 aloss2:3.7169 exploreP:0.6803\n",
      "Episode:13 meanR:0.0714 R:1.0000 rate:0.0769 aloss:1.4467 eloss:0.0896 aloss2:3.7115 exploreP:0.6605\n",
      "Episode:14 meanR:0.2000 R:2.0000 rate:0.1538 aloss:1.4550 eloss:0.0787 aloss2:3.7121 exploreP:0.6413\n",
      "Episode:15 meanR:0.4375 R:4.0000 rate:0.3077 aloss:1.4526 eloss:0.0786 aloss2:3.7169 exploreP:0.6226\n",
      "Episode:16 meanR:0.4118 R:0.0000 rate:0.0000 aloss:1.4439 eloss:0.0779 aloss2:3.7052 exploreP:0.6045\n",
      "Episode:17 meanR:0.5556 R:3.0000 rate:0.2308 aloss:1.4575 eloss:0.0962 aloss2:3.7063 exploreP:0.5869\n",
      "Episode:18 meanR:0.5263 R:0.0000 rate:0.0000 aloss:1.4627 eloss:0.0807 aloss2:3.7102 exploreP:0.5699\n",
      "Episode:19 meanR:0.5000 R:0.0000 rate:0.0000 aloss:1.4488 eloss:0.0996 aloss2:3.7002 exploreP:0.5533\n",
      "Episode:20 meanR:0.5238 R:1.0000 rate:0.0769 aloss:1.4630 eloss:0.0966 aloss2:3.7057 exploreP:0.5373\n",
      "Episode:21 meanR:0.5455 R:1.0000 rate:0.0769 aloss:1.4579 eloss:0.0796 aloss2:3.7010 exploreP:0.5217\n",
      "Episode:22 meanR:0.5652 R:1.0000 rate:0.0769 aloss:1.4476 eloss:0.0896 aloss2:3.6914 exploreP:0.5066\n",
      "Episode:23 meanR:0.5417 R:0.0000 rate:0.0000 aloss:1.4596 eloss:0.0738 aloss2:3.6973 exploreP:0.4919\n",
      "Episode:24 meanR:0.4800 R:-1.0000 rate:-0.0769 aloss:1.4526 eloss:0.0915 aloss2:3.6872 exploreP:0.4776\n",
      "Episode:25 meanR:0.5385 R:2.0000 rate:0.1538 aloss:1.4415 eloss:0.0843 aloss2:3.6701 exploreP:0.4638\n",
      "Episode:26 meanR:0.5556 R:1.0000 rate:0.0769 aloss:1.4596 eloss:0.0794 aloss2:3.6728 exploreP:0.4504\n",
      "Episode:27 meanR:0.6071 R:2.0000 rate:0.1538 aloss:1.4523 eloss:0.0875 aloss2:3.6604 exploreP:0.4374\n",
      "Episode:28 meanR:0.5862 R:0.0000 rate:0.0000 aloss:1.4525 eloss:0.0912 aloss2:3.6617 exploreP:0.4248\n",
      "Episode:29 meanR:0.5667 R:0.0000 rate:0.0000 aloss:1.4546 eloss:0.0909 aloss2:3.6438 exploreP:0.4125\n",
      "Episode:30 meanR:0.6129 R:2.0000 rate:0.1538 aloss:1.4451 eloss:0.0708 aloss2:3.6447 exploreP:0.4006\n",
      "Episode:31 meanR:0.5938 R:0.0000 rate:0.0000 aloss:1.4417 eloss:0.0923 aloss2:3.6251 exploreP:0.3891\n",
      "Episode:32 meanR:0.5152 R:-2.0000 rate:-0.1538 aloss:1.4433 eloss:0.0727 aloss2:3.6325 exploreP:0.3779\n",
      "Episode:33 meanR:0.5000 R:0.0000 rate:0.0000 aloss:1.4430 eloss:0.0863 aloss2:3.6306 exploreP:0.3670\n",
      "Episode:34 meanR:0.5429 R:2.0000 rate:0.1538 aloss:1.4434 eloss:0.0937 aloss2:3.6159 exploreP:0.3564\n",
      "Episode:35 meanR:0.5000 R:-1.0000 rate:-0.0769 aloss:1.4456 eloss:0.0751 aloss2:3.6238 exploreP:0.3462\n",
      "Episode:36 meanR:0.6216 R:5.0000 rate:0.3846 aloss:1.4515 eloss:0.0727 aloss2:3.6231 exploreP:0.3363\n",
      "Episode:37 meanR:0.6579 R:2.0000 rate:0.1538 aloss:1.4471 eloss:0.0947 aloss2:3.6090 exploreP:0.3266\n",
      "Episode:38 meanR:0.6410 R:0.0000 rate:0.0000 aloss:1.4459 eloss:0.0875 aloss2:3.6110 exploreP:0.3173\n",
      "Episode:39 meanR:0.7750 R:6.0000 rate:0.4615 aloss:1.4513 eloss:0.0643 aloss2:3.6100 exploreP:0.3082\n",
      "Episode:40 meanR:0.7805 R:1.0000 rate:0.0769 aloss:1.4484 eloss:0.0926 aloss2:3.6087 exploreP:0.2994\n",
      "Episode:41 meanR:0.7619 R:0.0000 rate:0.0000 aloss:1.4427 eloss:0.1048 aloss2:3.6058 exploreP:0.2908\n",
      "Episode:42 meanR:0.8372 R:4.0000 rate:0.3077 aloss:1.4541 eloss:0.0855 aloss2:3.6099 exploreP:0.2825\n",
      "Episode:43 meanR:0.8182 R:0.0000 rate:0.0000 aloss:1.4469 eloss:0.0981 aloss2:3.6048 exploreP:0.2745\n",
      "Episode:44 meanR:0.8000 R:0.0000 rate:0.0000 aloss:1.4447 eloss:0.1125 aloss2:3.5998 exploreP:0.2666\n",
      "Episode:45 meanR:0.7826 R:0.0000 rate:0.0000 aloss:1.4404 eloss:0.0883 aloss2:3.6060 exploreP:0.2591\n",
      "Episode:46 meanR:0.8085 R:2.0000 rate:0.1538 aloss:1.4546 eloss:0.0943 aloss2:3.6131 exploreP:0.2517\n",
      "Episode:47 meanR:0.8125 R:1.0000 rate:0.0769 aloss:1.4409 eloss:0.1161 aloss2:3.6200 exploreP:0.2446\n",
      "Episode:48 meanR:0.7551 R:-2.0000 rate:-0.1538 aloss:1.4507 eloss:0.1079 aloss2:3.6256 exploreP:0.2376\n",
      "Episode:49 meanR:0.6800 R:-3.0000 rate:-0.2308 aloss:1.4555 eloss:0.1157 aloss2:3.6244 exploreP:0.2309\n",
      "Episode:50 meanR:0.6667 R:0.0000 rate:0.0000 aloss:1.4485 eloss:0.0869 aloss2:3.6314 exploreP:0.2244\n",
      "Episode:51 meanR:0.6538 R:0.0000 rate:0.0000 aloss:1.4419 eloss:0.1008 aloss2:3.6183 exploreP:0.2180\n",
      "Episode:52 meanR:0.6415 R:0.0000 rate:0.0000 aloss:1.4522 eloss:0.0976 aloss2:3.6069 exploreP:0.2119\n",
      "Episode:53 meanR:0.6296 R:0.0000 rate:0.0000 aloss:1.4535 eloss:0.0847 aloss2:3.6158 exploreP:0.2059\n",
      "Episode:54 meanR:0.5636 R:-3.0000 rate:-0.2308 aloss:1.4467 eloss:0.1056 aloss2:3.6044 exploreP:0.2001\n",
      "Episode:55 meanR:0.5536 R:0.0000 rate:0.0000 aloss:1.4465 eloss:0.1017 aloss2:3.5964 exploreP:0.1945\n",
      "Episode:56 meanR:0.5439 R:0.0000 rate:0.0000 aloss:1.4526 eloss:0.0843 aloss2:3.6091 exploreP:0.1891\n",
      "Episode:57 meanR:0.5862 R:3.0000 rate:0.2308 aloss:1.4427 eloss:0.1002 aloss2:3.6099 exploreP:0.1838\n",
      "Episode:58 meanR:0.5763 R:0.0000 rate:0.0000 aloss:1.4372 eloss:0.1182 aloss2:3.5967 exploreP:0.1786\n",
      "Episode:59 meanR:0.5500 R:-1.0000 rate:-0.0769 aloss:1.4391 eloss:0.1090 aloss2:3.6033 exploreP:0.1736\n",
      "Episode:60 meanR:0.5574 R:1.0000 rate:0.0769 aloss:1.4375 eloss:0.0989 aloss2:3.6035 exploreP:0.1688\n",
      "Episode:61 meanR:0.5806 R:2.0000 rate:0.1538 aloss:1.4449 eloss:0.1049 aloss2:3.6031 exploreP:0.1641\n",
      "Episode:62 meanR:0.5397 R:-2.0000 rate:-0.1538 aloss:1.4429 eloss:0.0952 aloss2:3.6050 exploreP:0.1596\n",
      "Episode:63 meanR:0.5469 R:1.0000 rate:0.0769 aloss:1.4413 eloss:0.1125 aloss2:3.5991 exploreP:0.1551\n",
      "Episode:64 meanR:0.5385 R:0.0000 rate:0.0000 aloss:1.4275 eloss:0.1053 aloss2:3.5978 exploreP:0.1509\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, eloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        aloss_batch, eloss_batch, aloss2_batch = [], [], []\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        num_step = 0\n",
    "        rate = -1\n",
    "        \n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                if np.count_nonzero(dones) > 0 and len(dones) > 1 and np.max(rates) > 0:\n",
    "                    break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            eloss, _ = sess.run([model.e_loss, model.e_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print(len(dones), np.count_nonzero(dones), np.max(rates))\n",
    "            aloss_batch.append(aloss)\n",
    "            eloss_batch.append(eloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # Rating the memory\n",
    "        #if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1) # [-1, +1]\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                    \n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'eloss:{:.4f}'.format(np.mean(eloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        eloss_list.append([ep, np.mean(eloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss2_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
