{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)!  In this notebook, you will learn how to control an agent in a more challenging environment, where it can learn directly from raw pixels!  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/VisualBanana.app\"`\n",
    "- **Windows** (x86): `\"path/to/VisualBanana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/VisualBanana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/VisualBanana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/VisualBanana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `VisualBanana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"VisualBanana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='/home/arasdar/VisualBanana_Linux/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The environment state is an array of raw pixels with shape `(1, 84, 84, 3)`.  *Note that this code differs from the notebook for the project, where we are grabbing **`visual_observations`** (the raw pixels) instead of **`vector_observations`**.* A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States have shape: (1, 84, 84, 3)\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.visual_observations[0]\n",
    "print('States look like:')\n",
    "plt.imshow(np.squeeze(state))\n",
    "plt.show()\n",
    "state_size = state.shape\n",
    "print('States have shape:', state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.visual_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.visual_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    batch.append([state, action, reward, done])\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 1, 84, 84, 3), (300,), (300,), (300,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, actions.shape, rewards.shape, dones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('int64'), dtype('float64'), dtype('bool'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.dtype, actions.dtype, rewards.dtype, dones.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "#state = env_info.vector_observations[0]            # get the current state/ vector\n",
    "state = env_info.visual_observations[0]            # get the current state/ visual\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    #next_state = env_info.vector_observations[0]   # get the next state\n",
    "    next_state = state = env_info.visual_observations[0]            # get the current state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state.shape)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "#state = env_info.vector_observations[0]            # get the current state\n",
    "state = env_info.visual_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    #next_state = env_info.vector_observations[0]   # get the next state\n",
    "    next_state = env_info.visual_observations[0]            # get the current state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([[[[0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           ...,\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686]],\n",
       "  \n",
       "          [[0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           ...,\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686]],\n",
       "  \n",
       "          [[0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           ...,\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686],\n",
       "           [0.83921569, 0.7254902 , 0.59215686]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           ...,\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196]],\n",
       "  \n",
       "          [[0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           ...,\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196]],\n",
       "  \n",
       "          [[0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           [0.76862745, 0.72156863, 0.65882353],\n",
       "           ...,\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196],\n",
       "           [0.75686275, 0.70980392, 0.65490196]]]]), 0.0, False],\n",
       " (1, 84, 84, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([[[[0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          ...,\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686]],\n",
       " \n",
       "         [[0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          ...,\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686]],\n",
       " \n",
       "         [[0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          ...,\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686],\n",
       "          [0.83921569, 0.7254902 , 0.59215686]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          ...,\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196]],\n",
       " \n",
       "         [[0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          ...,\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196]],\n",
       " \n",
       "         [[0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          [0.76862745, 0.72156863, 0.65882353],\n",
       "          ...,\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196],\n",
       "          [0.75686275, 0.70980392, 0.65490196]]]]), 0.0, False]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,) (300, 1, 84, 84, 3) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0\n",
      "4\n",
      "1.0 0.0\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Has to be convolutional!\n",
    "# # Generator: Generating/prediting the actions\n",
    "# def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('generator', reuse=reuse):\n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return actions logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.conv2d(\n",
    "#     inputs, ----------\n",
    "#     filters, -------------\n",
    "#     kernel_size, ------------\n",
    "#     strides=(1, 1), ---------------\n",
    "#     padding='valid', ------------\n",
    "#     data_format='channels_last', -------------> NHWC and not NCHW\n",
    "#     dilation_rate=(1, 1), -------------------\n",
    "#     activation=None,\n",
    "#     use_bias=True, ------------------\n",
    "#     kernel_initializer=None,\n",
    "#     bias_initializer=tf.zeros_initializer(), ------------------\n",
    "#     kernel_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None,\n",
    "#     kernel_constraint=None,\n",
    "#     bias_constraint=None,\n",
    "#     trainable=True, ---------------------\n",
    "#     name=None,\n",
    "#     reuse=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, action_size, reuse=False, alpha=0.1, training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # (300, 1, 84, 84, 3)\n",
    "        inputs_conv = tf.reshape(states, [-1, 84, 84, 3])\n",
    "        print(states.shape, inputs_conv.shape)\n",
    "        \n",
    "        # Input layer is 32x32x3--> (84, 84, 3)\n",
    "        h1 = tf.layers.conv2d(inputs=inputs_conv, filters=8, kernel_size=5, strides=2, padding='same')\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        print(inputs_conv.shape, nl1.shape)\n",
    "        # 16x16x64--> (42, 42, 64)\n",
    "        h2 = tf.layers.conv2d(inputs=nl1, filters=16, kernel_size=5, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        print(nl1.shape, nl2.shape)\n",
    "        # 8x8x128--> (21, 21, 128)        \n",
    "        h3 = tf.layers.conv2d(inputs=nl2, filters=32, kernel_size=6, strides=1, padding='valid')\n",
    "        bn3 = tf.layers.batch_normalization(h3, training=training)\n",
    "        nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        print(nl2.shape, nl3.shape)\n",
    "        # 4x4x256--> (16, 16, 256)\n",
    "        h4 = tf.layers.conv2d(inputs=nl3, filters=64, kernel_size=5, strides=2, padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(h4, training=training)\n",
    "        nl4 = tf.maximum(alpha * bn4, bn4)\n",
    "        print(nl3.shape, nl4.shape)\n",
    "        # 4x4x256--> (8, 8, 512)\n",
    "\n",
    "        # Flatten it\n",
    "        inputs_fc = tf.reshape(tensor=nl4, shape=[-1, 8*8*64])\n",
    "        print(nl4.shape, inputs_fc.shape)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=inputs_fc, units=8*8*64*2)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        print(inputs_fc.shape, nl1.shape)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=8*8*64*2)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        print(nl1.shape, nl2.shape)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        print(nl2.shape, logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, action_size): \n",
    "    actions_logits = generator(states=states, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, states=self.states, actions=self.actions, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 1, 84, 84, 3) actions:(300,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print(np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size_ = (84, 84, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 4\n",
    "# state_size = 37\n",
    "state_size_ = (84, 84, 3)\n",
    "# hidden_size = 37*4             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 84, 84, 3) (?, 84, 84, 3)\n",
      "(?, 84, 84, 3) (?, 42, 42, 8)\n",
      "(?, 42, 42, 8) (?, 21, 21, 16)\n",
      "(?, 21, 21, 16) (?, 16, 16, 32)\n",
      "(?, 16, 16, 32) (?, 8, 8, 64)\n",
      "(?, 8, 8, 64) (?, 4096)\n",
      "(?, 4096) (?, 8192)\n",
      "(?, 8192) (?, 8192)\n",
      "(?, 8192) (?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]   # get the state\n",
    "state = env_info.visual_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    #next_state = env_info.vector_observations[0]   # get the next state\n",
    "    next_state = env_info.visual_observations[0]   # get the state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        #state = env_info.vector_observations[0]   # get the state\n",
    "        state = env_info.visual_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 84, 84, 3), array([ 1,  2,  1, 84, 84,  3]), array([ 1, 84, 84,  3]), 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size, np.array([1, 2, *state_size]), np.array([*state_size]), state_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:1.0000 R:1.0 loss:248.8413 exploreP:0.9707\n",
      "Episode:1 meanR:0.5000 R:0.0 loss:98.5150 exploreP:0.9423\n",
      "Episode:2 meanR:1.0000 R:2.0 loss:138.5586 exploreP:0.9148\n",
      "Episode:3 meanR:1.2500 R:2.0 loss:116.3516 exploreP:0.8881\n",
      "Episode:4 meanR:1.2000 R:1.0 loss:96.4441 exploreP:0.8621\n",
      "Episode:5 meanR:1.1667 R:1.0 loss:141.9816 exploreP:0.8369\n",
      "Episode:6 meanR:0.8571 R:-1.0 loss:140.2749 exploreP:0.8125\n",
      "Episode:7 meanR:0.7500 R:0.0 loss:250.7949 exploreP:0.7888\n",
      "Episode:8 meanR:0.6667 R:0.0 loss:225.4265 exploreP:0.7657\n",
      "Episode:9 meanR:0.6000 R:0.0 loss:367.4743 exploreP:0.7434\n",
      "Episode:10 meanR:0.5455 R:0.0 loss:217.8782 exploreP:0.7217\n",
      "Episode:11 meanR:0.5000 R:0.0 loss:167.7113 exploreP:0.7007\n",
      "Episode:12 meanR:0.5385 R:1.0 loss:248.5802 exploreP:0.6803\n",
      "Episode:13 meanR:0.4286 R:-1.0 loss:188.7454 exploreP:0.6605\n",
      "Episode:14 meanR:0.4000 R:0.0 loss:217.7022 exploreP:0.6413\n",
      "Episode:15 meanR:0.3750 R:0.0 loss:309.3282 exploreP:0.6226\n",
      "Episode:16 meanR:0.3529 R:0.0 loss:303.2146 exploreP:0.6045\n",
      "Episode:17 meanR:0.3333 R:0.0 loss:536.7278 exploreP:0.5869\n",
      "Episode:18 meanR:0.3684 R:1.0 loss:287.9916 exploreP:0.5699\n",
      "Episode:19 meanR:0.2000 R:-3.0 loss:206.4697 exploreP:0.5533\n",
      "Episode:20 meanR:0.1429 R:-1.0 loss:340.8331 exploreP:0.5373\n",
      "Episode:21 meanR:0.0455 R:-2.0 loss:262.6482 exploreP:0.5217\n",
      "Episode:22 meanR:0.0000 R:-1.0 loss:232.6121 exploreP:0.5066\n",
      "Episode:23 meanR:0.0417 R:1.0 loss:314.1636 exploreP:0.4919\n",
      "Episode:24 meanR:0.0400 R:0.0 loss:289.1122 exploreP:0.4776\n",
      "Episode:25 meanR:0.0769 R:1.0 loss:279.3121 exploreP:0.4638\n",
      "Episode:26 meanR:0.0370 R:-1.0 loss:198.2897 exploreP:0.4504\n",
      "Episode:27 meanR:0.0357 R:0.0 loss:297.1993 exploreP:0.4374\n",
      "Episode:28 meanR:0.0000 R:-1.0 loss:236.6259 exploreP:0.4248\n",
      "Episode:29 meanR:0.0667 R:2.0 loss:277.2083 exploreP:0.4125\n",
      "Episode:30 meanR:0.0000 R:-2.0 loss:245.0772 exploreP:0.4006\n",
      "Episode:31 meanR:0.0312 R:1.0 loss:167.4213 exploreP:0.3891\n",
      "Episode:32 meanR:0.0000 R:-1.0 loss:207.3603 exploreP:0.3779\n",
      "Episode:33 meanR:0.0000 R:0.0 loss:198.2442 exploreP:0.3670\n",
      "Episode:34 meanR:0.0000 R:0.0 loss:250.9612 exploreP:0.3564\n",
      "Episode:35 meanR:-0.0278 R:-1.0 loss:175.2073 exploreP:0.3462\n",
      "Episode:36 meanR:-0.0270 R:0.0 loss:195.2734 exploreP:0.3363\n",
      "Episode:37 meanR:0.0000 R:1.0 loss:157.7284 exploreP:0.3266\n",
      "Episode:38 meanR:0.0000 R:0.0 loss:171.5765 exploreP:0.3173\n",
      "Episode:39 meanR:-0.0250 R:-1.0 loss:182.5426 exploreP:0.3082\n",
      "Episode:40 meanR:-0.0488 R:-1.0 loss:261.9396 exploreP:0.2994\n",
      "Episode:41 meanR:-0.0714 R:-1.0 loss:252.9001 exploreP:0.2908\n",
      "Episode:42 meanR:-0.0465 R:1.0 loss:248.9527 exploreP:0.2825\n",
      "Episode:43 meanR:-0.0227 R:1.0 loss:245.7418 exploreP:0.2745\n",
      "Episode:44 meanR:-0.0444 R:-1.0 loss:227.4993 exploreP:0.2666\n",
      "Episode:45 meanR:-0.0217 R:1.0 loss:217.3096 exploreP:0.2591\n",
      "Episode:46 meanR:-0.0213 R:0.0 loss:207.9781 exploreP:0.2517\n",
      "Episode:47 meanR:-0.0417 R:-1.0 loss:194.1128 exploreP:0.2446\n",
      "Episode:48 meanR:-0.0408 R:0.0 loss:253.4826 exploreP:0.2376\n",
      "Episode:49 meanR:-0.0400 R:0.0 loss:186.0840 exploreP:0.2309\n",
      "Episode:50 meanR:-0.0196 R:1.0 loss:212.3565 exploreP:0.2244\n",
      "Episode:51 meanR:-0.0192 R:0.0 loss:215.7902 exploreP:0.2180\n",
      "Episode:52 meanR:-0.0189 R:0.0 loss:259.2079 exploreP:0.2119\n",
      "Episode:53 meanR:-0.0185 R:0.0 loss:224.5700 exploreP:0.2059\n",
      "Episode:54 meanR:-0.0182 R:0.0 loss:243.9743 exploreP:0.2001\n",
      "Episode:55 meanR:-0.0179 R:0.0 loss:241.2746 exploreP:0.1945\n",
      "Episode:56 meanR:-0.0351 R:-1.0 loss:283.8239 exploreP:0.1891\n",
      "Episode:57 meanR:-0.0345 R:0.0 loss:227.8337 exploreP:0.1838\n",
      "Episode:58 meanR:-0.0169 R:1.0 loss:202.4369 exploreP:0.1786\n",
      "Episode:59 meanR:-0.0167 R:0.0 loss:227.5840 exploreP:0.1736\n",
      "Episode:60 meanR:-0.0164 R:0.0 loss:274.6979 exploreP:0.1688\n",
      "Episode:61 meanR:-0.0161 R:0.0 loss:218.8218 exploreP:0.1641\n",
      "Episode:62 meanR:-0.0159 R:0.0 loss:240.5692 exploreP:0.1596\n",
      "Episode:63 meanR:-0.0156 R:0.0 loss:186.9172 exploreP:0.1551\n",
      "Episode:64 meanR:-0.0308 R:-1.0 loss:244.1738 exploreP:0.1509\n",
      "Episode:65 meanR:-0.0303 R:0.0 loss:258.4320 exploreP:0.1467\n",
      "Episode:66 meanR:-0.0448 R:-1.0 loss:295.6471 exploreP:0.1426\n",
      "Episode:67 meanR:-0.0441 R:0.0 loss:334.4271 exploreP:0.1387\n",
      "Episode:68 meanR:-0.0580 R:-1.0 loss:287.0720 exploreP:0.1349\n",
      "Episode:69 meanR:-0.0714 R:-1.0 loss:290.5946 exploreP:0.1312\n",
      "Episode:70 meanR:-0.0563 R:1.0 loss:313.9936 exploreP:0.1276\n",
      "Episode:71 meanR:-0.0556 R:0.0 loss:254.9819 exploreP:0.1242\n",
      "Episode:72 meanR:-0.0822 R:-2.0 loss:324.2359 exploreP:0.1208\n",
      "Episode:73 meanR:-0.0811 R:0.0 loss:239.7096 exploreP:0.1175\n",
      "Episode:74 meanR:-0.0800 R:0.0 loss:308.8934 exploreP:0.1143\n",
      "Episode:75 meanR:-0.0789 R:0.0 loss:280.2713 exploreP:0.1113\n",
      "Episode:76 meanR:-0.0909 R:-1.0 loss:243.9977 exploreP:0.1083\n",
      "Episode:77 meanR:-0.0897 R:0.0 loss:339.6904 exploreP:0.1054\n",
      "Episode:78 meanR:-0.0886 R:0.0 loss:278.7054 exploreP:0.1025\n",
      "Episode:79 meanR:-0.0875 R:0.0 loss:280.3116 exploreP:0.0998\n",
      "Episode:80 meanR:-0.0741 R:1.0 loss:366.1935 exploreP:0.0972\n",
      "Episode:81 meanR:-0.0610 R:1.0 loss:313.6347 exploreP:0.0946\n",
      "Episode:82 meanR:-0.0602 R:0.0 loss:270.2131 exploreP:0.0921\n",
      "Episode:83 meanR:-0.0595 R:0.0 loss:261.4596 exploreP:0.0897\n",
      "Episode:84 meanR:-0.0588 R:0.0 loss:283.8525 exploreP:0.0873\n",
      "Episode:85 meanR:-0.0581 R:0.0 loss:282.0177 exploreP:0.0850\n",
      "Episode:86 meanR:-0.0575 R:0.0 loss:243.2159 exploreP:0.0828\n",
      "Episode:87 meanR:-0.0682 R:-1.0 loss:293.9547 exploreP:0.0806\n",
      "Episode:88 meanR:-0.0899 R:-2.0 loss:388.8006 exploreP:0.0786\n",
      "Episode:89 meanR:-0.0889 R:0.0 loss:273.3302 exploreP:0.0765\n",
      "Episode:90 meanR:-0.0879 R:0.0 loss:268.4601 exploreP:0.0746\n",
      "Episode:91 meanR:-0.0870 R:0.0 loss:268.3294 exploreP:0.0727\n",
      "Episode:92 meanR:-0.0860 R:0.0 loss:324.3476 exploreP:0.0708\n",
      "Episode:93 meanR:-0.1064 R:-2.0 loss:321.3461 exploreP:0.0690\n",
      "Episode:94 meanR:-0.1053 R:0.0 loss:213.1640 exploreP:0.0673\n",
      "Episode:95 meanR:-0.1042 R:0.0 loss:272.1637 exploreP:0.0656\n",
      "Episode:96 meanR:-0.1031 R:0.0 loss:363.1512 exploreP:0.0639\n",
      "Episode:97 meanR:-0.1020 R:0.0 loss:365.2821 exploreP:0.0623\n",
      "Episode:98 meanR:-0.0909 R:1.0 loss:330.5969 exploreP:0.0608\n",
      "Episode:99 meanR:-0.0900 R:0.0 loss:319.7231 exploreP:0.0593\n",
      "Episode:100 meanR:-0.1000 R:0.0 loss:238.5157 exploreP:0.0578\n",
      "Episode:101 meanR:-0.1100 R:-1.0 loss:291.6884 exploreP:0.0564\n",
      "Episode:102 meanR:-0.1200 R:1.0 loss:366.0068 exploreP:0.0550\n",
      "Episode:103 meanR:-0.1600 R:-2.0 loss:238.6533 exploreP:0.0537\n",
      "Episode:104 meanR:-0.1700 R:0.0 loss:251.6301 exploreP:0.0524\n",
      "Episode:105 meanR:-0.1700 R:1.0 loss:370.3142 exploreP:0.0512\n",
      "Episode:106 meanR:-0.1700 R:-1.0 loss:257.0732 exploreP:0.0500\n",
      "Episode:107 meanR:-0.1700 R:0.0 loss:252.8187 exploreP:0.0488\n",
      "Episode:108 meanR:-0.1700 R:0.0 loss:337.5967 exploreP:0.0476\n",
      "Episode:109 meanR:-0.1800 R:-1.0 loss:315.1613 exploreP:0.0465\n",
      "Episode:110 meanR:-0.1700 R:1.0 loss:238.0646 exploreP:0.0454\n",
      "Episode:111 meanR:-0.1700 R:0.0 loss:499.0373 exploreP:0.0444\n",
      "Episode:112 meanR:-0.1900 R:-1.0 loss:296.0841 exploreP:0.0434\n",
      "Episode:113 meanR:-0.1800 R:0.0 loss:393.6409 exploreP:0.0424\n",
      "Episode:114 meanR:-0.1800 R:0.0 loss:299.0426 exploreP:0.0414\n",
      "Episode:115 meanR:-0.1800 R:0.0 loss:409.3640 exploreP:0.0405\n",
      "Episode:116 meanR:-0.1800 R:0.0 loss:455.4940 exploreP:0.0396\n",
      "Episode:117 meanR:-0.1900 R:-1.0 loss:301.1226 exploreP:0.0387\n",
      "Episode:118 meanR:-0.2000 R:0.0 loss:534.1295 exploreP:0.0379\n",
      "Episode:119 meanR:-0.1700 R:0.0 loss:384.5088 exploreP:0.0371\n",
      "Episode:120 meanR:-0.1500 R:1.0 loss:510.3531 exploreP:0.0363\n",
      "Episode:121 meanR:-0.1200 R:1.0 loss:407.6579 exploreP:0.0355\n",
      "Episode:122 meanR:-0.1100 R:0.0 loss:467.6116 exploreP:0.0347\n",
      "Episode:123 meanR:-0.1100 R:1.0 loss:350.3385 exploreP:0.0340\n",
      "Episode:124 meanR:-0.1100 R:0.0 loss:250.9838 exploreP:0.0333\n",
      "Episode:125 meanR:-0.1200 R:0.0 loss:434.0548 exploreP:0.0326\n",
      "Episode:126 meanR:-0.1100 R:0.0 loss:405.6800 exploreP:0.0319\n",
      "Episode:127 meanR:-0.1100 R:0.0 loss:322.6037 exploreP:0.0313\n",
      "Episode:128 meanR:-0.1100 R:-1.0 loss:423.3591 exploreP:0.0306\n",
      "Episode:129 meanR:-0.1300 R:0.0 loss:357.3825 exploreP:0.0300\n",
      "Episode:130 meanR:-0.1000 R:1.0 loss:445.9116 exploreP:0.0294\n",
      "Episode:131 meanR:-0.1100 R:0.0 loss:351.9807 exploreP:0.0289\n",
      "Episode:132 meanR:-0.0900 R:1.0 loss:468.9736 exploreP:0.0283\n",
      "Episode:133 meanR:-0.0900 R:0.0 loss:343.9332 exploreP:0.0278\n",
      "Episode:134 meanR:-0.0900 R:0.0 loss:350.4418 exploreP:0.0272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:135 meanR:-0.0800 R:0.0 loss:374.0437 exploreP:0.0267\n",
      "Episode:136 meanR:-0.0800 R:0.0 loss:363.0454 exploreP:0.0262\n",
      "Episode:137 meanR:-0.0900 R:0.0 loss:401.2091 exploreP:0.0258\n",
      "Episode:138 meanR:-0.0900 R:0.0 loss:435.3401 exploreP:0.0253\n",
      "Episode:139 meanR:-0.0800 R:0.0 loss:474.6342 exploreP:0.0248\n",
      "Episode:140 meanR:-0.0700 R:0.0 loss:505.8083 exploreP:0.0244\n",
      "Episode:141 meanR:-0.0600 R:0.0 loss:377.9589 exploreP:0.0240\n",
      "Episode:142 meanR:-0.0600 R:1.0 loss:521.7863 exploreP:0.0236\n",
      "Episode:143 meanR:-0.0700 R:0.0 loss:490.4817 exploreP:0.0232\n",
      "Episode:144 meanR:-0.0600 R:0.0 loss:436.1820 exploreP:0.0228\n",
      "Episode:145 meanR:-0.0600 R:1.0 loss:519.5782 exploreP:0.0224\n",
      "Episode:146 meanR:-0.0600 R:0.0 loss:723.5962 exploreP:0.0220\n",
      "Episode:147 meanR:-0.0400 R:1.0 loss:602.8748 exploreP:0.0217\n",
      "Episode:148 meanR:-0.0400 R:0.0 loss:423.8438 exploreP:0.0213\n",
      "Episode:149 meanR:-0.0400 R:0.0 loss:482.5841 exploreP:0.0210\n",
      "Episode:150 meanR:-0.0500 R:0.0 loss:504.0408 exploreP:0.0207\n",
      "Episode:151 meanR:-0.0500 R:0.0 loss:723.3088 exploreP:0.0204\n",
      "Episode:152 meanR:-0.0500 R:0.0 loss:596.7482 exploreP:0.0201\n",
      "Episode:153 meanR:-0.0500 R:0.0 loss:442.6340 exploreP:0.0198\n",
      "Episode:154 meanR:-0.0500 R:0.0 loss:441.5152 exploreP:0.0195\n",
      "Episode:155 meanR:-0.0500 R:0.0 loss:527.4358 exploreP:0.0192\n",
      "Episode:156 meanR:-0.0400 R:0.0 loss:580.2669 exploreP:0.0189\n",
      "Episode:157 meanR:-0.0400 R:0.0 loss:561.1137 exploreP:0.0187\n",
      "Episode:158 meanR:-0.0500 R:0.0 loss:554.0974 exploreP:0.0184\n",
      "Episode:159 meanR:-0.0500 R:0.0 loss:445.1397 exploreP:0.0181\n",
      "Episode:160 meanR:-0.0500 R:0.0 loss:492.5104 exploreP:0.0179\n",
      "Episode:161 meanR:-0.0500 R:0.0 loss:435.8407 exploreP:0.0177\n",
      "Episode:162 meanR:-0.0500 R:0.0 loss:530.7208 exploreP:0.0174\n",
      "Episode:163 meanR:-0.0500 R:0.0 loss:589.5009 exploreP:0.0172\n",
      "Episode:164 meanR:-0.0400 R:0.0 loss:564.5261 exploreP:0.0170\n",
      "Episode:165 meanR:-0.0500 R:-1.0 loss:574.2601 exploreP:0.0168\n",
      "Episode:166 meanR:-0.0400 R:0.0 loss:676.3545 exploreP:0.0166\n",
      "Episode:167 meanR:-0.0400 R:0.0 loss:645.7619 exploreP:0.0164\n",
      "Episode:168 meanR:-0.0300 R:0.0 loss:756.3470 exploreP:0.0162\n",
      "Episode:169 meanR:-0.0200 R:0.0 loss:412.6079 exploreP:0.0160\n",
      "Episode:170 meanR:-0.0300 R:0.0 loss:642.6783 exploreP:0.0159\n",
      "Episode:171 meanR:-0.0400 R:-1.0 loss:648.8265 exploreP:0.0157\n",
      "Episode:172 meanR:-0.0200 R:0.0 loss:559.2671 exploreP:0.0155\n",
      "Episode:173 meanR:-0.0200 R:0.0 loss:382.6082 exploreP:0.0154\n",
      "Episode:174 meanR:-0.0200 R:0.0 loss:481.4371 exploreP:0.0152\n",
      "Episode:175 meanR:-0.0200 R:0.0 loss:737.6821 exploreP:0.0150\n",
      "Episode:176 meanR:-0.0100 R:0.0 loss:503.3939 exploreP:0.0149\n",
      "Episode:177 meanR:-0.0100 R:0.0 loss:652.4615 exploreP:0.0147\n",
      "Episode:178 meanR:-0.0100 R:0.0 loss:498.0931 exploreP:0.0146\n",
      "Episode:179 meanR:-0.0100 R:0.0 loss:522.2544 exploreP:0.0145\n",
      "Episode:180 meanR:-0.0200 R:0.0 loss:462.7666 exploreP:0.0143\n",
      "Episode:181 meanR:-0.0300 R:0.0 loss:540.4528 exploreP:0.0142\n",
      "Episode:182 meanR:-0.0300 R:0.0 loss:660.1118 exploreP:0.0141\n",
      "Episode:183 meanR:-0.0400 R:-1.0 loss:634.8567 exploreP:0.0140\n",
      "Episode:184 meanR:-0.0400 R:0.0 loss:1081.3273 exploreP:0.0138\n",
      "Episode:185 meanR:-0.0400 R:0.0 loss:710.6197 exploreP:0.0137\n",
      "Episode:186 meanR:-0.0400 R:0.0 loss:554.2791 exploreP:0.0136\n",
      "Episode:187 meanR:-0.0300 R:0.0 loss:579.8691 exploreP:0.0135\n",
      "Episode:188 meanR:-0.0100 R:0.0 loss:718.2935 exploreP:0.0134\n",
      "Episode:189 meanR:-0.0100 R:0.0 loss:641.9267 exploreP:0.0133\n",
      "Episode:190 meanR:-0.0100 R:0.0 loss:659.6398 exploreP:0.0132\n",
      "Episode:191 meanR:-0.0200 R:-1.0 loss:553.5082 exploreP:0.0131\n",
      "Episode:192 meanR:-0.0200 R:0.0 loss:607.5403 exploreP:0.0130\n",
      "Episode:193 meanR:0.0000 R:0.0 loss:419.6587 exploreP:0.0129\n",
      "Episode:194 meanR:0.0000 R:0.0 loss:495.4466 exploreP:0.0129\n",
      "Episode:195 meanR:-0.0100 R:-1.0 loss:435.9998 exploreP:0.0128\n",
      "Episode:196 meanR:-0.0100 R:0.0 loss:346.9734 exploreP:0.0127\n",
      "Episode:197 meanR:-0.0100 R:0.0 loss:415.2933 exploreP:0.0126\n",
      "Episode:198 meanR:-0.0300 R:-1.0 loss:483.7874 exploreP:0.0125\n",
      "Episode:199 meanR:-0.0300 R:0.0 loss:394.5291 exploreP:0.0125\n",
      "Episode:200 meanR:-0.0300 R:0.0 loss:369.6117 exploreP:0.0124\n",
      "Episode:201 meanR:-0.0200 R:0.0 loss:341.5807 exploreP:0.0123\n",
      "Episode:202 meanR:-0.0200 R:1.0 loss:579.7727 exploreP:0.0122\n",
      "Episode:203 meanR:0.0000 R:0.0 loss:491.9295 exploreP:0.0122\n",
      "Episode:204 meanR:0.0000 R:0.0 loss:426.9656 exploreP:0.0121\n",
      "Episode:205 meanR:-0.0100 R:0.0 loss:529.2367 exploreP:0.0120\n",
      "Episode:206 meanR:0.0000 R:0.0 loss:375.0100 exploreP:0.0120\n",
      "Episode:207 meanR:0.0000 R:0.0 loss:417.0089 exploreP:0.0119\n",
      "Episode:208 meanR:0.0000 R:0.0 loss:424.8776 exploreP:0.0119\n",
      "Episode:209 meanR:0.0100 R:0.0 loss:372.9542 exploreP:0.0118\n",
      "Episode:210 meanR:0.0000 R:0.0 loss:539.8850 exploreP:0.0118\n",
      "Episode:211 meanR:0.0000 R:0.0 loss:485.0644 exploreP:0.0117\n",
      "Episode:212 meanR:0.0100 R:0.0 loss:393.2758 exploreP:0.0117\n",
      "Episode:213 meanR:0.0100 R:0.0 loss:460.3989 exploreP:0.0116\n",
      "Episode:214 meanR:0.0100 R:0.0 loss:469.0482 exploreP:0.0116\n",
      "Episode:215 meanR:0.0100 R:0.0 loss:475.1728 exploreP:0.0115\n",
      "Episode:216 meanR:0.0100 R:0.0 loss:387.1939 exploreP:0.0115\n",
      "Episode:217 meanR:0.0200 R:0.0 loss:433.6981 exploreP:0.0114\n",
      "Episode:218 meanR:0.0200 R:0.0 loss:676.2867 exploreP:0.0114\n",
      "Episode:219 meanR:0.0200 R:0.0 loss:421.3655 exploreP:0.0113\n",
      "Episode:220 meanR:0.0000 R:-1.0 loss:410.5952 exploreP:0.0113\n",
      "Episode:221 meanR:-0.0100 R:0.0 loss:595.7777 exploreP:0.0113\n",
      "Episode:222 meanR:-0.0200 R:-1.0 loss:679.6415 exploreP:0.0112\n",
      "Episode:223 meanR:-0.0300 R:0.0 loss:685.6007 exploreP:0.0112\n",
      "Episode:224 meanR:-0.0200 R:1.0 loss:552.7163 exploreP:0.0112\n",
      "Episode:225 meanR:-0.0200 R:0.0 loss:653.5510 exploreP:0.0111\n",
      "Episode:226 meanR:-0.0200 R:0.0 loss:567.0416 exploreP:0.0111\n",
      "Episode:227 meanR:-0.0200 R:0.0 loss:786.6166 exploreP:0.0111\n",
      "Episode:228 meanR:-0.0100 R:0.0 loss:613.6415 exploreP:0.0110\n",
      "Episode:229 meanR:-0.0100 R:0.0 loss:483.7775 exploreP:0.0110\n",
      "Episode:230 meanR:-0.0200 R:0.0 loss:610.2258 exploreP:0.0110\n",
      "Episode:231 meanR:-0.0200 R:0.0 loss:481.9817 exploreP:0.0109\n",
      "Episode:232 meanR:-0.0300 R:0.0 loss:699.9844 exploreP:0.0109\n",
      "Episode:233 meanR:-0.0300 R:0.0 loss:673.5317 exploreP:0.0109\n",
      "Episode:234 meanR:-0.0300 R:0.0 loss:680.4630 exploreP:0.0109\n",
      "Episode:235 meanR:-0.0300 R:0.0 loss:941.3252 exploreP:0.0108\n",
      "Episode:236 meanR:-0.0300 R:0.0 loss:1062.1648 exploreP:0.0108\n",
      "Episode:237 meanR:-0.0300 R:0.0 loss:820.9033 exploreP:0.0108\n",
      "Episode:238 meanR:-0.0300 R:0.0 loss:487.3333 exploreP:0.0108\n",
      "Episode:239 meanR:-0.0200 R:1.0 loss:753.7401 exploreP:0.0107\n",
      "Episode:240 meanR:-0.0200 R:0.0 loss:792.2615 exploreP:0.0107\n",
      "Episode:241 meanR:-0.0300 R:-1.0 loss:984.7264 exploreP:0.0107\n",
      "Episode:242 meanR:-0.0400 R:0.0 loss:744.5245 exploreP:0.0107\n",
      "Episode:243 meanR:-0.0400 R:0.0 loss:510.7658 exploreP:0.0107\n",
      "Episode:244 meanR:-0.0400 R:0.0 loss:801.3100 exploreP:0.0106\n",
      "Episode:245 meanR:-0.0500 R:0.0 loss:949.2711 exploreP:0.0106\n",
      "Episode:246 meanR:-0.0500 R:0.0 loss:729.7192 exploreP:0.0106\n",
      "Episode:247 meanR:-0.0500 R:1.0 loss:412.3159 exploreP:0.0106\n",
      "Episode:248 meanR:-0.0500 R:0.0 loss:469.5391 exploreP:0.0106\n",
      "Episode:249 meanR:-0.0500 R:0.0 loss:502.0991 exploreP:0.0105\n",
      "Episode:250 meanR:-0.0500 R:0.0 loss:623.1877 exploreP:0.0105\n",
      "Episode:251 meanR:-0.0500 R:0.0 loss:473.5453 exploreP:0.0105\n",
      "Episode:252 meanR:-0.0400 R:1.0 loss:567.3064 exploreP:0.0105\n",
      "Episode:253 meanR:-0.0400 R:0.0 loss:455.4624 exploreP:0.0105\n",
      "Episode:254 meanR:-0.0500 R:-1.0 loss:514.9630 exploreP:0.0105\n",
      "Episode:255 meanR:-0.0500 R:0.0 loss:606.7719 exploreP:0.0105\n",
      "Episode:256 meanR:-0.0500 R:0.0 loss:497.8489 exploreP:0.0104\n",
      "Episode:257 meanR:-0.0500 R:0.0 loss:522.8464 exploreP:0.0104\n",
      "Episode:258 meanR:-0.0400 R:1.0 loss:610.6472 exploreP:0.0104\n",
      "Episode:259 meanR:-0.0500 R:-1.0 loss:347.9768 exploreP:0.0104\n",
      "Episode:260 meanR:-0.0500 R:0.0 loss:579.9676 exploreP:0.0104\n",
      "Episode:261 meanR:-0.0500 R:0.0 loss:752.5627 exploreP:0.0104\n",
      "Episode:262 meanR:-0.0500 R:0.0 loss:412.4544 exploreP:0.0104\n",
      "Episode:263 meanR:-0.0400 R:1.0 loss:554.3138 exploreP:0.0104\n",
      "Episode:264 meanR:-0.0400 R:0.0 loss:614.6154 exploreP:0.0103\n",
      "Episode:265 meanR:-0.0300 R:0.0 loss:553.5895 exploreP:0.0103\n",
      "Episode:266 meanR:-0.0300 R:0.0 loss:724.4869 exploreP:0.0103\n",
      "Episode:267 meanR:-0.0400 R:-1.0 loss:672.8524 exploreP:0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:268 meanR:-0.0400 R:0.0 loss:568.5638 exploreP:0.0103\n",
      "Episode:269 meanR:-0.0400 R:0.0 loss:569.5616 exploreP:0.0103\n",
      "Episode:270 meanR:-0.0400 R:0.0 loss:600.9272 exploreP:0.0103\n",
      "Episode:271 meanR:-0.0200 R:1.0 loss:438.4185 exploreP:0.0103\n",
      "Episode:272 meanR:-0.0300 R:-1.0 loss:584.9707 exploreP:0.0103\n",
      "Episode:273 meanR:-0.0300 R:0.0 loss:718.0470 exploreP:0.0103\n",
      "Episode:274 meanR:-0.0300 R:0.0 loss:927.0754 exploreP:0.0103\n",
      "Episode:275 meanR:-0.0300 R:0.0 loss:603.3928 exploreP:0.0103\n",
      "Episode:276 meanR:-0.0300 R:0.0 loss:517.4122 exploreP:0.0102\n",
      "Episode:277 meanR:-0.0300 R:0.0 loss:783.4156 exploreP:0.0102\n",
      "Episode:278 meanR:-0.0300 R:0.0 loss:685.3973 exploreP:0.0102\n",
      "Episode:279 meanR:-0.0300 R:0.0 loss:518.9283 exploreP:0.0102\n",
      "Episode:280 meanR:-0.0300 R:0.0 loss:803.3394 exploreP:0.0102\n",
      "Episode:281 meanR:-0.0300 R:0.0 loss:910.9353 exploreP:0.0102\n",
      "Episode:282 meanR:-0.0300 R:0.0 loss:490.5414 exploreP:0.0102\n",
      "Episode:283 meanR:-0.0100 R:1.0 loss:480.5339 exploreP:0.0102\n",
      "Episode:284 meanR:-0.0100 R:0.0 loss:471.5822 exploreP:0.0102\n",
      "Episode:285 meanR:0.0000 R:1.0 loss:437.8679 exploreP:0.0102\n",
      "Episode:286 meanR:-0.0100 R:-1.0 loss:496.2125 exploreP:0.0102\n",
      "Episode:287 meanR:-0.0200 R:-1.0 loss:508.6495 exploreP:0.0102\n",
      "Episode:288 meanR:-0.0200 R:0.0 loss:660.8902 exploreP:0.0102\n",
      "Episode:289 meanR:-0.0200 R:0.0 loss:673.8608 exploreP:0.0102\n",
      "Episode:290 meanR:-0.0200 R:0.0 loss:558.1547 exploreP:0.0102\n",
      "Episode:291 meanR:-0.0100 R:0.0 loss:534.7353 exploreP:0.0102\n",
      "Episode:292 meanR:-0.0100 R:0.0 loss:460.3225 exploreP:0.0102\n",
      "Episode:293 meanR:-0.0100 R:0.0 loss:528.5569 exploreP:0.0101\n",
      "Episode:294 meanR:-0.0100 R:0.0 loss:678.1010 exploreP:0.0101\n",
      "Episode:295 meanR:0.0100 R:1.0 loss:672.4814 exploreP:0.0101\n",
      "Episode:296 meanR:0.0100 R:0.0 loss:737.5943 exploreP:0.0101\n",
      "Episode:297 meanR:0.0100 R:0.0 loss:454.9667 exploreP:0.0101\n",
      "Episode:298 meanR:0.0300 R:1.0 loss:639.0858 exploreP:0.0101\n",
      "Episode:299 meanR:0.0200 R:-1.0 loss:624.8241 exploreP:0.0101\n",
      "Episode:300 meanR:0.0200 R:0.0 loss:712.7100 exploreP:0.0101\n",
      "Episode:301 meanR:0.0300 R:1.0 loss:532.7781 exploreP:0.0101\n",
      "Episode:302 meanR:0.0100 R:-1.0 loss:735.2559 exploreP:0.0101\n",
      "Episode:303 meanR:0.0100 R:0.0 loss:896.0629 exploreP:0.0101\n",
      "Episode:304 meanR:0.0100 R:0.0 loss:620.8367 exploreP:0.0101\n",
      "Episode:305 meanR:0.0100 R:0.0 loss:810.4717 exploreP:0.0101\n",
      "Episode:306 meanR:0.0100 R:0.0 loss:678.7357 exploreP:0.0101\n",
      "Episode:307 meanR:0.0100 R:0.0 loss:977.2625 exploreP:0.0101\n",
      "Episode:308 meanR:0.0100 R:0.0 loss:925.0081 exploreP:0.0101\n",
      "Episode:309 meanR:0.0100 R:0.0 loss:874.0836 exploreP:0.0101\n",
      "Episode:310 meanR:0.0100 R:0.0 loss:697.2096 exploreP:0.0101\n",
      "Episode:311 meanR:0.0100 R:0.0 loss:690.4528 exploreP:0.0101\n",
      "Episode:312 meanR:0.0100 R:0.0 loss:761.7740 exploreP:0.0101\n",
      "Episode:313 meanR:0.0000 R:-1.0 loss:898.3623 exploreP:0.0101\n",
      "Episode:314 meanR:0.0000 R:0.0 loss:673.7449 exploreP:0.0101\n",
      "Episode:315 meanR:0.0000 R:0.0 loss:964.6912 exploreP:0.0101\n",
      "Episode:316 meanR:0.0000 R:0.0 loss:809.4416 exploreP:0.0101\n",
      "Episode:317 meanR:0.0000 R:0.0 loss:1049.3929 exploreP:0.0101\n",
      "Episode:318 meanR:-0.0100 R:-1.0 loss:820.7227 exploreP:0.0101\n",
      "Episode:319 meanR:-0.0100 R:0.0 loss:961.0196 exploreP:0.0101\n",
      "Episode:320 meanR:0.0000 R:0.0 loss:877.5488 exploreP:0.0101\n",
      "Episode:321 meanR:-0.0100 R:-1.0 loss:603.3027 exploreP:0.0101\n",
      "Episode:322 meanR:-0.0200 R:-2.0 loss:590.1213 exploreP:0.0101\n",
      "Episode:323 meanR:-0.0200 R:0.0 loss:1008.2146 exploreP:0.0101\n",
      "Episode:324 meanR:-0.0300 R:0.0 loss:668.8729 exploreP:0.0101\n",
      "Episode:325 meanR:-0.0200 R:1.0 loss:686.9382 exploreP:0.0101\n",
      "Episode:326 meanR:-0.0300 R:-1.0 loss:512.6122 exploreP:0.0101\n",
      "Episode:327 meanR:-0.0300 R:0.0 loss:797.3690 exploreP:0.0101\n",
      "Episode:328 meanR:-0.0300 R:0.0 loss:1058.1239 exploreP:0.0101\n",
      "Episode:329 meanR:-0.0300 R:0.0 loss:870.6645 exploreP:0.0100\n",
      "Episode:330 meanR:-0.0300 R:0.0 loss:903.5092 exploreP:0.0100\n",
      "Episode:331 meanR:-0.0400 R:-1.0 loss:809.4148 exploreP:0.0100\n",
      "Episode:332 meanR:-0.0400 R:0.0 loss:916.4615 exploreP:0.0100\n",
      "Episode:333 meanR:-0.0500 R:-1.0 loss:644.9482 exploreP:0.0100\n",
      "Episode:334 meanR:-0.0500 R:0.0 loss:501.8134 exploreP:0.0100\n",
      "Episode:335 meanR:-0.0500 R:0.0 loss:606.5087 exploreP:0.0100\n",
      "Episode:336 meanR:-0.0500 R:0.0 loss:779.9780 exploreP:0.0100\n",
      "Episode:337 meanR:-0.0500 R:0.0 loss:871.8854 exploreP:0.0100\n",
      "Episode:338 meanR:-0.0700 R:-2.0 loss:883.8929 exploreP:0.0100\n",
      "Episode:339 meanR:-0.0800 R:0.0 loss:850.7915 exploreP:0.0100\n",
      "Episode:340 meanR:-0.0800 R:0.0 loss:1057.0325 exploreP:0.0100\n",
      "Episode:341 meanR:-0.0700 R:0.0 loss:678.0531 exploreP:0.0100\n",
      "Episode:342 meanR:-0.0600 R:1.0 loss:524.2011 exploreP:0.0100\n",
      "Episode:343 meanR:-0.0700 R:-1.0 loss:781.0226 exploreP:0.0100\n",
      "Episode:344 meanR:-0.0700 R:0.0 loss:945.0811 exploreP:0.0100\n",
      "Episode:345 meanR:-0.0700 R:0.0 loss:1114.3402 exploreP:0.0100\n",
      "Episode:346 meanR:-0.0700 R:0.0 loss:1138.9663 exploreP:0.0100\n",
      "Episode:347 meanR:-0.0800 R:0.0 loss:860.7043 exploreP:0.0100\n",
      "Episode:348 meanR:-0.0700 R:1.0 loss:673.5203 exploreP:0.0100\n",
      "Episode:349 meanR:-0.0700 R:0.0 loss:825.2709 exploreP:0.0100\n",
      "Episode:350 meanR:-0.0700 R:0.0 loss:1068.1302 exploreP:0.0100\n",
      "Episode:351 meanR:-0.0700 R:0.0 loss:757.7694 exploreP:0.0100\n",
      "Episode:352 meanR:-0.0800 R:0.0 loss:923.2996 exploreP:0.0100\n",
      "Episode:353 meanR:-0.0800 R:0.0 loss:948.6017 exploreP:0.0100\n",
      "Episode:354 meanR:-0.0800 R:-1.0 loss:1009.1461 exploreP:0.0100\n",
      "Episode:355 meanR:-0.0800 R:0.0 loss:856.4385 exploreP:0.0100\n",
      "Episode:356 meanR:-0.0800 R:0.0 loss:879.6710 exploreP:0.0100\n",
      "Episode:357 meanR:-0.0800 R:0.0 loss:1255.3712 exploreP:0.0100\n",
      "Episode:358 meanR:-0.0900 R:0.0 loss:975.1525 exploreP:0.0100\n",
      "Episode:359 meanR:-0.0800 R:0.0 loss:985.4452 exploreP:0.0100\n",
      "Episode:360 meanR:-0.0800 R:0.0 loss:1685.8387 exploreP:0.0100\n",
      "Episode:361 meanR:-0.0800 R:0.0 loss:1089.5983 exploreP:0.0100\n",
      "Episode:362 meanR:-0.0800 R:0.0 loss:1241.3793 exploreP:0.0100\n",
      "Episode:363 meanR:-0.0900 R:0.0 loss:1359.4919 exploreP:0.0100\n",
      "Episode:364 meanR:-0.0900 R:0.0 loss:1245.5369 exploreP:0.0100\n",
      "Episode:365 meanR:-0.0900 R:0.0 loss:1085.1218 exploreP:0.0100\n",
      "Episode:366 meanR:-0.0900 R:0.0 loss:1244.8939 exploreP:0.0100\n",
      "Episode:367 meanR:-0.0800 R:0.0 loss:1047.6748 exploreP:0.0100\n",
      "Episode:368 meanR:-0.0800 R:0.0 loss:1152.3850 exploreP:0.0100\n",
      "Episode:369 meanR:-0.0800 R:0.0 loss:1434.4396 exploreP:0.0100\n",
      "Episode:370 meanR:-0.0800 R:0.0 loss:1393.6327 exploreP:0.0100\n",
      "Episode:371 meanR:-0.0800 R:1.0 loss:1201.5994 exploreP:0.0100\n",
      "Episode:372 meanR:-0.0700 R:0.0 loss:1466.9154 exploreP:0.0100\n",
      "Episode:373 meanR:-0.0700 R:0.0 loss:1085.3364 exploreP:0.0100\n",
      "Episode:374 meanR:-0.0700 R:0.0 loss:954.7257 exploreP:0.0100\n",
      "Episode:375 meanR:-0.0700 R:0.0 loss:1513.3275 exploreP:0.0100\n",
      "Episode:376 meanR:-0.0700 R:0.0 loss:1310.1892 exploreP:0.0100\n",
      "Episode:377 meanR:-0.0700 R:0.0 loss:1227.6217 exploreP:0.0100\n",
      "Episode:378 meanR:-0.0700 R:0.0 loss:1031.6228 exploreP:0.0100\n",
      "Episode:379 meanR:-0.0700 R:0.0 loss:1767.7448 exploreP:0.0100\n",
      "Episode:380 meanR:-0.0700 R:0.0 loss:1149.1886 exploreP:0.0100\n",
      "Episode:381 meanR:-0.0700 R:0.0 loss:1654.1359 exploreP:0.0100\n",
      "Episode:382 meanR:-0.0700 R:0.0 loss:1203.7920 exploreP:0.0100\n",
      "Episode:383 meanR:-0.0700 R:1.0 loss:1294.4377 exploreP:0.0100\n",
      "Episode:384 meanR:-0.0700 R:0.0 loss:1355.4796 exploreP:0.0100\n",
      "Episode:385 meanR:-0.0700 R:1.0 loss:1127.1586 exploreP:0.0100\n",
      "Episode:386 meanR:-0.0600 R:0.0 loss:1062.4891 exploreP:0.0100\n",
      "Episode:387 meanR:-0.0500 R:0.0 loss:1911.2106 exploreP:0.0100\n",
      "Episode:388 meanR:-0.0500 R:0.0 loss:1417.7114 exploreP:0.0100\n",
      "Episode:389 meanR:-0.0500 R:0.0 loss:1180.1112 exploreP:0.0100\n",
      "Episode:390 meanR:-0.0500 R:0.0 loss:1368.2225 exploreP:0.0100\n",
      "Episode:391 meanR:-0.0500 R:0.0 loss:1444.5963 exploreP:0.0100\n",
      "Episode:392 meanR:-0.0500 R:0.0 loss:2761.6641 exploreP:0.0100\n",
      "Episode:393 meanR:-0.0500 R:0.0 loss:1677.5076 exploreP:0.0100\n",
      "Episode:394 meanR:-0.0500 R:0.0 loss:1793.5018 exploreP:0.0100\n",
      "Episode:395 meanR:-0.0600 R:0.0 loss:1727.3679 exploreP:0.0100\n",
      "Episode:396 meanR:-0.0600 R:0.0 loss:1232.4301 exploreP:0.0100\n",
      "Episode:397 meanR:-0.0600 R:0.0 loss:1895.6573 exploreP:0.0100\n",
      "Episode:398 meanR:-0.0700 R:0.0 loss:1643.8109 exploreP:0.0100\n",
      "Episode:399 meanR:-0.0600 R:0.0 loss:1899.1881 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:400 meanR:-0.0600 R:0.0 loss:1361.5634 exploreP:0.0100\n",
      "Episode:401 meanR:-0.0700 R:0.0 loss:2350.1682 exploreP:0.0100\n",
      "Episode:402 meanR:-0.0700 R:-1.0 loss:1063.0642 exploreP:0.0100\n",
      "Episode:403 meanR:-0.0700 R:0.0 loss:1625.2964 exploreP:0.0100\n",
      "Episode:404 meanR:-0.0700 R:0.0 loss:2082.5566 exploreP:0.0100\n",
      "Episode:405 meanR:-0.0700 R:0.0 loss:1721.0906 exploreP:0.0100\n",
      "Episode:406 meanR:-0.0700 R:0.0 loss:1560.8313 exploreP:0.0100\n",
      "Episode:407 meanR:-0.0700 R:0.0 loss:2672.0085 exploreP:0.0100\n",
      "Episode:408 meanR:-0.0700 R:0.0 loss:958.2969 exploreP:0.0100\n",
      "Episode:409 meanR:-0.0700 R:0.0 loss:1965.1801 exploreP:0.0100\n",
      "Episode:410 meanR:-0.0700 R:0.0 loss:1730.0000 exploreP:0.0100\n",
      "Episode:411 meanR:-0.0700 R:0.0 loss:1165.2562 exploreP:0.0100\n",
      "Episode:412 meanR:-0.0700 R:0.0 loss:1576.4663 exploreP:0.0100\n",
      "Episode:413 meanR:-0.0600 R:0.0 loss:1490.6974 exploreP:0.0100\n",
      "Episode:414 meanR:-0.0700 R:-1.0 loss:2604.3857 exploreP:0.0100\n",
      "Episode:415 meanR:-0.0700 R:0.0 loss:1746.8824 exploreP:0.0100\n",
      "Episode:416 meanR:-0.0700 R:0.0 loss:2620.9119 exploreP:0.0100\n",
      "Episode:417 meanR:-0.0700 R:0.0 loss:2700.8147 exploreP:0.0100\n",
      "Episode:418 meanR:-0.0700 R:-1.0 loss:2640.7163 exploreP:0.0100\n",
      "Episode:419 meanR:-0.0700 R:0.0 loss:2139.1628 exploreP:0.0100\n",
      "Episode:420 meanR:-0.0800 R:-1.0 loss:1694.5813 exploreP:0.0100\n",
      "Episode:421 meanR:-0.0700 R:0.0 loss:2743.6655 exploreP:0.0100\n",
      "Episode:422 meanR:-0.0500 R:0.0 loss:2753.6716 exploreP:0.0100\n",
      "Episode:423 meanR:-0.0500 R:0.0 loss:4056.9695 exploreP:0.0100\n",
      "Episode:424 meanR:-0.0500 R:0.0 loss:3243.5483 exploreP:0.0100\n",
      "Episode:425 meanR:-0.0600 R:0.0 loss:3433.3960 exploreP:0.0100\n",
      "Episode:426 meanR:-0.0500 R:0.0 loss:3673.4229 exploreP:0.0100\n",
      "Episode:427 meanR:-0.0500 R:0.0 loss:2787.5767 exploreP:0.0100\n",
      "Episode:428 meanR:-0.0500 R:0.0 loss:1895.3455 exploreP:0.0100\n",
      "Episode:429 meanR:-0.0400 R:1.0 loss:2646.9399 exploreP:0.0100\n",
      "Episode:430 meanR:-0.0400 R:0.0 loss:2468.7229 exploreP:0.0100\n",
      "Episode:431 meanR:-0.0400 R:-1.0 loss:3105.3740 exploreP:0.0100\n",
      "Episode:432 meanR:-0.0400 R:0.0 loss:2907.1689 exploreP:0.0100\n",
      "Episode:433 meanR:-0.0300 R:0.0 loss:2691.5630 exploreP:0.0100\n",
      "Episode:434 meanR:-0.0300 R:0.0 loss:4629.6821 exploreP:0.0100\n",
      "Episode:435 meanR:-0.0300 R:0.0 loss:2951.0911 exploreP:0.0100\n",
      "Episode:436 meanR:-0.0300 R:0.0 loss:3709.9666 exploreP:0.0100\n",
      "Episode:437 meanR:-0.0300 R:0.0 loss:3450.5132 exploreP:0.0100\n",
      "Episode:438 meanR:-0.0100 R:0.0 loss:2925.2793 exploreP:0.0100\n",
      "Episode:439 meanR:-0.0100 R:0.0 loss:3830.9946 exploreP:0.0100\n",
      "Episode:440 meanR:-0.0100 R:0.0 loss:2346.5334 exploreP:0.0100\n",
      "Episode:441 meanR:-0.0100 R:0.0 loss:3379.2888 exploreP:0.0100\n",
      "Episode:442 meanR:-0.0200 R:0.0 loss:2688.6416 exploreP:0.0100\n",
      "Episode:443 meanR:0.0000 R:1.0 loss:3115.4922 exploreP:0.0100\n",
      "Episode:444 meanR:0.0000 R:0.0 loss:3972.9666 exploreP:0.0100\n",
      "Episode:445 meanR:-0.0100 R:-1.0 loss:3129.7615 exploreP:0.0100\n",
      "Episode:446 meanR:-0.0100 R:0.0 loss:3227.5247 exploreP:0.0100\n",
      "Episode:447 meanR:-0.0100 R:0.0 loss:3453.7441 exploreP:0.0100\n",
      "Episode:448 meanR:-0.0200 R:0.0 loss:3071.6697 exploreP:0.0100\n",
      "Episode:449 meanR:-0.0300 R:-1.0 loss:3849.0312 exploreP:0.0100\n",
      "Episode:450 meanR:-0.0300 R:0.0 loss:3335.8103 exploreP:0.0100\n",
      "Episode:451 meanR:-0.0300 R:0.0 loss:2954.2441 exploreP:0.0100\n",
      "Episode:452 meanR:-0.0300 R:0.0 loss:3230.0977 exploreP:0.0100\n",
      "Episode:453 meanR:-0.0300 R:0.0 loss:2896.1174 exploreP:0.0100\n",
      "Episode:454 meanR:-0.0200 R:0.0 loss:4017.2566 exploreP:0.0100\n",
      "Episode:455 meanR:-0.0200 R:0.0 loss:3251.0200 exploreP:0.0100\n",
      "Episode:456 meanR:-0.0200 R:0.0 loss:3307.4824 exploreP:0.0100\n",
      "Episode:457 meanR:-0.0200 R:0.0 loss:3260.5237 exploreP:0.0100\n",
      "Episode:458 meanR:-0.0300 R:-1.0 loss:3530.1216 exploreP:0.0100\n",
      "Episode:459 meanR:-0.0100 R:2.0 loss:3319.2727 exploreP:0.0100\n",
      "Episode:460 meanR:-0.0200 R:-1.0 loss:3146.5867 exploreP:0.0100\n",
      "Episode:461 meanR:-0.0200 R:0.0 loss:2393.6462 exploreP:0.0100\n",
      "Episode:462 meanR:-0.0100 R:1.0 loss:2404.1833 exploreP:0.0100\n",
      "Episode:463 meanR:-0.0100 R:0.0 loss:3040.7434 exploreP:0.0100\n",
      "Episode:464 meanR:-0.0100 R:0.0 loss:3485.0188 exploreP:0.0100\n",
      "Episode:465 meanR:-0.0100 R:0.0 loss:3360.8809 exploreP:0.0100\n",
      "Episode:466 meanR:0.0000 R:1.0 loss:2996.6609 exploreP:0.0100\n",
      "Episode:467 meanR:0.0300 R:3.0 loss:2184.2629 exploreP:0.0100\n",
      "Episode:468 meanR:0.0200 R:-1.0 loss:3676.2317 exploreP:0.0100\n",
      "Episode:469 meanR:0.0200 R:0.0 loss:3484.7898 exploreP:0.0100\n",
      "Episode:470 meanR:0.0200 R:0.0 loss:2722.1074 exploreP:0.0100\n",
      "Episode:471 meanR:0.0000 R:-1.0 loss:3579.5745 exploreP:0.0100\n",
      "Episode:472 meanR:0.0000 R:0.0 loss:2043.0931 exploreP:0.0100\n",
      "Episode:473 meanR:0.0000 R:0.0 loss:2580.7830 exploreP:0.0100\n",
      "Episode:474 meanR:0.0000 R:0.0 loss:2962.8904 exploreP:0.0100\n",
      "Episode:475 meanR:0.0000 R:0.0 loss:2470.9014 exploreP:0.0100\n",
      "Episode:476 meanR:-0.0100 R:-1.0 loss:3352.6880 exploreP:0.0100\n",
      "Episode:477 meanR:-0.0200 R:-1.0 loss:3342.9849 exploreP:0.0100\n",
      "Episode:478 meanR:-0.0200 R:0.0 loss:2743.5354 exploreP:0.0100\n",
      "Episode:479 meanR:-0.0200 R:0.0 loss:3713.7292 exploreP:0.0100\n",
      "Episode:480 meanR:-0.0200 R:0.0 loss:3422.6838 exploreP:0.0100\n",
      "Episode:481 meanR:-0.0200 R:0.0 loss:3237.6497 exploreP:0.0100\n",
      "Episode:482 meanR:-0.0200 R:0.0 loss:2653.3643 exploreP:0.0100\n",
      "Episode:483 meanR:-0.0200 R:1.0 loss:3525.8433 exploreP:0.0100\n",
      "Episode:484 meanR:-0.0200 R:0.0 loss:3488.9141 exploreP:0.0100\n",
      "Episode:485 meanR:-0.0300 R:0.0 loss:2220.2603 exploreP:0.0100\n",
      "Episode:486 meanR:-0.0100 R:2.0 loss:2610.6003 exploreP:0.0100\n",
      "Episode:487 meanR:-0.0100 R:0.0 loss:2185.3813 exploreP:0.0100\n",
      "Episode:488 meanR:-0.0100 R:0.0 loss:2232.1711 exploreP:0.0100\n",
      "Episode:489 meanR:-0.0300 R:-2.0 loss:3020.9263 exploreP:0.0100\n",
      "Episode:490 meanR:-0.0300 R:0.0 loss:3099.1150 exploreP:0.0100\n",
      "Episode:491 meanR:-0.0300 R:0.0 loss:1781.3125 exploreP:0.0100\n",
      "Episode:492 meanR:-0.0300 R:0.0 loss:1765.7972 exploreP:0.0100\n",
      "Episode:493 meanR:-0.0300 R:0.0 loss:2299.5093 exploreP:0.0100\n",
      "Episode:494 meanR:-0.0400 R:-1.0 loss:2811.2852 exploreP:0.0100\n",
      "Episode:495 meanR:-0.0500 R:-1.0 loss:1907.1453 exploreP:0.0100\n",
      "Episode:496 meanR:-0.0400 R:1.0 loss:1221.0479 exploreP:0.0100\n",
      "Episode:497 meanR:-0.0400 R:0.0 loss:1318.5864 exploreP:0.0100\n",
      "Episode:498 meanR:-0.0500 R:-1.0 loss:2609.7292 exploreP:0.0100\n",
      "Episode:499 meanR:-0.0500 R:0.0 loss:1962.0212 exploreP:0.0100\n",
      "Episode:500 meanR:-0.0500 R:0.0 loss:2010.4408 exploreP:0.0100\n",
      "Episode:501 meanR:-0.0500 R:0.0 loss:1548.8894 exploreP:0.0100\n",
      "Episode:502 meanR:-0.0400 R:0.0 loss:1898.0934 exploreP:0.0100\n",
      "Episode:503 meanR:-0.0300 R:1.0 loss:2387.3193 exploreP:0.0100\n",
      "Episode:504 meanR:-0.0300 R:0.0 loss:1952.3008 exploreP:0.0100\n",
      "Episode:505 meanR:-0.0300 R:0.0 loss:1427.9213 exploreP:0.0100\n",
      "Episode:506 meanR:-0.0300 R:0.0 loss:1519.1140 exploreP:0.0100\n",
      "Episode:507 meanR:-0.0400 R:-1.0 loss:1306.2477 exploreP:0.0100\n",
      "Episode:508 meanR:-0.0400 R:0.0 loss:1882.4938 exploreP:0.0100\n",
      "Episode:509 meanR:-0.0400 R:0.0 loss:1680.9043 exploreP:0.0100\n",
      "Episode:510 meanR:-0.0400 R:0.0 loss:1508.7281 exploreP:0.0100\n",
      "Episode:511 meanR:-0.0500 R:-1.0 loss:1888.4988 exploreP:0.0100\n",
      "Episode:512 meanR:-0.0500 R:0.0 loss:2135.9666 exploreP:0.0100\n",
      "Episode:513 meanR:-0.0500 R:0.0 loss:2153.7217 exploreP:0.0100\n",
      "Episode:514 meanR:-0.0400 R:0.0 loss:1868.7633 exploreP:0.0100\n",
      "Episode:515 meanR:-0.0400 R:0.0 loss:2132.8350 exploreP:0.0100\n",
      "Episode:516 meanR:-0.0400 R:0.0 loss:1887.7715 exploreP:0.0100\n",
      "Episode:517 meanR:-0.0300 R:1.0 loss:3979.7749 exploreP:0.0100\n",
      "Episode:518 meanR:-0.0200 R:0.0 loss:2700.0520 exploreP:0.0100\n",
      "Episode:519 meanR:-0.0200 R:0.0 loss:3203.1343 exploreP:0.0100\n",
      "Episode:520 meanR:-0.0200 R:-1.0 loss:2866.6921 exploreP:0.0100\n",
      "Episode:521 meanR:-0.0200 R:0.0 loss:2701.9456 exploreP:0.0100\n",
      "Episode:522 meanR:-0.0200 R:0.0 loss:3059.2842 exploreP:0.0100\n",
      "Episode:523 meanR:-0.0200 R:0.0 loss:1684.6261 exploreP:0.0100\n",
      "Episode:524 meanR:-0.0200 R:0.0 loss:2613.1670 exploreP:0.0100\n",
      "Episode:525 meanR:-0.0200 R:0.0 loss:3255.7849 exploreP:0.0100\n",
      "Episode:526 meanR:-0.0100 R:1.0 loss:2394.2251 exploreP:0.0100\n",
      "Episode:527 meanR:-0.0100 R:0.0 loss:2752.6521 exploreP:0.0100\n",
      "Episode:528 meanR:-0.0100 R:0.0 loss:2217.6304 exploreP:0.0100\n",
      "Episode:529 meanR:-0.0200 R:0.0 loss:2077.8416 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:530 meanR:-0.0200 R:0.0 loss:2341.8772 exploreP:0.0100\n",
      "Episode:531 meanR:-0.0100 R:0.0 loss:2249.6299 exploreP:0.0100\n",
      "Episode:532 meanR:-0.0100 R:0.0 loss:3121.6628 exploreP:0.0100\n",
      "Episode:533 meanR:-0.0100 R:0.0 loss:1780.6914 exploreP:0.0100\n",
      "Episode:534 meanR:-0.0100 R:0.0 loss:1849.2900 exploreP:0.0100\n",
      "Episode:535 meanR:-0.0100 R:0.0 loss:1841.1309 exploreP:0.0100\n",
      "Episode:536 meanR:-0.0100 R:0.0 loss:1976.1442 exploreP:0.0100\n",
      "Episode:537 meanR:-0.0100 R:0.0 loss:1583.0854 exploreP:0.0100\n",
      "Episode:538 meanR:-0.0100 R:0.0 loss:2685.2793 exploreP:0.0100\n",
      "Episode:539 meanR:-0.0100 R:0.0 loss:2333.8132 exploreP:0.0100\n",
      "Episode:540 meanR:0.0000 R:1.0 loss:2200.5166 exploreP:0.0100\n",
      "Episode:541 meanR:-0.0100 R:-1.0 loss:2724.4155 exploreP:0.0100\n",
      "Episode:542 meanR:-0.0200 R:-1.0 loss:1735.2957 exploreP:0.0100\n",
      "Episode:543 meanR:-0.0200 R:1.0 loss:2502.9563 exploreP:0.0100\n",
      "Episode:544 meanR:-0.0200 R:0.0 loss:1659.5033 exploreP:0.0100\n",
      "Episode:545 meanR:-0.0100 R:0.0 loss:1705.9202 exploreP:0.0100\n",
      "Episode:546 meanR:0.0000 R:1.0 loss:1516.0719 exploreP:0.0100\n",
      "Episode:547 meanR:0.0000 R:0.0 loss:2253.0137 exploreP:0.0100\n",
      "Episode:548 meanR:0.0000 R:0.0 loss:1614.2117 exploreP:0.0100\n",
      "Episode:549 meanR:0.0100 R:0.0 loss:2169.8516 exploreP:0.0100\n",
      "Episode:550 meanR:0.0100 R:0.0 loss:2526.2598 exploreP:0.0100\n",
      "Episode:551 meanR:0.0100 R:0.0 loss:2048.0532 exploreP:0.0100\n",
      "Episode:552 meanR:0.0100 R:0.0 loss:2168.1726 exploreP:0.0100\n",
      "Episode:553 meanR:0.0100 R:0.0 loss:2343.0295 exploreP:0.0100\n",
      "Episode:554 meanR:0.0100 R:0.0 loss:1694.3888 exploreP:0.0100\n",
      "Episode:555 meanR:0.0100 R:0.0 loss:2121.6287 exploreP:0.0100\n",
      "Episode:556 meanR:0.0100 R:0.0 loss:2296.6387 exploreP:0.0100\n",
      "Episode:557 meanR:0.0200 R:1.0 loss:2282.8801 exploreP:0.0100\n",
      "Episode:558 meanR:0.0300 R:0.0 loss:2554.8574 exploreP:0.0100\n",
      "Episode:559 meanR:0.0100 R:0.0 loss:1928.5375 exploreP:0.0100\n",
      "Episode:560 meanR:0.0100 R:-1.0 loss:2226.8425 exploreP:0.0100\n",
      "Episode:561 meanR:0.0100 R:0.0 loss:3433.5342 exploreP:0.0100\n",
      "Episode:562 meanR:0.0000 R:0.0 loss:2696.7766 exploreP:0.0100\n",
      "Episode:563 meanR:0.0000 R:0.0 loss:2884.8374 exploreP:0.0100\n",
      "Episode:564 meanR:0.0000 R:0.0 loss:2984.6555 exploreP:0.0100\n",
      "Episode:565 meanR:0.0000 R:0.0 loss:3407.3730 exploreP:0.0100\n",
      "Episode:566 meanR:-0.0100 R:0.0 loss:3849.2283 exploreP:0.0100\n",
      "Episode:567 meanR:-0.0300 R:1.0 loss:3395.6174 exploreP:0.0100\n",
      "Episode:568 meanR:-0.0200 R:0.0 loss:4953.2334 exploreP:0.0100\n",
      "Episode:569 meanR:-0.0200 R:0.0 loss:5148.2534 exploreP:0.0100\n",
      "Episode:570 meanR:-0.0200 R:0.0 loss:6120.5991 exploreP:0.0100\n",
      "Episode:571 meanR:-0.0100 R:0.0 loss:9668.0381 exploreP:0.0100\n",
      "Episode:572 meanR:-0.0100 R:0.0 loss:8567.0547 exploreP:0.0100\n",
      "Episode:573 meanR:-0.0100 R:0.0 loss:12949.9814 exploreP:0.0100\n",
      "Episode:574 meanR:-0.0100 R:0.0 loss:13923.0469 exploreP:0.0100\n",
      "Episode:575 meanR:0.0000 R:1.0 loss:16192.0068 exploreP:0.0100\n",
      "Episode:576 meanR:0.0100 R:0.0 loss:12853.3848 exploreP:0.0100\n",
      "Episode:577 meanR:0.0300 R:1.0 loss:15460.8604 exploreP:0.0100\n",
      "Episode:578 meanR:0.0300 R:0.0 loss:14257.0430 exploreP:0.0100\n",
      "Episode:579 meanR:0.0300 R:0.0 loss:13220.8037 exploreP:0.0100\n",
      "Episode:580 meanR:0.0300 R:0.0 loss:15172.0371 exploreP:0.0100\n",
      "Episode:581 meanR:0.0300 R:0.0 loss:12911.3584 exploreP:0.0100\n",
      "Episode:582 meanR:0.0300 R:0.0 loss:14590.6445 exploreP:0.0100\n",
      "Episode:583 meanR:0.0200 R:0.0 loss:13556.0654 exploreP:0.0100\n",
      "Episode:584 meanR:0.0200 R:0.0 loss:11681.5986 exploreP:0.0100\n",
      "Episode:585 meanR:0.0200 R:0.0 loss:14224.4404 exploreP:0.0100\n",
      "Episode:586 meanR:0.0000 R:0.0 loss:16213.5615 exploreP:0.0100\n",
      "Episode:587 meanR:0.0000 R:0.0 loss:14529.9766 exploreP:0.0100\n",
      "Episode:588 meanR:0.0000 R:0.0 loss:15652.7832 exploreP:0.0100\n",
      "Episode:589 meanR:0.0200 R:0.0 loss:17509.2031 exploreP:0.0100\n",
      "Episode:590 meanR:0.0200 R:0.0 loss:22093.1094 exploreP:0.0100\n",
      "Episode:591 meanR:0.0200 R:0.0 loss:16543.8457 exploreP:0.0100\n",
      "Episode:592 meanR:0.0200 R:0.0 loss:19726.0039 exploreP:0.0100\n",
      "Episode:593 meanR:0.0200 R:0.0 loss:12865.4043 exploreP:0.0100\n",
      "Episode:594 meanR:0.0400 R:1.0 loss:10241.6211 exploreP:0.0100\n",
      "Episode:595 meanR:0.0500 R:0.0 loss:9353.9512 exploreP:0.0100\n",
      "Episode:596 meanR:0.0400 R:0.0 loss:11522.7070 exploreP:0.0100\n",
      "Episode:597 meanR:0.0400 R:0.0 loss:8685.8584 exploreP:0.0100\n",
      "Episode:598 meanR:0.0500 R:0.0 loss:9435.6846 exploreP:0.0100\n",
      "Episode:599 meanR:0.0500 R:0.0 loss:8499.5547 exploreP:0.0100\n",
      "Episode:600 meanR:0.0600 R:1.0 loss:10443.1904 exploreP:0.0100\n",
      "Episode:601 meanR:0.0600 R:0.0 loss:10860.3867 exploreP:0.0100\n",
      "Episode:602 meanR:0.0600 R:0.0 loss:12294.5732 exploreP:0.0100\n",
      "Episode:603 meanR:0.0500 R:0.0 loss:9947.8398 exploreP:0.0100\n",
      "Episode:604 meanR:0.0500 R:0.0 loss:9730.9932 exploreP:0.0100\n",
      "Episode:605 meanR:0.0500 R:0.0 loss:11129.6318 exploreP:0.0100\n",
      "Episode:606 meanR:0.0500 R:0.0 loss:10227.7783 exploreP:0.0100\n",
      "Episode:607 meanR:0.0600 R:0.0 loss:20005.8770 exploreP:0.0100\n",
      "Episode:608 meanR:0.0600 R:0.0 loss:13898.1484 exploreP:0.0100\n",
      "Episode:609 meanR:0.0600 R:0.0 loss:12058.5283 exploreP:0.0100\n",
      "Episode:610 meanR:0.0600 R:0.0 loss:16000.4131 exploreP:0.0100\n",
      "Episode:611 meanR:0.0700 R:0.0 loss:13138.0352 exploreP:0.0100\n",
      "Episode:612 meanR:0.0700 R:0.0 loss:15847.8848 exploreP:0.0100\n",
      "Episode:613 meanR:0.0600 R:-1.0 loss:15459.2314 exploreP:0.0100\n",
      "Episode:614 meanR:0.0700 R:1.0 loss:15780.6562 exploreP:0.0100\n",
      "Episode:615 meanR:0.0700 R:0.0 loss:21556.7461 exploreP:0.0100\n",
      "Episode:616 meanR:0.0700 R:0.0 loss:17387.1543 exploreP:0.0100\n",
      "Episode:617 meanR:0.0600 R:0.0 loss:16783.4238 exploreP:0.0100\n",
      "Episode:618 meanR:0.0600 R:0.0 loss:14980.5469 exploreP:0.0100\n",
      "Episode:619 meanR:0.0600 R:0.0 loss:12349.9629 exploreP:0.0100\n",
      "Episode:620 meanR:0.0700 R:0.0 loss:19482.9668 exploreP:0.0100\n",
      "Episode:621 meanR:0.0700 R:0.0 loss:18501.6426 exploreP:0.0100\n",
      "Episode:622 meanR:0.0700 R:0.0 loss:21999.1992 exploreP:0.0100\n",
      "Episode:623 meanR:0.0800 R:1.0 loss:19394.7129 exploreP:0.0100\n",
      "Episode:624 meanR:0.0700 R:-1.0 loss:18826.4902 exploreP:0.0100\n",
      "Episode:625 meanR:0.0700 R:0.0 loss:22584.0703 exploreP:0.0100\n",
      "Episode:626 meanR:0.0600 R:0.0 loss:23177.5508 exploreP:0.0100\n",
      "Episode:627 meanR:0.0600 R:0.0 loss:25090.8691 exploreP:0.0100\n",
      "Episode:628 meanR:0.0600 R:0.0 loss:23573.1191 exploreP:0.0100\n",
      "Episode:629 meanR:0.0600 R:0.0 loss:20983.0000 exploreP:0.0100\n",
      "Episode:630 meanR:0.0600 R:0.0 loss:24438.3867 exploreP:0.0100\n",
      "Episode:631 meanR:0.0600 R:0.0 loss:30148.8203 exploreP:0.0100\n",
      "Episode:632 meanR:0.0600 R:0.0 loss:35796.1875 exploreP:0.0100\n",
      "Episode:633 meanR:0.0700 R:1.0 loss:36724.2656 exploreP:0.0100\n",
      "Episode:634 meanR:0.0800 R:1.0 loss:30930.6172 exploreP:0.0100\n",
      "Episode:635 meanR:0.0800 R:0.0 loss:29880.7559 exploreP:0.0100\n",
      "Episode:636 meanR:0.0800 R:0.0 loss:31436.5469 exploreP:0.0100\n",
      "Episode:637 meanR:0.0800 R:0.0 loss:30355.5469 exploreP:0.0100\n",
      "Episode:638 meanR:0.0700 R:-1.0 loss:29153.1406 exploreP:0.0100\n",
      "Episode:639 meanR:0.0700 R:0.0 loss:31494.3906 exploreP:0.0100\n",
      "Episode:640 meanR:0.0600 R:0.0 loss:30600.1074 exploreP:0.0100\n",
      "Episode:641 meanR:0.0700 R:0.0 loss:30169.8965 exploreP:0.0100\n",
      "Episode:642 meanR:0.0800 R:0.0 loss:26080.3730 exploreP:0.0100\n",
      "Episode:643 meanR:0.0700 R:0.0 loss:24347.7598 exploreP:0.0100\n",
      "Episode:644 meanR:0.0800 R:1.0 loss:27066.9785 exploreP:0.0100\n",
      "Episode:645 meanR:0.0800 R:0.0 loss:25161.0723 exploreP:0.0100\n",
      "Episode:646 meanR:0.0700 R:0.0 loss:20953.1855 exploreP:0.0100\n",
      "Episode:647 meanR:0.0700 R:0.0 loss:26033.6895 exploreP:0.0100\n",
      "Episode:648 meanR:0.0700 R:0.0 loss:21830.8633 exploreP:0.0100\n",
      "Episode:649 meanR:0.0700 R:0.0 loss:19296.6953 exploreP:0.0100\n",
      "Episode:650 meanR:0.0800 R:1.0 loss:22665.2539 exploreP:0.0100\n",
      "Episode:651 meanR:0.0800 R:0.0 loss:20732.0684 exploreP:0.0100\n",
      "Episode:652 meanR:0.0800 R:0.0 loss:24115.1543 exploreP:0.0100\n",
      "Episode:653 meanR:0.0900 R:1.0 loss:26772.3105 exploreP:0.0100\n",
      "Episode:654 meanR:0.1000 R:1.0 loss:25586.1758 exploreP:0.0100\n",
      "Episode:655 meanR:0.0900 R:-1.0 loss:27906.2324 exploreP:0.0100\n",
      "Episode:656 meanR:0.0900 R:0.0 loss:21910.2168 exploreP:0.0100\n",
      "Episode:657 meanR:0.0800 R:0.0 loss:20765.2227 exploreP:0.0100\n",
      "Episode:658 meanR:0.0800 R:0.0 loss:22368.9922 exploreP:0.0100\n",
      "Episode:659 meanR:0.0700 R:-1.0 loss:26301.5176 exploreP:0.0100\n",
      "Episode:660 meanR:0.0800 R:0.0 loss:16290.3271 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:661 meanR:0.0800 R:0.0 loss:22494.5781 exploreP:0.0100\n",
      "Episode:662 meanR:0.0700 R:-1.0 loss:18737.3848 exploreP:0.0100\n",
      "Episode:663 meanR:0.0700 R:0.0 loss:19402.5312 exploreP:0.0100\n",
      "Episode:664 meanR:0.0500 R:-2.0 loss:18439.3262 exploreP:0.0100\n",
      "Episode:665 meanR:0.0400 R:-1.0 loss:19495.8027 exploreP:0.0100\n",
      "Episode:666 meanR:0.0500 R:1.0 loss:16503.1543 exploreP:0.0100\n",
      "Episode:667 meanR:0.0400 R:0.0 loss:14074.9932 exploreP:0.0100\n",
      "Episode:668 meanR:0.0500 R:1.0 loss:15231.8770 exploreP:0.0100\n",
      "Episode:669 meanR:0.0500 R:0.0 loss:16980.1504 exploreP:0.0100\n",
      "Episode:670 meanR:0.0500 R:0.0 loss:15970.0654 exploreP:0.0100\n",
      "Episode:671 meanR:0.0500 R:0.0 loss:18085.4824 exploreP:0.0100\n",
      "Episode:672 meanR:0.0500 R:0.0 loss:16229.6396 exploreP:0.0100\n",
      "Episode:673 meanR:0.0500 R:0.0 loss:12454.2783 exploreP:0.0100\n",
      "Episode:674 meanR:0.0600 R:1.0 loss:13984.7588 exploreP:0.0100\n",
      "Episode:675 meanR:0.0400 R:-1.0 loss:15237.3887 exploreP:0.0100\n",
      "Episode:676 meanR:0.0500 R:1.0 loss:13259.5625 exploreP:0.0100\n",
      "Episode:677 meanR:0.0400 R:0.0 loss:14042.6816 exploreP:0.0100\n",
      "Episode:678 meanR:0.0400 R:0.0 loss:16396.3730 exploreP:0.0100\n",
      "Episode:679 meanR:0.0300 R:-1.0 loss:18991.7852 exploreP:0.0100\n",
      "Episode:680 meanR:0.0300 R:0.0 loss:14143.5029 exploreP:0.0100\n",
      "Episode:681 meanR:0.0400 R:1.0 loss:13699.3350 exploreP:0.0100\n",
      "Episode:682 meanR:0.0500 R:1.0 loss:16961.5898 exploreP:0.0100\n",
      "Episode:683 meanR:0.0500 R:0.0 loss:15089.3232 exploreP:0.0100\n",
      "Episode:684 meanR:0.0600 R:1.0 loss:14097.7334 exploreP:0.0100\n",
      "Episode:685 meanR:0.0500 R:-1.0 loss:11887.2363 exploreP:0.0100\n",
      "Episode:686 meanR:0.0400 R:-1.0 loss:13051.8555 exploreP:0.0100\n",
      "Episode:687 meanR:0.0400 R:0.0 loss:10958.2266 exploreP:0.0100\n",
      "Episode:688 meanR:0.0400 R:0.0 loss:14899.7383 exploreP:0.0100\n",
      "Episode:689 meanR:0.0400 R:0.0 loss:13416.7256 exploreP:0.0100\n",
      "Episode:690 meanR:0.0500 R:1.0 loss:14735.7148 exploreP:0.0100\n",
      "Episode:691 meanR:0.0500 R:0.0 loss:12249.7100 exploreP:0.0100\n",
      "Episode:692 meanR:0.0600 R:1.0 loss:10325.9336 exploreP:0.0100\n",
      "Episode:693 meanR:0.0600 R:0.0 loss:11303.7070 exploreP:0.0100\n",
      "Episode:694 meanR:0.0500 R:0.0 loss:17245.9043 exploreP:0.0100\n",
      "Episode:695 meanR:0.0500 R:0.0 loss:11518.2334 exploreP:0.0100\n",
      "Episode:696 meanR:0.0500 R:0.0 loss:12850.8193 exploreP:0.0100\n",
      "Episode:697 meanR:0.0500 R:0.0 loss:14848.7236 exploreP:0.0100\n",
      "Episode:698 meanR:0.0500 R:0.0 loss:13264.8877 exploreP:0.0100\n",
      "Episode:699 meanR:0.0500 R:0.0 loss:13001.3770 exploreP:0.0100\n",
      "Episode:700 meanR:0.0400 R:0.0 loss:15088.3965 exploreP:0.0100\n",
      "Episode:701 meanR:0.0400 R:0.0 loss:11582.5029 exploreP:0.0100\n",
      "Episode:702 meanR:0.0400 R:0.0 loss:10450.4062 exploreP:0.0100\n",
      "Episode:703 meanR:0.0400 R:0.0 loss:12460.8584 exploreP:0.0100\n",
      "Episode:704 meanR:0.0400 R:0.0 loss:17753.1426 exploreP:0.0100\n",
      "Episode:705 meanR:0.0500 R:1.0 loss:15978.8184 exploreP:0.0100\n",
      "Episode:706 meanR:0.0500 R:0.0 loss:13526.7188 exploreP:0.0100\n",
      "Episode:707 meanR:0.0600 R:1.0 loss:23575.0352 exploreP:0.0100\n",
      "Episode:708 meanR:0.0600 R:0.0 loss:17797.9824 exploreP:0.0100\n",
      "Episode:709 meanR:0.0600 R:0.0 loss:23537.1406 exploreP:0.0100\n",
      "Episode:710 meanR:0.0600 R:0.0 loss:26005.4141 exploreP:0.0100\n",
      "Episode:711 meanR:0.0600 R:0.0 loss:19978.7188 exploreP:0.0100\n",
      "Episode:712 meanR:0.0600 R:0.0 loss:20538.1797 exploreP:0.0100\n",
      "Episode:713 meanR:0.0700 R:0.0 loss:22200.4902 exploreP:0.0100\n",
      "Episode:714 meanR:0.0600 R:0.0 loss:28231.3672 exploreP:0.0100\n",
      "Episode:715 meanR:0.0600 R:0.0 loss:23943.8359 exploreP:0.0100\n",
      "Episode:716 meanR:0.0600 R:0.0 loss:19303.3418 exploreP:0.0100\n",
      "Episode:717 meanR:0.0500 R:-1.0 loss:23446.0234 exploreP:0.0100\n",
      "Episode:718 meanR:0.0500 R:0.0 loss:25621.4883 exploreP:0.0100\n",
      "Episode:719 meanR:0.0500 R:0.0 loss:22207.7754 exploreP:0.0100\n",
      "Episode:720 meanR:0.0500 R:0.0 loss:27711.6309 exploreP:0.0100\n",
      "Episode:721 meanR:0.0500 R:0.0 loss:24569.7324 exploreP:0.0100\n",
      "Episode:722 meanR:0.0600 R:1.0 loss:23736.8965 exploreP:0.0100\n",
      "Episode:723 meanR:0.0500 R:0.0 loss:27956.0508 exploreP:0.0100\n",
      "Episode:724 meanR:0.0600 R:0.0 loss:13319.7852 exploreP:0.0100\n",
      "Episode:725 meanR:0.0400 R:-2.0 loss:14805.8418 exploreP:0.0100\n",
      "Episode:726 meanR:0.0400 R:0.0 loss:18750.6973 exploreP:0.0100\n",
      "Episode:727 meanR:0.0300 R:-1.0 loss:23062.8457 exploreP:0.0100\n",
      "Episode:728 meanR:0.0300 R:0.0 loss:22392.0312 exploreP:0.0100\n",
      "Episode:729 meanR:0.0200 R:-1.0 loss:20397.8711 exploreP:0.0100\n",
      "Episode:730 meanR:0.0200 R:0.0 loss:18921.8125 exploreP:0.0100\n",
      "Episode:731 meanR:0.0200 R:0.0 loss:19735.5098 exploreP:0.0100\n",
      "Episode:732 meanR:0.0100 R:-1.0 loss:22036.1895 exploreP:0.0100\n",
      "Episode:733 meanR:0.0000 R:0.0 loss:10637.9443 exploreP:0.0100\n",
      "Episode:734 meanR:-0.0100 R:0.0 loss:17491.8809 exploreP:0.0100\n",
      "Episode:735 meanR:-0.0100 R:0.0 loss:20355.8633 exploreP:0.0100\n",
      "Episode:736 meanR:-0.0100 R:0.0 loss:22621.9531 exploreP:0.0100\n",
      "Episode:737 meanR:-0.0300 R:-2.0 loss:17910.7500 exploreP:0.0100\n",
      "Episode:738 meanR:-0.0200 R:0.0 loss:14735.2188 exploreP:0.0100\n",
      "Episode:739 meanR:-0.0200 R:0.0 loss:20054.5859 exploreP:0.0100\n",
      "Episode:740 meanR:-0.0200 R:0.0 loss:16686.3594 exploreP:0.0100\n",
      "Episode:741 meanR:-0.0200 R:0.0 loss:16192.2021 exploreP:0.0100\n",
      "Episode:742 meanR:-0.0300 R:-1.0 loss:17546.0488 exploreP:0.0100\n",
      "Episode:743 meanR:-0.0300 R:0.0 loss:15128.8369 exploreP:0.0100\n",
      "Episode:744 meanR:-0.0400 R:0.0 loss:15704.2266 exploreP:0.0100\n",
      "Episode:745 meanR:-0.0500 R:-1.0 loss:17446.1133 exploreP:0.0100\n",
      "Episode:746 meanR:-0.0500 R:0.0 loss:13066.7568 exploreP:0.0100\n",
      "Episode:747 meanR:-0.0500 R:0.0 loss:16160.1602 exploreP:0.0100\n",
      "Episode:748 meanR:-0.0400 R:1.0 loss:24081.4375 exploreP:0.0100\n",
      "Episode:749 meanR:-0.0400 R:0.0 loss:22430.9629 exploreP:0.0100\n",
      "Episode:750 meanR:-0.0500 R:0.0 loss:17965.3535 exploreP:0.0100\n",
      "Episode:751 meanR:-0.0500 R:0.0 loss:20985.3711 exploreP:0.0100\n",
      "Episode:752 meanR:-0.0500 R:0.0 loss:20914.6230 exploreP:0.0100\n",
      "Episode:753 meanR:-0.0500 R:1.0 loss:18699.7793 exploreP:0.0100\n",
      "Episode:754 meanR:-0.0600 R:0.0 loss:16309.8564 exploreP:0.0100\n",
      "Episode:755 meanR:-0.0500 R:0.0 loss:21666.9258 exploreP:0.0100\n",
      "Episode:756 meanR:-0.0500 R:0.0 loss:12953.9854 exploreP:0.0100\n",
      "Episode:757 meanR:-0.0400 R:1.0 loss:17477.1973 exploreP:0.0100\n",
      "Episode:758 meanR:-0.0400 R:0.0 loss:19306.0625 exploreP:0.0100\n",
      "Episode:759 meanR:-0.0400 R:-1.0 loss:29915.9863 exploreP:0.0100\n",
      "Episode:760 meanR:-0.0400 R:0.0 loss:17437.1543 exploreP:0.0100\n",
      "Episode:761 meanR:-0.0400 R:0.0 loss:16549.4824 exploreP:0.0100\n",
      "Episode:762 meanR:-0.0300 R:0.0 loss:17439.4570 exploreP:0.0100\n",
      "Episode:763 meanR:-0.0300 R:0.0 loss:17734.9492 exploreP:0.0100\n",
      "Episode:764 meanR:-0.0100 R:0.0 loss:21363.7676 exploreP:0.0100\n",
      "Episode:765 meanR:-0.0100 R:-1.0 loss:13145.4316 exploreP:0.0100\n",
      "Episode:766 meanR:-0.0100 R:1.0 loss:19979.5137 exploreP:0.0100\n",
      "Episode:767 meanR:-0.0100 R:0.0 loss:26775.0898 exploreP:0.0100\n",
      "Episode:768 meanR:-0.0200 R:0.0 loss:27601.3223 exploreP:0.0100\n",
      "Episode:769 meanR:-0.0200 R:0.0 loss:21156.8203 exploreP:0.0100\n",
      "Episode:770 meanR:-0.0200 R:0.0 loss:23933.7207 exploreP:0.0100\n",
      "Episode:771 meanR:-0.0200 R:0.0 loss:20022.9805 exploreP:0.0100\n",
      "Episode:772 meanR:-0.0200 R:0.0 loss:23017.5566 exploreP:0.0100\n",
      "Episode:773 meanR:-0.0200 R:0.0 loss:15565.1836 exploreP:0.0100\n",
      "Episode:774 meanR:-0.0300 R:0.0 loss:17502.8457 exploreP:0.0100\n",
      "Episode:775 meanR:-0.0100 R:1.0 loss:19844.5059 exploreP:0.0100\n",
      "Episode:776 meanR:-0.0200 R:0.0 loss:14948.4902 exploreP:0.0100\n",
      "Episode:777 meanR:-0.0200 R:0.0 loss:17558.2598 exploreP:0.0100\n",
      "Episode:778 meanR:-0.0300 R:-1.0 loss:15257.9863 exploreP:0.0100\n",
      "Episode:779 meanR:-0.0200 R:0.0 loss:25694.8457 exploreP:0.0100\n",
      "Episode:780 meanR:-0.0100 R:1.0 loss:12113.0459 exploreP:0.0100\n",
      "Episode:781 meanR:0.0000 R:2.0 loss:17012.0293 exploreP:0.0100\n",
      "Episode:782 meanR:-0.0100 R:0.0 loss:14517.8799 exploreP:0.0100\n",
      "Episode:783 meanR:-0.0100 R:0.0 loss:13219.5938 exploreP:0.0100\n",
      "Episode:784 meanR:-0.0200 R:0.0 loss:20992.9414 exploreP:0.0100\n",
      "Episode:785 meanR:-0.0100 R:0.0 loss:12389.5596 exploreP:0.0100\n",
      "Episode:786 meanR:0.0000 R:0.0 loss:13797.8965 exploreP:0.0100\n",
      "Episode:787 meanR:0.0000 R:0.0 loss:14265.0703 exploreP:0.0100\n",
      "Episode:788 meanR:0.0000 R:0.0 loss:16548.7207 exploreP:0.0100\n",
      "Episode:789 meanR:0.0000 R:0.0 loss:16786.0820 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:790 meanR:-0.0200 R:-1.0 loss:11950.0186 exploreP:0.0100\n",
      "Episode:791 meanR:-0.0200 R:0.0 loss:16156.0967 exploreP:0.0100\n",
      "Episode:792 meanR:0.0000 R:3.0 loss:11112.1416 exploreP:0.0100\n",
      "Episode:793 meanR:0.0000 R:0.0 loss:8685.8721 exploreP:0.0100\n",
      "Episode:794 meanR:0.0100 R:1.0 loss:10388.0586 exploreP:0.0100\n",
      "Episode:795 meanR:0.0100 R:0.0 loss:10461.4189 exploreP:0.0100\n",
      "Episode:796 meanR:0.0000 R:-1.0 loss:13150.6621 exploreP:0.0100\n",
      "Episode:797 meanR:0.0000 R:0.0 loss:9743.1963 exploreP:0.0100\n",
      "Episode:798 meanR:-0.0100 R:-1.0 loss:10270.8770 exploreP:0.0100\n",
      "Episode:799 meanR:-0.0200 R:-1.0 loss:13572.5146 exploreP:0.0100\n",
      "Episode:800 meanR:-0.0200 R:0.0 loss:9087.3447 exploreP:0.0100\n",
      "Episode:801 meanR:-0.0300 R:-1.0 loss:9718.5098 exploreP:0.0100\n",
      "Episode:802 meanR:-0.0300 R:0.0 loss:6693.3052 exploreP:0.0100\n",
      "Episode:803 meanR:-0.0300 R:0.0 loss:13304.8047 exploreP:0.0100\n",
      "Episode:804 meanR:-0.0400 R:-1.0 loss:8574.0010 exploreP:0.0100\n",
      "Episode:805 meanR:-0.0500 R:0.0 loss:7583.1177 exploreP:0.0100\n",
      "Episode:806 meanR:-0.0400 R:1.0 loss:8272.0020 exploreP:0.0100\n",
      "Episode:807 meanR:-0.0500 R:0.0 loss:7408.7656 exploreP:0.0100\n",
      "Episode:808 meanR:-0.0500 R:0.0 loss:8540.7803 exploreP:0.0100\n",
      "Episode:809 meanR:-0.0500 R:0.0 loss:9062.5078 exploreP:0.0100\n",
      "Episode:810 meanR:-0.0500 R:0.0 loss:7187.1509 exploreP:0.0100\n",
      "Episode:811 meanR:-0.0500 R:0.0 loss:6584.3926 exploreP:0.0100\n",
      "Episode:812 meanR:-0.0500 R:0.0 loss:9193.0137 exploreP:0.0100\n",
      "Episode:813 meanR:-0.0500 R:0.0 loss:8120.2734 exploreP:0.0100\n",
      "Episode:814 meanR:-0.0800 R:-3.0 loss:9738.8750 exploreP:0.0100\n",
      "Episode:815 meanR:-0.0800 R:0.0 loss:6424.4321 exploreP:0.0100\n",
      "Episode:816 meanR:-0.0700 R:1.0 loss:6257.9482 exploreP:0.0100\n",
      "Episode:817 meanR:-0.0600 R:0.0 loss:7014.2100 exploreP:0.0100\n",
      "Episode:818 meanR:-0.0600 R:0.0 loss:6801.7368 exploreP:0.0100\n",
      "Episode:819 meanR:-0.0600 R:0.0 loss:7769.4883 exploreP:0.0100\n",
      "Episode:820 meanR:-0.0700 R:-1.0 loss:7245.0835 exploreP:0.0100\n",
      "Episode:821 meanR:-0.0700 R:0.0 loss:6681.8652 exploreP:0.0100\n",
      "Episode:822 meanR:-0.0800 R:0.0 loss:6413.9414 exploreP:0.0100\n",
      "Episode:823 meanR:-0.0800 R:0.0 loss:6475.6719 exploreP:0.0100\n",
      "Episode:824 meanR:-0.0800 R:0.0 loss:5877.1533 exploreP:0.0100\n",
      "Episode:825 meanR:-0.0700 R:-1.0 loss:7533.4766 exploreP:0.0100\n",
      "Episode:826 meanR:-0.0700 R:0.0 loss:6559.8740 exploreP:0.0100\n",
      "Episode:827 meanR:-0.0600 R:0.0 loss:3734.7393 exploreP:0.0100\n",
      "Episode:828 meanR:-0.0700 R:-1.0 loss:7582.2334 exploreP:0.0100\n",
      "Episode:829 meanR:-0.0600 R:0.0 loss:4847.7354 exploreP:0.0100\n",
      "Episode:830 meanR:-0.0600 R:0.0 loss:4354.0952 exploreP:0.0100\n",
      "Episode:831 meanR:-0.0600 R:0.0 loss:6582.3940 exploreP:0.0100\n",
      "Episode:832 meanR:-0.0500 R:0.0 loss:5415.5293 exploreP:0.0100\n",
      "Episode:833 meanR:-0.0500 R:0.0 loss:5593.2974 exploreP:0.0100\n",
      "Episode:834 meanR:-0.0500 R:0.0 loss:4662.0723 exploreP:0.0100\n",
      "Episode:835 meanR:-0.0500 R:0.0 loss:4389.8062 exploreP:0.0100\n",
      "Episode:836 meanR:-0.0500 R:0.0 loss:4520.1104 exploreP:0.0100\n",
      "Episode:837 meanR:-0.0300 R:0.0 loss:6444.2544 exploreP:0.0100\n",
      "Episode:838 meanR:-0.0400 R:-1.0 loss:5653.2617 exploreP:0.0100\n",
      "Episode:839 meanR:-0.0600 R:-2.0 loss:4139.0498 exploreP:0.0100\n",
      "Episode:840 meanR:-0.0600 R:0.0 loss:5328.3511 exploreP:0.0100\n",
      "Episode:841 meanR:-0.0600 R:0.0 loss:4243.0552 exploreP:0.0100\n",
      "Episode:842 meanR:-0.0500 R:0.0 loss:3909.2012 exploreP:0.0100\n",
      "Episode:843 meanR:-0.0600 R:-1.0 loss:5514.5669 exploreP:0.0100\n",
      "Episode:844 meanR:-0.0700 R:-1.0 loss:3772.9021 exploreP:0.0100\n",
      "Episode:845 meanR:-0.0600 R:0.0 loss:6056.4082 exploreP:0.0100\n",
      "Episode:846 meanR:-0.0600 R:0.0 loss:5412.4814 exploreP:0.0100\n",
      "Episode:847 meanR:-0.0700 R:-1.0 loss:3426.6079 exploreP:0.0100\n",
      "Episode:848 meanR:-0.0800 R:0.0 loss:5100.4673 exploreP:0.0100\n",
      "Episode:849 meanR:-0.0800 R:0.0 loss:4984.0581 exploreP:0.0100\n",
      "Episode:850 meanR:-0.0600 R:2.0 loss:5193.7334 exploreP:0.0100\n",
      "Episode:851 meanR:-0.0600 R:0.0 loss:3620.0200 exploreP:0.0100\n",
      "Episode:852 meanR:-0.0700 R:-1.0 loss:4649.7446 exploreP:0.0100\n",
      "Episode:853 meanR:-0.0800 R:0.0 loss:4691.2075 exploreP:0.0100\n",
      "Episode:854 meanR:-0.0800 R:0.0 loss:3312.4329 exploreP:0.0100\n",
      "Episode:855 meanR:-0.0800 R:0.0 loss:5258.6465 exploreP:0.0100\n",
      "Episode:856 meanR:-0.0800 R:0.0 loss:2800.9070 exploreP:0.0100\n",
      "Episode:857 meanR:-0.0900 R:0.0 loss:3794.9133 exploreP:0.0100\n",
      "Episode:858 meanR:-0.0900 R:0.0 loss:3015.5459 exploreP:0.0100\n",
      "Episode:859 meanR:-0.0700 R:1.0 loss:4266.0688 exploreP:0.0100\n",
      "Episode:860 meanR:-0.0800 R:-1.0 loss:3659.8625 exploreP:0.0100\n",
      "Episode:861 meanR:-0.0800 R:0.0 loss:3077.7966 exploreP:0.0100\n",
      "Episode:862 meanR:-0.0800 R:0.0 loss:4245.7173 exploreP:0.0100\n",
      "Episode:863 meanR:-0.0900 R:-1.0 loss:3473.6108 exploreP:0.0100\n",
      "Episode:864 meanR:-0.0800 R:1.0 loss:3829.8274 exploreP:0.0100\n",
      "Episode:865 meanR:-0.0700 R:0.0 loss:3583.8696 exploreP:0.0100\n",
      "Episode:866 meanR:-0.0800 R:0.0 loss:3197.5532 exploreP:0.0100\n",
      "Episode:867 meanR:-0.0800 R:0.0 loss:3431.6611 exploreP:0.0100\n",
      "Episode:868 meanR:-0.0700 R:1.0 loss:3065.3198 exploreP:0.0100\n",
      "Episode:869 meanR:-0.0600 R:1.0 loss:2354.2061 exploreP:0.0100\n",
      "Episode:870 meanR:-0.0600 R:0.0 loss:3515.5112 exploreP:0.0100\n",
      "Episode:871 meanR:-0.0600 R:0.0 loss:3477.0310 exploreP:0.0100\n",
      "Episode:872 meanR:-0.0700 R:-1.0 loss:3999.3584 exploreP:0.0100\n",
      "Episode:873 meanR:-0.0700 R:0.0 loss:3605.1057 exploreP:0.0100\n",
      "Episode:874 meanR:-0.0700 R:0.0 loss:3393.0483 exploreP:0.0100\n",
      "Episode:875 meanR:-0.0800 R:0.0 loss:3478.0488 exploreP:0.0100\n",
      "Episode:876 meanR:-0.0800 R:0.0 loss:3277.6333 exploreP:0.0100\n",
      "Episode:877 meanR:-0.0800 R:0.0 loss:3070.2397 exploreP:0.0100\n",
      "Episode:878 meanR:-0.0700 R:0.0 loss:2254.5071 exploreP:0.0100\n",
      "Episode:879 meanR:-0.0700 R:0.0 loss:3263.7778 exploreP:0.0100\n",
      "Episode:880 meanR:-0.0700 R:1.0 loss:4256.4785 exploreP:0.0100\n",
      "Episode:881 meanR:-0.0900 R:0.0 loss:2845.6721 exploreP:0.0100\n",
      "Episode:882 meanR:-0.0900 R:0.0 loss:2905.8955 exploreP:0.0100\n",
      "Episode:883 meanR:-0.0900 R:0.0 loss:2513.3076 exploreP:0.0100\n",
      "Episode:884 meanR:-0.1000 R:-1.0 loss:3215.9629 exploreP:0.0100\n",
      "Episode:885 meanR:-0.1000 R:0.0 loss:2495.1375 exploreP:0.0100\n",
      "Episode:886 meanR:-0.1000 R:0.0 loss:2857.5273 exploreP:0.0100\n",
      "Episode:887 meanR:-0.0900 R:1.0 loss:2769.7224 exploreP:0.0100\n",
      "Episode:888 meanR:-0.0900 R:0.0 loss:2902.0525 exploreP:0.0100\n",
      "Episode:889 meanR:-0.0800 R:1.0 loss:2934.5901 exploreP:0.0100\n",
      "Episode:890 meanR:-0.0700 R:0.0 loss:2327.0837 exploreP:0.0100\n",
      "Episode:891 meanR:-0.0700 R:0.0 loss:3539.0945 exploreP:0.0100\n",
      "Episode:892 meanR:-0.1000 R:0.0 loss:2066.4497 exploreP:0.0100\n",
      "Episode:893 meanR:-0.1000 R:0.0 loss:2481.7820 exploreP:0.0100\n",
      "Episode:894 meanR:-0.1100 R:0.0 loss:2451.3191 exploreP:0.0100\n",
      "Episode:895 meanR:-0.1100 R:0.0 loss:3521.3174 exploreP:0.0100\n",
      "Episode:896 meanR:-0.1000 R:0.0 loss:3298.8108 exploreP:0.0100\n",
      "Episode:897 meanR:-0.0900 R:1.0 loss:3379.1238 exploreP:0.0100\n",
      "Episode:898 meanR:-0.0700 R:1.0 loss:2468.5725 exploreP:0.0100\n",
      "Episode:899 meanR:-0.0600 R:0.0 loss:3432.6238 exploreP:0.0100\n",
      "Episode:900 meanR:-0.0600 R:0.0 loss:2202.1238 exploreP:0.0100\n",
      "Episode:901 meanR:-0.0500 R:0.0 loss:2313.5688 exploreP:0.0100\n",
      "Episode:902 meanR:-0.0400 R:1.0 loss:2985.0266 exploreP:0.0100\n",
      "Episode:903 meanR:-0.0400 R:0.0 loss:2139.2195 exploreP:0.0100\n",
      "Episode:904 meanR:-0.0300 R:0.0 loss:1960.2238 exploreP:0.0100\n",
      "Episode:905 meanR:-0.0300 R:0.0 loss:2654.2170 exploreP:0.0100\n",
      "Episode:906 meanR:-0.0500 R:-1.0 loss:2252.7756 exploreP:0.0100\n",
      "Episode:907 meanR:-0.0500 R:0.0 loss:2032.5233 exploreP:0.0100\n",
      "Episode:908 meanR:-0.0500 R:0.0 loss:1880.5354 exploreP:0.0100\n",
      "Episode:909 meanR:-0.0500 R:0.0 loss:2884.6931 exploreP:0.0100\n",
      "Episode:910 meanR:-0.0500 R:0.0 loss:3020.6138 exploreP:0.0100\n",
      "Episode:911 meanR:-0.0500 R:0.0 loss:2749.7795 exploreP:0.0100\n",
      "Episode:912 meanR:-0.0500 R:0.0 loss:2290.2939 exploreP:0.0100\n",
      "Episode:913 meanR:-0.0500 R:0.0 loss:2789.9963 exploreP:0.0100\n",
      "Episode:914 meanR:-0.0200 R:0.0 loss:3401.1731 exploreP:0.0100\n",
      "Episode:915 meanR:-0.0200 R:0.0 loss:2664.7834 exploreP:0.0100\n",
      "Episode:916 meanR:-0.0400 R:-1.0 loss:2265.9348 exploreP:0.0100\n",
      "Episode:917 meanR:-0.0400 R:0.0 loss:4277.8579 exploreP:0.0100\n",
      "Episode:918 meanR:-0.0400 R:0.0 loss:2876.8132 exploreP:0.0100\n",
      "Episode:919 meanR:-0.0400 R:0.0 loss:3324.1099 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:920 meanR:-0.0200 R:1.0 loss:3332.6318 exploreP:0.0100\n",
      "Episode:921 meanR:-0.0200 R:0.0 loss:3100.8679 exploreP:0.0100\n",
      "Episode:922 meanR:-0.0200 R:0.0 loss:3536.4792 exploreP:0.0100\n",
      "Episode:923 meanR:-0.0200 R:0.0 loss:2634.4409 exploreP:0.0100\n",
      "Episode:924 meanR:-0.0200 R:0.0 loss:2572.5305 exploreP:0.0100\n",
      "Episode:925 meanR:-0.0100 R:0.0 loss:2938.1406 exploreP:0.0100\n",
      "Episode:926 meanR:-0.0100 R:0.0 loss:3245.4404 exploreP:0.0100\n",
      "Episode:927 meanR:-0.0100 R:0.0 loss:2885.7395 exploreP:0.0100\n",
      "Episode:928 meanR:0.0000 R:0.0 loss:2939.0315 exploreP:0.0100\n",
      "Episode:929 meanR:0.0000 R:0.0 loss:3455.2893 exploreP:0.0100\n",
      "Episode:930 meanR:0.0000 R:0.0 loss:2808.3657 exploreP:0.0100\n",
      "Episode:931 meanR:0.0100 R:1.0 loss:2404.7605 exploreP:0.0100\n",
      "Episode:932 meanR:0.0100 R:0.0 loss:4000.5525 exploreP:0.0100\n",
      "Episode:933 meanR:0.0100 R:0.0 loss:2893.8530 exploreP:0.0100\n",
      "Episode:934 meanR:0.0100 R:0.0 loss:2449.3403 exploreP:0.0100\n",
      "Episode:935 meanR:0.0100 R:0.0 loss:3315.0874 exploreP:0.0100\n",
      "Episode:936 meanR:0.0100 R:0.0 loss:1909.4446 exploreP:0.0100\n",
      "Episode:937 meanR:0.0100 R:0.0 loss:2640.6711 exploreP:0.0100\n",
      "Episode:938 meanR:0.0200 R:0.0 loss:2603.2087 exploreP:0.0100\n",
      "Episode:939 meanR:0.0400 R:0.0 loss:3042.3130 exploreP:0.0100\n",
      "Episode:940 meanR:0.0400 R:0.0 loss:2557.8218 exploreP:0.0100\n",
      "Episode:941 meanR:0.0400 R:0.0 loss:2753.7112 exploreP:0.0100\n",
      "Episode:942 meanR:0.0400 R:0.0 loss:1588.5327 exploreP:0.0100\n",
      "Episode:943 meanR:0.0500 R:0.0 loss:2342.6697 exploreP:0.0100\n",
      "Episode:944 meanR:0.0600 R:0.0 loss:2203.6057 exploreP:0.0100\n",
      "Episode:945 meanR:0.0500 R:-1.0 loss:3111.4639 exploreP:0.0100\n",
      "Episode:946 meanR:0.0500 R:0.0 loss:2844.3103 exploreP:0.0100\n",
      "Episode:947 meanR:0.0600 R:0.0 loss:1929.2603 exploreP:0.0100\n",
      "Episode:948 meanR:0.0600 R:0.0 loss:1983.4425 exploreP:0.0100\n",
      "Episode:949 meanR:0.0600 R:0.0 loss:2877.3203 exploreP:0.0100\n",
      "Episode:950 meanR:0.0400 R:0.0 loss:2510.8928 exploreP:0.0100\n",
      "Episode:951 meanR:0.0400 R:0.0 loss:2097.5913 exploreP:0.0100\n",
      "Episode:952 meanR:0.0500 R:0.0 loss:3625.2080 exploreP:0.0100\n",
      "Episode:953 meanR:0.0500 R:0.0 loss:2516.6028 exploreP:0.0100\n",
      "Episode:954 meanR:0.0500 R:0.0 loss:2049.6782 exploreP:0.0100\n",
      "Episode:955 meanR:0.0500 R:0.0 loss:1848.5150 exploreP:0.0100\n",
      "Episode:956 meanR:0.0500 R:0.0 loss:1968.9550 exploreP:0.0100\n",
      "Episode:957 meanR:0.0500 R:0.0 loss:2065.8408 exploreP:0.0100\n",
      "Episode:958 meanR:0.0500 R:0.0 loss:1858.5850 exploreP:0.0100\n",
      "Episode:959 meanR:0.0300 R:-1.0 loss:2590.1724 exploreP:0.0100\n",
      "Episode:960 meanR:0.0400 R:0.0 loss:2485.5220 exploreP:0.0100\n",
      "Episode:961 meanR:0.0400 R:0.0 loss:2850.4692 exploreP:0.0100\n",
      "Episode:962 meanR:0.0400 R:0.0 loss:2986.2610 exploreP:0.0100\n",
      "Episode:963 meanR:0.0500 R:0.0 loss:2776.1738 exploreP:0.0100\n",
      "Episode:964 meanR:0.0400 R:0.0 loss:2502.1558 exploreP:0.0100\n",
      "Episode:965 meanR:0.0400 R:0.0 loss:1888.4393 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        #state = env_info.vector_observations[0]   # get the current state\n",
    "        state = env_info.visual_observations[0]   # get the current state\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, \n",
    "                                         feed_dict={model.states: state.reshape([1, *state_size])}) # visual\n",
    "                                         #feed_dict={model.states: state.reshape([1, -1])}) # vector\n",
    "                action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            #next_state = env_info.vector_observations[0]   # get the next state\n",
    "            next_state = env_info.visual_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "        # Save the model at the end of each 10 training episodes/epochs\n",
    "        if (ep % 10) == 0:\n",
    "            saver.save(sess, 'checkpoints/model-nav-pixel.ckpt')\n",
    "        \n",
    "    # Save the model at the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav-pixel.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        #state = env_info.vector_observations[0]   # get the current state\n",
    "        state = env_info.visual_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            #state = env_info.vector_observations[0]   # get the next state\n",
    "            state = env_info.visual_observations[0]   # get the current state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
