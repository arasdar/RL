{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.11.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.5241856479297082 -2.4869053002172326\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    training = tf.placeholder(tf.bool, [], name='training')\n",
    "    return states, actions, targetQs, rates, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates, training):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size, training=training)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states, training=training)\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                   labels=rates)) # 0-1\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.nn.sigmoid(targetQs))) # 0-1\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, training=training, \n",
    "                        reuse=True) # Qs\n",
    "    dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                   labels=rates)) # 0-1\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                    labels=tf.nn.sigmoid(targetQs))) # 0-1\n",
    "    return actions_logits, gQs, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates, self.training = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            rates=self.rates, training=self.training) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss,\n",
    "                                           g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(buffer, batch_size):\n",
    "    idx = np.random.choice(np.arange(len(buffer)), size=batch_size, replace=False)\n",
    "    return np.array([buffer[ii] for ii in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rate = -1 # [-1, +1] or [0, 1]\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500 # [0, 1]\n",
    "        total_reward = 0\n",
    "        for idx in range(num_step):\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = memory.buffer\n",
    "def minibatch(batch):\n",
    "    rates = np.array([each[-1] for each in batch])\n",
    "    ##############################\n",
    "    states = np.array([each[0] for each in batch])[rates > 0]\n",
    "    actions = np.array([each[1] for each in batch])[rates > 0]\n",
    "    next_states = np.array([each[2] for each in batch])[rates > 0]\n",
    "    rewards = np.array([each[3] for each in batch])[rates > 0]\n",
    "    dones = np.array([each[4] for each in batch])[rates > 0]\n",
    "    rates = np.array([each[5] for each in batch])[rates > 0]\n",
    "    ##########################################\n",
    "    maxpercentage = 0.6\n",
    "    maxstates = states[rates > (max(rates)*maxpercentage)]\n",
    "    maxactions = actions[rates > (max(rates)*maxpercentage)]\n",
    "    maxnext_states = next_states[rates > (max(rates)*maxpercentage)]\n",
    "    maxrewards = rewards[rates > (max(rates)*maxpercentage)]\n",
    "    maxdones = dones[rates > (max(rates)*maxpercentage)]\n",
    "    maxrates = rates[rates > (max(rates)*maxpercentage)]\n",
    "    ##################################################\n",
    "    minpercentage = 0.4\n",
    "    minstates = states[rates < (max(rates)*minpercentage)]\n",
    "    minactions = actions[rates < (max(rates)*minpercentage)]\n",
    "    minnext_states = next_states[rates < (max(rates)*minpercentage)]\n",
    "    minrewards = rewards[rates < (max(rates)*minpercentage)]\n",
    "    mindones = dones[rates < (max(rates)*minpercentage)]\n",
    "    minrates = rates[rates < (max(rates)*minpercentage)]\n",
    "    #################################################\n",
    "    maxstates = sample(batch_size=batch_size, buffer=maxstates)\n",
    "    minstates = sample(batch_size=batch_size, buffer=minstates)\n",
    "    maxactions = sample(batch_size=batch_size, buffer=maxactions)\n",
    "    minactions = sample(batch_size=batch_size, buffer=minactions)\n",
    "    maxnext_states = sample(batch_size=batch_size, buffer=maxnext_states)\n",
    "    minnext_states = sample(batch_size=batch_size, buffer=minnext_states)\n",
    "    maxrewards = sample(batch_size=batch_size, buffer=maxrewards)\n",
    "    minrewards = sample(batch_size=batch_size, buffer=minrewards)\n",
    "    maxdones = sample(batch_size=batch_size, buffer=maxdones)\n",
    "    mindones = sample(batch_size=batch_size, buffer=mindones)\n",
    "    maxrates = sample(batch_size=batch_size, buffer=maxrates)\n",
    "    minrates = sample(batch_size=batch_size, buffer=minrates)\n",
    "    #################################################################\n",
    "    states = np.concatenate([maxstates, minstates], axis=0)\n",
    "    actions = np.concatenate([maxactions, minactions], axis=0)\n",
    "    next_states = np.concatenate([maxnext_states, minnext_states], axis=0)\n",
    "    rewards = np.concatenate([maxrewards, minrewards], axis=0)\n",
    "    dones = np.concatenate([maxdones, mindones], axis=0)\n",
    "    rates = np.concatenate([maxrates, minrates], axis=0)\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = memory.buffer\n",
    "states, actions, next_states, rewards, dones, rates = minibatch(batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 4),\n",
       " (200,),\n",
       " (200, 4),\n",
       " (200,),\n",
       " (200,),\n",
       " (200,),\n",
       " dtype('float64'),\n",
       " dtype('int64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape, \\\n",
    "states.dtype, actions.dtype, next_states.dtype, rewards.dtype, dones.dtype, rates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:19.0000 R:19.0000 rate:0.0380 gloss:1.4138 dloss:1.4583 exploreP:0.9981\n",
      "Episode:1 meanR:19.0000 R:19.0000 rate:0.0380 gloss:1.4093 dloss:1.4551 exploreP:0.9962\n",
      "Episode:2 meanR:26.3333 R:41.0000 rate:0.0820 gloss:1.3972 dloss:1.4395 exploreP:0.9922\n",
      "Episode:3 meanR:34.2500 R:58.0000 rate:0.1160 gloss:1.3715 dloss:1.4118 exploreP:0.9865\n",
      "Episode:4 meanR:30.2000 R:14.0000 rate:0.0280 gloss:1.3541 dloss:1.3958 exploreP:0.9852\n",
      "Episode:5 meanR:29.1667 R:24.0000 rate:0.0480 gloss:1.3494 dloss:1.3927 exploreP:0.9828\n",
      "Episode:6 meanR:29.4286 R:31.0000 rate:0.0620 gloss:1.3431 dloss:1.3840 exploreP:0.9798\n",
      "Episode:7 meanR:28.3750 R:21.0000 rate:0.0420 gloss:1.3359 dloss:1.3736 exploreP:0.9778\n",
      "Episode:8 meanR:27.3333 R:19.0000 rate:0.0380 gloss:1.3292 dloss:1.3670 exploreP:0.9759\n",
      "Episode:9 meanR:25.9000 R:13.0000 rate:0.0260 gloss:1.3275 dloss:1.3628 exploreP:0.9747\n",
      "Episode:10 meanR:24.9091 R:15.0000 rate:0.0300 gloss:1.3255 dloss:1.3619 exploreP:0.9732\n",
      "Episode:11 meanR:23.7500 R:11.0000 rate:0.0220 gloss:1.3263 dloss:1.3605 exploreP:0.9722\n",
      "Episode:12 meanR:26.5385 R:60.0000 rate:0.1200 gloss:1.3178 dloss:1.3553 exploreP:0.9664\n",
      "Episode:13 meanR:25.7857 R:16.0000 rate:0.0320 gloss:1.3133 dloss:1.3468 exploreP:0.9649\n",
      "Episode:14 meanR:24.6667 R:9.0000 rate:0.0180 gloss:1.3103 dloss:1.3383 exploreP:0.9640\n",
      "Episode:15 meanR:24.0000 R:14.0000 rate:0.0280 gloss:1.3097 dloss:1.3449 exploreP:0.9627\n",
      "Episode:16 meanR:26.5294 R:67.0000 rate:0.1340 gloss:1.3060 dloss:1.3353 exploreP:0.9563\n",
      "Episode:17 meanR:25.8889 R:15.0000 rate:0.0300 gloss:1.3037 dloss:1.3310 exploreP:0.9549\n",
      "Episode:18 meanR:25.4737 R:18.0000 rate:0.0360 gloss:1.3023 dloss:1.3286 exploreP:0.9532\n",
      "Episode:19 meanR:26.0000 R:36.0000 rate:0.0720 gloss:1.2994 dloss:1.3238 exploreP:0.9498\n",
      "Episode:20 meanR:25.5238 R:16.0000 rate:0.0320 gloss:1.2978 dloss:1.3225 exploreP:0.9483\n",
      "Episode:21 meanR:26.3182 R:43.0000 rate:0.0860 gloss:1.2958 dloss:1.3185 exploreP:0.9443\n",
      "Episode:22 meanR:25.6522 R:11.0000 rate:0.0220 gloss:1.2941 dloss:1.3147 exploreP:0.9433\n",
      "Episode:23 meanR:26.3333 R:42.0000 rate:0.0840 gloss:1.2925 dloss:1.3127 exploreP:0.9394\n",
      "Episode:24 meanR:25.8000 R:13.0000 rate:0.0260 gloss:1.2901 dloss:1.3082 exploreP:0.9382\n",
      "Episode:25 meanR:25.1923 R:10.0000 rate:0.0200 gloss:1.2916 dloss:1.3086 exploreP:0.9372\n",
      "Episode:26 meanR:25.9259 R:45.0000 rate:0.0900 gloss:1.2893 dloss:1.3051 exploreP:0.9331\n",
      "Episode:27 meanR:25.4286 R:12.0000 rate:0.0240 gloss:1.2861 dloss:1.3036 exploreP:0.9320\n",
      "Episode:28 meanR:25.1034 R:16.0000 rate:0.0320 gloss:1.2860 dloss:1.3003 exploreP:0.9305\n",
      "Episode:29 meanR:24.8333 R:17.0000 rate:0.0340 gloss:1.2864 dloss:1.2987 exploreP:0.9289\n",
      "Episode:30 meanR:24.9355 R:28.0000 rate:0.0560 gloss:1.2851 dloss:1.2984 exploreP:0.9264\n",
      "Episode:31 meanR:25.1250 R:31.0000 rate:0.0620 gloss:1.2836 dloss:1.2972 exploreP:0.9235\n",
      "Episode:32 meanR:24.9091 R:18.0000 rate:0.0360 gloss:1.2839 dloss:1.2959 exploreP:0.9219\n",
      "Episode:33 meanR:24.5294 R:12.0000 rate:0.0240 gloss:1.2835 dloss:1.2956 exploreP:0.9208\n",
      "Episode:34 meanR:24.4857 R:23.0000 rate:0.0460 gloss:1.2823 dloss:1.2938 exploreP:0.9187\n",
      "Episode:35 meanR:24.3056 R:18.0000 rate:0.0360 gloss:1.2818 dloss:1.2935 exploreP:0.9171\n",
      "Episode:36 meanR:24.7838 R:42.0000 rate:0.0840 gloss:1.2810 dloss:1.2914 exploreP:0.9133\n",
      "Episode:37 meanR:24.4474 R:12.0000 rate:0.0240 gloss:1.2800 dloss:1.2912 exploreP:0.9122\n",
      "Episode:38 meanR:24.1282 R:12.0000 rate:0.0240 gloss:1.2808 dloss:1.2920 exploreP:0.9111\n",
      "Episode:39 meanR:24.0000 R:19.0000 rate:0.0380 gloss:1.2800 dloss:1.2894 exploreP:0.9094\n",
      "Episode:40 meanR:25.1951 R:73.0000 rate:0.1460 gloss:1.2794 dloss:1.2882 exploreP:0.9028\n",
      "Episode:41 meanR:25.2619 R:28.0000 rate:0.0560 gloss:1.2784 dloss:1.2861 exploreP:0.9003\n",
      "Episode:42 meanR:24.9767 R:13.0000 rate:0.0260 gloss:1.2785 dloss:1.2854 exploreP:0.8992\n",
      "Episode:43 meanR:25.2955 R:39.0000 rate:0.0780 gloss:1.2776 dloss:1.2840 exploreP:0.8957\n",
      "Episode:44 meanR:25.2667 R:24.0000 rate:0.0480 gloss:1.2785 dloss:1.2840 exploreP:0.8936\n",
      "Episode:45 meanR:25.0000 R:13.0000 rate:0.0260 gloss:1.2782 dloss:1.2830 exploreP:0.8925\n",
      "Episode:46 meanR:24.9362 R:22.0000 rate:0.0440 gloss:1.2789 dloss:1.2855 exploreP:0.8905\n",
      "Episode:47 meanR:24.7083 R:14.0000 rate:0.0280 gloss:1.2779 dloss:1.2839 exploreP:0.8893\n",
      "Episode:48 meanR:25.0000 R:39.0000 rate:0.0780 gloss:1.2774 dloss:1.2833 exploreP:0.8859\n",
      "Episode:49 meanR:24.7800 R:14.0000 rate:0.0280 gloss:1.2766 dloss:1.2806 exploreP:0.8846\n",
      "Episode:50 meanR:24.5490 R:13.0000 rate:0.0260 gloss:1.2762 dloss:1.2818 exploreP:0.8835\n",
      "Episode:51 meanR:24.3846 R:16.0000 rate:0.0320 gloss:1.2746 dloss:1.2790 exploreP:0.8821\n",
      "Episode:52 meanR:24.9434 R:54.0000 rate:0.1080 gloss:1.2757 dloss:1.2796 exploreP:0.8774\n",
      "Episode:53 meanR:24.6667 R:10.0000 rate:0.0200 gloss:1.2751 dloss:1.2788 exploreP:0.8765\n",
      "Episode:54 meanR:24.4727 R:14.0000 rate:0.0280 gloss:1.2745 dloss:1.2781 exploreP:0.8753\n",
      "Episode:55 meanR:24.3214 R:16.0000 rate:0.0320 gloss:1.2754 dloss:1.2798 exploreP:0.8739\n",
      "Episode:56 meanR:24.2281 R:19.0000 rate:0.0380 gloss:1.2754 dloss:1.2789 exploreP:0.8723\n",
      "Episode:57 meanR:24.1897 R:22.0000 rate:0.0440 gloss:1.2755 dloss:1.2786 exploreP:0.8704\n",
      "Episode:58 meanR:24.4068 R:37.0000 rate:0.0740 gloss:1.2752 dloss:1.2787 exploreP:0.8672\n",
      "Episode:59 meanR:24.4833 R:29.0000 rate:0.0580 gloss:1.2751 dloss:1.2772 exploreP:0.8647\n",
      "Episode:60 meanR:24.2459 R:10.0000 rate:0.0200 gloss:1.2738 dloss:1.2762 exploreP:0.8639\n",
      "Episode:61 meanR:24.1129 R:16.0000 rate:0.0320 gloss:1.2757 dloss:1.2789 exploreP:0.8625\n",
      "Episode:62 meanR:24.0952 R:23.0000 rate:0.0460 gloss:1.2752 dloss:1.2777 exploreP:0.8606\n",
      "Episode:63 meanR:24.0469 R:21.0000 rate:0.0420 gloss:1.2760 dloss:1.2772 exploreP:0.8588\n",
      "Episode:64 meanR:24.8154 R:74.0000 rate:0.1480 gloss:1.2760 dloss:1.2780 exploreP:0.8525\n",
      "Episode:65 meanR:24.6364 R:13.0000 rate:0.0260 gloss:1.2758 dloss:1.2778 exploreP:0.8514\n",
      "Episode:66 meanR:24.4478 R:12.0000 rate:0.0240 gloss:1.2754 dloss:1.2769 exploreP:0.8504\n",
      "Episode:67 meanR:24.9853 R:61.0000 rate:0.1220 gloss:1.2758 dloss:1.2770 exploreP:0.8453\n",
      "Episode:68 meanR:25.1449 R:36.0000 rate:0.0720 gloss:1.2755 dloss:1.2769 exploreP:0.8423\n",
      "Episode:69 meanR:25.1429 R:25.0000 rate:0.0500 gloss:1.2753 dloss:1.2764 exploreP:0.8402\n",
      "Episode:70 meanR:25.0986 R:22.0000 rate:0.0440 gloss:1.2762 dloss:1.2771 exploreP:0.8384\n",
      "Episode:71 meanR:25.1389 R:28.0000 rate:0.0560 gloss:1.2760 dloss:1.2762 exploreP:0.8361\n",
      "Episode:72 meanR:24.9726 R:13.0000 rate:0.0260 gloss:1.2772 dloss:1.2778 exploreP:0.8350\n",
      "Episode:73 meanR:24.9324 R:22.0000 rate:0.0440 gloss:1.2769 dloss:1.2774 exploreP:0.8332\n",
      "Episode:74 meanR:24.7467 R:11.0000 rate:0.0220 gloss:1.2768 dloss:1.2777 exploreP:0.8323\n",
      "Episode:75 meanR:24.6842 R:20.0000 rate:0.0400 gloss:1.2770 dloss:1.2778 exploreP:0.8307\n",
      "Episode:76 meanR:24.5584 R:15.0000 rate:0.0300 gloss:1.2773 dloss:1.2773 exploreP:0.8294\n",
      "Episode:77 meanR:24.5513 R:24.0000 rate:0.0480 gloss:1.2758 dloss:1.2762 exploreP:0.8275\n",
      "Episode:78 meanR:24.3797 R:11.0000 rate:0.0220 gloss:1.2764 dloss:1.2766 exploreP:0.8266\n",
      "Episode:79 meanR:24.4875 R:33.0000 rate:0.0660 gloss:1.2769 dloss:1.2767 exploreP:0.8239\n",
      "Episode:80 meanR:24.5309 R:28.0000 rate:0.0560 gloss:1.2773 dloss:1.2771 exploreP:0.8216\n",
      "Episode:81 meanR:24.4756 R:20.0000 rate:0.0400 gloss:1.2765 dloss:1.2765 exploreP:0.8200\n",
      "Episode:82 meanR:24.3614 R:15.0000 rate:0.0300 gloss:1.2768 dloss:1.2771 exploreP:0.8188\n",
      "Episode:83 meanR:24.2024 R:11.0000 rate:0.0220 gloss:1.2771 dloss:1.2768 exploreP:0.8179\n",
      "Episode:84 meanR:24.0471 R:11.0000 rate:0.0220 gloss:1.2773 dloss:1.2772 exploreP:0.8170\n",
      "Episode:85 meanR:23.8953 R:11.0000 rate:0.0220 gloss:1.2778 dloss:1.2774 exploreP:0.8161\n",
      "Episode:86 meanR:23.9310 R:27.0000 rate:0.0540 gloss:1.2778 dloss:1.2774 exploreP:0.8139\n",
      "Episode:87 meanR:23.8750 R:19.0000 rate:0.0380 gloss:1.2776 dloss:1.2776 exploreP:0.8124\n",
      "Episode:88 meanR:23.7640 R:14.0000 rate:0.0280 gloss:1.2763 dloss:1.2761 exploreP:0.8113\n",
      "Episode:89 meanR:24.0667 R:51.0000 rate:0.1020 gloss:1.2772 dloss:1.2770 exploreP:0.8072\n",
      "Episode:90 meanR:23.9670 R:15.0000 rate:0.0300 gloss:1.2763 dloss:1.2761 exploreP:0.8060\n",
      "Episode:91 meanR:24.1739 R:43.0000 rate:0.0860 gloss:1.2771 dloss:1.2770 exploreP:0.8026\n",
      "Episode:92 meanR:24.1398 R:21.0000 rate:0.0420 gloss:1.2771 dloss:1.2767 exploreP:0.8009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:93 meanR:24.0106 R:12.0000 rate:0.0240 gloss:1.2753 dloss:1.2750 exploreP:0.8000\n",
      "Episode:94 meanR:23.9158 R:15.0000 rate:0.0300 gloss:1.2786 dloss:1.2780 exploreP:0.7988\n",
      "Episode:95 meanR:23.7917 R:12.0000 rate:0.0240 gloss:1.2772 dloss:1.2762 exploreP:0.7978\n",
      "Episode:96 meanR:23.6495 R:10.0000 rate:0.0200 gloss:1.2764 dloss:1.2755 exploreP:0.7971\n",
      "Episode:97 meanR:23.6633 R:25.0000 rate:0.0500 gloss:1.2778 dloss:1.2773 exploreP:0.7951\n",
      "Episode:98 meanR:23.6162 R:19.0000 rate:0.0380 gloss:1.2762 dloss:1.2760 exploreP:0.7936\n",
      "Episode:99 meanR:23.5000 R:12.0000 rate:0.0240 gloss:1.2779 dloss:1.2769 exploreP:0.7927\n",
      "Episode:100 meanR:23.4700 R:16.0000 rate:0.0320 gloss:1.2769 dloss:1.2764 exploreP:0.7914\n",
      "Episode:101 meanR:23.5700 R:29.0000 rate:0.0580 gloss:1.2768 dloss:1.2761 exploreP:0.7892\n",
      "Episode:102 meanR:23.6600 R:50.0000 rate:0.1000 gloss:1.2763 dloss:1.2759 exploreP:0.7853\n",
      "Episode:103 meanR:23.1700 R:9.0000 rate:0.0180 gloss:1.2761 dloss:1.2754 exploreP:0.7846\n",
      "Episode:104 meanR:23.3200 R:29.0000 rate:0.0580 gloss:1.2752 dloss:1.2746 exploreP:0.7823\n",
      "Episode:105 meanR:23.2400 R:16.0000 rate:0.0320 gloss:1.2746 dloss:1.2743 exploreP:0.7811\n",
      "Episode:106 meanR:23.1200 R:19.0000 rate:0.0380 gloss:1.2758 dloss:1.2748 exploreP:0.7796\n",
      "Episode:107 meanR:23.0700 R:16.0000 rate:0.0320 gloss:1.2760 dloss:1.2756 exploreP:0.7784\n",
      "Episode:108 meanR:23.0500 R:17.0000 rate:0.0340 gloss:1.2760 dloss:1.2755 exploreP:0.7771\n",
      "Episode:109 meanR:23.0300 R:11.0000 rate:0.0220 gloss:1.2768 dloss:1.2760 exploreP:0.7762\n",
      "Episode:110 meanR:23.2100 R:33.0000 rate:0.0660 gloss:1.2760 dloss:1.2752 exploreP:0.7737\n",
      "Episode:111 meanR:23.4100 R:31.0000 rate:0.0620 gloss:1.2751 dloss:1.2742 exploreP:0.7714\n",
      "Episode:112 meanR:22.9500 R:14.0000 rate:0.0280 gloss:1.2748 dloss:1.2741 exploreP:0.7703\n",
      "Episode:113 meanR:23.0300 R:24.0000 rate:0.0480 gloss:1.2743 dloss:1.2733 exploreP:0.7685\n",
      "Episode:114 meanR:23.3100 R:37.0000 rate:0.0740 gloss:1.2741 dloss:1.2729 exploreP:0.7657\n",
      "Episode:115 meanR:23.2900 R:12.0000 rate:0.0240 gloss:1.2738 dloss:1.2729 exploreP:0.7648\n",
      "Episode:116 meanR:22.7500 R:13.0000 rate:0.0260 gloss:1.2738 dloss:1.2726 exploreP:0.7638\n",
      "Episode:117 meanR:22.8600 R:26.0000 rate:0.0520 gloss:1.2729 dloss:1.2719 exploreP:0.7618\n",
      "Episode:118 meanR:22.9100 R:23.0000 rate:0.0460 gloss:1.2738 dloss:1.2729 exploreP:0.7601\n",
      "Episode:119 meanR:22.6500 R:10.0000 rate:0.0200 gloss:1.2739 dloss:1.2729 exploreP:0.7593\n",
      "Episode:120 meanR:22.6300 R:14.0000 rate:0.0280 gloss:1.2731 dloss:1.2720 exploreP:0.7583\n",
      "Episode:121 meanR:22.3600 R:16.0000 rate:0.0320 gloss:1.2727 dloss:1.2720 exploreP:0.7571\n",
      "Episode:122 meanR:22.3500 R:10.0000 rate:0.0200 gloss:1.2737 dloss:1.2727 exploreP:0.7564\n",
      "Episode:123 meanR:22.1600 R:23.0000 rate:0.0460 gloss:1.2717 dloss:1.2709 exploreP:0.7546\n",
      "Episode:124 meanR:22.3200 R:29.0000 rate:0.0580 gloss:1.2728 dloss:1.2718 exploreP:0.7525\n",
      "Episode:125 meanR:22.3700 R:15.0000 rate:0.0300 gloss:1.2707 dloss:1.2697 exploreP:0.7514\n",
      "Episode:126 meanR:22.0200 R:10.0000 rate:0.0200 gloss:1.2714 dloss:1.2704 exploreP:0.7506\n",
      "Episode:127 meanR:22.1200 R:22.0000 rate:0.0440 gloss:1.2713 dloss:1.2703 exploreP:0.7490\n",
      "Episode:128 meanR:22.0900 R:13.0000 rate:0.0260 gloss:1.2715 dloss:1.2704 exploreP:0.7480\n",
      "Episode:129 meanR:22.0200 R:10.0000 rate:0.0200 gloss:1.2715 dloss:1.2708 exploreP:0.7473\n",
      "Episode:130 meanR:21.8300 R:9.0000 rate:0.0180 gloss:1.2702 dloss:1.2690 exploreP:0.7466\n",
      "Episode:131 meanR:21.6500 R:13.0000 rate:0.0260 gloss:1.2715 dloss:1.2707 exploreP:0.7457\n",
      "Episode:132 meanR:21.6500 R:18.0000 rate:0.0360 gloss:1.2716 dloss:1.2705 exploreP:0.7444\n",
      "Episode:133 meanR:21.9600 R:43.0000 rate:0.0860 gloss:1.2714 dloss:1.2706 exploreP:0.7412\n",
      "Episode:134 meanR:21.8500 R:12.0000 rate:0.0240 gloss:1.2707 dloss:1.2700 exploreP:0.7403\n",
      "Episode:135 meanR:21.8800 R:21.0000 rate:0.0420 gloss:1.2714 dloss:1.2706 exploreP:0.7388\n",
      "Episode:136 meanR:21.8700 R:41.0000 rate:0.0820 gloss:1.2713 dloss:1.2703 exploreP:0.7358\n",
      "Episode:137 meanR:21.8800 R:13.0000 rate:0.0260 gloss:1.2710 dloss:1.2699 exploreP:0.7349\n",
      "Episode:138 meanR:21.9500 R:19.0000 rate:0.0380 gloss:1.2699 dloss:1.2692 exploreP:0.7335\n",
      "Episode:139 meanR:21.8900 R:13.0000 rate:0.0260 gloss:1.2703 dloss:1.2693 exploreP:0.7326\n",
      "Episode:140 meanR:21.3000 R:14.0000 rate:0.0280 gloss:1.2701 dloss:1.2692 exploreP:0.7316\n",
      "Episode:141 meanR:21.1500 R:13.0000 rate:0.0260 gloss:1.2695 dloss:1.2685 exploreP:0.7306\n",
      "Episode:142 meanR:21.3600 R:34.0000 rate:0.0680 gloss:1.2694 dloss:1.2683 exploreP:0.7282\n",
      "Episode:143 meanR:21.3900 R:42.0000 rate:0.0840 gloss:1.2692 dloss:1.2683 exploreP:0.7252\n",
      "Episode:144 meanR:21.3500 R:20.0000 rate:0.0400 gloss:1.2681 dloss:1.2670 exploreP:0.7237\n",
      "Episode:145 meanR:21.3500 R:13.0000 rate:0.0260 gloss:1.2673 dloss:1.2660 exploreP:0.7228\n",
      "Episode:146 meanR:21.2300 R:10.0000 rate:0.0200 gloss:1.2676 dloss:1.2666 exploreP:0.7221\n",
      "Episode:147 meanR:21.2100 R:12.0000 rate:0.0240 gloss:1.2671 dloss:1.2658 exploreP:0.7212\n",
      "Episode:148 meanR:20.9500 R:13.0000 rate:0.0260 gloss:1.2669 dloss:1.2659 exploreP:0.7203\n",
      "Episode:149 meanR:20.9600 R:15.0000 rate:0.0300 gloss:1.2664 dloss:1.2656 exploreP:0.7192\n",
      "Episode:150 meanR:20.9500 R:12.0000 rate:0.0240 gloss:1.2663 dloss:1.2653 exploreP:0.7184\n",
      "Episode:151 meanR:20.9700 R:18.0000 rate:0.0360 gloss:1.2654 dloss:1.2645 exploreP:0.7171\n",
      "Episode:152 meanR:20.6000 R:17.0000 rate:0.0340 gloss:1.2658 dloss:1.2649 exploreP:0.7159\n",
      "Episode:153 meanR:20.7400 R:24.0000 rate:0.0480 gloss:1.2649 dloss:1.2639 exploreP:0.7142\n",
      "Episode:154 meanR:20.7100 R:11.0000 rate:0.0220 gloss:1.2638 dloss:1.2631 exploreP:0.7135\n",
      "Episode:155 meanR:20.6600 R:11.0000 rate:0.0220 gloss:1.2638 dloss:1.2631 exploreP:0.7127\n",
      "Episode:156 meanR:20.8900 R:42.0000 rate:0.0840 gloss:1.2629 dloss:1.2621 exploreP:0.7097\n",
      "Episode:157 meanR:20.7900 R:12.0000 rate:0.0240 gloss:1.2621 dloss:1.2615 exploreP:0.7089\n",
      "Episode:158 meanR:20.6100 R:19.0000 rate:0.0380 gloss:1.2617 dloss:1.2609 exploreP:0.7076\n",
      "Episode:159 meanR:20.4300 R:11.0000 rate:0.0220 gloss:1.2622 dloss:1.2611 exploreP:0.7068\n",
      "Episode:160 meanR:20.5900 R:26.0000 rate:0.0520 gloss:1.2605 dloss:1.2598 exploreP:0.7050\n",
      "Episode:161 meanR:20.7400 R:31.0000 rate:0.0620 gloss:1.2603 dloss:1.2597 exploreP:0.7028\n",
      "Episode:162 meanR:20.6600 R:15.0000 rate:0.0300 gloss:1.2592 dloss:1.2587 exploreP:0.7018\n",
      "Episode:163 meanR:20.5800 R:13.0000 rate:0.0260 gloss:1.2602 dloss:1.2595 exploreP:0.7009\n",
      "Episode:164 meanR:19.9900 R:15.0000 rate:0.0300 gloss:1.2601 dloss:1.2594 exploreP:0.6999\n",
      "Episode:165 meanR:20.0200 R:16.0000 rate:0.0320 gloss:1.2595 dloss:1.2589 exploreP:0.6988\n",
      "Episode:166 meanR:20.0800 R:18.0000 rate:0.0360 gloss:1.2593 dloss:1.2583 exploreP:0.6975\n",
      "Episode:167 meanR:19.6200 R:15.0000 rate:0.0300 gloss:1.2581 dloss:1.2575 exploreP:0.6965\n",
      "Episode:168 meanR:19.3900 R:13.0000 rate:0.0260 gloss:1.2587 dloss:1.2583 exploreP:0.6956\n",
      "Episode:169 meanR:19.2700 R:13.0000 rate:0.0260 gloss:1.2586 dloss:1.2575 exploreP:0.6947\n",
      "Episode:170 meanR:19.2000 R:15.0000 rate:0.0300 gloss:1.2590 dloss:1.2584 exploreP:0.6937\n",
      "Episode:171 meanR:19.0600 R:14.0000 rate:0.0280 gloss:1.2585 dloss:1.2578 exploreP:0.6927\n",
      "Episode:172 meanR:19.0700 R:14.0000 rate:0.0280 gloss:1.2578 dloss:1.2571 exploreP:0.6918\n",
      "Episode:173 meanR:19.0000 R:15.0000 rate:0.0300 gloss:1.2580 dloss:1.2571 exploreP:0.6908\n",
      "Episode:174 meanR:19.0200 R:13.0000 rate:0.0260 gloss:1.2581 dloss:1.2576 exploreP:0.6899\n",
      "Episode:175 meanR:18.9300 R:11.0000 rate:0.0220 gloss:1.2582 dloss:1.2578 exploreP:0.6891\n",
      "Episode:176 meanR:18.9500 R:17.0000 rate:0.0340 gloss:1.2583 dloss:1.2574 exploreP:0.6880\n",
      "Episode:177 meanR:18.8400 R:13.0000 rate:0.0260 gloss:1.2577 dloss:1.2571 exploreP:0.6871\n",
      "Episode:178 meanR:18.9300 R:20.0000 rate:0.0400 gloss:1.2583 dloss:1.2576 exploreP:0.6857\n",
      "Episode:179 meanR:18.8700 R:27.0000 rate:0.0540 gloss:1.2571 dloss:1.2565 exploreP:0.6839\n",
      "Episode:180 meanR:18.7900 R:20.0000 rate:0.0400 gloss:1.2556 dloss:1.2551 exploreP:0.6826\n",
      "Episode:181 meanR:18.7100 R:12.0000 rate:0.0240 gloss:1.2562 dloss:1.2556 exploreP:0.6818\n",
      "Episode:182 meanR:18.6800 R:12.0000 rate:0.0240 gloss:1.2570 dloss:1.2565 exploreP:0.6810\n",
      "Episode:183 meanR:18.6900 R:12.0000 rate:0.0240 gloss:1.2572 dloss:1.2567 exploreP:0.6802\n",
      "Episode:184 meanR:18.7200 R:14.0000 rate:0.0280 gloss:1.2562 dloss:1.2558 exploreP:0.6792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:185 meanR:18.7200 R:11.0000 rate:0.0220 gloss:1.2577 dloss:1.2575 exploreP:0.6785\n",
      "Episode:186 meanR:18.5600 R:11.0000 rate:0.0220 gloss:1.2582 dloss:1.2576 exploreP:0.6777\n",
      "Episode:187 meanR:18.5100 R:14.0000 rate:0.0280 gloss:1.2571 dloss:1.2564 exploreP:0.6768\n",
      "Episode:188 meanR:18.5200 R:15.0000 rate:0.0300 gloss:1.2572 dloss:1.2565 exploreP:0.6758\n",
      "Episode:189 meanR:18.0900 R:8.0000 rate:0.0160 gloss:1.2582 dloss:1.2575 exploreP:0.6753\n",
      "Episode:190 meanR:18.0900 R:15.0000 rate:0.0300 gloss:1.2581 dloss:1.2578 exploreP:0.6743\n",
      "Episode:191 meanR:17.8300 R:17.0000 rate:0.0340 gloss:1.2581 dloss:1.2577 exploreP:0.6732\n",
      "Episode:192 meanR:17.9700 R:35.0000 rate:0.0700 gloss:1.2579 dloss:1.2573 exploreP:0.6708\n",
      "Episode:193 meanR:17.9600 R:11.0000 rate:0.0220 gloss:1.2579 dloss:1.2573 exploreP:0.6701\n",
      "Episode:194 meanR:17.9200 R:11.0000 rate:0.0220 gloss:1.2573 dloss:1.2569 exploreP:0.6694\n",
      "Episode:195 meanR:17.9300 R:13.0000 rate:0.0260 gloss:1.2574 dloss:1.2569 exploreP:0.6685\n",
      "Episode:196 meanR:17.9300 R:10.0000 rate:0.0200 gloss:1.2568 dloss:1.2564 exploreP:0.6679\n",
      "Episode:197 meanR:17.8200 R:14.0000 rate:0.0280 gloss:1.2563 dloss:1.2555 exploreP:0.6669\n",
      "Episode:198 meanR:17.7700 R:14.0000 rate:0.0280 gloss:1.2570 dloss:1.2564 exploreP:0.6660\n",
      "Episode:199 meanR:17.7900 R:14.0000 rate:0.0280 gloss:1.2566 dloss:1.2559 exploreP:0.6651\n",
      "Episode:200 meanR:17.7300 R:10.0000 rate:0.0200 gloss:1.2565 dloss:1.2559 exploreP:0.6645\n",
      "Episode:201 meanR:17.5300 R:9.0000 rate:0.0180 gloss:1.2585 dloss:1.2579 exploreP:0.6639\n",
      "Episode:202 meanR:17.1100 R:8.0000 rate:0.0160 gloss:1.2572 dloss:1.2571 exploreP:0.6633\n",
      "Episode:203 meanR:17.1100 R:9.0000 rate:0.0180 gloss:1.2569 dloss:1.2564 exploreP:0.6628\n",
      "Episode:204 meanR:16.9400 R:12.0000 rate:0.0240 gloss:1.2584 dloss:1.2578 exploreP:0.6620\n",
      "Episode:205 meanR:16.8900 R:11.0000 rate:0.0220 gloss:1.2581 dloss:1.2577 exploreP:0.6613\n",
      "Episode:206 meanR:16.8200 R:12.0000 rate:0.0240 gloss:1.2576 dloss:1.2573 exploreP:0.6605\n",
      "Episode:207 meanR:16.8400 R:18.0000 rate:0.0360 gloss:1.2583 dloss:1.2576 exploreP:0.6593\n",
      "Episode:208 meanR:16.8200 R:15.0000 rate:0.0300 gloss:1.2597 dloss:1.2589 exploreP:0.6583\n",
      "Episode:209 meanR:16.8700 R:16.0000 rate:0.0320 gloss:1.2594 dloss:1.2592 exploreP:0.6573\n",
      "Episode:210 meanR:16.6700 R:13.0000 rate:0.0260 gloss:1.2601 dloss:1.2595 exploreP:0.6565\n",
      "Episode:211 meanR:16.4600 R:10.0000 rate:0.0200 gloss:1.2591 dloss:1.2584 exploreP:0.6558\n",
      "Episode:212 meanR:16.4200 R:10.0000 rate:0.0200 gloss:1.2590 dloss:1.2585 exploreP:0.6552\n",
      "Episode:213 meanR:16.3400 R:16.0000 rate:0.0320 gloss:1.2595 dloss:1.2591 exploreP:0.6541\n",
      "Episode:214 meanR:16.1500 R:18.0000 rate:0.0360 gloss:1.2601 dloss:1.2595 exploreP:0.6530\n",
      "Episode:215 meanR:16.1600 R:13.0000 rate:0.0260 gloss:1.2605 dloss:1.2599 exploreP:0.6521\n",
      "Episode:216 meanR:16.1300 R:10.0000 rate:0.0200 gloss:1.2618 dloss:1.2615 exploreP:0.6515\n",
      "Episode:217 meanR:16.0400 R:17.0000 rate:0.0340 gloss:1.2607 dloss:1.2601 exploreP:0.6504\n",
      "Episode:218 meanR:15.9100 R:10.0000 rate:0.0200 gloss:1.2601 dloss:1.2598 exploreP:0.6498\n",
      "Episode:219 meanR:15.9300 R:12.0000 rate:0.0240 gloss:1.2625 dloss:1.2622 exploreP:0.6490\n",
      "Episode:220 meanR:15.9200 R:13.0000 rate:0.0260 gloss:1.2609 dloss:1.2603 exploreP:0.6482\n",
      "Episode:221 meanR:15.8900 R:13.0000 rate:0.0260 gloss:1.2633 dloss:1.2631 exploreP:0.6473\n",
      "Episode:222 meanR:15.8900 R:10.0000 rate:0.0200 gloss:1.2618 dloss:1.2613 exploreP:0.6467\n",
      "Episode:223 meanR:15.8400 R:18.0000 rate:0.0360 gloss:1.2621 dloss:1.2617 exploreP:0.6456\n",
      "Episode:224 meanR:15.6600 R:11.0000 rate:0.0220 gloss:1.2627 dloss:1.2624 exploreP:0.6449\n",
      "Episode:225 meanR:15.6300 R:12.0000 rate:0.0240 gloss:1.2625 dloss:1.2622 exploreP:0.6441\n",
      "Episode:226 meanR:15.6400 R:11.0000 rate:0.0220 gloss:1.2644 dloss:1.2642 exploreP:0.6434\n",
      "Episode:227 meanR:15.5100 R:9.0000 rate:0.0180 gloss:1.2642 dloss:1.2641 exploreP:0.6428\n",
      "Episode:228 meanR:15.4900 R:11.0000 rate:0.0220 gloss:1.2637 dloss:1.2633 exploreP:0.6421\n",
      "Episode:229 meanR:15.5200 R:13.0000 rate:0.0260 gloss:1.2629 dloss:1.2625 exploreP:0.6413\n",
      "Episode:230 meanR:15.5700 R:14.0000 rate:0.0280 gloss:1.2634 dloss:1.2630 exploreP:0.6404\n",
      "Episode:231 meanR:15.5900 R:15.0000 rate:0.0300 gloss:1.2634 dloss:1.2632 exploreP:0.6395\n",
      "Episode:232 meanR:15.5400 R:13.0000 rate:0.0260 gloss:1.2635 dloss:1.2631 exploreP:0.6387\n",
      "Episode:233 meanR:15.2400 R:13.0000 rate:0.0260 gloss:1.2640 dloss:1.2636 exploreP:0.6379\n",
      "Episode:234 meanR:15.2300 R:11.0000 rate:0.0220 gloss:1.2644 dloss:1.2640 exploreP:0.6372\n",
      "Episode:235 meanR:15.1600 R:14.0000 rate:0.0280 gloss:1.2632 dloss:1.2628 exploreP:0.6363\n",
      "Episode:236 meanR:14.8400 R:9.0000 rate:0.0180 gloss:1.2658 dloss:1.2656 exploreP:0.6357\n",
      "Episode:237 meanR:14.8000 R:9.0000 rate:0.0180 gloss:1.2641 dloss:1.2639 exploreP:0.6352\n",
      "Episode:238 meanR:14.7600 R:15.0000 rate:0.0300 gloss:1.2641 dloss:1.2638 exploreP:0.6342\n",
      "Episode:239 meanR:14.8600 R:23.0000 rate:0.0460 gloss:1.2646 dloss:1.2645 exploreP:0.6328\n",
      "Episode:240 meanR:14.8100 R:9.0000 rate:0.0180 gloss:1.2667 dloss:1.2665 exploreP:0.6322\n",
      "Episode:241 meanR:14.7900 R:11.0000 rate:0.0220 gloss:1.2657 dloss:1.2654 exploreP:0.6315\n",
      "Episode:242 meanR:14.6000 R:15.0000 rate:0.0300 gloss:1.2651 dloss:1.2646 exploreP:0.6306\n",
      "Episode:243 meanR:14.3000 R:12.0000 rate:0.0240 gloss:1.2659 dloss:1.2658 exploreP:0.6299\n",
      "Episode:244 meanR:14.2200 R:12.0000 rate:0.0240 gloss:1.2652 dloss:1.2650 exploreP:0.6291\n",
      "Episode:245 meanR:14.5500 R:46.0000 rate:0.0920 gloss:1.2671 dloss:1.2669 exploreP:0.6263\n",
      "Episode:246 meanR:14.5600 R:11.0000 rate:0.0220 gloss:1.2686 dloss:1.2685 exploreP:0.6256\n",
      "Episode:247 meanR:14.6200 R:18.0000 rate:0.0360 gloss:1.2684 dloss:1.2680 exploreP:0.6245\n",
      "Episode:248 meanR:14.5800 R:9.0000 rate:0.0180 gloss:1.2683 dloss:1.2679 exploreP:0.6239\n",
      "Episode:249 meanR:14.6400 R:21.0000 rate:0.0420 gloss:1.2675 dloss:1.2671 exploreP:0.6227\n",
      "Episode:250 meanR:14.6600 R:14.0000 rate:0.0280 gloss:1.2673 dloss:1.2667 exploreP:0.6218\n",
      "Episode:251 meanR:14.5700 R:9.0000 rate:0.0180 gloss:1.2691 dloss:1.2687 exploreP:0.6212\n",
      "Episode:252 meanR:14.5400 R:14.0000 rate:0.0280 gloss:1.2683 dloss:1.2681 exploreP:0.6204\n",
      "Episode:253 meanR:14.4000 R:10.0000 rate:0.0200 gloss:1.2692 dloss:1.2688 exploreP:0.6198\n",
      "Episode:254 meanR:14.3800 R:9.0000 rate:0.0180 gloss:1.2699 dloss:1.2696 exploreP:0.6192\n",
      "Episode:255 meanR:14.3700 R:10.0000 rate:0.0200 gloss:1.2705 dloss:1.2702 exploreP:0.6186\n",
      "Episode:256 meanR:14.0900 R:14.0000 rate:0.0280 gloss:1.2698 dloss:1.2697 exploreP:0.6178\n",
      "Episode:257 meanR:14.0600 R:9.0000 rate:0.0180 gloss:1.2709 dloss:1.2708 exploreP:0.6172\n",
      "Episode:258 meanR:14.0200 R:15.0000 rate:0.0300 gloss:1.2703 dloss:1.2701 exploreP:0.6163\n",
      "Episode:259 meanR:14.1800 R:27.0000 rate:0.0540 gloss:1.2704 dloss:1.2702 exploreP:0.6147\n",
      "Episode:260 meanR:14.1000 R:18.0000 rate:0.0360 gloss:1.2709 dloss:1.2705 exploreP:0.6136\n",
      "Episode:261 meanR:13.9700 R:18.0000 rate:0.0360 gloss:1.2714 dloss:1.2713 exploreP:0.6125\n",
      "Episode:262 meanR:14.0600 R:24.0000 rate:0.0480 gloss:1.2725 dloss:1.2722 exploreP:0.6111\n",
      "Episode:263 meanR:14.0400 R:11.0000 rate:0.0220 gloss:1.2728 dloss:1.2725 exploreP:0.6104\n",
      "Episode:264 meanR:14.0100 R:12.0000 rate:0.0240 gloss:1.2729 dloss:1.2726 exploreP:0.6097\n",
      "Episode:265 meanR:13.9800 R:13.0000 rate:0.0260 gloss:1.2728 dloss:1.2726 exploreP:0.6089\n",
      "Episode:266 meanR:13.9100 R:11.0000 rate:0.0220 gloss:1.2736 dloss:1.2734 exploreP:0.6082\n",
      "Episode:267 meanR:13.8600 R:10.0000 rate:0.0200 gloss:1.2737 dloss:1.2734 exploreP:0.6076\n",
      "Episode:268 meanR:13.8400 R:11.0000 rate:0.0220 gloss:1.2727 dloss:1.2728 exploreP:0.6070\n",
      "Episode:269 meanR:13.8300 R:12.0000 rate:0.0240 gloss:1.2719 dloss:1.2714 exploreP:0.6063\n",
      "Episode:270 meanR:13.8200 R:14.0000 rate:0.0280 gloss:1.2736 dloss:1.2734 exploreP:0.6054\n",
      "Episode:271 meanR:13.8000 R:12.0000 rate:0.0240 gloss:1.2734 dloss:1.2731 exploreP:0.6047\n",
      "Episode:272 meanR:13.7400 R:8.0000 rate:0.0160 gloss:1.2735 dloss:1.2730 exploreP:0.6043\n",
      "Episode:273 meanR:13.7000 R:11.0000 rate:0.0220 gloss:1.2727 dloss:1.2723 exploreP:0.6036\n",
      "Episode:274 meanR:13.8000 R:23.0000 rate:0.0460 gloss:1.2738 dloss:1.2736 exploreP:0.6022\n",
      "Episode:275 meanR:13.7800 R:9.0000 rate:0.0180 gloss:1.2748 dloss:1.2749 exploreP:0.6017\n",
      "Episode:276 meanR:13.7400 R:13.0000 rate:0.0260 gloss:1.2736 dloss:1.2735 exploreP:0.6009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:277 meanR:13.7000 R:9.0000 rate:0.0180 gloss:1.2743 dloss:1.2740 exploreP:0.6004\n",
      "Episode:278 meanR:13.6300 R:13.0000 rate:0.0260 gloss:1.2737 dloss:1.2735 exploreP:0.5996\n",
      "Episode:279 meanR:13.5300 R:17.0000 rate:0.0340 gloss:1.2746 dloss:1.2743 exploreP:0.5986\n",
      "Episode:280 meanR:13.4500 R:12.0000 rate:0.0240 gloss:1.2741 dloss:1.2737 exploreP:0.5979\n",
      "Episode:281 meanR:13.4500 R:12.0000 rate:0.0240 gloss:1.2743 dloss:1.2740 exploreP:0.5972\n",
      "Episode:282 meanR:13.4600 R:13.0000 rate:0.0260 gloss:1.2744 dloss:1.2739 exploreP:0.5965\n",
      "Episode:283 meanR:13.4500 R:11.0000 rate:0.0220 gloss:1.2758 dloss:1.2757 exploreP:0.5958\n",
      "Episode:284 meanR:13.4400 R:13.0000 rate:0.0260 gloss:1.2762 dloss:1.2760 exploreP:0.5951\n",
      "Episode:285 meanR:13.4900 R:16.0000 rate:0.0320 gloss:1.2768 dloss:1.2765 exploreP:0.5941\n",
      "Episode:286 meanR:13.4600 R:8.0000 rate:0.0160 gloss:1.2752 dloss:1.2748 exploreP:0.5937\n",
      "Episode:287 meanR:13.4800 R:16.0000 rate:0.0320 gloss:1.2776 dloss:1.2774 exploreP:0.5927\n",
      "Episode:288 meanR:13.4700 R:14.0000 rate:0.0280 gloss:1.2773 dloss:1.2771 exploreP:0.5919\n",
      "Episode:289 meanR:13.5000 R:11.0000 rate:0.0220 gloss:1.2759 dloss:1.2760 exploreP:0.5913\n",
      "Episode:290 meanR:13.5300 R:18.0000 rate:0.0360 gloss:1.2766 dloss:1.2764 exploreP:0.5902\n",
      "Episode:291 meanR:13.5700 R:21.0000 rate:0.0420 gloss:1.2771 dloss:1.2767 exploreP:0.5890\n",
      "Episode:292 meanR:13.3200 R:10.0000 rate:0.0200 gloss:1.2767 dloss:1.2767 exploreP:0.5884\n",
      "Episode:293 meanR:13.3100 R:10.0000 rate:0.0200 gloss:1.2766 dloss:1.2763 exploreP:0.5878\n",
      "Episode:294 meanR:13.3000 R:10.0000 rate:0.0200 gloss:1.2774 dloss:1.2769 exploreP:0.5873\n",
      "Episode:295 meanR:13.2800 R:11.0000 rate:0.0220 gloss:1.2766 dloss:1.2765 exploreP:0.5866\n",
      "Episode:296 meanR:13.3200 R:14.0000 rate:0.0280 gloss:1.2781 dloss:1.2778 exploreP:0.5858\n",
      "Episode:297 meanR:13.3300 R:15.0000 rate:0.0300 gloss:1.2776 dloss:1.2774 exploreP:0.5850\n",
      "Episode:298 meanR:13.2900 R:10.0000 rate:0.0200 gloss:1.2784 dloss:1.2783 exploreP:0.5844\n",
      "Episode:299 meanR:13.2500 R:10.0000 rate:0.0200 gloss:1.2780 dloss:1.2777 exploreP:0.5838\n",
      "Episode:300 meanR:13.2500 R:10.0000 rate:0.0200 gloss:1.2784 dloss:1.2783 exploreP:0.5832\n",
      "Episode:301 meanR:13.3100 R:15.0000 rate:0.0300 gloss:1.2791 dloss:1.2788 exploreP:0.5824\n",
      "Episode:302 meanR:13.4400 R:21.0000 rate:0.0420 gloss:1.2792 dloss:1.2790 exploreP:0.5812\n",
      "Episode:303 meanR:13.4800 R:13.0000 rate:0.0260 gloss:1.2787 dloss:1.2783 exploreP:0.5804\n",
      "Episode:304 meanR:13.5100 R:15.0000 rate:0.0300 gloss:1.2793 dloss:1.2792 exploreP:0.5796\n",
      "Episode:305 meanR:13.5100 R:11.0000 rate:0.0220 gloss:1.2795 dloss:1.2793 exploreP:0.5790\n",
      "Episode:306 meanR:13.5300 R:14.0000 rate:0.0280 gloss:1.2780 dloss:1.2777 exploreP:0.5782\n",
      "Episode:307 meanR:13.5100 R:16.0000 rate:0.0320 gloss:1.2788 dloss:1.2789 exploreP:0.5773\n",
      "Episode:308 meanR:13.4400 R:8.0000 rate:0.0160 gloss:1.2784 dloss:1.2783 exploreP:0.5768\n",
      "Episode:309 meanR:13.4200 R:14.0000 rate:0.0280 gloss:1.2789 dloss:1.2788 exploreP:0.5760\n",
      "Episode:310 meanR:13.4500 R:16.0000 rate:0.0320 gloss:1.2783 dloss:1.2782 exploreP:0.5751\n",
      "Episode:311 meanR:13.4600 R:11.0000 rate:0.0220 gloss:1.2792 dloss:1.2789 exploreP:0.5745\n",
      "Episode:312 meanR:13.4600 R:10.0000 rate:0.0200 gloss:1.2794 dloss:1.2792 exploreP:0.5739\n",
      "Episode:313 meanR:13.4000 R:10.0000 rate:0.0200 gloss:1.2796 dloss:1.2796 exploreP:0.5734\n",
      "Episode:314 meanR:13.3200 R:10.0000 rate:0.0200 gloss:1.2796 dloss:1.2798 exploreP:0.5728\n",
      "Episode:315 meanR:13.3600 R:17.0000 rate:0.0340 gloss:1.2796 dloss:1.2793 exploreP:0.5718\n",
      "Episode:316 meanR:13.3700 R:11.0000 rate:0.0220 gloss:1.2801 dloss:1.2801 exploreP:0.5712\n",
      "Episode:317 meanR:13.2900 R:9.0000 rate:0.0180 gloss:1.2798 dloss:1.2798 exploreP:0.5707\n",
      "Episode:318 meanR:13.3000 R:11.0000 rate:0.0220 gloss:1.2792 dloss:1.2791 exploreP:0.5701\n",
      "Episode:319 meanR:13.3900 R:21.0000 rate:0.0420 gloss:1.2800 dloss:1.2798 exploreP:0.5689\n",
      "Episode:320 meanR:13.4900 R:23.0000 rate:0.0460 gloss:1.2806 dloss:1.2804 exploreP:0.5676\n",
      "Episode:321 meanR:13.4500 R:9.0000 rate:0.0180 gloss:1.2806 dloss:1.2805 exploreP:0.5671\n",
      "Episode:322 meanR:13.4400 R:9.0000 rate:0.0180 gloss:1.2808 dloss:1.2807 exploreP:0.5666\n",
      "Episode:323 meanR:13.3800 R:12.0000 rate:0.0240 gloss:1.2809 dloss:1.2809 exploreP:0.5660\n",
      "Episode:324 meanR:13.4200 R:15.0000 rate:0.0300 gloss:1.2802 dloss:1.2801 exploreP:0.5651\n",
      "Episode:325 meanR:13.4500 R:15.0000 rate:0.0300 gloss:1.2816 dloss:1.2814 exploreP:0.5643\n",
      "Episode:326 meanR:13.4800 R:14.0000 rate:0.0280 gloss:1.2829 dloss:1.2827 exploreP:0.5635\n",
      "Episode:327 meanR:13.5300 R:14.0000 rate:0.0280 gloss:1.2817 dloss:1.2813 exploreP:0.5627\n",
      "Episode:328 meanR:13.5300 R:11.0000 rate:0.0220 gloss:1.2829 dloss:1.2826 exploreP:0.5621\n",
      "Episode:329 meanR:13.5100 R:11.0000 rate:0.0220 gloss:1.2819 dloss:1.2820 exploreP:0.5615\n",
      "Episode:330 meanR:13.5300 R:16.0000 rate:0.0320 gloss:1.2818 dloss:1.2817 exploreP:0.5607\n",
      "Episode:331 meanR:13.5500 R:17.0000 rate:0.0340 gloss:1.2820 dloss:1.2819 exploreP:0.5597\n",
      "Episode:332 meanR:13.5800 R:16.0000 rate:0.0320 gloss:1.2821 dloss:1.2820 exploreP:0.5588\n",
      "Episode:333 meanR:13.6100 R:16.0000 rate:0.0320 gloss:1.2818 dloss:1.2816 exploreP:0.5580\n",
      "Episode:334 meanR:13.7700 R:27.0000 rate:0.0540 gloss:1.2831 dloss:1.2831 exploreP:0.5565\n",
      "Episode:335 meanR:13.7400 R:11.0000 rate:0.0220 gloss:1.2822 dloss:1.2821 exploreP:0.5559\n",
      "Episode:336 meanR:13.7600 R:11.0000 rate:0.0220 gloss:1.2813 dloss:1.2815 exploreP:0.5553\n",
      "Episode:337 meanR:13.8400 R:17.0000 rate:0.0340 gloss:1.2824 dloss:1.2822 exploreP:0.5544\n",
      "Episode:338 meanR:13.8200 R:13.0000 rate:0.0260 gloss:1.2824 dloss:1.2823 exploreP:0.5536\n",
      "Episode:339 meanR:13.7100 R:12.0000 rate:0.0240 gloss:1.2831 dloss:1.2830 exploreP:0.5530\n",
      "Episode:340 meanR:13.7800 R:16.0000 rate:0.0320 gloss:1.2825 dloss:1.2823 exploreP:0.5521\n",
      "Episode:341 meanR:13.8100 R:14.0000 rate:0.0280 gloss:1.2821 dloss:1.2820 exploreP:0.5514\n",
      "Episode:342 meanR:13.8000 R:14.0000 rate:0.0280 gloss:1.2834 dloss:1.2833 exploreP:0.5506\n",
      "Episode:343 meanR:13.8100 R:13.0000 rate:0.0260 gloss:1.2846 dloss:1.2844 exploreP:0.5499\n",
      "Episode:344 meanR:13.8300 R:14.0000 rate:0.0280 gloss:1.2835 dloss:1.2834 exploreP:0.5492\n",
      "Episode:345 meanR:13.5200 R:15.0000 rate:0.0300 gloss:1.2842 dloss:1.2838 exploreP:0.5483\n",
      "Episode:346 meanR:13.5700 R:16.0000 rate:0.0320 gloss:1.2842 dloss:1.2842 exploreP:0.5475\n",
      "Episode:347 meanR:13.5000 R:11.0000 rate:0.0220 gloss:1.2847 dloss:1.2846 exploreP:0.5469\n",
      "Episode:348 meanR:13.5000 R:9.0000 rate:0.0180 gloss:1.2846 dloss:1.2842 exploreP:0.5464\n",
      "Episode:349 meanR:13.4300 R:14.0000 rate:0.0280 gloss:1.2863 dloss:1.2861 exploreP:0.5457\n",
      "Episode:350 meanR:13.4500 R:16.0000 rate:0.0320 gloss:1.2849 dloss:1.2849 exploreP:0.5448\n",
      "Episode:351 meanR:13.5000 R:14.0000 rate:0.0280 gloss:1.2841 dloss:1.2841 exploreP:0.5441\n",
      "Episode:352 meanR:13.4800 R:12.0000 rate:0.0240 gloss:1.2853 dloss:1.2850 exploreP:0.5434\n",
      "Episode:353 meanR:13.4900 R:11.0000 rate:0.0220 gloss:1.2863 dloss:1.2861 exploreP:0.5428\n",
      "Episode:354 meanR:13.5100 R:11.0000 rate:0.0220 gloss:1.2844 dloss:1.2844 exploreP:0.5422\n",
      "Episode:355 meanR:13.5400 R:13.0000 rate:0.0260 gloss:1.2854 dloss:1.2853 exploreP:0.5416\n",
      "Episode:356 meanR:13.5100 R:11.0000 rate:0.0220 gloss:1.2846 dloss:1.2845 exploreP:0.5410\n",
      "Episode:357 meanR:13.5300 R:11.0000 rate:0.0220 gloss:1.2855 dloss:1.2856 exploreP:0.5404\n",
      "Episode:358 meanR:13.5400 R:16.0000 rate:0.0320 gloss:1.2855 dloss:1.2853 exploreP:0.5395\n",
      "Episode:359 meanR:13.4700 R:20.0000 rate:0.0400 gloss:1.2845 dloss:1.2841 exploreP:0.5385\n",
      "Episode:360 meanR:13.4300 R:14.0000 rate:0.0280 gloss:1.2849 dloss:1.2849 exploreP:0.5377\n",
      "Episode:361 meanR:13.3600 R:11.0000 rate:0.0220 gloss:1.2857 dloss:1.2858 exploreP:0.5372\n",
      "Episode:362 meanR:13.2300 R:11.0000 rate:0.0220 gloss:1.2854 dloss:1.2855 exploreP:0.5366\n",
      "Episode:363 meanR:13.2400 R:12.0000 rate:0.0240 gloss:1.2855 dloss:1.2855 exploreP:0.5359\n",
      "Episode:364 meanR:13.2500 R:13.0000 rate:0.0260 gloss:1.2851 dloss:1.2852 exploreP:0.5353\n",
      "Episode:365 meanR:13.2400 R:12.0000 rate:0.0240 gloss:1.2851 dloss:1.2852 exploreP:0.5346\n",
      "Episode:366 meanR:13.2800 R:15.0000 rate:0.0300 gloss:1.2865 dloss:1.2867 exploreP:0.5338\n",
      "Episode:367 meanR:13.3200 R:14.0000 rate:0.0280 gloss:1.2863 dloss:1.2863 exploreP:0.5331\n",
      "Episode:368 meanR:13.3700 R:16.0000 rate:0.0320 gloss:1.2869 dloss:1.2868 exploreP:0.5323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:369 meanR:13.3400 R:9.0000 rate:0.0180 gloss:1.2877 dloss:1.2877 exploreP:0.5318\n",
      "Episode:370 meanR:13.3000 R:10.0000 rate:0.0200 gloss:1.2877 dloss:1.2875 exploreP:0.5313\n",
      "Episode:371 meanR:13.3000 R:12.0000 rate:0.0240 gloss:1.2870 dloss:1.2874 exploreP:0.5307\n",
      "Episode:372 meanR:13.3600 R:14.0000 rate:0.0280 gloss:1.2874 dloss:1.2871 exploreP:0.5299\n",
      "Episode:373 meanR:13.4000 R:15.0000 rate:0.0300 gloss:1.2877 dloss:1.2875 exploreP:0.5292\n",
      "Episode:374 meanR:13.3200 R:15.0000 rate:0.0300 gloss:1.2887 dloss:1.2887 exploreP:0.5284\n",
      "Episode:375 meanR:13.3200 R:9.0000 rate:0.0180 gloss:1.2884 dloss:1.2882 exploreP:0.5279\n",
      "Episode:376 meanR:13.3100 R:12.0000 rate:0.0240 gloss:1.2878 dloss:1.2877 exploreP:0.5273\n",
      "Episode:377 meanR:13.3200 R:10.0000 rate:0.0200 gloss:1.2873 dloss:1.2872 exploreP:0.5268\n",
      "Episode:378 meanR:13.4100 R:22.0000 rate:0.0440 gloss:1.2873 dloss:1.2872 exploreP:0.5256\n",
      "Episode:379 meanR:13.3300 R:9.0000 rate:0.0180 gloss:1.2867 dloss:1.2864 exploreP:0.5252\n",
      "Episode:380 meanR:13.3300 R:12.0000 rate:0.0240 gloss:1.2872 dloss:1.2871 exploreP:0.5246\n",
      "Episode:381 meanR:13.3300 R:12.0000 rate:0.0240 gloss:1.2879 dloss:1.2878 exploreP:0.5239\n",
      "Episode:382 meanR:13.3300 R:13.0000 rate:0.0260 gloss:1.2878 dloss:1.2880 exploreP:0.5233\n",
      "Episode:383 meanR:13.3300 R:11.0000 rate:0.0220 gloss:1.2886 dloss:1.2887 exploreP:0.5227\n",
      "Episode:384 meanR:13.3000 R:10.0000 rate:0.0200 gloss:1.2875 dloss:1.2876 exploreP:0.5222\n",
      "Episode:385 meanR:13.2700 R:13.0000 rate:0.0260 gloss:1.2874 dloss:1.2875 exploreP:0.5215\n",
      "Episode:386 meanR:13.3000 R:11.0000 rate:0.0220 gloss:1.2873 dloss:1.2875 exploreP:0.5210\n",
      "Episode:387 meanR:13.2900 R:15.0000 rate:0.0300 gloss:1.2882 dloss:1.2882 exploreP:0.5202\n",
      "Episode:388 meanR:13.3200 R:17.0000 rate:0.0340 gloss:1.2881 dloss:1.2881 exploreP:0.5193\n",
      "Episode:389 meanR:13.3300 R:12.0000 rate:0.0240 gloss:1.2885 dloss:1.2884 exploreP:0.5187\n",
      "Episode:390 meanR:13.2300 R:8.0000 rate:0.0160 gloss:1.2876 dloss:1.2875 exploreP:0.5183\n",
      "Episode:391 meanR:13.1700 R:15.0000 rate:0.0300 gloss:1.2886 dloss:1.2884 exploreP:0.5176\n",
      "Episode:392 meanR:13.2300 R:16.0000 rate:0.0320 gloss:1.2888 dloss:1.2886 exploreP:0.5167\n",
      "Episode:393 meanR:13.2200 R:9.0000 rate:0.0180 gloss:1.2888 dloss:1.2889 exploreP:0.5163\n",
      "Episode:394 meanR:13.2700 R:15.0000 rate:0.0300 gloss:1.2887 dloss:1.2886 exploreP:0.5155\n",
      "Episode:395 meanR:13.2800 R:12.0000 rate:0.0240 gloss:1.2890 dloss:1.2889 exploreP:0.5149\n",
      "Episode:396 meanR:13.2900 R:15.0000 rate:0.0300 gloss:1.2885 dloss:1.2884 exploreP:0.5142\n",
      "Episode:397 meanR:13.2500 R:11.0000 rate:0.0220 gloss:1.2883 dloss:1.2883 exploreP:0.5136\n",
      "Episode:398 meanR:13.2700 R:12.0000 rate:0.0240 gloss:1.2882 dloss:1.2885 exploreP:0.5130\n",
      "Episode:399 meanR:13.2900 R:12.0000 rate:0.0240 gloss:1.2890 dloss:1.2888 exploreP:0.5124\n",
      "Episode:400 meanR:13.3500 R:16.0000 rate:0.0320 gloss:1.2890 dloss:1.2889 exploreP:0.5116\n",
      "Episode:401 meanR:13.3400 R:14.0000 rate:0.0280 gloss:1.2879 dloss:1.2878 exploreP:0.5109\n",
      "Episode:402 meanR:13.2400 R:11.0000 rate:0.0220 gloss:1.2892 dloss:1.2893 exploreP:0.5103\n",
      "Episode:403 meanR:13.2300 R:12.0000 rate:0.0240 gloss:1.2881 dloss:1.2879 exploreP:0.5097\n",
      "Episode:404 meanR:13.2400 R:16.0000 rate:0.0320 gloss:1.2878 dloss:1.2879 exploreP:0.5089\n",
      "Episode:405 meanR:13.2700 R:14.0000 rate:0.0280 gloss:1.2882 dloss:1.2884 exploreP:0.5083\n",
      "Episode:406 meanR:13.2400 R:11.0000 rate:0.0220 gloss:1.2887 dloss:1.2885 exploreP:0.5077\n",
      "Episode:407 meanR:13.2500 R:17.0000 rate:0.0340 gloss:1.2883 dloss:1.2883 exploreP:0.5069\n",
      "Episode:408 meanR:13.2600 R:9.0000 rate:0.0180 gloss:1.2882 dloss:1.2883 exploreP:0.5064\n",
      "Episode:409 meanR:13.2800 R:16.0000 rate:0.0320 gloss:1.2881 dloss:1.2882 exploreP:0.5056\n",
      "Episode:410 meanR:13.2400 R:12.0000 rate:0.0240 gloss:1.2887 dloss:1.2886 exploreP:0.5050\n",
      "Episode:411 meanR:13.2600 R:13.0000 rate:0.0260 gloss:1.2884 dloss:1.2886 exploreP:0.5044\n",
      "Episode:412 meanR:13.3000 R:14.0000 rate:0.0280 gloss:1.2892 dloss:1.2893 exploreP:0.5037\n",
      "Episode:413 meanR:13.3200 R:12.0000 rate:0.0240 gloss:1.2887 dloss:1.2886 exploreP:0.5031\n",
      "Episode:414 meanR:13.3200 R:10.0000 rate:0.0200 gloss:1.2886 dloss:1.2886 exploreP:0.5026\n",
      "Episode:415 meanR:13.3000 R:15.0000 rate:0.0300 gloss:1.2889 dloss:1.2890 exploreP:0.5019\n",
      "Episode:416 meanR:13.2800 R:9.0000 rate:0.0180 gloss:1.2898 dloss:1.2901 exploreP:0.5014\n",
      "Episode:417 meanR:13.3200 R:13.0000 rate:0.0260 gloss:1.2899 dloss:1.2899 exploreP:0.5008\n",
      "Episode:418 meanR:13.3300 R:12.0000 rate:0.0240 gloss:1.2889 dloss:1.2888 exploreP:0.5002\n",
      "Episode:419 meanR:13.2300 R:11.0000 rate:0.0220 gloss:1.2896 dloss:1.2896 exploreP:0.4997\n",
      "Episode:420 meanR:13.1000 R:10.0000 rate:0.0200 gloss:1.2905 dloss:1.2905 exploreP:0.4992\n",
      "Episode:421 meanR:13.1200 R:11.0000 rate:0.0220 gloss:1.2900 dloss:1.2898 exploreP:0.4986\n",
      "Episode:422 meanR:13.1800 R:15.0000 rate:0.0300 gloss:1.2895 dloss:1.2897 exploreP:0.4979\n",
      "Episode:423 meanR:13.2200 R:16.0000 rate:0.0320 gloss:1.2904 dloss:1.2904 exploreP:0.4971\n",
      "Episode:424 meanR:13.2000 R:13.0000 rate:0.0260 gloss:1.2906 dloss:1.2907 exploreP:0.4965\n",
      "Episode:425 meanR:13.2100 R:16.0000 rate:0.0320 gloss:1.2907 dloss:1.2907 exploreP:0.4957\n",
      "Episode:426 meanR:13.2600 R:19.0000 rate:0.0380 gloss:1.2897 dloss:1.2897 exploreP:0.4948\n",
      "Episode:427 meanR:13.2900 R:17.0000 rate:0.0340 gloss:1.2897 dloss:1.2896 exploreP:0.4940\n",
      "Episode:428 meanR:13.2900 R:11.0000 rate:0.0220 gloss:1.2905 dloss:1.2907 exploreP:0.4934\n",
      "Episode:429 meanR:13.2900 R:11.0000 rate:0.0220 gloss:1.2903 dloss:1.2902 exploreP:0.4929\n",
      "Episode:430 meanR:13.2800 R:15.0000 rate:0.0300 gloss:1.2911 dloss:1.2913 exploreP:0.4922\n",
      "Episode:431 meanR:13.2300 R:12.0000 rate:0.0240 gloss:1.2908 dloss:1.2908 exploreP:0.4916\n",
      "Episode:432 meanR:13.2300 R:16.0000 rate:0.0320 gloss:1.2919 dloss:1.2917 exploreP:0.4908\n",
      "Episode:433 meanR:13.1700 R:10.0000 rate:0.0200 gloss:1.2906 dloss:1.2905 exploreP:0.4903\n",
      "Episode:434 meanR:13.0000 R:10.0000 rate:0.0200 gloss:1.2920 dloss:1.2916 exploreP:0.4899\n",
      "Episode:435 meanR:13.0000 R:11.0000 rate:0.0220 gloss:1.2909 dloss:1.2908 exploreP:0.4893\n",
      "Episode:436 meanR:12.9900 R:10.0000 rate:0.0200 gloss:1.2914 dloss:1.2914 exploreP:0.4889\n",
      "Episode:437 meanR:12.9900 R:17.0000 rate:0.0340 gloss:1.2909 dloss:1.2908 exploreP:0.4880\n",
      "Episode:438 meanR:13.0400 R:18.0000 rate:0.0360 gloss:1.2905 dloss:1.2903 exploreP:0.4872\n",
      "Episode:439 meanR:13.0600 R:14.0000 rate:0.0280 gloss:1.2910 dloss:1.2912 exploreP:0.4865\n",
      "Episode:440 meanR:13.0300 R:13.0000 rate:0.0260 gloss:1.2896 dloss:1.2897 exploreP:0.4859\n",
      "Episode:441 meanR:13.0000 R:11.0000 rate:0.0220 gloss:1.2901 dloss:1.2904 exploreP:0.4854\n",
      "Episode:442 meanR:13.0900 R:23.0000 rate:0.0460 gloss:1.2905 dloss:1.2907 exploreP:0.4843\n",
      "Episode:443 meanR:13.0800 R:12.0000 rate:0.0240 gloss:1.2917 dloss:1.2917 exploreP:0.4837\n",
      "Episode:444 meanR:13.0400 R:10.0000 rate:0.0200 gloss:1.2914 dloss:1.2915 exploreP:0.4832\n",
      "Episode:445 meanR:13.0000 R:11.0000 rate:0.0220 gloss:1.2912 dloss:1.2910 exploreP:0.4827\n",
      "Episode:446 meanR:12.9300 R:9.0000 rate:0.0180 gloss:1.2906 dloss:1.2905 exploreP:0.4823\n",
      "Episode:447 meanR:12.9200 R:10.0000 rate:0.0200 gloss:1.2906 dloss:1.2908 exploreP:0.4818\n",
      "Episode:448 meanR:12.9300 R:10.0000 rate:0.0200 gloss:1.2910 dloss:1.2908 exploreP:0.4814\n",
      "Episode:449 meanR:12.9000 R:11.0000 rate:0.0220 gloss:1.2911 dloss:1.2910 exploreP:0.4808\n",
      "Episode:450 meanR:12.8400 R:10.0000 rate:0.0200 gloss:1.2907 dloss:1.2907 exploreP:0.4804\n",
      "Episode:451 meanR:12.8100 R:11.0000 rate:0.0220 gloss:1.2908 dloss:1.2910 exploreP:0.4798\n",
      "Episode:452 meanR:12.8000 R:11.0000 rate:0.0220 gloss:1.2906 dloss:1.2905 exploreP:0.4793\n",
      "Episode:453 meanR:12.8100 R:12.0000 rate:0.0240 gloss:1.2905 dloss:1.2907 exploreP:0.4788\n",
      "Episode:454 meanR:12.8400 R:14.0000 rate:0.0280 gloss:1.2901 dloss:1.2902 exploreP:0.4781\n",
      "Episode:455 meanR:12.8000 R:9.0000 rate:0.0180 gloss:1.2898 dloss:1.2903 exploreP:0.4777\n",
      "Episode:456 meanR:12.7900 R:10.0000 rate:0.0200 gloss:1.2908 dloss:1.2913 exploreP:0.4772\n",
      "Episode:457 meanR:12.7900 R:11.0000 rate:0.0220 gloss:1.2908 dloss:1.2911 exploreP:0.4767\n",
      "Episode:458 meanR:12.8900 R:26.0000 rate:0.0520 gloss:1.2918 dloss:1.2920 exploreP:0.4755\n",
      "Episode:459 meanR:12.8000 R:11.0000 rate:0.0220 gloss:1.2923 dloss:1.2923 exploreP:0.4750\n",
      "Episode:460 meanR:12.7900 R:13.0000 rate:0.0260 gloss:1.2913 dloss:1.2914 exploreP:0.4744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:461 meanR:12.8400 R:16.0000 rate:0.0320 gloss:1.2926 dloss:1.2925 exploreP:0.4736\n",
      "Episode:462 meanR:12.8300 R:10.0000 rate:0.0200 gloss:1.2924 dloss:1.2924 exploreP:0.4732\n",
      "Episode:463 meanR:12.8500 R:14.0000 rate:0.0280 gloss:1.2912 dloss:1.2913 exploreP:0.4725\n",
      "Episode:464 meanR:12.8500 R:13.0000 rate:0.0260 gloss:1.2917 dloss:1.2918 exploreP:0.4719\n",
      "Episode:465 meanR:12.8300 R:10.0000 rate:0.0200 gloss:1.2933 dloss:1.2932 exploreP:0.4715\n",
      "Episode:466 meanR:12.7800 R:10.0000 rate:0.0200 gloss:1.2920 dloss:1.2921 exploreP:0.4710\n",
      "Episode:467 meanR:12.7400 R:10.0000 rate:0.0200 gloss:1.2932 dloss:1.2932 exploreP:0.4705\n",
      "Episode:468 meanR:12.6900 R:11.0000 rate:0.0220 gloss:1.2919 dloss:1.2919 exploreP:0.4700\n",
      "Episode:469 meanR:12.7600 R:16.0000 rate:0.0320 gloss:1.2931 dloss:1.2933 exploreP:0.4693\n",
      "Episode:470 meanR:12.7700 R:11.0000 rate:0.0220 gloss:1.2924 dloss:1.2923 exploreP:0.4688\n",
      "Episode:471 meanR:12.7500 R:10.0000 rate:0.0200 gloss:1.2930 dloss:1.2933 exploreP:0.4683\n",
      "Episode:472 meanR:12.7900 R:18.0000 rate:0.0360 gloss:1.2936 dloss:1.2937 exploreP:0.4675\n",
      "Episode:473 meanR:12.7200 R:8.0000 rate:0.0160 gloss:1.2941 dloss:1.2941 exploreP:0.4671\n",
      "Episode:474 meanR:12.6900 R:12.0000 rate:0.0240 gloss:1.2936 dloss:1.2938 exploreP:0.4666\n",
      "Episode:475 meanR:12.7300 R:13.0000 rate:0.0260 gloss:1.2934 dloss:1.2934 exploreP:0.4660\n",
      "Episode:476 meanR:12.7200 R:11.0000 rate:0.0220 gloss:1.2930 dloss:1.2929 exploreP:0.4655\n",
      "Episode:477 meanR:12.7500 R:13.0000 rate:0.0260 gloss:1.2939 dloss:1.2939 exploreP:0.4649\n",
      "Episode:478 meanR:12.6200 R:9.0000 rate:0.0180 gloss:1.2929 dloss:1.2930 exploreP:0.4645\n",
      "Episode:479 meanR:12.6500 R:12.0000 rate:0.0240 gloss:1.2944 dloss:1.2944 exploreP:0.4640\n",
      "Episode:480 meanR:12.6800 R:15.0000 rate:0.0300 gloss:1.2927 dloss:1.2929 exploreP:0.4633\n",
      "Episode:481 meanR:12.7300 R:17.0000 rate:0.0340 gloss:1.2930 dloss:1.2931 exploreP:0.4625\n",
      "Episode:482 meanR:12.7500 R:15.0000 rate:0.0300 gloss:1.2928 dloss:1.2930 exploreP:0.4618\n",
      "Episode:483 meanR:12.7400 R:10.0000 rate:0.0200 gloss:1.2931 dloss:1.2932 exploreP:0.4614\n",
      "Episode:484 meanR:12.7300 R:9.0000 rate:0.0180 gloss:1.2939 dloss:1.2940 exploreP:0.4610\n",
      "Episode:485 meanR:12.7800 R:18.0000 rate:0.0360 gloss:1.2935 dloss:1.2934 exploreP:0.4602\n",
      "Episode:486 meanR:12.7700 R:10.0000 rate:0.0200 gloss:1.2935 dloss:1.2935 exploreP:0.4597\n",
      "Episode:487 meanR:12.7000 R:8.0000 rate:0.0160 gloss:1.2942 dloss:1.2945 exploreP:0.4594\n",
      "Episode:488 meanR:12.6300 R:10.0000 rate:0.0200 gloss:1.2930 dloss:1.2931 exploreP:0.4589\n",
      "Episode:489 meanR:12.6500 R:14.0000 rate:0.0280 gloss:1.2933 dloss:1.2934 exploreP:0.4583\n",
      "Episode:490 meanR:12.6800 R:11.0000 rate:0.0220 gloss:1.2927 dloss:1.2930 exploreP:0.4578\n",
      "Episode:491 meanR:12.6600 R:13.0000 rate:0.0260 gloss:1.2921 dloss:1.2922 exploreP:0.4572\n",
      "Episode:492 meanR:12.5900 R:9.0000 rate:0.0180 gloss:1.2923 dloss:1.2925 exploreP:0.4568\n",
      "Episode:493 meanR:12.6400 R:14.0000 rate:0.0280 gloss:1.2924 dloss:1.2923 exploreP:0.4562\n",
      "Episode:494 meanR:12.6200 R:13.0000 rate:0.0260 gloss:1.2924 dloss:1.2926 exploreP:0.4556\n",
      "Episode:495 meanR:12.6000 R:10.0000 rate:0.0200 gloss:1.2928 dloss:1.2929 exploreP:0.4551\n",
      "Episode:496 meanR:12.5600 R:11.0000 rate:0.0220 gloss:1.2929 dloss:1.2928 exploreP:0.4547\n",
      "Episode:497 meanR:12.5700 R:12.0000 rate:0.0240 gloss:1.2926 dloss:1.2927 exploreP:0.4541\n",
      "Episode:498 meanR:12.5500 R:10.0000 rate:0.0200 gloss:1.2927 dloss:1.2929 exploreP:0.4537\n",
      "Episode:499 meanR:12.5500 R:12.0000 rate:0.0240 gloss:1.2935 dloss:1.2936 exploreP:0.4531\n",
      "Episode:500 meanR:12.4900 R:10.0000 rate:0.0200 gloss:1.2925 dloss:1.2926 exploreP:0.4527\n",
      "Episode:501 meanR:12.4800 R:13.0000 rate:0.0260 gloss:1.2919 dloss:1.2916 exploreP:0.4521\n",
      "Episode:502 meanR:12.4700 R:10.0000 rate:0.0200 gloss:1.2925 dloss:1.2926 exploreP:0.4517\n",
      "Episode:503 meanR:12.4600 R:11.0000 rate:0.0220 gloss:1.2923 dloss:1.2925 exploreP:0.4512\n",
      "Episode:504 meanR:12.3900 R:9.0000 rate:0.0180 gloss:1.2928 dloss:1.2930 exploreP:0.4508\n",
      "Episode:505 meanR:12.3700 R:12.0000 rate:0.0240 gloss:1.2936 dloss:1.2938 exploreP:0.4503\n",
      "Episode:506 meanR:12.4400 R:18.0000 rate:0.0360 gloss:1.2942 dloss:1.2943 exploreP:0.4495\n",
      "Episode:507 meanR:12.3800 R:11.0000 rate:0.0220 gloss:1.2940 dloss:1.2942 exploreP:0.4490\n",
      "Episode:508 meanR:12.4100 R:12.0000 rate:0.0240 gloss:1.2934 dloss:1.2935 exploreP:0.4485\n",
      "Episode:509 meanR:12.3400 R:9.0000 rate:0.0180 gloss:1.2926 dloss:1.2926 exploreP:0.4481\n",
      "Episode:510 meanR:12.3300 R:11.0000 rate:0.0220 gloss:1.2928 dloss:1.2928 exploreP:0.4476\n",
      "Episode:511 meanR:12.3100 R:11.0000 rate:0.0220 gloss:1.2923 dloss:1.2925 exploreP:0.4471\n",
      "Episode:512 meanR:12.2700 R:10.0000 rate:0.0200 gloss:1.2916 dloss:1.2917 exploreP:0.4467\n",
      "Episode:513 meanR:12.3000 R:15.0000 rate:0.0300 gloss:1.2920 dloss:1.2922 exploreP:0.4460\n",
      "Episode:514 meanR:12.3400 R:14.0000 rate:0.0280 gloss:1.2920 dloss:1.2923 exploreP:0.4454\n",
      "Episode:515 meanR:12.3000 R:11.0000 rate:0.0220 gloss:1.2929 dloss:1.2929 exploreP:0.4449\n",
      "Episode:516 meanR:12.3200 R:11.0000 rate:0.0220 gloss:1.2931 dloss:1.2934 exploreP:0.4445\n",
      "Episode:517 meanR:12.3400 R:15.0000 rate:0.0300 gloss:1.2932 dloss:1.2935 exploreP:0.4438\n",
      "Episode:518 meanR:12.3300 R:11.0000 rate:0.0220 gloss:1.2924 dloss:1.2928 exploreP:0.4433\n",
      "Episode:519 meanR:12.3400 R:12.0000 rate:0.0240 gloss:1.2939 dloss:1.2940 exploreP:0.4428\n",
      "Episode:520 meanR:12.3600 R:12.0000 rate:0.0240 gloss:1.2937 dloss:1.2938 exploreP:0.4423\n",
      "Episode:521 meanR:12.3700 R:12.0000 rate:0.0240 gloss:1.2937 dloss:1.2938 exploreP:0.4418\n",
      "Episode:522 meanR:12.3300 R:11.0000 rate:0.0220 gloss:1.2942 dloss:1.2944 exploreP:0.4413\n",
      "Episode:523 meanR:12.2600 R:9.0000 rate:0.0180 gloss:1.2950 dloss:1.2949 exploreP:0.4409\n",
      "Episode:524 meanR:12.2100 R:8.0000 rate:0.0160 gloss:1.2947 dloss:1.2951 exploreP:0.4406\n",
      "Episode:525 meanR:12.1400 R:9.0000 rate:0.0180 gloss:1.2933 dloss:1.2933 exploreP:0.4402\n",
      "Episode:526 meanR:12.0900 R:14.0000 rate:0.0280 gloss:1.2935 dloss:1.2935 exploreP:0.4396\n",
      "Episode:527 meanR:12.0400 R:12.0000 rate:0.0240 gloss:1.2935 dloss:1.2939 exploreP:0.4391\n",
      "Episode:528 meanR:12.0200 R:9.0000 rate:0.0180 gloss:1.2935 dloss:1.2935 exploreP:0.4387\n",
      "Episode:529 meanR:12.0300 R:12.0000 rate:0.0240 gloss:1.2940 dloss:1.2939 exploreP:0.4382\n",
      "Episode:530 meanR:11.9900 R:11.0000 rate:0.0220 gloss:1.2929 dloss:1.2930 exploreP:0.4377\n",
      "Episode:531 meanR:12.0400 R:17.0000 rate:0.0340 gloss:1.2940 dloss:1.2944 exploreP:0.4370\n",
      "Episode:532 meanR:12.0000 R:12.0000 rate:0.0240 gloss:1.2938 dloss:1.2939 exploreP:0.4365\n",
      "Episode:533 meanR:12.0400 R:14.0000 rate:0.0280 gloss:1.2942 dloss:1.2944 exploreP:0.4359\n",
      "Episode:534 meanR:12.0400 R:10.0000 rate:0.0200 gloss:1.2936 dloss:1.2940 exploreP:0.4354\n",
      "Episode:535 meanR:12.0400 R:11.0000 rate:0.0220 gloss:1.2936 dloss:1.2939 exploreP:0.4350\n",
      "Episode:536 meanR:12.0300 R:9.0000 rate:0.0180 gloss:1.2938 dloss:1.2939 exploreP:0.4346\n",
      "Episode:537 meanR:11.9600 R:10.0000 rate:0.0200 gloss:1.2938 dloss:1.2938 exploreP:0.4342\n",
      "Episode:538 meanR:11.8700 R:9.0000 rate:0.0180 gloss:1.2931 dloss:1.2935 exploreP:0.4338\n",
      "Episode:539 meanR:11.8500 R:12.0000 rate:0.0240 gloss:1.2937 dloss:1.2937 exploreP:0.4333\n",
      "Episode:540 meanR:11.8400 R:12.0000 rate:0.0240 gloss:1.2937 dloss:1.2937 exploreP:0.4328\n",
      "Episode:541 meanR:11.8300 R:10.0000 rate:0.0200 gloss:1.2928 dloss:1.2930 exploreP:0.4323\n",
      "Episode:542 meanR:11.7100 R:11.0000 rate:0.0220 gloss:1.2933 dloss:1.2937 exploreP:0.4319\n",
      "Episode:543 meanR:11.6800 R:9.0000 rate:0.0180 gloss:1.2930 dloss:1.2935 exploreP:0.4315\n",
      "Episode:544 meanR:11.6700 R:9.0000 rate:0.0180 gloss:1.2928 dloss:1.2931 exploreP:0.4311\n",
      "Episode:545 meanR:11.6700 R:11.0000 rate:0.0220 gloss:1.2932 dloss:1.2934 exploreP:0.4307\n",
      "Episode:546 meanR:11.7100 R:13.0000 rate:0.0260 gloss:1.2930 dloss:1.2931 exploreP:0.4301\n",
      "Episode:547 meanR:11.7700 R:16.0000 rate:0.0320 gloss:1.2936 dloss:1.2940 exploreP:0.4294\n",
      "Episode:548 meanR:11.8100 R:14.0000 rate:0.0280 gloss:1.2933 dloss:1.2936 exploreP:0.4288\n",
      "Episode:549 meanR:11.8100 R:11.0000 rate:0.0220 gloss:1.2932 dloss:1.2934 exploreP:0.4284\n",
      "Episode:550 meanR:11.8000 R:9.0000 rate:0.0180 gloss:1.2934 dloss:1.2936 exploreP:0.4280\n",
      "Episode:551 meanR:11.8100 R:12.0000 rate:0.0240 gloss:1.2929 dloss:1.2931 exploreP:0.4275\n",
      "Episode:552 meanR:11.8300 R:13.0000 rate:0.0260 gloss:1.2923 dloss:1.2924 exploreP:0.4270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:553 meanR:11.8000 R:9.0000 rate:0.0180 gloss:1.2918 dloss:1.2921 exploreP:0.4266\n",
      "Episode:554 meanR:11.8600 R:20.0000 rate:0.0400 gloss:1.2926 dloss:1.2927 exploreP:0.4258\n",
      "Episode:555 meanR:11.8900 R:12.0000 rate:0.0240 gloss:1.2940 dloss:1.2943 exploreP:0.4253\n",
      "Episode:556 meanR:11.9000 R:11.0000 rate:0.0220 gloss:1.2930 dloss:1.2934 exploreP:0.4248\n",
      "Episode:557 meanR:11.9300 R:14.0000 rate:0.0280 gloss:1.2927 dloss:1.2929 exploreP:0.4242\n",
      "Episode:558 meanR:11.8000 R:13.0000 rate:0.0260 gloss:1.2928 dloss:1.2931 exploreP:0.4237\n",
      "Episode:559 meanR:11.7900 R:10.0000 rate:0.0200 gloss:1.2939 dloss:1.2941 exploreP:0.4233\n",
      "Episode:560 meanR:11.7800 R:12.0000 rate:0.0240 gloss:1.2942 dloss:1.2946 exploreP:0.4228\n",
      "Episode:561 meanR:11.8200 R:20.0000 rate:0.0400 gloss:1.2944 dloss:1.2946 exploreP:0.4220\n",
      "Episode:562 meanR:11.8200 R:10.0000 rate:0.0200 gloss:1.2949 dloss:1.2951 exploreP:0.4215\n",
      "Episode:563 meanR:11.7900 R:11.0000 rate:0.0220 gloss:1.2949 dloss:1.2950 exploreP:0.4211\n",
      "Episode:564 meanR:11.8000 R:14.0000 rate:0.0280 gloss:1.2943 dloss:1.2944 exploreP:0.4205\n",
      "Episode:565 meanR:11.8100 R:11.0000 rate:0.0220 gloss:1.2953 dloss:1.2955 exploreP:0.4201\n",
      "Episode:566 meanR:11.8200 R:11.0000 rate:0.0220 gloss:1.2942 dloss:1.2943 exploreP:0.4196\n",
      "Episode:567 meanR:11.8100 R:9.0000 rate:0.0180 gloss:1.2943 dloss:1.2945 exploreP:0.4192\n",
      "Episode:568 meanR:11.8500 R:15.0000 rate:0.0300 gloss:1.2938 dloss:1.2940 exploreP:0.4186\n",
      "Episode:569 meanR:11.8300 R:14.0000 rate:0.0280 gloss:1.2930 dloss:1.2932 exploreP:0.4181\n",
      "Episode:570 meanR:11.8600 R:14.0000 rate:0.0280 gloss:1.2931 dloss:1.2931 exploreP:0.4175\n",
      "Episode:571 meanR:11.8600 R:10.0000 rate:0.0200 gloss:1.2931 dloss:1.2933 exploreP:0.4171\n",
      "Episode:572 meanR:11.7800 R:10.0000 rate:0.0200 gloss:1.2928 dloss:1.2929 exploreP:0.4167\n",
      "Episode:573 meanR:11.7900 R:9.0000 rate:0.0180 gloss:1.2940 dloss:1.2943 exploreP:0.4163\n",
      "Episode:574 meanR:11.7600 R:9.0000 rate:0.0180 gloss:1.2932 dloss:1.2937 exploreP:0.4159\n",
      "Episode:575 meanR:11.7500 R:12.0000 rate:0.0240 gloss:1.2933 dloss:1.2937 exploreP:0.4155\n",
      "Episode:576 meanR:11.8300 R:19.0000 rate:0.0380 gloss:1.2941 dloss:1.2946 exploreP:0.4147\n",
      "Episode:577 meanR:11.8400 R:14.0000 rate:0.0280 gloss:1.2945 dloss:1.2948 exploreP:0.4141\n",
      "Episode:578 meanR:11.8300 R:8.0000 rate:0.0160 gloss:1.2940 dloss:1.2940 exploreP:0.4138\n",
      "Episode:579 meanR:11.8300 R:12.0000 rate:0.0240 gloss:1.2950 dloss:1.2952 exploreP:0.4133\n",
      "Episode:580 meanR:11.8300 R:15.0000 rate:0.0300 gloss:1.2943 dloss:1.2947 exploreP:0.4127\n",
      "Episode:581 meanR:11.7600 R:10.0000 rate:0.0200 gloss:1.2927 dloss:1.2930 exploreP:0.4123\n",
      "Episode:582 meanR:11.7000 R:9.0000 rate:0.0180 gloss:1.2936 dloss:1.2939 exploreP:0.4119\n",
      "Episode:583 meanR:11.6900 R:9.0000 rate:0.0180 gloss:1.2950 dloss:1.2954 exploreP:0.4116\n",
      "Episode:584 meanR:11.6900 R:9.0000 rate:0.0180 gloss:1.2943 dloss:1.2947 exploreP:0.4112\n",
      "Episode:585 meanR:11.7000 R:19.0000 rate:0.0380 gloss:1.2948 dloss:1.2950 exploreP:0.4105\n",
      "Episode:586 meanR:11.7500 R:15.0000 rate:0.0300 gloss:1.2947 dloss:1.2949 exploreP:0.4099\n",
      "Episode:587 meanR:11.8200 R:15.0000 rate:0.0300 gloss:1.2954 dloss:1.2955 exploreP:0.4093\n",
      "Episode:588 meanR:11.8300 R:11.0000 rate:0.0220 gloss:1.2946 dloss:1.2947 exploreP:0.4088\n",
      "Episode:589 meanR:11.8100 R:12.0000 rate:0.0240 gloss:1.2940 dloss:1.2942 exploreP:0.4083\n",
      "Episode:590 meanR:11.8200 R:12.0000 rate:0.0240 gloss:1.2949 dloss:1.2951 exploreP:0.4079\n",
      "Episode:591 meanR:11.7900 R:10.0000 rate:0.0200 gloss:1.2936 dloss:1.2939 exploreP:0.4075\n",
      "Episode:592 meanR:11.8100 R:11.0000 rate:0.0220 gloss:1.2945 dloss:1.2948 exploreP:0.4070\n",
      "Episode:593 meanR:11.7900 R:12.0000 rate:0.0240 gloss:1.2947 dloss:1.2950 exploreP:0.4066\n",
      "Episode:594 meanR:11.7800 R:12.0000 rate:0.0240 gloss:1.2948 dloss:1.2951 exploreP:0.4061\n",
      "Episode:595 meanR:11.8100 R:13.0000 rate:0.0260 gloss:1.2950 dloss:1.2954 exploreP:0.4056\n",
      "Episode:596 meanR:11.8000 R:10.0000 rate:0.0200 gloss:1.2943 dloss:1.2943 exploreP:0.4052\n",
      "Episode:597 meanR:11.7800 R:10.0000 rate:0.0200 gloss:1.2954 dloss:1.2957 exploreP:0.4048\n",
      "Episode:598 meanR:11.8200 R:14.0000 rate:0.0280 gloss:1.2949 dloss:1.2954 exploreP:0.4042\n",
      "Episode:599 meanR:11.8400 R:14.0000 rate:0.0280 gloss:1.2942 dloss:1.2945 exploreP:0.4037\n",
      "Episode:600 meanR:11.8500 R:11.0000 rate:0.0220 gloss:1.2949 dloss:1.2952 exploreP:0.4032\n",
      "Episode:601 meanR:11.9100 R:19.0000 rate:0.0380 gloss:1.2957 dloss:1.2959 exploreP:0.4025\n",
      "Episode:602 meanR:11.9300 R:12.0000 rate:0.0240 gloss:1.2961 dloss:1.2962 exploreP:0.4020\n",
      "Episode:603 meanR:11.9300 R:11.0000 rate:0.0220 gloss:1.2949 dloss:1.2950 exploreP:0.4016\n",
      "Episode:604 meanR:11.9800 R:14.0000 rate:0.0280 gloss:1.2954 dloss:1.2957 exploreP:0.4010\n",
      "Episode:605 meanR:11.9600 R:10.0000 rate:0.0200 gloss:1.2953 dloss:1.2954 exploreP:0.4006\n",
      "Episode:606 meanR:11.9100 R:13.0000 rate:0.0260 gloss:1.2948 dloss:1.2950 exploreP:0.4001\n",
      "Episode:607 meanR:11.9500 R:15.0000 rate:0.0300 gloss:1.2947 dloss:1.2952 exploreP:0.3996\n",
      "Episode:608 meanR:11.9300 R:10.0000 rate:0.0200 gloss:1.2950 dloss:1.2954 exploreP:0.3992\n",
      "Episode:609 meanR:11.9600 R:12.0000 rate:0.0240 gloss:1.2960 dloss:1.2965 exploreP:0.3987\n",
      "Episode:610 meanR:11.9700 R:12.0000 rate:0.0240 gloss:1.2961 dloss:1.2963 exploreP:0.3982\n",
      "Episode:611 meanR:11.9800 R:12.0000 rate:0.0240 gloss:1.2956 dloss:1.2960 exploreP:0.3978\n",
      "Episode:612 meanR:12.0200 R:14.0000 rate:0.0280 gloss:1.2966 dloss:1.2969 exploreP:0.3972\n",
      "Episode:613 meanR:11.9800 R:11.0000 rate:0.0220 gloss:1.2960 dloss:1.2962 exploreP:0.3968\n",
      "Episode:614 meanR:12.0000 R:16.0000 rate:0.0320 gloss:1.2950 dloss:1.2952 exploreP:0.3962\n",
      "Episode:615 meanR:11.9800 R:9.0000 rate:0.0180 gloss:1.2954 dloss:1.2956 exploreP:0.3958\n",
      "Episode:616 meanR:12.0500 R:18.0000 rate:0.0360 gloss:1.2954 dloss:1.2959 exploreP:0.3951\n",
      "Episode:617 meanR:12.0000 R:10.0000 rate:0.0200 gloss:1.2958 dloss:1.2960 exploreP:0.3948\n",
      "Episode:618 meanR:12.0200 R:13.0000 rate:0.0260 gloss:1.2966 dloss:1.2969 exploreP:0.3943\n",
      "Episode:619 meanR:12.0200 R:12.0000 rate:0.0240 gloss:1.2964 dloss:1.2969 exploreP:0.3938\n",
      "Episode:620 meanR:12.0000 R:10.0000 rate:0.0200 gloss:1.2959 dloss:1.2961 exploreP:0.3934\n",
      "Episode:621 meanR:12.0000 R:12.0000 rate:0.0240 gloss:1.2958 dloss:1.2961 exploreP:0.3930\n",
      "Episode:622 meanR:11.9900 R:10.0000 rate:0.0200 gloss:1.2956 dloss:1.2960 exploreP:0.3926\n",
      "Episode:623 meanR:12.0400 R:14.0000 rate:0.0280 gloss:1.2959 dloss:1.2963 exploreP:0.3920\n",
      "Episode:624 meanR:12.0600 R:10.0000 rate:0.0200 gloss:1.2958 dloss:1.2959 exploreP:0.3917\n",
      "Episode:625 meanR:12.0900 R:12.0000 rate:0.0240 gloss:1.2962 dloss:1.2964 exploreP:0.3912\n",
      "Episode:626 meanR:12.0500 R:10.0000 rate:0.0200 gloss:1.2959 dloss:1.2962 exploreP:0.3908\n",
      "Episode:627 meanR:12.0900 R:16.0000 rate:0.0320 gloss:1.2952 dloss:1.2956 exploreP:0.3902\n",
      "Episode:628 meanR:12.1300 R:13.0000 rate:0.0260 gloss:1.2949 dloss:1.2952 exploreP:0.3897\n",
      "Episode:629 meanR:12.1500 R:14.0000 rate:0.0280 gloss:1.2954 dloss:1.2958 exploreP:0.3892\n",
      "Episode:630 meanR:12.1400 R:10.0000 rate:0.0200 gloss:1.2942 dloss:1.2944 exploreP:0.3888\n",
      "Episode:631 meanR:12.0800 R:11.0000 rate:0.0220 gloss:1.2957 dloss:1.2962 exploreP:0.3884\n",
      "Episode:632 meanR:12.1400 R:18.0000 rate:0.0360 gloss:1.2954 dloss:1.2957 exploreP:0.3877\n",
      "Episode:633 meanR:12.1100 R:11.0000 rate:0.0220 gloss:1.2957 dloss:1.2958 exploreP:0.3873\n",
      "Episode:634 meanR:12.1400 R:13.0000 rate:0.0260 gloss:1.2956 dloss:1.2958 exploreP:0.3868\n",
      "Episode:635 meanR:12.1300 R:10.0000 rate:0.0200 gloss:1.2959 dloss:1.2962 exploreP:0.3864\n",
      "Episode:636 meanR:12.1500 R:11.0000 rate:0.0220 gloss:1.2955 dloss:1.2958 exploreP:0.3860\n",
      "Episode:637 meanR:12.1500 R:10.0000 rate:0.0200 gloss:1.2960 dloss:1.2963 exploreP:0.3856\n",
      "Episode:638 meanR:12.2100 R:15.0000 rate:0.0300 gloss:1.2948 dloss:1.2950 exploreP:0.3851\n",
      "Episode:639 meanR:12.1900 R:10.0000 rate:0.0200 gloss:1.2946 dloss:1.2948 exploreP:0.3847\n",
      "Episode:640 meanR:12.1700 R:10.0000 rate:0.0200 gloss:1.2946 dloss:1.2947 exploreP:0.3843\n",
      "Episode:641 meanR:12.2000 R:13.0000 rate:0.0260 gloss:1.2944 dloss:1.2948 exploreP:0.3838\n",
      "Episode:642 meanR:12.2200 R:13.0000 rate:0.0260 gloss:1.2955 dloss:1.2959 exploreP:0.3833\n",
      "Episode:643 meanR:12.2500 R:12.0000 rate:0.0240 gloss:1.2947 dloss:1.2950 exploreP:0.3829\n",
      "Episode:644 meanR:12.2600 R:10.0000 rate:0.0200 gloss:1.2942 dloss:1.2944 exploreP:0.3825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:645 meanR:12.2500 R:10.0000 rate:0.0200 gloss:1.2945 dloss:1.2948 exploreP:0.3822\n",
      "Episode:646 meanR:12.2300 R:11.0000 rate:0.0220 gloss:1.2939 dloss:1.2941 exploreP:0.3817\n",
      "Episode:647 meanR:12.1500 R:8.0000 rate:0.0160 gloss:1.2952 dloss:1.2955 exploreP:0.3814\n",
      "Episode:648 meanR:12.1300 R:12.0000 rate:0.0240 gloss:1.2955 dloss:1.2958 exploreP:0.3810\n",
      "Episode:649 meanR:12.1400 R:12.0000 rate:0.0240 gloss:1.2955 dloss:1.2959 exploreP:0.3806\n",
      "Episode:650 meanR:12.1400 R:9.0000 rate:0.0180 gloss:1.2942 dloss:1.2945 exploreP:0.3802\n",
      "Episode:651 meanR:12.1200 R:10.0000 rate:0.0200 gloss:1.2957 dloss:1.2961 exploreP:0.3799\n",
      "Episode:652 meanR:12.1000 R:11.0000 rate:0.0220 gloss:1.2948 dloss:1.2951 exploreP:0.3794\n",
      "Episode:653 meanR:12.1300 R:12.0000 rate:0.0240 gloss:1.2955 dloss:1.2958 exploreP:0.3790\n",
      "Episode:654 meanR:12.0600 R:13.0000 rate:0.0260 gloss:1.2950 dloss:1.2952 exploreP:0.3785\n",
      "Episode:655 meanR:12.1200 R:18.0000 rate:0.0360 gloss:1.2948 dloss:1.2951 exploreP:0.3779\n",
      "Episode:656 meanR:12.1100 R:10.0000 rate:0.0200 gloss:1.2948 dloss:1.2952 exploreP:0.3775\n",
      "Episode:657 meanR:12.0800 R:11.0000 rate:0.0220 gloss:1.2955 dloss:1.2960 exploreP:0.3771\n",
      "Episode:658 meanR:12.0700 R:12.0000 rate:0.0240 gloss:1.2949 dloss:1.2952 exploreP:0.3766\n",
      "Episode:659 meanR:12.0800 R:11.0000 rate:0.0220 gloss:1.2955 dloss:1.2956 exploreP:0.3762\n",
      "Episode:660 meanR:12.0700 R:11.0000 rate:0.0220 gloss:1.2955 dloss:1.2958 exploreP:0.3758\n",
      "Episode:661 meanR:11.9700 R:10.0000 rate:0.0200 gloss:1.2946 dloss:1.2950 exploreP:0.3755\n",
      "Episode:662 meanR:11.9900 R:12.0000 rate:0.0240 gloss:1.2947 dloss:1.2950 exploreP:0.3750\n",
      "Episode:663 meanR:11.9800 R:10.0000 rate:0.0200 gloss:1.2950 dloss:1.2952 exploreP:0.3747\n",
      "Episode:664 meanR:11.9400 R:10.0000 rate:0.0200 gloss:1.2948 dloss:1.2952 exploreP:0.3743\n",
      "Episode:665 meanR:11.9300 R:10.0000 rate:0.0200 gloss:1.2955 dloss:1.2958 exploreP:0.3739\n",
      "Episode:666 meanR:11.9600 R:14.0000 rate:0.0280 gloss:1.2957 dloss:1.2960 exploreP:0.3734\n",
      "Episode:667 meanR:11.9800 R:11.0000 rate:0.0220 gloss:1.2955 dloss:1.2956 exploreP:0.3730\n",
      "Episode:668 meanR:11.9600 R:13.0000 rate:0.0260 gloss:1.2948 dloss:1.2952 exploreP:0.3726\n",
      "Episode:669 meanR:11.9200 R:10.0000 rate:0.0200 gloss:1.2955 dloss:1.2960 exploreP:0.3722\n",
      "Episode:670 meanR:11.9700 R:19.0000 rate:0.0380 gloss:1.2953 dloss:1.2957 exploreP:0.3715\n",
      "Episode:671 meanR:11.9700 R:10.0000 rate:0.0200 gloss:1.2954 dloss:1.2959 exploreP:0.3712\n",
      "Episode:672 meanR:11.9800 R:11.0000 rate:0.0220 gloss:1.2948 dloss:1.2952 exploreP:0.3708\n",
      "Episode:673 meanR:12.0100 R:12.0000 rate:0.0240 gloss:1.2954 dloss:1.2957 exploreP:0.3703\n",
      "Episode:674 meanR:12.0400 R:12.0000 rate:0.0240 gloss:1.2959 dloss:1.2963 exploreP:0.3699\n",
      "Episode:675 meanR:12.0100 R:9.0000 rate:0.0180 gloss:1.2958 dloss:1.2960 exploreP:0.3696\n",
      "Episode:676 meanR:11.9200 R:10.0000 rate:0.0200 gloss:1.2957 dloss:1.2960 exploreP:0.3692\n",
      "Episode:677 meanR:11.8700 R:9.0000 rate:0.0180 gloss:1.2960 dloss:1.2963 exploreP:0.3689\n",
      "Episode:678 meanR:11.8900 R:10.0000 rate:0.0200 gloss:1.2954 dloss:1.2958 exploreP:0.3685\n",
      "Episode:679 meanR:11.8800 R:11.0000 rate:0.0220 gloss:1.2953 dloss:1.2958 exploreP:0.3681\n",
      "Episode:680 meanR:11.8500 R:12.0000 rate:0.0240 gloss:1.2953 dloss:1.2958 exploreP:0.3677\n",
      "Episode:681 meanR:11.8700 R:12.0000 rate:0.0240 gloss:1.2949 dloss:1.2954 exploreP:0.3673\n",
      "Episode:682 meanR:11.9000 R:12.0000 rate:0.0240 gloss:1.2953 dloss:1.2956 exploreP:0.3668\n",
      "Episode:683 meanR:11.9100 R:10.0000 rate:0.0200 gloss:1.2954 dloss:1.2959 exploreP:0.3665\n",
      "Episode:684 meanR:11.9500 R:13.0000 rate:0.0260 gloss:1.2963 dloss:1.2965 exploreP:0.3660\n",
      "Episode:685 meanR:11.8600 R:10.0000 rate:0.0200 gloss:1.2955 dloss:1.2959 exploreP:0.3657\n",
      "Episode:686 meanR:11.8300 R:12.0000 rate:0.0240 gloss:1.2962 dloss:1.2966 exploreP:0.3652\n",
      "Episode:687 meanR:11.7600 R:8.0000 rate:0.0160 gloss:1.2962 dloss:1.2964 exploreP:0.3650\n",
      "Episode:688 meanR:11.7500 R:10.0000 rate:0.0200 gloss:1.2959 dloss:1.2962 exploreP:0.3646\n",
      "Episode:689 meanR:11.7300 R:10.0000 rate:0.0200 gloss:1.2958 dloss:1.2962 exploreP:0.3643\n",
      "Episode:690 meanR:11.7400 R:13.0000 rate:0.0260 gloss:1.2953 dloss:1.2958 exploreP:0.3638\n",
      "Episode:691 meanR:11.8300 R:19.0000 rate:0.0380 gloss:1.2960 dloss:1.2963 exploreP:0.3631\n",
      "Episode:692 meanR:11.8100 R:9.0000 rate:0.0180 gloss:1.2962 dloss:1.2963 exploreP:0.3628\n",
      "Episode:693 meanR:11.8100 R:12.0000 rate:0.0240 gloss:1.2967 dloss:1.2970 exploreP:0.3624\n",
      "Episode:694 meanR:11.7900 R:10.0000 rate:0.0200 gloss:1.2969 dloss:1.2971 exploreP:0.3620\n",
      "Episode:695 meanR:11.7800 R:12.0000 rate:0.0240 gloss:1.2980 dloss:1.2983 exploreP:0.3616\n",
      "Episode:696 meanR:11.7600 R:8.0000 rate:0.0160 gloss:1.2969 dloss:1.2970 exploreP:0.3613\n",
      "Episode:697 meanR:11.7700 R:11.0000 rate:0.0220 gloss:1.2965 dloss:1.2967 exploreP:0.3609\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], [] # goal\n",
    "gloss_list, dloss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111*2):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1]), \n",
    "                                                                          model.training: False})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rate = -1 # [-1, +1] or [0, 1]\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # memory incremented\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # [0, 1]\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1:\n",
    "                        memory.buffer[-1-idx][-1] = rate\n",
    "                        \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states, actions, next_states, rewards, dones, rates = minibatch(batch=batch)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states,\n",
    "                                                                   model.training: False})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            dloss, _ = sess.run([model.d_loss, model.d_opt],feed_dict = {model.states: states, \n",
    "                                                                         model.actions: actions,\n",
    "                                                                         model.targetQs: targetQs, \n",
    "                                                                         model.rates: rates,\n",
    "                                                                         model.training: True})\n",
    "            gloss, _ = sess.run([model.g_loss, model.g_opt],feed_dict = {model.states: states, \n",
    "                                                                         model.actions: actions,\n",
    "                                                                         model.targetQs: targetQs, \n",
    "                                                                         model.rates: rates,\n",
    "                                                                         model.training: True})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
