{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: -2.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, array([0.        , 1.        , 0.        , 0.        , 0.14733367,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.11928118,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.47576329,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.45386043,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.99189001,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.74783498,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.12713307,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, array([0.        , 1.        , 0.        , 0.        , 0.14733367,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.11928118,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.47576329,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.45386043,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.99189001,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.74783498,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.12713307,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (300,) (300, 37) (300,) (300,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 10.711230278015137 -9.99843692779541\n",
      "actions: 3 0\n",
      "rewards: 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data into the model\n",
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    reward = tf.placeholder(tf.float32, [], name='reward')\n",
    "    return states, actions, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, reward, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    loss_prob = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                          labels=actions_labels))\n",
    "    reward_prob = tf.nn.sigmoid(reward)\n",
    "    loss = loss_prob * -reward_prob\n",
    "    return actions_logits, loss, reward_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.reward = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss, self.reward_prob = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, reward=self.reward) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(300, 37) actions:(300,)\n",
      "action size:4\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "state_size = 37                # number of units for the input state/observation -- simulation\n",
    "action_size = 4                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5734\n",
      "Episode: 1 total_reward: 0.0 reward_prob: 0.5000 loss: -0.4462\n",
      "Episode: 2 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5719\n",
      "Episode: 3 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8171\n",
      "Episode: 4 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5855\n",
      "Episode: 5 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5623\n",
      "Episode: 6 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5724\n",
      "Episode: 7 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5276\n",
      "Episode: 8 total_reward: 3.0 reward_prob: 0.9526 loss: -1.0407\n",
      "Episode: 9 total_reward: 1.0 reward_prob: 0.7311 loss: -0.7913\n",
      "Episode: 10 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5851\n",
      "Episode: 11 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6126\n",
      "Episode: 12 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5783\n",
      "Episode: 13 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6387\n",
      "Episode: 14 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3342\n",
      "Episode: 15 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6549\n",
      "Episode: 16 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6545\n",
      "Episode: 17 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5841\n",
      "Episode: 18 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3429\n",
      "Episode: 19 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8634\n",
      "Episode: 20 total_reward: 2.0 reward_prob: 0.8808 loss: -1.0664\n",
      "Episode: 21 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8398\n",
      "Episode: 22 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8743\n",
      "Episode: 23 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3174\n",
      "Episode: 24 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8754\n",
      "Episode: 25 total_reward: 2.0 reward_prob: 0.8808 loss: -1.0541\n",
      "Episode: 26 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5961\n",
      "Episode: 27 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8706\n",
      "Episode: 28 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6009\n",
      "Episode: 29 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0571\n",
      "Episode: 30 total_reward: -1.0 reward_prob: 0.2689 loss: -0.2961\n",
      "Episode: 31 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8691\n",
      "Episode: 32 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1406\n",
      "Episode: 33 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6078\n",
      "Episode: 34 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8040\n",
      "Episode: 35 total_reward: 0.0 reward_prob: 0.5000 loss: -0.5849\n",
      "Episode: 36 total_reward: 2.0 reward_prob: 0.8808 loss: -1.0836\n",
      "Episode: 37 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8626\n",
      "Episode: 38 total_reward: 1.0 reward_prob: 0.7311 loss: -0.8944\n",
      "Episode: 39 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3392\n",
      "Episode: 40 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6109\n",
      "Episode: 41 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6182\n",
      "Episode: 42 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6173\n",
      "Episode: 43 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6258\n",
      "Episode: 44 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3361\n",
      "Episode: 45 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1546\n",
      "Episode: 46 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6399\n",
      "Episode: 47 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3545\n",
      "Episode: 48 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6216\n",
      "Episode: 49 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9143\n",
      "Episode: 50 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6523\n",
      "Episode: 51 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6453\n",
      "Episode: 52 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6150\n",
      "Episode: 53 total_reward: 5.0 reward_prob: 0.9933 loss: -1.2236\n",
      "Episode: 54 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9100\n",
      "Episode: 55 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6485\n",
      "Episode: 56 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9557\n",
      "Episode: 57 total_reward: 2.0 reward_prob: 0.8808 loss: -1.0980\n",
      "Episode: 58 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1515\n",
      "Episode: 59 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9185\n",
      "Episode: 60 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3468\n",
      "Episode: 61 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6399\n",
      "Episode: 62 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6118\n",
      "Episode: 63 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6444\n",
      "Episode: 64 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9254\n",
      "Episode: 65 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6339\n",
      "Episode: 66 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3384\n",
      "Episode: 67 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6296\n",
      "Episode: 68 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6323\n",
      "Episode: 69 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3439\n",
      "Episode: 70 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9272\n",
      "Episode: 71 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6389\n",
      "Episode: 72 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3427\n",
      "Episode: 73 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3515\n",
      "Episode: 74 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1524\n",
      "Episode: 75 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6346\n",
      "Episode: 76 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6302\n",
      "Episode: 77 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6347\n",
      "Episode: 78 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9285\n",
      "Episode: 79 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3451\n",
      "Episode: 80 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6368\n",
      "Episode: 81 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6393\n",
      "Episode: 82 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6477\n",
      "Episode: 83 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2165\n",
      "Episode: 84 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6549\n",
      "Episode: 85 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9449\n",
      "Episode: 86 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6471\n",
      "Episode: 87 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1539\n",
      "Episode: 88 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1263\n",
      "Episode: 89 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6436\n",
      "Episode: 90 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3449\n",
      "Episode: 91 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9463\n",
      "Episode: 92 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6470\n",
      "Episode: 93 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9484\n",
      "Episode: 94 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6512\n",
      "Episode: 95 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6447\n",
      "Episode: 96 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1435\n",
      "Episode: 97 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3514\n",
      "Episode: 98 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6701\n",
      "Episode: 99 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6511\n",
      "Episode: 100 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6457\n",
      "Episode: 101 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6398\n",
      "Episode: 102 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9538\n",
      "Episode: 103 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6531\n",
      "Episode: 104 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6443\n",
      "Episode: 105 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6334\n",
      "Episode: 106 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3534\n",
      "Episode: 107 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6379\n",
      "Episode: 108 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6574\n",
      "Episode: 109 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9523\n",
      "Episode: 110 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6528\n",
      "Episode: 111 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2566\n",
      "Episode: 112 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6598\n",
      "Episode: 113 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9662\n",
      "Episode: 114 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9627\n",
      "Episode: 115 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9497\n",
      "Episode: 116 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9723\n",
      "Episode: 117 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3544\n",
      "Episode: 118 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9538\n",
      "Episode: 119 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3511\n",
      "Episode: 120 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6516\n",
      "Episode: 121 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9632\n",
      "Episode: 122 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9450\n",
      "Episode: 123 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3526\n",
      "Episode: 124 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6517\n",
      "Episode: 125 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6607\n",
      "Episode: 126 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3540\n",
      "Episode: 127 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 128 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1543\n",
      "Episode: 129 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2403\n",
      "Episode: 130 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9577\n",
      "Episode: 131 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2494\n",
      "Episode: 132 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9570\n",
      "Episode: 133 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9615\n",
      "Episode: 134 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9588\n",
      "Episode: 135 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2431\n",
      "Episode: 136 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1585\n",
      "Episode: 137 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6533\n",
      "Episode: 138 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6555\n",
      "Episode: 139 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2482\n",
      "Episode: 140 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6612\n",
      "Episode: 141 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1572\n",
      "Episode: 142 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1598\n",
      "Episode: 143 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6583\n",
      "Episode: 144 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9565\n",
      "Episode: 145 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1501\n",
      "Episode: 146 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1607\n",
      "Episode: 147 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3529\n",
      "Episode: 148 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6492\n",
      "Episode: 149 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2505\n",
      "Episode: 150 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9674\n",
      "Episode: 151 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9614\n",
      "Episode: 152 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6590\n",
      "Episode: 153 total_reward: 4.0 reward_prob: 0.9820 loss: -1.2965\n",
      "Episode: 154 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6645\n",
      "Episode: 155 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3583\n",
      "Episode: 156 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1658\n",
      "Episode: 157 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9580\n",
      "Episode: 158 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6602\n",
      "Episode: 159 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6567\n",
      "Episode: 160 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2581\n",
      "Episode: 161 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6654\n",
      "Episode: 162 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6631\n",
      "Episode: 163 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1733\n",
      "Episode: 164 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0628\n",
      "Episode: 165 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6618\n",
      "Episode: 166 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9815\n",
      "Episode: 167 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3563\n",
      "Episode: 168 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9693\n",
      "Episode: 169 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6522\n",
      "Episode: 170 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9681\n",
      "Episode: 171 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1554\n",
      "Episode: 172 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6648\n",
      "Episode: 173 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9588\n",
      "Episode: 174 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6688\n",
      "Episode: 175 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3655\n",
      "Episode: 176 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6619\n",
      "Episode: 177 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6627\n",
      "Episode: 178 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3596\n",
      "Episode: 179 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3575\n",
      "Episode: 180 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6575\n",
      "Episode: 181 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1567\n",
      "Episode: 182 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9563\n",
      "Episode: 183 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6583\n",
      "Episode: 184 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6610\n",
      "Episode: 185 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9710\n",
      "Episode: 186 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6625\n",
      "Episode: 187 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3554\n",
      "Episode: 188 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6452\n",
      "Episode: 189 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9661\n",
      "Episode: 190 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9589\n",
      "Episode: 191 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3597\n",
      "Episode: 192 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6596\n",
      "Episode: 193 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9680\n",
      "Episode: 194 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1606\n",
      "Episode: 195 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9678\n",
      "Episode: 196 total_reward: 4.0 reward_prob: 0.9820 loss: -1.3015\n",
      "Episode: 197 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6612\n",
      "Episode: 198 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1719\n",
      "Episode: 199 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9813\n",
      "Episode: 200 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9719\n",
      "Episode: 201 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6694\n",
      "Episode: 202 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6636\n",
      "Episode: 203 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6707\n",
      "Episode: 204 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9664\n",
      "Episode: 205 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6603\n",
      "Episode: 206 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9675\n",
      "Episode: 207 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1647\n",
      "Episode: 208 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9705\n",
      "Episode: 209 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1669\n",
      "Episode: 210 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6650\n",
      "Episode: 211 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9740\n",
      "Episode: 212 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2644\n",
      "Episode: 213 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6648\n",
      "Episode: 214 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6689\n",
      "Episode: 215 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9742\n",
      "Episode: 216 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1719\n",
      "Episode: 217 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6650\n",
      "Episode: 218 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6656\n",
      "Episode: 219 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6701\n",
      "Episode: 220 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9835\n",
      "Episode: 221 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6665\n",
      "Episode: 222 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9766\n",
      "Episode: 223 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3588\n",
      "Episode: 224 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6701\n",
      "Episode: 225 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6843\n",
      "Episode: 226 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9708\n",
      "Episode: 227 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1723\n",
      "Episode: 228 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6680\n",
      "Episode: 229 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6691\n",
      "Episode: 230 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6686\n",
      "Episode: 231 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3589\n",
      "Episode: 232 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1596\n",
      "Episode: 233 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9751\n",
      "Episode: 234 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6742\n",
      "Episode: 235 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6661\n",
      "Episode: 236 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9699\n",
      "Episode: 237 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9762\n",
      "Episode: 238 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6679\n",
      "Episode: 239 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6696\n",
      "Episode: 240 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1798\n",
      "Episode: 241 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3598\n",
      "Episode: 242 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6752\n",
      "Episode: 243 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6710\n",
      "Episode: 244 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2737\n",
      "Episode: 245 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3614\n",
      "Episode: 246 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6710\n",
      "Episode: 247 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6821\n",
      "Episode: 248 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6711\n",
      "Episode: 249 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6682\n",
      "Episode: 250 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6694\n",
      "Episode: 251 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3575\n",
      "Episode: 252 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9765\n",
      "Episode: 253 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 254 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3590\n",
      "Episode: 255 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9733\n",
      "Episode: 256 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6705\n",
      "Episode: 257 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9767\n",
      "Episode: 258 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6691\n",
      "Episode: 259 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6714\n",
      "Episode: 260 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6720\n",
      "Episode: 261 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1817\n",
      "Episode: 262 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9920\n",
      "Episode: 263 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6708\n",
      "Episode: 264 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3669\n",
      "Episode: 265 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6783\n",
      "Episode: 266 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1595\n",
      "Episode: 267 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6679\n",
      "Episode: 268 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9680\n",
      "Episode: 269 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1730\n",
      "Episode: 270 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6688\n",
      "Episode: 271 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6669\n",
      "Episode: 272 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6662\n",
      "Episode: 273 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1811\n",
      "Episode: 274 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3586\n",
      "Episode: 275 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3595\n",
      "Episode: 276 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6700\n",
      "Episode: 277 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6579\n",
      "Episode: 278 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6633\n",
      "Episode: 279 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2671\n",
      "Episode: 280 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9715\n",
      "Episode: 281 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9751\n",
      "Episode: 282 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6675\n",
      "Episode: 283 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6673\n",
      "Episode: 284 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9771\n",
      "Episode: 285 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1597\n",
      "Episode: 286 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6640\n",
      "Episode: 287 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6649\n",
      "Episode: 288 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6650\n",
      "Episode: 289 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6677\n",
      "Episode: 290 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3639\n",
      "Episode: 291 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6781\n",
      "Episode: 292 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3613\n",
      "Episode: 293 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6634\n",
      "Episode: 294 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1748\n",
      "Episode: 295 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3620\n",
      "Episode: 296 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6695\n",
      "Episode: 297 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1595\n",
      "Episode: 298 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6707\n",
      "Episode: 299 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3570\n",
      "Episode: 300 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6665\n",
      "Episode: 301 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1591\n",
      "Episode: 302 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3593\n",
      "Episode: 303 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6806\n",
      "Episode: 304 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2726\n",
      "Episode: 305 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3595\n",
      "Episode: 306 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6723\n",
      "Episode: 307 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6656\n",
      "Episode: 308 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6666\n",
      "Episode: 309 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3602\n",
      "Episode: 310 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2702\n",
      "Episode: 311 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6707\n",
      "Episode: 312 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9742\n",
      "Episode: 313 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6669\n",
      "Episode: 314 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6716\n",
      "Episode: 315 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3592\n",
      "Episode: 316 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6665\n",
      "Episode: 317 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6684\n",
      "Episode: 318 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6691\n",
      "Episode: 319 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9794\n",
      "Episode: 320 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9511\n",
      "Episode: 321 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2759\n",
      "Episode: 322 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1846\n",
      "Episode: 323 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9845\n",
      "Episode: 324 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3619\n",
      "Episode: 325 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1807\n",
      "Episode: 326 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9838\n",
      "Episode: 327 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2818\n",
      "Episode: 328 total_reward: -4.0 reward_prob: 0.0180 loss: -0.0242\n",
      "Episode: 329 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9847\n",
      "Episode: 330 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6722\n",
      "Episode: 331 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1867\n",
      "Episode: 332 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6690\n",
      "Episode: 333 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9716\n",
      "Episode: 334 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6732\n",
      "Episode: 335 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3641\n",
      "Episode: 336 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6736\n",
      "Episode: 337 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9856\n",
      "Episode: 338 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6739\n",
      "Episode: 339 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6720\n",
      "Episode: 340 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3601\n",
      "Episode: 341 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1805\n",
      "Episode: 342 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1834\n",
      "Episode: 343 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6787\n",
      "Episode: 344 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6690\n",
      "Episode: 345 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1601\n",
      "Episode: 346 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9833\n",
      "Episode: 347 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1864\n",
      "Episode: 348 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6767\n",
      "Episode: 349 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6806\n",
      "Episode: 350 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3646\n",
      "Episode: 351 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9828\n",
      "Episode: 352 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6739\n",
      "Episode: 353 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9848\n",
      "Episode: 354 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6715\n",
      "Episode: 355 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6770\n",
      "Episode: 356 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6733\n",
      "Episode: 357 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9793\n",
      "Episode: 358 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6721\n",
      "Episode: 359 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6719\n",
      "Episode: 360 total_reward: 4.0 reward_prob: 0.9820 loss: -1.3258\n",
      "Episode: 361 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6741\n",
      "Episode: 362 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6716\n",
      "Episode: 363 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6725\n",
      "Episode: 364 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6763\n",
      "Episode: 365 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6700\n",
      "Episode: 366 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3622\n",
      "Episode: 367 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2772\n",
      "Episode: 368 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9817\n",
      "Episode: 369 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6774\n",
      "Episode: 370 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6734\n",
      "Episode: 371 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6731\n",
      "Episode: 372 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3613\n",
      "Episode: 373 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9823\n",
      "Episode: 374 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9885\n",
      "Episode: 375 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1902\n",
      "Episode: 376 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1909\n",
      "Episode: 377 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9909\n",
      "Episode: 378 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1830\n",
      "Episode: 379 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3621\n",
      "Episode: 381 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6733\n",
      "Episode: 382 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3598\n",
      "Episode: 383 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6756\n",
      "Episode: 384 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3607\n",
      "Episode: 385 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6779\n",
      "Episode: 386 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9832\n",
      "Episode: 387 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 388 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6758\n",
      "Episode: 389 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6762\n",
      "Episode: 390 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9926\n",
      "Episode: 391 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1611\n",
      "Episode: 392 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6744\n",
      "Episode: 393 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1858\n",
      "Episode: 394 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6767\n",
      "Episode: 395 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6764\n",
      "Episode: 396 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6763\n",
      "Episode: 397 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6728\n",
      "Episode: 398 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6745\n",
      "Episode: 399 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9975\n",
      "Episode: 400 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6747\n",
      "Episode: 401 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9835\n",
      "Episode: 402 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6734\n",
      "Episode: 403 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6721\n",
      "Episode: 404 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6708\n",
      "Episode: 405 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3620\n",
      "Episode: 406 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3640\n",
      "Episode: 407 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1877\n",
      "Episode: 408 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6759\n",
      "Episode: 409 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6870\n",
      "Episode: 410 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6689\n",
      "Episode: 411 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6758\n",
      "Episode: 412 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6728\n",
      "Episode: 413 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6743\n",
      "Episode: 414 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1605\n",
      "Episode: 415 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1598\n",
      "Episode: 416 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9854\n",
      "Episode: 417 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6721\n",
      "Episode: 418 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6758\n",
      "Episode: 419 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6769\n",
      "Episode: 420 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1888\n",
      "Episode: 421 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2916\n",
      "Episode: 422 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1886\n",
      "Episode: 423 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9865\n",
      "Episode: 424 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6802\n",
      "Episode: 425 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3652\n",
      "Episode: 426 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1608\n",
      "Episode: 427 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1613\n",
      "Episode: 428 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1613\n",
      "Episode: 429 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6762\n",
      "Episode: 430 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6748\n",
      "Episode: 431 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9931\n",
      "Episode: 432 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1614\n",
      "Episode: 433 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3638\n",
      "Episode: 434 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1607\n",
      "Episode: 435 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6730\n",
      "Episode: 436 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9852\n",
      "Episode: 437 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6770\n",
      "Episode: 438 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6754\n",
      "Episode: 439 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6765\n",
      "Episode: 440 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3621\n",
      "Episode: 441 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3667\n",
      "Episode: 442 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3647\n",
      "Episode: 443 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 444 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6764\n",
      "Episode: 445 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6802\n",
      "Episode: 446 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 447 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9961\n",
      "Episode: 448 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3639\n",
      "Episode: 449 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1612\n",
      "Episode: 450 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9867\n",
      "Episode: 451 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6743\n",
      "Episode: 452 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6727\n",
      "Episode: 453 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3626\n",
      "Episode: 454 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1890\n",
      "Episode: 455 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6752\n",
      "Episode: 456 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6749\n",
      "Episode: 457 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6742\n",
      "Episode: 458 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6740\n",
      "Episode: 459 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3619\n",
      "Episode: 460 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9867\n",
      "Episode: 461 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6790\n",
      "Episode: 462 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9894\n",
      "Episode: 463 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3642\n",
      "Episode: 464 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6757\n",
      "Episode: 465 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6766\n",
      "Episode: 466 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6745\n",
      "Episode: 467 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6739\n",
      "Episode: 468 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6734\n",
      "Episode: 469 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9880\n",
      "Episode: 470 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9908\n",
      "Episode: 471 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9897\n",
      "Episode: 472 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9914\n",
      "Episode: 473 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9938\n",
      "Episode: 474 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 475 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1929\n",
      "Episode: 476 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6747\n",
      "Episode: 477 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9884\n",
      "Episode: 478 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9878\n",
      "Episode: 479 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6777\n",
      "Episode: 480 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6807\n",
      "Episode: 481 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6762\n",
      "Episode: 482 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6740\n",
      "Episode: 483 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6755\n",
      "Episode: 484 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3636\n",
      "Episode: 485 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9809\n",
      "Episode: 486 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9889\n",
      "Episode: 487 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3665\n",
      "Episode: 488 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9945\n",
      "Episode: 489 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1967\n",
      "Episode: 490 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9971\n",
      "Episode: 491 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 492 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6787\n",
      "Episode: 493 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6821\n",
      "Episode: 494 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6771\n",
      "Episode: 495 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6791\n",
      "Episode: 496 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1615\n",
      "Episode: 497 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6802\n",
      "Episode: 498 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9914\n",
      "Episode: 499 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1617\n",
      "Episode: 500 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6803\n",
      "Episode: 501 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9907\n",
      "Episode: 502 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3653\n",
      "Episode: 503 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6802\n",
      "Episode: 504 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3667\n",
      "Episode: 505 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 506 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3655\n",
      "Episode: 507 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3661\n",
      "Episode: 508 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9918\n",
      "Episode: 509 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 510 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6789\n",
      "Episode: 511 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1617\n",
      "Episode: 512 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9911\n",
      "Episode: 513 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6806\n",
      "Episode: 514 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3645\n",
      "Episode: 515 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9909\n",
      "Episode: 516 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 517 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6792\n",
      "Episode: 518 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9963\n",
      "Episode: 519 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3663\n",
      "Episode: 520 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3673\n",
      "Episode: 521 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9878\n",
      "Episode: 522 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3660\n",
      "Episode: 523 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6775\n",
      "Episode: 524 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6763\n",
      "Episode: 525 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6755\n",
      "Episode: 526 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3642\n",
      "Episode: 527 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3688\n",
      "Episode: 528 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9901\n",
      "Episode: 529 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9917\n",
      "Episode: 530 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 531 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6764\n",
      "Episode: 532 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6762\n",
      "Episode: 533 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6755\n",
      "Episode: 534 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6782\n",
      "Episode: 535 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1622\n",
      "Episode: 536 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6779\n",
      "Episode: 537 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6793\n",
      "Episode: 538 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9928\n",
      "Episode: 539 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1619\n",
      "Episode: 540 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6774\n",
      "Episode: 541 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1966\n",
      "Episode: 542 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1981\n",
      "Episode: 543 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1620\n",
      "Episode: 544 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6858\n",
      "Episode: 545 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9869\n",
      "Episode: 546 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9880\n",
      "Episode: 547 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1929\n",
      "Episode: 548 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3639\n",
      "Episode: 549 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1949\n",
      "Episode: 550 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6787\n",
      "Episode: 551 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9937\n",
      "Episode: 552 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6810\n",
      "Episode: 553 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6793\n",
      "Episode: 554 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6791\n",
      "Episode: 555 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6795\n",
      "Episode: 556 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6794\n",
      "Episode: 557 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6747\n",
      "Episode: 558 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6740\n",
      "Episode: 559 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6673\n",
      "Episode: 560 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3641\n",
      "Episode: 561 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6778\n",
      "Episode: 562 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6817\n",
      "Episode: 563 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9916\n",
      "Episode: 564 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9905\n",
      "Episode: 565 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6838\n",
      "Episode: 566 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1849\n",
      "Episode: 567 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9905\n",
      "Episode: 568 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6804\n",
      "Episode: 569 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 570 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6771\n",
      "Episode: 571 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6802\n",
      "Episode: 572 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6782\n",
      "Episode: 573 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3673\n",
      "Episode: 574 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3640\n",
      "Episode: 575 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3657\n",
      "Episode: 576 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6797\n",
      "Episode: 577 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9942\n",
      "Episode: 578 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6822\n",
      "Episode: 579 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6808\n",
      "Episode: 580 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6800\n",
      "Episode: 581 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9907\n",
      "Episode: 582 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6779\n",
      "Episode: 583 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6782\n",
      "Episode: 584 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6777\n",
      "Episode: 585 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6803\n",
      "Episode: 586 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1931\n",
      "Episode: 587 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 588 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6746\n",
      "Episode: 589 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0644\n",
      "Episode: 590 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9911\n",
      "Episode: 591 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9937\n",
      "Episode: 592 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9957\n",
      "Episode: 593 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6792\n",
      "Episode: 594 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9933\n",
      "Episode: 595 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0645\n",
      "Episode: 596 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6763\n",
      "Episode: 597 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9950\n",
      "Episode: 598 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3666\n",
      "Episode: 599 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6805\n",
      "Episode: 600 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6791\n",
      "Episode: 601 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9959\n",
      "Episode: 602 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6801\n",
      "Episode: 603 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6773\n",
      "Episode: 604 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6822\n",
      "Episode: 605 total_reward: 2.0 reward_prob: 0.8808 loss: -1.2004\n",
      "Episode: 606 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9952\n",
      "Episode: 607 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6800\n",
      "Episode: 608 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6801\n",
      "Episode: 609 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6791\n",
      "Episode: 610 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6792\n",
      "Episode: 611 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6812\n",
      "Episode: 612 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0647\n",
      "Episode: 613 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3675\n",
      "Episode: 614 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6797\n",
      "Episode: 615 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3658\n",
      "Episode: 616 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1967\n",
      "Episode: 617 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1962\n",
      "Episode: 618 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1617\n",
      "Episode: 619 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9902\n",
      "Episode: 620 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3661\n",
      "Episode: 621 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9927\n",
      "Episode: 622 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3648\n",
      "Episode: 623 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6759\n",
      "Episode: 624 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6767\n",
      "Episode: 625 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3659\n",
      "Episode: 626 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6791\n",
      "Episode: 627 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3644\n",
      "Episode: 628 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9908\n",
      "Episode: 629 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6793\n",
      "Episode: 630 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6785\n",
      "Episode: 631 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 632 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6568\n",
      "Episode: 633 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1711\n",
      "Episode: 634 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9860\n",
      "Episode: 635 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6747\n",
      "Episode: 636 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1768\n",
      "Episode: 637 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9836\n",
      "Episode: 638 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3597\n",
      "Episode: 639 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6775\n",
      "Episode: 640 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3615\n",
      "Episode: 641 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3628\n",
      "Episode: 642 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1958\n",
      "Episode: 643 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6809\n",
      "Episode: 644 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6846\n",
      "Episode: 645 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6851\n",
      "Episode: 646 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3655\n",
      "Episode: 647 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9926\n",
      "Episode: 648 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9919\n",
      "Episode: 649 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3650\n",
      "Episode: 650 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6765\n",
      "Episode: 651 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6835\n",
      "Episode: 652 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6729\n",
      "Episode: 653 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9831\n",
      "Episode: 654 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1939\n",
      "Episode: 655 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9732\n",
      "Episode: 656 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3572\n",
      "Episode: 657 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9728\n",
      "Episode: 658 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3605\n",
      "Episode: 659 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6753\n",
      "Episode: 660 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1607\n",
      "Episode: 661 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6734\n",
      "Episode: 662 total_reward: -3.0 reward_prob: 0.0474 loss: -0.0641\n",
      "Episode: 663 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6771\n",
      "Episode: 664 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9869\n",
      "Episode: 665 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1908\n",
      "Episode: 666 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1616\n",
      "Episode: 667 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3654\n",
      "Episode: 668 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3661\n",
      "Episode: 669 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6729\n",
      "Episode: 670 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3677\n",
      "Episode: 671 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3658\n",
      "Episode: 672 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1997\n",
      "Episode: 673 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1968\n",
      "Episode: 674 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6785\n",
      "Episode: 675 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9941\n",
      "Episode: 676 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3656\n",
      "Episode: 677 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1955\n",
      "Episode: 678 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3655\n",
      "Episode: 679 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6788\n",
      "Episode: 680 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9936\n",
      "Episode: 681 total_reward: 1.0 reward_prob: 0.7311 loss: -1.0042\n",
      "Episode: 682 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6798\n",
      "Episode: 683 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6777\n",
      "Episode: 684 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9924\n",
      "Episode: 685 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1619\n",
      "Episode: 686 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1973\n",
      "Episode: 687 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9931\n",
      "Episode: 688 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n",
      "Episode: 689 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6808\n",
      "Episode: 690 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6819\n",
      "Episode: 691 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6770\n",
      "Episode: 692 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6789\n",
      "Episode: 693 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6810\n",
      "Episode: 694 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3654\n",
      "Episode: 695 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6796\n",
      "Episode: 696 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9904\n",
      "Episode: 697 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6780\n",
      "Episode: 698 total_reward: 2.0 reward_prob: 0.8808 loss: -1.2008\n",
      "Episode: 699 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6817\n",
      "Episode: 700 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2981\n",
      "Episode: 701 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6814\n",
      "Episode: 702 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6876\n",
      "Episode: 703 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6829\n",
      "Episode: 704 total_reward: 1.0 reward_prob: 0.7311 loss: -1.0041\n",
      "Episode: 705 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1623\n",
      "Episode: 706 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6786\n",
      "Episode: 707 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3669\n",
      "Episode: 708 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3636\n",
      "Episode: 709 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1613\n",
      "Episode: 710 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6771\n",
      "Episode: 711 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9906\n",
      "Episode: 712 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3650\n",
      "Episode: 713 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9917\n",
      "Episode: 714 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6794\n",
      "Episode: 715 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2966\n",
      "Episode: 716 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n",
      "Episode: 717 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9970\n",
      "Episode: 718 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6807\n",
      "Episode: 719 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9965\n",
      "Episode: 720 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6812\n",
      "Episode: 721 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3657\n",
      "Episode: 722 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6784\n",
      "Episode: 723 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6787\n",
      "Episode: 724 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3681\n",
      "Episode: 725 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6811\n",
      "Episode: 726 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6816\n",
      "Episode: 727 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9966\n",
      "Episode: 728 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3669\n",
      "Episode: 729 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6819\n",
      "Episode: 730 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6812\n",
      "Episode: 731 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6810\n",
      "Episode: 732 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6809\n",
      "Episode: 733 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9966\n",
      "Episode: 734 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3664\n",
      "Episode: 735 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6815\n",
      "Episode: 736 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6778\n",
      "Episode: 737 total_reward: 1.0 reward_prob: 0.7311 loss: -1.0013\n",
      "Episode: 738 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9962\n",
      "Episode: 739 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9940\n",
      "Episode: 740 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9948\n",
      "Episode: 741 total_reward: 2.0 reward_prob: 0.8808 loss: -1.2024\n",
      "Episode: 742 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n",
      "Episode: 743 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6806\n",
      "Episode: 744 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6829\n",
      "Episode: 745 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6732\n",
      "Episode: 746 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2873\n",
      "Episode: 747 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3627\n",
      "Episode: 748 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6756\n",
      "Episode: 749 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9904\n",
      "Episode: 750 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9945\n",
      "Episode: 751 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6803\n",
      "Episode: 752 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9919\n",
      "Episode: 753 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6715\n",
      "Episode: 754 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6795\n",
      "Episode: 755 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6738\n",
      "Episode: 756 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6773\n",
      "Episode: 757 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 758 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3637\n",
      "Episode: 759 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3614\n",
      "Episode: 760 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9937\n",
      "Episode: 761 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6783\n",
      "Episode: 762 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6839\n",
      "Episode: 763 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6776\n",
      "Episode: 764 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6768\n",
      "Episode: 765 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6764\n",
      "Episode: 766 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6774\n",
      "Episode: 767 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3650\n",
      "Episode: 768 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6803\n",
      "Episode: 769 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6886\n",
      "Episode: 770 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6792\n",
      "Episode: 771 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6777\n",
      "Episode: 772 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6780\n",
      "Episode: 773 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9949\n",
      "Episode: 774 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3664\n",
      "Episode: 775 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n",
      "Episode: 776 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6735\n",
      "Episode: 777 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9907\n",
      "Episode: 778 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3666\n",
      "Episode: 779 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3662\n",
      "Episode: 780 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3662\n",
      "Episode: 781 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1983\n",
      "Episode: 782 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6813\n",
      "Episode: 783 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3656\n",
      "Episode: 784 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9957\n",
      "Episode: 785 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9972\n",
      "Episode: 786 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3651\n",
      "Episode: 787 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6827\n",
      "Episode: 788 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1615\n",
      "Episode: 789 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3641\n",
      "Episode: 790 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1923\n",
      "Episode: 791 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3637\n",
      "Episode: 792 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9886\n",
      "Episode: 793 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6799\n",
      "Episode: 794 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3633\n",
      "Episode: 795 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1610\n",
      "Episode: 796 total_reward: -2.0 reward_prob: 0.1192 loss: -0.1610\n",
      "Episode: 797 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3616\n",
      "Episode: 798 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6807\n",
      "Episode: 799 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6819\n",
      "Episode: 800 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9944\n",
      "Episode: 801 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6788\n",
      "Episode: 802 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6668\n",
      "Episode: 803 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9913\n",
      "Episode: 804 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6766\n",
      "Episode: 805 total_reward: 3.0 reward_prob: 0.9526 loss: -1.2937\n",
      "Episode: 806 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6814\n",
      "Episode: 807 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9949\n",
      "Episode: 808 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6824\n",
      "Episode: 809 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3671\n",
      "Episode: 810 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3678\n",
      "Episode: 811 total_reward: 2.0 reward_prob: 0.8808 loss: -1.1998\n",
      "Episode: 812 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9976\n",
      "Episode: 813 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9993\n",
      "Episode: 814 total_reward: 1.0 reward_prob: 0.7311 loss: -1.0007\n",
      "Episode: 815 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6826\n",
      "Episode: 816 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6813\n",
      "Episode: 817 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3675\n",
      "Episode: 818 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3668\n",
      "Episode: 819 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6858\n",
      "Episode: 820 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6832\n",
      "Episode: 821 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6812\n",
      "Episode: 822 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3666\n",
      "Episode: 823 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6883\n",
      "Episode: 824 total_reward: 6.0 reward_prob: 0.9975 loss: -1.3593\n",
      "Episode: 825 total_reward: 1.0 reward_prob: 0.7311 loss: -0.9940\n",
      "Episode: 826 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6794\n",
      "Episode: 827 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6782\n",
      "Episode: 828 total_reward: 0.0 reward_prob: 0.5000 loss: -0.6779\n",
      "Episode: 829 total_reward: -1.0 reward_prob: 0.2689 loss: -0.3689\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1000):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # env first state\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment        \n",
    "        state = env_info.vector_observations[0]   # get the next state\n",
    "\n",
    "        # Training steps\n",
    "        #for _ in range(max_steps): # start=0, step=1, stop=max_steps/done/reward\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            batch.append([state, action])\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done is True:\n",
    "                break\n",
    "        \n",
    "        # Training batches\n",
    "        #batch = memory.buffer\n",
    "        states = np.array([each[0] for each in batch])\n",
    "        actions = np.array([each[1] for each in batch])\n",
    "        loss, _, reward_prob = sess.run([model.loss, model.opt, model.reward_prob],\n",
    "                                        feed_dict = {model.states: states, \n",
    "                                                     model.actions: actions,\n",
    "                                                     model.reward: total_reward})\n",
    "        # Print out\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'total_reward: {}'.format(total_reward),\n",
    "              'reward_prob: {:.4f}'.format(reward_prob),\n",
    "              'loss: {:.4f}'.format(loss))\n",
    "        # Plotting out\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, loss])\n",
    "        # The task is episodic, and in order to solve the environment, \n",
    "        # your agent must get an average score of +13 over 100 consecutive episodes.\n",
    "        if total_reward == 13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import gym\n",
    "# # # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('CartPole-v1')\n",
    "# # # env = gym.make('Acrobot-v1')\n",
    "# # # env = gym.make('MountainCar-v0')\n",
    "# # # env = gym.make('Pendulum-v0')\n",
    "# # # env = gym.make('Blackjack-v0')\n",
    "# # # env = gym.make('FrozenLake-v0')\n",
    "# # # env = gym.make('AirRaid-ram-v0')\n",
    "# # # env = gym.make('AirRaid-v0')\n",
    "# # # env = gym.make('BipedalWalker-v2')\n",
    "# # # env = gym.make('Copy-v0')\n",
    "# # # env = gym.make('CarRacing-v0')\n",
    "# # # env = gym.make('Ant-v2') #mujoco\n",
    "# # # env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     #sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/model-nav.ckpt')    \n",
    "#     #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "#     # Episodes/epochs\n",
    "#     for _ in range(1):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "\n",
    "#         # Steps/batches\n",
    "#         #for _ in range(111111111111111111):\n",
    "#         while True:\n",
    "#             env.render()\n",
    "#             action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "#             action = np.argmax(action_logits)\n",
    "#             state, reward, done, _ = env.step(action)\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#         # Closing the env\n",
    "#         print('total_reward: {:.2f}'.format(total_reward))\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
