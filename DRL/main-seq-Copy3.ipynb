{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cortical reinforcement learning: Policy gradients + Q-learning + GAN\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], \n",
    "batch[0][1].shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.575437423199295 -2.783263689550791\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 0.7310585786300049)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.max(np.array(rewards))), sigmoid(np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size) # hidden size\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    g_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    d_initial_state = cell.zero_state(batch_size, tf.float32) # feedback or lateral/recurrent connection from output\n",
    "    return states, actions, targetQs, cell, g_initial_state, d_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use batch-norm\n",
    "#   x_norm = tf.layers.batch_normalization(x, training=training)\n",
    "\n",
    "#   # ...\n",
    "\n",
    "#   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#   with tf.control_dependencies(update_ops):\n",
    "#     train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "# Whether to return the output in: \n",
    "# training mode (normalized with statistics of the current batch) or \n",
    "# inference mode (normalized with moving statistics). \n",
    "# NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Generator/Controller: Generating/prediting the actions\n",
    "# def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('generator', reuse=reuse):\n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return actions logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP & Conv\n",
    "# # Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "# def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "#     with tf.variable_scope('discriminator', reuse=reuse):\n",
    "#         # Fusion/merge states and actions/ SA/ SM\n",
    "#         x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "#         bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "#         nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "#         nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "#         # return rewards logits\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def discriminator(states, actions, initial_state, cell, lstm_size, reuse=False): \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=x_fused, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs,\n",
    "               cell, g_initial_state, d_initial_state):\n",
    "    # G/Actor\n",
    "    #actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_logits, g_final_state = generator(states=states, num_classes=action_size, \n",
    "                                              cell=cell, initial_state=g_initial_state, lstm_size=hidden_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs)\n",
    "    \n",
    "    # D/Critic\n",
    "    #Qs_logits = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    Qs_logits, d_final_state = discriminator(states=states, actions=actions_logits, \n",
    "                                             cell=cell, initial_state=d_initial_state, lstm_size=hidden_size)\n",
    "    d_lossQ = tf.reduce_mean(tf.square(tf.reshape(Qs_logits, [-1]) - targetQs))\n",
    "    d_lossQ_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(Qs_logits, [-1]),\n",
    "                                                                          labels=tf.nn.sigmoid(targetQs)))\n",
    "    d_loss = d_lossQ_sigm + d_lossQ\n",
    "\n",
    "    return actions_logits, Qs_logits, g_final_state, d_final_state, g_loss, d_loss, d_lossQ, d_lossQ_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss Tensor for action prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prediction for generated/prob/logits action\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize RNN\n",
    "    # g_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(g_loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    # d_grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(d_loss, d_vars), clip_norm=5) # usually around 1-5\n",
    "    g_grads=tf.gradients(g_loss, g_vars)\n",
    "    d_grads=tf.gradients(d_loss, d_vars)\n",
    "    g_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(g_grads, g_vars))\n",
    "    d_opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(d_grads, d_vars))\n",
    "    \n",
    "    # # Optimize MLP & CNN\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    #     g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    #     d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.g_initial_state, self.d_initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_final_state, self.d_final_state, self.g_loss, self.d_loss, self.d_lossQ, self.d_lossQ_sigm = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, cell=cell, targetQs=self.targetQs,\n",
    "            g_initial_state=self.g_initial_state, d_initial_state=self.d_initial_state)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1111, 4) actions:(1111,)\n",
      "action size:2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 128               # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n",
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01247583, -0.00584241,  0.01096915,  0.03930041]),\n",
       " 0,\n",
       " array([ 0.01235898, -0.20111993,  0.01175516,  0.33542393]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:9.0000 rate:0.0180 gloss:0.7377 dloss:1.6647 dlossQ:0.9935 dlossQsigm:0.6712\n",
      "Episode:1 meanR:13.0000 rate:0.0340 gloss:0.8930 dloss:1.5136 dlossQ:0.9269 dlossQsigm:0.5866\n",
      "Episode:2 meanR:14.0000 rate:0.0320 gloss:1.2566 dloss:1.1777 dlossQ:0.7704 dlossQsigm:0.4072\n",
      "Episode:3 meanR:18.2500 rate:0.0620 gloss:1.8390 dloss:1.2642 dlossQ:1.0526 dlossQsigm:0.2116\n",
      "Episode:4 meanR:22.8000 rate:0.0820 gloss:3.1333 dloss:2.2828 dlossQ:2.1870 dlossQsigm:0.0958\n",
      "Episode:5 meanR:22.0000 rate:0.0360 gloss:3.3645 dloss:2.6228 dlossQ:2.4820 dlossQsigm:0.1408\n",
      "Episode:6 meanR:21.1429 rate:0.0320 gloss:3.1622 dloss:2.4589 dlossQ:2.3603 dlossQsigm:0.0986\n",
      "Episode:7 meanR:23.3750 rate:0.0780 gloss:3.6771 dloss:3.0989 dlossQ:3.0086 dlossQsigm:0.0902\n",
      "Episode:8 meanR:24.8889 rate:0.0740 gloss:2.2959 dloss:3.7846 dlossQ:3.6749 dlossQsigm:0.1097\n",
      "Episode:9 meanR:24.8000 rate:0.0480 gloss:1.6948 dloss:5.6669 dlossQ:5.5159 dlossQsigm:0.1510\n",
      "Episode:10 meanR:24.6364 rate:0.0460 gloss:0.8844 dloss:5.9479 dlossQ:5.8132 dlossQsigm:0.1347\n",
      "Episode:11 meanR:24.5833 rate:0.0480 gloss:0.5971 dloss:7.2730 dlossQ:7.1275 dlossQsigm:0.1455\n",
      "Episode:12 meanR:25.7692 rate:0.0800 gloss:0.5705 dloss:6.2247 dlossQ:6.0897 dlossQsigm:0.1349\n",
      "Episode:13 meanR:25.5000 rate:0.0440 gloss:0.3283 dloss:5.2809 dlossQ:5.1570 dlossQsigm:0.1239\n",
      "Episode:14 meanR:25.4000 rate:0.0480 gloss:0.2028 dloss:4.5181 dlossQ:4.4039 dlossQsigm:0.1142\n",
      "Episode:15 meanR:26.8125 rate:0.0960 gloss:0.2324 dloss:2.8476 dlossQ:2.7695 dlossQsigm:0.0780\n",
      "Episode:16 meanR:27.0000 rate:0.0600 gloss:0.3111 dloss:3.3067 dlossQ:3.2114 dlossQsigm:0.0952\n",
      "Episode:17 meanR:27.6111 rate:0.0760 gloss:0.1465 dloss:2.9489 dlossQ:2.8715 dlossQsigm:0.0774\n",
      "Episode:18 meanR:28.2105 rate:0.0780 gloss:0.0545 dloss:2.6699 dlossQ:2.5975 dlossQsigm:0.0723\n",
      "Episode:19 meanR:28.5000 rate:0.0680 gloss:0.0103 dloss:0.9617 dlossQ:0.9125 dlossQsigm:0.0492\n",
      "Episode:20 meanR:28.9048 rate:0.0740 gloss:0.0041 dloss:0.7688 dlossQ:0.7250 dlossQsigm:0.0438\n",
      "Episode:21 meanR:28.8636 rate:0.0560 gloss:0.0028 dloss:0.5614 dlossQ:0.5224 dlossQsigm:0.0391\n",
      "Episode:22 meanR:30.0000 rate:0.1100 gloss:0.0104 dloss:0.7070 dlossQ:0.6675 dlossQsigm:0.0395\n",
      "Episode:23 meanR:30.2083 rate:0.0700 gloss:0.0067 dloss:0.8316 dlossQ:0.7886 dlossQsigm:0.0430\n",
      "Episode:24 meanR:30.8000 rate:0.0900 gloss:0.0616 dloss:1.1251 dlossQ:1.0831 dlossQsigm:0.0421\n",
      "Episode:25 meanR:31.1923 rate:0.0820 gloss:0.0272 dloss:0.7409 dlossQ:0.7021 dlossQsigm:0.0388\n",
      "Episode:26 meanR:31.5926 rate:0.0840 gloss:0.1335 dloss:1.1401 dlossQ:1.1087 dlossQsigm:0.0315\n",
      "Episode:27 meanR:31.9643 rate:0.0840 gloss:0.0138 dloss:1.2255 dlossQ:1.1710 dlossQsigm:0.0545\n",
      "Episode:28 meanR:31.8966 rate:0.0600 gloss:0.0067 dloss:0.7985 dlossQ:0.7611 dlossQsigm:0.0374\n",
      "Episode:29 meanR:31.8667 rate:0.0620 gloss:0.0016 dloss:0.4302 dlossQ:0.3920 dlossQsigm:0.0382\n",
      "Episode:30 meanR:32.0645 rate:0.0760 gloss:0.0086 dloss:0.6418 dlossQ:0.6008 dlossQsigm:0.0410\n",
      "Episode:31 meanR:34.0312 rate:0.1900 gloss:0.1313 dloss:3.1651 dlossQ:3.1146 dlossQsigm:0.0505\n",
      "Episode:32 meanR:33.6667 rate:0.0440 gloss:0.0246 dloss:4.2247 dlossQ:4.1506 dlossQsigm:0.0741\n",
      "Episode:33 meanR:33.3824 rate:0.0480 gloss:0.0167 dloss:4.5402 dlossQ:4.4581 dlossQsigm:0.0821\n",
      "Episode:34 meanR:34.0286 rate:0.1120 gloss:0.0061 dloss:2.6986 dlossQ:2.6456 dlossQsigm:0.0530\n",
      "Episode:35 meanR:34.2778 rate:0.0860 gloss:0.0075 dloss:2.4914 dlossQ:2.4473 dlossQsigm:0.0441\n",
      "Episode:36 meanR:34.1351 rate:0.0580 gloss:0.0894 dloss:3.4332 dlossQ:3.3839 dlossQsigm:0.0493\n",
      "Episode:37 meanR:34.1579 rate:0.0700 gloss:0.0069 dloss:3.9014 dlossQ:3.8111 dlossQsigm:0.0903\n",
      "Episode:38 meanR:34.2564 rate:0.0760 gloss:0.0056 dloss:1.9052 dlossQ:1.8455 dlossQsigm:0.0597\n",
      "Episode:39 meanR:34.0000 rate:0.0480 gloss:0.0055 dloss:1.3393 dlossQ:1.2824 dlossQsigm:0.0570\n",
      "Episode:40 meanR:34.3415 rate:0.0960 gloss:0.0090 dloss:1.9250 dlossQ:1.8699 dlossQsigm:0.0550\n",
      "Episode:41 meanR:34.5000 rate:0.0820 gloss:0.0059 dloss:1.5988 dlossQ:1.5520 dlossQsigm:0.0468\n",
      "Episode:42 meanR:34.4884 rate:0.0680 gloss:0.0149 dloss:1.8286 dlossQ:1.7797 dlossQsigm:0.0489\n",
      "Episode:43 meanR:34.7500 rate:0.0920 gloss:0.0445 dloss:2.0821 dlossQ:2.0456 dlossQsigm:0.0365\n",
      "Episode:44 meanR:35.4000 rate:0.1280 gloss:0.0071 dloss:0.9166 dlossQ:0.8850 dlossQsigm:0.0316\n",
      "Episode:45 meanR:35.3261 rate:0.0640 gloss:0.0121 dloss:1.4657 dlossQ:1.4010 dlossQsigm:0.0647\n",
      "Episode:46 meanR:35.6596 rate:0.1020 gloss:0.0104 dloss:1.4060 dlossQ:1.3591 dlossQsigm:0.0469\n",
      "Episode:47 meanR:36.2500 rate:0.1280 gloss:0.0032 dloss:1.7473 dlossQ:1.7022 dlossQsigm:0.0451\n",
      "Episode:48 meanR:36.3061 rate:0.0780 gloss:0.0020 dloss:1.5984 dlossQ:1.5534 dlossQsigm:0.0449\n",
      "Episode:49 meanR:37.5200 rate:0.1940 gloss:0.0024 dloss:0.9838 dlossQ:0.9578 dlossQsigm:0.0261\n",
      "Episode:50 meanR:37.6471 rate:0.0880 gloss:0.0009 dloss:0.9292 dlossQ:0.9099 dlossQsigm:0.0193\n",
      "Episode:51 meanR:38.0769 rate:0.1200 gloss:0.0035 dloss:1.6462 dlossQ:1.6119 dlossQsigm:0.0343\n",
      "Episode:52 meanR:37.9057 rate:0.0580 gloss:0.0067 dloss:2.1746 dlossQ:2.1395 dlossQsigm:0.0351\n",
      "Episode:53 meanR:37.7963 rate:0.0640 gloss:0.0020 dloss:1.2658 dlossQ:1.2227 dlossQsigm:0.0431\n",
      "Episode:54 meanR:37.5636 rate:0.0500 gloss:0.0041 dloss:1.8232 dlossQ:1.7837 dlossQsigm:0.0394\n",
      "Episode:55 meanR:37.3929 rate:0.0560 gloss:0.0030 dloss:1.1364 dlossQ:1.0871 dlossQsigm:0.0493\n",
      "Episode:56 meanR:37.5263 rate:0.0900 gloss:0.0032 dloss:1.9138 dlossQ:1.8750 dlossQsigm:0.0388\n",
      "Episode:57 meanR:37.2586 rate:0.0440 gloss:0.0021 dloss:0.9658 dlossQ:0.9213 dlossQsigm:0.0445\n",
      "Episode:58 meanR:37.4746 rate:0.1000 gloss:0.0010 dloss:0.8358 dlossQ:0.7991 dlossQsigm:0.0368\n",
      "Episode:59 meanR:37.3167 rate:0.0560 gloss:0.0007 dloss:0.6690 dlossQ:0.6321 dlossQsigm:0.0368\n",
      "Episode:60 meanR:38.0328 rate:0.1620 gloss:0.0006 dloss:1.1620 dlossQ:1.1321 dlossQsigm:0.0298\n",
      "Episode:61 meanR:38.0806 rate:0.0820 gloss:0.0012 dloss:1.3844 dlossQ:1.3559 dlossQsigm:0.0285\n",
      "Episode:62 meanR:37.8730 rate:0.0500 gloss:0.0007 dloss:1.0247 dlossQ:0.9926 dlossQsigm:0.0321\n",
      "Episode:63 meanR:37.8438 rate:0.0720 gloss:0.0007 dloss:1.3532 dlossQ:1.3147 dlossQsigm:0.0385\n",
      "Episode:64 meanR:37.8615 rate:0.0780 gloss:0.0017 dloss:2.2776 dlossQ:2.2419 dlossQsigm:0.0357\n",
      "Episode:65 meanR:38.1061 rate:0.1080 gloss:0.0087 dloss:2.9476 dlossQ:2.9112 dlossQsigm:0.0364\n",
      "Episode:66 meanR:39.0299 rate:0.2000 gloss:0.0009 dloss:1.7080 dlossQ:1.6735 dlossQsigm:0.0345\n",
      "Episode:67 meanR:39.6471 rate:0.1620 gloss:0.0062 dloss:3.0788 dlossQ:3.0376 dlossQsigm:0.0411\n",
      "Episode:68 meanR:39.7246 rate:0.0900 gloss:0.0008 dloss:3.8175 dlossQ:3.7875 dlossQsigm:0.0300\n",
      "Episode:69 meanR:39.7143 rate:0.0780 gloss:0.0004 dloss:5.5563 dlossQ:5.4515 dlossQsigm:0.1049\n",
      "Episode:70 meanR:39.5352 rate:0.0540 gloss:0.0003 dloss:4.2422 dlossQ:4.1210 dlossQsigm:0.1212\n",
      "Episode:71 meanR:39.3750 rate:0.0560 gloss:0.0010 dloss:3.2527 dlossQ:3.1462 dlossQsigm:0.1066\n",
      "Episode:72 meanR:39.3014 rate:0.0680 gloss:0.0395 dloss:6.9549 dlossQ:6.8414 dlossQsigm:0.1135\n",
      "Episode:73 meanR:39.1757 rate:0.0600 gloss:0.0003 dloss:3.4068 dlossQ:3.2903 dlossQsigm:0.1165\n",
      "Episode:74 meanR:39.3467 rate:0.1040 gloss:0.0004 dloss:0.9839 dlossQ:0.9404 dlossQsigm:0.0435\n",
      "Episode:75 meanR:39.6053 rate:0.1180 gloss:0.0003 dloss:1.1595 dlossQ:1.1243 dlossQsigm:0.0352\n",
      "Episode:76 meanR:39.4805 rate:0.0600 gloss:0.0014 dloss:0.7052 dlossQ:0.6763 dlossQsigm:0.0289\n",
      "Episode:77 meanR:39.4615 rate:0.0760 gloss:0.0015 dloss:0.7067 dlossQ:0.6703 dlossQsigm:0.0363\n",
      "Episode:78 meanR:39.3418 rate:0.0600 gloss:0.0007 dloss:1.0700 dlossQ:1.0201 dlossQsigm:0.0499\n",
      "Episode:79 meanR:39.3625 rate:0.0820 gloss:0.0057 dloss:1.0857 dlossQ:1.0446 dlossQsigm:0.0411\n",
      "Episode:80 meanR:39.3827 rate:0.0820 gloss:0.0059 dloss:1.0735 dlossQ:1.0235 dlossQsigm:0.0499\n",
      "Episode:81 meanR:39.4268 rate:0.0860 gloss:0.0094 dloss:2.0041 dlossQ:1.8824 dlossQsigm:0.1217\n",
      "Episode:82 meanR:40.0241 rate:0.1780 gloss:0.0024 dloss:1.7689 dlossQ:1.7189 dlossQsigm:0.0500\n",
      "Episode:83 meanR:39.8571 rate:0.0520 gloss:-0.0268 dloss:2.2144 dlossQ:2.1378 dlossQsigm:0.0766\n",
      "Episode:84 meanR:40.0118 rate:0.1060 gloss:0.0010 dloss:4.0511 dlossQ:3.9837 dlossQsigm:0.0674\n",
      "Episode:85 meanR:39.8837 rate:0.0580 gloss:0.0168 dloss:1.9451 dlossQ:1.9006 dlossQsigm:0.0445\n",
      "Episode:86 meanR:39.8851 rate:0.0800 gloss:0.0065 dloss:1.4143 dlossQ:1.3553 dlossQsigm:0.0590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:87 meanR:39.8295 rate:0.0700 gloss:0.0056 dloss:2.1132 dlossQ:2.0434 dlossQsigm:0.0698\n",
      "Episode:88 meanR:39.6517 rate:0.0480 gloss:0.0198 dloss:3.2899 dlossQ:3.2168 dlossQsigm:0.0731\n",
      "Episode:89 meanR:39.6556 rate:0.0800 gloss:0.0018 dloss:1.5911 dlossQ:1.5243 dlossQsigm:0.0668\n",
      "Episode:90 meanR:40.3626 rate:0.2080 gloss:0.0003 dloss:1.2967 dlossQ:1.2730 dlossQsigm:0.0237\n",
      "Episode:91 meanR:40.1630 rate:0.0440 gloss:0.0003 dloss:1.9205 dlossQ:1.8799 dlossQsigm:0.0406\n",
      "Episode:92 meanR:40.3441 rate:0.1140 gloss:0.0001 dloss:1.0107 dlossQ:0.9810 dlossQsigm:0.0297\n",
      "Episode:93 meanR:40.4362 rate:0.0980 gloss:0.0025 dloss:2.3838 dlossQ:2.3457 dlossQsigm:0.0381\n",
      "Episode:94 meanR:40.4105 rate:0.0760 gloss:0.0017 dloss:1.6016 dlossQ:1.5616 dlossQsigm:0.0400\n",
      "Episode:95 meanR:40.2812 rate:0.0560 gloss:0.0011 dloss:1.3279 dlossQ:1.2709 dlossQsigm:0.0570\n",
      "Episode:96 meanR:40.1443 rate:0.0540 gloss:0.0010 dloss:0.7708 dlossQ:0.7229 dlossQsigm:0.0479\n",
      "Episode:97 meanR:40.3776 rate:0.1260 gloss:0.0143 dloss:0.7178 dlossQ:0.6753 dlossQsigm:0.0425\n",
      "Episode:98 meanR:40.1818 rate:0.0420 gloss:0.0028 dloss:0.9510 dlossQ:0.9067 dlossQsigm:0.0443\n",
      "Episode:99 meanR:40.2400 rate:0.0920 gloss:0.0077 dloss:1.8764 dlossQ:1.8401 dlossQsigm:0.0363\n",
      "Episode:100 meanR:40.4800 rate:0.0660 gloss:0.0161 dloss:2.2501 dlossQ:2.2133 dlossQsigm:0.0369\n",
      "Episode:101 meanR:40.8300 rate:0.1040 gloss:0.0088 dloss:3.0509 dlossQ:3.0191 dlossQsigm:0.0318\n",
      "Episode:102 meanR:40.9400 rate:0.0540 gloss:0.0047 dloss:1.8545 dlossQ:1.7798 dlossQsigm:0.0746\n",
      "Episode:103 meanR:40.9300 rate:0.0600 gloss:0.0026 dloss:1.9000 dlossQ:1.8489 dlossQsigm:0.0511\n",
      "Episode:104 meanR:41.1000 rate:0.1160 gloss:0.0004 dloss:1.2883 dlossQ:1.2430 dlossQsigm:0.0454\n",
      "Episode:105 meanR:41.2100 rate:0.0580 gloss:0.0003 dloss:0.8693 dlossQ:0.7830 dlossQsigm:0.0863\n",
      "Episode:106 meanR:42.0200 rate:0.1940 gloss:0.0156 dloss:0.9854 dlossQ:0.9562 dlossQsigm:0.0292\n",
      "Episode:107 meanR:41.9100 rate:0.0560 gloss:0.0025 dloss:1.3220 dlossQ:1.2506 dlossQsigm:0.0714\n",
      "Episode:108 meanR:41.8600 rate:0.0640 gloss:0.0013 dloss:1.3087 dlossQ:1.2736 dlossQsigm:0.0351\n",
      "Episode:109 meanR:42.0200 rate:0.0800 gloss:0.0010 dloss:1.6382 dlossQ:1.5983 dlossQsigm:0.0399\n",
      "Episode:110 meanR:42.1800 rate:0.0780 gloss:0.0001 dloss:0.9862 dlossQ:0.9331 dlossQsigm:0.0531\n",
      "Episode:111 meanR:42.3900 rate:0.0900 gloss:0.0002 dloss:1.6648 dlossQ:1.6091 dlossQsigm:0.0557\n",
      "Episode:112 meanR:42.6300 rate:0.1280 gloss:0.0027 dloss:1.6074 dlossQ:1.5746 dlossQsigm:0.0328\n",
      "Episode:113 meanR:42.6100 rate:0.0400 gloss:0.0018 dloss:2.4924 dlossQ:2.4558 dlossQsigm:0.0366\n",
      "Episode:114 meanR:42.6300 rate:0.0520 gloss:0.0012 dloss:1.9802 dlossQ:1.9363 dlossQsigm:0.0439\n",
      "Episode:115 meanR:42.4800 rate:0.0660 gloss:0.0009 dloss:0.7862 dlossQ:0.7302 dlossQsigm:0.0560\n",
      "Episode:116 meanR:42.5100 rate:0.0660 gloss:0.0008 dloss:1.0138 dlossQ:0.8554 dlossQsigm:0.1584\n",
      "Episode:117 meanR:42.4600 rate:0.0660 gloss:0.0001 dloss:1.1281 dlossQ:0.7964 dlossQsigm:0.3316\n",
      "Episode:118 meanR:42.3100 rate:0.0480 gloss:0.0001 dloss:1.1716 dlossQ:1.1155 dlossQsigm:0.0561\n",
      "Episode:119 meanR:42.2100 rate:0.0480 gloss:0.0001 dloss:2.4521 dlossQ:2.3710 dlossQsigm:0.0811\n",
      "Episode:120 meanR:42.2500 rate:0.0820 gloss:0.0000 dloss:1.0768 dlossQ:1.0249 dlossQsigm:0.0519\n",
      "Episode:121 meanR:42.3600 rate:0.0780 gloss:0.0004 dloss:1.1257 dlossQ:1.0878 dlossQsigm:0.0379\n",
      "Episode:122 meanR:42.0800 rate:0.0540 gloss:0.0000 dloss:0.5705 dlossQ:0.5192 dlossQsigm:0.0513\n",
      "Episode:123 meanR:41.9500 rate:0.0440 gloss:0.0000 dloss:2.0237 dlossQ:1.8568 dlossQsigm:0.1669\n",
      "Episode:124 meanR:41.8200 rate:0.0640 gloss:0.0000 dloss:0.6160 dlossQ:0.5620 dlossQsigm:0.0540\n",
      "Episode:125 meanR:41.7100 rate:0.0600 gloss:0.0000 dloss:0.5526 dlossQ:0.4843 dlossQsigm:0.0683\n",
      "Episode:126 meanR:41.6400 rate:0.0700 gloss:0.0003 dloss:1.1078 dlossQ:1.0015 dlossQsigm:0.1063\n",
      "Episode:127 meanR:41.8000 rate:0.1160 gloss:0.0016 dloss:1.1881 dlossQ:1.1225 dlossQsigm:0.0656\n",
      "Episode:128 meanR:41.7400 rate:0.0480 gloss:0.0003 dloss:0.9567 dlossQ:0.8923 dlossQsigm:0.0644\n",
      "Episode:129 meanR:41.8800 rate:0.0900 gloss:0.0007 dloss:1.1775 dlossQ:1.1028 dlossQsigm:0.0747\n",
      "Episode:130 meanR:41.7200 rate:0.0440 gloss:0.0002 dloss:1.2927 dlossQ:1.1353 dlossQsigm:0.1574\n",
      "Episode:131 meanR:41.1100 rate:0.0680 gloss:0.0000 dloss:1.6024 dlossQ:1.2813 dlossQsigm:0.3210\n",
      "Episode:132 meanR:41.5900 rate:0.1400 gloss:0.0016 dloss:0.6157 dlossQ:0.5714 dlossQsigm:0.0443\n",
      "Episode:133 meanR:41.9500 rate:0.1200 gloss:0.0027 dloss:1.2945 dlossQ:1.2767 dlossQsigm:0.0178\n",
      "Episode:134 meanR:41.9600 rate:0.1140 gloss:0.0008 dloss:0.6546 dlossQ:0.6272 dlossQsigm:0.0273\n",
      "Episode:135 meanR:41.9800 rate:0.0900 gloss:0.0005 dloss:0.8434 dlossQ:0.8063 dlossQsigm:0.0371\n",
      "Episode:136 meanR:41.9000 rate:0.0420 gloss:0.0004 dloss:1.0683 dlossQ:1.0261 dlossQsigm:0.0422\n",
      "Episode:137 meanR:42.0000 rate:0.0900 gloss:0.0019 dloss:1.5608 dlossQ:1.5099 dlossQsigm:0.0509\n",
      "Episode:138 meanR:41.8400 rate:0.0440 gloss:0.0003 dloss:1.4066 dlossQ:1.3601 dlossQsigm:0.0464\n",
      "Episode:139 meanR:42.0400 rate:0.0880 gloss:0.0002 dloss:0.8821 dlossQ:0.7330 dlossQsigm:0.1491\n",
      "Episode:140 meanR:41.9500 rate:0.0780 gloss:0.0006 dloss:0.9262 dlossQ:0.7753 dlossQsigm:0.1509\n",
      "Episode:141 meanR:41.7800 rate:0.0480 gloss:0.0005 dloss:1.0592 dlossQ:0.8835 dlossQsigm:0.1757\n",
      "Episode:142 meanR:41.7200 rate:0.0560 gloss:0.0004 dloss:1.3045 dlossQ:1.2621 dlossQsigm:0.0423\n",
      "Episode:143 meanR:41.5300 rate:0.0540 gloss:0.0004 dloss:1.1035 dlossQ:1.0486 dlossQsigm:0.0549\n",
      "Episode:144 meanR:41.3400 rate:0.0900 gloss:0.0002 dloss:0.8097 dlossQ:0.4828 dlossQsigm:0.3269\n",
      "Episode:145 meanR:41.3600 rate:0.0680 gloss:0.0000 dloss:1.3225 dlossQ:1.2699 dlossQsigm:0.0525\n",
      "Episode:146 meanR:41.1100 rate:0.0520 gloss:0.0000 dloss:1.2510 dlossQ:1.2023 dlossQsigm:0.0487\n",
      "Episode:147 meanR:40.8800 rate:0.0820 gloss:0.0014 dloss:1.3153 dlossQ:1.2757 dlossQsigm:0.0396\n",
      "Episode:148 meanR:40.8000 rate:0.0620 gloss:0.0002 dloss:1.2352 dlossQ:1.1845 dlossQsigm:0.0508\n",
      "Episode:149 meanR:40.3300 rate:0.1000 gloss:0.0002 dloss:1.1242 dlossQ:1.0832 dlossQsigm:0.0410\n",
      "Episode:150 meanR:40.3300 rate:0.0880 gloss:0.0003 dloss:1.2550 dlossQ:1.1972 dlossQsigm:0.0578\n",
      "Episode:151 meanR:39.9800 rate:0.0500 gloss:0.0002 dloss:0.9545 dlossQ:0.8996 dlossQsigm:0.0548\n",
      "Episode:152 meanR:40.1300 rate:0.0880 gloss:0.0001 dloss:1.3913 dlossQ:1.3381 dlossQsigm:0.0532\n",
      "Episode:153 meanR:40.0900 rate:0.0560 gloss:0.0096 dloss:1.1575 dlossQ:1.0933 dlossQsigm:0.0642\n",
      "Episode:154 meanR:40.1600 rate:0.0640 gloss:0.0357 dloss:1.0331 dlossQ:0.9965 dlossQsigm:0.0366\n",
      "Episode:155 meanR:40.2400 rate:0.0720 gloss:0.0015 dloss:1.1716 dlossQ:1.1324 dlossQsigm:0.0392\n",
      "Episode:156 meanR:40.1200 rate:0.0660 gloss:0.0003 dloss:1.1868 dlossQ:1.1458 dlossQsigm:0.0411\n",
      "Episode:157 meanR:40.1800 rate:0.0560 gloss:0.1602 dloss:1.4323 dlossQ:1.3895 dlossQsigm:0.0429\n",
      "Episode:158 meanR:40.0700 rate:0.0780 gloss:0.0005 dloss:0.7560 dlossQ:0.7112 dlossQsigm:0.0447\n",
      "Episode:159 meanR:40.0700 rate:0.0560 gloss:0.0544 dloss:1.3964 dlossQ:1.3611 dlossQsigm:0.0353\n",
      "Episode:160 meanR:40.1200 rate:0.1720 gloss:0.0016 dloss:1.1552 dlossQ:1.1232 dlossQsigm:0.0320\n",
      "Episode:161 meanR:39.9400 rate:0.0460 gloss:0.0009 dloss:1.7097 dlossQ:1.6660 dlossQsigm:0.0437\n",
      "Episode:162 meanR:39.9300 rate:0.0480 gloss:0.0005 dloss:2.3788 dlossQ:2.2502 dlossQsigm:0.1285\n",
      "Episode:163 meanR:39.8900 rate:0.0640 gloss:0.0004 dloss:1.4790 dlossQ:1.2732 dlossQsigm:0.2057\n",
      "Episode:164 meanR:40.2900 rate:0.1580 gloss:0.0044 dloss:2.1909 dlossQ:2.1608 dlossQsigm:0.0301\n",
      "Episode:165 meanR:39.9700 rate:0.0440 gloss:0.0006 dloss:2.3340 dlossQ:2.2938 dlossQsigm:0.0403\n",
      "Episode:166 meanR:39.3700 rate:0.0800 gloss:0.0005 dloss:1.8582 dlossQ:1.8248 dlossQsigm:0.0334\n",
      "Episode:167 meanR:38.9300 rate:0.0740 gloss:0.0017 dloss:1.9140 dlossQ:1.8636 dlossQsigm:0.0504\n",
      "Episode:168 meanR:38.8300 rate:0.0700 gloss:0.0292 dloss:3.1713 dlossQ:3.0616 dlossQsigm:0.1097\n",
      "Episode:169 meanR:38.7400 rate:0.0600 gloss:0.0265 dloss:2.9536 dlossQ:2.8237 dlossQsigm:0.1299\n",
      "Episode:170 meanR:38.8400 rate:0.0740 gloss:0.0253 dloss:6.1258 dlossQ:6.0111 dlossQsigm:0.1147\n",
      "Episode:171 meanR:38.9200 rate:0.0720 gloss:0.0544 dloss:1.6494 dlossQ:1.5971 dlossQsigm:0.0524\n",
      "Episode:172 meanR:38.8600 rate:0.0560 gloss:0.0550 dloss:1.5127 dlossQ:1.4607 dlossQsigm:0.0520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:173 meanR:38.8200 rate:0.0520 gloss:0.0078 dloss:1.6631 dlossQ:1.5922 dlossQsigm:0.0708\n",
      "Episode:174 meanR:38.7900 rate:0.0980 gloss:0.0159 dloss:1.4166 dlossQ:1.3746 dlossQsigm:0.0420\n",
      "Episode:175 meanR:38.6200 rate:0.0840 gloss:0.0094 dloss:0.5346 dlossQ:0.4970 dlossQsigm:0.0376\n",
      "Episode:176 meanR:39.0900 rate:0.1540 gloss:0.0148 dloss:1.5901 dlossQ:1.5567 dlossQsigm:0.0333\n",
      "Episode:177 meanR:39.0400 rate:0.0660 gloss:0.0040 dloss:1.5295 dlossQ:1.4953 dlossQsigm:0.0342\n",
      "Episode:178 meanR:39.2000 rate:0.0920 gloss:0.0020 dloss:1.4643 dlossQ:1.3729 dlossQsigm:0.0914\n",
      "Episode:179 meanR:39.1900 rate:0.0800 gloss:0.0014 dloss:1.0598 dlossQ:1.0111 dlossQsigm:0.0487\n",
      "Episode:180 meanR:39.1300 rate:0.0700 gloss:0.0056 dloss:0.7033 dlossQ:0.6690 dlossQsigm:0.0343\n",
      "Episode:181 meanR:39.2400 rate:0.1080 gloss:0.0065 dloss:4.1721 dlossQ:4.1390 dlossQsigm:0.0332\n",
      "Episode:182 meanR:38.6100 rate:0.0520 gloss:0.0057 dloss:2.7727 dlossQ:2.7362 dlossQsigm:0.0365\n",
      "Episode:183 meanR:38.7900 rate:0.0880 gloss:0.0012 dloss:2.2010 dlossQ:2.1642 dlossQsigm:0.0368\n",
      "Episode:184 meanR:38.8700 rate:0.1220 gloss:0.0007 dloss:1.5351 dlossQ:1.5046 dlossQsigm:0.0306\n",
      "Episode:185 meanR:39.0300 rate:0.0900 gloss:0.0013 dloss:1.1888 dlossQ:1.1518 dlossQsigm:0.0370\n",
      "Episode:186 meanR:39.0000 rate:0.0740 gloss:0.7119 dloss:3.6994 dlossQ:3.6475 dlossQsigm:0.0519\n",
      "Episode:187 meanR:39.1000 rate:0.0900 gloss:0.0117 dloss:2.9442 dlossQ:2.8776 dlossQsigm:0.0666\n",
      "Episode:188 meanR:39.3600 rate:0.1000 gloss:0.0024 dloss:1.9539 dlossQ:1.9039 dlossQsigm:0.0500\n",
      "Episode:189 meanR:39.3100 rate:0.0700 gloss:0.0048 dloss:1.4467 dlossQ:1.4109 dlossQsigm:0.0357\n",
      "Episode:190 meanR:38.5000 rate:0.0460 gloss:0.0032 dloss:1.5557 dlossQ:1.5174 dlossQsigm:0.0383\n",
      "Episode:191 meanR:38.7100 rate:0.0860 gloss:0.0308 dloss:1.1130 dlossQ:1.0772 dlossQsigm:0.0359\n",
      "Episode:192 meanR:38.4000 rate:0.0520 gloss:0.0455 dloss:1.0412 dlossQ:0.9941 dlossQsigm:0.0471\n",
      "Episode:193 meanR:38.1500 rate:0.0480 gloss:0.0289 dloss:1.1978 dlossQ:1.1472 dlossQsigm:0.0506\n",
      "Episode:194 meanR:38.0400 rate:0.0540 gloss:0.0264 dloss:1.4028 dlossQ:1.3456 dlossQsigm:0.0572\n",
      "Episode:195 meanR:38.0200 rate:0.0520 gloss:0.0171 dloss:0.6297 dlossQ:0.5805 dlossQsigm:0.0493\n",
      "Episode:196 meanR:38.0800 rate:0.0660 gloss:0.0449 dloss:0.6946 dlossQ:0.6473 dlossQsigm:0.0473\n",
      "Episode:197 meanR:37.9600 rate:0.1020 gloss:0.0132 dloss:0.6979 dlossQ:0.6563 dlossQsigm:0.0416\n",
      "Episode:198 meanR:38.2100 rate:0.0920 gloss:0.0093 dloss:1.0337 dlossQ:1.0069 dlossQsigm:0.0268\n",
      "Episode:199 meanR:38.1200 rate:0.0740 gloss:0.1145 dloss:1.7202 dlossQ:1.6936 dlossQsigm:0.0266\n",
      "Episode:200 meanR:38.1600 rate:0.0740 gloss:0.0430 dloss:3.7605 dlossQ:3.6815 dlossQsigm:0.0791\n",
      "Episode:201 meanR:37.9400 rate:0.0600 gloss:0.0200 dloss:3.3024 dlossQ:3.2068 dlossQsigm:0.0956\n",
      "Episode:202 meanR:37.8800 rate:0.0420 gloss:0.0147 dloss:2.5503 dlossQ:2.4788 dlossQsigm:0.0715\n",
      "Episode:203 meanR:37.9400 rate:0.0720 gloss:0.0025 dloss:2.4352 dlossQ:2.3789 dlossQsigm:0.0563\n",
      "Episode:204 meanR:37.7600 rate:0.0800 gloss:0.0130 dloss:1.9104 dlossQ:1.8538 dlossQsigm:0.0566\n",
      "Episode:205 meanR:37.8000 rate:0.0660 gloss:0.0378 dloss:2.0218 dlossQ:1.9721 dlossQsigm:0.0497\n",
      "Episode:206 meanR:37.1100 rate:0.0560 gloss:0.0251 dloss:1.8011 dlossQ:1.7569 dlossQsigm:0.0442\n",
      "Episode:207 meanR:37.1700 rate:0.0680 gloss:0.0071 dloss:1.3013 dlossQ:1.2654 dlossQsigm:0.0359\n",
      "Episode:208 meanR:37.2500 rate:0.0800 gloss:0.0180 dloss:1.3756 dlossQ:1.3222 dlossQsigm:0.0535\n",
      "Episode:209 meanR:37.1900 rate:0.0680 gloss:0.0040 dloss:1.7218 dlossQ:1.6193 dlossQsigm:0.1024\n",
      "Episode:210 meanR:37.3300 rate:0.1060 gloss:0.0311 dloss:3.2576 dlossQ:3.2151 dlossQsigm:0.0425\n",
      "Episode:211 meanR:37.2800 rate:0.0800 gloss:0.0166 dloss:2.6452 dlossQ:2.5193 dlossQsigm:0.1259\n",
      "Episode:212 meanR:37.0500 rate:0.0820 gloss:0.0117 dloss:2.4211 dlossQ:2.3073 dlossQsigm:0.1137\n",
      "Episode:213 meanR:37.2300 rate:0.0760 gloss:0.0239 dloss:2.9369 dlossQ:2.8934 dlossQsigm:0.0435\n",
      "Episode:214 meanR:37.2700 rate:0.0600 gloss:0.0707 dloss:2.0499 dlossQ:2.0138 dlossQsigm:0.0361\n",
      "Episode:215 meanR:37.2800 rate:0.0680 gloss:0.0043 dloss:2.3096 dlossQ:2.1116 dlossQsigm:0.1979\n",
      "Episode:216 meanR:37.2600 rate:0.0620 gloss:0.0056 dloss:1.3182 dlossQ:1.2649 dlossQsigm:0.0533\n",
      "Episode:217 meanR:37.4600 rate:0.1060 gloss:0.0263 dloss:0.9068 dlossQ:0.8570 dlossQsigm:0.0499\n",
      "Episode:218 meanR:37.5600 rate:0.0680 gloss:0.0064 dloss:1.2067 dlossQ:1.1624 dlossQsigm:0.0443\n",
      "Episode:219 meanR:37.5400 rate:0.0440 gloss:0.0048 dloss:1.0980 dlossQ:1.0562 dlossQsigm:0.0418\n",
      "Episode:220 meanR:37.3700 rate:0.0480 gloss:0.0023 dloss:1.4138 dlossQ:1.3668 dlossQsigm:0.0469\n",
      "Episode:221 meanR:38.0000 rate:0.2040 gloss:0.0001 dloss:0.7853 dlossQ:0.7574 dlossQsigm:0.0280\n",
      "Episode:222 meanR:38.2800 rate:0.1100 gloss:0.0035 dloss:1.4014 dlossQ:1.3813 dlossQsigm:0.0200\n",
      "Episode:223 meanR:38.4400 rate:0.0760 gloss:0.0025 dloss:1.3962 dlossQ:1.3625 dlossQsigm:0.0337\n",
      "Episode:224 meanR:38.3600 rate:0.0480 gloss:0.0024 dloss:1.2925 dlossQ:1.2401 dlossQsigm:0.0523\n",
      "Episode:225 meanR:38.4900 rate:0.0860 gloss:0.0165 dloss:0.8903 dlossQ:0.8556 dlossQsigm:0.0347\n",
      "Episode:226 meanR:38.4600 rate:0.0640 gloss:0.0057 dloss:1.2036 dlossQ:1.1612 dlossQsigm:0.0423\n",
      "Episode:227 meanR:38.1800 rate:0.0600 gloss:0.0123 dloss:0.9548 dlossQ:0.9189 dlossQsigm:0.0359\n",
      "Episode:228 meanR:38.1800 rate:0.0480 gloss:0.0046 dloss:1.3872 dlossQ:1.3334 dlossQsigm:0.0538\n",
      "Episode:229 meanR:38.4000 rate:0.1340 gloss:0.0059 dloss:1.3838 dlossQ:1.3206 dlossQsigm:0.0631\n",
      "Episode:230 meanR:38.6300 rate:0.0900 gloss:0.0046 dloss:1.6582 dlossQ:1.6289 dlossQsigm:0.0293\n",
      "Episode:231 meanR:38.7300 rate:0.0880 gloss:0.0015 dloss:6.6874 dlossQ:6.6132 dlossQsigm:0.0743\n",
      "Episode:232 meanR:38.2800 rate:0.0500 gloss:0.0119 dloss:3.5198 dlossQ:3.4620 dlossQsigm:0.0578\n",
      "Episode:233 meanR:38.2600 rate:0.1160 gloss:0.0004 dloss:2.0612 dlossQ:2.0259 dlossQsigm:0.0354\n",
      "Episode:234 meanR:37.9400 rate:0.0500 gloss:0.0006 dloss:0.9578 dlossQ:0.9137 dlossQsigm:0.0442\n",
      "Episode:235 meanR:37.9200 rate:0.0860 gloss:0.0015 dloss:2.8091 dlossQ:2.7609 dlossQsigm:0.0482\n",
      "Episode:236 meanR:38.0200 rate:0.0620 gloss:0.0007 dloss:2.3690 dlossQ:2.3188 dlossQsigm:0.0502\n",
      "Episode:237 meanR:37.8300 rate:0.0520 gloss:0.0006 dloss:1.8937 dlossQ:1.8450 dlossQsigm:0.0487\n",
      "Episode:238 meanR:38.0100 rate:0.0800 gloss:0.0052 dloss:2.7397 dlossQ:2.6980 dlossQsigm:0.0416\n",
      "Episode:239 meanR:37.8500 rate:0.0560 gloss:0.0015 dloss:1.3699 dlossQ:1.3015 dlossQsigm:0.0685\n",
      "Episode:240 meanR:37.9400 rate:0.0960 gloss:0.0102 dloss:2.5083 dlossQ:2.4600 dlossQsigm:0.0483\n",
      "Episode:241 meanR:37.9900 rate:0.0580 gloss:0.0031 dloss:2.9908 dlossQ:2.9373 dlossQsigm:0.0534\n",
      "Episode:242 meanR:38.0100 rate:0.0600 gloss:0.0010 dloss:2.3822 dlossQ:2.2578 dlossQsigm:0.1244\n",
      "Episode:243 meanR:38.4000 rate:0.1320 gloss:0.0031 dloss:1.9620 dlossQ:1.9056 dlossQsigm:0.0564\n",
      "Episode:244 meanR:38.4600 rate:0.1020 gloss:0.0017 dloss:1.4902 dlossQ:1.4547 dlossQsigm:0.0356\n",
      "Episode:245 meanR:38.4700 rate:0.0700 gloss:0.0078 dloss:1.2834 dlossQ:1.2481 dlossQsigm:0.0353\n",
      "Episode:246 meanR:38.7300 rate:0.1040 gloss:0.0033 dloss:0.9869 dlossQ:0.9575 dlossQsigm:0.0294\n",
      "Episode:247 meanR:38.7400 rate:0.0840 gloss:0.0029 dloss:2.2075 dlossQ:2.1520 dlossQsigm:0.0555\n",
      "Episode:248 meanR:39.1100 rate:0.1360 gloss:0.0046 dloss:1.6355 dlossQ:1.6028 dlossQsigm:0.0326\n",
      "Episode:249 meanR:38.8800 rate:0.0540 gloss:0.0005 dloss:1.9106 dlossQ:1.8788 dlossQsigm:0.0318\n",
      "Episode:250 meanR:38.8800 rate:0.0880 gloss:0.0011 dloss:2.6428 dlossQ:2.5865 dlossQsigm:0.0563\n",
      "Episode:251 meanR:38.8800 rate:0.0500 gloss:0.0003 dloss:1.6983 dlossQ:1.5799 dlossQsigm:0.1184\n",
      "Episode:252 meanR:39.0200 rate:0.1160 gloss:0.0015 dloss:1.5017 dlossQ:1.4135 dlossQsigm:0.0882\n",
      "Episode:253 meanR:39.0300 rate:0.0580 gloss:-0.0440 dloss:1.8922 dlossQ:1.6914 dlossQsigm:0.2008\n",
      "Episode:254 meanR:38.9100 rate:0.0400 gloss:-0.0237 dloss:1.7445 dlossQ:1.4837 dlossQsigm:0.2609\n",
      "Episode:255 meanR:38.8700 rate:0.0640 gloss:-0.0121 dloss:1.0857 dlossQ:0.8348 dlossQsigm:0.2510\n",
      "Episode:256 meanR:38.9600 rate:0.0840 gloss:0.0662 dloss:0.8572 dlossQ:0.7864 dlossQsigm:0.0708\n",
      "Episode:257 meanR:39.1500 rate:0.0940 gloss:0.0208 dloss:2.4202 dlossQ:2.3484 dlossQsigm:0.0718\n",
      "Episode:258 meanR:39.4500 rate:0.1380 gloss:0.0053 dloss:2.2282 dlossQ:2.1895 dlossQsigm:0.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:259 meanR:39.8600 rate:0.1380 gloss:0.0012 dloss:3.1884 dlossQ:3.1703 dlossQsigm:0.0181\n",
      "Episode:260 meanR:39.3400 rate:0.0680 gloss:0.0002 dloss:3.4971 dlossQ:3.4579 dlossQsigm:0.0392\n",
      "Episode:261 meanR:39.4100 rate:0.0600 gloss:0.0033 dloss:4.4855 dlossQ:4.4209 dlossQsigm:0.0646\n",
      "Episode:262 meanR:39.8900 rate:0.1440 gloss:0.0007 dloss:1.9184 dlossQ:1.8810 dlossQsigm:0.0374\n",
      "Episode:263 meanR:39.8600 rate:0.0580 gloss:0.0006 dloss:1.6807 dlossQ:1.6094 dlossQsigm:0.0713\n",
      "Episode:264 meanR:39.3000 rate:0.0460 gloss:0.0016 dloss:1.6266 dlossQ:1.5739 dlossQsigm:0.0527\n",
      "Episode:265 meanR:39.3000 rate:0.0440 gloss:0.0016 dloss:1.7119 dlossQ:1.6579 dlossQsigm:0.0540\n",
      "Episode:266 meanR:39.4600 rate:0.1120 gloss:0.0015 dloss:1.0093 dlossQ:0.9703 dlossQsigm:0.0390\n",
      "Episode:267 meanR:39.3500 rate:0.0520 gloss:0.0010 dloss:0.8975 dlossQ:0.8455 dlossQsigm:0.0520\n",
      "Episode:268 meanR:39.6800 rate:0.1360 gloss:0.0003 dloss:1.0211 dlossQ:0.9930 dlossQsigm:0.0281\n",
      "Episode:269 meanR:39.6900 rate:0.0620 gloss:0.0003 dloss:0.7306 dlossQ:0.6623 dlossQsigm:0.0683\n",
      "Episode:270 meanR:39.8400 rate:0.1040 gloss:0.0020 dloss:2.4799 dlossQ:2.4185 dlossQsigm:0.0614\n",
      "Episode:271 meanR:39.7100 rate:0.0460 gloss:0.0011 dloss:2.0734 dlossQ:1.9740 dlossQsigm:0.0994\n",
      "Episode:272 meanR:39.8500 rate:0.0840 gloss:0.0003 dloss:1.5764 dlossQ:1.4823 dlossQsigm:0.0941\n",
      "Episode:273 meanR:39.9600 rate:0.0740 gloss:-0.0153 dloss:2.5944 dlossQ:2.2009 dlossQsigm:0.3935\n",
      "Episode:274 meanR:39.8100 rate:0.0680 gloss:1.4744 dloss:1.1730 dlossQ:0.9998 dlossQsigm:0.1731\n",
      "Episode:275 meanR:39.8000 rate:0.0820 gloss:0.7474 dloss:2.6258 dlossQ:2.5573 dlossQsigm:0.0686\n",
      "Episode:276 meanR:39.2600 rate:0.0460 gloss:0.1890 dloss:1.3597 dlossQ:1.3155 dlossQsigm:0.0442\n",
      "Episode:277 meanR:39.3000 rate:0.0740 gloss:1.2223 dloss:1.3369 dlossQ:1.2890 dlossQsigm:0.0479\n",
      "Episode:278 meanR:39.6100 rate:0.1540 gloss:0.0519 dloss:2.3561 dlossQ:2.3058 dlossQsigm:0.0503\n",
      "Episode:279 meanR:40.0900 rate:0.1760 gloss:0.0056 dloss:3.2745 dlossQ:3.2384 dlossQsigm:0.0361\n",
      "Episode:280 meanR:40.3500 rate:0.1220 gloss:0.0011 dloss:3.3476 dlossQ:3.3125 dlossQsigm:0.0350\n",
      "Episode:281 meanR:40.3500 rate:0.1080 gloss:0.1114 dloss:4.0073 dlossQ:3.9428 dlossQsigm:0.0645\n",
      "Episode:282 meanR:40.3300 rate:0.0480 gloss:0.0548 dloss:4.1636 dlossQ:4.1094 dlossQsigm:0.0542\n",
      "Episode:283 meanR:40.1600 rate:0.0540 gloss:0.0279 dloss:4.2943 dlossQ:4.2205 dlossQsigm:0.0738\n",
      "Episode:284 meanR:39.8200 rate:0.0540 gloss:0.1472 dloss:3.7141 dlossQ:3.6547 dlossQsigm:0.0594\n",
      "Episode:285 meanR:39.6600 rate:0.0580 gloss:0.0104 dloss:2.9803 dlossQ:2.9203 dlossQsigm:0.0599\n",
      "Episode:286 meanR:40.6700 rate:0.2760 gloss:0.0141 dloss:2.9107 dlossQ:2.8925 dlossQsigm:0.0182\n",
      "Episode:287 meanR:40.5000 rate:0.0560 gloss:0.0047 dloss:3.4277 dlossQ:3.3833 dlossQsigm:0.0444\n",
      "Episode:288 meanR:40.4500 rate:0.0900 gloss:0.0015 dloss:5.6497 dlossQ:5.2421 dlossQsigm:0.4075\n",
      "Episode:289 meanR:40.3500 rate:0.0500 gloss:0.0012 dloss:5.3075 dlossQ:4.9946 dlossQsigm:0.3130\n",
      "Episode:290 meanR:40.6500 rate:0.1060 gloss:0.0040 dloss:1.3084 dlossQ:1.2600 dlossQsigm:0.0485\n",
      "Episode:291 meanR:40.5400 rate:0.0640 gloss:0.0073 dloss:1.0235 dlossQ:0.9819 dlossQsigm:0.0416\n",
      "Episode:292 meanR:40.5100 rate:0.0460 gloss:0.0130 dloss:0.9659 dlossQ:0.8678 dlossQsigm:0.0981\n",
      "Episode:293 meanR:40.6900 rate:0.0840 gloss:0.0061 dloss:1.5432 dlossQ:1.4985 dlossQsigm:0.0446\n",
      "Episode:294 meanR:40.7600 rate:0.0680 gloss:0.1163 dloss:1.3566 dlossQ:1.3120 dlossQsigm:0.0446\n",
      "Episode:295 meanR:41.0500 rate:0.1100 gloss:0.0084 dloss:1.2978 dlossQ:1.2608 dlossQsigm:0.0371\n",
      "Episode:296 meanR:42.0600 rate:0.2680 gloss:0.0022 dloss:3.4488 dlossQ:3.4308 dlossQsigm:0.0180\n",
      "Episode:297 meanR:42.1600 rate:0.1220 gloss:0.0056 dloss:3.2814 dlossQ:3.2486 dlossQsigm:0.0328\n",
      "Episode:298 meanR:42.3900 rate:0.1380 gloss:0.0028 dloss:1.9075 dlossQ:1.8798 dlossQsigm:0.0276\n",
      "Episode:299 meanR:42.4400 rate:0.0840 gloss:0.0021 dloss:2.2129 dlossQ:2.1760 dlossQsigm:0.0369\n",
      "Episode:300 meanR:42.3400 rate:0.0540 gloss:0.0027 dloss:1.9435 dlossQ:1.8752 dlossQsigm:0.0683\n",
      "Episode:301 meanR:42.2700 rate:0.0460 gloss:0.0047 dloss:1.6356 dlossQ:1.5939 dlossQsigm:0.0417\n",
      "Episode:302 meanR:42.5500 rate:0.0980 gloss:0.0009 dloss:1.2294 dlossQ:1.0165 dlossQsigm:0.2129\n",
      "Episode:303 meanR:42.5900 rate:0.0800 gloss:0.0024 dloss:0.9052 dlossQ:0.8439 dlossQsigm:0.0613\n",
      "Episode:304 meanR:42.5800 rate:0.0780 gloss:0.0014 dloss:1.6181 dlossQ:1.5892 dlossQsigm:0.0290\n",
      "Episode:305 meanR:42.9400 rate:0.1380 gloss:0.0011 dloss:1.7176 dlossQ:1.6881 dlossQsigm:0.0295\n",
      "Episode:306 meanR:42.8800 rate:0.0440 gloss:0.0014 dloss:2.1310 dlossQ:2.1034 dlossQsigm:0.0275\n",
      "Episode:307 meanR:43.0500 rate:0.1020 gloss:0.0013 dloss:1.7390 dlossQ:1.7050 dlossQsigm:0.0340\n",
      "Episode:308 meanR:43.0900 rate:0.0880 gloss:0.0013 dloss:2.8027 dlossQ:2.7414 dlossQsigm:0.0613\n",
      "Episode:309 meanR:43.0500 rate:0.0600 gloss:0.0083 dloss:3.8747 dlossQ:3.8091 dlossQsigm:0.0656\n",
      "Episode:310 meanR:42.8800 rate:0.0720 gloss:0.0040 dloss:1.8977 dlossQ:1.7663 dlossQsigm:0.1314\n",
      "Episode:311 meanR:42.8000 rate:0.0640 gloss:-0.0004 dloss:2.5126 dlossQ:2.0901 dlossQsigm:0.4225\n",
      "Episode:312 meanR:42.7400 rate:0.0700 gloss:0.0013 dloss:0.9025 dlossQ:0.5796 dlossQsigm:0.3229\n",
      "Episode:313 meanR:42.6500 rate:0.0580 gloss:0.0027 dloss:1.5208 dlossQ:1.3411 dlossQsigm:0.1797\n",
      "Episode:314 meanR:42.8500 rate:0.1000 gloss:0.0015 dloss:2.2832 dlossQ:1.9823 dlossQsigm:0.3008\n",
      "Episode:315 meanR:42.9600 rate:0.0900 gloss:0.0006 dloss:3.2857 dlossQ:3.1608 dlossQsigm:0.1249\n",
      "Episode:316 meanR:43.1000 rate:0.0900 gloss:0.0006 dloss:4.2451 dlossQ:4.1757 dlossQsigm:0.0693\n",
      "Episode:317 meanR:42.7900 rate:0.0440 gloss:0.0005 dloss:2.2497 dlossQ:2.1940 dlossQsigm:0.0556\n",
      "Episode:318 meanR:42.9800 rate:0.1060 gloss:0.0005 dloss:2.2090 dlossQ:2.1483 dlossQsigm:0.0607\n",
      "Episode:319 meanR:43.1800 rate:0.0840 gloss:0.0003 dloss:3.3830 dlossQ:3.3112 dlossQsigm:0.0718\n",
      "Episode:320 meanR:43.4800 rate:0.1080 gloss:0.0001 dloss:0.6148 dlossQ:0.5803 dlossQsigm:0.0345\n",
      "Episode:321 meanR:42.9800 rate:0.1040 gloss:0.0003 dloss:1.8660 dlossQ:1.8301 dlossQsigm:0.0360\n",
      "Episode:322 meanR:43.2100 rate:0.1560 gloss:0.0012 dloss:2.1095 dlossQ:2.0775 dlossQsigm:0.0320\n",
      "Episode:323 meanR:43.1100 rate:0.0560 gloss:0.0008 dloss:7.5923 dlossQ:7.5433 dlossQsigm:0.0490\n",
      "Episode:324 meanR:43.1700 rate:0.0600 gloss:0.0658 dloss:4.2635 dlossQ:4.2069 dlossQsigm:0.0566\n",
      "Episode:325 meanR:43.5500 rate:0.1620 gloss:0.0007 dloss:3.3276 dlossQ:3.2488 dlossQsigm:0.0788\n",
      "Episode:326 meanR:44.1800 rate:0.1900 gloss:0.0009 dloss:1.6363 dlossQ:1.6062 dlossQsigm:0.0301\n",
      "Episode:327 meanR:44.3500 rate:0.0940 gloss:0.0017 dloss:1.5898 dlossQ:1.5609 dlossQsigm:0.0289\n",
      "Episode:328 meanR:44.3600 rate:0.0500 gloss:0.0010 dloss:3.0823 dlossQ:3.0337 dlossQsigm:0.0486\n",
      "Episode:329 meanR:43.9600 rate:0.0540 gloss:0.0011 dloss:5.5466 dlossQ:5.4683 dlossQsigm:0.0783\n",
      "Episode:330 meanR:43.7600 rate:0.0500 gloss:0.0028 dloss:5.3288 dlossQ:5.2343 dlossQsigm:0.0944\n",
      "Episode:331 meanR:43.5800 rate:0.0520 gloss:0.0079 dloss:6.8950 dlossQ:6.7780 dlossQsigm:0.1170\n",
      "Episode:332 meanR:43.6100 rate:0.0560 gloss:0.0131 dloss:4.2810 dlossQ:4.2204 dlossQsigm:0.0606\n",
      "Episode:333 meanR:43.3600 rate:0.0660 gloss:0.0127 dloss:1.0917 dlossQ:1.0095 dlossQsigm:0.0822\n",
      "Episode:334 meanR:43.4200 rate:0.0620 gloss:0.0019 dloss:0.7966 dlossQ:0.6763 dlossQsigm:0.1203\n",
      "Episode:335 meanR:43.5000 rate:0.1020 gloss:0.0127 dloss:1.2130 dlossQ:1.1514 dlossQsigm:0.0616\n",
      "Episode:336 meanR:43.5000 rate:0.0620 gloss:0.0244 dloss:0.7913 dlossQ:0.7446 dlossQsigm:0.0467\n",
      "Episode:337 meanR:43.6500 rate:0.0820 gloss:0.0074 dloss:0.6768 dlossQ:0.6370 dlossQsigm:0.0398\n",
      "Episode:338 meanR:43.5200 rate:0.0540 gloss:0.0082 dloss:1.3543 dlossQ:1.3075 dlossQsigm:0.0467\n",
      "Episode:339 meanR:43.5300 rate:0.0580 gloss:0.0340 dloss:1.2895 dlossQ:1.2385 dlossQsigm:0.0510\n",
      "Episode:340 meanR:44.0200 rate:0.1940 gloss:0.0002 dloss:1.1916 dlossQ:1.1457 dlossQsigm:0.0459\n",
      "Episode:341 meanR:43.9800 rate:0.0500 gloss:0.0004 dloss:0.8887 dlossQ:0.8415 dlossQsigm:0.0473\n",
      "Episode:342 meanR:44.1000 rate:0.0840 gloss:0.0009 dloss:1.3423 dlossQ:1.3137 dlossQsigm:0.0286\n",
      "Episode:343 meanR:44.0000 rate:0.1120 gloss:0.0006 dloss:2.5799 dlossQ:2.4941 dlossQsigm:0.0859\n",
      "Episode:344 meanR:43.8300 rate:0.0680 gloss:0.0075 dloss:3.4557 dlossQ:3.3731 dlossQsigm:0.0826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:345 meanR:43.7000 rate:0.0440 gloss:0.0012 dloss:3.6601 dlossQ:3.5645 dlossQsigm:0.0956\n",
      "Episode:346 meanR:43.7400 rate:0.1120 gloss:-0.0280 dloss:5.1708 dlossQ:4.9448 dlossQsigm:0.2260\n",
      "Episode:347 meanR:43.6600 rate:0.0680 gloss:11.8788 dloss:13.3856 dlossQ:13.0974 dlossQsigm:0.2882\n",
      "Episode:348 meanR:43.2800 rate:0.0600 gloss:0.3550 dloss:6.3569 dlossQ:6.2732 dlossQsigm:0.0837\n",
      "Episode:349 meanR:43.2600 rate:0.0500 gloss:0.0308 dloss:5.0737 dlossQ:4.9838 dlossQsigm:0.0899\n",
      "Episode:350 meanR:43.1900 rate:0.0740 gloss:0.0302 dloss:9.9951 dlossQ:9.8481 dlossQsigm:0.1469\n",
      "Episode:351 meanR:43.4200 rate:0.0960 gloss:0.0439 dloss:1.5552 dlossQ:1.5036 dlossQsigm:0.0516\n",
      "Episode:352 meanR:43.2000 rate:0.0720 gloss:0.0063 dloss:1.2949 dlossQ:1.2433 dlossQsigm:0.0517\n",
      "Episode:353 meanR:43.7600 rate:0.1700 gloss:0.0067 dloss:4.2378 dlossQ:4.1860 dlossQsigm:0.0518\n",
      "Episode:354 meanR:43.7800 rate:0.0440 gloss:0.0020 dloss:4.7802 dlossQ:4.7198 dlossQsigm:0.0604\n",
      "Episode:355 meanR:43.8100 rate:0.0700 gloss:0.0017 dloss:5.4255 dlossQ:5.3599 dlossQsigm:0.0656\n",
      "Episode:356 meanR:43.7200 rate:0.0660 gloss:0.0160 dloss:5.2137 dlossQ:5.1331 dlossQsigm:0.0806\n",
      "Episode:357 meanR:43.7600 rate:0.1020 gloss:0.0230 dloss:1.1917 dlossQ:1.1448 dlossQsigm:0.0468\n",
      "Episode:358 meanR:43.3600 rate:0.0580 gloss:0.0611 dloss:2.4259 dlossQ:2.3558 dlossQsigm:0.0701\n",
      "Episode:359 meanR:43.4500 rate:0.1560 gloss:0.0088 dloss:3.6102 dlossQ:3.5493 dlossQsigm:0.0610\n",
      "Episode:360 meanR:43.3200 rate:0.0420 gloss:0.0482 dloss:4.4989 dlossQ:4.4375 dlossQsigm:0.0614\n",
      "Episode:361 meanR:43.2600 rate:0.0480 gloss:0.0039 dloss:3.8293 dlossQ:3.7606 dlossQsigm:0.0688\n",
      "Episode:362 meanR:42.9100 rate:0.0740 gloss:0.0008 dloss:3.7837 dlossQ:3.5901 dlossQsigm:0.1936\n",
      "Episode:363 meanR:42.8700 rate:0.0500 gloss:0.0010 dloss:2.6318 dlossQ:2.5095 dlossQsigm:0.1223\n",
      "Episode:364 meanR:43.1300 rate:0.0980 gloss:0.0018 dloss:0.8027 dlossQ:0.7404 dlossQsigm:0.0623\n",
      "Episode:365 meanR:43.2000 rate:0.0580 gloss:0.0349 dloss:0.8163 dlossQ:0.7693 dlossQsigm:0.0470\n",
      "Episode:366 meanR:43.1200 rate:0.0960 gloss:0.0024 dloss:1.1447 dlossQ:1.0975 dlossQsigm:0.0472\n",
      "Episode:367 meanR:43.3000 rate:0.0880 gloss:0.0148 dloss:1.9598 dlossQ:1.8993 dlossQsigm:0.0605\n",
      "Episode:368 meanR:42.8800 rate:0.0520 gloss:0.0065 dloss:1.8268 dlossQ:1.7669 dlossQsigm:0.0599\n",
      "Episode:369 meanR:43.3700 rate:0.1600 gloss:0.0043 dloss:3.0840 dlossQ:3.0313 dlossQsigm:0.0526\n",
      "Episode:370 meanR:43.1500 rate:0.0600 gloss:0.0040 dloss:2.7851 dlossQ:2.7339 dlossQsigm:0.0512\n",
      "Episode:371 meanR:43.4400 rate:0.1040 gloss:-0.4862 dloss:4.0292 dlossQ:3.8773 dlossQsigm:0.1519\n",
      "Episode:372 meanR:43.3000 rate:0.0560 gloss:0.2054 dloss:3.8079 dlossQ:3.5919 dlossQsigm:0.2161\n",
      "Episode:373 meanR:43.3800 rate:0.0900 gloss:0.2650 dloss:4.9802 dlossQ:4.9065 dlossQsigm:0.0737\n",
      "Episode:374 meanR:43.3400 rate:0.0600 gloss:0.0766 dloss:8.5597 dlossQ:8.4523 dlossQsigm:0.1075\n",
      "Episode:375 meanR:43.1800 rate:0.0500 gloss:0.0073 dloss:4.9304 dlossQ:4.8409 dlossQsigm:0.0895\n",
      "Episode:376 meanR:43.3000 rate:0.0700 gloss:0.0009 dloss:2.8664 dlossQ:2.7374 dlossQsigm:0.1290\n",
      "Episode:377 meanR:43.4900 rate:0.1120 gloss:0.0020 dloss:3.9010 dlossQ:3.8224 dlossQsigm:0.0786\n",
      "Episode:378 meanR:43.0600 rate:0.0680 gloss:0.0031 dloss:3.5013 dlossQ:3.4329 dlossQsigm:0.0684\n",
      "Episode:379 meanR:42.7500 rate:0.1140 gloss:0.0021 dloss:2.0768 dlossQ:2.0287 dlossQsigm:0.0482\n",
      "Episode:380 meanR:42.4200 rate:0.0560 gloss:0.0003 dloss:2.5928 dlossQ:2.4418 dlossQsigm:0.1510\n",
      "Episode:381 meanR:42.1200 rate:0.0480 gloss:0.0005 dloss:2.0078 dlossQ:1.8231 dlossQsigm:0.1848\n",
      "Episode:382 meanR:42.3800 rate:0.1000 gloss:-1.2812 dloss:3.2066 dlossQ:3.0389 dlossQsigm:0.1677\n",
      "Episode:383 meanR:42.3300 rate:0.0440 gloss:2.3314 dloss:6.6305 dlossQ:6.4540 dlossQsigm:0.1765\n",
      "Episode:384 meanR:42.2500 rate:0.0380 gloss:0.5302 dloss:2.3951 dlossQ:2.2899 dlossQsigm:0.1051\n",
      "Episode:385 meanR:42.4700 rate:0.1020 gloss:0.0135 dloss:3.7727 dlossQ:3.7089 dlossQsigm:0.0638\n",
      "Episode:386 meanR:41.7100 rate:0.1240 gloss:0.0461 dloss:5.7914 dlossQ:5.7482 dlossQsigm:0.0432\n",
      "Episode:387 meanR:41.7700 rate:0.0680 gloss:0.0095 dloss:5.6009 dlossQ:5.5488 dlossQsigm:0.0522\n",
      "Episode:388 meanR:41.6600 rate:0.0680 gloss:0.1017 dloss:6.3601 dlossQ:6.2691 dlossQsigm:0.0910\n",
      "Episode:389 meanR:41.8900 rate:0.0960 gloss:0.0039 dloss:2.2948 dlossQ:2.2301 dlossQsigm:0.0648\n",
      "Episode:390 meanR:41.6700 rate:0.0620 gloss:0.0030 dloss:2.3863 dlossQ:2.3220 dlossQsigm:0.0643\n",
      "Episode:391 meanR:41.6600 rate:0.0620 gloss:0.0006 dloss:5.4624 dlossQ:5.1075 dlossQsigm:0.3549\n",
      "Episode:392 meanR:41.6700 rate:0.0480 gloss:-0.0016 dloss:3.2685 dlossQ:2.9721 dlossQsigm:0.2964\n",
      "Episode:393 meanR:41.6500 rate:0.0800 gloss:-0.0149 dloss:2.3760 dlossQ:2.1475 dlossQsigm:0.2285\n",
      "Episode:394 meanR:41.8100 rate:0.1000 gloss:0.0019 dloss:4.9596 dlossQ:4.8772 dlossQsigm:0.0824\n",
      "Episode:395 meanR:41.6200 rate:0.0720 gloss:0.0016 dloss:4.2966 dlossQ:4.2217 dlossQsigm:0.0749\n",
      "Episode:396 meanR:40.7900 rate:0.1020 gloss:0.0016 dloss:5.2132 dlossQ:5.1546 dlossQsigm:0.0586\n",
      "Episode:397 meanR:40.6200 rate:0.0880 gloss:0.0003 dloss:3.7410 dlossQ:3.6829 dlossQsigm:0.0581\n",
      "Episode:398 meanR:40.1800 rate:0.0500 gloss:0.0006 dloss:4.7352 dlossQ:4.6596 dlossQsigm:0.0756\n",
      "Episode:399 meanR:40.0000 rate:0.0480 gloss:0.0014 dloss:5.9554 dlossQ:5.8572 dlossQsigm:0.0982\n",
      "Episode:400 meanR:40.1000 rate:0.0740 gloss:0.0007 dloss:5.2224 dlossQ:5.1273 dlossQsigm:0.0951\n",
      "Episode:401 meanR:40.1900 rate:0.0640 gloss:0.0007 dloss:6.4740 dlossQ:6.3453 dlossQsigm:0.1287\n",
      "Episode:402 meanR:40.0700 rate:0.0740 gloss:0.0005 dloss:1.9673 dlossQ:1.9032 dlossQsigm:0.0641\n",
      "Episode:403 meanR:39.9600 rate:0.0580 gloss:0.0006 dloss:1.7452 dlossQ:1.6835 dlossQsigm:0.0618\n",
      "Episode:404 meanR:40.2300 rate:0.1320 gloss:0.0007 dloss:2.0963 dlossQ:2.0499 dlossQsigm:0.0464\n",
      "Episode:405 meanR:39.8300 rate:0.0580 gloss:0.0005 dloss:2.9269 dlossQ:2.8638 dlossQsigm:0.0631\n",
      "Episode:406 meanR:39.8400 rate:0.0460 gloss:0.0004 dloss:2.0288 dlossQ:1.9848 dlossQsigm:0.0440\n",
      "Episode:407 meanR:39.8200 rate:0.0980 gloss:0.0002 dloss:2.2170 dlossQ:2.1617 dlossQsigm:0.0553\n",
      "Episode:408 meanR:40.2600 rate:0.1760 gloss:0.0001 dloss:0.9919 dlossQ:0.9649 dlossQsigm:0.0270\n",
      "Episode:409 meanR:40.1900 rate:0.0460 gloss:0.0002 dloss:1.4044 dlossQ:1.3646 dlossQsigm:0.0398\n",
      "Episode:410 meanR:40.1300 rate:0.0600 gloss:0.0001 dloss:0.9688 dlossQ:0.9329 dlossQsigm:0.0359\n",
      "Episode:411 meanR:40.0600 rate:0.0500 gloss:0.0002 dloss:1.3637 dlossQ:1.3110 dlossQsigm:0.0528\n",
      "Episode:412 meanR:40.0900 rate:0.0760 gloss:0.0002 dloss:1.5936 dlossQ:1.5268 dlossQsigm:0.0668\n",
      "Episode:413 meanR:40.0200 rate:0.0440 gloss:0.0002 dloss:0.7633 dlossQ:0.7100 dlossQsigm:0.0534\n",
      "Episode:414 meanR:40.2100 rate:0.1380 gloss:0.0002 dloss:1.5157 dlossQ:1.4835 dlossQsigm:0.0322\n",
      "Episode:415 meanR:39.9900 rate:0.0460 gloss:0.0003 dloss:1.7332 dlossQ:1.6794 dlossQsigm:0.0538\n",
      "Episode:416 meanR:39.9800 rate:0.0880 gloss:0.0254 dloss:5.2233 dlossQ:5.1746 dlossQsigm:0.0487\n",
      "Episode:417 meanR:40.1000 rate:0.0680 gloss:0.0039 dloss:5.9462 dlossQ:5.8768 dlossQsigm:0.0694\n",
      "Episode:418 meanR:39.8700 rate:0.0600 gloss:0.0133 dloss:5.2013 dlossQ:5.1247 dlossQsigm:0.0766\n",
      "Episode:419 meanR:39.6900 rate:0.0480 gloss:0.0021 dloss:4.0864 dlossQ:4.0263 dlossQsigm:0.0602\n",
      "Episode:420 meanR:39.4100 rate:0.0520 gloss:0.0014 dloss:1.9012 dlossQ:1.7878 dlossQsigm:0.1133\n",
      "Episode:421 meanR:39.2200 rate:0.0660 gloss:0.0015 dloss:2.5048 dlossQ:2.4336 dlossQsigm:0.0711\n",
      "Episode:422 meanR:38.6600 rate:0.0440 gloss:0.0008 dloss:0.9553 dlossQ:0.8973 dlossQsigm:0.0580\n",
      "Episode:423 meanR:38.5900 rate:0.0420 gloss:0.0009 dloss:1.5240 dlossQ:1.4507 dlossQsigm:0.0733\n",
      "Episode:424 meanR:38.6100 rate:0.0640 gloss:0.0006 dloss:1.4363 dlossQ:1.3728 dlossQsigm:0.0635\n",
      "Episode:425 meanR:38.2400 rate:0.0880 gloss:0.0005 dloss:1.9447 dlossQ:1.8795 dlossQsigm:0.0652\n",
      "Episode:426 meanR:37.7100 rate:0.0840 gloss:0.0004 dloss:1.8982 dlossQ:1.8401 dlossQsigm:0.0581\n",
      "Episode:427 meanR:37.5000 rate:0.0520 gloss:0.0005 dloss:2.2643 dlossQ:2.1976 dlossQsigm:0.0667\n",
      "Episode:428 meanR:37.6300 rate:0.0760 gloss:0.0006 dloss:2.1111 dlossQ:2.0412 dlossQsigm:0.0699\n",
      "Episode:429 meanR:37.6100 rate:0.0500 gloss:0.0004 dloss:1.4945 dlossQ:1.4404 dlossQsigm:0.0541\n",
      "Episode:430 meanR:37.6300 rate:0.0540 gloss:0.0009 dloss:1.3655 dlossQ:1.3067 dlossQsigm:0.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:431 meanR:38.0500 rate:0.1360 gloss:0.0005 dloss:3.9167 dlossQ:3.8542 dlossQsigm:0.0626\n",
      "Episode:432 meanR:38.0100 rate:0.0480 gloss:0.0003 dloss:4.0263 dlossQ:3.9655 dlossQsigm:0.0608\n",
      "Episode:433 meanR:38.5100 rate:0.1660 gloss:0.0022 dloss:4.6821 dlossQ:4.6247 dlossQsigm:0.0574\n",
      "Episode:434 meanR:38.5500 rate:0.0700 gloss:0.0020 dloss:3.9132 dlossQ:3.8616 dlossQsigm:0.0516\n",
      "Episode:435 meanR:38.3000 rate:0.0520 gloss:0.0016 dloss:5.2559 dlossQ:5.1856 dlossQsigm:0.0703\n",
      "Episode:436 meanR:38.4300 rate:0.0880 gloss:0.0006 dloss:3.7850 dlossQ:3.7084 dlossQsigm:0.0766\n",
      "Episode:437 meanR:38.3000 rate:0.0560 gloss:0.0009 dloss:4.6331 dlossQ:4.5368 dlossQsigm:0.0963\n",
      "Episode:438 meanR:38.6400 rate:0.1220 gloss:0.0006 dloss:3.2106 dlossQ:3.1659 dlossQsigm:0.0448\n",
      "Episode:439 meanR:38.9100 rate:0.1120 gloss:0.0053 dloss:3.9934 dlossQ:3.9325 dlossQsigm:0.0609\n",
      "Episode:440 meanR:38.2300 rate:0.0580 gloss:0.0019 dloss:4.2716 dlossQ:4.1906 dlossQsigm:0.0810\n",
      "Episode:441 meanR:38.2200 rate:0.0480 gloss:0.0004 dloss:4.9015 dlossQ:4.7289 dlossQsigm:0.1726\n",
      "Episode:442 meanR:38.0300 rate:0.0460 gloss:0.0001 dloss:3.9598 dlossQ:3.7292 dlossQsigm:0.2306\n",
      "Episode:443 meanR:38.1700 rate:0.1400 gloss:0.0001 dloss:2.7504 dlossQ:2.6949 dlossQsigm:0.0555\n",
      "Episode:444 meanR:38.1200 rate:0.0580 gloss:0.0001 dloss:2.6682 dlossQ:2.5924 dlossQsigm:0.0758\n",
      "Episode:445 meanR:38.1700 rate:0.0540 gloss:0.0001 dloss:3.1404 dlossQ:2.9348 dlossQsigm:0.2057\n",
      "Episode:446 meanR:38.0500 rate:0.0880 gloss:0.0001 dloss:3.0922 dlossQ:3.0183 dlossQsigm:0.0739\n",
      "Episode:447 meanR:38.0300 rate:0.0640 gloss:0.0001 dloss:0.8480 dlossQ:0.7500 dlossQsigm:0.0979\n",
      "Episode:448 meanR:38.2200 rate:0.0980 gloss:0.0024 dloss:2.8183 dlossQ:2.7590 dlossQsigm:0.0593\n",
      "Episode:449 meanR:38.4800 rate:0.1020 gloss:0.0002 dloss:3.1917 dlossQ:3.1427 dlossQsigm:0.0490\n",
      "Episode:450 meanR:38.3700 rate:0.0520 gloss:0.0003 dloss:4.1984 dlossQ:4.1378 dlossQsigm:0.0606\n",
      "Episode:451 meanR:38.1300 rate:0.0480 gloss:0.0002 dloss:3.9246 dlossQ:3.8644 dlossQsigm:0.0602\n",
      "Episode:452 meanR:38.3600 rate:0.1180 gloss:0.0003 dloss:4.5659 dlossQ:4.4898 dlossQsigm:0.0761\n",
      "Episode:453 meanR:37.8800 rate:0.0740 gloss:0.0001 dloss:4.2254 dlossQ:4.1374 dlossQsigm:0.0880\n",
      "Episode:454 meanR:38.1300 rate:0.0940 gloss:0.0003 dloss:8.2734 dlossQ:8.1624 dlossQsigm:0.1109\n",
      "Episode:455 meanR:38.1500 rate:0.0740 gloss:0.0003 dloss:8.5230 dlossQ:8.3899 dlossQsigm:0.1332\n",
      "Episode:456 meanR:38.0400 rate:0.0440 gloss:0.0002 dloss:7.8064 dlossQ:7.6821 dlossQsigm:0.1243\n",
      "Episode:457 meanR:38.1500 rate:0.1240 gloss:0.0003 dloss:7.9556 dlossQ:7.8523 dlossQsigm:0.1033\n",
      "Episode:458 meanR:38.5700 rate:0.1420 gloss:0.0002 dloss:2.8919 dlossQ:2.8520 dlossQsigm:0.0400\n",
      "Episode:459 meanR:38.4600 rate:0.1340 gloss:0.0001 dloss:3.1070 dlossQ:3.0554 dlossQsigm:0.0516\n",
      "Episode:460 meanR:38.4800 rate:0.0460 gloss:0.0001 dloss:4.5834 dlossQ:4.5015 dlossQsigm:0.0819\n",
      "Episode:461 meanR:38.4500 rate:0.0420 gloss:0.0001 dloss:5.6879 dlossQ:5.5808 dlossQsigm:0.1071\n",
      "Episode:462 meanR:38.3200 rate:0.0480 gloss:0.0002 dloss:5.9019 dlossQ:5.7943 dlossQsigm:0.1076\n",
      "Episode:463 meanR:38.2800 rate:0.0420 gloss:0.0002 dloss:4.5957 dlossQ:4.4921 dlossQsigm:0.1037\n",
      "Episode:464 meanR:38.0300 rate:0.0480 gloss:0.0002 dloss:3.4917 dlossQ:3.3869 dlossQsigm:0.1048\n",
      "Episode:465 meanR:38.6900 rate:0.1900 gloss:0.0001 dloss:1.5521 dlossQ:1.5140 dlossQsigm:0.0381\n",
      "Episode:466 meanR:38.4200 rate:0.0420 gloss:0.0001 dloss:1.5304 dlossQ:1.4897 dlossQsigm:0.0408\n",
      "Episode:467 meanR:38.2000 rate:0.0440 gloss:0.0001 dloss:2.5134 dlossQ:2.4576 dlossQsigm:0.0558\n",
      "Episode:468 meanR:38.2800 rate:0.0680 gloss:0.0001 dloss:2.6028 dlossQ:2.5269 dlossQsigm:0.0759\n",
      "Episode:469 meanR:38.4400 rate:0.1920 gloss:0.0002 dloss:2.6181 dlossQ:2.5851 dlossQsigm:0.0329\n",
      "Episode:470 meanR:38.3800 rate:0.0480 gloss:0.0001 dloss:4.3525 dlossQ:4.2996 dlossQsigm:0.0529\n",
      "Episode:471 meanR:38.2600 rate:0.0800 gloss:0.0001 dloss:4.0687 dlossQ:4.0076 dlossQsigm:0.0611\n",
      "Episode:472 meanR:38.7600 rate:0.1560 gloss:0.0001 dloss:2.9602 dlossQ:2.8963 dlossQsigm:0.0639\n",
      "Episode:473 meanR:38.9900 rate:0.1360 gloss:0.0004 dloss:2.4821 dlossQ:2.4493 dlossQsigm:0.0328\n",
      "Episode:474 meanR:39.1300 rate:0.0880 gloss:0.0001 dloss:3.3190 dlossQ:3.2653 dlossQsigm:0.0537\n",
      "Episode:475 meanR:39.2300 rate:0.0700 gloss:0.0001 dloss:2.5837 dlossQ:2.5353 dlossQsigm:0.0484\n",
      "Episode:476 meanR:39.4300 rate:0.1100 gloss:0.0000 dloss:2.6363 dlossQ:2.5893 dlossQsigm:0.0470\n",
      "Episode:477 meanR:39.0800 rate:0.0420 gloss:0.0001 dloss:4.1602 dlossQ:4.0755 dlossQsigm:0.0847\n",
      "Episode:478 meanR:39.4100 rate:0.1340 gloss:0.0001 dloss:5.1939 dlossQ:5.1314 dlossQsigm:0.0625\n",
      "Episode:479 meanR:39.1600 rate:0.0640 gloss:0.0002 dloss:4.4348 dlossQ:4.3643 dlossQsigm:0.0705\n",
      "Episode:480 meanR:39.1800 rate:0.0600 gloss:0.0001 dloss:4.1618 dlossQ:4.1234 dlossQsigm:0.0384\n",
      "Episode:481 meanR:39.1600 rate:0.0440 gloss:0.0001 dloss:2.7786 dlossQ:2.7231 dlossQsigm:0.0556\n",
      "Episode:482 meanR:39.1100 rate:0.0900 gloss:0.0000 dloss:4.2040 dlossQ:3.8723 dlossQsigm:0.3317\n",
      "Episode:483 meanR:39.1500 rate:0.0520 gloss:0.0001 dloss:2.3182 dlossQ:2.2424 dlossQsigm:0.0758\n",
      "Episode:484 meanR:39.2100 rate:0.0500 gloss:0.0001 dloss:2.2108 dlossQ:2.1347 dlossQsigm:0.0762\n",
      "Episode:485 meanR:38.9500 rate:0.0500 gloss:0.0001 dloss:2.4019 dlossQ:2.3220 dlossQsigm:0.0799\n",
      "Episode:486 meanR:38.5900 rate:0.0520 gloss:0.0000 dloss:1.7735 dlossQ:1.7052 dlossQsigm:0.0683\n",
      "Episode:487 meanR:38.4700 rate:0.0440 gloss:0.0001 dloss:1.7299 dlossQ:1.6500 dlossQsigm:0.0799\n",
      "Episode:488 meanR:38.6100 rate:0.0960 gloss:0.0001 dloss:2.2701 dlossQ:2.1917 dlossQsigm:0.0785\n",
      "Episode:489 meanR:38.4900 rate:0.0720 gloss:0.0001 dloss:2.2856 dlossQ:2.2199 dlossQsigm:0.0657\n",
      "Episode:490 meanR:38.7200 rate:0.1080 gloss:0.0001 dloss:2.7159 dlossQ:2.6542 dlossQsigm:0.0617\n",
      "Episode:491 meanR:38.8500 rate:0.0880 gloss:0.0001 dloss:2.7872 dlossQ:2.7213 dlossQsigm:0.0659\n",
      "Episode:492 meanR:38.9800 rate:0.0740 gloss:0.0000 dloss:2.4395 dlossQ:2.3780 dlossQsigm:0.0615\n",
      "Episode:493 meanR:39.1000 rate:0.1040 gloss:0.0000 dloss:2.2065 dlossQ:2.1444 dlossQsigm:0.0622\n",
      "Episode:494 meanR:39.3200 rate:0.1440 gloss:0.0007 dloss:3.1138 dlossQ:3.0616 dlossQsigm:0.0522\n",
      "Episode:495 meanR:39.2700 rate:0.0620 gloss:0.0000 dloss:2.5346 dlossQ:2.4933 dlossQsigm:0.0413\n",
      "Episode:496 meanR:39.0600 rate:0.0600 gloss:0.0000 dloss:3.1801 dlossQ:3.1283 dlossQsigm:0.0518\n",
      "Episode:497 meanR:39.1500 rate:0.1060 gloss:0.0000 dloss:2.0924 dlossQ:2.0416 dlossQsigm:0.0508\n",
      "Episode:498 meanR:39.1400 rate:0.0480 gloss:0.0000 dloss:1.0911 dlossQ:1.0464 dlossQsigm:0.0447\n",
      "Episode:499 meanR:39.1500 rate:0.0500 gloss:0.0000 dloss:3.0245 dlossQ:2.9683 dlossQsigm:0.0562\n",
      "Episode:500 meanR:39.1000 rate:0.0640 gloss:0.0000 dloss:2.7058 dlossQ:2.6526 dlossQsigm:0.0532\n",
      "Episode:501 meanR:39.1000 rate:0.0640 gloss:0.0000 dloss:1.5225 dlossQ:1.4571 dlossQsigm:0.0655\n",
      "Episode:502 meanR:38.9600 rate:0.0460 gloss:0.0000 dloss:0.6876 dlossQ:0.6284 dlossQsigm:0.0591\n",
      "Episode:503 meanR:39.2600 rate:0.1180 gloss:0.0000 dloss:1.1812 dlossQ:1.1412 dlossQsigm:0.0400\n",
      "Episode:504 meanR:39.0700 rate:0.0940 gloss:0.0000 dloss:1.3313 dlossQ:1.3057 dlossQsigm:0.0255\n",
      "Episode:505 meanR:39.3500 rate:0.1140 gloss:0.0006 dloss:1.8534 dlossQ:1.8213 dlossQsigm:0.0321\n",
      "Episode:506 meanR:39.6200 rate:0.1000 gloss:0.0001 dloss:3.4232 dlossQ:3.3853 dlossQsigm:0.0379\n",
      "Episode:507 meanR:39.3900 rate:0.0520 gloss:0.0002 dloss:3.3672 dlossQ:3.3158 dlossQsigm:0.0514\n",
      "Episode:508 meanR:39.1500 rate:0.1280 gloss:0.0001 dloss:1.7483 dlossQ:1.7150 dlossQsigm:0.0333\n",
      "Episode:509 meanR:39.1500 rate:0.0460 gloss:0.0013 dloss:2.4581 dlossQ:2.4136 dlossQsigm:0.0445\n",
      "Episode:510 meanR:39.0700 rate:0.0440 gloss:0.0000 dloss:3.4212 dlossQ:3.3525 dlossQsigm:0.0686\n",
      "Episode:511 meanR:39.1700 rate:0.0700 gloss:0.0000 dloss:1.9760 dlossQ:1.9302 dlossQsigm:0.0458\n",
      "Episode:512 meanR:39.0600 rate:0.0540 gloss:0.0002 dloss:2.1020 dlossQ:2.0400 dlossQsigm:0.0620\n",
      "Episode:513 meanR:39.1400 rate:0.0600 gloss:0.0000 dloss:1.6428 dlossQ:1.5861 dlossQsigm:0.0567\n",
      "Episode:514 meanR:38.7400 rate:0.0580 gloss:0.0000 dloss:1.2627 dlossQ:1.1715 dlossQsigm:0.0912\n",
      "Episode:515 meanR:39.0300 rate:0.1040 gloss:0.0000 dloss:1.0176 dlossQ:0.8852 dlossQsigm:0.1324\n",
      "Episode:516 meanR:38.8900 rate:0.0600 gloss:0.0000 dloss:0.8018 dlossQ:0.7265 dlossQsigm:0.0753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:517 meanR:38.8600 rate:0.0620 gloss:0.0000 dloss:0.8303 dlossQ:0.7776 dlossQsigm:0.0528\n",
      "Episode:518 meanR:38.9000 rate:0.0680 gloss:0.0001 dloss:0.7955 dlossQ:0.7568 dlossQsigm:0.0388\n",
      "Episode:519 meanR:39.1100 rate:0.0900 gloss:0.0000 dloss:0.6457 dlossQ:0.5792 dlossQsigm:0.0664\n",
      "Episode:520 meanR:39.0900 rate:0.0480 gloss:0.0000 dloss:0.6259 dlossQ:0.5899 dlossQsigm:0.0361\n",
      "Episode:521 meanR:39.0000 rate:0.0480 gloss:0.0001 dloss:1.7824 dlossQ:1.7268 dlossQsigm:0.0555\n",
      "Episode:522 meanR:39.2100 rate:0.0860 gloss:0.0000 dloss:0.4224 dlossQ:0.3758 dlossQsigm:0.0466\n",
      "Episode:523 meanR:40.6300 rate:0.3260 gloss:0.0497 dloss:2.5356 dlossQ:2.5255 dlossQsigm:0.0101\n",
      "Episode:524 meanR:40.7900 rate:0.0960 gloss:0.0032 dloss:2.1834 dlossQ:2.1612 dlossQsigm:0.0222\n",
      "Episode:525 meanR:40.8300 rate:0.0960 gloss:0.0042 dloss:5.0214 dlossQ:4.9629 dlossQsigm:0.0585\n",
      "Episode:526 meanR:40.8300 rate:0.0840 gloss:0.0000 dloss:2.9265 dlossQ:2.8871 dlossQsigm:0.0395\n",
      "Episode:527 meanR:40.8200 rate:0.0500 gloss:0.0000 dloss:0.9411 dlossQ:0.9004 dlossQsigm:0.0407\n",
      "Episode:528 meanR:40.7100 rate:0.0540 gloss:0.0000 dloss:1.0624 dlossQ:1.0223 dlossQsigm:0.0401\n",
      "Episode:529 meanR:41.0300 rate:0.1140 gloss:0.0009 dloss:3.2197 dlossQ:3.1746 dlossQsigm:0.0450\n",
      "Episode:530 meanR:41.0100 rate:0.0500 gloss:0.0002 dloss:6.5613 dlossQ:6.1830 dlossQsigm:0.3782\n",
      "Episode:531 meanR:40.7200 rate:0.0780 gloss:0.0001 dloss:7.4088 dlossQ:7.1132 dlossQsigm:0.2956\n",
      "Episode:532 meanR:40.7900 rate:0.0620 gloss:0.0000 dloss:13.4589 dlossQ:13.1167 dlossQsigm:0.3422\n",
      "Episode:533 meanR:40.4900 rate:0.1060 gloss:0.0003 dloss:12.3663 dlossQ:12.0716 dlossQsigm:0.2947\n",
      "Episode:534 meanR:40.4800 rate:0.0680 gloss:0.0001 dloss:10.2646 dlossQ:9.9789 dlossQsigm:0.2858\n",
      "Episode:535 meanR:40.9300 rate:0.1420 gloss:0.0005 dloss:9.0925 dlossQ:8.8988 dlossQsigm:0.1936\n",
      "Episode:536 meanR:41.1200 rate:0.1260 gloss:0.0002 dloss:6.7799 dlossQ:6.7343 dlossQsigm:0.0456\n",
      "Episode:537 meanR:41.1300 rate:0.0580 gloss:0.0001 dloss:7.4319 dlossQ:7.2656 dlossQsigm:0.1663\n",
      "Episode:538 meanR:40.7600 rate:0.0480 gloss:0.0000 dloss:11.0086 dlossQ:10.7270 dlossQsigm:0.2816\n",
      "Episode:539 meanR:40.4200 rate:0.0440 gloss:-0.0000 dloss:12.5218 dlossQ:12.1888 dlossQsigm:0.3330\n",
      "Episode:540 meanR:40.3800 rate:0.0500 gloss:-0.0000 dloss:15.1897 dlossQ:14.7290 dlossQsigm:0.4607\n",
      "Episode:541 meanR:40.4000 rate:0.0520 gloss:-0.0001 dloss:8.8406 dlossQ:8.4791 dlossQsigm:0.3615\n",
      "Episode:542 meanR:40.4600 rate:0.0580 gloss:-0.0000 dloss:2.0922 dlossQ:1.5168 dlossQsigm:0.5754\n",
      "Episode:543 meanR:40.2700 rate:0.1020 gloss:0.0000 dloss:1.2753 dlossQ:1.0955 dlossQsigm:0.1798\n",
      "Episode:544 meanR:40.2300 rate:0.0500 gloss:0.0000 dloss:1.3558 dlossQ:1.2083 dlossQsigm:0.1474\n",
      "Episode:545 meanR:40.3400 rate:0.0760 gloss:0.0000 dloss:3.1005 dlossQ:3.0241 dlossQsigm:0.0764\n",
      "Episode:546 meanR:40.1500 rate:0.0500 gloss:0.0000 dloss:3.7708 dlossQ:3.7185 dlossQsigm:0.0523\n",
      "Episode:547 meanR:40.4400 rate:0.1220 gloss:0.0000 dloss:2.3078 dlossQ:2.2490 dlossQsigm:0.0588\n",
      "Episode:548 meanR:40.3100 rate:0.0720 gloss:0.0000 dloss:2.1959 dlossQ:2.1364 dlossQsigm:0.0595\n",
      "Episode:549 meanR:40.2900 rate:0.0980 gloss:0.0000 dloss:1.7913 dlossQ:1.7602 dlossQsigm:0.0311\n",
      "Episode:550 meanR:40.4700 rate:0.0880 gloss:0.0005 dloss:3.3387 dlossQ:3.2984 dlossQsigm:0.0403\n",
      "Episode:551 meanR:40.6700 rate:0.0880 gloss:0.0001 dloss:3.1435 dlossQ:3.0881 dlossQsigm:0.0554\n",
      "Episode:552 meanR:40.3600 rate:0.0560 gloss:0.0000 dloss:2.1765 dlossQ:2.1079 dlossQsigm:0.0686\n",
      "Episode:553 meanR:40.4900 rate:0.1000 gloss:0.0001 dloss:4.1016 dlossQ:3.8501 dlossQsigm:0.2515\n",
      "Episode:554 meanR:40.2400 rate:0.0440 gloss:0.0001 dloss:6.6614 dlossQ:6.5287 dlossQsigm:0.1327\n",
      "Episode:555 meanR:40.4300 rate:0.1120 gloss:0.0009 dloss:7.2743 dlossQ:7.2081 dlossQsigm:0.0662\n",
      "Episode:556 meanR:40.8100 rate:0.1200 gloss:0.0009 dloss:2.8164 dlossQ:2.7375 dlossQsigm:0.0789\n",
      "Episode:557 meanR:40.7100 rate:0.1040 gloss:0.0001 dloss:2.9040 dlossQ:2.8565 dlossQsigm:0.0475\n",
      "Episode:558 meanR:40.2400 rate:0.0480 gloss:0.0001 dloss:2.5132 dlossQ:2.4633 dlossQsigm:0.0499\n",
      "Episode:559 meanR:39.8900 rate:0.0640 gloss:0.0070 dloss:2.5537 dlossQ:2.3451 dlossQsigm:0.2086\n",
      "Episode:560 meanR:39.9000 rate:0.0480 gloss:0.0001 dloss:4.7479 dlossQ:4.6526 dlossQsigm:0.0953\n",
      "Episode:561 meanR:39.9500 rate:0.0520 gloss:0.0001 dloss:3.7846 dlossQ:3.6871 dlossQsigm:0.0975\n",
      "Episode:562 meanR:40.2900 rate:0.1160 gloss:0.0000 dloss:1.7500 dlossQ:1.6900 dlossQsigm:0.0600\n",
      "Episode:563 meanR:40.9600 rate:0.1760 gloss:0.0000 dloss:1.3315 dlossQ:1.3041 dlossQsigm:0.0274\n",
      "Episode:564 meanR:41.8400 rate:0.2240 gloss:0.0003 dloss:1.2913 dlossQ:1.2649 dlossQsigm:0.0263\n",
      "Episode:565 meanR:41.1700 rate:0.0560 gloss:0.0001 dloss:3.1885 dlossQ:3.0995 dlossQsigm:0.0890\n",
      "Episode:566 meanR:41.2100 rate:0.0500 gloss:0.0000 dloss:3.9922 dlossQ:3.8475 dlossQsigm:0.1447\n",
      "Episode:567 meanR:41.3100 rate:0.0640 gloss:0.0000 dloss:7.3620 dlossQ:7.1292 dlossQsigm:0.2328\n",
      "Episode:568 meanR:41.1900 rate:0.0440 gloss:-0.0000 dloss:6.7417 dlossQ:6.4766 dlossQsigm:0.2651\n",
      "Episode:569 meanR:40.6100 rate:0.0760 gloss:0.0000 dloss:3.3878 dlossQ:3.2783 dlossQsigm:0.1095\n",
      "Episode:570 meanR:40.8600 rate:0.0980 gloss:0.0000 dloss:3.9269 dlossQ:3.8455 dlossQsigm:0.0814\n",
      "Episode:571 meanR:40.6800 rate:0.0440 gloss:0.0000 dloss:1.4819 dlossQ:1.2488 dlossQsigm:0.2331\n",
      "Episode:572 meanR:40.6300 rate:0.1460 gloss:0.0001 dloss:2.6452 dlossQ:2.5933 dlossQsigm:0.0519\n",
      "Episode:573 meanR:40.5600 rate:0.1220 gloss:0.0000 dloss:2.0518 dlossQ:2.0303 dlossQsigm:0.0215\n",
      "Episode:574 meanR:40.6900 rate:0.1140 gloss:0.0000 dloss:1.7914 dlossQ:1.7600 dlossQsigm:0.0313\n",
      "Episode:575 meanR:40.6400 rate:0.0600 gloss:0.0000 dloss:5.0308 dlossQ:4.9587 dlossQsigm:0.0721\n",
      "Episode:576 meanR:40.8600 rate:0.1540 gloss:0.0000 dloss:2.8532 dlossQ:2.8052 dlossQsigm:0.0480\n",
      "Episode:577 meanR:40.9900 rate:0.0680 gloss:0.0000 dloss:1.4903 dlossQ:1.4560 dlossQsigm:0.0343\n",
      "Episode:578 meanR:40.5600 rate:0.0480 gloss:0.0000 dloss:1.3718 dlossQ:1.3439 dlossQsigm:0.0279\n",
      "Episode:579 meanR:40.6700 rate:0.0860 gloss:0.0000 dloss:0.8573 dlossQ:0.8175 dlossQsigm:0.0398\n",
      "Episode:580 meanR:40.7200 rate:0.0700 gloss:0.0000 dloss:1.3958 dlossQ:1.3527 dlossQsigm:0.0431\n",
      "Episode:581 meanR:40.7200 rate:0.0440 gloss:0.0000 dloss:1.2677 dlossQ:1.2155 dlossQsigm:0.0522\n",
      "Episode:582 meanR:40.7800 rate:0.1020 gloss:0.0008 dloss:1.7446 dlossQ:1.7062 dlossQsigm:0.0384\n",
      "Episode:583 meanR:40.7400 rate:0.0440 gloss:0.0001 dloss:4.3645 dlossQ:4.2728 dlossQsigm:0.0917\n",
      "Episode:584 meanR:40.7200 rate:0.0460 gloss:0.0000 dloss:3.3596 dlossQ:3.1507 dlossQsigm:0.2089\n",
      "Episode:585 meanR:40.6800 rate:0.0420 gloss:0.0000 dloss:3.2347 dlossQ:2.9966 dlossQsigm:0.2381\n",
      "Episode:586 meanR:40.6800 rate:0.0520 gloss:0.0000 dloss:2.4043 dlossQ:2.0393 dlossQsigm:0.3651\n",
      "Episode:587 meanR:40.7000 rate:0.0480 gloss:0.0000 dloss:1.9598 dlossQ:1.4640 dlossQsigm:0.4958\n",
      "Episode:588 meanR:40.4900 rate:0.0540 gloss:0.0000 dloss:0.8494 dlossQ:0.7798 dlossQsigm:0.0696\n",
      "Episode:589 meanR:40.6100 rate:0.0960 gloss:0.0000 dloss:1.1985 dlossQ:1.1455 dlossQsigm:0.0530\n",
      "Episode:590 meanR:41.0400 rate:0.1940 gloss:0.0000 dloss:1.5565 dlossQ:1.5330 dlossQsigm:0.0234\n",
      "Episode:591 meanR:41.1800 rate:0.1160 gloss:0.0000 dloss:2.1705 dlossQ:2.1475 dlossQsigm:0.0229\n",
      "Episode:592 meanR:41.1300 rate:0.0640 gloss:0.0000 dloss:4.1228 dlossQ:4.0162 dlossQsigm:0.1066\n",
      "Episode:593 meanR:40.9400 rate:0.0660 gloss:-0.0000 dloss:4.0440 dlossQ:3.8004 dlossQsigm:0.2436\n",
      "Episode:594 meanR:40.8200 rate:0.1200 gloss:0.0002 dloss:4.1454 dlossQ:3.9510 dlossQsigm:0.1944\n",
      "Episode:595 meanR:40.8200 rate:0.0620 gloss:0.0000 dloss:3.2483 dlossQ:3.0315 dlossQsigm:0.2168\n",
      "Episode:596 meanR:40.8200 rate:0.0600 gloss:-0.0000 dloss:6.7262 dlossQ:6.4746 dlossQsigm:0.2517\n",
      "Episode:597 meanR:40.5500 rate:0.0520 gloss:-0.0000 dloss:7.3502 dlossQ:7.1106 dlossQsigm:0.2396\n",
      "Episode:598 meanR:40.5400 rate:0.0460 gloss:0.0001 dloss:10.8839 dlossQ:10.5158 dlossQsigm:0.3681\n",
      "Episode:599 meanR:40.4900 rate:0.0400 gloss:-0.0000 dloss:7.1814 dlossQ:6.8808 dlossQsigm:0.3006\n",
      "Episode:600 meanR:40.4200 rate:0.0500 gloss:0.0000 dloss:3.1187 dlossQ:2.9768 dlossQsigm:0.1419\n",
      "Episode:601 meanR:41.2500 rate:0.2300 gloss:0.0002 dloss:1.6806 dlossQ:1.6611 dlossQsigm:0.0195\n",
      "Episode:602 meanR:41.8200 rate:0.1600 gloss:0.0000 dloss:4.1430 dlossQ:4.1161 dlossQsigm:0.0269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:603 meanR:41.7400 rate:0.1020 gloss:0.0015 dloss:3.4340 dlossQ:3.4034 dlossQsigm:0.0306\n",
      "Episode:604 meanR:41.7800 rate:0.1020 gloss:0.0000 dloss:6.9871 dlossQ:6.9370 dlossQsigm:0.0501\n",
      "Episode:605 meanR:41.7000 rate:0.0980 gloss:0.0021 dloss:5.7118 dlossQ:5.6666 dlossQsigm:0.0452\n",
      "Episode:606 meanR:41.7200 rate:0.1040 gloss:0.0027 dloss:4.1356 dlossQ:4.0858 dlossQsigm:0.0498\n",
      "Episode:607 meanR:42.0500 rate:0.1180 gloss:0.0264 dloss:2.6321 dlossQ:2.5921 dlossQsigm:0.0399\n",
      "Episode:608 meanR:42.0100 rate:0.1200 gloss:0.0005 dloss:2.7228 dlossQ:2.6783 dlossQsigm:0.0444\n",
      "Episode:609 meanR:42.1000 rate:0.0640 gloss:0.0003 dloss:2.5722 dlossQ:2.4855 dlossQsigm:0.0867\n",
      "Episode:610 meanR:42.4600 rate:0.1160 gloss:0.0007 dloss:2.1781 dlossQ:2.1292 dlossQsigm:0.0489\n",
      "Episode:611 meanR:42.3700 rate:0.0520 gloss:0.0002 dloss:3.1299 dlossQ:3.0283 dlossQsigm:0.1016\n",
      "Episode:612 meanR:42.6200 rate:0.1040 gloss:0.0001 dloss:4.7856 dlossQ:4.7057 dlossQsigm:0.0799\n",
      "Episode:613 meanR:42.5700 rate:0.0500 gloss:0.0001 dloss:5.3828 dlossQ:5.2897 dlossQsigm:0.0932\n",
      "Episode:614 meanR:42.5600 rate:0.0560 gloss:0.0001 dloss:3.0177 dlossQ:2.9684 dlossQsigm:0.0493\n",
      "Episode:615 meanR:42.2700 rate:0.0460 gloss:0.0001 dloss:5.7591 dlossQ:5.7003 dlossQsigm:0.0587\n",
      "Episode:616 meanR:42.2100 rate:0.0480 gloss:0.0001 dloss:4.3643 dlossQ:4.2598 dlossQsigm:0.1045\n",
      "Episode:617 meanR:42.1500 rate:0.0500 gloss:0.0001 dloss:4.2148 dlossQ:4.0924 dlossQsigm:0.1224\n",
      "Episode:618 meanR:42.2200 rate:0.0820 gloss:0.0001 dloss:2.2007 dlossQ:2.1337 dlossQsigm:0.0669\n",
      "Episode:619 meanR:42.0200 rate:0.0500 gloss:0.0000 dloss:1.6166 dlossQ:1.5514 dlossQsigm:0.0652\n",
      "Episode:620 meanR:42.1800 rate:0.0800 gloss:0.0000 dloss:1.3673 dlossQ:1.3168 dlossQsigm:0.0505\n",
      "Episode:621 meanR:42.8200 rate:0.1760 gloss:0.0000 dloss:7.4824 dlossQ:7.4638 dlossQsigm:0.0186\n",
      "Episode:622 meanR:42.6300 rate:0.0480 gloss:0.0000 dloss:0.9555 dlossQ:0.9248 dlossQsigm:0.0307\n",
      "Episode:623 meanR:41.2800 rate:0.0560 gloss:0.0000 dloss:1.0602 dlossQ:0.8883 dlossQsigm:0.1718\n",
      "Episode:624 meanR:41.0800 rate:0.0560 gloss:0.0000 dloss:0.6039 dlossQ:0.5492 dlossQsigm:0.0547\n",
      "Episode:625 meanR:41.1100 rate:0.1020 gloss:0.0000 dloss:1.5415 dlossQ:1.4838 dlossQsigm:0.0578\n",
      "Episode:626 meanR:41.0900 rate:0.0800 gloss:0.0000 dloss:2.7250 dlossQ:2.6594 dlossQsigm:0.0656\n",
      "Episode:627 meanR:41.0700 rate:0.0460 gloss:0.0000 dloss:3.3750 dlossQ:3.2438 dlossQsigm:0.1312\n",
      "Episode:628 meanR:41.8900 rate:0.2180 gloss:0.0015 dloss:1.9199 dlossQ:1.8990 dlossQsigm:0.0208\n",
      "Episode:629 meanR:41.5300 rate:0.0420 gloss:0.0004 dloss:1.9722 dlossQ:1.8922 dlossQsigm:0.0800\n",
      "Episode:630 meanR:41.6200 rate:0.0680 gloss:0.0000 dloss:3.7016 dlossQ:3.5324 dlossQsigm:0.1692\n",
      "Episode:631 meanR:41.6600 rate:0.0860 gloss:0.0000 dloss:2.7192 dlossQ:2.4491 dlossQsigm:0.2701\n",
      "Episode:632 meanR:41.6800 rate:0.0660 gloss:0.0000 dloss:1.9939 dlossQ:1.9472 dlossQsigm:0.0466\n",
      "Episode:633 meanR:41.6700 rate:0.1040 gloss:0.0586 dloss:4.4451 dlossQ:4.4071 dlossQsigm:0.0380\n",
      "Episode:634 meanR:41.6000 rate:0.0540 gloss:0.0001 dloss:3.3912 dlossQ:3.3281 dlossQsigm:0.0631\n",
      "Episode:635 meanR:41.1900 rate:0.0600 gloss:0.0000 dloss:4.2752 dlossQ:3.9257 dlossQsigm:0.3495\n",
      "Episode:636 meanR:40.8600 rate:0.0600 gloss:0.0000 dloss:4.5018 dlossQ:4.2181 dlossQsigm:0.2837\n",
      "Episode:637 meanR:40.8900 rate:0.0640 gloss:0.0000 dloss:1.8095 dlossQ:1.7080 dlossQsigm:0.1015\n",
      "Episode:638 meanR:41.3700 rate:0.1440 gloss:0.0000 dloss:2.7935 dlossQ:2.6985 dlossQsigm:0.0950\n",
      "Episode:639 meanR:41.3500 rate:0.0400 gloss:0.0000 dloss:2.7140 dlossQ:2.5245 dlossQsigm:0.1895\n",
      "Episode:640 meanR:41.5900 rate:0.0980 gloss:0.0000 dloss:3.6673 dlossQ:3.5843 dlossQsigm:0.0830\n",
      "Episode:641 meanR:41.6500 rate:0.0640 gloss:0.0000 dloss:1.4931 dlossQ:1.4451 dlossQsigm:0.0479\n",
      "Episode:642 meanR:41.6400 rate:0.0560 gloss:0.0000 dloss:2.2469 dlossQ:2.1918 dlossQsigm:0.0551\n",
      "Episode:643 meanR:41.3700 rate:0.0480 gloss:0.0000 dloss:2.6641 dlossQ:2.5941 dlossQsigm:0.0699\n",
      "Episode:644 meanR:41.4400 rate:0.0640 gloss:0.0000 dloss:2.6736 dlossQ:2.6140 dlossQsigm:0.0597\n",
      "Episode:645 meanR:41.4700 rate:0.0820 gloss:0.0000 dloss:1.0053 dlossQ:0.8992 dlossQsigm:0.1061\n",
      "Episode:646 meanR:41.5900 rate:0.0740 gloss:0.0000 dloss:1.6391 dlossQ:1.5672 dlossQsigm:0.0719\n",
      "Episode:647 meanR:41.7100 rate:0.1460 gloss:0.0000 dloss:1.9399 dlossQ:1.9042 dlossQsigm:0.0357\n",
      "Episode:648 meanR:41.9500 rate:0.1200 gloss:0.0000 dloss:3.3487 dlossQ:3.3112 dlossQsigm:0.0376\n",
      "Episode:649 meanR:41.9800 rate:0.1040 gloss:0.0002 dloss:2.3848 dlossQ:2.3407 dlossQsigm:0.0440\n",
      "Episode:650 meanR:41.8100 rate:0.0540 gloss:0.0001 dloss:2.2853 dlossQ:2.1889 dlossQsigm:0.0964\n",
      "Episode:651 meanR:41.6900 rate:0.0640 gloss:0.0000 dloss:3.9233 dlossQ:3.8206 dlossQsigm:0.1028\n",
      "Episode:652 meanR:41.8600 rate:0.0900 gloss:0.0001 dloss:4.4723 dlossQ:4.2807 dlossQsigm:0.1916\n",
      "Episode:653 meanR:41.6000 rate:0.0480 gloss:0.0000 dloss:3.3135 dlossQ:3.2717 dlossQsigm:0.0418\n",
      "Episode:654 meanR:41.5900 rate:0.0420 gloss:0.0000 dloss:1.9465 dlossQ:1.7983 dlossQsigm:0.1482\n",
      "Episode:655 meanR:41.2900 rate:0.0520 gloss:0.0000 dloss:1.0117 dlossQ:0.9135 dlossQsigm:0.0982\n",
      "Episode:656 meanR:41.1900 rate:0.1000 gloss:0.0001 dloss:1.3528 dlossQ:1.2365 dlossQsigm:0.1163\n",
      "Episode:657 meanR:41.1600 rate:0.0980 gloss:0.0000 dloss:2.9892 dlossQ:2.9379 dlossQsigm:0.0513\n",
      "Episode:658 meanR:41.3600 rate:0.0880 gloss:0.0001 dloss:2.7792 dlossQ:2.7376 dlossQsigm:0.0416\n",
      "Episode:659 meanR:42.0100 rate:0.1940 gloss:0.0002 dloss:1.5261 dlossQ:1.5015 dlossQsigm:0.0246\n",
      "Episode:660 meanR:42.0200 rate:0.0500 gloss:0.0001 dloss:2.1185 dlossQ:2.0782 dlossQsigm:0.0403\n",
      "Episode:661 meanR:42.0900 rate:0.0660 gloss:0.0001 dloss:2.4443 dlossQ:2.4064 dlossQsigm:0.0379\n",
      "Episode:662 meanR:41.8300 rate:0.0640 gloss:0.0000 dloss:3.1759 dlossQ:3.0739 dlossQsigm:0.1020\n",
      "Episode:663 meanR:41.3100 rate:0.0720 gloss:0.0000 dloss:3.4454 dlossQ:3.3905 dlossQsigm:0.0548\n",
      "Episode:664 meanR:40.5700 rate:0.0760 gloss:0.0000 dloss:2.3591 dlossQ:2.2964 dlossQsigm:0.0627\n",
      "Episode:665 meanR:40.6300 rate:0.0680 gloss:0.0000 dloss:1.4476 dlossQ:1.4052 dlossQsigm:0.0424\n",
      "Episode:666 meanR:40.6800 rate:0.0600 gloss:0.0000 dloss:0.9053 dlossQ:0.8553 dlossQsigm:0.0500\n",
      "Episode:667 meanR:40.9300 rate:0.1140 gloss:0.0000 dloss:1.9994 dlossQ:1.9329 dlossQsigm:0.0665\n",
      "Episode:668 meanR:41.1500 rate:0.0880 gloss:0.0000 dloss:2.1698 dlossQ:2.1156 dlossQsigm:0.0541\n",
      "Episode:669 meanR:41.0100 rate:0.0480 gloss:0.0000 dloss:2.7318 dlossQ:2.6654 dlossQsigm:0.0664\n",
      "Episode:670 meanR:40.7500 rate:0.0460 gloss:0.0000 dloss:6.4659 dlossQ:6.4301 dlossQsigm:0.0357\n",
      "Episode:671 meanR:40.7900 rate:0.0520 gloss:0.0000 dloss:3.8700 dlossQ:3.8079 dlossQsigm:0.0621\n",
      "Episode:672 meanR:40.3600 rate:0.0600 gloss:0.0000 dloss:3.4138 dlossQ:3.3308 dlossQsigm:0.0830\n",
      "Episode:673 meanR:39.9800 rate:0.0460 gloss:0.0000 dloss:1.9154 dlossQ:1.8536 dlossQsigm:0.0618\n",
      "Episode:674 meanR:39.8300 rate:0.0840 gloss:0.0000 dloss:2.1576 dlossQ:2.0844 dlossQsigm:0.0731\n",
      "Episode:675 meanR:40.1500 rate:0.1240 gloss:0.0000 dloss:1.1831 dlossQ:1.1133 dlossQsigm:0.0698\n",
      "Episode:676 meanR:39.6500 rate:0.0540 gloss:0.0000 dloss:1.0376 dlossQ:0.9561 dlossQsigm:0.0815\n",
      "Episode:677 meanR:40.0200 rate:0.1420 gloss:0.0002 dloss:2.1009 dlossQ:1.9210 dlossQsigm:0.1799\n",
      "Episode:678 meanR:40.2400 rate:0.0920 gloss:0.0000 dloss:3.7688 dlossQ:3.6999 dlossQsigm:0.0689\n",
      "Episode:679 meanR:40.0600 rate:0.0500 gloss:0.0001 dloss:3.7762 dlossQ:3.7017 dlossQsigm:0.0745\n",
      "Episode:680 meanR:40.0100 rate:0.0600 gloss:0.0000 dloss:6.2942 dlossQ:6.0776 dlossQsigm:0.2167\n",
      "Episode:681 meanR:40.1000 rate:0.0620 gloss:0.0000 dloss:2.9224 dlossQ:2.8630 dlossQsigm:0.0594\n",
      "Episode:682 meanR:40.1900 rate:0.1200 gloss:0.0000 dloss:3.0060 dlossQ:2.8663 dlossQsigm:0.1397\n",
      "Episode:683 meanR:40.3900 rate:0.0840 gloss:0.0000 dloss:3.2624 dlossQ:3.1479 dlossQsigm:0.1145\n",
      "Episode:684 meanR:40.5400 rate:0.0760 gloss:0.0000 dloss:3.0750 dlossQ:2.9104 dlossQsigm:0.1646\n",
      "Episode:685 meanR:40.6200 rate:0.0580 gloss:0.0000 dloss:3.5042 dlossQ:3.4379 dlossQsigm:0.0664\n",
      "Episode:686 meanR:40.6000 rate:0.0480 gloss:0.0000 dloss:1.4876 dlossQ:1.2412 dlossQsigm:0.2464\n",
      "Episode:687 meanR:40.8000 rate:0.0880 gloss:0.0000 dloss:0.8152 dlossQ:0.7140 dlossQsigm:0.1012\n",
      "Episode:688 meanR:40.8500 rate:0.0640 gloss:0.0000 dloss:1.1717 dlossQ:1.0942 dlossQsigm:0.0775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:689 meanR:41.2800 rate:0.1820 gloss:0.0001 dloss:1.9591 dlossQ:1.8607 dlossQsigm:0.0984\n",
      "Episode:690 meanR:40.5700 rate:0.0520 gloss:0.0000 dloss:2.5412 dlossQ:2.4593 dlossQsigm:0.0819\n",
      "Episode:691 meanR:40.1900 rate:0.0400 gloss:0.0001 dloss:2.5347 dlossQ:2.4886 dlossQsigm:0.0460\n",
      "Episode:692 meanR:40.1100 rate:0.0480 gloss:0.0000 dloss:2.7814 dlossQ:2.5283 dlossQsigm:0.2531\n",
      "Episode:693 meanR:40.1600 rate:0.0760 gloss:0.0000 dloss:2.0088 dlossQ:1.7753 dlossQsigm:0.2335\n",
      "Episode:694 meanR:40.1100 rate:0.1100 gloss:0.0005 dloss:3.3554 dlossQ:3.3036 dlossQsigm:0.0519\n",
      "Episode:695 meanR:40.5700 rate:0.1540 gloss:0.0945 dloss:3.0818 dlossQ:3.0533 dlossQsigm:0.0285\n",
      "Episode:696 meanR:40.9500 rate:0.1360 gloss:0.0154 dloss:6.9463 dlossQ:6.9213 dlossQsigm:0.0250\n",
      "Episode:697 meanR:41.1000 rate:0.0820 gloss:0.0030 dloss:8.2081 dlossQ:8.1575 dlossQsigm:0.0506\n",
      "Episode:698 meanR:41.1200 rate:0.0500 gloss:0.0000 dloss:4.7257 dlossQ:4.6737 dlossQsigm:0.0520\n",
      "Episode:699 meanR:41.3600 rate:0.0880 gloss:0.0000 dloss:4.8288 dlossQ:4.7600 dlossQsigm:0.0688\n",
      "Episode:700 meanR:41.3900 rate:0.0560 gloss:0.0000 dloss:1.1853 dlossQ:1.1383 dlossQsigm:0.0470\n",
      "Episode:701 meanR:40.7200 rate:0.0960 gloss:0.0000 dloss:1.2712 dlossQ:1.1945 dlossQsigm:0.0767\n",
      "Episode:702 meanR:40.3100 rate:0.0780 gloss:0.0000 dloss:0.7679 dlossQ:0.6896 dlossQsigm:0.0782\n",
      "Episode:703 meanR:40.4600 rate:0.1320 gloss:0.0001 dloss:3.5595 dlossQ:3.5203 dlossQsigm:0.0392\n",
      "Episode:704 meanR:40.1900 rate:0.0480 gloss:0.0001 dloss:4.1245 dlossQ:4.0752 dlossQsigm:0.0492\n",
      "Episode:705 meanR:39.9400 rate:0.0480 gloss:0.0000 dloss:2.6810 dlossQ:2.5893 dlossQsigm:0.0917\n",
      "Episode:706 meanR:39.7300 rate:0.0620 gloss:0.0000 dloss:2.4195 dlossQ:2.3643 dlossQsigm:0.0552\n",
      "Episode:707 meanR:39.3600 rate:0.0440 gloss:0.0000 dloss:3.1292 dlossQ:3.0545 dlossQsigm:0.0747\n",
      "Episode:708 meanR:39.1000 rate:0.0680 gloss:0.0001 dloss:3.3232 dlossQ:3.2528 dlossQsigm:0.0704\n",
      "Episode:709 meanR:39.0200 rate:0.0480 gloss:0.0000 dloss:1.5473 dlossQ:1.4975 dlossQsigm:0.0497\n",
      "Episode:710 meanR:38.7000 rate:0.0520 gloss:0.0000 dloss:1.2428 dlossQ:1.1885 dlossQsigm:0.0542\n",
      "Episode:711 meanR:38.6600 rate:0.0440 gloss:0.0001 dloss:1.1752 dlossQ:1.0896 dlossQsigm:0.0856\n",
      "Episode:712 meanR:38.4800 rate:0.0680 gloss:0.0000 dloss:0.7347 dlossQ:0.6594 dlossQsigm:0.0752\n",
      "Episode:713 meanR:38.8300 rate:0.1200 gloss:0.0000 dloss:1.5948 dlossQ:1.5270 dlossQsigm:0.0678\n",
      "Episode:714 meanR:39.1600 rate:0.1220 gloss:0.0000 dloss:7.4030 dlossQ:7.3536 dlossQsigm:0.0493\n",
      "Episode:715 meanR:39.5000 rate:0.1140 gloss:0.0001 dloss:3.4544 dlossQ:3.4067 dlossQsigm:0.0477\n",
      "Episode:716 meanR:39.5400 rate:0.0560 gloss:0.0000 dloss:7.6328 dlossQ:7.5788 dlossQsigm:0.0540\n",
      "Episode:717 meanR:39.5300 rate:0.0480 gloss:0.0000 dloss:2.8057 dlossQ:2.7356 dlossQsigm:0.0702\n",
      "Episode:718 meanR:39.4600 rate:0.0680 gloss:0.0000 dloss:3.2385 dlossQ:3.1953 dlossQsigm:0.0432\n",
      "Episode:719 meanR:39.7000 rate:0.0980 gloss:0.0001 dloss:5.0769 dlossQ:5.0015 dlossQsigm:0.0753\n",
      "Episode:720 meanR:39.5200 rate:0.0440 gloss:0.0000 dloss:6.2786 dlossQ:6.1600 dlossQsigm:0.1186\n",
      "Episode:721 meanR:38.9800 rate:0.0680 gloss:0.0000 dloss:4.4832 dlossQ:4.3994 dlossQsigm:0.0838\n",
      "Episode:722 meanR:38.9700 rate:0.0460 gloss:0.0000 dloss:6.9574 dlossQ:6.9114 dlossQsigm:0.0460\n",
      "Episode:723 meanR:39.0100 rate:0.0640 gloss:0.0000 dloss:3.5655 dlossQ:3.5117 dlossQsigm:0.0538\n",
      "Episode:724 meanR:39.0800 rate:0.0700 gloss:0.0000 dloss:2.7843 dlossQ:2.7182 dlossQsigm:0.0662\n",
      "Episode:725 meanR:38.7800 rate:0.0420 gloss:0.0000 dloss:2.3767 dlossQ:2.3156 dlossQsigm:0.0611\n",
      "Episode:726 meanR:38.9100 rate:0.1060 gloss:0.0001 dloss:2.7744 dlossQ:2.7249 dlossQsigm:0.0495\n",
      "Episode:727 meanR:39.1100 rate:0.0860 gloss:0.0000 dloss:5.5664 dlossQ:5.4446 dlossQsigm:0.1217\n",
      "Episode:728 meanR:38.3500 rate:0.0660 gloss:0.0000 dloss:9.6011 dlossQ:9.5380 dlossQsigm:0.0631\n",
      "Episode:729 meanR:38.4600 rate:0.0640 gloss:0.0000 dloss:5.0162 dlossQ:4.9253 dlossQsigm:0.0909\n",
      "Episode:730 meanR:38.4000 rate:0.0560 gloss:0.0000 dloss:3.5945 dlossQ:3.5201 dlossQsigm:0.0744\n",
      "Episode:731 meanR:39.0000 rate:0.2060 gloss:0.0001 dloss:2.1771 dlossQ:2.1491 dlossQsigm:0.0280\n",
      "Episode:732 meanR:38.8800 rate:0.0420 gloss:0.0001 dloss:2.8623 dlossQ:2.8190 dlossQsigm:0.0433\n",
      "Episode:733 meanR:38.7900 rate:0.0860 gloss:0.0000 dloss:5.1669 dlossQ:5.1173 dlossQsigm:0.0496\n",
      "Episode:734 meanR:39.2700 rate:0.1500 gloss:0.0000 dloss:8.0659 dlossQ:7.9395 dlossQsigm:0.1264\n",
      "Episode:735 meanR:39.2900 rate:0.0640 gloss:0.0000 dloss:14.6455 dlossQ:14.5835 dlossQsigm:0.0620\n",
      "Episode:736 meanR:39.2500 rate:0.0520 gloss:0.0000 dloss:6.2672 dlossQ:6.1856 dlossQsigm:0.0816\n",
      "Episode:737 meanR:39.2500 rate:0.0640 gloss:0.0000 dloss:6.0937 dlossQ:5.7101 dlossQsigm:0.3835\n",
      "Episode:738 meanR:39.0100 rate:0.0960 gloss:-0.0000 dloss:1.2743 dlossQ:0.9006 dlossQsigm:0.3737\n",
      "Episode:739 meanR:39.3700 rate:0.1120 gloss:0.0003 dloss:3.6417 dlossQ:3.5629 dlossQsigm:0.0788\n",
      "Episode:740 meanR:39.1000 rate:0.0440 gloss:0.0001 dloss:3.7190 dlossQ:3.6326 dlossQsigm:0.0864\n",
      "Episode:741 meanR:39.3100 rate:0.1060 gloss:0.0303 dloss:4.9319 dlossQ:4.8591 dlossQsigm:0.0728\n",
      "Episode:742 meanR:39.2500 rate:0.0440 gloss:0.0005 dloss:4.0747 dlossQ:3.9847 dlossQsigm:0.0900\n",
      "Episode:743 meanR:39.2200 rate:0.0420 gloss:0.0006 dloss:4.5703 dlossQ:4.4646 dlossQsigm:0.1057\n",
      "Episode:744 meanR:39.3400 rate:0.0880 gloss:0.0001 dloss:12.7254 dlossQ:12.6229 dlossQsigm:0.1025\n",
      "Episode:745 meanR:39.1700 rate:0.0480 gloss:0.0000 dloss:16.6805 dlossQ:16.5136 dlossQsigm:0.1669\n",
      "Episode:746 meanR:39.0400 rate:0.0480 gloss:0.0000 dloss:2.9389 dlossQ:2.8652 dlossQsigm:0.0737\n",
      "Episode:747 meanR:38.8400 rate:0.1060 gloss:0.0010 dloss:5.4100 dlossQ:5.3425 dlossQsigm:0.0674\n",
      "Episode:748 meanR:38.7000 rate:0.0920 gloss:0.0000 dloss:4.7634 dlossQ:4.7010 dlossQsigm:0.0624\n",
      "Episode:749 meanR:38.9100 rate:0.1460 gloss:0.0015 dloss:4.1734 dlossQ:4.1160 dlossQsigm:0.0575\n",
      "Episode:750 meanR:39.2400 rate:0.1200 gloss:0.0000 dloss:3.3075 dlossQ:3.2685 dlossQsigm:0.0390\n",
      "Episode:751 meanR:39.1500 rate:0.0460 gloss:0.0000 dloss:4.3230 dlossQ:4.2638 dlossQsigm:0.0593\n",
      "Episode:752 meanR:38.9800 rate:0.0560 gloss:0.0000 dloss:4.2835 dlossQ:4.2098 dlossQsigm:0.0737\n",
      "Episode:753 meanR:38.9500 rate:0.0420 gloss:0.0000 dloss:2.4955 dlossQ:2.4274 dlossQsigm:0.0680\n",
      "Episode:754 meanR:39.0400 rate:0.0600 gloss:0.0000 dloss:2.0532 dlossQ:1.9760 dlossQsigm:0.0771\n",
      "Episode:755 meanR:39.2400 rate:0.0920 gloss:0.0000 dloss:1.0778 dlossQ:1.0225 dlossQsigm:0.0553\n",
      "Episode:756 meanR:38.9800 rate:0.0480 gloss:0.0000 dloss:0.9646 dlossQ:0.9010 dlossQsigm:0.0636\n",
      "Episode:757 meanR:38.8100 rate:0.0640 gloss:0.0000 dloss:1.2672 dlossQ:1.0337 dlossQsigm:0.2335\n",
      "Episode:758 meanR:39.4600 rate:0.2180 gloss:0.0000 dloss:1.1529 dlossQ:1.1120 dlossQsigm:0.0409\n",
      "Episode:759 meanR:38.7900 rate:0.0600 gloss:0.0000 dloss:2.5829 dlossQ:2.5384 dlossQsigm:0.0445\n",
      "Episode:760 meanR:39.0400 rate:0.1000 gloss:0.0005 dloss:4.2600 dlossQ:4.1836 dlossQsigm:0.0764\n",
      "Episode:761 meanR:39.0000 rate:0.0580 gloss:0.0000 dloss:3.3278 dlossQ:3.2297 dlossQsigm:0.0980\n",
      "Episode:762 meanR:38.9100 rate:0.0460 gloss:0.0000 dloss:3.0413 dlossQ:2.9669 dlossQsigm:0.0743\n",
      "Episode:763 meanR:38.8600 rate:0.0620 gloss:0.0000 dloss:2.7541 dlossQ:2.6362 dlossQsigm:0.1178\n",
      "Episode:764 meanR:38.9200 rate:0.0880 gloss:0.0001 dloss:2.2595 dlossQ:2.1082 dlossQsigm:0.1513\n",
      "Episode:765 meanR:38.8100 rate:0.0460 gloss:0.0000 dloss:1.0390 dlossQ:0.9505 dlossQsigm:0.0885\n",
      "Episode:766 meanR:38.7100 rate:0.0400 gloss:0.0000 dloss:1.2523 dlossQ:1.2033 dlossQsigm:0.0490\n",
      "Episode:767 meanR:38.5000 rate:0.0720 gloss:0.0000 dloss:0.9847 dlossQ:0.8204 dlossQsigm:0.1643\n",
      "Episode:768 meanR:38.4200 rate:0.0720 gloss:0.0000 dloss:0.7463 dlossQ:0.6890 dlossQsigm:0.0573\n",
      "Episode:769 meanR:38.4200 rate:0.0480 gloss:0.0000 dloss:1.0066 dlossQ:0.9492 dlossQsigm:0.0574\n",
      "Episode:770 meanR:38.4200 rate:0.0460 gloss:0.0000 dloss:0.8487 dlossQ:0.7687 dlossQsigm:0.0800\n",
      "Episode:771 meanR:38.3800 rate:0.0440 gloss:0.0000 dloss:1.1716 dlossQ:1.1142 dlossQsigm:0.0575\n",
      "Episode:772 meanR:38.3000 rate:0.0440 gloss:0.0000 dloss:1.2372 dlossQ:1.1719 dlossQsigm:0.0653\n",
      "Episode:773 meanR:38.3200 rate:0.0500 gloss:0.0000 dloss:1.9971 dlossQ:1.9156 dlossQsigm:0.0815\n",
      "Episode:774 meanR:38.9100 rate:0.2020 gloss:0.0005 dloss:4.1317 dlossQ:4.0850 dlossQsigm:0.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:775 meanR:38.5100 rate:0.0440 gloss:0.0000 dloss:5.9868 dlossQ:5.9343 dlossQsigm:0.0526\n",
      "Episode:776 meanR:38.4800 rate:0.0480 gloss:0.0000 dloss:4.7516 dlossQ:4.7093 dlossQsigm:0.0423\n",
      "Episode:777 meanR:38.5200 rate:0.1500 gloss:0.0000 dloss:11.9502 dlossQ:11.7572 dlossQsigm:0.1931\n",
      "Episode:778 meanR:38.7100 rate:0.1300 gloss:0.0005 dloss:4.0770 dlossQ:4.0394 dlossQsigm:0.0376\n",
      "Episode:779 meanR:39.1300 rate:0.1340 gloss:0.0000 dloss:4.3470 dlossQ:4.3089 dlossQsigm:0.0380\n",
      "Episode:780 meanR:39.5000 rate:0.1340 gloss:0.0000 dloss:6.9589 dlossQ:6.9106 dlossQsigm:0.0483\n",
      "Episode:781 meanR:39.4600 rate:0.0540 gloss:0.0000 dloss:4.5192 dlossQ:4.4618 dlossQsigm:0.0574\n",
      "Episode:782 meanR:39.2600 rate:0.0800 gloss:0.0000 dloss:6.5044 dlossQ:6.4288 dlossQsigm:0.0756\n",
      "Episode:783 meanR:39.3200 rate:0.0960 gloss:0.0036 dloss:4.9333 dlossQ:4.8472 dlossQsigm:0.0861\n",
      "Episode:784 meanR:39.2300 rate:0.0580 gloss:0.0000 dloss:4.5086 dlossQ:4.4411 dlossQsigm:0.0675\n",
      "Episode:785 meanR:39.4000 rate:0.0920 gloss:0.0000 dloss:4.7874 dlossQ:4.6571 dlossQsigm:0.1303\n",
      "Episode:786 meanR:39.5100 rate:0.0700 gloss:0.0000 dloss:4.2341 dlossQ:4.0897 dlossQsigm:0.1444\n",
      "Episode:787 meanR:39.8600 rate:0.1580 gloss:0.0001 dloss:3.5676 dlossQ:3.4561 dlossQsigm:0.1116\n",
      "Episode:788 meanR:39.7800 rate:0.0480 gloss:0.0000 dloss:3.7838 dlossQ:3.6863 dlossQsigm:0.0975\n",
      "Episode:789 meanR:39.1700 rate:0.0600 gloss:0.0000 dloss:4.5496 dlossQ:4.4327 dlossQsigm:0.1169\n",
      "Episode:790 meanR:39.2000 rate:0.0580 gloss:0.0000 dloss:4.5793 dlossQ:4.3599 dlossQsigm:0.2193\n",
      "Episode:791 meanR:39.6100 rate:0.1220 gloss:0.0000 dloss:2.1553 dlossQ:2.0934 dlossQsigm:0.0618\n",
      "Episode:792 meanR:39.7200 rate:0.0700 gloss:0.0000 dloss:3.6619 dlossQ:3.5448 dlossQsigm:0.1171\n",
      "Episode:793 meanR:39.6000 rate:0.0520 gloss:0.0000 dloss:3.0448 dlossQ:2.9742 dlossQsigm:0.0706\n",
      "Episode:794 meanR:39.4500 rate:0.0800 gloss:0.0000 dloss:2.5125 dlossQ:2.4599 dlossQsigm:0.0526\n",
      "Episode:795 meanR:38.9800 rate:0.0600 gloss:0.0000 dloss:2.6160 dlossQ:2.5493 dlossQsigm:0.0667\n",
      "Episode:796 meanR:38.5600 rate:0.0520 gloss:0.0000 dloss:1.0891 dlossQ:0.9552 dlossQsigm:0.1339\n",
      "Episode:797 meanR:38.4800 rate:0.0660 gloss:0.0000 dloss:2.6979 dlossQ:2.6422 dlossQsigm:0.0557\n",
      "Episode:798 meanR:38.5000 rate:0.0540 gloss:0.0000 dloss:1.9908 dlossQ:1.9285 dlossQsigm:0.0624\n",
      "Episode:799 meanR:38.3600 rate:0.0600 gloss:0.0000 dloss:1.7954 dlossQ:1.7404 dlossQsigm:0.0550\n",
      "Episode:800 meanR:38.3500 rate:0.0540 gloss:0.0000 dloss:0.9008 dlossQ:0.7754 dlossQsigm:0.1253\n",
      "Episode:801 meanR:38.1500 rate:0.0560 gloss:0.0000 dloss:0.8412 dlossQ:0.7795 dlossQsigm:0.0617\n",
      "Episode:802 meanR:38.0700 rate:0.0620 gloss:0.0000 dloss:0.8318 dlossQ:0.7759 dlossQsigm:0.0559\n",
      "Episode:803 meanR:37.6300 rate:0.0440 gloss:0.0000 dloss:0.8479 dlossQ:0.7913 dlossQsigm:0.0566\n",
      "Episode:804 meanR:38.0100 rate:0.1240 gloss:0.0000 dloss:1.6679 dlossQ:1.6237 dlossQsigm:0.0442\n",
      "Episode:805 meanR:38.0200 rate:0.0500 gloss:0.0000 dloss:2.0107 dlossQ:1.9653 dlossQsigm:0.0454\n",
      "Episode:806 meanR:38.1900 rate:0.0960 gloss:0.0000 dloss:0.9393 dlossQ:0.8790 dlossQsigm:0.0603\n",
      "Episode:807 meanR:38.2400 rate:0.0540 gloss:0.0000 dloss:0.9798 dlossQ:0.9407 dlossQsigm:0.0390\n",
      "Episode:808 meanR:38.4300 rate:0.1060 gloss:0.0000 dloss:4.3577 dlossQ:4.2706 dlossQsigm:0.0872\n",
      "Episode:809 meanR:38.5300 rate:0.0680 gloss:0.0000 dloss:4.1537 dlossQ:4.0015 dlossQsigm:0.1521\n",
      "Episode:810 meanR:39.0800 rate:0.1620 gloss:0.0000 dloss:4.4991 dlossQ:4.3701 dlossQsigm:0.1290\n",
      "Episode:811 meanR:39.1800 rate:0.0640 gloss:0.0000 dloss:4.9841 dlossQ:4.8446 dlossQsigm:0.1395\n",
      "Episode:812 meanR:39.1200 rate:0.0560 gloss:0.0000 dloss:4.9095 dlossQ:4.7896 dlossQsigm:0.1200\n",
      "Episode:813 meanR:38.9700 rate:0.0900 gloss:0.0002 dloss:4.6013 dlossQ:4.4566 dlossQsigm:0.1447\n",
      "Episode:814 meanR:38.9300 rate:0.1140 gloss:0.0000 dloss:3.4096 dlossQ:3.3617 dlossQsigm:0.0479\n",
      "Episode:815 meanR:38.6400 rate:0.0560 gloss:0.0000 dloss:2.4012 dlossQ:2.3526 dlossQsigm:0.0487\n",
      "Episode:816 meanR:38.6200 rate:0.0520 gloss:0.0000 dloss:1.6598 dlossQ:1.6159 dlossQsigm:0.0439\n",
      "Episode:817 meanR:38.6100 rate:0.0460 gloss:0.0000 dloss:1.6362 dlossQ:1.5955 dlossQsigm:0.0407\n",
      "Episode:818 meanR:38.7700 rate:0.1000 gloss:0.0002 dloss:4.4404 dlossQ:4.3148 dlossQsigm:0.1256\n",
      "Episode:819 meanR:39.0200 rate:0.1480 gloss:0.0000 dloss:3.5580 dlossQ:3.5093 dlossQsigm:0.0488\n",
      "Episode:820 meanR:39.0400 rate:0.0480 gloss:0.0000 dloss:3.3093 dlossQ:3.2493 dlossQsigm:0.0600\n",
      "Episode:821 meanR:39.6400 rate:0.1880 gloss:0.0000 dloss:0.9286 dlossQ:0.8963 dlossQsigm:0.0324\n",
      "Episode:822 meanR:39.6200 rate:0.0420 gloss:0.0000 dloss:1.1380 dlossQ:1.1035 dlossQsigm:0.0344\n",
      "Episode:823 meanR:39.5800 rate:0.0560 gloss:0.0000 dloss:0.9132 dlossQ:0.8797 dlossQsigm:0.0336\n",
      "Episode:824 meanR:39.5900 rate:0.0720 gloss:0.0000 dloss:1.4873 dlossQ:1.4326 dlossQsigm:0.0547\n",
      "Episode:825 meanR:39.8800 rate:0.1000 gloss:0.0782 dloss:4.3650 dlossQ:4.2819 dlossQsigm:0.0831\n",
      "Episode:826 meanR:39.6500 rate:0.0600 gloss:0.0003 dloss:8.7080 dlossQ:8.5843 dlossQsigm:0.1237\n",
      "Episode:827 meanR:39.7200 rate:0.1000 gloss:0.0024 dloss:10.0077 dlossQ:9.9264 dlossQsigm:0.0814\n",
      "Episode:828 meanR:39.9600 rate:0.1140 gloss:0.0005 dloss:6.6365 dlossQ:6.5440 dlossQsigm:0.0925\n",
      "Episode:829 meanR:39.9400 rate:0.0600 gloss:0.0002 dloss:5.4955 dlossQ:5.4270 dlossQsigm:0.0685\n",
      "Episode:830 meanR:39.9800 rate:0.0640 gloss:0.0001 dloss:5.3852 dlossQ:5.3108 dlossQsigm:0.0745\n",
      "Episode:831 meanR:39.4400 rate:0.0980 gloss:0.0000 dloss:4.5578 dlossQ:4.4794 dlossQsigm:0.0784\n",
      "Episode:832 meanR:39.5700 rate:0.0680 gloss:0.0000 dloss:5.3158 dlossQ:5.2039 dlossQsigm:0.1119\n",
      "Episode:833 meanR:39.5300 rate:0.0780 gloss:0.0000 dloss:3.2506 dlossQ:3.1663 dlossQsigm:0.0844\n",
      "Episode:834 meanR:39.0800 rate:0.0600 gloss:0.0000 dloss:3.5539 dlossQ:3.4630 dlossQsigm:0.0909\n",
      "Episode:835 meanR:39.0800 rate:0.0640 gloss:0.0000 dloss:2.1156 dlossQ:2.0481 dlossQsigm:0.0675\n",
      "Episode:836 meanR:39.4800 rate:0.1320 gloss:0.0002 dloss:7.6089 dlossQ:7.5393 dlossQsigm:0.0696\n",
      "Episode:837 meanR:39.6600 rate:0.1000 gloss:0.0020 dloss:6.9065 dlossQ:6.8361 dlossQsigm:0.0704\n",
      "Episode:838 meanR:39.4700 rate:0.0580 gloss:0.0005 dloss:6.1211 dlossQ:6.0496 dlossQsigm:0.0716\n",
      "Episode:839 meanR:39.4500 rate:0.1080 gloss:0.0007 dloss:7.8435 dlossQ:7.7622 dlossQsigm:0.0812\n",
      "Episode:840 meanR:39.4900 rate:0.0520 gloss:0.0003 dloss:7.2649 dlossQ:7.1757 dlossQsigm:0.0892\n",
      "Episode:841 meanR:39.3300 rate:0.0740 gloss:0.0000 dloss:1.9958 dlossQ:1.9440 dlossQsigm:0.0519\n",
      "Episode:842 meanR:39.4400 rate:0.0660 gloss:0.0000 dloss:0.8631 dlossQ:0.8149 dlossQsigm:0.0482\n",
      "Episode:843 meanR:39.6100 rate:0.0760 gloss:0.0000 dloss:2.2875 dlossQ:2.2261 dlossQsigm:0.0614\n",
      "Episode:844 meanR:39.3900 rate:0.0440 gloss:0.0000 dloss:1.0819 dlossQ:1.0311 dlossQsigm:0.0509\n",
      "Episode:845 meanR:39.4700 rate:0.0640 gloss:0.0000 dloss:1.0619 dlossQ:1.0046 dlossQsigm:0.0573\n",
      "Episode:846 meanR:39.7500 rate:0.1040 gloss:0.0030 dloss:2.4290 dlossQ:2.3624 dlossQsigm:0.0665\n",
      "Episode:847 meanR:39.4600 rate:0.0480 gloss:0.0000 dloss:2.9699 dlossQ:2.9207 dlossQsigm:0.0492\n",
      "Episode:848 meanR:39.2600 rate:0.0520 gloss:0.0001 dloss:6.2749 dlossQ:6.2009 dlossQsigm:0.0740\n",
      "Episode:849 meanR:39.0500 rate:0.1040 gloss:0.0000 dloss:13.4178 dlossQ:13.2779 dlossQsigm:0.1399\n",
      "Episode:850 meanR:39.0700 rate:0.1240 gloss:0.0000 dloss:2.3849 dlossQ:2.3377 dlossQsigm:0.0471\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "saver = tf.train.Saver()\n",
    "rewards_list, g_loss_list, d_loss_list = [], [], []\n",
    "rates_list, d_lossQ_list, d_lossQsigm_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        batch = [] # every data batch\n",
    "        total_reward = 0\n",
    "        state = env.reset() # env first state\n",
    "        g_initial_state = sess.run(model.g_initial_state)\n",
    "        d_initial_state = sess.run(model.d_initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing/inference\n",
    "            action_logits, g_final_state, d_final_state = sess.run(\n",
    "                fetches=[model.actions_logits, model.g_final_state, model.d_final_state], \n",
    "                feed_dict={model.states: np.reshape(state, [1, -1]),\n",
    "                           model.g_initial_state: g_initial_state,\n",
    "                           model.d_initial_state: d_initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([g_initial_state, g_final_state,\n",
    "                                  d_initial_state, d_final_state])\n",
    "            total_reward += reward\n",
    "            g_initial_state = g_final_state\n",
    "            d_initial_state = d_final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            g_initial_states = np.array([each[0] for each in rnn_states])\n",
    "            g_final_states = np.array([each[1] for each in rnn_states])\n",
    "            d_initial_states = np.array([each[2] for each in rnn_states])\n",
    "            d_final_states = np.array([each[3] for each in rnn_states])\n",
    "            nextQs_logits = sess.run(fetches = model.Qs_logits,\n",
    "                                     feed_dict = {model.states: next_states, \n",
    "                                                  model.g_initial_state: g_final_states[0].reshape([1, -1]),\n",
    "                                                  model.d_initial_state: d_final_states[0].reshape([1, -1])})\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            g_loss, d_loss, d_lossQ, d_lossQsigm, _, _ = sess.run(\n",
    "                fetches=[model.g_loss, model.d_loss, \n",
    "                         model.d_lossQ, model.d_lossQ_sigm,\n",
    "                         model.g_opt, model.d_opt], \n",
    "                feed_dict = {model.states: states, model.actions: actions,\n",
    "                             model.targetQs: targetQs,\n",
    "                             model.g_initial_state: g_initial_states[0].reshape([1, -1]),\n",
    "                             model.d_initial_state: d_initial_states[0].reshape([1, -1])})\n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        # Episode total reward and success rate/prob\n",
    "        episode_reward.append(total_reward) # stopping criteria\n",
    "        rate = total_reward/ 500 # success is 500 points: 0-1\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(g_loss),\n",
    "              'dloss:{:.4f}'.format(d_loss),\n",
    "              'dlossQ:{:.4f}'.format(d_lossQ),\n",
    "              'dlossQsigm:{:.4f}'.format(d_lossQsigm))\n",
    "        # Ploting out\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rates_list.append([ep, rate])\n",
    "        g_loss_list.append([ep, g_loss])\n",
    "        d_loss_list.append([ep, d_loss])\n",
    "        d_lossQ_list.append([ep, d_lossQ])\n",
    "        d_lossQsigm_list.append([ep, d_lossQsigm])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-seq3.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossR_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_lossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(100):\n",
    "    #while True:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        #for _ in range(111111111111111111):\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Print and break condition\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "        # if total_reward == 500:\n",
    "        #     break\n",
    "                \n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
