{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/aras/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score and steps: 0.0 and 299\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "for steps in range(1111111):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([action, state, reward, done])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, array([1.        , 0.        , 0.        , 0.        , 0.35186431,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.37953866,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.11957462,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.43679786,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.7516005 ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.6708644 ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.36187497,\n",
       "         0.        , 0.        ]), 0.0, False], (37,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, array([1.        , 0.        , 0.        , 0.        , 0.35186431,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.37953866,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.11957462,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.43679786,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.7516005 ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.6708644 ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.36187497,\n",
       "        0.        , 0.        ]), 0.0, False]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[1] for each in batch])\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 37) (300,) (300,)\n",
      "float64 float64 int64 bool\n",
      "3 0 4\n",
      "1.0 -1.0\n",
      "10.491800308227539 -10.474946975708008\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([776], (1, 37))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.observation_space, env.action_space\n",
    "env_info.agents, env_info.vector_observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 300 steps for an episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0], array([], shape=(0, 0), dtype=float64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.rewards, env_info.memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1), 'discrete', 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.previous_vector_actions.shape, brain.vector_action_space_type, brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_step%: 0.00299\n",
      "memory_step%: 0.00599\n",
      "memory_step%: 0.00899\n",
      "memory_step%: 0.01199\n",
      "memory_step%: 0.01499\n",
      "memory_step%: 0.01799\n",
      "memory_step%: 0.02099\n",
      "memory_step%: 0.02399\n",
      "memory_step%: 0.02699\n",
      "memory_step%: 0.02999\n",
      "memory_step%: 0.03299\n",
      "memory_step%: 0.03599\n",
      "memory_step%: 0.03899\n",
      "memory_step%: 0.04199\n",
      "memory_step%: 0.04499\n",
      "memory_step%: 0.04799\n",
      "memory_step%: 0.05099\n",
      "memory_step%: 0.05399\n",
      "memory_step%: 0.05699\n",
      "memory_step%: 0.05999\n",
      "memory_step%: 0.06299\n",
      "memory_step%: 0.06599\n",
      "memory_step%: 0.06899\n",
      "memory_step%: 0.07199\n",
      "memory_step%: 0.07499\n",
      "memory_step%: 0.07799\n",
      "memory_step%: 0.08099\n",
      "memory_step%: 0.08399\n",
      "memory_step%: 0.08699\n",
      "memory_step%: 0.08999\n",
      "memory_step%: 0.09299\n",
      "memory_step%: 0.09599\n",
      "memory_step%: 0.09899\n",
      "memory_step%: 0.10199\n",
      "memory_step%: 0.10499\n",
      "memory_step%: 0.10799\n",
      "memory_step%: 0.11099\n",
      "memory_step%: 0.11399\n",
      "memory_step%: 0.11699\n",
      "memory_step%: 0.11999\n",
      "memory_step%: 0.12299\n",
      "memory_step%: 0.12599\n",
      "memory_step%: 0.12899\n",
      "memory_step%: 0.13199\n",
      "memory_step%: 0.13499\n",
      "memory_step%: 0.13799\n",
      "memory_step%: 0.14099\n",
      "memory_step%: 0.14399\n",
      "memory_step%: 0.14699\n",
      "memory_step%: 0.14999\n",
      "memory_step%: 0.15299\n",
      "memory_step%: 0.15599\n",
      "memory_step%: 0.15899\n",
      "memory_step%: 0.16199\n",
      "memory_step%: 0.16499\n",
      "memory_step%: 0.16799\n",
      "memory_step%: 0.17099\n",
      "memory_step%: 0.17399\n",
      "memory_step%: 0.17699\n",
      "memory_step%: 0.17999\n",
      "memory_step%: 0.18299\n",
      "memory_step%: 0.18599\n",
      "memory_step%: 0.18899\n",
      "memory_step%: 0.19199\n",
      "memory_step%: 0.19499\n",
      "memory_step%: 0.19799\n",
      "memory_step%: 0.20099\n",
      "memory_step%: 0.20399\n",
      "memory_step%: 0.20699\n",
      "memory_step%: 0.20999\n",
      "memory_step%: 0.21299\n",
      "memory_step%: 0.21599\n",
      "memory_step%: 0.21899\n",
      "memory_step%: 0.22199\n",
      "memory_step%: 0.22499\n",
      "memory_step%: 0.22799\n",
      "memory_step%: 0.23099\n",
      "memory_step%: 0.23399\n",
      "memory_step%: 0.23699\n",
      "memory_step%: 0.23999\n",
      "memory_step%: 0.24299\n",
      "memory_step%: 0.24599\n",
      "memory_step%: 0.24899\n",
      "memory_step%: 0.25199\n",
      "memory_step%: 0.25499\n",
      "memory_step%: 0.25799\n",
      "memory_step%: 0.26099\n",
      "memory_step%: 0.26399\n",
      "memory_step%: 0.26699\n",
      "memory_step%: 0.26999\n",
      "memory_step%: 0.27299\n",
      "memory_step%: 0.27599\n",
      "memory_step%: 0.27899\n",
      "memory_step%: 0.28199\n",
      "memory_step%: 0.28499\n",
      "memory_step%: 0.28799\n",
      "memory_step%: 0.29099\n",
      "memory_step%: 0.29399\n",
      "memory_step%: 0.29699\n",
      "memory_step%: 0.29999\n",
      "memory_step%: 0.30299\n",
      "memory_step%: 0.30599\n",
      "memory_step%: 0.30899\n",
      "memory_step%: 0.31199\n",
      "memory_step%: 0.31499\n",
      "memory_step%: 0.31799\n",
      "memory_step%: 0.32099\n",
      "memory_step%: 0.32399\n",
      "memory_step%: 0.32699\n",
      "memory_step%: 0.32999\n",
      "memory_step%: 0.33299\n",
      "memory_step%: 0.33599\n",
      "memory_step%: 0.33899\n",
      "memory_step%: 0.34199\n",
      "memory_step%: 0.34499\n",
      "memory_step%: 0.34799\n",
      "memory_step%: 0.35099\n",
      "memory_step%: 0.35399\n",
      "memory_step%: 0.35699\n",
      "memory_step%: 0.35999\n",
      "memory_step%: 0.36299\n",
      "memory_step%: 0.36599\n",
      "memory_step%: 0.36899\n",
      "memory_step%: 0.37199\n",
      "memory_step%: 0.37499\n",
      "memory_step%: 0.37799\n",
      "memory_step%: 0.38099\n",
      "memory_step%: 0.38399\n",
      "memory_step%: 0.38699\n",
      "memory_step%: 0.38999\n",
      "memory_step%: 0.39299\n",
      "memory_step%: 0.39599\n",
      "memory_step%: 0.39899\n",
      "memory_step%: 0.40199\n",
      "memory_step%: 0.40499\n",
      "memory_step%: 0.40799\n",
      "memory_step%: 0.41099\n",
      "memory_step%: 0.41399\n",
      "memory_step%: 0.41699\n",
      "memory_step%: 0.41999\n",
      "memory_step%: 0.42299\n",
      "memory_step%: 0.42599\n",
      "memory_step%: 0.42899\n",
      "memory_step%: 0.43199\n",
      "memory_step%: 0.43499\n",
      "memory_step%: 0.43799\n",
      "memory_step%: 0.44099\n",
      "memory_step%: 0.44399\n",
      "memory_step%: 0.44699\n",
      "memory_step%: 0.44999\n",
      "memory_step%: 0.45299\n",
      "memory_step%: 0.45599\n",
      "memory_step%: 0.45899\n",
      "memory_step%: 0.46199\n",
      "memory_step%: 0.46499\n",
      "memory_step%: 0.46799\n",
      "memory_step%: 0.47099\n",
      "memory_step%: 0.47399\n",
      "memory_step%: 0.47699\n",
      "memory_step%: 0.47999\n",
      "memory_step%: 0.48299\n",
      "memory_step%: 0.48599\n",
      "memory_step%: 0.48899\n",
      "memory_step%: 0.49199\n",
      "memory_step%: 0.49499\n",
      "memory_step%: 0.49799\n",
      "memory_step%: 0.50099\n",
      "memory_step%: 0.50399\n",
      "memory_step%: 0.50699\n",
      "memory_step%: 0.50999\n",
      "memory_step%: 0.51299\n",
      "memory_step%: 0.51599\n",
      "memory_step%: 0.51899\n",
      "memory_step%: 0.52199\n",
      "memory_step%: 0.52499\n",
      "memory_step%: 0.52799\n",
      "memory_step%: 0.53099\n",
      "memory_step%: 0.53399\n",
      "memory_step%: 0.53699\n",
      "memory_step%: 0.53999\n",
      "memory_step%: 0.54299\n",
      "memory_step%: 0.54599\n",
      "memory_step%: 0.54899\n",
      "memory_step%: 0.55199\n",
      "memory_step%: 0.55499\n",
      "memory_step%: 0.55799\n",
      "memory_step%: 0.56099\n",
      "memory_step%: 0.56399\n",
      "memory_step%: 0.56699\n",
      "memory_step%: 0.56999\n",
      "memory_step%: 0.57299\n",
      "memory_step%: 0.57599\n",
      "memory_step%: 0.57899\n",
      "memory_step%: 0.58199\n",
      "memory_step%: 0.58499\n",
      "memory_step%: 0.58799\n",
      "memory_step%: 0.59099\n",
      "memory_step%: 0.59399\n",
      "memory_step%: 0.59699\n",
      "memory_step%: 0.59999\n",
      "memory_step%: 0.60299\n",
      "memory_step%: 0.60599\n",
      "memory_step%: 0.60899\n",
      "memory_step%: 0.61199\n",
      "memory_step%: 0.61499\n",
      "memory_step%: 0.61799\n",
      "memory_step%: 0.62099\n",
      "memory_step%: 0.62399\n",
      "memory_step%: 0.62699\n",
      "memory_step%: 0.62999\n",
      "memory_step%: 0.63299\n",
      "memory_step%: 0.63599\n",
      "memory_step%: 0.63899\n",
      "memory_step%: 0.64199\n",
      "memory_step%: 0.64499\n",
      "memory_step%: 0.64799\n",
      "memory_step%: 0.65099\n",
      "memory_step%: 0.65399\n",
      "memory_step%: 0.65699\n",
      "memory_step%: 0.65999\n",
      "memory_step%: 0.66299\n",
      "memory_step%: 0.66599\n",
      "memory_step%: 0.66899\n",
      "memory_step%: 0.67199\n",
      "memory_step%: 0.67499\n",
      "memory_step%: 0.67799\n",
      "memory_step%: 0.68099\n",
      "memory_step%: 0.68399\n",
      "memory_step%: 0.68699\n",
      "memory_step%: 0.68999\n",
      "memory_step%: 0.69299\n",
      "memory_step%: 0.69599\n",
      "memory_step%: 0.69899\n",
      "memory_step%: 0.70199\n",
      "memory_step%: 0.70499\n",
      "memory_step%: 0.70799\n",
      "memory_step%: 0.71099\n",
      "memory_step%: 0.71399\n",
      "memory_step%: 0.71699\n",
      "memory_step%: 0.71999\n",
      "memory_step%: 0.72299\n",
      "memory_step%: 0.72599\n",
      "memory_step%: 0.72899\n",
      "memory_step%: 0.73199\n",
      "memory_step%: 0.73499\n",
      "memory_step%: 0.73799\n",
      "memory_step%: 0.74099\n",
      "memory_step%: 0.74399\n",
      "memory_step%: 0.74699\n",
      "memory_step%: 0.74999\n",
      "memory_step%: 0.75299\n",
      "memory_step%: 0.75599\n",
      "memory_step%: 0.75899\n",
      "memory_step%: 0.76199\n",
      "memory_step%: 0.76499\n",
      "memory_step%: 0.76799\n",
      "memory_step%: 0.77099\n",
      "memory_step%: 0.77399\n",
      "memory_step%: 0.77699\n",
      "memory_step%: 0.77999\n",
      "memory_step%: 0.78299\n",
      "memory_step%: 0.78599\n",
      "memory_step%: 0.78899\n",
      "memory_step%: 0.79199\n",
      "memory_step%: 0.79499\n",
      "memory_step%: 0.79799\n",
      "memory_step%: 0.80099\n",
      "memory_step%: 0.80399\n",
      "memory_step%: 0.80699\n",
      "memory_step%: 0.80999\n",
      "memory_step%: 0.81299\n",
      "memory_step%: 0.81599\n",
      "memory_step%: 0.81899\n",
      "memory_step%: 0.82199\n",
      "memory_step%: 0.82499\n",
      "memory_step%: 0.82799\n",
      "memory_step%: 0.83099\n",
      "memory_step%: 0.83399\n",
      "memory_step%: 0.83699\n",
      "memory_step%: 0.83999\n",
      "memory_step%: 0.84299\n",
      "memory_step%: 0.84599\n",
      "memory_step%: 0.84899\n",
      "memory_step%: 0.85199\n",
      "memory_step%: 0.85499\n",
      "memory_step%: 0.85799\n",
      "memory_step%: 0.86099\n",
      "memory_step%: 0.86399\n",
      "memory_step%: 0.86699\n",
      "memory_step%: 0.86999\n",
      "memory_step%: 0.87299\n",
      "memory_step%: 0.87599\n",
      "memory_step%: 0.87899\n",
      "memory_step%: 0.88199\n",
      "memory_step%: 0.88499\n",
      "memory_step%: 0.88799\n",
      "memory_step%: 0.89099\n",
      "memory_step%: 0.89399\n",
      "memory_step%: 0.89699\n",
      "memory_step%: 0.89999\n",
      "memory_step%: 0.90299\n",
      "memory_step%: 0.90599\n",
      "memory_step%: 0.90899\n",
      "memory_step%: 0.91199\n",
      "memory_step%: 0.91499\n",
      "memory_step%: 0.91799\n",
      "memory_step%: 0.92099\n",
      "memory_step%: 0.92399\n",
      "memory_step%: 0.92699\n",
      "memory_step%: 0.92999\n",
      "memory_step%: 0.93299\n",
      "memory_step%: 0.93599\n",
      "memory_step%: 0.93899\n",
      "memory_step%: 0.94199\n",
      "memory_step%: 0.94499\n",
      "memory_step%: 0.94799\n",
      "memory_step%: 0.95099\n",
      "memory_step%: 0.95399\n",
      "memory_step%: 0.95699\n",
      "memory_step%: 0.95999\n",
      "memory_step%: 0.96299\n",
      "memory_step%: 0.96599\n",
      "memory_step%: 0.96899\n",
      "memory_step%: 0.97199\n",
      "memory_step%: 0.97499\n",
      "memory_step%: 0.97799\n",
      "memory_step%: 0.98099\n",
      "memory_step%: 0.98399\n",
      "memory_step%: 0.98699\n",
      "memory_step%: 0.98999\n",
      "memory_step%: 0.99299\n",
      "memory_step%: 0.99599\n",
      "memory_step%: 0.99899\n"
     ]
    }
   ],
   "source": [
    "#state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for memory_step in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        print('memory_step%:', memory_step/memory_size)\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        rate = total_reward/13\n",
    "        rate_clipped = np.clip(a=rate, a_max=1, a_min=0)\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate_clipped\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:-1.0000 R:-1.0000 rate:-0.0769 gloss:-6.9069 dlossA:0.4829 dlossQ:0.1751 exploreP:0.9707\n",
      "Episode:1 meanR:-1.5000 R:-2.0000 rate:-0.1538 gloss:-77.1990 dlossA:0.3626 dlossQ:0.1068 exploreP:0.9423\n",
      "Episode:2 meanR:-1.6667 R:-2.0000 rate:-0.1538 gloss:-319.3445 dlossA:0.3909 dlossQ:0.1963 exploreP:0.9148\n",
      "Episode:3 meanR:-1.2500 R:0.0000 rate:0.0000 gloss:-1005.6064 dlossA:0.4007 dlossQ:0.4141 exploreP:0.8881\n",
      "Episode:4 meanR:-1.0000 R:0.0000 rate:0.0000 gloss:-2700.9255 dlossA:0.4348 dlossQ:2.5160 exploreP:0.8621\n",
      "Episode:5 meanR:-1.1667 R:-2.0000 rate:-0.1538 gloss:-4791.1260 dlossA:0.4800 dlossQ:5.5895 exploreP:0.8369\n",
      "Episode:6 meanR:-1.1429 R:-1.0000 rate:-0.0769 gloss:-11913.0469 dlossA:0.5851 dlossQ:18.5628 exploreP:0.8125\n",
      "Episode:7 meanR:-1.0000 R:0.0000 rate:0.0000 gloss:-12518.2129 dlossA:0.4519 dlossQ:2.2688 exploreP:0.7888\n",
      "Episode:8 meanR:-0.8889 R:0.0000 rate:0.0000 gloss:-29970.9668 dlossA:0.7370 dlossQ:22.5055 exploreP:0.7657\n",
      "Episode:9 meanR:-0.9000 R:-1.0000 rate:-0.0769 gloss:-34915.1992 dlossA:0.8916 dlossQ:16.8459 exploreP:0.7434\n",
      "Episode:10 meanR:-0.7273 R:1.0000 rate:0.0769 gloss:-45671.9180 dlossA:0.9298 dlossQ:19.0026 exploreP:0.7217\n",
      "Episode:11 meanR:-0.5833 R:1.0000 rate:0.0769 gloss:-62213.0859 dlossA:1.6592 dlossQ:95.9680 exploreP:0.7007\n",
      "Episode:12 meanR:-0.4615 R:1.0000 rate:0.0769 gloss:-81557.0938 dlossA:1.1148 dlossQ:39.4075 exploreP:0.6803\n",
      "Episode:13 meanR:-0.3571 R:1.0000 rate:0.0769 gloss:-80331.6094 dlossA:1.1069 dlossQ:28.7690 exploreP:0.6605\n",
      "Episode:14 meanR:-0.3333 R:0.0000 rate:0.0000 gloss:-123317.6250 dlossA:1.7895 dlossQ:92.0397 exploreP:0.6413\n",
      "Episode:15 meanR:-0.2500 R:1.0000 rate:0.0769 gloss:-157914.7656 dlossA:1.8418 dlossQ:87.8476 exploreP:0.6226\n",
      "Episode:16 meanR:-0.1765 R:1.0000 rate:0.0769 gloss:-501956.7500 dlossA:6.1178 dlossQ:4598.7832 exploreP:0.6045\n",
      "Episode:17 meanR:-0.2222 R:-1.0000 rate:-0.0769 gloss:-256766.1406 dlossA:1.8043 dlossQ:128.4086 exploreP:0.5869\n",
      "Episode:18 meanR:-0.1579 R:1.0000 rate:0.0769 gloss:-535126.8125 dlossA:4.0581 dlossQ:803.4534 exploreP:0.5699\n",
      "Episode:19 meanR:-0.2000 R:-1.0000 rate:-0.0769 gloss:-390299.9062 dlossA:1.7039 dlossQ:123.9283 exploreP:0.5533\n",
      "Episode:20 meanR:-0.1905 R:0.0000 rate:0.0000 gloss:-632729.4375 dlossA:2.9619 dlossQ:358.3231 exploreP:0.5373\n",
      "Episode:21 meanR:-0.1818 R:0.0000 rate:0.0000 gloss:-656773.1250 dlossA:7.1708 dlossQ:3451.3411 exploreP:0.5217\n",
      "Episode:22 meanR:-0.2609 R:-2.0000 rate:-0.1538 gloss:-843462.0000 dlossA:3.3401 dlossQ:999.5730 exploreP:0.5066\n",
      "Episode:23 meanR:-0.2917 R:-1.0000 rate:-0.0769 gloss:-863605.4375 dlossA:2.8450 dlossQ:353.4261 exploreP:0.4919\n",
      "Episode:24 meanR:-0.2800 R:0.0000 rate:0.0000 gloss:-736973.2500 dlossA:3.8978 dlossQ:552.7712 exploreP:0.4776\n",
      "Episode:25 meanR:-0.3077 R:-1.0000 rate:-0.0769 gloss:-1318392.7500 dlossA:5.1190 dlossQ:1478.6603 exploreP:0.4638\n",
      "Episode:26 meanR:-0.3704 R:-2.0000 rate:-0.1538 gloss:-2228221.7500 dlossA:8.4936 dlossQ:6364.9023 exploreP:0.4504\n",
      "Episode:27 meanR:-0.3571 R:0.0000 rate:0.0000 gloss:-1528382.8750 dlossA:3.8452 dlossQ:586.9589 exploreP:0.4374\n",
      "Episode:28 meanR:-0.3448 R:0.0000 rate:0.0000 gloss:-3367737.0000 dlossA:10.0475 dlossQ:10871.8555 exploreP:0.4248\n",
      "Episode:29 meanR:-0.2667 R:2.0000 rate:0.1538 gloss:-5058340.5000 dlossA:15.7295 dlossQ:25132.5859 exploreP:0.4125\n",
      "Episode:30 meanR:-0.2581 R:0.0000 rate:0.0000 gloss:-3372287.5000 dlossA:5.8282 dlossQ:1669.7896 exploreP:0.4006\n",
      "Episode:31 meanR:-0.2500 R:0.0000 rate:0.0000 gloss:-2680048.0000 dlossA:4.6651 dlossQ:924.7186 exploreP:0.3891\n",
      "Episode:32 meanR:-0.2424 R:0.0000 rate:0.0000 gloss:-5075419.5000 dlossA:9.8988 dlossQ:4457.0142 exploreP:0.3779\n",
      "Episode:33 meanR:-0.2353 R:0.0000 rate:0.0000 gloss:-3799018.7500 dlossA:6.4414 dlossQ:1596.4435 exploreP:0.3670\n",
      "Episode:34 meanR:-0.2000 R:1.0000 rate:0.0769 gloss:-4844093.0000 dlossA:8.3646 dlossQ:2577.6243 exploreP:0.3564\n",
      "Episode:35 meanR:-0.1667 R:1.0000 rate:0.0769 gloss:-4020161.7500 dlossA:6.7769 dlossQ:1517.3953 exploreP:0.3462\n",
      "Episode:36 meanR:-0.1622 R:0.0000 rate:0.0000 gloss:-22017670.0000 dlossA:32.3818 dlossQ:174374.8438 exploreP:0.3363\n",
      "Episode:37 meanR:-0.1579 R:0.0000 rate:0.0000 gloss:-11958352.0000 dlossA:9.1919 dlossQ:7133.6084 exploreP:0.3266\n",
      "Episode:38 meanR:-0.1538 R:0.0000 rate:0.0000 gloss:-9890347.0000 dlossA:7.3133 dlossQ:3814.1179 exploreP:0.3173\n",
      "Episode:39 meanR:-0.1500 R:0.0000 rate:0.0000 gloss:-13194185.0000 dlossA:11.9678 dlossQ:9127.1787 exploreP:0.3082\n",
      "Episode:40 meanR:-0.1220 R:1.0000 rate:0.0769 gloss:-12417555.0000 dlossA:10.9378 dlossQ:6019.2705 exploreP:0.2994\n",
      "Episode:41 meanR:-0.1190 R:0.0000 rate:0.0000 gloss:-10871654.0000 dlossA:11.8632 dlossQ:5665.3623 exploreP:0.2908\n",
      "Episode:42 meanR:-0.1163 R:0.0000 rate:0.0000 gloss:-14262967.0000 dlossA:13.9310 dlossQ:8220.5479 exploreP:0.2825\n",
      "Episode:43 meanR:-0.0682 R:2.0000 rate:0.1538 gloss:-13088445.0000 dlossA:14.4175 dlossQ:7886.0200 exploreP:0.2745\n",
      "Episode:44 meanR:-0.0667 R:0.0000 rate:0.0000 gloss:-16457431.0000 dlossA:17.7906 dlossQ:11775.0596 exploreP:0.2666\n",
      "Episode:45 meanR:-0.0870 R:-1.0000 rate:-0.0769 gloss:-14860435.0000 dlossA:14.7033 dlossQ:7438.6240 exploreP:0.2591\n",
      "Episode:46 meanR:-0.0638 R:1.0000 rate:0.0769 gloss:-13261074.0000 dlossA:14.4896 dlossQ:5265.2202 exploreP:0.2517\n",
      "Episode:47 meanR:-0.0625 R:0.0000 rate:0.0000 gloss:-10810628.0000 dlossA:11.1465 dlossQ:2800.0771 exploreP:0.2446\n",
      "Episode:48 meanR:-0.0816 R:-1.0000 rate:-0.0769 gloss:-17539246.0000 dlossA:23.6051 dlossQ:17632.8691 exploreP:0.2376\n",
      "Episode:49 meanR:-0.0800 R:0.0000 rate:0.0000 gloss:-27223340.0000 dlossA:49.1192 dlossQ:132453.6250 exploreP:0.2309\n",
      "Episode:50 meanR:-0.0588 R:1.0000 rate:0.0769 gloss:-21054700.0000 dlossA:15.3642 dlossQ:8844.8760 exploreP:0.2244\n",
      "Episode:51 meanR:-0.0577 R:0.0000 rate:0.0000 gloss:-18952000.0000 dlossA:23.9036 dlossQ:21827.9609 exploreP:0.2180\n",
      "Episode:52 meanR:-0.0566 R:0.0000 rate:0.0000 gloss:-40811404.0000 dlossA:34.7224 dlossQ:51611.2695 exploreP:0.2119\n",
      "Episode:53 meanR:-0.0556 R:0.0000 rate:0.0000 gloss:-33622368.0000 dlossA:21.1977 dlossQ:17577.8574 exploreP:0.2059\n",
      "Episode:54 meanR:-0.0545 R:0.0000 rate:0.0000 gloss:-31128876.0000 dlossA:22.7056 dlossQ:19980.9102 exploreP:0.2001\n",
      "Episode:55 meanR:-0.0357 R:1.0000 rate:0.0769 gloss:-121605192.0000 dlossA:85.4473 dlossQ:1003500.9375 exploreP:0.1945\n",
      "Episode:56 meanR:-0.0175 R:1.0000 rate:0.0769 gloss:-100374896.0000 dlossA:39.6703 dlossQ:116069.8281 exploreP:0.1891\n",
      "Episode:57 meanR:0.0000 R:1.0000 rate:0.0769 gloss:-79612656.0000 dlossA:26.1846 dlossQ:50511.3008 exploreP:0.1838\n",
      "Episode:58 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-120277016.0000 dlossA:42.0434 dlossQ:147366.9688 exploreP:0.1786\n",
      "Episode:59 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-93757048.0000 dlossA:37.2974 dlossQ:76754.0703 exploreP:0.1736\n",
      "Episode:60 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-94191928.0000 dlossA:33.2849 dlossQ:56018.0391 exploreP:0.1688\n",
      "Episode:61 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-107492216.0000 dlossA:39.8398 dlossQ:81945.3359 exploreP:0.1641\n",
      "Episode:62 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-108670312.0000 dlossA:37.4197 dlossQ:73145.5859 exploreP:0.1596\n",
      "Episode:63 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-76506944.0000 dlossA:25.3564 dlossQ:24003.3438 exploreP:0.1551\n",
      "Episode:64 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-93053184.0000 dlossA:33.9062 dlossQ:48763.1133 exploreP:0.1509\n",
      "Episode:65 meanR:0.0152 R:1.0000 rate:0.0769 gloss:-130708464.0000 dlossA:104.4146 dlossQ:633318.8125 exploreP:0.1467\n",
      "Episode:66 meanR:0.0299 R:1.0000 rate:0.0769 gloss:-147453376.0000 dlossA:41.4449 dlossQ:93036.0703 exploreP:0.1426\n",
      "Episode:67 meanR:0.0294 R:0.0000 rate:0.0000 gloss:-143440992.0000 dlossA:41.2424 dlossQ:83642.5469 exploreP:0.1387\n",
      "Episode:68 meanR:0.0000 R:-2.0000 rate:-0.1538 gloss:-167735536.0000 dlossA:54.9019 dlossQ:121214.6172 exploreP:0.1349\n",
      "Episode:69 meanR:-0.0143 R:-1.0000 rate:-0.0769 gloss:-200791392.0000 dlossA:157.3754 dlossQ:1604682.5000 exploreP:0.1312\n",
      "Episode:70 meanR:-0.0141 R:0.0000 rate:0.0000 gloss:-196120176.0000 dlossA:34.6765 dlossQ:82199.9766 exploreP:0.1276\n",
      "Episode:71 meanR:-0.0139 R:0.0000 rate:0.0000 gloss:-182653856.0000 dlossA:34.6001 dlossQ:66720.9844 exploreP:0.1242\n",
      "Episode:72 meanR:-0.0137 R:0.0000 rate:0.0000 gloss:-254113168.0000 dlossA:119.8910 dlossQ:1089368.8750 exploreP:0.1208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:73 meanR:-0.0135 R:0.0000 rate:0.0000 gloss:-283036672.0000 dlossA:48.6431 dlossQ:137938.7188 exploreP:0.1175\n",
      "Episode:74 meanR:-0.0267 R:-1.0000 rate:-0.0769 gloss:-362214240.0000 dlossA:64.2790 dlossQ:234793.8125 exploreP:0.1143\n",
      "Episode:75 meanR:-0.0263 R:0.0000 rate:0.0000 gloss:-273323104.0000 dlossA:59.7721 dlossQ:197252.0781 exploreP:0.1113\n",
      "Episode:76 meanR:-0.0390 R:-1.0000 rate:-0.0769 gloss:-348165952.0000 dlossA:69.3539 dlossQ:221121.9531 exploreP:0.1083\n",
      "Episode:77 meanR:-0.0385 R:0.0000 rate:0.0000 gloss:-313758720.0000 dlossA:62.4059 dlossQ:154023.3906 exploreP:0.1054\n",
      "Episode:78 meanR:-0.0380 R:0.0000 rate:0.0000 gloss:-424525728.0000 dlossA:193.3999 dlossQ:1934426.0000 exploreP:0.1025\n",
      "Episode:79 meanR:-0.0375 R:0.0000 rate:0.0000 gloss:-274140128.0000 dlossA:41.0935 dlossQ:81626.3750 exploreP:0.0998\n",
      "Episode:80 meanR:-0.0370 R:0.0000 rate:0.0000 gloss:-525860928.0000 dlossA:91.0624 dlossQ:400858.7188 exploreP:0.0972\n",
      "Episode:81 meanR:-0.0366 R:0.0000 rate:0.0000 gloss:-329542304.0000 dlossA:77.0966 dlossQ:306796.2500 exploreP:0.0946\n",
      "Episode:82 meanR:-0.0361 R:0.0000 rate:0.0000 gloss:-650732032.0000 dlossA:160.4179 dlossQ:1540679.8750 exploreP:0.0921\n",
      "Episode:83 meanR:-0.0238 R:1.0000 rate:0.0769 gloss:-443782528.0000 dlossA:86.6014 dlossQ:295170.8125 exploreP:0.0897\n",
      "Episode:84 meanR:-0.0235 R:0.0000 rate:0.0000 gloss:-531832896.0000 dlossA:90.5350 dlossQ:392029.8750 exploreP:0.0873\n",
      "Episode:85 meanR:-0.0116 R:1.0000 rate:0.0769 gloss:-575404416.0000 dlossA:101.2654 dlossQ:513699.5625 exploreP:0.0850\n",
      "Episode:86 meanR:0.0000 R:1.0000 rate:0.0769 gloss:-491173792.0000 dlossA:81.5366 dlossQ:266924.5625 exploreP:0.0828\n",
      "Episode:87 meanR:-0.0114 R:-1.0000 rate:-0.0769 gloss:-548019584.0000 dlossA:75.5448 dlossQ:230518.5000 exploreP:0.0806\n",
      "Episode:88 meanR:-0.0112 R:0.0000 rate:0.0000 gloss:-537619648.0000 dlossA:162.9731 dlossQ:1557851.6250 exploreP:0.0786\n",
      "Episode:89 meanR:-0.0111 R:0.0000 rate:0.0000 gloss:-789963584.0000 dlossA:183.9374 dlossQ:1036416.6250 exploreP:0.0765\n",
      "Episode:90 meanR:-0.0110 R:0.0000 rate:0.0000 gloss:-661111616.0000 dlossA:90.5413 dlossQ:322890.5000 exploreP:0.0746\n",
      "Episode:91 meanR:-0.0217 R:-1.0000 rate:-0.0769 gloss:-742683008.0000 dlossA:163.3394 dlossQ:771710.8125 exploreP:0.0727\n",
      "Episode:92 meanR:-0.0215 R:0.0000 rate:0.0000 gloss:-793953856.0000 dlossA:92.5488 dlossQ:473623.9062 exploreP:0.0708\n",
      "Episode:93 meanR:-0.0106 R:1.0000 rate:0.0769 gloss:-993981888.0000 dlossA:178.8250 dlossQ:2603484.7500 exploreP:0.0690\n",
      "Episode:94 meanR:-0.0211 R:-1.0000 rate:-0.0769 gloss:-1043853248.0000 dlossA:111.9327 dlossQ:525017.0000 exploreP:0.0673\n",
      "Episode:95 meanR:-0.0312 R:-1.0000 rate:-0.0769 gloss:-888785280.0000 dlossA:144.6830 dlossQ:921226.1250 exploreP:0.0656\n",
      "Episode:96 meanR:-0.0309 R:0.0000 rate:0.0000 gloss:-1151546112.0000 dlossA:203.4302 dlossQ:1696429.5000 exploreP:0.0639\n",
      "Episode:97 meanR:-0.0408 R:-1.0000 rate:-0.0769 gloss:-1260731648.0000 dlossA:177.2786 dlossQ:1110594.5000 exploreP:0.0623\n",
      "Episode:98 meanR:-0.0505 R:-1.0000 rate:-0.0769 gloss:-1158771968.0000 dlossA:195.7567 dlossQ:1478221.2500 exploreP:0.0608\n",
      "Episode:99 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-8318804992.0000 dlossA:540.9952 dlossQ:109411544.0000 exploreP:0.0593\n",
      "Episode:100 meanR:-0.0400 R:0.0000 rate:0.0000 gloss:-2815535872.0000 dlossA:142.4242 dlossQ:1905740.3750 exploreP:0.0578\n",
      "Episode:101 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-3140043520.0000 dlossA:162.1836 dlossQ:2301775.7500 exploreP:0.0564\n",
      "Episode:102 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-2678653696.0000 dlossA:137.2921 dlossQ:1934617.8750 exploreP:0.0550\n",
      "Episode:103 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-2747371264.0000 dlossA:666.7607 dlossQ:34277736.0000 exploreP:0.0537\n",
      "Episode:104 meanR:0.0200 R:1.0000 rate:0.0769 gloss:-3846660096.0000 dlossA:151.2285 dlossQ:2899435.0000 exploreP:0.0524\n",
      "Episode:105 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-4245855488.0000 dlossA:294.5957 dlossQ:9708293.0000 exploreP:0.0512\n",
      "Episode:106 meanR:0.0500 R:0.0000 rate:0.0000 gloss:-2830405376.0000 dlossA:138.1446 dlossQ:1822108.3750 exploreP:0.0500\n",
      "Episode:107 meanR:0.0500 R:0.0000 rate:0.0000 gloss:-2815177728.0000 dlossA:133.2211 dlossQ:1549278.0000 exploreP:0.0488\n",
      "Episode:108 meanR:0.0500 R:0.0000 rate:0.0000 gloss:-3366578432.0000 dlossA:157.7086 dlossQ:2554127.0000 exploreP:0.0476\n",
      "Episode:109 meanR:0.0600 R:0.0000 rate:0.0000 gloss:-2876634624.0000 dlossA:170.7592 dlossQ:2320930.7500 exploreP:0.0465\n",
      "Episode:110 meanR:0.0500 R:0.0000 rate:0.0000 gloss:-3324603392.0000 dlossA:179.0991 dlossQ:2092841.6250 exploreP:0.0454\n",
      "Episode:111 meanR:0.0300 R:-1.0000 rate:-0.0769 gloss:-2728838400.0000 dlossA:147.2545 dlossQ:1179106.3750 exploreP:0.0444\n",
      "Episode:112 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-3230639872.0000 dlossA:303.3431 dlossQ:4676772.5000 exploreP:0.0434\n",
      "Episode:113 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-3122657536.0000 dlossA:185.2906 dlossQ:1820777.0000 exploreP:0.0424\n",
      "Episode:114 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-3271829248.0000 dlossA:174.6489 dlossQ:1555706.5000 exploreP:0.0414\n",
      "Episode:115 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-2469962752.0000 dlossA:319.7380 dlossQ:4608026.5000 exploreP:0.0405\n",
      "Episode:116 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-2614744576.0000 dlossA:198.9624 dlossQ:1643778.7500 exploreP:0.0396\n",
      "Episode:117 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-3497577728.0000 dlossA:221.0814 dlossQ:2045190.8750 exploreP:0.0387\n",
      "Episode:118 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-3088664832.0000 dlossA:259.0081 dlossQ:3062776.0000 exploreP:0.0379\n",
      "Episode:119 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-3986206720.0000 dlossA:337.0016 dlossQ:6385624.0000 exploreP:0.0371\n",
      "Episode:120 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-5410457088.0000 dlossA:656.3353 dlossQ:19387372.0000 exploreP:0.0363\n",
      "Episode:121 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-5127659520.0000 dlossA:213.5173 dlossQ:3000043.2500 exploreP:0.0355\n",
      "Episode:122 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-4638076416.0000 dlossA:280.3335 dlossQ:3756046.0000 exploreP:0.0347\n",
      "Episode:123 meanR:0.0300 R:1.0000 rate:0.0769 gloss:-4410587136.0000 dlossA:281.7061 dlossQ:4228868.5000 exploreP:0.0340\n",
      "Episode:124 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-5468396544.0000 dlossA:246.3523 dlossQ:2581549.2500 exploreP:0.0333\n",
      "Episode:125 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-4378044416.0000 dlossA:208.6425 dlossQ:1814440.5000 exploreP:0.0326\n",
      "Episode:126 meanR:0.0600 R:0.0000 rate:0.0000 gloss:-4718283264.0000 dlossA:251.2093 dlossQ:2310693.7500 exploreP:0.0319\n",
      "Episode:127 meanR:0.0600 R:0.0000 rate:0.0000 gloss:-4841509376.0000 dlossA:522.9705 dlossQ:17014954.0000 exploreP:0.0313\n",
      "Episode:128 meanR:0.0600 R:0.0000 rate:0.0000 gloss:-7566765568.0000 dlossA:508.7366 dlossQ:12758700.0000 exploreP:0.0306\n",
      "Episode:129 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-6832355840.0000 dlossA:347.4223 dlossQ:4512998.0000 exploreP:0.0300\n",
      "Episode:130 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-6239007232.0000 dlossA:497.3479 dlossQ:11724342.0000 exploreP:0.0294\n",
      "Episode:131 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-7636445184.0000 dlossA:353.5605 dlossQ:4516323.0000 exploreP:0.0289\n",
      "Episode:132 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-7916660736.0000 dlossA:401.3332 dlossQ:9969905.0000 exploreP:0.0283\n",
      "Episode:133 meanR:0.0300 R:-1.0000 rate:-0.0769 gloss:-7122102784.0000 dlossA:328.8349 dlossQ:8140708.0000 exploreP:0.0278\n",
      "Episode:134 meanR:0.0300 R:1.0000 rate:0.0769 gloss:-8606180352.0000 dlossA:464.0476 dlossQ:10991141.0000 exploreP:0.0272\n",
      "Episode:135 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-8492934144.0000 dlossA:428.9935 dlossQ:6356674.5000 exploreP:0.0267\n",
      "Episode:136 meanR:0.0300 R:1.0000 rate:0.0769 gloss:-9668337664.0000 dlossA:733.5839 dlossQ:39006120.0000 exploreP:0.0262\n",
      "Episode:137 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-8006887936.0000 dlossA:286.7373 dlossQ:3900988.5000 exploreP:0.0258\n",
      "Episode:138 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-8186011136.0000 dlossA:295.0811 dlossQ:4275486.5000 exploreP:0.0253\n",
      "Episode:139 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-8287492096.0000 dlossA:437.4493 dlossQ:7060510.5000 exploreP:0.0248\n",
      "Episode:140 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-8388424704.0000 dlossA:375.7519 dlossQ:4639694.5000 exploreP:0.0244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:141 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-9811270656.0000 dlossA:863.6113 dlossQ:39768232.0000 exploreP:0.0240\n",
      "Episode:142 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-8564270592.0000 dlossA:284.2389 dlossQ:3335907.0000 exploreP:0.0236\n",
      "Episode:143 meanR:-0.0100 R:-1.0000 rate:-0.0769 gloss:-7012984832.0000 dlossA:422.9839 dlossQ:8507991.0000 exploreP:0.0232\n",
      "Episode:144 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-12561436672.0000 dlossA:627.9620 dlossQ:16896228.0000 exploreP:0.0228\n",
      "Episode:145 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-13921690624.0000 dlossA:582.2523 dlossQ:13968337.0000 exploreP:0.0224\n",
      "Episode:146 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-12448468992.0000 dlossA:447.6716 dlossQ:8679965.0000 exploreP:0.0220\n",
      "Episode:147 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-12081718272.0000 dlossA:481.5461 dlossQ:9113263.0000 exploreP:0.0217\n",
      "Episode:148 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-12885232640.0000 dlossA:787.8142 dlossQ:31231424.0000 exploreP:0.0213\n",
      "Episode:149 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-13987726336.0000 dlossA:884.0150 dlossQ:28331596.0000 exploreP:0.0210\n",
      "Episode:150 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-13138971648.0000 dlossA:536.9202 dlossQ:10993180.0000 exploreP:0.0207\n",
      "Episode:151 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-20139266048.0000 dlossA:1327.2021 dlossQ:139498160.0000 exploreP:0.0204\n",
      "Episode:152 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-19264409600.0000 dlossA:589.2867 dlossQ:19559048.0000 exploreP:0.0201\n",
      "Episode:153 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-23004225536.0000 dlossA:993.5171 dlossQ:53589144.0000 exploreP:0.0198\n",
      "Episode:154 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-20715919360.0000 dlossA:450.2005 dlossQ:11079901.0000 exploreP:0.0195\n",
      "Episode:155 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-18616590336.0000 dlossA:383.9412 dlossQ:7132016.0000 exploreP:0.0192\n",
      "Episode:156 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-19464065024.0000 dlossA:477.1151 dlossQ:11235960.0000 exploreP:0.0189\n",
      "Episode:157 meanR:-0.0400 R:0.0000 rate:0.0000 gloss:-17311483904.0000 dlossA:662.2410 dlossQ:21736464.0000 exploreP:0.0187\n",
      "Episode:158 meanR:-0.0300 R:1.0000 rate:0.0769 gloss:-17130910720.0000 dlossA:395.8549 dlossQ:6107152.0000 exploreP:0.0184\n",
      "Episode:159 meanR:-0.0200 R:1.0000 rate:0.0769 gloss:-16559549440.0000 dlossA:538.0672 dlossQ:12526536.0000 exploreP:0.0181\n",
      "Episode:160 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-25693761536.0000 dlossA:1123.9216 dlossQ:50830184.0000 exploreP:0.0179\n",
      "Episode:161 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-21335431168.0000 dlossA:486.4537 dlossQ:10714064.0000 exploreP:0.0177\n",
      "Episode:162 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-18543835136.0000 dlossA:695.1891 dlossQ:29708066.0000 exploreP:0.0174\n",
      "Episode:163 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-23391649792.0000 dlossA:1640.3197 dlossQ:120466216.0000 exploreP:0.0172\n",
      "Episode:164 meanR:0.0000 R:2.0000 rate:0.1538 gloss:-33471793152.0000 dlossA:642.8318 dlossQ:22013148.0000 exploreP:0.0170\n",
      "Episode:165 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-32478740480.0000 dlossA:1057.0299 dlossQ:67599576.0000 exploreP:0.0168\n",
      "Episode:166 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-24486639616.0000 dlossA:590.0393 dlossQ:18526126.0000 exploreP:0.0166\n",
      "Episode:167 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-42665287680.0000 dlossA:1684.4331 dlossQ:127138968.0000 exploreP:0.0164\n",
      "Episode:168 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-32454426624.0000 dlossA:630.5533 dlossQ:24151628.0000 exploreP:0.0162\n",
      "Episode:169 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-37434650624.0000 dlossA:903.8656 dlossQ:79757984.0000 exploreP:0.0160\n",
      "Episode:170 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-35950149632.0000 dlossA:973.8649 dlossQ:56156968.0000 exploreP:0.0159\n",
      "Episode:171 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-39715815424.0000 dlossA:607.9066 dlossQ:24381748.0000 exploreP:0.0157\n",
      "Episode:172 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-39714119680.0000 dlossA:817.1061 dlossQ:29693270.0000 exploreP:0.0155\n",
      "Episode:173 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-40705232896.0000 dlossA:1180.1392 dlossQ:59719244.0000 exploreP:0.0154\n",
      "Episode:174 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-38791847936.0000 dlossA:778.4294 dlossQ:28942376.0000 exploreP:0.0152\n",
      "Episode:175 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-22523492352.0000 dlossA:386.4098 dlossQ:5135768.5000 exploreP:0.0150\n",
      "Episode:176 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-25068324864.0000 dlossA:847.7866 dlossQ:35575456.0000 exploreP:0.0149\n",
      "Episode:177 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-50215444480.0000 dlossA:1381.9197 dlossQ:119024760.0000 exploreP:0.0147\n",
      "Episode:178 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-47766921216.0000 dlossA:2396.1968 dlossQ:344257952.0000 exploreP:0.0146\n",
      "Episode:179 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-63066087424.0000 dlossA:1534.5863 dlossQ:167327568.0000 exploreP:0.0145\n",
      "Episode:180 meanR:0.0400 R:1.0000 rate:0.0769 gloss:-58421805056.0000 dlossA:1129.1360 dlossQ:74498824.0000 exploreP:0.0143\n",
      "Episode:181 meanR:0.0500 R:1.0000 rate:0.0769 gloss:-40341966848.0000 dlossA:569.5034 dlossQ:19437596.0000 exploreP:0.0142\n",
      "Episode:182 meanR:0.0500 R:0.0000 rate:0.0000 gloss:-41662062592.0000 dlossA:756.5428 dlossQ:25282758.0000 exploreP:0.0141\n",
      "Episode:183 meanR:0.0300 R:-1.0000 rate:-0.0769 gloss:-79098494976.0000 dlossA:3579.4670 dlossQ:924507200.0000 exploreP:0.0140\n",
      "Episode:184 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-77552508928.0000 dlossA:804.0704 dlossQ:54613516.0000 exploreP:0.0138\n",
      "Episode:185 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-74193051648.0000 dlossA:949.6611 dlossQ:67829168.0000 exploreP:0.0137\n",
      "Episode:186 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-73066799104.0000 dlossA:967.5967 dlossQ:75346112.0000 exploreP:0.0136\n",
      "Episode:187 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-72368586752.0000 dlossA:1233.1198 dlossQ:147629360.0000 exploreP:0.0135\n",
      "Episode:188 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-78374977536.0000 dlossA:786.5001 dlossQ:55211644.0000 exploreP:0.0134\n",
      "Episode:189 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-53678649344.0000 dlossA:714.5020 dlossQ:31337384.0000 exploreP:0.0133\n",
      "Episode:190 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-94641577984.0000 dlossA:1607.4337 dlossQ:127956080.0000 exploreP:0.0132\n",
      "Episode:191 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-70503694336.0000 dlossA:1077.2742 dlossQ:58306648.0000 exploreP:0.0131\n",
      "Episode:192 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-54239432704.0000 dlossA:828.7681 dlossQ:30703616.0000 exploreP:0.0130\n",
      "Episode:193 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-59331756032.0000 dlossA:1916.5100 dlossQ:237825008.0000 exploreP:0.0129\n",
      "Episode:194 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-63128420352.0000 dlossA:768.2278 dlossQ:37195460.0000 exploreP:0.0129\n",
      "Episode:195 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-108060114944.0000 dlossA:5791.5684 dlossQ:2526994176.0000 exploreP:0.0128\n",
      "Episode:196 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-96148955136.0000 dlossA:810.8450 dlossQ:75419720.0000 exploreP:0.0127\n",
      "Episode:197 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-179045351424.0000 dlossA:2199.3083 dlossQ:536044288.0000 exploreP:0.0126\n",
      "Episode:198 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-191347245056.0000 dlossA:1384.1044 dlossQ:219288432.0000 exploreP:0.0125\n",
      "Episode:199 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-108306472960.0000 dlossA:970.3954 dlossQ:81159752.0000 exploreP:0.0125\n",
      "Episode:200 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-102281011200.0000 dlossA:858.8514 dlossQ:56471716.0000 exploreP:0.0124\n",
      "Episode:201 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-152949850112.0000 dlossA:1190.3960 dlossQ:143147056.0000 exploreP:0.0123\n",
      "Episode:202 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-127546802176.0000 dlossA:1158.8604 dlossQ:112578664.0000 exploreP:0.0122\n",
      "Episode:203 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-128268419072.0000 dlossA:1156.8505 dlossQ:120092656.0000 exploreP:0.0122\n",
      "Episode:204 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-125746618368.0000 dlossA:1002.6177 dlossQ:81419440.0000 exploreP:0.0121\n",
      "Episode:205 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-144494788608.0000 dlossA:1506.1924 dlossQ:157322640.0000 exploreP:0.0120\n",
      "Episode:206 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-116229824512.0000 dlossA:1297.8273 dlossQ:94576520.0000 exploreP:0.0120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:207 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-85434466304.0000 dlossA:887.5261 dlossQ:39186728.0000 exploreP:0.0119\n",
      "Episode:208 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-138156982272.0000 dlossA:3287.1882 dlossQ:656013376.0000 exploreP:0.0119\n",
      "Episode:209 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-133987655680.0000 dlossA:1727.2306 dlossQ:159948192.0000 exploreP:0.0118\n",
      "Episode:210 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-145395236864.0000 dlossA:1474.2606 dlossQ:144105184.0000 exploreP:0.0118\n",
      "Episode:211 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-142124154880.0000 dlossA:1948.9767 dlossQ:252264144.0000 exploreP:0.0117\n",
      "Episode:212 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-104924454912.0000 dlossA:1639.3015 dlossQ:167734688.0000 exploreP:0.0117\n",
      "Episode:213 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-150209708032.0000 dlossA:8708.8408 dlossQ:3509609984.0000 exploreP:0.0116\n",
      "Episode:214 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-180048445440.0000 dlossA:1088.7241 dlossQ:120072208.0000 exploreP:0.0116\n",
      "Episode:215 meanR:0.0400 R:1.0000 rate:0.0769 gloss:-193007714304.0000 dlossA:1173.6915 dlossQ:133148560.0000 exploreP:0.0115\n",
      "Episode:216 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-182358163456.0000 dlossA:1236.9386 dlossQ:126616480.0000 exploreP:0.0115\n",
      "Episode:217 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-191655149568.0000 dlossA:1278.8378 dlossQ:141622528.0000 exploreP:0.0114\n",
      "Episode:218 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-202289496064.0000 dlossA:2493.1812 dlossQ:478226848.0000 exploreP:0.0114\n",
      "Episode:219 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-165067620352.0000 dlossA:1331.0662 dlossQ:125373768.0000 exploreP:0.0113\n",
      "Episode:220 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-198091997184.0000 dlossA:1624.8806 dlossQ:180279424.0000 exploreP:0.0113\n",
      "Episode:221 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-166993051648.0000 dlossA:1378.2642 dlossQ:98794368.0000 exploreP:0.0113\n",
      "Episode:222 meanR:0.0400 R:0.0000 rate:0.0000 gloss:-154716667904.0000 dlossA:1881.5145 dlossQ:181369600.0000 exploreP:0.0112\n",
      "Episode:223 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-159753715712.0000 dlossA:3595.1750 dlossQ:656725248.0000 exploreP:0.0112\n",
      "Episode:224 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-177340940288.0000 dlossA:3260.5354 dlossQ:751814144.0000 exploreP:0.0112\n",
      "Episode:225 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-156514254848.0000 dlossA:1456.8715 dlossQ:128285408.0000 exploreP:0.0111\n",
      "Episode:226 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-107302150144.0000 dlossA:1955.8363 dlossQ:184407648.0000 exploreP:0.0111\n",
      "Episode:227 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-178709233664.0000 dlossA:2261.6846 dlossQ:304921600.0000 exploreP:0.0111\n",
      "Episode:228 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-253631184896.0000 dlossA:2057.2439 dlossQ:284981504.0000 exploreP:0.0110\n",
      "Episode:229 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-196544528384.0000 dlossA:1929.9083 dlossQ:177242688.0000 exploreP:0.0110\n",
      "Episode:230 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-247585898496.0000 dlossA:4039.1357 dlossQ:1021236224.0000 exploreP:0.0110\n",
      "Episode:231 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-268190236672.0000 dlossA:3072.4399 dlossQ:438283360.0000 exploreP:0.0109\n",
      "Episode:232 meanR:0.0200 R:-1.0000 rate:-0.0769 gloss:-212836007936.0000 dlossA:2243.9707 dlossQ:273171072.0000 exploreP:0.0109\n",
      "Episode:233 meanR:0.0300 R:0.0000 rate:0.0000 gloss:-194420457472.0000 dlossA:2338.2112 dlossQ:286322304.0000 exploreP:0.0109\n",
      "Episode:234 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-362959339520.0000 dlossA:4738.7695 dlossQ:2270631936.0000 exploreP:0.0109\n",
      "Episode:235 meanR:0.0300 R:1.0000 rate:0.0769 gloss:-481366736896.0000 dlossA:2177.6584 dlossQ:543493568.0000 exploreP:0.0108\n",
      "Episode:236 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-375384571904.0000 dlossA:1855.7007 dlossQ:448978112.0000 exploreP:0.0108\n",
      "Episode:237 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-260065198080.0000 dlossA:1306.1665 dlossQ:154312544.0000 exploreP:0.0108\n",
      "Episode:238 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-439761469440.0000 dlossA:2434.4426 dlossQ:618617152.0000 exploreP:0.0108\n",
      "Episode:239 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-312142299136.0000 dlossA:1634.1292 dlossQ:232416896.0000 exploreP:0.0107\n",
      "Episode:240 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-357209571328.0000 dlossA:2010.1418 dlossQ:352366336.0000 exploreP:0.0107\n",
      "Episode:241 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-358144966656.0000 dlossA:2051.9158 dlossQ:315641024.0000 exploreP:0.0107\n",
      "Episode:242 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-281784483840.0000 dlossA:1715.5612 dlossQ:182654064.0000 exploreP:0.0107\n",
      "Episode:243 meanR:0.0200 R:-1.0000 rate:-0.0769 gloss:-217572605952.0000 dlossA:1583.4423 dlossQ:136358656.0000 exploreP:0.0107\n",
      "Episode:244 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-292944609280.0000 dlossA:3295.3989 dlossQ:614318400.0000 exploreP:0.0106\n",
      "Episode:245 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-278278144000.0000 dlossA:2342.5637 dlossQ:347377344.0000 exploreP:0.0106\n",
      "Episode:246 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-509708599296.0000 dlossA:14319.7236 dlossQ:14866111488.0000 exploreP:0.0106\n",
      "Episode:247 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-379888074752.0000 dlossA:1485.5927 dlossQ:212957632.0000 exploreP:0.0106\n",
      "Episode:248 meanR:0.0200 R:1.0000 rate:0.0769 gloss:-495481061376.0000 dlossA:1705.1868 dlossQ:396293120.0000 exploreP:0.0106\n",
      "Episode:249 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-613138563072.0000 dlossA:2679.2786 dlossQ:1230259328.0000 exploreP:0.0105\n",
      "Episode:250 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-565374484480.0000 dlossA:2270.7146 dlossQ:712400128.0000 exploreP:0.0105\n",
      "Episode:251 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-751184379904.0000 dlossA:2598.2122 dlossQ:1118600832.0000 exploreP:0.0105\n",
      "Episode:252 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-585209872384.0000 dlossA:3298.9438 dlossQ:1419706624.0000 exploreP:0.0105\n",
      "Episode:253 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-694746808320.0000 dlossA:3127.9380 dlossQ:1201346944.0000 exploreP:0.0105\n",
      "Episode:254 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-764199436288.0000 dlossA:3451.8162 dlossQ:1114055936.0000 exploreP:0.0105\n",
      "Episode:255 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1398388293632.0000 dlossA:6738.2490 dlossQ:9060504576.0000 exploreP:0.0105\n",
      "Episode:256 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-949834940416.0000 dlossA:4240.0811 dlossQ:2066929792.0000 exploreP:0.0104\n",
      "Episode:257 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-750984167424.0000 dlossA:3410.3833 dlossQ:1686932864.0000 exploreP:0.0104\n",
      "Episode:258 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-545500004352.0000 dlossA:1949.8853 dlossQ:416502720.0000 exploreP:0.0104\n",
      "Episode:259 meanR:-0.0100 R:-1.0000 rate:-0.0769 gloss:-726229843968.0000 dlossA:5387.7432 dlossQ:2610222080.0000 exploreP:0.0104\n",
      "Episode:260 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-789576417280.0000 dlossA:6050.5259 dlossQ:3245018112.0000 exploreP:0.0104\n",
      "Episode:261 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-581036539904.0000 dlossA:2451.1484 dlossQ:659761792.0000 exploreP:0.0104\n",
      "Episode:262 meanR:0.0000 R:1.0000 rate:0.0769 gloss:-428456706048.0000 dlossA:1789.7750 dlossQ:168837120.0000 exploreP:0.0104\n",
      "Episode:263 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-917697855488.0000 dlossA:7073.1948 dlossQ:6380109824.0000 exploreP:0.0104\n",
      "Episode:264 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-627164577792.0000 dlossA:2389.3562 dlossQ:561476224.0000 exploreP:0.0103\n",
      "Episode:265 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-547915235328.0000 dlossA:3417.7166 dlossQ:1028747456.0000 exploreP:0.0103\n",
      "Episode:266 meanR:-0.0300 R:-1.0000 rate:-0.0769 gloss:-588896600064.0000 dlossA:3015.0957 dlossQ:642533312.0000 exploreP:0.0103\n",
      "Episode:267 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-383337824256.0000 dlossA:2495.2471 dlossQ:346161376.0000 exploreP:0.0103\n",
      "Episode:268 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-815857664000.0000 dlossA:9838.3848 dlossQ:7853581824.0000 exploreP:0.0103\n",
      "Episode:269 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-671898992640.0000 dlossA:2730.0283 dlossQ:784442112.0000 exploreP:0.0103\n",
      "Episode:270 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-575438389248.0000 dlossA:2799.1165 dlossQ:561869504.0000 exploreP:0.0103\n",
      "Episode:271 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-614448889856.0000 dlossA:7578.0952 dlossQ:4705108992.0000 exploreP:0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:272 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-336874078208.0000 dlossA:4027.5771 dlossQ:1229126912.0000 exploreP:0.0103\n",
      "Episode:273 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-692834271232.0000 dlossA:5398.3257 dlossQ:2257538816.0000 exploreP:0.0103\n",
      "Episode:274 meanR:-0.0200 R:1.0000 rate:0.0769 gloss:-654862319616.0000 dlossA:2924.3843 dlossQ:712295488.0000 exploreP:0.0103\n",
      "Episode:275 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-455325483008.0000 dlossA:2795.8691 dlossQ:466320864.0000 exploreP:0.0103\n",
      "Episode:276 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-610725920768.0000 dlossA:3282.0710 dlossQ:1014050240.0000 exploreP:0.0102\n",
      "Episode:277 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-741638537216.0000 dlossA:5164.2388 dlossQ:3746606592.0000 exploreP:0.0102\n",
      "Episode:278 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-826264518656.0000 dlossA:6377.8965 dlossQ:3658332672.0000 exploreP:0.0102\n",
      "Episode:279 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-475789426688.0000 dlossA:2727.3684 dlossQ:480333760.0000 exploreP:0.0102\n",
      "Episode:280 meanR:-0.0200 R:1.0000 rate:0.0769 gloss:-686884585472.0000 dlossA:4748.6191 dlossQ:1349623040.0000 exploreP:0.0102\n",
      "Episode:281 meanR:-0.0200 R:1.0000 rate:0.0769 gloss:-651211243520.0000 dlossA:6869.0068 dlossQ:3748484096.0000 exploreP:0.0102\n",
      "Episode:282 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-380210479104.0000 dlossA:4459.1484 dlossQ:970922624.0000 exploreP:0.0102\n",
      "Episode:283 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-812052119552.0000 dlossA:6944.8877 dlossQ:2324838912.0000 exploreP:0.0102\n",
      "Episode:284 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-929000718336.0000 dlossA:5483.3267 dlossQ:2114404992.0000 exploreP:0.0102\n",
      "Episode:285 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-682817421312.0000 dlossA:4077.9600 dlossQ:1042438144.0000 exploreP:0.0102\n",
      "Episode:286 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-517181505536.0000 dlossA:3191.7488 dlossQ:600964544.0000 exploreP:0.0102\n",
      "Episode:287 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-755537870848.0000 dlossA:5063.5449 dlossQ:1567713152.0000 exploreP:0.0102\n",
      "Episode:288 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-602585169920.0000 dlossA:5123.0811 dlossQ:1213425408.0000 exploreP:0.0102\n",
      "Episode:289 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-814787330048.0000 dlossA:9051.6035 dlossQ:4540410368.0000 exploreP:0.0102\n",
      "Episode:290 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-1036428181504.0000 dlossA:21749.1172 dlossQ:33568577536.0000 exploreP:0.0102\n",
      "Episode:291 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-615967424512.0000 dlossA:2405.6912 dlossQ:485057152.0000 exploreP:0.0102\n",
      "Episode:292 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-771442147328.0000 dlossA:2428.3796 dlossQ:519565472.0000 exploreP:0.0102\n",
      "Episode:293 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-781897957376.0000 dlossA:6192.2314 dlossQ:4629964288.0000 exploreP:0.0101\n",
      "Episode:294 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1061448515584.0000 dlossA:3708.5000 dlossQ:1714054272.0000 exploreP:0.0101\n",
      "Episode:295 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-1263694249984.0000 dlossA:3964.8542 dlossQ:2342458112.0000 exploreP:0.0101\n",
      "Episode:296 meanR:0.0200 R:1.0000 rate:0.0769 gloss:-1097102196736.0000 dlossA:5153.7812 dlossQ:3572893952.0000 exploreP:0.0101\n",
      "Episode:297 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-2329782255616.0000 dlossA:9666.1123 dlossQ:36122075136.0000 exploreP:0.0101\n",
      "Episode:298 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-931533225984.0000 dlossA:3131.8992 dlossQ:1222464896.0000 exploreP:0.0101\n",
      "Episode:299 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1118306172928.0000 dlossA:5976.3340 dlossQ:3788251136.0000 exploreP:0.0101\n",
      "Episode:300 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1094623559680.0000 dlossA:6559.9971 dlossQ:5776705536.0000 exploreP:0.0101\n",
      "Episode:301 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-749734002688.0000 dlossA:3912.1045 dlossQ:1832433536.0000 exploreP:0.0101\n",
      "Episode:302 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-1356176556032.0000 dlossA:8942.7969 dlossQ:8733500416.0000 exploreP:0.0101\n",
      "Episode:303 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-1081499516928.0000 dlossA:4424.7607 dlossQ:2122931200.0000 exploreP:0.0101\n",
      "Episode:304 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-880282959872.0000 dlossA:3478.5159 dlossQ:1144494720.0000 exploreP:0.0101\n",
      "Episode:305 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-829117366272.0000 dlossA:4540.7373 dlossQ:2280591616.0000 exploreP:0.0101\n",
      "Episode:306 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1091888414720.0000 dlossA:5598.7534 dlossQ:3073855232.0000 exploreP:0.0101\n",
      "Episode:307 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-1002912088064.0000 dlossA:12513.7266 dlossQ:10558078976.0000 exploreP:0.0101\n",
      "Episode:308 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-696735956992.0000 dlossA:3673.9575 dlossQ:1152537216.0000 exploreP:0.0101\n",
      "Episode:309 meanR:0.0200 R:0.0000 rate:0.0000 gloss:-686737784832.0000 dlossA:3956.5034 dlossQ:1251602432.0000 exploreP:0.0101\n",
      "Episode:310 meanR:0.0100 R:-1.0000 rate:-0.0769 gloss:-982615785472.0000 dlossA:5796.2183 dlossQ:2312292608.0000 exploreP:0.0101\n",
      "Episode:311 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-946583502848.0000 dlossA:5615.7539 dlossQ:1855428736.0000 exploreP:0.0101\n",
      "Episode:312 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-478318755840.0000 dlossA:4679.4395 dlossQ:1178738688.0000 exploreP:0.0101\n",
      "Episode:313 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-558596292608.0000 dlossA:17536.4766 dlossQ:15282223104.0000 exploreP:0.0101\n",
      "Episode:314 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-582316392448.0000 dlossA:3699.4182 dlossQ:931749888.0000 exploreP:0.0101\n",
      "Episode:315 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-661180448768.0000 dlossA:6685.9590 dlossQ:2630268672.0000 exploreP:0.0101\n",
      "Episode:316 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-757131575296.0000 dlossA:3983.2534 dlossQ:1112983936.0000 exploreP:0.0101\n",
      "Episode:317 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-444781821952.0000 dlossA:2947.6833 dlossQ:362690048.0000 exploreP:0.0101\n",
      "Episode:318 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-617865412608.0000 dlossA:5119.7451 dlossQ:1398955008.0000 exploreP:0.0101\n",
      "Episode:319 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-593371070464.0000 dlossA:9604.0928 dlossQ:6066886656.0000 exploreP:0.0101\n",
      "Episode:320 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-442512146432.0000 dlossA:10336.8818 dlossQ:7950227968.0000 exploreP:0.0101\n",
      "Episode:321 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-689327308800.0000 dlossA:6698.3999 dlossQ:2877316096.0000 exploreP:0.0101\n",
      "Episode:322 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-625457364992.0000 dlossA:3941.1013 dlossQ:989479360.0000 exploreP:0.0101\n",
      "Episode:323 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-472659918848.0000 dlossA:4065.3254 dlossQ:763505408.0000 exploreP:0.0101\n",
      "Episode:324 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-645215748096.0000 dlossA:7265.4224 dlossQ:2859594240.0000 exploreP:0.0101\n",
      "Episode:325 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-535425777664.0000 dlossA:5376.2427 dlossQ:1387033600.0000 exploreP:0.0101\n",
      "Episode:326 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-641185349632.0000 dlossA:5797.6323 dlossQ:1388432640.0000 exploreP:0.0101\n",
      "Episode:327 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-566219505664.0000 dlossA:9998.8438 dlossQ:4801250304.0000 exploreP:0.0101\n",
      "Episode:328 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-467122946048.0000 dlossA:7921.7793 dlossQ:3271938048.0000 exploreP:0.0101\n",
      "Episode:329 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-379646443520.0000 dlossA:5678.3159 dlossQ:1325531392.0000 exploreP:0.0100\n",
      "Episode:330 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-384036274176.0000 dlossA:5572.8296 dlossQ:2196234240.0000 exploreP:0.0100\n",
      "Episode:331 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-496877273088.0000 dlossA:5582.2793 dlossQ:1275538176.0000 exploreP:0.0100\n",
      "Episode:332 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-424478146560.0000 dlossA:5303.6841 dlossQ:1530493824.0000 exploreP:0.0100\n",
      "Episode:333 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-468422230016.0000 dlossA:5010.9326 dlossQ:1102909696.0000 exploreP:0.0100\n",
      "Episode:334 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-559625863168.0000 dlossA:15974.2529 dlossQ:11527421952.0000 exploreP:0.0100\n",
      "Episode:335 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-736160972800.0000 dlossA:8118.8867 dlossQ:5524254720.0000 exploreP:0.0100\n",
      "Episode:336 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-477645438976.0000 dlossA:4779.6899 dlossQ:1297498752.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:337 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-856040210432.0000 dlossA:19492.7793 dlossQ:27765784576.0000 exploreP:0.0100\n",
      "Episode:338 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-841080897536.0000 dlossA:5626.7002 dlossQ:2562486784.0000 exploreP:0.0100\n",
      "Episode:339 meanR:0.0100 R:1.0000 rate:0.0769 gloss:-558155366400.0000 dlossA:5044.0425 dlossQ:1977612544.0000 exploreP:0.0100\n",
      "Episode:340 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-774278479872.0000 dlossA:21852.6426 dlossQ:30601424896.0000 exploreP:0.0100\n",
      "Episode:341 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-846885355520.0000 dlossA:7132.0474 dlossQ:4061129984.0000 exploreP:0.0100\n",
      "Episode:342 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-690090278912.0000 dlossA:6494.4194 dlossQ:3266386176.0000 exploreP:0.0100\n",
      "Episode:343 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-567320051712.0000 dlossA:4260.9321 dlossQ:1291562368.0000 exploreP:0.0100\n",
      "Episode:344 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-909159956480.0000 dlossA:18112.4160 dlossQ:26843568128.0000 exploreP:0.0100\n",
      "Episode:345 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-598161555456.0000 dlossA:5309.3281 dlossQ:2664342528.0000 exploreP:0.0100\n",
      "Episode:346 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-920024186880.0000 dlossA:7876.6084 dlossQ:4285745408.0000 exploreP:0.0100\n",
      "Episode:347 meanR:0.0200 R:1.0000 rate:0.0769 gloss:-487610318848.0000 dlossA:5207.5581 dlossQ:1936109952.0000 exploreP:0.0100\n",
      "Episode:348 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-575625428992.0000 dlossA:5652.1333 dlossQ:2160238336.0000 exploreP:0.0100\n",
      "Episode:349 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-417728921600.0000 dlossA:4128.8267 dlossQ:866720192.0000 exploreP:0.0100\n",
      "Episode:350 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-416988823552.0000 dlossA:5072.2769 dlossQ:1312040832.0000 exploreP:0.0100\n",
      "Episode:351 meanR:0.0000 R:-1.0000 rate:-0.0769 gloss:-441279479808.0000 dlossA:8956.9971 dlossQ:4139114496.0000 exploreP:0.0100\n",
      "Episode:352 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-501578137600.0000 dlossA:6038.7964 dlossQ:1704425984.0000 exploreP:0.0100\n",
      "Episode:353 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-567362584576.0000 dlossA:15613.0869 dlossQ:11372179456.0000 exploreP:0.0100\n",
      "Episode:354 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-923760132096.0000 dlossA:22591.1191 dlossQ:42766241792.0000 exploreP:0.0100\n",
      "Episode:355 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-509315121152.0000 dlossA:6371.8081 dlossQ:3226235136.0000 exploreP:0.0100\n",
      "Episode:356 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-5769467527168.0000 dlossA:24661.1426 dlossQ:387949953024.0000 exploreP:0.0100\n",
      "Episode:357 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-612126425088.0000 dlossA:4318.2285 dlossQ:1530910720.0000 exploreP:0.0100\n",
      "Episode:358 meanR:0.0000 R:0.0000 rate:0.0000 gloss:-788689649664.0000 dlossA:5308.3857 dlossQ:4452849664.0000 exploreP:0.0100\n",
      "Episode:359 meanR:0.0100 R:0.0000 rate:0.0000 gloss:-819377733632.0000 dlossA:5525.6821 dlossQ:4295419392.0000 exploreP:0.0100\n",
      "Episode:360 meanR:-0.0100 R:-2.0000 rate:-0.1538 gloss:-714319134720.0000 dlossA:5012.6582 dlossQ:2332011008.0000 exploreP:0.0100\n",
      "Episode:361 meanR:-0.0100 R:0.0000 rate:0.0000 gloss:-734791532544.0000 dlossA:4761.2441 dlossQ:1842598400.0000 exploreP:0.0100\n",
      "Episode:362 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-974723612672.0000 dlossA:7416.7900 dlossQ:8250534912.0000 exploreP:0.0100\n",
      "Episode:363 meanR:-0.0200 R:0.0000 rate:0.0000 gloss:-873068036096.0000 dlossA:6898.3667 dlossQ:6043416064.0000 exploreP:0.0100\n",
      "Episode:364 meanR:-0.0300 R:-1.0000 rate:-0.0769 gloss:-847298756608.0000 dlossA:6930.1816 dlossQ:6057150464.0000 exploreP:0.0100\n",
      "Episode:365 meanR:-0.0300 R:0.0000 rate:0.0000 gloss:-1284619239424.0000 dlossA:8235.0654 dlossQ:9072380928.0000 exploreP:0.0100\n",
      "Episode:366 meanR:-0.0300 R:-1.0000 rate:-0.0769 gloss:-460054986752.0000 dlossA:4662.3813 dlossQ:1220520960.0000 exploreP:0.0100\n",
      "Episode:367 meanR:-0.0400 R:-1.0000 rate:-0.0769 gloss:-536860065792.0000 dlossA:6794.3232 dlossQ:4709290496.0000 exploreP:0.0100\n",
      "Episode:368 meanR:-0.0400 R:0.0000 rate:0.0000 gloss:-513197211648.0000 dlossA:6022.2124 dlossQ:3175342080.0000 exploreP:0.0100\n",
      "Episode:369 meanR:-0.0500 R:-1.0000 rate:-0.0769 gloss:-622608777216.0000 dlossA:12731.1465 dlossQ:16295961600.0000 exploreP:0.0100\n",
      "Episode:370 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-386569469952.0000 dlossA:6368.4453 dlossQ:2555736320.0000 exploreP:0.0100\n",
      "Episode:371 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-583519764480.0000 dlossA:8085.1733 dlossQ:4578458112.0000 exploreP:0.0100\n",
      "Episode:372 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-957955178496.0000 dlossA:18532.8633 dlossQ:26851852288.0000 exploreP:0.0100\n",
      "Episode:373 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-551563165696.0000 dlossA:7209.9185 dlossQ:4594015744.0000 exploreP:0.0100\n",
      "Episode:374 meanR:-0.0500 R:1.0000 rate:0.0769 gloss:-492810469376.0000 dlossA:9732.6895 dlossQ:6526297088.0000 exploreP:0.0100\n",
      "Episode:375 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-650097262592.0000 dlossA:9785.1387 dlossQ:6104573440.0000 exploreP:0.0100\n",
      "Episode:376 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-685667909632.0000 dlossA:10531.9854 dlossQ:7091312640.0000 exploreP:0.0100\n",
      "Episode:377 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-289004093440.0000 dlossA:5762.9375 dlossQ:1651070848.0000 exploreP:0.0100\n",
      "Episode:378 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-390712557568.0000 dlossA:13687.1445 dlossQ:12276581376.0000 exploreP:0.0100\n",
      "Episode:379 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-368842801152.0000 dlossA:5905.7280 dlossQ:1898145664.0000 exploreP:0.0100\n",
      "Episode:380 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-424775057408.0000 dlossA:6507.0737 dlossQ:1933817088.0000 exploreP:0.0100\n",
      "Episode:381 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-225196212224.0000 dlossA:9352.0732 dlossQ:4425935360.0000 exploreP:0.0100\n",
      "Episode:382 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-1003251367936.0000 dlossA:35585.1523 dlossQ:140408619008.0000 exploreP:0.0100\n",
      "Episode:383 meanR:-0.0600 R:1.0000 rate:0.0769 gloss:-1365221572608.0000 dlossA:28574.7637 dlossQ:225225736192.0000 exploreP:0.0100\n",
      "Episode:384 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-324823613440.0000 dlossA:4751.7793 dlossQ:1534106240.0000 exploreP:0.0100\n",
      "Episode:385 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-484794204160.0000 dlossA:7126.7002 dlossQ:5701987840.0000 exploreP:0.0100\n",
      "Episode:386 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-881270718464.0000 dlossA:20215.6816 dlossQ:46961475584.0000 exploreP:0.0100\n",
      "Episode:387 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-300558024704.0000 dlossA:5069.1748 dlossQ:1707404544.0000 exploreP:0.0100\n",
      "Episode:388 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-468493533184.0000 dlossA:7415.4233 dlossQ:6898634752.0000 exploreP:0.0100\n",
      "Episode:389 meanR:-0.0800 R:-1.0000 rate:-0.0769 gloss:-291500228608.0000 dlossA:5050.4590 dlossQ:1586165760.0000 exploreP:0.0100\n",
      "Episode:390 meanR:-0.0700 R:1.0000 rate:0.0769 gloss:-316219850752.0000 dlossA:5854.8315 dlossQ:3180493568.0000 exploreP:0.0100\n",
      "Episode:391 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-356456071168.0000 dlossA:23203.6309 dlossQ:40792842240.0000 exploreP:0.0100\n",
      "Episode:392 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-387980066816.0000 dlossA:6426.1338 dlossQ:6383412736.0000 exploreP:0.0100\n",
      "Episode:393 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-307111034880.0000 dlossA:6897.8452 dlossQ:4790429696.0000 exploreP:0.0100\n",
      "Episode:394 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-415993528320.0000 dlossA:12424.2188 dlossQ:12430203904.0000 exploreP:0.0100\n",
      "Episode:395 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-159977553920.0000 dlossA:25393.9609 dlossQ:57122934784.0000 exploreP:0.0100\n",
      "Episode:396 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-364883247104.0000 dlossA:6366.8447 dlossQ:3862583552.0000 exploreP:0.0100\n",
      "Episode:397 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-626727976960.0000 dlossA:16255.4482 dlossQ:27083102208.0000 exploreP:0.0100\n",
      "Episode:398 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-416183517184.0000 dlossA:9285.4521 dlossQ:7874281472.0000 exploreP:0.0100\n",
      "Episode:399 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-483009560576.0000 dlossA:17552.4609 dlossQ:57403146240.0000 exploreP:0.0100\n",
      "Episode:400 meanR:-0.0700 R:0.0000 rate:0.0000 gloss:-196740349952.0000 dlossA:11658.3438 dlossQ:12585273344.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:401 meanR:-0.0600 R:0.0000 rate:0.0000 gloss:-205164150784.0000 dlossA:7342.5210 dlossQ:3420567296.0000 exploreP:0.0100\n",
      "Episode:402 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-378401816576.0000 dlossA:11637.4082 dlossQ:14883336192.0000 exploreP:0.0100\n",
      "Episode:403 meanR:-0.0500 R:0.0000 rate:0.0000 gloss:-219922120704.0000 dlossA:6003.5659 dlossQ:2820881664.0000 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/13\n",
    "                rate_clipped = np.clip(a=rate, a_max=1, a_min=0)\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1:\n",
    "                        memory.buffer[-1-idx][-1] = rate_clipped\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps, arr = np.array(loss_list).T\n",
    "# smoothed_arr = running_mean(arr, 10)\n",
    "# plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "# plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF session for training\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "#     # Testing episodes/epochs\n",
    "#     for _ in range(1):\n",
    "#         total_reward = 0\n",
    "#         #state = env.reset()\n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "#         # Testing steps/batches\n",
    "#         while True:\n",
    "#             action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "#             action = np.argmax(action_logits)\n",
    "#             #state, reward, done, _ = env.step(action)\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             total_reward += reward\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#         print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
