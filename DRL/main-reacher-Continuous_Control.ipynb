{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/aras/unity-envs/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                           labels=actions)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size, \n",
    "                                                                           action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "        self.rates = deque(maxlen=max_size) # rates\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), # ==  self.rates\n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx], [self.rates[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state size:{}'.format(states.shape), \n",
    "#       'actions:{}'.format(actions.shape)) \n",
    "# print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 33), (1, 4), 4, 0, 4, 33)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "env_info.vector_observations.shape, env_info.previous_vector_actions.shape, \\\n",
    "brain.vector_action_space_size, brain.number_visual_observations, \\\n",
    "brain.vector_action_space_size, brain.vector_observation_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e4)             # experience mini-batch size == one episode size is 1000/int(1e3) steps\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.029999999329447746\n",
      "reward >>>>>> 0 0.019999999552965164\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.009999999776482582\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.03999999910593033\n",
      "reward >>>>>> 0 0.019999999552965164\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    if reward > 0: print('reward >>>>>> 0', reward)\n",
    "    if reward < 0: print('reward <<<<< 0', reward)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    memory.rates.append(-1) # empty\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        rate = total_reward/30\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.rates[-1-idx] == -1:\n",
    "                memory.rates[-1-idx] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(10000, 33) (10000, 4) (10000, 33) (10000,) (10000,) (10000,)\n",
      "(1001, 33) (1001, 4) (1001, 33) (1001,) (1001,) (1001,)\n"
     ]
    }
   ],
   "source": [
    "batch = memory.buffer\n",
    "percentage = 0.9\n",
    "# statesL, actionsL, next_statesL, rewardsL, donesL, ratesL = [], [], [], [], [], []\n",
    "# for idx in range(memory_size// batch_size):\n",
    "idx_arr = np.arange(memory_size// batch_size)\n",
    "idx = np.random.choice(idx_arr)\n",
    "states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "print(actions.dtype)\n",
    "next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "states = states[rates >= (np.max(rates)*percentage)]\n",
    "actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "# statesL.append(states)\n",
    "# actionsL.append(actions)\n",
    "# next_statesL.append(next_states)\n",
    "# rewardsL.append(rewards)\n",
    "# donesL.append(dones)\n",
    "# ratesL.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_arr = np.arange(memory_size// batch_size)\n",
    "# idx = np.random.choice(idx_arr)\n",
    "# idx*batch_size, (idx+1)*batch_size, memory_size// batch_size, memory_size, (10+1)*batch_size, \\\n",
    "# idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.000000 rate:0.0000 gloss:-360.5320 dlossA:0.1029 dlossQ:0.7023 exploreP:0.9057\n",
      "Episode:1 meanR:0.2000 R:0.400000 rate:0.0133 gloss:-6540.2520 dlossA:0.1410 dlossQ:3.1096 exploreP:0.8204\n",
      "Episode:2 meanR:0.2500 R:0.350000 rate:0.0117 gloss:-35057.0781 dlossA:0.2427 dlossQ:17.4989 exploreP:0.7432\n",
      "Episode:3 meanR:0.3625 R:0.700000 rate:0.0233 gloss:-155546.0156 dlossA:0.4973 dlossQ:62.3151 exploreP:0.6734\n",
      "Episode:4 meanR:0.4160 R:0.630000 rate:0.0210 gloss:94925016.0000 dlossA:1.6635 dlossQ:449.9792 exploreP:0.6102\n",
      "Episode:5 meanR:0.3683 R:0.130000 rate:0.0043 gloss:102444536.0000 dlossA:0.8881 dlossQ:390.2564 exploreP:0.5530\n",
      "Episode:6 meanR:0.3157 R:0.000000 rate:0.0000 gloss:236902992.0000 dlossA:1.4376 dlossQ:1050.8639 exploreP:0.5013\n",
      "Episode:7 meanR:0.2762 R:0.000000 rate:0.0000 gloss:390146464.0000 dlossA:1.8746 dlossQ:2088.5886 exploreP:0.4545\n",
      "Episode:8 meanR:0.2456 R:0.000000 rate:0.0000 gloss:380554112.0000 dlossA:1.8996 dlossQ:1815.8145 exploreP:0.4121\n",
      "Episode:9 meanR:0.2210 R:0.000000 rate:0.0000 gloss:379277792.0000 dlossA:2.5047 dlossQ:2611.6484 exploreP:0.3738\n",
      "Episode:10 meanR:0.2164 R:0.170000 rate:0.0057 gloss:290949472.0000 dlossA:2.7276 dlossQ:2568.6855 exploreP:0.3392\n",
      "Episode:11 meanR:0.1983 R:0.000000 rate:0.0000 gloss:195931888.0000 dlossA:2.6716 dlossQ:1581.8508 exploreP:0.3078\n",
      "Episode:12 meanR:0.2115 R:0.370000 rate:0.0123 gloss:142527520.0000 dlossA:2.6711 dlossQ:1653.0975 exploreP:0.2795\n",
      "Episode:13 meanR:0.2093 R:0.180000 rate:0.0060 gloss:30652102.0000 dlossA:2.6918 dlossQ:1428.5876 exploreP:0.2538\n",
      "Episode:14 meanR:0.2867 R:1.370000 rate:0.0457 gloss:-372129632.0000 dlossA:2.5576 dlossQ:1925.3029 exploreP:0.2306\n",
      "Episode:15 meanR:0.3087 R:0.640000 rate:0.0213 gloss:-19578744.0000 dlossA:2.3716 dlossQ:540.9808 exploreP:0.2096\n",
      "Episode:16 meanR:0.3500 R:1.010000 rate:0.0337 gloss:-134836992.0000 dlossA:1.9422 dlossQ:594.3536 exploreP:0.1905\n",
      "Episode:17 meanR:0.3556 R:0.450000 rate:0.0150 gloss:-169071632.0000 dlossA:1.6824 dlossQ:1033.0214 exploreP:0.1734\n",
      "Episode:18 meanR:0.3642 R:0.520000 rate:0.0173 gloss:-254006976.0000 dlossA:1.4475 dlossQ:1143.4071 exploreP:0.1578\n",
      "Episode:19 meanR:0.4260 R:1.600000 rate:0.0533 gloss:-227963712.0000 dlossA:1.1422 dlossQ:632.2134 exploreP:0.1437\n",
      "Episode:20 meanR:0.4371 R:0.660000 rate:0.0220 gloss:-321968672.0000 dlossA:0.8471 dlossQ:550.5381 exploreP:0.1310\n",
      "Episode:21 meanR:0.4668 R:1.090000 rate:0.0363 gloss:-308452736.0000 dlossA:0.7801 dlossQ:617.6031 exploreP:0.1195\n",
      "Episode:22 meanR:0.4522 R:0.130000 rate:0.0043 gloss:-315535584.0000 dlossA:0.6668 dlossQ:775.1761 exploreP:0.1090\n",
      "Episode:23 meanR:0.4375 R:0.100000 rate:0.0033 gloss:-711511872.0000 dlossA:0.7625 dlossQ:1405.6184 exploreP:0.0996\n",
      "Episode:24 meanR:0.4200 R:0.000000 rate:0.0000 gloss:24800716.0000 dlossA:0.4903 dlossQ:226.2363 exploreP:0.0911\n",
      "Episode:25 meanR:0.4146 R:0.280000 rate:0.0093 gloss:-390745920.0000 dlossA:0.8352 dlossQ:1958.3534 exploreP:0.0833\n",
      "Episode:26 meanR:0.3993 R:0.000000 rate:0.0000 gloss:-362463392.0000 dlossA:0.8027 dlossQ:1472.1617 exploreP:0.0764\n",
      "Episode:27 meanR:0.4154 R:0.850000 rate:0.0283 gloss:-385295840.0000 dlossA:0.5912 dlossQ:632.4432 exploreP:0.0700\n",
      "Episode:28 meanR:0.4055 R:0.130000 rate:0.0043 gloss:-535163616.0000 dlossA:0.6914 dlossQ:1588.7284 exploreP:0.0643\n",
      "Episode:29 meanR:0.3920 R:0.000000 rate:0.0000 gloss:-472368288.0000 dlossA:0.8114 dlossQ:1641.4575 exploreP:0.0591\n",
      "Episode:30 meanR:0.3832 R:0.120000 rate:0.0040 gloss:-273221888.0000 dlossA:0.4839 dlossQ:421.2758 exploreP:0.0545\n",
      "Episode:31 meanR:0.4112 R:1.280000 rate:0.0427 gloss:-720493760.0000 dlossA:0.6900 dlossQ:1116.7499 exploreP:0.0502\n",
      "Episode:32 meanR:0.4045 R:0.190000 rate:0.0063 gloss:-248115808.0000 dlossA:0.6649 dlossQ:549.6838 exploreP:0.0464\n",
      "Episode:33 meanR:0.4071 R:0.490000 rate:0.0163 gloss:-684175744.0000 dlossA:0.7432 dlossQ:824.4123 exploreP:0.0429\n",
      "Episode:34 meanR:0.4011 R:0.200000 rate:0.0067 gloss:290261248.0000 dlossA:0.5700 dlossQ:268.9364 exploreP:0.0398\n",
      "Episode:35 meanR:0.3956 R:0.200000 rate:0.0067 gloss:-155249584.0000 dlossA:0.8842 dlossQ:662.1227 exploreP:0.0370\n",
      "Episode:36 meanR:0.3849 R:0.000000 rate:0.0000 gloss:-250875984.0000 dlossA:1.0047 dlossQ:764.3635 exploreP:0.0344\n",
      "Episode:37 meanR:0.3758 R:0.040000 rate:0.0013 gloss:-383062112.0000 dlossA:1.0443 dlossQ:699.0949 exploreP:0.0321\n",
      "Episode:38 meanR:0.3715 R:0.210000 rate:0.0070 gloss:-497961120.0000 dlossA:1.0957 dlossQ:1052.0471 exploreP:0.0300\n",
      "Episode:39 meanR:0.3697 R:0.300000 rate:0.0100 gloss:-85285168.0000 dlossA:1.0670 dlossQ:779.0255 exploreP:0.0281\n",
      "Episode:40 meanR:0.3607 R:0.000000 rate:0.0000 gloss:-460645600.0000 dlossA:0.9561 dlossQ:632.8428 exploreP:0.0263\n",
      "Episode:41 meanR:0.3626 R:0.440000 rate:0.0147 gloss:-692702976.0000 dlossA:1.0356 dlossQ:979.4040 exploreP:0.0248\n",
      "Episode:42 meanR:0.3542 R:0.000000 rate:0.0000 gloss:-65167156.0000 dlossA:0.8347 dlossQ:788.9877 exploreP:0.0234\n",
      "Episode:43 meanR:0.4009 R:2.410000 rate:0.0803 gloss:-1073804800.0000 dlossA:0.9429 dlossQ:1221.6931 exploreP:0.0221\n",
      "Episode:44 meanR:0.4242 R:1.450000 rate:0.0483 gloss:584979840.0000 dlossA:1.0583 dlossQ:432.2315 exploreP:0.0209\n",
      "Episode:45 meanR:0.4350 R:0.920000 rate:0.0307 gloss:-186105952.0000 dlossA:0.9672 dlossQ:510.1729 exploreP:0.0199\n",
      "Episode:46 meanR:0.4257 R:0.000000 rate:0.0000 gloss:-210743952.0000 dlossA:1.1388 dlossQ:866.2687 exploreP:0.0190\n",
      "Episode:47 meanR:0.4294 R:0.600000 rate:0.0200 gloss:-244755904.0000 dlossA:0.9966 dlossQ:636.2563 exploreP:0.0181\n",
      "Episode:48 meanR:0.4276 R:0.340000 rate:0.0113 gloss:-356439392.0000 dlossA:1.0224 dlossQ:689.1587 exploreP:0.0173\n",
      "Episode:49 meanR:0.4224 R:0.170000 rate:0.0057 gloss:-333357952.0000 dlossA:0.8091 dlossQ:344.9268 exploreP:0.0166\n",
      "Episode:50 meanR:0.4175 R:0.170000 rate:0.0057 gloss:-436093760.0000 dlossA:0.7664 dlossQ:411.2985 exploreP:0.0160\n",
      "Episode:51 meanR:0.4140 R:0.240000 rate:0.0080 gloss:-306940192.0000 dlossA:0.8475 dlossQ:460.6589 exploreP:0.0154\n",
      "Episode:52 meanR:0.4062 R:0.000000 rate:0.0000 gloss:-171564560.0000 dlossA:0.8844 dlossQ:497.4433 exploreP:0.0149\n",
      "Episode:53 meanR:0.4013 R:0.140000 rate:0.0047 gloss:-371848896.0000 dlossA:0.9692 dlossQ:464.4819 exploreP:0.0144\n",
      "Episode:54 meanR:0.4007 R:0.370000 rate:0.0123 gloss:43341524.0000 dlossA:0.6158 dlossQ:122.5305 exploreP:0.0140\n",
      "Episode:55 meanR:0.3979 R:0.240000 rate:0.0080 gloss:-78903824.0000 dlossA:0.8060 dlossQ:350.4028 exploreP:0.0136\n",
      "Episode:56 meanR:0.3949 R:0.230000 rate:0.0077 gloss:-314815552.0000 dlossA:1.0498 dlossQ:708.9727 exploreP:0.0133\n",
      "Episode:57 meanR:0.3881 R:0.000000 rate:0.0000 gloss:-279788896.0000 dlossA:0.8031 dlossQ:403.2124 exploreP:0.0130\n",
      "Episode:58 meanR:0.3973 R:0.930000 rate:0.0310 gloss:-154184880.0000 dlossA:0.7646 dlossQ:309.6524 exploreP:0.0127\n",
      "Episode:59 meanR:0.3977 R:0.420000 rate:0.0140 gloss:4543344.0000 dlossA:0.8464 dlossQ:275.0150 exploreP:0.0124\n",
      "Episode:60 meanR:0.3949 R:0.230000 rate:0.0077 gloss:-293507488.0000 dlossA:0.7297 dlossQ:340.9199 exploreP:0.0122\n",
      "Episode:61 meanR:0.3889 R:0.020000 rate:0.0007 gloss:-569264128.0000 dlossA:1.1711 dlossQ:992.0219 exploreP:0.0120\n",
      "Episode:62 meanR:0.3827 R:0.000000 rate:0.0000 gloss:-362640288.0000 dlossA:0.8560 dlossQ:469.1234 exploreP:0.0118\n",
      "Episode:63 meanR:0.3820 R:0.340000 rate:0.0113 gloss:-968519808.0000 dlossA:1.6265 dlossQ:1080.2024 exploreP:0.0116\n",
      "Episode:64 meanR:0.3762 R:0.000000 rate:0.0000 gloss:-592870016.0000 dlossA:1.3749 dlossQ:807.1769 exploreP:0.0115\n",
      "Episode:65 meanR:0.3729 R:0.160000 rate:0.0053 gloss:-1744849280.0000 dlossA:2.1883 dlossQ:1387.5975 exploreP:0.0113\n",
      "Episode:66 meanR:0.3687 R:0.090000 rate:0.0030 gloss:-2331364608.0000 dlossA:2.5853 dlossQ:1501.1736 exploreP:0.0112\n",
      "Episode:67 meanR:0.3632 R:0.000000 rate:0.0000 gloss:-3733178112.0000 dlossA:3.2166 dlossQ:2012.9843 exploreP:0.0111\n",
      "Episode:68 meanR:0.3726 R:1.010000 rate:0.0337 gloss:-10989650944.0000 dlossA:5.1109 dlossQ:5980.2603 exploreP:0.0110\n",
      "Episode:69 meanR:0.3683 R:0.070000 rate:0.0023 gloss:-1561869696.0000 dlossA:5.2008 dlossQ:6333.4492 exploreP:0.0109\n",
      "Episode:70 meanR:0.3631 R:0.000000 rate:0.0000 gloss:1337608960.0000 dlossA:4.2087 dlossQ:3300.5474 exploreP:0.0108\n",
      "Episode:71 meanR:0.3618 R:0.270000 rate:0.0090 gloss:1338196224.0000 dlossA:6.4218 dlossQ:6419.4717 exploreP:0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:72 meanR:0.3627 R:0.430000 rate:0.0143 gloss:-2271539200.0000 dlossA:5.0604 dlossQ:3875.8528 exploreP:0.0107\n",
      "Episode:73 meanR:0.3578 R:0.000000 rate:0.0000 gloss:-3902618624.0000 dlossA:7.0309 dlossQ:6254.6636 exploreP:0.0106\n",
      "Episode:74 meanR:0.3545 R:0.110000 rate:0.0037 gloss:-1746834432.0000 dlossA:4.6028 dlossQ:3608.8792 exploreP:0.0105\n",
      "Episode:75 meanR:0.3563 R:0.490000 rate:0.0163 gloss:-2199609088.0000 dlossA:6.3691 dlossQ:6253.6445 exploreP:0.0105\n",
      "Episode:76 meanR:0.3539 R:0.170000 rate:0.0057 gloss:-779677568.0000 dlossA:5.4355 dlossQ:4214.3857 exploreP:0.0104\n",
      "Episode:77 meanR:0.3494 R:0.000000 rate:0.0000 gloss:-5248725504.0000 dlossA:6.0075 dlossQ:5667.9585 exploreP:0.0104\n",
      "Episode:78 meanR:0.3477 R:0.220000 rate:0.0073 gloss:-2173766912.0000 dlossA:7.8854 dlossQ:7706.0083 exploreP:0.0104\n",
      "Episode:79 meanR:0.3439 R:0.040000 rate:0.0013 gloss:5401230848.0000 dlossA:6.3218 dlossQ:7259.2036 exploreP:0.0103\n",
      "Episode:80 meanR:0.3415 R:0.150000 rate:0.0050 gloss:3746609152.0000 dlossA:4.5769 dlossQ:4403.4854 exploreP:0.0103\n",
      "Episode:81 meanR:0.3398 R:0.200000 rate:0.0067 gloss:-4417543680.0000 dlossA:6.8235 dlossQ:11946.7637 exploreP:0.0103\n",
      "Episode:82 meanR:0.3439 R:0.680000 rate:0.0227 gloss:-4443067904.0000 dlossA:4.4616 dlossQ:6063.1147 exploreP:0.0102\n",
      "Episode:83 meanR:0.3414 R:0.140000 rate:0.0047 gloss:-7674456064.0000 dlossA:10.4561 dlossQ:16231.3447 exploreP:0.0102\n",
      "Episode:84 meanR:0.3441 R:0.570000 rate:0.0190 gloss:-5217835008.0000 dlossA:7.1648 dlossQ:8518.5410 exploreP:0.0102\n",
      "Episode:85 meanR:0.3401 R:0.000000 rate:0.0000 gloss:-6767922176.0000 dlossA:6.9119 dlossQ:8223.5811 exploreP:0.0102\n",
      "Episode:86 meanR:0.3394 R:0.280000 rate:0.0093 gloss:-12375421952.0000 dlossA:6.8700 dlossQ:9582.8818 exploreP:0.0102\n",
      "Episode:87 meanR:0.3395 R:0.350000 rate:0.0117 gloss:-8333069312.0000 dlossA:5.8465 dlossQ:6618.3940 exploreP:0.0101\n",
      "Episode:88 meanR:0.3402 R:0.400000 rate:0.0133 gloss:-8295005184.0000 dlossA:5.7302 dlossQ:5855.8838 exploreP:0.0101\n",
      "Episode:89 meanR:0.3364 R:0.000000 rate:0.0000 gloss:-5367272960.0000 dlossA:5.6332 dlossQ:6629.5093 exploreP:0.0101\n",
      "Episode:90 meanR:0.3327 R:0.000000 rate:0.0000 gloss:-5104472576.0000 dlossA:4.1509 dlossQ:3779.0410 exploreP:0.0101\n",
      "Episode:91 meanR:0.3327 R:0.330000 rate:0.0110 gloss:-15522574336.0000 dlossA:5.3989 dlossQ:7730.8203 exploreP:0.0101\n",
      "Episode:92 meanR:0.3291 R:0.000000 rate:0.0000 gloss:-18672195584.0000 dlossA:4.9100 dlossQ:8391.4717 exploreP:0.0101\n",
      "Episode:93 meanR:0.3294 R:0.350000 rate:0.0117 gloss:-22658347008.0000 dlossA:5.4464 dlossQ:7679.8652 exploreP:0.0101\n",
      "Episode:94 meanR:0.3313 R:0.510000 rate:0.0170 gloss:-12284890112.0000 dlossA:5.2551 dlossQ:6459.8994 exploreP:0.0101\n",
      "Episode:95 meanR:0.3353 R:0.720000 rate:0.0240 gloss:-10064041984.0000 dlossA:5.1320 dlossQ:6628.7314 exploreP:0.0101\n",
      "Episode:96 meanR:0.3355 R:0.350000 rate:0.0117 gloss:-19829848064.0000 dlossA:4.9263 dlossQ:6725.2539 exploreP:0.0101\n",
      "Episode:97 meanR:0.3490 R:1.660000 rate:0.0553 gloss:-12220563456.0000 dlossA:5.3884 dlossQ:9374.0684 exploreP:0.0101\n",
      "Episode:98 meanR:0.3510 R:0.550000 rate:0.0183 gloss:-16421177344.0000 dlossA:5.0825 dlossQ:6166.8711 exploreP:0.0100\n",
      "Episode:99 meanR:0.3498 R:0.230000 rate:0.0077 gloss:-33651533824.0000 dlossA:5.0355 dlossQ:7396.8726 exploreP:0.0100\n",
      "Episode:100 meanR:0.3498 R:0.000000 rate:0.0000 gloss:-26919680000.0000 dlossA:5.7249 dlossQ:9480.8574 exploreP:0.0100\n",
      "Episode:101 meanR:0.3478 R:0.200000 rate:0.0067 gloss:-33171144704.0000 dlossA:5.4526 dlossQ:7587.0698 exploreP:0.0100\n",
      "Episode:102 meanR:0.3501 R:0.580000 rate:0.0193 gloss:-39253667840.0000 dlossA:6.0276 dlossQ:10347.7100 exploreP:0.0100\n",
      "Episode:103 meanR:0.3466 R:0.350000 rate:0.0117 gloss:-31433240576.0000 dlossA:7.0411 dlossQ:9990.5791 exploreP:0.0100\n",
      "Episode:104 meanR:0.3496 R:0.930000 rate:0.0310 gloss:-19268956160.0000 dlossA:6.0224 dlossQ:7709.0151 exploreP:0.0100\n",
      "Episode:105 meanR:0.3483 R:0.000000 rate:0.0000 gloss:-2267554560.0000 dlossA:5.6729 dlossQ:8056.7124 exploreP:0.0100\n",
      "Episode:106 meanR:0.3483 R:0.000000 rate:0.0000 gloss:-46337368064.0000 dlossA:6.8143 dlossQ:23885.5859 exploreP:0.0100\n",
      "Episode:107 meanR:0.3524 R:0.410000 rate:0.0137 gloss:-54575865856.0000 dlossA:6.3326 dlossQ:16576.7227 exploreP:0.0100\n",
      "Episode:108 meanR:0.3581 R:0.570000 rate:0.0190 gloss:-22798342144.0000 dlossA:5.6913 dlossQ:8321.0303 exploreP:0.0100\n",
      "Episode:109 meanR:0.3659 R:0.780000 rate:0.0260 gloss:-32552749056.0000 dlossA:5.4131 dlossQ:7951.3174 exploreP:0.0100\n",
      "Episode:110 meanR:0.3653 R:0.110000 rate:0.0037 gloss:-31496646656.0000 dlossA:4.7913 dlossQ:6745.7002 exploreP:0.0100\n",
      "Episode:111 meanR:0.3743 R:0.900000 rate:0.0300 gloss:-58567827456.0000 dlossA:5.6093 dlossQ:9013.1387 exploreP:0.0100\n",
      "Episode:112 meanR:0.3765 R:0.590000 rate:0.0197 gloss:-51083161600.0000 dlossA:7.0393 dlossQ:15772.7100 exploreP:0.0100\n",
      "Episode:113 meanR:0.3747 R:0.000000 rate:0.0000 gloss:-47250907136.0000 dlossA:6.5123 dlossQ:9779.4746 exploreP:0.0100\n",
      "Episode:114 meanR:0.3610 R:0.000000 rate:0.0000 gloss:-57281134592.0000 dlossA:6.3220 dlossQ:10148.7656 exploreP:0.0100\n",
      "Episode:115 meanR:0.3567 R:0.210000 rate:0.0070 gloss:-35773935616.0000 dlossA:6.6458 dlossQ:11921.5420 exploreP:0.0100\n",
      "Episode:116 meanR:0.3568 R:1.020000 rate:0.0340 gloss:-62304894976.0000 dlossA:8.1552 dlossQ:23899.0254 exploreP:0.0100\n",
      "Episode:117 meanR:0.3563 R:0.400000 rate:0.0133 gloss:-72917770240.0000 dlossA:9.6997 dlossQ:34336.2148 exploreP:0.0100\n",
      "Episode:118 meanR:0.3523 R:0.120000 rate:0.0040 gloss:-73351372800.0000 dlossA:6.3914 dlossQ:12557.0674 exploreP:0.0100\n",
      "Episode:119 meanR:0.3472 R:1.090000 rate:0.0363 gloss:-80149798912.0000 dlossA:6.5613 dlossQ:15181.2090 exploreP:0.0100\n",
      "Episode:120 meanR:0.3406 R:0.000000 rate:0.0000 gloss:-78502821888.0000 dlossA:5.2811 dlossQ:7607.7290 exploreP:0.0100\n",
      "Episode:121 meanR:0.3340 R:0.430000 rate:0.0143 gloss:-67614117888.0000 dlossA:5.8213 dlossQ:9113.2295 exploreP:0.0100\n",
      "Episode:122 meanR:0.3327 R:0.000000 rate:0.0000 gloss:-105454272512.0000 dlossA:7.0056 dlossQ:22385.7070 exploreP:0.0100\n",
      "Episode:123 meanR:0.3322 R:0.050000 rate:0.0017 gloss:-93932331008.0000 dlossA:7.6341 dlossQ:13729.0205 exploreP:0.0100\n",
      "Episode:124 meanR:0.3358 R:0.360000 rate:0.0120 gloss:-108048252928.0000 dlossA:7.9462 dlossQ:16517.7246 exploreP:0.0100\n",
      "Episode:125 meanR:0.3364 R:0.340000 rate:0.0113 gloss:-79832670208.0000 dlossA:6.3091 dlossQ:12293.2070 exploreP:0.0100\n",
      "Episode:126 meanR:0.3401 R:0.370000 rate:0.0123 gloss:-141539229696.0000 dlossA:8.4601 dlossQ:25806.5488 exploreP:0.0100\n",
      "Episode:127 meanR:0.3356 R:0.400000 rate:0.0133 gloss:-139171266560.0000 dlossA:9.3284 dlossQ:29423.7246 exploreP:0.0100\n",
      "Episode:128 meanR:0.3353 R:0.100000 rate:0.0033 gloss:-139854823424.0000 dlossA:7.9404 dlossQ:21416.7188 exploreP:0.0100\n",
      "Episode:129 meanR:0.3372 R:0.190000 rate:0.0063 gloss:-135444996096.0000 dlossA:7.6755 dlossQ:15266.1162 exploreP:0.0100\n",
      "Episode:130 meanR:0.3370 R:0.100000 rate:0.0033 gloss:-105767968768.0000 dlossA:8.3885 dlossQ:16627.2285 exploreP:0.0100\n",
      "Episode:131 meanR:0.3307 R:0.650000 rate:0.0217 gloss:-121823805440.0000 dlossA:7.9412 dlossQ:16107.9775 exploreP:0.0100\n",
      "Episode:132 meanR:0.3315 R:0.270000 rate:0.0090 gloss:-148410925056.0000 dlossA:9.8822 dlossQ:25508.6797 exploreP:0.0100\n",
      "Episode:133 meanR:0.3286 R:0.200000 rate:0.0067 gloss:-165594841088.0000 dlossA:9.7580 dlossQ:20140.5195 exploreP:0.0100\n",
      "Episode:134 meanR:0.3282 R:0.160000 rate:0.0053 gloss:-152069767168.0000 dlossA:10.2448 dlossQ:26066.9141 exploreP:0.0100\n",
      "Episode:135 meanR:0.3329 R:0.670000 rate:0.0223 gloss:-119587692544.0000 dlossA:9.5021 dlossQ:29597.2676 exploreP:0.0100\n",
      "Episode:136 meanR:0.3329 R:0.000000 rate:0.0000 gloss:-247054696448.0000 dlossA:9.7849 dlossQ:31781.8613 exploreP:0.0100\n",
      "Episode:137 meanR:0.3325 R:0.000000 rate:0.0000 gloss:-711661387776.0000 dlossA:15.0835 dlossQ:127881.2109 exploreP:0.0100\n",
      "Episode:138 meanR:0.3388 R:0.840000 rate:0.0280 gloss:-518693781504.0000 dlossA:8.6938 dlossQ:24446.0137 exploreP:0.0100\n",
      "Episode:139 meanR:0.3409 R:0.510000 rate:0.0170 gloss:-290837200896.0000 dlossA:9.0210 dlossQ:29749.6973 exploreP:0.0100\n",
      "Episode:140 meanR:0.3481 R:0.720000 rate:0.0240 gloss:-1802504896512.0000 dlossA:14.0693 dlossQ:240888.3125 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:141 meanR:0.3437 R:0.000000 rate:0.0000 gloss:-1459960020992.0000 dlossA:11.6220 dlossQ:100744.9297 exploreP:0.0100\n",
      "Episode:142 meanR:0.3437 R:0.000000 rate:0.0000 gloss:-1289396551680.0000 dlossA:14.6310 dlossQ:124397.2734 exploreP:0.0100\n",
      "Episode:143 meanR:0.3368 R:1.720000 rate:0.0573 gloss:-1261709426688.0000 dlossA:12.8929 dlossQ:88236.3359 exploreP:0.0100\n",
      "Episode:144 meanR:0.3361 R:1.380000 rate:0.0460 gloss:-1641987178496.0000 dlossA:18.9428 dlossQ:224033.9531 exploreP:0.0100\n",
      "Episode:145 meanR:0.3322 R:0.530000 rate:0.0177 gloss:-907476205568.0000 dlossA:13.2916 dlossQ:81702.0000 exploreP:0.0100\n",
      "Episode:146 meanR:0.3345 R:0.230000 rate:0.0077 gloss:-1521188077568.0000 dlossA:17.1912 dlossQ:168987.5781 exploreP:0.0100\n",
      "Episode:147 meanR:0.3424 R:1.390000 rate:0.0463 gloss:-2623072632832.0000 dlossA:20.1482 dlossQ:314162.3125 exploreP:0.0100\n",
      "Episode:148 meanR:0.3493 R:1.030000 rate:0.0343 gloss:-489144844288.0000 dlossA:17.7789 dlossQ:132596.9531 exploreP:0.0100\n",
      "Episode:149 meanR:0.3577 R:1.010000 rate:0.0337 gloss:-479943229440.0000 dlossA:21.0338 dlossQ:193048.2969 exploreP:0.0100\n",
      "Episode:150 meanR:0.3627 R:0.670000 rate:0.0223 gloss:-2674513936384.0000 dlossA:21.6331 dlossQ:210592.0156 exploreP:0.0100\n",
      "Episode:151 meanR:0.3639 R:0.360000 rate:0.0120 gloss:-3594986455040.0000 dlossA:21.7210 dlossQ:456571.5000 exploreP:0.0100\n",
      "Episode:152 meanR:0.3672 R:0.330000 rate:0.0110 gloss:-1829502976000.0000 dlossA:20.7933 dlossQ:240151.3125 exploreP:0.0100\n",
      "Episode:153 meanR:0.3852 R:1.940000 rate:0.0647 gloss:-3880810708992.0000 dlossA:22.1204 dlossQ:203927.1875 exploreP:0.0100\n",
      "Episode:154 meanR:0.3967 R:1.520000 rate:0.0507 gloss:-3963952300032.0000 dlossA:22.4902 dlossQ:195164.5469 exploreP:0.0100\n",
      "Episode:155 meanR:0.3985 R:0.420000 rate:0.0140 gloss:-3629186285568.0000 dlossA:25.1137 dlossQ:274017.2188 exploreP:0.0100\n",
      "Episode:156 meanR:0.4038 R:0.760000 rate:0.0253 gloss:-3447454695424.0000 dlossA:21.9018 dlossQ:167573.2344 exploreP:0.0100\n",
      "Episode:157 meanR:0.4160 R:1.220000 rate:0.0407 gloss:-5714887049216.0000 dlossA:26.8053 dlossQ:249094.2500 exploreP:0.0100\n",
      "Episode:158 meanR:0.4091 R:0.240000 rate:0.0080 gloss:288643088384.0000 dlossA:27.1401 dlossQ:230918.2500 exploreP:0.0100\n",
      "Episode:159 meanR:0.4084 R:0.350000 rate:0.0117 gloss:-14607608832.0000 dlossA:24.3679 dlossQ:171015.2500 exploreP:0.0100\n",
      "Episode:160 meanR:0.4109 R:0.480000 rate:0.0160 gloss:-4131074605056.0000 dlossA:26.9047 dlossQ:264678.6562 exploreP:0.0100\n",
      "Episode:161 meanR:0.4368 R:2.610000 rate:0.0870 gloss:-11203746201600.0000 dlossA:37.7906 dlossQ:1533459.8750 exploreP:0.0100\n",
      "Episode:162 meanR:0.4420 R:0.520000 rate:0.0173 gloss:-5322764713984.0000 dlossA:24.3995 dlossQ:383582.6875 exploreP:0.0100\n",
      "Episode:163 meanR:0.4528 R:1.420000 rate:0.0473 gloss:-8877993099264.0000 dlossA:21.8163 dlossQ:226792.5625 exploreP:0.0100\n",
      "Episode:164 meanR:0.4641 R:1.130000 rate:0.0377 gloss:-9622652977152.0000 dlossA:24.9600 dlossQ:281429.4062 exploreP:0.0100\n",
      "Episode:165 meanR:0.4669 R:0.440000 rate:0.0147 gloss:-5532769320960.0000 dlossA:23.4742 dlossQ:225244.6719 exploreP:0.0100\n",
      "Episode:166 meanR:0.4660 R:0.000000 rate:0.0000 gloss:-8076948668416.0000 dlossA:26.1581 dlossQ:262149.8438 exploreP:0.0100\n",
      "Episode:167 meanR:0.4720 R:0.600000 rate:0.0200 gloss:-11700598210560.0000 dlossA:37.0837 dlossQ:491322.0312 exploreP:0.0100\n",
      "Episode:168 meanR:0.4629 R:0.100000 rate:0.0033 gloss:3329411252224.0000 dlossA:29.5378 dlossQ:310701.9375 exploreP:0.0100\n",
      "Episode:169 meanR:0.4663 R:0.410000 rate:0.0137 gloss:683609620480.0000 dlossA:22.1568 dlossQ:137447.5312 exploreP:0.0100\n",
      "Episode:170 meanR:0.4688 R:0.250000 rate:0.0083 gloss:-5055569723392.0000 dlossA:32.3118 dlossQ:392731.9375 exploreP:0.0100\n",
      "Episode:171 meanR:0.4685 R:0.240000 rate:0.0080 gloss:-15694292844544.0000 dlossA:43.3979 dlossQ:1904185.6250 exploreP:0.0100\n",
      "Episode:172 meanR:0.4671 R:0.290000 rate:0.0097 gloss:-4876533235712.0000 dlossA:35.8818 dlossQ:987753.0000 exploreP:0.0100\n",
      "Episode:173 meanR:0.4857 R:1.860000 rate:0.0620 gloss:-6329565446144.0000 dlossA:21.5003 dlossQ:271751.2812 exploreP:0.0100\n",
      "Episode:174 meanR:0.4905 R:0.590000 rate:0.0197 gloss:9728297009152.0000 dlossA:45.4707 dlossQ:1152374.3750 exploreP:0.0100\n",
      "Episode:175 meanR:0.4933 R:0.770000 rate:0.0257 gloss:6539072503808.0000 dlossA:31.3956 dlossQ:433381.4062 exploreP:0.0100\n",
      "Episode:176 meanR:0.4952 R:0.360000 rate:0.0120 gloss:-198061998080.0000 dlossA:32.9288 dlossQ:378937.1562 exploreP:0.0100\n",
      "Episode:177 meanR:0.4977 R:0.250000 rate:0.0083 gloss:-13328579559424.0000 dlossA:50.3266 dlossQ:1149890.7500 exploreP:0.0100\n",
      "Episode:178 meanR:0.5048 R:0.930000 rate:0.0310 gloss:15617091436544.0000 dlossA:45.1879 dlossQ:1118146.8750 exploreP:0.0100\n",
      "Episode:179 meanR:0.5057 R:0.130000 rate:0.0043 gloss:-1071618588672.0000 dlossA:43.7270 dlossQ:849782.5625 exploreP:0.0100\n",
      "Episode:180 meanR:0.5196 R:1.540000 rate:0.0513 gloss:-27857583603712.0000 dlossA:75.1600 dlossQ:2835594.0000 exploreP:0.0100\n",
      "Episode:181 meanR:0.5230 R:0.540000 rate:0.0180 gloss:-83952115122176.0000 dlossA:97.3762 dlossQ:9080051.0000 exploreP:0.0100\n",
      "Episode:182 meanR:0.5227 R:0.650000 rate:0.0217 gloss:-29162129588224.0000 dlossA:42.5664 dlossQ:1348923.6250 exploreP:0.0100\n",
      "Episode:183 meanR:0.5300 R:0.870000 rate:0.0290 gloss:-202616877350912.0000 dlossA:64.1673 dlossQ:4384549.0000 exploreP:0.0100\n",
      "Episode:184 meanR:0.5327 R:0.840000 rate:0.0280 gloss:-67366729809920.0000 dlossA:65.6771 dlossQ:2637906.0000 exploreP:0.0100\n",
      "Episode:185 meanR:0.5580 R:2.530000 rate:0.0843 gloss:-29764301619200.0000 dlossA:46.8903 dlossQ:1045750.5625 exploreP:0.0100\n",
      "Episode:186 meanR:0.5593 R:0.410000 rate:0.0137 gloss:-14973409427456.0000 dlossA:43.0870 dlossQ:673177.8125 exploreP:0.0100\n",
      "Episode:187 meanR:0.5588 R:0.300000 rate:0.0100 gloss:-5485572915200.0000 dlossA:60.4650 dlossQ:1258239.1250 exploreP:0.0100\n",
      "Episode:188 meanR:0.5585 R:0.370000 rate:0.0123 gloss:24202505617408.0000 dlossA:47.8529 dlossQ:642589.1250 exploreP:0.0100\n",
      "Episode:189 meanR:0.5607 R:0.220000 rate:0.0073 gloss:14444042125312.0000 dlossA:64.4840 dlossQ:1175686.7500 exploreP:0.0100\n",
      "Episode:190 meanR:0.5629 R:0.220000 rate:0.0073 gloss:2265664454656.0000 dlossA:79.2476 dlossQ:3532558.7500 exploreP:0.0100\n",
      "Episode:191 meanR:0.5596 R:0.000000 rate:0.0000 gloss:-36086562684928.0000 dlossA:116.4124 dlossQ:5207735.5000 exploreP:0.0100\n",
      "Episode:192 meanR:0.5596 R:0.000000 rate:0.0000 gloss:4083847790592.0000 dlossA:59.9936 dlossQ:1179646.7500 exploreP:0.0100\n",
      "Episode:193 meanR:0.5571 R:0.100000 rate:0.0033 gloss:-222964117143552.0000 dlossA:72.2642 dlossQ:2776907.0000 exploreP:0.0100\n",
      "Episode:194 meanR:0.5580 R:0.600000 rate:0.0200 gloss:4198890995712.0000 dlossA:49.1762 dlossQ:692561.4375 exploreP:0.0100\n",
      "Episode:195 meanR:0.5761 R:2.530000 rate:0.0843 gloss:19517908451328.0000 dlossA:50.4962 dlossQ:612928.8125 exploreP:0.0100\n",
      "Episode:196 meanR:0.5733 R:0.070000 rate:0.0023 gloss:29962853679104.0000 dlossA:43.1036 dlossQ:495970.1562 exploreP:0.0100\n",
      "Episode:197 meanR:0.5628 R:0.610000 rate:0.0203 gloss:12628042711040.0000 dlossA:43.0742 dlossQ:482574.9688 exploreP:0.0100\n",
      "Episode:198 meanR:0.5716 R:1.430000 rate:0.0477 gloss:30572197969920.0000 dlossA:51.5454 dlossQ:625596.1875 exploreP:0.0100\n",
      "Episode:199 meanR:0.5723 R:0.300000 rate:0.0100 gloss:-8904918433792.0000 dlossA:49.9231 dlossQ:829254.0625 exploreP:0.0100\n",
      "Episode:200 meanR:0.5758 R:0.350000 rate:0.0117 gloss:-14575646801920.0000 dlossA:63.6265 dlossQ:1405908.2500 exploreP:0.0100\n",
      "Episode:201 meanR:0.5784 R:0.460000 rate:0.0153 gloss:-13926570917888.0000 dlossA:69.6180 dlossQ:2037429.6250 exploreP:0.0100\n",
      "Episode:202 meanR:0.5730 R:0.040000 rate:0.0013 gloss:-7129763151872.0000 dlossA:39.0499 dlossQ:507272.4375 exploreP:0.0100\n",
      "Episode:203 meanR:0.5833 R:1.380000 rate:0.0460 gloss:-268978836471808.0000 dlossA:78.5220 dlossQ:5782232.5000 exploreP:0.0100\n",
      "Episode:204 meanR:0.5751 R:0.110000 rate:0.0037 gloss:-15850733043712.0000 dlossA:31.1121 dlossQ:315419.2500 exploreP:0.0100\n",
      "Episode:205 meanR:0.5801 R:0.500000 rate:0.0167 gloss:1901128318976.0000 dlossA:41.0490 dlossQ:624276.3750 exploreP:0.0100\n",
      "Episode:206 meanR:0.5855 R:0.540000 rate:0.0180 gloss:-17758215471104.0000 dlossA:29.5206 dlossQ:370520.6875 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:207 meanR:0.5868 R:0.540000 rate:0.0180 gloss:-41678371028992.0000 dlossA:40.4188 dlossQ:789435.6875 exploreP:0.0100\n",
      "Episode:208 meanR:0.5970 R:1.590000 rate:0.0530 gloss:-19130937769984.0000 dlossA:60.5590 dlossQ:1448932.5000 exploreP:0.0100\n",
      "Episode:209 meanR:0.6102 R:2.100000 rate:0.0700 gloss:-37359215181824.0000 dlossA:45.1508 dlossQ:772463.8750 exploreP:0.0100\n",
      "Episode:210 meanR:0.6201 R:1.100000 rate:0.0367 gloss:-63519961645056.0000 dlossA:50.4826 dlossQ:1485068.1250 exploreP:0.0100\n",
      "Episode:211 meanR:0.6159 R:0.480000 rate:0.0160 gloss:-68659695321088.0000 dlossA:58.6595 dlossQ:1917926.1250 exploreP:0.0100\n",
      "Episode:212 meanR:0.6166 R:0.660000 rate:0.0220 gloss:-124812437487616.0000 dlossA:70.9000 dlossQ:2348899.5000 exploreP:0.0100\n",
      "Episode:213 meanR:0.6305 R:1.390000 rate:0.0463 gloss:-323171391635456.0000 dlossA:66.9534 dlossQ:6394850.5000 exploreP:0.0100\n",
      "Episode:214 meanR:0.6484 R:1.790000 rate:0.0597 gloss:-16533273182208.0000 dlossA:30.6029 dlossQ:315908.0625 exploreP:0.0100\n",
      "Episode:215 meanR:0.6484 R:0.210000 rate:0.0070 gloss:-22501524504576.0000 dlossA:42.2804 dlossQ:598832.6250 exploreP:0.0100\n",
      "Episode:216 meanR:0.6413 R:0.310000 rate:0.0103 gloss:-35450018332672.0000 dlossA:38.0455 dlossQ:589675.8750 exploreP:0.0100\n",
      "Episode:217 meanR:0.6444 R:0.710000 rate:0.0237 gloss:-41359763308544.0000 dlossA:52.5704 dlossQ:1246885.2500 exploreP:0.0100\n",
      "Episode:218 meanR:0.6432 R:0.000000 rate:0.0000 gloss:-29059553689600.0000 dlossA:64.9614 dlossQ:1551159.3750 exploreP:0.0100\n",
      "Episode:219 meanR:0.6366 R:0.430000 rate:0.0143 gloss:-32202035298304.0000 dlossA:42.0790 dlossQ:649054.5000 exploreP:0.0100\n",
      "Episode:220 meanR:0.6401 R:0.350000 rate:0.0117 gloss:-40915246776320.0000 dlossA:49.4815 dlossQ:1140554.6250 exploreP:0.0100\n",
      "Episode:221 meanR:0.6515 R:1.570000 rate:0.0523 gloss:-72183602741248.0000 dlossA:64.9989 dlossQ:1905698.8750 exploreP:0.0100\n",
      "Episode:222 meanR:0.6618 R:1.030000 rate:0.0343 gloss:-110705357357056.0000 dlossA:56.5015 dlossQ:1481946.2500 exploreP:0.0100\n",
      "Episode:223 meanR:0.6719 R:1.060000 rate:0.0353 gloss:-452578320056320.0000 dlossA:87.5669 dlossQ:6598813.5000 exploreP:0.0100\n",
      "Episode:224 meanR:0.6772 R:0.890000 rate:0.0297 gloss:-20968642707456.0000 dlossA:40.4024 dlossQ:626834.1875 exploreP:0.0100\n",
      "Episode:225 meanR:0.7069 R:3.310000 rate:0.1103 gloss:-22478533427200.0000 dlossA:43.7222 dlossQ:623820.6250 exploreP:0.0100\n",
      "Episode:226 meanR:0.7162 R:1.300000 rate:0.0433 gloss:-46089579266048.0000 dlossA:33.5085 dlossQ:401102.9375 exploreP:0.0100\n",
      "Episode:227 meanR:0.7122 R:0.000000 rate:0.0000 gloss:-57593443647488.0000 dlossA:57.5323 dlossQ:1248758.6250 exploreP:0.0100\n",
      "Episode:228 meanR:0.7233 R:1.210000 rate:0.0403 gloss:-63115911757824.0000 dlossA:60.7032 dlossQ:1239760.2500 exploreP:0.0100\n",
      "Episode:229 meanR:0.7341 R:1.270000 rate:0.0423 gloss:-97899450990592.0000 dlossA:59.3122 dlossQ:2567021.0000 exploreP:0.0100\n",
      "Episode:230 meanR:0.7444 R:1.130000 rate:0.0377 gloss:-137831649378304.0000 dlossA:55.3713 dlossQ:1744832.3750 exploreP:0.0100\n",
      "Episode:231 meanR:0.7491 R:1.120000 rate:0.0373 gloss:-79271448018944.0000 dlossA:56.4606 dlossQ:1201616.0000 exploreP:0.0100\n",
      "Episode:232 meanR:0.7639 R:1.750000 rate:0.0583 gloss:-138323708346368.0000 dlossA:50.3932 dlossQ:1032297.8750 exploreP:0.0100\n",
      "Episode:233 meanR:0.7834 R:2.150000 rate:0.0717 gloss:-434402924429312.0000 dlossA:93.3281 dlossQ:3498922.5000 exploreP:0.0100\n",
      "Episode:234 meanR:0.7918 R:1.000000 rate:0.0333 gloss:-64984394498048.0000 dlossA:37.9796 dlossQ:503164.3438 exploreP:0.0100\n",
      "Episode:235 meanR:0.7875 R:0.240000 rate:0.0080 gloss:-66382469267456.0000 dlossA:61.0548 dlossQ:1355559.3750 exploreP:0.0100\n",
      "Episode:236 meanR:0.7875 R:0.000000 rate:0.0000 gloss:-87101265674240.0000 dlossA:42.8052 dlossQ:660598.6875 exploreP:0.0100\n",
      "Episode:237 meanR:0.8150 R:2.750000 rate:0.0917 gloss:-89397655502848.0000 dlossA:43.5543 dlossQ:740727.9375 exploreP:0.0100\n",
      "Episode:238 meanR:0.8218 R:1.520000 rate:0.0507 gloss:-70672873160704.0000 dlossA:61.1715 dlossQ:1832205.1250 exploreP:0.0100\n",
      "Episode:239 meanR:0.8323 R:1.560000 rate:0.0520 gloss:-90754865168384.0000 dlossA:49.8279 dlossQ:1015319.5000 exploreP:0.0100\n",
      "Episode:240 meanR:0.8378 R:1.270000 rate:0.0423 gloss:-123256015159296.0000 dlossA:80.0900 dlossQ:3342179.5000 exploreP:0.0100\n",
      "Episode:241 meanR:0.8465 R:0.870000 rate:0.0290 gloss:-146790942769152.0000 dlossA:61.8762 dlossQ:1885209.6250 exploreP:0.0100\n",
      "Episode:242 meanR:0.8536 R:0.710000 rate:0.0237 gloss:-155215235907584.0000 dlossA:54.4919 dlossQ:1060445.1250 exploreP:0.0100\n",
      "Episode:243 meanR:0.8542 R:1.780000 rate:0.0593 gloss:-544885723627520.0000 dlossA:94.8700 dlossQ:5902756.0000 exploreP:0.0100\n",
      "Episode:244 meanR:0.8513 R:1.090000 rate:0.0363 gloss:-84854200860672.0000 dlossA:49.2700 dlossQ:1026180.7500 exploreP:0.0100\n",
      "Episode:245 meanR:0.8609 R:1.490000 rate:0.0497 gloss:29685041856512.0000 dlossA:70.0016 dlossQ:2077827.2500 exploreP:0.0100\n",
      "Episode:246 meanR:0.8724 R:1.380000 rate:0.0460 gloss:-94411534893056.0000 dlossA:45.0032 dlossQ:743229.2500 exploreP:0.0100\n",
      "Episode:247 meanR:0.8791 R:2.060000 rate:0.0687 gloss:-99771964456960.0000 dlossA:62.5382 dlossQ:1454839.0000 exploreP:0.0100\n",
      "Episode:248 meanR:0.8847 R:1.590000 rate:0.0530 gloss:-141888791248896.0000 dlossA:66.3181 dlossQ:1669805.8750 exploreP:0.0100\n",
      "Episode:249 meanR:0.8879 R:1.330000 rate:0.0443 gloss:-81117679976448.0000 dlossA:45.7986 dlossQ:710924.6875 exploreP:0.0100\n",
      "Episode:250 meanR:0.8952 R:1.400000 rate:0.0467 gloss:-177473266384896.0000 dlossA:57.8522 dlossQ:1416231.5000 exploreP:0.0100\n",
      "Episode:251 meanR:0.9183 R:2.670000 rate:0.0890 gloss:-398085318508544.0000 dlossA:89.5563 dlossQ:6493361.5000 exploreP:0.0100\n",
      "Episode:252 meanR:0.9257 R:1.070000 rate:0.0357 gloss:-69329202708480.0000 dlossA:63.0154 dlossQ:1256640.3750 exploreP:0.0100\n",
      "Episode:253 meanR:0.9174 R:1.110000 rate:0.0370 gloss:-697708041994240.0000 dlossA:75.7336 dlossQ:4166123.5000 exploreP:0.0100\n",
      "Episode:254 meanR:0.9073 R:0.510000 rate:0.0170 gloss:-83785467035648.0000 dlossA:56.6640 dlossQ:1005994.8125 exploreP:0.0100\n",
      "Episode:255 meanR:0.9272 R:2.410000 rate:0.0803 gloss:-201805950615552.0000 dlossA:79.5079 dlossQ:1852298.6250 exploreP:0.0100\n",
      "Episode:256 meanR:0.9232 R:0.360000 rate:0.0120 gloss:-175865908428800.0000 dlossA:55.4087 dlossQ:1734923.7500 exploreP:0.0100\n",
      "Episode:257 meanR:0.9180 R:0.700000 rate:0.0233 gloss:-198884500439040.0000 dlossA:73.4951 dlossQ:2764347.0000 exploreP:0.0100\n",
      "Episode:258 meanR:0.9287 R:1.310000 rate:0.0437 gloss:-279595156766720.0000 dlossA:83.7021 dlossQ:2312170.5000 exploreP:0.0100\n",
      "Episode:259 meanR:0.9362 R:1.100000 rate:0.0367 gloss:-408873974366208.0000 dlossA:67.1174 dlossQ:2449749.7500 exploreP:0.0100\n",
      "Episode:260 meanR:0.9424 R:1.100000 rate:0.0367 gloss:-279352809881600.0000 dlossA:80.7656 dlossQ:3503156.5000 exploreP:0.0100\n",
      "Episode:261 meanR:0.9225 R:0.620000 rate:0.0207 gloss:-510153061302272.0000 dlossA:87.3691 dlossQ:5082669.5000 exploreP:0.0100\n",
      "Episode:262 meanR:0.9198 R:0.250000 rate:0.0083 gloss:-702398146281472.0000 dlossA:104.8939 dlossQ:4755760.0000 exploreP:0.0100\n",
      "Episode:263 meanR:0.9123 R:0.670000 rate:0.0223 gloss:-2759570174771200.0000 dlossA:139.5990 dlossQ:22591552.0000 exploreP:0.0100\n",
      "Episode:264 meanR:0.9058 R:0.480000 rate:0.0160 gloss:-230746866319360.0000 dlossA:54.9728 dlossQ:1938203.5000 exploreP:0.0100\n",
      "Episode:265 meanR:0.9052 R:0.380000 rate:0.0127 gloss:-574194312019968.0000 dlossA:86.5563 dlossQ:6276251.5000 exploreP:0.0100\n",
      "Episode:266 meanR:0.9131 R:0.790000 rate:0.0263 gloss:-558937849987072.0000 dlossA:68.9205 dlossQ:3767591.7500 exploreP:0.0100\n",
      "Episode:267 meanR:0.9163 R:0.920000 rate:0.0307 gloss:-1269200551149568.0000 dlossA:109.5274 dlossQ:11204424.0000 exploreP:0.0100\n",
      "Episode:268 meanR:0.9200 R:0.470000 rate:0.0157 gloss:-2949361558355968.0000 dlossA:162.3304 dlossQ:43557124.0000 exploreP:0.0100\n",
      "Episode:269 meanR:0.9165 R:0.060000 rate:0.0020 gloss:-660686967406592.0000 dlossA:91.7273 dlossQ:6770745.0000 exploreP:0.0100\n",
      "Episode:270 meanR:0.9148 R:0.080000 rate:0.0027 gloss:-172746805870592.0000 dlossA:89.9301 dlossQ:5992258.0000 exploreP:0.0100\n",
      "Episode:271 meanR:0.9156 R:0.320000 rate:0.0107 gloss:-899698407768064.0000 dlossA:92.2068 dlossQ:7139831.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:272 meanR:0.9127 R:0.000000 rate:0.0000 gloss:-1175519932448768.0000 dlossA:102.0082 dlossQ:9815439.0000 exploreP:0.0100\n",
      "Episode:273 meanR:0.8957 R:0.160000 rate:0.0053 gloss:-928980353941504.0000 dlossA:85.1738 dlossQ:5118488.5000 exploreP:0.0100\n",
      "Episode:274 meanR:0.8905 R:0.070000 rate:0.0023 gloss:-543877077401600.0000 dlossA:87.9458 dlossQ:3970630.5000 exploreP:0.0100\n",
      "Episode:275 meanR:0.8832 R:0.040000 rate:0.0013 gloss:-1371682363146240.0000 dlossA:138.5815 dlossQ:9219262.0000 exploreP:0.0100\n",
      "Episode:276 meanR:0.8796 R:0.000000 rate:0.0000 gloss:-1067769269321728.0000 dlossA:75.9002 dlossQ:3085536.5000 exploreP:0.0100\n",
      "Episode:277 meanR:0.8780 R:0.090000 rate:0.0030 gloss:-1599714122268672.0000 dlossA:150.9716 dlossQ:15240578.0000 exploreP:0.0100\n",
      "Episode:278 meanR:0.8754 R:0.670000 rate:0.0223 gloss:-1262049430601728.0000 dlossA:85.3141 dlossQ:3987049.2500 exploreP:0.0100\n",
      "Episode:279 meanR:0.8797 R:0.560000 rate:0.0187 gloss:4015576452169728.0000 dlossA:199.3711 dlossQ:18286104.0000 exploreP:0.0100\n",
      "Episode:280 meanR:0.8690 R:0.470000 rate:0.0157 gloss:611813863456768.0000 dlossA:217.5137 dlossQ:18927658.0000 exploreP:0.0100\n",
      "Episode:281 meanR:0.8750 R:1.140000 rate:0.0380 gloss:795308556550144.0000 dlossA:98.5835 dlossQ:7888172.0000 exploreP:0.0100\n",
      "Episode:282 meanR:0.8722 R:0.370000 rate:0.0123 gloss:743887470592.0000 dlossA:75.2615 dlossQ:1845925.3750 exploreP:0.0100\n",
      "Episode:283 meanR:0.8641 R:0.060000 rate:0.0020 gloss:-27223673274368.0000 dlossA:103.4865 dlossQ:4901692.0000 exploreP:0.0100\n",
      "Episode:284 meanR:0.8748 R:1.910000 rate:0.0637 gloss:399222981525504.0000 dlossA:135.2202 dlossQ:5911677.0000 exploreP:0.0100\n",
      "Episode:285 meanR:0.8531 R:0.360000 rate:0.0120 gloss:-651189049884672.0000 dlossA:184.4708 dlossQ:11267477.0000 exploreP:0.0100\n",
      "Episode:286 meanR:0.8520 R:0.300000 rate:0.0100 gloss:-213538207432704.0000 dlossA:76.2888 dlossQ:2389526.7500 exploreP:0.0100\n",
      "Episode:287 meanR:0.8528 R:0.380000 rate:0.0127 gloss:-124084566360064.0000 dlossA:110.4485 dlossQ:5856538.5000 exploreP:0.0100\n",
      "Episode:288 meanR:0.8491 R:0.000000 rate:0.0000 gloss:1771235520282624.0000 dlossA:149.5232 dlossQ:26770914.0000 exploreP:0.0100\n",
      "Episode:289 meanR:0.8469 R:0.000000 rate:0.0000 gloss:842829081346048.0000 dlossA:137.7230 dlossQ:10094717.0000 exploreP:0.0100\n",
      "Episode:290 meanR:0.8447 R:0.000000 rate:0.0000 gloss:74081315586048.0000 dlossA:188.2749 dlossQ:21882040.0000 exploreP:0.0100\n",
      "Episode:291 meanR:0.8447 R:0.000000 rate:0.0000 gloss:-878342286868480.0000 dlossA:76.1165 dlossQ:2387083.7500 exploreP:0.0100\n",
      "Episode:292 meanR:0.8447 R:0.000000 rate:0.0000 gloss:-1920388325965824.0000 dlossA:117.6901 dlossQ:11725214.0000 exploreP:0.0100\n",
      "Episode:293 meanR:0.8437 R:0.000000 rate:0.0000 gloss:-4585495526899712.0000 dlossA:188.0691 dlossQ:54801912.0000 exploreP:0.0100\n",
      "Episode:294 meanR:0.8405 R:0.280000 rate:0.0093 gloss:-1966153349988352.0000 dlossA:128.8254 dlossQ:11539630.0000 exploreP:0.0100\n",
      "Episode:295 meanR:0.8191 R:0.390000 rate:0.0130 gloss:-9391578426638336.0000 dlossA:225.8026 dlossQ:102304904.0000 exploreP:0.0100\n",
      "Episode:296 meanR:0.8190 R:0.060000 rate:0.0020 gloss:-1320766331158528.0000 dlossA:71.3732 dlossQ:9526430.0000 exploreP:0.0100\n",
      "Episode:297 meanR:0.8174 R:0.450000 rate:0.0150 gloss:-540680782872576.0000 dlossA:83.7794 dlossQ:7824828.5000 exploreP:0.0100\n",
      "Episode:298 meanR:0.8031 R:0.000000 rate:0.0000 gloss:-28177682701221888.0000 dlossA:247.1868 dlossQ:222246432.0000 exploreP:0.0100\n",
      "Episode:299 meanR:0.8042 R:0.410000 rate:0.0137 gloss:-19926794975051776.0000 dlossA:180.8055 dlossQ:69628328.0000 exploreP:0.0100\n",
      "Episode:300 meanR:0.8013 R:0.060000 rate:0.0020 gloss:-13243398513229824.0000 dlossA:192.8589 dlossQ:163251232.0000 exploreP:0.0100\n",
      "Episode:301 meanR:0.7999 R:0.320000 rate:0.0107 gloss:-9516763838414848.0000 dlossA:127.7218 dlossQ:35362924.0000 exploreP:0.0100\n",
      "Episode:302 meanR:0.7995 R:0.000000 rate:0.0000 gloss:-14571729892409344.0000 dlossA:152.7057 dlossQ:44535432.0000 exploreP:0.0100\n",
      "Episode:303 meanR:0.7857 R:0.000000 rate:0.0000 gloss:-11849731017801728.0000 dlossA:166.8708 dlossQ:27365702.0000 exploreP:0.0100\n",
      "Episode:304 meanR:0.7846 R:0.000000 rate:0.0000 gloss:-1758170900856832.0000 dlossA:142.7800 dlossQ:11763055.0000 exploreP:0.0100\n",
      "Episode:305 meanR:0.7805 R:0.090000 rate:0.0030 gloss:-14106235968159744.0000 dlossA:274.3413 dlossQ:97995928.0000 exploreP:0.0100\n",
      "Episode:306 meanR:0.7751 R:0.000000 rate:0.0000 gloss:-3050842777190400.0000 dlossA:128.3278 dlossQ:14133277.0000 exploreP:0.0100\n",
      "Episode:307 meanR:0.7697 R:0.000000 rate:0.0000 gloss:-1398278948126720.0000 dlossA:87.4733 dlossQ:3985891.2500 exploreP:0.0100\n",
      "Episode:308 meanR:0.7538 R:0.000000 rate:0.0000 gloss:-83774539699322880.0000 dlossA:361.7661 dlossQ:400369984.0000 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    idx_arr = np.arange(memory_size// batch_size) # randomness\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0 # each episode\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        num_step = 0 # each episode\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1 # 1000 episode length\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p >= np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "                action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "                #print('exploRE', action.dtype, action.shape)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                #print('exploIT', action.dtype, action.shape)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            #print(action.dtype, action.shape)\n",
    "            #print(action.reshape([-1]).dtype)\n",
    "            #print(action.reshape([-1]).shape)\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            memory.rates.append(-1) # empty\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/30 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.rates[-1-idx] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.rates[-1-idx] = rate # rate the trajectory/data\n",
    "\n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            percentage = 0.9\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(actions.dtype,actions.shape)\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array(memory.rates)[idx*batch_size:(idx+1)*batch_size]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            states = states[rates >= (np.max(rates)*percentage)]\n",
    "            actions = actions[rates >= (np.max(rates)*percentage)]\n",
    "            #print(actions.dtype,actions.shape)\n",
    "            next_states = next_states[rates >= (np.max(rates)*percentage)]\n",
    "            rewards = rewards[rates >= (np.max(rates)*percentage)]\n",
    "            dones = dones[rates >= (np.max(rates)*percentage)]\n",
    "            rates = rates[rates >= (np.max(rates)*percentage)]\n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            #print(targetQs.shape, actions.shape, rates.shape, states.shape)\n",
    "            #print(targetQs.dtype, actions.dtype, rates.dtype, states.dtype)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
