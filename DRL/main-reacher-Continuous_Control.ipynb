{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float64, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float64, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float64, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits,\n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob_actions * targetQs) # DPG\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    dloss = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) #Qs\n",
    "    dloss += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dloss /= 2 # if dQs==gQs\n",
    "    gloss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    gloss2 = tf.reduce_mean(gQs)\n",
    "    gloss3 = tf.reduce_mean(dQs)\n",
    "    gloss4 = tf.reduce_mean(targetQs)\n",
    "    return actions_logits, gQs, gloss, dloss, gloss1, gloss2, gloss3, gloss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        \n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, glearning_rate, dlearning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2, self.g_loss3, self.g_loss4 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                           g_learning_rate=glearning_rate, d_learning_rate=dlearning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.697119985869685\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "glearning_rate = 0.0001         # Q-network learning rate\n",
    "dlearning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100              # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, \n",
    "              glearning_rate=glearning_rate, dlearning_rate=dlearning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "num_steps = 0\n",
    "for _ in range(memory_size):\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Steps:1000 meanR:0.0000 R:0.0000 gloss:0.5825 gloss1-lgP:0.8450 gloss2gQs:0.7646 gloss3dQs:0.7457 gloss4tgtQ:0.7574 dloss:0.0589 exploreP:0.9057\n",
      "Episode:1 Steps:1000 meanR:0.0600 R:0.1200 gloss:0.4479 gloss1-lgP:0.8812 gloss2gQs:0.5601 gloss3dQs:0.5463 gloss4tgtQ:0.5538 dloss:0.0374 exploreP:0.8204\n",
      "Episode:2 Steps:1000 meanR:0.0400 R:0.0000 gloss:0.2329 gloss1-lgP:0.7268 gloss2gQs:0.3202 gloss3dQs:0.3170 gloss4tgtQ:0.3165 dloss:0.0117 exploreP:0.7432\n",
      "Episode:3 Steps:1000 meanR:0.0575 R:0.1100 gloss:0.0395 gloss1-lgP:0.7081 gloss2gQs:0.0574 gloss3dQs:0.0566 gloss4tgtQ:0.0569 dloss:0.0067 exploreP:0.6734\n",
      "Episode:4 Steps:1000 meanR:0.0460 R:0.0000 gloss:0.0389 gloss1-lgP:0.7056 gloss2gQs:0.0566 gloss3dQs:0.0556 gloss4tgtQ:0.0563 dloss:0.0049 exploreP:0.6102\n",
      "Episode:5 Steps:1000 meanR:0.0383 R:0.0000 gloss:0.0188 gloss1-lgP:0.7080 gloss2gQs:0.0281 gloss3dQs:0.0269 gloss4tgtQ:0.0276 dloss:0.0036 exploreP:0.5530\n",
      "Episode:6 Steps:1000 meanR:0.0471 R:0.1000 gloss:0.0236 gloss1-lgP:0.7056 gloss2gQs:0.0344 gloss3dQs:0.0337 gloss4tgtQ:0.0342 dloss:0.0027 exploreP:0.5013\n",
      "Episode:7 Steps:1000 meanR:0.0412 R:0.0000 gloss:0.0186 gloss1-lgP:0.7097 gloss2gQs:0.0273 gloss3dQs:0.0268 gloss4tgtQ:0.0271 dloss:0.0023 exploreP:0.4545\n",
      "Episode:8 Steps:1000 meanR:0.0367 R:0.0000 gloss:0.0256 gloss1-lgP:0.7107 gloss2gQs:0.0372 gloss3dQs:0.0367 gloss4tgtQ:0.0370 dloss:0.0018 exploreP:0.4121\n",
      "Episode:9 Steps:1000 meanR:0.0330 R:0.0000 gloss:0.0230 gloss1-lgP:0.6998 gloss2gQs:0.0338 gloss3dQs:0.0329 gloss4tgtQ:0.0334 dloss:0.0013 exploreP:0.3738\n",
      "Episode:10 Steps:1000 meanR:0.0727 R:0.4700 gloss:0.0303 gloss1-lgP:0.6974 gloss2gQs:0.0441 gloss3dQs:0.0434 gloss4tgtQ:0.0438 dloss:0.0010 exploreP:0.3392\n",
      "Episode:11 Steps:1000 meanR:0.0667 R:0.0000 gloss:0.0281 gloss1-lgP:0.6927 gloss2gQs:0.0410 gloss3dQs:0.0406 gloss4tgtQ:0.0408 dloss:0.0008 exploreP:0.3078\n",
      "Episode:12 Steps:1000 meanR:0.0769 R:0.2000 gloss:0.3627 gloss1-lgP:0.9286 gloss2gQs:0.2201 gloss3dQs:0.1961 gloss4tgtQ:0.2179 dloss:0.0284 exploreP:0.2795\n",
      "Episode:13 Steps:1000 meanR:0.1243 R:0.7400 gloss:0.3490 gloss1-lgP:0.7675 gloss2gQs:0.5031 gloss3dQs:0.5019 gloss4tgtQ:0.4977 dloss:0.0169 exploreP:0.2538\n",
      "Episode:14 Steps:1000 meanR:0.1493 R:0.5000 gloss:0.2458 gloss1-lgP:0.8172 gloss2gQs:0.2699 gloss3dQs:0.2621 gloss4tgtQ:0.2671 dloss:0.0273 exploreP:0.2306\n",
      "Episode:15 Steps:1000 meanR:0.1687 R:0.4600 gloss:0.2080 gloss1-lgP:0.6854 gloss2gQs:0.3140 gloss3dQs:0.3148 gloss4tgtQ:0.3105 dloss:0.0217 exploreP:0.2096\n",
      "Episode:16 Steps:1000 meanR:0.1588 R:0.0000 gloss:0.0397 gloss1-lgP:0.6811 gloss2gQs:0.0610 gloss3dQs:0.0620 gloss4tgtQ:0.0607 dloss:0.0034 exploreP:0.1905\n",
      "Episode:17 Steps:1000 meanR:0.1883 R:0.6900 gloss:0.1967 gloss1-lgP:0.8012 gloss2gQs:0.2660 gloss3dQs:0.2597 gloss4tgtQ:0.2636 dloss:0.0146 exploreP:0.1734\n",
      "Episode:18 Steps:1000 meanR:0.1858 R:0.1400 gloss:0.0475 gloss1-lgP:0.6732 gloss2gQs:0.0728 gloss3dQs:0.0736 gloss4tgtQ:0.0724 dloss:0.0029 exploreP:0.1578\n",
      "Episode:19 Steps:1000 meanR:0.1845 R:0.1600 gloss:0.0438 gloss1-lgP:0.7911 gloss2gQs:0.0514 gloss3dQs:0.0492 gloss4tgtQ:0.0512 dloss:0.0059 exploreP:0.1437\n",
      "Episode:20 Steps:1000 meanR:0.1757 R:0.0000 gloss:0.0268 gloss1-lgP:0.7067 gloss2gQs:0.0417 gloss3dQs:0.0435 gloss4tgtQ:0.0415 dloss:0.0036 exploreP:0.1310\n",
      "Episode:21 Steps:1000 meanR:0.1759 R:0.1800 gloss:0.0419 gloss1-lgP:0.8644 gloss2gQs:0.0548 gloss3dQs:0.0539 gloss4tgtQ:0.0547 dloss:0.0035 exploreP:0.1195\n",
      "Episode:22 Steps:1000 meanR:0.1826 R:0.3300 gloss:0.0493 gloss1-lgP:0.7375 gloss2gQs:0.0692 gloss3dQs:0.0686 gloss4tgtQ:0.0690 dloss:0.0024 exploreP:0.1090\n",
      "Episode:23 Steps:1000 meanR:0.1967 R:0.5200 gloss:0.0103 gloss1-lgP:0.7610 gloss2gQs:0.0121 gloss3dQs:0.0115 gloss4tgtQ:0.0122 dloss:0.0023 exploreP:0.0996\n",
      "Episode:24 Steps:1000 meanR:0.2076 R:0.4700 gloss:0.0329 gloss1-lgP:0.6634 gloss2gQs:0.0499 gloss3dQs:0.0501 gloss4tgtQ:0.0499 dloss:0.0010 exploreP:0.0911\n",
      "Episode:25 Steps:1000 meanR:0.2019 R:0.0600 gloss:0.0099 gloss1-lgP:0.6590 gloss2gQs:0.0159 gloss3dQs:0.0160 gloss4tgtQ:0.0160 dloss:0.0005 exploreP:0.0833\n",
      "Episode:26 Steps:1000 meanR:0.1944 R:0.0000 gloss:0.0200 gloss1-lgP:0.6745 gloss2gQs:0.0304 gloss3dQs:0.0303 gloss4tgtQ:0.0305 dloss:0.0005 exploreP:0.0764\n",
      "Episode:27 Steps:1000 meanR:0.2029 R:0.4300 gloss:0.0082 gloss1-lgP:0.7562 gloss2gQs:0.0135 gloss3dQs:0.0137 gloss4tgtQ:0.0136 dloss:0.0004 exploreP:0.0700\n",
      "Episode:28 Steps:1000 meanR:0.1997 R:0.1100 gloss:0.0191 gloss1-lgP:0.6950 gloss2gQs:0.0256 gloss3dQs:0.0258 gloss4tgtQ:0.0256 dloss:0.0004 exploreP:0.0643\n",
      "Episode:29 Steps:1000 meanR:0.1990 R:0.1800 gloss:0.0058 gloss1-lgP:0.6967 gloss2gQs:0.0084 gloss3dQs:0.0087 gloss4tgtQ:0.0086 dloss:0.0002 exploreP:0.0591\n",
      "Episode:30 Steps:1000 meanR:0.1926 R:0.0000 gloss:0.0132 gloss1-lgP:0.6711 gloss2gQs:0.0201 gloss3dQs:0.0202 gloss4tgtQ:0.0202 dloss:0.0002 exploreP:0.0545\n",
      "Episode:31 Steps:1000 meanR:0.1944 R:0.2500 gloss:0.0100 gloss1-lgP:0.6482 gloss2gQs:0.0154 gloss3dQs:0.0154 gloss4tgtQ:0.0155 dloss:0.0001 exploreP:0.0502\n",
      "Episode:32 Steps:1000 meanR:0.1885 R:0.0000 gloss:0.0119 gloss1-lgP:0.6537 gloss2gQs:0.0184 gloss3dQs:0.0184 gloss4tgtQ:0.0184 dloss:0.0001 exploreP:0.0464\n",
      "Episode:33 Steps:1000 meanR:0.1953 R:0.4200 gloss:0.0136 gloss1-lgP:0.6649 gloss2gQs:0.0209 gloss3dQs:0.0208 gloss4tgtQ:0.0209 dloss:0.0001 exploreP:0.0429\n",
      "Episode:34 Steps:1000 meanR:0.2403 R:1.7700 gloss:0.0135 gloss1-lgP:0.6425 gloss2gQs:0.0210 gloss3dQs:0.0210 gloss4tgtQ:0.0210 dloss:0.0000 exploreP:0.0398\n",
      "Episode:35 Steps:1000 meanR:0.2414 R:0.2800 gloss:0.0229 gloss1-lgP:0.6460 gloss2gQs:0.0356 gloss3dQs:0.0355 gloss4tgtQ:0.0356 dloss:0.0001 exploreP:0.0370\n",
      "Episode:36 Steps:1000 meanR:0.2376 R:0.1000 gloss:0.0217 gloss1-lgP:0.6376 gloss2gQs:0.0339 gloss3dQs:0.0339 gloss4tgtQ:0.0339 dloss:0.0000 exploreP:0.0344\n",
      "Episode:37 Steps:1000 meanR:0.2468 R:0.5900 gloss:0.0229 gloss1-lgP:0.6370 gloss2gQs:0.0357 gloss3dQs:0.0357 gloss4tgtQ:0.0358 dloss:0.0001 exploreP:0.0321\n",
      "Episode:38 Steps:1000 meanR:0.2533 R:0.5000 gloss:0.0258 gloss1-lgP:0.6411 gloss2gQs:0.0399 gloss3dQs:0.0399 gloss4tgtQ:0.0399 dloss:0.0000 exploreP:0.0300\n",
      "Episode:39 Steps:1000 meanR:0.2520 R:0.2000 gloss:0.0243 gloss1-lgP:0.6417 gloss2gQs:0.0376 gloss3dQs:0.0376 gloss4tgtQ:0.0376 dloss:0.0000 exploreP:0.0281\n",
      "Episode:40 Steps:1000 meanR:0.2824 R:1.5000 gloss:0.0295 gloss1-lgP:0.6432 gloss2gQs:0.0456 gloss3dQs:0.0456 gloss4tgtQ:0.0456 dloss:0.0001 exploreP:0.0263\n",
      "Episode:41 Steps:1000 meanR:0.2902 R:0.6100 gloss:0.0329 gloss1-lgP:0.6597 gloss2gQs:0.0503 gloss3dQs:0.0503 gloss4tgtQ:0.0503 dloss:0.0001 exploreP:0.0248\n",
      "Episode:42 Steps:1000 meanR:0.2865 R:0.1300 gloss:0.0361 gloss1-lgP:0.6472 gloss2gQs:0.0555 gloss3dQs:0.0554 gloss4tgtQ:0.0555 dloss:0.0001 exploreP:0.0234\n",
      "Episode:43 Steps:1000 meanR:0.2800 R:0.0000 gloss:0.0337 gloss1-lgP:0.6466 gloss2gQs:0.0518 gloss3dQs:0.0518 gloss4tgtQ:0.0518 dloss:0.0001 exploreP:0.0221\n",
      "Episode:44 Steps:1000 meanR:0.2807 R:0.3100 gloss:0.0276 gloss1-lgP:0.6518 gloss2gQs:0.0424 gloss3dQs:0.0423 gloss4tgtQ:0.0423 dloss:0.0001 exploreP:0.0209\n",
      "Episode:45 Steps:1000 meanR:0.2761 R:0.0700 gloss:0.0255 gloss1-lgP:0.6475 gloss2gQs:0.0392 gloss3dQs:0.0392 gloss4tgtQ:0.0392 dloss:0.0001 exploreP:0.0199\n",
      "Episode:46 Steps:1000 meanR:0.2926 R:1.0500 gloss:0.0560 gloss1-lgP:0.7272 gloss2gQs:0.0596 gloss3dQs:0.0569 gloss4tgtQ:0.0595 dloss:0.0009 exploreP:0.0190\n",
      "Episode:47 Steps:1000 meanR:0.2865 R:0.0000 gloss:0.1326 gloss1-lgP:0.6480 gloss2gQs:0.2064 gloss3dQs:0.2064 gloss4tgtQ:0.2047 dloss:0.0025 exploreP:0.0181\n",
      "Episode:48 Steps:1000 meanR:0.2806 R:0.0000 gloss:0.0279 gloss1-lgP:0.6446 gloss2gQs:0.0433 gloss3dQs:0.0433 gloss4tgtQ:0.0432 dloss:0.0001 exploreP:0.0173\n",
      "Episode:49 Steps:1000 meanR:0.2868 R:0.5900 gloss:0.0227 gloss1-lgP:0.6445 gloss2gQs:0.0349 gloss3dQs:0.0349 gloss4tgtQ:0.0349 dloss:0.0001 exploreP:0.0166\n",
      "Episode:50 Steps:1000 meanR:0.2831 R:0.1000 gloss:0.0240 gloss1-lgP:0.6589 gloss2gQs:0.0368 gloss3dQs:0.0365 gloss4tgtQ:0.0366 dloss:0.0002 exploreP:0.0160\n",
      "Episode:51 Steps:1000 meanR:0.2777 R:0.0000 gloss:0.0190 gloss1-lgP:0.6403 gloss2gQs:0.0294 gloss3dQs:0.0294 gloss4tgtQ:0.0294 dloss:0.0001 exploreP:0.0154\n",
      "Episode:52 Steps:1000 meanR:0.2734 R:0.0500 gloss:0.0196 gloss1-lgP:0.6417 gloss2gQs:0.0304 gloss3dQs:0.0302 gloss4tgtQ:0.0303 dloss:0.0001 exploreP:0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:53 Steps:1000 meanR:0.2852 R:0.9100 gloss:0.0170 gloss1-lgP:0.6387 gloss2gQs:0.0263 gloss3dQs:0.0263 gloss4tgtQ:0.0263 dloss:0.0000 exploreP:0.0144\n",
      "Episode:54 Steps:1000 meanR:0.2800 R:0.0000 gloss:0.0178 gloss1-lgP:0.6459 gloss2gQs:0.0275 gloss3dQs:0.0275 gloss4tgtQ:0.0275 dloss:0.0001 exploreP:0.0140\n",
      "Episode:55 Steps:1000 meanR:0.2750 R:0.0000 gloss:0.0182 gloss1-lgP:0.6449 gloss2gQs:0.0283 gloss3dQs:0.0283 gloss4tgtQ:0.0283 dloss:0.0001 exploreP:0.0136\n",
      "Episode:56 Steps:1000 meanR:0.2853 R:0.8600 gloss:0.1464 gloss1-lgP:0.7928 gloss2gQs:0.1825 gloss3dQs:0.1789 gloss4tgtQ:0.1808 dloss:0.0037 exploreP:0.0133\n",
      "Episode:57 Steps:1000 meanR:0.2803 R:0.0000 gloss:0.0253 gloss1-lgP:0.6332 gloss2gQs:0.0399 gloss3dQs:0.0399 gloss4tgtQ:0.0397 dloss:0.0001 exploreP:0.0130\n",
      "Episode:58 Steps:1000 meanR:0.2834 R:0.4600 gloss:0.0124 gloss1-lgP:0.6323 gloss2gQs:0.0193 gloss3dQs:0.0192 gloss4tgtQ:0.0193 dloss:0.0001 exploreP:0.0127\n",
      "Episode:59 Steps:1000 meanR:0.2880 R:0.5600 gloss:0.0142 gloss1-lgP:0.6335 gloss2gQs:0.0221 gloss3dQs:0.0221 gloss4tgtQ:0.0221 dloss:0.0000 exploreP:0.0124\n",
      "Episode:60 Steps:1000 meanR:0.2833 R:0.0000 gloss:0.0148 gloss1-lgP:0.6348 gloss2gQs:0.0231 gloss3dQs:0.0231 gloss4tgtQ:0.0231 dloss:0.0000 exploreP:0.0122\n",
      "Episode:61 Steps:1000 meanR:0.2877 R:0.5600 gloss:0.0333 gloss1-lgP:0.7098 gloss2gQs:0.0424 gloss3dQs:0.0420 gloss4tgtQ:0.0423 dloss:0.0003 exploreP:0.0120\n",
      "Episode:62 Steps:1000 meanR:0.2933 R:0.6400 gloss:0.0374 gloss1-lgP:0.6654 gloss2gQs:0.0564 gloss3dQs:0.0556 gloss4tgtQ:0.0560 dloss:0.0003 exploreP:0.0118\n",
      "Episode:63 Steps:1000 meanR:0.2902 R:0.0900 gloss:0.0203 gloss1-lgP:0.6380 gloss2gQs:0.0315 gloss3dQs:0.0315 gloss4tgtQ:0.0315 dloss:0.0000 exploreP:0.0116\n",
      "Episode:64 Steps:1000 meanR:0.3120 R:1.7100 gloss:0.0209 gloss1-lgP:0.6382 gloss2gQs:0.0325 gloss3dQs:0.0325 gloss4tgtQ:0.0325 dloss:0.0000 exploreP:0.0115\n",
      "Episode:65 Steps:1000 meanR:0.3132 R:0.3900 gloss:0.0295 gloss1-lgP:0.6421 gloss2gQs:0.0456 gloss3dQs:0.0456 gloss4tgtQ:0.0457 dloss:0.0001 exploreP:0.0113\n",
      "Episode:66 Steps:1000 meanR:0.3107 R:0.1500 gloss:0.0291 gloss1-lgP:0.6434 gloss2gQs:0.0449 gloss3dQs:0.0449 gloss4tgtQ:0.0449 dloss:0.0001 exploreP:0.0112\n",
      "Episode:67 Steps:1000 meanR:0.3101 R:0.2700 gloss:0.0277 gloss1-lgP:0.6432 gloss2gQs:0.0427 gloss3dQs:0.0427 gloss4tgtQ:0.0427 dloss:0.0001 exploreP:0.0111\n",
      "Episode:68 Steps:1000 meanR:0.3128 R:0.4900 gloss:0.0287 gloss1-lgP:0.6450 gloss2gQs:0.0442 gloss3dQs:0.0441 gloss4tgtQ:0.0441 dloss:0.0001 exploreP:0.0110\n",
      "Episode:69 Steps:1000 meanR:0.3150 R:0.4700 gloss:0.0307 gloss1-lgP:0.6596 gloss2gQs:0.0468 gloss3dQs:0.0467 gloss4tgtQ:0.0467 dloss:0.0001 exploreP:0.0109\n",
      "Episode:70 Steps:1000 meanR:0.3106 R:0.0000 gloss:0.0266 gloss1-lgP:0.6402 gloss2gQs:0.0410 gloss3dQs:0.0410 gloss4tgtQ:0.0410 dloss:0.0001 exploreP:0.0108\n",
      "Episode:71 Steps:1000 meanR:0.3160 R:0.7000 gloss:0.0261 gloss1-lgP:0.6418 gloss2gQs:0.0402 gloss3dQs:0.0403 gloss4tgtQ:0.0403 dloss:0.0001 exploreP:0.0107\n",
      "Episode:72 Steps:1000 meanR:0.3160 R:0.3200 gloss:0.0288 gloss1-lgP:0.6427 gloss2gQs:0.0443 gloss3dQs:0.0443 gloss4tgtQ:0.0443 dloss:0.0001 exploreP:0.0107\n",
      "Episode:73 Steps:1000 meanR:0.3126 R:0.0600 gloss:0.0276 gloss1-lgP:0.6375 gloss2gQs:0.0426 gloss3dQs:0.0426 gloss4tgtQ:0.0426 dloss:0.0001 exploreP:0.0106\n",
      "Episode:74 Steps:1000 meanR:0.3112 R:0.2100 gloss:0.0891 gloss1-lgP:0.7683 gloss2gQs:0.0999 gloss3dQs:0.0975 gloss4tgtQ:0.0991 dloss:0.0011 exploreP:0.0105\n",
      "Episode:75 Steps:1000 meanR:0.3071 R:0.0000 gloss:0.0496 gloss1-lgP:0.6372 gloss2gQs:0.0783 gloss3dQs:0.0783 gloss4tgtQ:0.0778 dloss:0.0003 exploreP:0.0105\n",
      "Episode:76 Steps:1000 meanR:0.3044 R:0.1000 gloss:0.0184 gloss1-lgP:0.6353 gloss2gQs:0.0286 gloss3dQs:0.0286 gloss4tgtQ:0.0285 dloss:0.0000 exploreP:0.0104\n",
      "Episode:77 Steps:1000 meanR:0.3035 R:0.2300 gloss:0.0147 gloss1-lgP:0.6351 gloss2gQs:0.0229 gloss3dQs:0.0228 gloss4tgtQ:0.0228 dloss:0.0000 exploreP:0.0104\n",
      "Episode:78 Steps:1000 meanR:0.3010 R:0.1100 gloss:0.0131 gloss1-lgP:0.6347 gloss2gQs:0.0203 gloss3dQs:0.0203 gloss4tgtQ:0.0203 dloss:0.0000 exploreP:0.0104\n",
      "Episode:79 Steps:1000 meanR:0.3045 R:0.5800 gloss:0.0134 gloss1-lgP:0.6340 gloss2gQs:0.0208 gloss3dQs:0.0208 gloss4tgtQ:0.0208 dloss:0.0000 exploreP:0.0103\n",
      "Episode:80 Steps:1000 meanR:0.3164 R:1.2700 gloss:0.0134 gloss1-lgP:0.6343 gloss2gQs:0.0207 gloss3dQs:0.0207 gloss4tgtQ:0.0207 dloss:0.0000 exploreP:0.0103\n",
      "Episode:81 Steps:1000 meanR:0.3216 R:0.7400 gloss:0.0208 gloss1-lgP:0.6361 gloss2gQs:0.0321 gloss3dQs:0.0321 gloss4tgtQ:0.0322 dloss:0.0001 exploreP:0.0103\n",
      "Episode:82 Steps:1000 meanR:0.3408 R:1.9200 gloss:-0.0094 gloss1-lgP:0.6746 gloss2gQs:0.0250 gloss3dQs:0.0268 gloss4tgtQ:0.0252 dloss:0.0004 exploreP:0.0102\n",
      "Episode:83 Steps:1000 meanR:0.3379 R:0.0900 gloss:1.1813 gloss1-lgP:1.0242 gloss2gQs:1.0180 gloss3dQs:0.9324 gloss4tgtQ:1.0078 dloss:0.1799 exploreP:0.0102\n",
      "Episode:84 Steps:1000 meanR:0.3339 R:0.0000 gloss:0.2852 gloss1-lgP:0.6365 gloss2gQs:0.4571 gloss3dQs:0.4564 gloss4tgtQ:0.4532 dloss:0.0029 exploreP:0.0102\n",
      "Episode:85 Steps:1000 meanR:0.3328 R:0.2400 gloss:0.1432 gloss1-lgP:0.6346 gloss2gQs:0.2286 gloss3dQs:0.2284 gloss4tgtQ:0.2265 dloss:0.0015 exploreP:0.0102\n",
      "Episode:86 Steps:1000 meanR:0.3290 R:0.0000 gloss:0.0654 gloss1-lgP:0.6342 gloss2gQs:0.1026 gloss3dQs:0.1028 gloss4tgtQ:0.1021 dloss:0.0011 exploreP:0.0102\n",
      "Episode:87 Steps:1000 meanR:0.3312 R:0.5300 gloss:0.0477 gloss1-lgP:0.6351 gloss2gQs:0.0740 gloss3dQs:0.0741 gloss4tgtQ:0.0737 dloss:0.0008 exploreP:0.0101\n",
      "Episode:88 Steps:1000 meanR:0.3338 R:0.5600 gloss:0.0413 gloss1-lgP:0.6977 gloss2gQs:0.0612 gloss3dQs:0.0613 gloss4tgtQ:0.0612 dloss:0.0017 exploreP:0.0101\n",
      "Episode:89 Steps:1000 meanR:0.3318 R:0.1500 gloss:0.0403 gloss1-lgP:0.6384 gloss2gQs:0.0623 gloss3dQs:0.0624 gloss4tgtQ:0.0622 dloss:0.0007 exploreP:0.0101\n",
      "Episode:90 Steps:1000 meanR:0.3281 R:0.0000 gloss:0.0315 gloss1-lgP:0.6349 gloss2gQs:0.0488 gloss3dQs:0.0488 gloss4tgtQ:0.0488 dloss:0.0004 exploreP:0.0101\n",
      "Episode:91 Steps:1000 meanR:0.3336 R:0.8300 gloss:0.0519 gloss1-lgP:0.6697 gloss2gQs:0.0779 gloss3dQs:0.0771 gloss4tgtQ:0.0775 dloss:0.0019 exploreP:0.0101\n",
      "Episode:92 Steps:1000 meanR:0.3347 R:0.4400 gloss:0.0202 gloss1-lgP:0.6343 gloss2gQs:0.0313 gloss3dQs:0.0315 gloss4tgtQ:0.0313 dloss:0.0002 exploreP:0.0101\n",
      "Episode:93 Steps:1000 meanR:0.3320 R:0.0800 gloss:0.0164 gloss1-lgP:0.6396 gloss2gQs:0.0253 gloss3dQs:0.0253 gloss4tgtQ:0.0253 dloss:0.0001 exploreP:0.0101\n",
      "Episode:94 Steps:1000 meanR:0.3345 R:0.5700 gloss:0.0266 gloss1-lgP:0.6500 gloss2gQs:0.0410 gloss3dQs:0.0409 gloss4tgtQ:0.0410 dloss:0.0002 exploreP:0.0101\n",
      "Episode:95 Steps:1000 meanR:0.3310 R:0.0000 gloss:0.0200 gloss1-lgP:0.6440 gloss2gQs:0.0307 gloss3dQs:0.0307 gloss4tgtQ:0.0307 dloss:0.0001 exploreP:0.0101\n",
      "Episode:96 Steps:1000 meanR:0.3276 R:0.0000 gloss:0.0172 gloss1-lgP:0.6414 gloss2gQs:0.0264 gloss3dQs:0.0264 gloss4tgtQ:0.0264 dloss:0.0001 exploreP:0.0101\n",
      "Episode:97 Steps:1000 meanR:0.3279 R:0.3500 gloss:-0.4377 gloss1-lgP:2.7197 gloss2gQs:-0.0664 gloss3dQs:-0.1325 gloss4tgtQ:-0.0656 dloss:0.0851 exploreP:0.0101\n",
      "Episode:98 Steps:1000 meanR:0.3306 R:0.6000 gloss:4.8400 gloss1-lgP:4.3816 gloss2gQs:0.6715 gloss3dQs:0.6725 gloss4tgtQ:0.6644 dloss:0.1198 exploreP:0.0100\n",
      "Episode:99 Steps:1000 meanR:0.3344 R:0.7100 gloss:0.2889 gloss1-lgP:0.6710 gloss2gQs:0.4315 gloss3dQs:0.4301 gloss4tgtQ:0.4273 dloss:0.0025 exploreP:0.0100\n",
      "Episode:100 Steps:1000 meanR:0.3344 R:0.0000 gloss:0.1326 gloss1-lgP:0.6470 gloss2gQs:0.2076 gloss3dQs:0.2074 gloss4tgtQ:0.2054 dloss:0.0011 exploreP:0.0100\n",
      "Episode:101 Steps:1000 meanR:0.3387 R:0.5500 gloss:0.0502 gloss1-lgP:0.6411 gloss2gQs:0.0799 gloss3dQs:0.0805 gloss4tgtQ:0.0794 dloss:0.0008 exploreP:0.0100\n",
      "Episode:102 Steps:1000 meanR:0.3387 R:0.0000 gloss:0.0284 gloss1-lgP:0.7190 gloss2gQs:0.0488 gloss3dQs:0.0485 gloss4tgtQ:0.0486 dloss:0.0006 exploreP:0.0100\n",
      "Episode:103 Steps:1000 meanR:0.3407 R:0.3100 gloss:0.0305 gloss1-lgP:0.7288 gloss2gQs:0.0508 gloss3dQs:0.0508 gloss4tgtQ:0.0507 dloss:0.0003 exploreP:0.0100\n",
      "Episode:104 Steps:1000 meanR:0.3462 R:0.5500 gloss:0.0217 gloss1-lgP:0.7207 gloss2gQs:0.0372 gloss3dQs:0.0372 gloss4tgtQ:0.0370 dloss:0.0002 exploreP:0.0100\n",
      "Episode:105 Steps:1000 meanR:0.3567 R:1.0500 gloss:0.0195 gloss1-lgP:0.7749 gloss2gQs:0.0306 gloss3dQs:0.0306 gloss4tgtQ:0.0306 dloss:0.0004 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:106 Steps:1000 meanR:0.3633 R:0.7600 gloss:0.0171 gloss1-lgP:0.6578 gloss2gQs:0.0289 gloss3dQs:0.0291 gloss4tgtQ:0.0291 dloss:0.0001 exploreP:0.0100\n",
      "Episode:107 Steps:1000 meanR:0.3745 R:1.1200 gloss:0.0237 gloss1-lgP:0.5881 gloss2gQs:0.0403 gloss3dQs:0.0404 gloss4tgtQ:0.0404 dloss:0.0001 exploreP:0.0100\n",
      "Episode:108 Steps:1000 meanR:0.3875 R:1.3000 gloss:0.0280 gloss1-lgP:0.5935 gloss2gQs:0.0473 gloss3dQs:0.0475 gloss4tgtQ:0.0474 dloss:0.0002 exploreP:0.0100\n",
      "Episode:109 Steps:1000 meanR:0.3997 R:1.2200 gloss:0.0323 gloss1-lgP:0.5870 gloss2gQs:0.0547 gloss3dQs:0.0547 gloss4tgtQ:0.0548 dloss:0.0001 exploreP:0.0100\n",
      "Episode:110 Steps:1000 meanR:0.4087 R:1.3700 gloss:0.0416 gloss1-lgP:0.5956 gloss2gQs:0.0712 gloss3dQs:0.0711 gloss4tgtQ:0.0712 dloss:0.0001 exploreP:0.0100\n",
      "Episode:111 Steps:1000 meanR:0.4189 R:1.0200 gloss:0.0442 gloss1-lgP:0.5768 gloss2gQs:0.0764 gloss3dQs:0.0764 gloss4tgtQ:0.0765 dloss:0.0001 exploreP:0.0100\n",
      "Episode:112 Steps:1000 meanR:0.4275 R:1.0600 gloss:0.0503 gloss1-lgP:0.5738 gloss2gQs:0.0877 gloss3dQs:0.0877 gloss4tgtQ:0.0877 dloss:0.0001 exploreP:0.0100\n",
      "Episode:113 Steps:1000 meanR:0.4298 R:0.9700 gloss:0.0499 gloss1-lgP:0.5712 gloss2gQs:0.0875 gloss3dQs:0.0875 gloss4tgtQ:0.0875 dloss:0.0001 exploreP:0.0100\n",
      "Episode:114 Steps:1000 meanR:0.4346 R:0.9800 gloss:0.0520 gloss1-lgP:0.5724 gloss2gQs:0.0911 gloss3dQs:0.0911 gloss4tgtQ:0.0911 dloss:0.0001 exploreP:0.0100\n",
      "Episode:115 Steps:1000 meanR:0.4367 R:0.6700 gloss:0.0553 gloss1-lgP:0.5739 gloss2gQs:0.0962 gloss3dQs:0.0962 gloss4tgtQ:0.0962 dloss:0.0001 exploreP:0.0100\n",
      "Episode:116 Steps:1000 meanR:0.4401 R:0.3400 gloss:0.0565 gloss1-lgP:0.5790 gloss2gQs:0.0982 gloss3dQs:0.0982 gloss4tgtQ:0.0982 dloss:0.0001 exploreP:0.0100\n",
      "Episode:117 Steps:1000 meanR:0.4387 R:0.5500 gloss:0.0497 gloss1-lgP:0.5804 gloss2gQs:0.0858 gloss3dQs:0.0857 gloss4tgtQ:0.0858 dloss:0.0001 exploreP:0.0100\n",
      "Episode:118 Steps:1000 meanR:0.4406 R:0.3300 gloss:0.0495 gloss1-lgP:0.5799 gloss2gQs:0.0858 gloss3dQs:0.0858 gloss4tgtQ:0.0858 dloss:0.0001 exploreP:0.0100\n",
      "Episode:119 Steps:1000 meanR:0.4436 R:0.4600 gloss:0.0454 gloss1-lgP:0.5784 gloss2gQs:0.0789 gloss3dQs:0.0788 gloss4tgtQ:0.0788 dloss:0.0001 exploreP:0.0100\n",
      "Episode:120 Steps:1000 meanR:0.4555 R:1.1900 gloss:0.0362 gloss1-lgP:0.5798 gloss2gQs:0.0625 gloss3dQs:0.0625 gloss4tgtQ:0.0625 dloss:0.0001 exploreP:0.0100\n",
      "Episode:121 Steps:1000 meanR:0.4631 R:0.9400 gloss:0.0406 gloss1-lgP:0.5739 gloss2gQs:0.0707 gloss3dQs:0.0707 gloss4tgtQ:0.0707 dloss:0.0001 exploreP:0.0100\n",
      "Episode:122 Steps:1000 meanR:0.4734 R:1.3600 gloss:0.0406 gloss1-lgP:0.5739 gloss2gQs:0.0704 gloss3dQs:0.0704 gloss4tgtQ:0.0704 dloss:0.0001 exploreP:0.0100\n",
      "Episode:123 Steps:1000 meanR:0.4725 R:0.4300 gloss:0.0404 gloss1-lgP:0.5758 gloss2gQs:0.0702 gloss3dQs:0.0702 gloss4tgtQ:0.0701 dloss:0.0001 exploreP:0.0100\n",
      "Episode:124 Steps:1000 meanR:0.4849 R:1.7100 gloss:0.0393 gloss1-lgP:0.5797 gloss2gQs:0.0675 gloss3dQs:0.0675 gloss4tgtQ:0.0676 dloss:0.0001 exploreP:0.0100\n",
      "Episode:125 Steps:1000 meanR:0.4943 R:1.0000 gloss:0.0424 gloss1-lgP:0.5795 gloss2gQs:0.0731 gloss3dQs:0.0730 gloss4tgtQ:0.0731 dloss:0.0001 exploreP:0.0100\n",
      "Episode:126 Steps:1000 meanR:0.5027 R:0.8400 gloss:0.0444 gloss1-lgP:0.5818 gloss2gQs:0.0761 gloss3dQs:0.0761 gloss4tgtQ:0.0761 dloss:0.0001 exploreP:0.0100\n",
      "Episode:127 Steps:1000 meanR:0.5009 R:0.2500 gloss:0.0464 gloss1-lgP:0.5831 gloss2gQs:0.0794 gloss3dQs:0.0794 gloss4tgtQ:0.0794 dloss:0.0001 exploreP:0.0100\n",
      "Episode:128 Steps:1000 meanR:0.5039 R:0.4100 gloss:0.0462 gloss1-lgP:0.5823 gloss2gQs:0.0791 gloss3dQs:0.0791 gloss4tgtQ:0.0791 dloss:0.0001 exploreP:0.0100\n",
      "Episode:129 Steps:1000 meanR:0.5061 R:0.4000 gloss:0.0484 gloss1-lgP:0.5840 gloss2gQs:0.0829 gloss3dQs:0.0828 gloss4tgtQ:0.0828 dloss:0.0001 exploreP:0.0100\n",
      "Episode:130 Steps:1000 meanR:0.5072 R:0.1100 gloss:0.0475 gloss1-lgP:0.5877 gloss2gQs:0.0809 gloss3dQs:0.0809 gloss4tgtQ:0.0809 dloss:0.0001 exploreP:0.0100\n",
      "Episode:131 Steps:1000 meanR:0.5127 R:0.8000 gloss:0.0441 gloss1-lgP:0.5931 gloss2gQs:0.0747 gloss3dQs:0.0746 gloss4tgtQ:0.0746 dloss:0.0001 exploreP:0.0100\n",
      "Episode:132 Steps:1000 meanR:0.5205 R:0.7800 gloss:0.0404 gloss1-lgP:0.5946 gloss2gQs:0.0683 gloss3dQs:0.0683 gloss4tgtQ:0.0683 dloss:0.0001 exploreP:0.0100\n",
      "Episode:133 Steps:1000 meanR:0.5236 R:0.7300 gloss:0.0378 gloss1-lgP:0.5941 gloss2gQs:0.0640 gloss3dQs:0.0640 gloss4tgtQ:0.0639 dloss:0.0001 exploreP:0.0100\n",
      "Episode:134 Steps:1000 meanR:0.5121 R:0.6200 gloss:0.0356 gloss1-lgP:0.5901 gloss2gQs:0.0607 gloss3dQs:0.0607 gloss4tgtQ:0.0607 dloss:0.0001 exploreP:0.0100\n",
      "Episode:135 Steps:1000 meanR:0.5095 R:0.0200 gloss:0.0283 gloss1-lgP:0.5897 gloss2gQs:0.0483 gloss3dQs:0.0483 gloss4tgtQ:0.0483 dloss:0.0001 exploreP:0.0100\n",
      "Episode:136 Steps:1000 meanR:0.5238 R:1.5300 gloss:0.0283 gloss1-lgP:0.5889 gloss2gQs:0.0484 gloss3dQs:0.0484 gloss4tgtQ:0.0484 dloss:0.0001 exploreP:0.0100\n",
      "Episode:137 Steps:1000 meanR:0.5259 R:0.8000 gloss:0.0318 gloss1-lgP:0.5883 gloss2gQs:0.0545 gloss3dQs:0.0546 gloss4tgtQ:0.0545 dloss:0.0001 exploreP:0.0100\n",
      "Episode:138 Steps:1000 meanR:0.5298 R:0.8900 gloss:0.0321 gloss1-lgP:0.5891 gloss2gQs:0.0548 gloss3dQs:0.0548 gloss4tgtQ:0.0549 dloss:0.0001 exploreP:0.0100\n",
      "Episode:139 Steps:1000 meanR:0.5305 R:0.2700 gloss:0.0328 gloss1-lgP:0.5883 gloss2gQs:0.0560 gloss3dQs:0.0560 gloss4tgtQ:0.0560 dloss:0.0001 exploreP:0.0100\n",
      "Episode:140 Steps:1000 meanR:0.5261 R:1.0600 gloss:0.0368 gloss1-lgP:0.5862 gloss2gQs:0.0630 gloss3dQs:0.0630 gloss4tgtQ:0.0630 dloss:0.0001 exploreP:0.0100\n",
      "Episode:141 Steps:1000 meanR:0.5264 R:0.6400 gloss:0.0355 gloss1-lgP:0.5822 gloss2gQs:0.0607 gloss3dQs:0.0607 gloss4tgtQ:0.0607 dloss:0.0001 exploreP:0.0100\n",
      "Episode:142 Steps:1000 meanR:0.5316 R:0.6500 gloss:0.0378 gloss1-lgP:0.5825 gloss2gQs:0.0645 gloss3dQs:0.0645 gloss4tgtQ:0.0645 dloss:0.0001 exploreP:0.0100\n",
      "Episode:143 Steps:1000 meanR:0.5407 R:0.9100 gloss:0.0394 gloss1-lgP:0.5836 gloss2gQs:0.0671 gloss3dQs:0.0671 gloss4tgtQ:0.0671 dloss:0.0001 exploreP:0.0100\n",
      "Episode:144 Steps:1000 meanR:0.5475 R:0.9900 gloss:0.0451 gloss1-lgP:0.6412 gloss2gQs:0.0740 gloss3dQs:0.0739 gloss4tgtQ:0.0740 dloss:0.0001 exploreP:0.0100\n",
      "Episode:145 Steps:1000 meanR:0.5508 R:0.4000 gloss:0.0436 gloss1-lgP:0.5879 gloss2gQs:0.0736 gloss3dQs:0.0736 gloss4tgtQ:0.0736 dloss:0.0001 exploreP:0.0100\n",
      "Episode:146 Steps:1000 meanR:0.5486 R:0.8300 gloss:0.0411 gloss1-lgP:0.5886 gloss2gQs:0.0693 gloss3dQs:0.0694 gloss4tgtQ:0.0694 dloss:0.0001 exploreP:0.0100\n",
      "Episode:147 Steps:1000 meanR:0.5562 R:0.7600 gloss:0.0406 gloss1-lgP:0.5880 gloss2gQs:0.0687 gloss3dQs:0.0687 gloss4tgtQ:0.0687 dloss:0.0001 exploreP:0.0100\n",
      "Episode:148 Steps:1000 meanR:0.5593 R:0.3100 gloss:0.0379 gloss1-lgP:0.5892 gloss2gQs:0.0640 gloss3dQs:0.0640 gloss4tgtQ:0.0640 dloss:0.0001 exploreP:0.0100\n",
      "Episode:149 Steps:1000 meanR:0.5614 R:0.8000 gloss:0.0369 gloss1-lgP:0.5890 gloss2gQs:0.0621 gloss3dQs:0.0621 gloss4tgtQ:0.0621 dloss:0.0001 exploreP:0.0100\n",
      "Episode:150 Steps:1000 meanR:0.5699 R:0.9500 gloss:0.0385 gloss1-lgP:0.5891 gloss2gQs:0.0648 gloss3dQs:0.0648 gloss4tgtQ:0.0648 dloss:0.0001 exploreP:0.0100\n",
      "Episode:151 Steps:1000 meanR:0.5800 R:1.0100 gloss:0.0390 gloss1-lgP:0.5912 gloss2gQs:0.0655 gloss3dQs:0.0656 gloss4tgtQ:0.0656 dloss:0.0001 exploreP:0.0100\n",
      "Episode:152 Steps:1000 meanR:0.5861 R:0.6600 gloss:0.0394 gloss1-lgP:0.5918 gloss2gQs:0.0664 gloss3dQs:0.0665 gloss4tgtQ:0.0664 dloss:0.0001 exploreP:0.0100\n",
      "Episode:153 Steps:1000 meanR:0.5801 R:0.3100 gloss:0.0402 gloss1-lgP:0.5923 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:154 Steps:1000 meanR:0.5822 R:0.2100 gloss:0.0401 gloss1-lgP:0.5928 gloss2gQs:0.0677 gloss3dQs:0.0677 gloss4tgtQ:0.0677 dloss:0.0001 exploreP:0.0100\n",
      "Episode:155 Steps:1000 meanR:0.5883 R:0.6100 gloss:0.0336 gloss1-lgP:0.5914 gloss2gQs:0.0569 gloss3dQs:0.0569 gloss4tgtQ:0.0569 dloss:0.0001 exploreP:0.0100\n",
      "Episode:156 Steps:1000 meanR:0.5897 R:1.0000 gloss:0.0353 gloss1-lgP:0.5920 gloss2gQs:0.0596 gloss3dQs:0.0597 gloss4tgtQ:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:157 Steps:1000 meanR:0.5934 R:0.3700 gloss:0.0354 gloss1-lgP:0.5937 gloss2gQs:0.0597 gloss3dQs:0.0597 gloss4tgtQ:0.0597 dloss:0.0001 exploreP:0.0100\n",
      "Episode:158 Steps:1000 meanR:0.5929 R:0.4100 gloss:0.0373 gloss1-lgP:0.5929 gloss2gQs:0.0628 gloss3dQs:0.0629 gloss4tgtQ:0.0629 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:159 Steps:1000 meanR:0.5935 R:0.6200 gloss:0.0332 gloss1-lgP:0.5950 gloss2gQs:0.0560 gloss3dQs:0.0560 gloss4tgtQ:0.0560 dloss:0.0001 exploreP:0.0100\n",
      "Episode:160 Steps:1000 meanR:0.5949 R:0.1400 gloss:0.0339 gloss1-lgP:0.5950 gloss2gQs:0.0572 gloss3dQs:0.0572 gloss4tgtQ:0.0572 dloss:0.0001 exploreP:0.0100\n",
      "Episode:161 Steps:1000 meanR:0.5926 R:0.3300 gloss:0.0281 gloss1-lgP:0.5936 gloss2gQs:0.0474 gloss3dQs:0.0474 gloss4tgtQ:0.0474 dloss:0.0001 exploreP:0.0100\n",
      "Episode:162 Steps:1000 meanR:0.5877 R:0.1500 gloss:0.0247 gloss1-lgP:0.5930 gloss2gQs:0.0416 gloss3dQs:0.0416 gloss4tgtQ:0.0416 dloss:0.0000 exploreP:0.0100\n",
      "Episode:163 Steps:1000 meanR:0.5929 R:0.6100 gloss:0.0217 gloss1-lgP:0.6012 gloss2gQs:0.0366 gloss3dQs:0.0365 gloss4tgtQ:0.0365 dloss:0.0000 exploreP:0.0100\n",
      "Episode:164 Steps:1000 meanR:0.5836 R:0.7800 gloss:0.0244 gloss1-lgP:0.5941 gloss2gQs:0.0411 gloss3dQs:0.0410 gloss4tgtQ:0.0411 dloss:0.0000 exploreP:0.0100\n",
      "Episode:165 Steps:1000 meanR:0.5836 R:0.3900 gloss:0.0281 gloss1-lgP:0.5950 gloss2gQs:0.0474 gloss3dQs:0.0473 gloss4tgtQ:0.0474 dloss:0.0000 exploreP:0.0100\n",
      "Episode:166 Steps:1000 meanR:0.5941 R:1.2000 gloss:0.0280 gloss1-lgP:0.5943 gloss2gQs:0.0472 gloss3dQs:0.0472 gloss4tgtQ:0.0472 dloss:0.0000 exploreP:0.0100\n",
      "Episode:167 Steps:1000 meanR:0.6047 R:1.3300 gloss:0.0304 gloss1-lgP:0.5984 gloss2gQs:0.0508 gloss3dQs:0.0508 gloss4tgtQ:0.0508 dloss:0.0001 exploreP:0.0100\n",
      "Episode:168 Steps:1000 meanR:0.6052 R:0.5400 gloss:0.0313 gloss1-lgP:0.5957 gloss2gQs:0.0523 gloss3dQs:0.0524 gloss4tgtQ:0.0524 dloss:0.0001 exploreP:0.0100\n",
      "Episode:169 Steps:1000 meanR:0.6063 R:0.5800 gloss:0.0341 gloss1-lgP:0.5934 gloss2gQs:0.0573 gloss3dQs:0.0573 gloss4tgtQ:0.0573 dloss:0.0001 exploreP:0.0100\n",
      "Episode:170 Steps:1000 meanR:0.6128 R:0.6500 gloss:0.0322 gloss1-lgP:0.5964 gloss2gQs:0.0536 gloss3dQs:0.0536 gloss4tgtQ:0.0536 dloss:0.0001 exploreP:0.0100\n",
      "Episode:171 Steps:1000 meanR:0.6092 R:0.3400 gloss:0.8284 gloss1-lgP:1.8369 gloss2gQs:0.1252 gloss3dQs:0.0860 gloss4tgtQ:0.1247 dloss:0.0288 exploreP:0.0100\n",
      "Episode:172 Steps:1000 meanR:0.6136 R:0.7600 gloss:0.3159 gloss1-lgP:0.6581 gloss2gQs:0.4475 gloss3dQs:0.4511 gloss4tgtQ:0.4435 dloss:0.0039 exploreP:0.0100\n",
      "Episode:173 Steps:1000 meanR:0.6173 R:0.4300 gloss:0.0835 gloss1-lgP:0.6002 gloss2gQs:0.1409 gloss3dQs:0.1408 gloss4tgtQ:0.1401 dloss:0.0004 exploreP:0.0100\n",
      "Episode:174 Steps:1000 meanR:0.6289 R:1.3700 gloss:0.0520 gloss1-lgP:0.5997 gloss2gQs:0.0865 gloss3dQs:0.0866 gloss4tgtQ:0.0864 dloss:0.0002 exploreP:0.0100\n",
      "Episode:175 Steps:1000 meanR:0.6422 R:1.3300 gloss:0.0460 gloss1-lgP:0.6014 gloss2gQs:0.0766 gloss3dQs:0.0766 gloss4tgtQ:0.0765 dloss:0.0002 exploreP:0.0100\n",
      "Episode:176 Steps:1000 meanR:0.6455 R:0.4300 gloss:0.0455 gloss1-lgP:0.6004 gloss2gQs:0.0759 gloss3dQs:0.0759 gloss4tgtQ:0.0759 dloss:0.0001 exploreP:0.0100\n",
      "Episode:177 Steps:1000 meanR:0.6492 R:0.6000 gloss:0.0410 gloss1-lgP:0.5990 gloss2gQs:0.0690 gloss3dQs:0.0690 gloss4tgtQ:0.0689 dloss:0.0001 exploreP:0.0100\n",
      "Episode:178 Steps:1000 meanR:0.6552 R:0.7100 gloss:0.0393 gloss1-lgP:0.5980 gloss2gQs:0.0663 gloss3dQs:0.0662 gloss4tgtQ:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:179 Steps:1000 meanR:0.6503 R:0.0900 gloss:0.0360 gloss1-lgP:0.5986 gloss2gQs:0.0602 gloss3dQs:0.0602 gloss4tgtQ:0.0602 dloss:0.0001 exploreP:0.0100\n",
      "Episode:180 Steps:1000 meanR:0.6481 R:1.0500 gloss:0.0360 gloss1-lgP:0.5960 gloss2gQs:0.0605 gloss3dQs:0.0605 gloss4tgtQ:0.0605 dloss:0.0001 exploreP:0.0100\n",
      "Episode:181 Steps:1000 meanR:0.6455 R:0.4800 gloss:0.0390 gloss1-lgP:0.5978 gloss2gQs:0.0652 gloss3dQs:0.0652 gloss4tgtQ:0.0652 dloss:0.0001 exploreP:0.0100\n",
      "Episode:182 Steps:1000 meanR:0.6322 R:0.5900 gloss:0.0406 gloss1-lgP:0.6023 gloss2gQs:0.0674 gloss3dQs:0.0674 gloss4tgtQ:0.0674 dloss:0.0001 exploreP:0.0100\n",
      "Episode:183 Steps:1000 meanR:0.6411 R:0.9800 gloss:0.0415 gloss1-lgP:0.6041 gloss2gQs:0.0687 gloss3dQs:0.0687 gloss4tgtQ:0.0688 dloss:0.0001 exploreP:0.0100\n",
      "Episode:184 Steps:1000 meanR:0.6453 R:0.4200 gloss:0.0403 gloss1-lgP:0.6050 gloss2gQs:0.0666 gloss3dQs:0.0666 gloss4tgtQ:0.0666 dloss:0.0001 exploreP:0.0100\n",
      "Episode:185 Steps:1000 meanR:0.6498 R:0.6900 gloss:0.0340 gloss1-lgP:0.6056 gloss2gQs:0.0561 gloss3dQs:0.0561 gloss4tgtQ:0.0561 dloss:0.0001 exploreP:0.0100\n",
      "Episode:186 Steps:1000 meanR:0.6625 R:1.2700 gloss:0.0329 gloss1-lgP:0.6047 gloss2gQs:0.0543 gloss3dQs:0.0543 gloss4tgtQ:0.0543 dloss:0.0001 exploreP:0.0100\n",
      "Episode:187 Steps:1000 meanR:0.6647 R:0.7500 gloss:0.0380 gloss1-lgP:0.6033 gloss2gQs:0.0626 gloss3dQs:0.0626 gloss4tgtQ:0.0626 dloss:0.0001 exploreP:0.0100\n",
      "Episode:188 Steps:1000 meanR:0.6606 R:0.1500 gloss:0.0363 gloss1-lgP:0.6053 gloss2gQs:0.0594 gloss3dQs:0.0594 gloss4tgtQ:0.0594 dloss:0.0001 exploreP:0.0100\n",
      "Episode:189 Steps:1000 meanR:0.6624 R:0.3300 gloss:0.0374 gloss1-lgP:0.6081 gloss2gQs:0.0611 gloss3dQs:0.0611 gloss4tgtQ:0.0611 dloss:0.0001 exploreP:0.0100\n",
      "Episode:190 Steps:1000 meanR:0.6734 R:1.1000 gloss:0.0357 gloss1-lgP:0.6097 gloss2gQs:0.0582 gloss3dQs:0.0582 gloss4tgtQ:0.0582 dloss:0.0001 exploreP:0.0100\n",
      "Episode:191 Steps:1000 meanR:0.6788 R:1.3700 gloss:0.0380 gloss1-lgP:0.6076 gloss2gQs:0.0622 gloss3dQs:0.0623 gloss4tgtQ:0.0623 dloss:0.0001 exploreP:0.0100\n",
      "Episode:192 Steps:1000 meanR:0.6838 R:0.9400 gloss:0.0401 gloss1-lgP:0.6067 gloss2gQs:0.0658 gloss3dQs:0.0658 gloss4tgtQ:0.0658 dloss:0.0001 exploreP:0.0100\n",
      "Episode:193 Steps:1000 meanR:0.6890 R:0.6000 gloss:0.0412 gloss1-lgP:0.6056 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:194 Steps:1000 meanR:0.6905 R:0.7200 gloss:0.0435 gloss1-lgP:0.6050 gloss2gQs:0.0718 gloss3dQs:0.0718 gloss4tgtQ:0.0718 dloss:0.0001 exploreP:0.0100\n",
      "Episode:195 Steps:1000 meanR:0.7019 R:1.1400 gloss:0.0430 gloss1-lgP:0.6057 gloss2gQs:0.0708 gloss3dQs:0.0708 gloss4tgtQ:0.0708 dloss:0.0001 exploreP:0.0100\n",
      "Episode:196 Steps:1000 meanR:0.7098 R:0.7900 gloss:0.0472 gloss1-lgP:0.6084 gloss2gQs:0.0773 gloss3dQs:0.0773 gloss4tgtQ:0.0773 dloss:0.0001 exploreP:0.0100\n",
      "Episode:197 Steps:1000 meanR:0.7162 R:0.9900 gloss:0.0430 gloss1-lgP:0.6089 gloss2gQs:0.0704 gloss3dQs:0.0704 gloss4tgtQ:0.0704 dloss:0.0001 exploreP:0.0100\n",
      "Episode:198 Steps:1000 meanR:0.7129 R:0.2700 gloss:0.0437 gloss1-lgP:0.6090 gloss2gQs:0.0715 gloss3dQs:0.0715 gloss4tgtQ:0.0716 dloss:0.0001 exploreP:0.0100\n",
      "Episode:199 Steps:1000 meanR:0.7135 R:0.7700 gloss:0.0501 gloss1-lgP:0.6090 gloss2gQs:0.0822 gloss3dQs:0.0822 gloss4tgtQ:0.0822 dloss:0.0001 exploreP:0.0100\n",
      "Episode:200 Steps:1000 meanR:0.7210 R:0.7500 gloss:0.0477 gloss1-lgP:0.6106 gloss2gQs:0.0780 gloss3dQs:0.0779 gloss4tgtQ:0.0780 dloss:0.0001 exploreP:0.0100\n",
      "Episode:201 Steps:1000 meanR:0.7234 R:0.7900 gloss:0.0480 gloss1-lgP:0.6112 gloss2gQs:0.0784 gloss3dQs:0.0784 gloss4tgtQ:0.0784 dloss:0.0001 exploreP:0.0100\n",
      "Episode:202 Steps:1000 meanR:0.7269 R:0.3500 gloss:0.0430 gloss1-lgP:0.6131 gloss2gQs:0.0700 gloss3dQs:0.0700 gloss4tgtQ:0.0699 dloss:0.0001 exploreP:0.0100\n",
      "Episode:203 Steps:1000 meanR:0.7312 R:0.7400 gloss:0.0407 gloss1-lgP:0.6145 gloss2gQs:0.0661 gloss3dQs:0.0661 gloss4tgtQ:0.0661 dloss:0.0001 exploreP:0.0100\n",
      "Episode:204 Steps:1000 meanR:0.7288 R:0.3100 gloss:0.0409 gloss1-lgP:0.6130 gloss2gQs:0.0663 gloss3dQs:0.0663 gloss4tgtQ:0.0663 dloss:0.0001 exploreP:0.0100\n",
      "Episode:205 Steps:1000 meanR:0.7251 R:0.6800 gloss:0.1774 gloss1-lgP:0.7631 gloss2gQs:0.2222 gloss3dQs:0.2183 gloss4tgtQ:0.2206 dloss:0.0013 exploreP:0.0100\n",
      "Episode:206 Steps:1000 meanR:0.7231 R:0.5600 gloss:0.0466 gloss1-lgP:0.6131 gloss2gQs:0.0757 gloss3dQs:0.0757 gloss4tgtQ:0.0756 dloss:0.0001 exploreP:0.0100\n",
      "Episode:207 Steps:1000 meanR:0.7182 R:0.6300 gloss:0.0374 gloss1-lgP:0.6111 gloss2gQs:0.0611 gloss3dQs:0.0610 gloss4tgtQ:0.0610 dloss:0.0001 exploreP:0.0100\n",
      "Episode:208 Steps:1000 meanR:0.7173 R:1.2100 gloss:0.0363 gloss1-lgP:0.6106 gloss2gQs:0.0594 gloss3dQs:0.0593 gloss4tgtQ:0.0593 dloss:0.0001 exploreP:0.0100\n",
      "Episode:209 Steps:1000 meanR:0.7170 R:1.1900 gloss:0.0381 gloss1-lgP:0.6107 gloss2gQs:0.0624 gloss3dQs:0.0623 gloss4tgtQ:0.0624 dloss:0.0001 exploreP:0.0100\n",
      "Episode:210 Steps:1000 meanR:0.7075 R:0.4200 gloss:0.0417 gloss1-lgP:0.6108 gloss2gQs:0.0683 gloss3dQs:0.0683 gloss4tgtQ:0.0683 dloss:0.0001 exploreP:0.0100\n",
      "Episode:211 Steps:1000 meanR:0.7036 R:0.6300 gloss:0.0391 gloss1-lgP:0.6090 gloss2gQs:0.0642 gloss3dQs:0.0642 gloss4tgtQ:0.0642 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:212 Steps:1000 meanR:0.7006 R:0.7600 gloss:0.0377 gloss1-lgP:0.6089 gloss2gQs:0.0620 gloss3dQs:0.0620 gloss4tgtQ:0.0620 dloss:0.0001 exploreP:0.0100\n",
      "Episode:213 Steps:1000 meanR:0.7006 R:0.9700 gloss:0.0402 gloss1-lgP:0.6090 gloss2gQs:0.0662 gloss3dQs:0.0662 gloss4tgtQ:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:214 Steps:1000 meanR:0.6955 R:0.4700 gloss:0.0423 gloss1-lgP:0.6103 gloss2gQs:0.0697 gloss3dQs:0.0697 gloss4tgtQ:0.0697 dloss:0.0001 exploreP:0.0100\n",
      "Episode:215 Steps:1000 meanR:0.6926 R:0.3800 gloss:0.0388 gloss1-lgP:0.6112 gloss2gQs:0.0634 gloss3dQs:0.0634 gloss4tgtQ:0.0634 dloss:0.0001 exploreP:0.0100\n",
      "Episode:216 Steps:1000 meanR:0.6961 R:0.6900 gloss:0.0387 gloss1-lgP:0.6097 gloss2gQs:0.0636 gloss3dQs:0.0636 gloss4tgtQ:0.0635 dloss:0.0001 exploreP:0.0100\n",
      "Episode:217 Steps:1000 meanR:0.6906 R:0.0000 gloss:0.0389 gloss1-lgP:0.6080 gloss2gQs:0.0638 gloss3dQs:0.0638 gloss4tgtQ:0.0639 dloss:0.0001 exploreP:0.0100\n",
      "Episode:218 Steps:1000 meanR:0.6933 R:0.6000 gloss:0.0349 gloss1-lgP:0.6081 gloss2gQs:0.0571 gloss3dQs:0.0572 gloss4tgtQ:0.0572 dloss:0.0001 exploreP:0.0100\n",
      "Episode:219 Steps:1000 meanR:0.7013 R:1.2600 gloss:0.0387 gloss1-lgP:0.6081 gloss2gQs:0.0635 gloss3dQs:0.0635 gloss4tgtQ:0.0635 dloss:0.0001 exploreP:0.0100\n",
      "Episode:220 Steps:1000 meanR:0.6925 R:0.3100 gloss:0.0340 gloss1-lgP:0.6076 gloss2gQs:0.0558 gloss3dQs:0.0558 gloss4tgtQ:0.0558 dloss:0.0001 exploreP:0.0100\n",
      "Episode:221 Steps:1000 meanR:0.6880 R:0.4900 gloss:0.0365 gloss1-lgP:0.6105 gloss2gQs:0.0597 gloss3dQs:0.0597 gloss4tgtQ:0.0597 dloss:0.0001 exploreP:0.0100\n",
      "Episode:222 Steps:1000 meanR:0.6800 R:0.5600 gloss:0.0293 gloss1-lgP:0.6109 gloss2gQs:0.0478 gloss3dQs:0.0478 gloss4tgtQ:0.0477 dloss:0.0001 exploreP:0.0100\n",
      "Episode:223 Steps:1000 meanR:0.6855 R:0.9800 gloss:0.0321 gloss1-lgP:0.6102 gloss2gQs:0.0523 gloss3dQs:0.0523 gloss4tgtQ:0.0523 dloss:0.0001 exploreP:0.0100\n",
      "Episode:224 Steps:1000 meanR:0.6724 R:0.4000 gloss:0.0312 gloss1-lgP:0.6105 gloss2gQs:0.0508 gloss3dQs:0.0509 gloss4tgtQ:0.0508 dloss:0.0001 exploreP:0.0100\n",
      "Episode:225 Steps:1000 meanR:0.6682 R:0.5800 gloss:0.0323 gloss1-lgP:0.6124 gloss2gQs:0.0524 gloss3dQs:0.0524 gloss4tgtQ:0.0524 dloss:0.0001 exploreP:0.0100\n",
      "Episode:226 Steps:1000 meanR:0.6615 R:0.1700 gloss:0.0331 gloss1-lgP:0.6157 gloss2gQs:0.0534 gloss3dQs:0.0534 gloss4tgtQ:0.0534 dloss:0.0001 exploreP:0.0100\n",
      "Episode:227 Steps:1000 meanR:0.6638 R:0.4800 gloss:0.0317 gloss1-lgP:0.6201 gloss2gQs:0.0510 gloss3dQs:0.0510 gloss4tgtQ:0.0510 dloss:0.0001 exploreP:0.0100\n",
      "Episode:228 Steps:1000 meanR:0.6730 R:1.3300 gloss:0.0336 gloss1-lgP:0.6222 gloss2gQs:0.0541 gloss3dQs:0.0541 gloss4tgtQ:0.0541 dloss:0.0001 exploreP:0.0100\n",
      "Episode:229 Steps:1000 meanR:0.6775 R:0.8500 gloss:0.0341 gloss1-lgP:0.6223 gloss2gQs:0.0548 gloss3dQs:0.0548 gloss4tgtQ:0.0548 dloss:0.0001 exploreP:0.0100\n",
      "Episode:230 Steps:1000 meanR:0.6846 R:0.8200 gloss:0.0368 gloss1-lgP:0.6492 gloss2gQs:0.0577 gloss3dQs:0.0577 gloss4tgtQ:0.0577 dloss:0.0001 exploreP:0.0100\n",
      "Episode:231 Steps:1000 meanR:0.6881 R:1.1500 gloss:0.0409 gloss1-lgP:0.6202 gloss2gQs:0.0659 gloss3dQs:0.0658 gloss4tgtQ:0.0659 dloss:0.0001 exploreP:0.0100\n",
      "Episode:232 Steps:1000 meanR:0.6910 R:1.0700 gloss:0.0423 gloss1-lgP:0.6205 gloss2gQs:0.0678 gloss3dQs:0.0678 gloss4tgtQ:0.0678 dloss:0.0001 exploreP:0.0100\n",
      "Episode:233 Steps:1000 meanR:0.6862 R:0.2500 gloss:0.0449 gloss1-lgP:0.6225 gloss2gQs:0.0718 gloss3dQs:0.0718 gloss4tgtQ:0.0718 dloss:0.0001 exploreP:0.0100\n",
      "Episode:234 Steps:1000 meanR:0.6841 R:0.4100 gloss:0.0404 gloss1-lgP:0.6235 gloss2gQs:0.0645 gloss3dQs:0.0645 gloss4tgtQ:0.0645 dloss:0.0001 exploreP:0.0100\n",
      "Episode:235 Steps:1000 meanR:0.6900 R:0.6100 gloss:0.0400 gloss1-lgP:0.6209 gloss2gQs:0.0641 gloss3dQs:0.0641 gloss4tgtQ:0.0641 dloss:0.0001 exploreP:0.0100\n",
      "Episode:236 Steps:1000 meanR:0.6750 R:0.0300 gloss:0.0403 gloss1-lgP:0.6179 gloss2gQs:0.0649 gloss3dQs:0.0649 gloss4tgtQ:0.0649 dloss:0.0001 exploreP:0.0100\n",
      "Episode:237 Steps:1000 meanR:0.6738 R:0.6800 gloss:0.0402 gloss1-lgP:0.6161 gloss2gQs:0.0649 gloss3dQs:0.0649 gloss4tgtQ:0.0649 dloss:0.0001 exploreP:0.0100\n",
      "Episode:238 Steps:1000 meanR:0.6737 R:0.8800 gloss:0.0410 gloss1-lgP:0.6155 gloss2gQs:0.0662 gloss3dQs:0.0663 gloss4tgtQ:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:239 Steps:1000 meanR:0.6750 R:0.4000 gloss:0.0369 gloss1-lgP:0.6165 gloss2gQs:0.0596 gloss3dQs:0.0596 gloss4tgtQ:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:240 Steps:1000 meanR:0.6644 R:0.0000 gloss:0.0361 gloss1-lgP:0.6188 gloss2gQs:0.0582 gloss3dQs:0.0582 gloss4tgtQ:0.0582 dloss:0.0001 exploreP:0.0100\n",
      "Episode:241 Steps:1000 meanR:0.6632 R:0.5200 gloss:0.0304 gloss1-lgP:0.6211 gloss2gQs:0.0488 gloss3dQs:0.0488 gloss4tgtQ:0.0488 dloss:0.0001 exploreP:0.0100\n",
      "Episode:242 Steps:1000 meanR:0.6644 R:0.7700 gloss:0.0296 gloss1-lgP:0.6206 gloss2gQs:0.0477 gloss3dQs:0.0477 gloss4tgtQ:0.0477 dloss:0.0001 exploreP:0.0100\n",
      "Episode:243 Steps:1000 meanR:0.6583 R:0.3000 gloss:0.0263 gloss1-lgP:0.6208 gloss2gQs:0.0425 gloss3dQs:0.0425 gloss4tgtQ:0.0425 dloss:0.0000 exploreP:0.0100\n",
      "Episode:244 Steps:1000 meanR:0.6500 R:0.1600 gloss:0.0272 gloss1-lgP:0.6206 gloss2gQs:0.0439 gloss3dQs:0.0439 gloss4tgtQ:0.0439 dloss:0.0001 exploreP:0.0100\n",
      "Episode:245 Steps:1000 meanR:0.6530 R:0.7000 gloss:0.0270 gloss1-lgP:0.6203 gloss2gQs:0.0435 gloss3dQs:0.0435 gloss4tgtQ:0.0435 dloss:0.0000 exploreP:0.0100\n",
      "Episode:246 Steps:1000 meanR:0.6542 R:0.9500 gloss:0.0292 gloss1-lgP:0.6205 gloss2gQs:0.0471 gloss3dQs:0.0471 gloss4tgtQ:0.0471 dloss:0.0001 exploreP:0.0100\n",
      "Episode:247 Steps:1000 meanR:0.6531 R:0.6500 gloss:0.0304 gloss1-lgP:0.6211 gloss2gQs:0.0489 gloss3dQs:0.0489 gloss4tgtQ:0.0489 dloss:0.0000 exploreP:0.0100\n",
      "Episode:248 Steps:1000 meanR:0.6554 R:0.5400 gloss:0.0294 gloss1-lgP:0.6216 gloss2gQs:0.0472 gloss3dQs:0.0472 gloss4tgtQ:0.0473 dloss:0.0001 exploreP:0.0100\n",
      "Episode:249 Steps:1000 meanR:0.6537 R:0.6300 gloss:0.0290 gloss1-lgP:0.6212 gloss2gQs:0.0468 gloss3dQs:0.0468 gloss4tgtQ:0.0469 dloss:0.0001 exploreP:0.0100\n",
      "Episode:250 Steps:1000 meanR:0.6514 R:0.7200 gloss:0.0301 gloss1-lgP:0.6197 gloss2gQs:0.0487 gloss3dQs:0.0487 gloss4tgtQ:0.0487 dloss:0.0000 exploreP:0.0100\n",
      "Episode:251 Steps:1000 meanR:0.6450 R:0.3700 gloss:0.0305 gloss1-lgP:0.6171 gloss2gQs:0.0496 gloss3dQs:0.0496 gloss4tgtQ:0.0496 dloss:0.0001 exploreP:0.0100\n",
      "Episode:252 Steps:1000 meanR:0.6493 R:1.0900 gloss:0.0333 gloss1-lgP:0.6164 gloss2gQs:0.0542 gloss3dQs:0.0542 gloss4tgtQ:0.0542 dloss:0.0001 exploreP:0.0100\n",
      "Episode:253 Steps:1000 meanR:0.6499 R:0.3700 gloss:0.0330 gloss1-lgP:0.6157 gloss2gQs:0.0538 gloss3dQs:0.0538 gloss4tgtQ:0.0538 dloss:0.0001 exploreP:0.0100\n",
      "Episode:254 Steps:1000 meanR:0.6502 R:0.2400 gloss:0.0332 gloss1-lgP:0.6144 gloss2gQs:0.0541 gloss3dQs:0.0541 gloss4tgtQ:0.0541 dloss:0.0001 exploreP:0.0100\n",
      "Episode:255 Steps:1000 meanR:0.6609 R:1.6800 gloss:0.0364 gloss1-lgP:0.6162 gloss2gQs:0.0591 gloss3dQs:0.0591 gloss4tgtQ:0.0591 dloss:0.0001 exploreP:0.0100\n",
      "Episode:256 Steps:1000 meanR:0.6556 R:0.4700 gloss:0.0364 gloss1-lgP:0.6179 gloss2gQs:0.0590 gloss3dQs:0.0590 gloss4tgtQ:0.0590 dloss:0.0001 exploreP:0.0100\n",
      "Episode:257 Steps:1000 meanR:0.6628 R:1.0900 gloss:0.0385 gloss1-lgP:0.6176 gloss2gQs:0.0625 gloss3dQs:0.0625 gloss4tgtQ:0.0625 dloss:0.0001 exploreP:0.0100\n",
      "Episode:258 Steps:1000 meanR:0.6635 R:0.4800 gloss:0.0427 gloss1-lgP:0.6177 gloss2gQs:0.0693 gloss3dQs:0.0693 gloss4tgtQ:0.0693 dloss:0.0001 exploreP:0.0100\n",
      "Episode:259 Steps:1000 meanR:0.6617 R:0.4400 gloss:0.0377 gloss1-lgP:0.6186 gloss2gQs:0.0608 gloss3dQs:0.0609 gloss4tgtQ:0.0609 dloss:0.0001 exploreP:0.0100\n",
      "Episode:260 Steps:1000 meanR:0.6654 R:0.5100 gloss:0.0387 gloss1-lgP:0.6180 gloss2gQs:0.0624 gloss3dQs:0.0624 gloss4tgtQ:0.0624 dloss:0.0001 exploreP:0.0100\n",
      "Episode:261 Steps:1000 meanR:0.6666 R:0.4500 gloss:0.0421 gloss1-lgP:0.6189 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:262 Steps:1000 meanR:0.6802 R:1.5100 gloss:0.0401 gloss1-lgP:0.6192 gloss2gQs:0.0647 gloss3dQs:0.0647 gloss4tgtQ:0.0647 dloss:0.0001 exploreP:0.0100\n",
      "Episode:263 Steps:1000 meanR:0.6802 R:0.6100 gloss:0.0425 gloss1-lgP:0.6201 gloss2gQs:0.0684 gloss3dQs:0.0684 gloss4tgtQ:0.0684 dloss:0.0001 exploreP:0.0100\n",
      "Episode:264 Steps:1000 meanR:0.6839 R:1.1500 gloss:0.0472 gloss1-lgP:0.6218 gloss2gQs:0.0758 gloss3dQs:0.0758 gloss4tgtQ:0.0758 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:265 Steps:1000 meanR:0.6839 R:0.3900 gloss:0.0418 gloss1-lgP:0.6234 gloss2gQs:0.0670 gloss3dQs:0.0670 gloss4tgtQ:0.0670 dloss:0.0001 exploreP:0.0100\n",
      "Episode:266 Steps:1000 meanR:0.6838 R:1.1900 gloss:0.0405 gloss1-lgP:0.6235 gloss2gQs:0.0649 gloss3dQs:0.0649 gloss4tgtQ:0.0649 dloss:0.0001 exploreP:0.0100\n",
      "Episode:267 Steps:1000 meanR:0.6764 R:0.5900 gloss:0.0427 gloss1-lgP:0.6242 gloss2gQs:0.0683 gloss3dQs:0.0683 gloss4tgtQ:0.0683 dloss:0.0001 exploreP:0.0100\n",
      "Episode:268 Steps:1000 meanR:0.6783 R:0.7300 gloss:0.0383 gloss1-lgP:0.6255 gloss2gQs:0.0610 gloss3dQs:0.0610 gloss4tgtQ:0.0610 dloss:0.0001 exploreP:0.0100\n",
      "Episode:269 Steps:1000 meanR:0.6758 R:0.3300 gloss:0.0396 gloss1-lgP:0.6262 gloss2gQs:0.0631 gloss3dQs:0.0631 gloss4tgtQ:0.0631 dloss:0.0001 exploreP:0.0100\n",
      "Episode:270 Steps:1000 meanR:0.6751 R:0.5800 gloss:0.0427 gloss1-lgP:0.6279 gloss2gQs:0.0681 gloss3dQs:0.0681 gloss4tgtQ:0.0681 dloss:0.0001 exploreP:0.0100\n",
      "Episode:271 Steps:1000 meanR:0.6818 R:1.0100 gloss:0.0412 gloss1-lgP:0.6287 gloss2gQs:0.0654 gloss3dQs:0.0655 gloss4tgtQ:0.0655 dloss:0.0001 exploreP:0.0100\n",
      "Episode:272 Steps:1000 meanR:0.6838 R:0.9600 gloss:0.0415 gloss1-lgP:0.6294 gloss2gQs:0.0660 gloss3dQs:0.0660 gloss4tgtQ:0.0660 dloss:0.0001 exploreP:0.0100\n",
      "Episode:273 Steps:1000 meanR:0.6847 R:0.5200 gloss:0.0408 gloss1-lgP:0.6294 gloss2gQs:0.0646 gloss3dQs:0.0646 gloss4tgtQ:0.0646 dloss:0.0001 exploreP:0.0100\n",
      "Episode:274 Steps:1000 meanR:0.6731 R:0.2100 gloss:0.0376 gloss1-lgP:0.6286 gloss2gQs:0.0596 gloss3dQs:0.0596 gloss4tgtQ:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:275 Steps:1000 meanR:0.6664 R:0.6600 gloss:0.0391 gloss1-lgP:0.6287 gloss2gQs:0.0620 gloss3dQs:0.0620 gloss4tgtQ:0.0620 dloss:0.0001 exploreP:0.0100\n",
      "Episode:276 Steps:1000 meanR:0.6717 R:0.9600 gloss:0.0350 gloss1-lgP:0.6286 gloss2gQs:0.0555 gloss3dQs:0.0556 gloss4tgtQ:0.0556 dloss:0.0001 exploreP:0.0100\n",
      "Episode:277 Steps:1000 meanR:0.6691 R:0.3400 gloss:0.0368 gloss1-lgP:0.6291 gloss2gQs:0.0583 gloss3dQs:0.0583 gloss4tgtQ:0.0583 dloss:0.0001 exploreP:0.0100\n",
      "Episode:278 Steps:1000 meanR:0.6620 R:0.0000 gloss:0.0365 gloss1-lgP:0.6272 gloss2gQs:0.0580 gloss3dQs:0.0580 gloss4tgtQ:0.0580 dloss:0.0001 exploreP:0.0100\n",
      "Episode:279 Steps:1000 meanR:0.6670 R:0.5900 gloss:0.3559 gloss1-lgP:0.8193 gloss2gQs:0.3605 gloss3dQs:0.3442 gloss4tgtQ:0.3570 dloss:0.0126 exploreP:0.0100\n",
      "Episode:280 Steps:1000 meanR:0.6647 R:0.8200 gloss:0.2319 gloss1-lgP:0.6283 gloss2gQs:0.3730 gloss3dQs:0.3731 gloss4tgtQ:0.3696 dloss:0.0016 exploreP:0.0100\n",
      "Episode:281 Steps:1000 meanR:0.6688 R:0.8900 gloss:0.0569 gloss1-lgP:0.6271 gloss2gQs:0.0905 gloss3dQs:0.0906 gloss4tgtQ:0.0900 dloss:0.0004 exploreP:0.0100\n",
      "Episode:282 Steps:1000 meanR:0.6671 R:0.4200 gloss:0.0350 gloss1-lgP:0.6263 gloss2gQs:0.0554 gloss3dQs:0.0555 gloss4tgtQ:0.0554 dloss:0.0002 exploreP:0.0100\n",
      "Episode:283 Steps:1000 meanR:0.6589 R:0.1600 gloss:0.0312 gloss1-lgP:0.6241 gloss2gQs:0.0498 gloss3dQs:0.0499 gloss4tgtQ:0.0497 dloss:0.0001 exploreP:0.0100\n",
      "Episode:284 Steps:1000 meanR:0.6578 R:0.3100 gloss:0.0317 gloss1-lgP:0.6231 gloss2gQs:0.0506 gloss3dQs:0.0506 gloss4tgtQ:0.0506 dloss:0.0001 exploreP:0.0100\n",
      "Episode:285 Steps:1000 meanR:0.6530 R:0.2100 gloss:0.0302 gloss1-lgP:0.6217 gloss2gQs:0.0484 gloss3dQs:0.0485 gloss4tgtQ:0.0484 dloss:0.0001 exploreP:0.0100\n",
      "Episode:286 Steps:1000 meanR:0.6485 R:0.8200 gloss:0.0316 gloss1-lgP:0.6303 gloss2gQs:0.0507 gloss3dQs:0.0506 gloss4tgtQ:0.0506 dloss:0.0001 exploreP:0.0100\n",
      "Episode:287 Steps:1000 meanR:0.6491 R:0.8100 gloss:0.0268 gloss1-lgP:0.6199 gloss2gQs:0.0431 gloss3dQs:0.0430 gloss4tgtQ:0.0430 dloss:0.0001 exploreP:0.0100\n",
      "Episode:288 Steps:1000 meanR:0.6565 R:0.8900 gloss:1.7471 gloss1-lgP:1.2012 gloss2gQs:1.4999 gloss3dQs:1.4259 gloss4tgtQ:1.4852 dloss:0.1014 exploreP:0.0100\n",
      "Episode:289 Steps:1000 meanR:0.6627 R:0.9500 gloss:0.5842 gloss1-lgP:0.6213 gloss2gQs:0.9648 gloss3dQs:0.9669 gloss4tgtQ:0.9551 dloss:0.0060 exploreP:0.0100\n",
      "Episode:290 Steps:1000 meanR:0.6592 R:0.7500 gloss:0.0926 gloss1-lgP:0.6236 gloss2gQs:0.1522 gloss3dQs:0.1524 gloss4tgtQ:0.1511 dloss:0.0015 exploreP:0.0100\n",
      "Episode:291 Steps:1000 meanR:0.6498 R:0.4300 gloss:0.0309 gloss1-lgP:0.6223 gloss2gQs:0.0491 gloss3dQs:0.0491 gloss4tgtQ:0.0493 dloss:0.0009 exploreP:0.0100\n",
      "Episode:292 Steps:1000 meanR:0.6501 R:0.9700 gloss:0.0328 gloss1-lgP:0.6214 gloss2gQs:0.0522 gloss3dQs:0.0523 gloss4tgtQ:0.0524 dloss:0.0005 exploreP:0.0100\n",
      "Episode:293 Steps:1000 meanR:0.6459 R:0.1800 gloss:0.0323 gloss1-lgP:0.6240 gloss2gQs:0.0510 gloss3dQs:0.0510 gloss4tgtQ:0.0510 dloss:0.0003 exploreP:0.0100\n",
      "Episode:294 Steps:1000 meanR:0.6499 R:1.1200 gloss:0.0315 gloss1-lgP:0.6248 gloss2gQs:0.0495 gloss3dQs:0.0496 gloss4tgtQ:0.0496 dloss:0.0002 exploreP:0.0100\n",
      "Episode:295 Steps:1000 meanR:0.6420 R:0.3500 gloss:0.0398 gloss1-lgP:0.6236 gloss2gQs:0.0636 gloss3dQs:0.0636 gloss4tgtQ:0.0637 dloss:0.0002 exploreP:0.0100\n",
      "Episode:296 Steps:1000 meanR:0.6486 R:1.4500 gloss:0.0376 gloss1-lgP:0.6222 gloss2gQs:0.0604 gloss3dQs:0.0604 gloss4tgtQ:0.0605 dloss:0.0002 exploreP:0.0100\n",
      "Episode:297 Steps:1000 meanR:0.6402 R:0.1500 gloss:0.0414 gloss1-lgP:0.6243 gloss2gQs:0.0663 gloss3dQs:0.0663 gloss4tgtQ:0.0663 dloss:0.0001 exploreP:0.0100\n",
      "Episode:298 Steps:1000 meanR:0.6436 R:0.6100 gloss:0.0424 gloss1-lgP:0.6252 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:299 Steps:1000 meanR:0.6508 R:1.4900 gloss:0.0447 gloss1-lgP:0.6257 gloss2gQs:0.0716 gloss3dQs:0.0716 gloss4tgtQ:0.0715 dloss:0.0001 exploreP:0.0100\n",
      "Episode:300 Steps:1000 meanR:0.6440 R:0.0700 gloss:0.0410 gloss1-lgP:0.6236 gloss2gQs:0.0658 gloss3dQs:0.0658 gloss4tgtQ:0.0658 dloss:0.0001 exploreP:0.0100\n",
      "Episode:301 Steps:1000 meanR:0.6420 R:0.5900 gloss:0.0423 gloss1-lgP:0.6242 gloss2gQs:0.0677 gloss3dQs:0.0678 gloss4tgtQ:0.0677 dloss:0.0001 exploreP:0.0100\n",
      "Episode:302 Steps:1000 meanR:0.6470 R:0.8500 gloss:0.0483 gloss1-lgP:0.6263 gloss2gQs:0.0776 gloss3dQs:0.0775 gloss4tgtQ:0.0776 dloss:0.0001 exploreP:0.0100\n",
      "Episode:303 Steps:1000 meanR:0.6423 R:0.2700 gloss:0.2170 gloss1-lgP:0.7568 gloss2gQs:0.2465 gloss3dQs:0.2416 gloss4tgtQ:0.2447 dloss:0.0032 exploreP:0.0100\n",
      "Episode:304 Steps:1000 meanR:0.6430 R:0.3800 gloss:0.1094 gloss1-lgP:0.6233 gloss2gQs:0.1783 gloss3dQs:0.1783 gloss4tgtQ:0.1771 dloss:0.0005 exploreP:0.0100\n",
      "Episode:305 Steps:1000 meanR:0.6419 R:0.5700 gloss:0.0362 gloss1-lgP:0.6222 gloss2gQs:0.0584 gloss3dQs:0.0585 gloss4tgtQ:0.0584 dloss:0.0001 exploreP:0.0100\n",
      "Episode:306 Steps:1000 meanR:0.6451 R:0.8800 gloss:0.0357 gloss1-lgP:0.6249 gloss2gQs:0.0573 gloss3dQs:0.0573 gloss4tgtQ:0.0573 dloss:0.0001 exploreP:0.0100\n",
      "Episode:307 Steps:1000 meanR:0.6510 R:1.2200 gloss:0.0372 gloss1-lgP:0.6252 gloss2gQs:0.0596 gloss3dQs:0.0596 gloss4tgtQ:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:308 Steps:1000 meanR:0.6424 R:0.3500 gloss:0.0392 gloss1-lgP:0.6261 gloss2gQs:0.0626 gloss3dQs:0.0626 gloss4tgtQ:0.0626 dloss:0.0001 exploreP:0.0100\n",
      "Episode:309 Steps:1000 meanR:0.6317 R:0.1200 gloss:0.0318 gloss1-lgP:0.6263 gloss2gQs:0.0506 gloss3dQs:0.0506 gloss4tgtQ:0.0506 dloss:0.0001 exploreP:0.0100\n",
      "Episode:310 Steps:1000 meanR:0.6362 R:0.8700 gloss:0.0364 gloss1-lgP:0.6273 gloss2gQs:0.0580 gloss3dQs:0.0579 gloss4tgtQ:0.0580 dloss:0.0001 exploreP:0.0100\n",
      "Episode:311 Steps:1000 meanR:0.6354 R:0.5500 gloss:0.0372 gloss1-lgP:0.6249 gloss2gQs:0.0594 gloss3dQs:0.0594 gloss4tgtQ:0.0594 dloss:0.0001 exploreP:0.0100\n",
      "Episode:312 Steps:1000 meanR:0.6342 R:0.6400 gloss:0.0339 gloss1-lgP:0.6252 gloss2gQs:0.0540 gloss3dQs:0.0540 gloss4tgtQ:0.0540 dloss:0.0001 exploreP:0.0100\n",
      "Episode:313 Steps:1000 meanR:0.6277 R:0.3200 gloss:0.0310 gloss1-lgP:0.6432 gloss2gQs:0.0495 gloss3dQs:0.0495 gloss4tgtQ:0.0495 dloss:0.0001 exploreP:0.0100\n",
      "Episode:314 Steps:1000 meanR:0.6340 R:1.1000 gloss:0.0358 gloss1-lgP:0.6362 gloss2gQs:0.0567 gloss3dQs:0.0567 gloss4tgtQ:0.0567 dloss:0.0001 exploreP:0.0100\n",
      "Episode:315 Steps:1000 meanR:0.6377 R:0.7500 gloss:0.0390 gloss1-lgP:0.6288 gloss2gQs:0.0619 gloss3dQs:0.0619 gloss4tgtQ:0.0619 dloss:0.0001 exploreP:0.0100\n",
      "Episode:316 Steps:1000 meanR:0.6473 R:1.6500 gloss:0.0398 gloss1-lgP:0.6284 gloss2gQs:0.0633 gloss3dQs:0.0632 gloss4tgtQ:0.0633 dloss:0.0001 exploreP:0.0100\n",
      "Episode:317 Steps:1000 meanR:0.6482 R:0.0900 gloss:0.0404 gloss1-lgP:0.6261 gloss2gQs:0.0644 gloss3dQs:0.0643 gloss4tgtQ:0.0643 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:318 Steps:1000 meanR:0.6451 R:0.2900 gloss:0.0345 gloss1-lgP:0.6279 gloss2gQs:0.0549 gloss3dQs:0.0548 gloss4tgtQ:0.0548 dloss:0.0001 exploreP:0.0100\n",
      "Episode:319 Steps:1000 meanR:0.6399 R:0.7400 gloss:0.0412 gloss1-lgP:0.6259 gloss2gQs:0.0657 gloss3dQs:0.0657 gloss4tgtQ:0.0657 dloss:0.0001 exploreP:0.0100\n",
      "Episode:320 Steps:1000 meanR:0.6491 R:1.2300 gloss:0.0405 gloss1-lgP:0.6253 gloss2gQs:0.0646 gloss3dQs:0.0646 gloss4tgtQ:0.0646 dloss:0.0001 exploreP:0.0100\n",
      "Episode:321 Steps:1000 meanR:0.6581 R:1.3900 gloss:0.0397 gloss1-lgP:0.6253 gloss2gQs:0.0629 gloss3dQs:0.0629 gloss4tgtQ:0.0630 dloss:0.0001 exploreP:0.0100\n",
      "Episode:322 Steps:1000 meanR:0.6581 R:0.5600 gloss:0.0480 gloss1-lgP:0.6325 gloss2gQs:0.0758 gloss3dQs:0.0757 gloss4tgtQ:0.0758 dloss:0.0001 exploreP:0.0100\n",
      "Episode:323 Steps:1000 meanR:0.6511 R:0.2800 gloss:0.0467 gloss1-lgP:0.6312 gloss2gQs:0.0736 gloss3dQs:0.0735 gloss4tgtQ:0.0735 dloss:0.0001 exploreP:0.0100\n",
      "Episode:324 Steps:1000 meanR:0.6634 R:1.6300 gloss:0.0495 gloss1-lgP:0.6289 gloss2gQs:0.0780 gloss3dQs:0.0780 gloss4tgtQ:0.0780 dloss:0.0001 exploreP:0.0100\n",
      "Episode:325 Steps:1000 meanR:0.6676 R:1.0000 gloss:0.0513 gloss1-lgP:0.6310 gloss2gQs:0.0809 gloss3dQs:0.0809 gloss4tgtQ:0.0809 dloss:0.0001 exploreP:0.0100\n",
      "Episode:326 Steps:1000 meanR:0.6778 R:1.1900 gloss:0.0478 gloss1-lgP:0.6297 gloss2gQs:0.0752 gloss3dQs:0.0752 gloss4tgtQ:0.0752 dloss:0.0001 exploreP:0.0100\n",
      "Episode:327 Steps:1000 meanR:0.6767 R:0.3700 gloss:0.0481 gloss1-lgP:0.6286 gloss2gQs:0.0759 gloss3dQs:0.0759 gloss4tgtQ:0.0759 dloss:0.0001 exploreP:0.0100\n",
      "Episode:328 Steps:1000 meanR:0.6754 R:1.2000 gloss:0.0519 gloss1-lgP:0.6263 gloss2gQs:0.0821 gloss3dQs:0.0821 gloss4tgtQ:0.0821 dloss:0.0001 exploreP:0.0100\n",
      "Episode:329 Steps:1000 meanR:0.6766 R:0.9700 gloss:0.0558 gloss1-lgP:0.6282 gloss2gQs:0.0883 gloss3dQs:0.0883 gloss4tgtQ:0.0883 dloss:0.0001 exploreP:0.0100\n",
      "Episode:330 Steps:1000 meanR:0.6740 R:0.5600 gloss:0.0546 gloss1-lgP:0.6280 gloss2gQs:0.0863 gloss3dQs:0.0863 gloss4tgtQ:0.0863 dloss:0.0001 exploreP:0.0100\n",
      "Episode:331 Steps:1000 meanR:0.6678 R:0.5300 gloss:0.0514 gloss1-lgP:0.6292 gloss2gQs:0.0814 gloss3dQs:0.0814 gloss4tgtQ:0.0814 dloss:0.0001 exploreP:0.0100\n",
      "Episode:332 Steps:1000 meanR:0.6576 R:0.0500 gloss:0.0509 gloss1-lgP:0.6323 gloss2gQs:0.0804 gloss3dQs:0.0804 gloss4tgtQ:0.0804 dloss:0.0001 exploreP:0.0100\n",
      "Episode:333 Steps:1000 meanR:0.6626 R:0.7500 gloss:0.0500 gloss1-lgP:0.6324 gloss2gQs:0.0789 gloss3dQs:0.0789 gloss4tgtQ:0.0789 dloss:0.0001 exploreP:0.0100\n",
      "Episode:334 Steps:1000 meanR:0.6655 R:0.7000 gloss:0.0443 gloss1-lgP:0.6311 gloss2gQs:0.0701 gloss3dQs:0.0701 gloss4tgtQ:0.0700 dloss:0.0001 exploreP:0.0100\n",
      "Episode:335 Steps:1000 meanR:0.6629 R:0.3500 gloss:0.0387 gloss1-lgP:0.6321 gloss2gQs:0.0611 gloss3dQs:0.0611 gloss4tgtQ:0.0611 dloss:0.0001 exploreP:0.0100\n",
      "Episode:336 Steps:1000 meanR:0.6716 R:0.9000 gloss:0.0389 gloss1-lgP:0.6306 gloss2gQs:0.0615 gloss3dQs:0.0616 gloss4tgtQ:0.0616 dloss:0.0001 exploreP:0.0100\n",
      "Episode:337 Steps:1000 meanR:0.6702 R:0.5400 gloss:0.0369 gloss1-lgP:0.6318 gloss2gQs:0.0586 gloss3dQs:0.0586 gloss4tgtQ:0.0586 dloss:0.0001 exploreP:0.0100\n",
      "Episode:338 Steps:1000 meanR:0.6728 R:1.1400 gloss:0.0369 gloss1-lgP:0.6326 gloss2gQs:0.0584 gloss3dQs:0.0584 gloss4tgtQ:0.0584 dloss:0.0001 exploreP:0.0100\n",
      "Episode:339 Steps:1000 meanR:0.6849 R:1.6100 gloss:0.0367 gloss1-lgP:0.6304 gloss2gQs:0.0583 gloss3dQs:0.0583 gloss4tgtQ:0.0583 dloss:0.0001 exploreP:0.0100\n",
      "Episode:340 Steps:1000 meanR:0.6928 R:0.7900 gloss:0.0421 gloss1-lgP:0.6310 gloss2gQs:0.0668 gloss3dQs:0.0669 gloss4tgtQ:0.0669 dloss:0.0001 exploreP:0.0100\n",
      "Episode:341 Steps:1000 meanR:0.6912 R:0.3600 gloss:0.0388 gloss1-lgP:0.6327 gloss2gQs:0.0614 gloss3dQs:0.0614 gloss4tgtQ:0.0614 dloss:0.0001 exploreP:0.0100\n",
      "Episode:342 Steps:1000 meanR:0.6895 R:0.6000 gloss:0.0414 gloss1-lgP:0.6316 gloss2gQs:0.0656 gloss3dQs:0.0656 gloss4tgtQ:0.0656 dloss:0.0001 exploreP:0.0100\n",
      "Episode:343 Steps:1000 meanR:0.6873 R:0.0800 gloss:0.0431 gloss1-lgP:0.6310 gloss2gQs:0.0685 gloss3dQs:0.0685 gloss4tgtQ:0.0685 dloss:0.0001 exploreP:0.0100\n",
      "Episode:344 Steps:1000 meanR:0.6908 R:0.5100 gloss:0.0371 gloss1-lgP:0.6306 gloss2gQs:0.0589 gloss3dQs:0.0589 gloss4tgtQ:0.0589 dloss:0.0001 exploreP:0.0100\n",
      "Episode:345 Steps:1000 meanR:0.6946 R:1.0800 gloss:0.0417 gloss1-lgP:0.6311 gloss2gQs:0.0662 gloss3dQs:0.0662 gloss4tgtQ:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:346 Steps:1000 meanR:0.6877 R:0.2600 gloss:0.0419 gloss1-lgP:0.6303 gloss2gQs:0.0665 gloss3dQs:0.0665 gloss4tgtQ:0.0665 dloss:0.0001 exploreP:0.0100\n",
      "Episode:347 Steps:1000 meanR:0.6900 R:0.8800 gloss:0.0401 gloss1-lgP:0.6310 gloss2gQs:0.0635 gloss3dQs:0.0635 gloss4tgtQ:0.0635 dloss:0.0001 exploreP:0.0100\n",
      "Episode:348 Steps:1000 meanR:0.6959 R:1.1300 gloss:0.0420 gloss1-lgP:0.6309 gloss2gQs:0.0666 gloss3dQs:0.0666 gloss4tgtQ:0.0666 dloss:0.0001 exploreP:0.0100\n",
      "Episode:349 Steps:1000 meanR:0.6948 R:0.5200 gloss:0.0390 gloss1-lgP:0.6315 gloss2gQs:0.0620 gloss3dQs:0.0620 gloss4tgtQ:0.0620 dloss:0.0001 exploreP:0.0100\n",
      "Episode:350 Steps:1000 meanR:0.6977 R:1.0100 gloss:0.0397 gloss1-lgP:0.6308 gloss2gQs:0.0633 gloss3dQs:0.0633 gloss4tgtQ:0.0633 dloss:0.0001 exploreP:0.0100\n",
      "Episode:351 Steps:1000 meanR:0.7102 R:1.6200 gloss:0.0419 gloss1-lgP:0.6293 gloss2gQs:0.0668 gloss3dQs:0.0669 gloss4tgtQ:0.0669 dloss:0.0001 exploreP:0.0100\n",
      "Episode:352 Steps:1000 meanR:0.7027 R:0.3400 gloss:0.0431 gloss1-lgP:0.6291 gloss2gQs:0.0686 gloss3dQs:0.0686 gloss4tgtQ:0.0686 dloss:0.0001 exploreP:0.0100\n",
      "Episode:353 Steps:1000 meanR:0.7121 R:1.3100 gloss:0.0422 gloss1-lgP:0.6271 gloss2gQs:0.0671 gloss3dQs:0.0671 gloss4tgtQ:0.0671 dloss:0.0001 exploreP:0.0100\n",
      "Episode:354 Steps:1000 meanR:0.7140 R:0.4300 gloss:0.0663 gloss1-lgP:0.6684 gloss2gQs:0.0975 gloss3dQs:0.0970 gloss4tgtQ:0.0973 dloss:0.0002 exploreP:0.0100\n",
      "Episode:355 Steps:1000 meanR:0.7051 R:0.7900 gloss:0.0479 gloss1-lgP:0.6302 gloss2gQs:0.0758 gloss3dQs:0.0759 gloss4tgtQ:0.0758 dloss:0.0001 exploreP:0.0100\n",
      "Episode:356 Steps:1000 meanR:0.7129 R:1.2500 gloss:0.0502 gloss1-lgP:0.6295 gloss2gQs:0.0795 gloss3dQs:0.0796 gloss4tgtQ:0.0796 dloss:0.0001 exploreP:0.0100\n",
      "Episode:357 Steps:1000 meanR:0.7051 R:0.3100 gloss:0.0526 gloss1-lgP:0.6305 gloss2gQs:0.0833 gloss3dQs:0.0833 gloss4tgtQ:0.0833 dloss:0.0001 exploreP:0.0100\n",
      "Episode:358 Steps:1000 meanR:0.7079 R:0.7600 gloss:0.0505 gloss1-lgP:0.6305 gloss2gQs:0.0798 gloss3dQs:0.0798 gloss4tgtQ:0.0798 dloss:0.0001 exploreP:0.0100\n",
      "Episode:359 Steps:1000 meanR:0.7122 R:0.8700 gloss:0.0492 gloss1-lgP:0.6312 gloss2gQs:0.0776 gloss3dQs:0.0776 gloss4tgtQ:0.0776 dloss:0.0001 exploreP:0.0100\n",
      "Episode:360 Steps:1000 meanR:0.7113 R:0.4200 gloss:0.0491 gloss1-lgP:0.6320 gloss2gQs:0.0772 gloss3dQs:0.0772 gloss4tgtQ:0.0772 dloss:0.0001 exploreP:0.0100\n",
      "Episode:361 Steps:1000 meanR:0.7147 R:0.7900 gloss:0.0431 gloss1-lgP:0.6313 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:362 Steps:1000 meanR:0.7155 R:1.5900 gloss:0.0431 gloss1-lgP:0.6325 gloss2gQs:0.0680 gloss3dQs:0.0680 gloss4tgtQ:0.0680 dloss:0.0001 exploreP:0.0100\n",
      "Episode:363 Steps:1000 meanR:0.7126 R:0.3200 gloss:0.0458 gloss1-lgP:0.6342 gloss2gQs:0.0722 gloss3dQs:0.0723 gloss4tgtQ:0.0722 dloss:0.0001 exploreP:0.0100\n",
      "Episode:364 Steps:1000 meanR:0.7047 R:0.3600 gloss:0.0408 gloss1-lgP:0.6339 gloss2gQs:0.0643 gloss3dQs:0.0644 gloss4tgtQ:0.0644 dloss:0.0001 exploreP:0.0100\n",
      "Episode:365 Steps:1000 meanR:0.7080 R:0.7200 gloss:0.0426 gloss1-lgP:0.6359 gloss2gQs:0.0670 gloss3dQs:0.0670 gloss4tgtQ:0.0670 dloss:0.0001 exploreP:0.0100\n",
      "Episode:366 Steps:1000 meanR:0.7042 R:0.8100 gloss:0.0428 gloss1-lgP:0.6367 gloss2gQs:0.0670 gloss3dQs:0.0670 gloss4tgtQ:0.0670 dloss:0.0001 exploreP:0.0100\n",
      "Episode:367 Steps:1000 meanR:0.7142 R:1.5900 gloss:0.0452 gloss1-lgP:0.6364 gloss2gQs:0.0710 gloss3dQs:0.0711 gloss4tgtQ:0.0711 dloss:0.0001 exploreP:0.0100\n",
      "Episode:368 Steps:1000 meanR:0.7137 R:0.6800 gloss:0.0448 gloss1-lgP:0.6369 gloss2gQs:0.0703 gloss3dQs:0.0703 gloss4tgtQ:0.0703 dloss:0.0001 exploreP:0.0100\n",
      "Episode:369 Steps:1000 meanR:0.7172 R:0.6800 gloss:0.0460 gloss1-lgP:0.6356 gloss2gQs:0.0724 gloss3dQs:0.0724 gloss4tgtQ:0.0724 dloss:0.0001 exploreP:0.0100\n",
      "Episode:370 Steps:1000 meanR:0.7188 R:0.7400 gloss:0.0457 gloss1-lgP:0.6363 gloss2gQs:0.0718 gloss3dQs:0.0718 gloss4tgtQ:0.0718 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:371 Steps:1000 meanR:0.7233 R:1.4600 gloss:0.0501 gloss1-lgP:0.6375 gloss2gQs:0.0785 gloss3dQs:0.0785 gloss4tgtQ:0.0785 dloss:0.0001 exploreP:0.0100\n",
      "Episode:372 Steps:1000 meanR:0.7213 R:0.7600 gloss:0.0473 gloss1-lgP:0.6369 gloss2gQs:0.0741 gloss3dQs:0.0741 gloss4tgtQ:0.0741 dloss:0.0001 exploreP:0.0100\n",
      "Episode:373 Steps:1000 meanR:0.7232 R:0.7100 gloss:0.0492 gloss1-lgP:0.6359 gloss2gQs:0.0771 gloss3dQs:0.0771 gloss4tgtQ:0.0771 dloss:0.0001 exploreP:0.0100\n",
      "Episode:374 Steps:1000 meanR:0.7243 R:0.3200 gloss:0.0496 gloss1-lgP:0.6401 gloss2gQs:0.0777 gloss3dQs:0.0777 gloss4tgtQ:0.0777 dloss:0.0001 exploreP:0.0100\n",
      "Episode:375 Steps:1000 meanR:0.7251 R:0.7400 gloss:0.0475 gloss1-lgP:0.6332 gloss2gQs:0.0749 gloss3dQs:0.0749 gloss4tgtQ:0.0749 dloss:0.0001 exploreP:0.0100\n",
      "Episode:376 Steps:1000 meanR:0.7247 R:0.9200 gloss:0.0475 gloss1-lgP:0.6331 gloss2gQs:0.0747 gloss3dQs:0.0747 gloss4tgtQ:0.0747 dloss:0.0001 exploreP:0.0100\n",
      "Episode:377 Steps:1000 meanR:0.7360 R:1.4700 gloss:0.0468 gloss1-lgP:0.6335 gloss2gQs:0.0732 gloss3dQs:0.0732 gloss4tgtQ:0.0733 dloss:0.0001 exploreP:0.0100\n",
      "Episode:378 Steps:1000 meanR:0.7518 R:1.5800 gloss:0.0504 gloss1-lgP:0.6338 gloss2gQs:0.0789 gloss3dQs:0.0789 gloss4tgtQ:0.0789 dloss:0.0001 exploreP:0.0100\n",
      "Episode:379 Steps:1000 meanR:0.7503 R:0.4400 gloss:0.0584 gloss1-lgP:0.6348 gloss2gQs:0.0916 gloss3dQs:0.0915 gloss4tgtQ:0.0915 dloss:0.0002 exploreP:0.0100\n",
      "Episode:380 Steps:1000 meanR:0.7536 R:1.1500 gloss:0.0526 gloss1-lgP:0.6329 gloss2gQs:0.0826 gloss3dQs:0.0826 gloss4tgtQ:0.0826 dloss:0.0001 exploreP:0.0100\n",
      "Episode:381 Steps:1000 meanR:0.7556 R:1.0900 gloss:0.0544 gloss1-lgP:0.6363 gloss2gQs:0.0850 gloss3dQs:0.0850 gloss4tgtQ:0.0850 dloss:0.0001 exploreP:0.0100\n",
      "Episode:382 Steps:1000 meanR:0.7561 R:0.4700 gloss:0.0590 gloss1-lgP:0.6347 gloss2gQs:0.0923 gloss3dQs:0.0923 gloss4tgtQ:0.0923 dloss:0.0001 exploreP:0.0100\n",
      "Episode:383 Steps:1000 meanR:0.7629 R:0.8400 gloss:0.0521 gloss1-lgP:0.6355 gloss2gQs:0.0814 gloss3dQs:0.0814 gloss4tgtQ:0.0814 dloss:0.0001 exploreP:0.0100\n",
      "Episode:384 Steps:1000 meanR:0.7659 R:0.6100 gloss:0.0557 gloss1-lgP:0.6395 gloss2gQs:0.0867 gloss3dQs:0.0867 gloss4tgtQ:0.0867 dloss:0.0001 exploreP:0.0100\n",
      "Episode:385 Steps:1000 meanR:0.7726 R:0.8800 gloss:0.0537 gloss1-lgP:0.6403 gloss2gQs:0.0835 gloss3dQs:0.0835 gloss4tgtQ:0.0835 dloss:0.0001 exploreP:0.0100\n",
      "Episode:386 Steps:1000 meanR:0.7664 R:0.2000 gloss:0.0568 gloss1-lgP:0.6405 gloss2gQs:0.0883 gloss3dQs:0.0883 gloss4tgtQ:0.0883 dloss:0.0001 exploreP:0.0100\n",
      "Episode:387 Steps:1000 meanR:0.7603 R:0.2000 gloss:0.0519 gloss1-lgP:0.6407 gloss2gQs:0.0805 gloss3dQs:0.0805 gloss4tgtQ:0.0805 dloss:0.0001 exploreP:0.0100\n",
      "Episode:388 Steps:1000 meanR:0.7573 R:0.5900 gloss:0.0411 gloss1-lgP:0.6404 gloss2gQs:0.0638 gloss3dQs:0.0638 gloss4tgtQ:0.0638 dloss:0.0001 exploreP:0.0100\n",
      "Episode:389 Steps:1000 meanR:0.7592 R:1.1400 gloss:0.0425 gloss1-lgP:0.6404 gloss2gQs:0.0660 gloss3dQs:0.0660 gloss4tgtQ:0.0660 dloss:0.0001 exploreP:0.0100\n",
      "Episode:390 Steps:1000 meanR:0.7596 R:0.7900 gloss:0.0423 gloss1-lgP:0.6426 gloss2gQs:0.0655 gloss3dQs:0.0655 gloss4tgtQ:0.0655 dloss:0.0001 exploreP:0.0100\n",
      "Episode:391 Steps:1000 meanR:0.7668 R:1.1500 gloss:0.0407 gloss1-lgP:0.6424 gloss2gQs:0.0631 gloss3dQs:0.0631 gloss4tgtQ:0.0631 dloss:0.0001 exploreP:0.0100\n",
      "Episode:392 Steps:1000 meanR:0.7617 R:0.4600 gloss:0.0425 gloss1-lgP:0.6416 gloss2gQs:0.0658 gloss3dQs:0.0658 gloss4tgtQ:0.0658 dloss:0.0001 exploreP:0.0100\n",
      "Episode:393 Steps:1000 meanR:0.7633 R:0.3400 gloss:0.0383 gloss1-lgP:0.6409 gloss2gQs:0.0594 gloss3dQs:0.0594 gloss4tgtQ:0.0594 dloss:0.0001 exploreP:0.0100\n",
      "Episode:394 Steps:1000 meanR:0.7532 R:0.1100 gloss:1.6991 gloss1-lgP:1.7390 gloss2gQs:0.2465 gloss3dQs:0.1359 gloss4tgtQ:0.2444 dloss:0.1622 exploreP:0.0100\n",
      "Episode:395 Steps:1000 meanR:0.7587 R:0.9000 gloss:0.8848 gloss1-lgP:0.6880 gloss2gQs:1.2325 gloss3dQs:1.2369 gloss4tgtQ:1.2200 dloss:0.0219 exploreP:0.0100\n",
      "Episode:396 Steps:1000 meanR:0.7474 R:0.3200 gloss:0.3407 gloss1-lgP:0.6411 gloss2gQs:0.5372 gloss3dQs:0.5375 gloss4tgtQ:0.5325 dloss:0.0020 exploreP:0.0100\n",
      "Episode:397 Steps:1000 meanR:0.7474 R:0.1500 gloss:0.1621 gloss1-lgP:0.6393 gloss2gQs:0.2560 gloss3dQs:0.2562 gloss4tgtQ:0.2538 dloss:0.0008 exploreP:0.0100\n",
      "Episode:398 Steps:1000 meanR:0.7472 R:0.5900 gloss:0.0684 gloss1-lgP:0.6380 gloss2gQs:0.1075 gloss3dQs:0.1077 gloss4tgtQ:0.1070 dloss:0.0005 exploreP:0.0100\n",
      "Episode:399 Steps:1000 meanR:0.7404 R:0.8100 gloss:0.0382 gloss1-lgP:0.6377 gloss2gQs:0.0598 gloss3dQs:0.0600 gloss4tgtQ:0.0597 dloss:0.0003 exploreP:0.0100\n",
      "Episode:400 Steps:1000 meanR:0.7482 R:0.8500 gloss:0.0284 gloss1-lgP:0.6394 gloss2gQs:0.0448 gloss3dQs:0.0449 gloss4tgtQ:0.0449 dloss:0.0002 exploreP:0.0100\n",
      "Episode:401 Steps:1000 meanR:0.7505 R:0.8200 gloss:0.0347 gloss1-lgP:0.6370 gloss2gQs:0.0550 gloss3dQs:0.0548 gloss4tgtQ:0.0549 dloss:0.0002 exploreP:0.0100\n",
      "Episode:402 Steps:1000 meanR:0.7492 R:0.7200 gloss:0.0328 gloss1-lgP:0.6362 gloss2gQs:0.0519 gloss3dQs:0.0518 gloss4tgtQ:0.0518 dloss:0.0001 exploreP:0.0100\n",
      "Episode:403 Steps:1000 meanR:0.7497 R:0.3200 gloss:0.0356 gloss1-lgP:0.6344 gloss2gQs:0.0564 gloss3dQs:0.0563 gloss4tgtQ:0.0563 dloss:0.0001 exploreP:0.0100\n",
      "Episode:404 Steps:1000 meanR:0.7505 R:0.4600 gloss:0.0340 gloss1-lgP:0.6335 gloss2gQs:0.0536 gloss3dQs:0.0536 gloss4tgtQ:0.0536 dloss:0.0001 exploreP:0.0100\n",
      "Episode:405 Steps:1000 meanR:0.7540 R:0.9200 gloss:0.0349 gloss1-lgP:0.6353 gloss2gQs:0.0551 gloss3dQs:0.0551 gloss4tgtQ:0.0551 dloss:0.0001 exploreP:0.0100\n",
      "Episode:406 Steps:1000 meanR:0.7499 R:0.4700 gloss:0.0360 gloss1-lgP:0.6348 gloss2gQs:0.0566 gloss3dQs:0.0566 gloss4tgtQ:0.0566 dloss:0.0001 exploreP:0.0100\n",
      "Episode:407 Steps:1000 meanR:0.7416 R:0.3900 gloss:0.0351 gloss1-lgP:0.6350 gloss2gQs:0.0553 gloss3dQs:0.0553 gloss4tgtQ:0.0553 dloss:0.0001 exploreP:0.0100\n",
      "Episode:408 Steps:1000 meanR:0.7439 R:0.5800 gloss:0.0373 gloss1-lgP:0.6352 gloss2gQs:0.0588 gloss3dQs:0.0588 gloss4tgtQ:0.0588 dloss:0.0001 exploreP:0.0100\n",
      "Episode:409 Steps:1000 meanR:0.7443 R:0.1600 gloss:0.0364 gloss1-lgP:0.6350 gloss2gQs:0.0574 gloss3dQs:0.0574 gloss4tgtQ:0.0574 dloss:0.0001 exploreP:0.0100\n",
      "Episode:410 Steps:1000 meanR:0.7425 R:0.6900 gloss:0.0356 gloss1-lgP:0.6343 gloss2gQs:0.0562 gloss3dQs:0.0562 gloss4tgtQ:0.0562 dloss:0.0001 exploreP:0.0100\n",
      "Episode:411 Steps:1000 meanR:0.7397 R:0.2700 gloss:0.0329 gloss1-lgP:0.6346 gloss2gQs:0.0519 gloss3dQs:0.0519 gloss4tgtQ:0.0519 dloss:0.0000 exploreP:0.0100\n",
      "Episode:412 Steps:1000 meanR:0.7475 R:1.4200 gloss:0.0304 gloss1-lgP:0.6352 gloss2gQs:0.0480 gloss3dQs:0.0480 gloss4tgtQ:0.0480 dloss:0.0000 exploreP:0.0100\n",
      "Episode:413 Steps:1000 meanR:0.7503 R:0.6000 gloss:70.3726 gloss1-lgP:19.0429 gloss2gQs:1.0283 gloss3dQs:-1.8000 gloss4tgtQ:1.0164 dloss:29.6020 exploreP:0.0100\n",
      "Episode:414 Steps:1000 meanR:0.7468 R:0.7500 gloss:1.9388 gloss1-lgP:1.1658 gloss2gQs:0.9486 gloss3dQs:0.9243 gloss4tgtQ:0.9391 dloss:0.0831 exploreP:0.0100\n",
      "Episode:415 Steps:1000 meanR:0.7462 R:0.6900 gloss:0.4094 gloss1-lgP:0.6450 gloss2gQs:0.6520 gloss3dQs:0.6411 gloss4tgtQ:0.6467 dloss:0.0216 exploreP:0.0100\n",
      "Episode:416 Steps:1000 meanR:0.7403 R:1.0600 gloss:0.2137 gloss1-lgP:0.6315 gloss2gQs:0.3512 gloss3dQs:0.3451 gloss4tgtQ:0.3484 dloss:0.0091 exploreP:0.0100\n",
      "Episode:417 Steps:1000 meanR:0.7423 R:0.2900 gloss:-3.9958 gloss1-lgP:1.3861 gloss2gQs:-0.1166 gloss3dQs:0.0261 gloss4tgtQ:-0.1142 dloss:0.3960 exploreP:0.0100\n",
      "Episode:418 Steps:1000 meanR:0.7398 R:0.0400 gloss:202.2439 gloss1-lgP:48.2693 gloss2gQs:2.3070 gloss3dQs:0.0373 gloss4tgtQ:2.2830 dloss:57.4665 exploreP:0.0100\n",
      "Episode:419 Steps:1000 meanR:0.7389 R:0.6500 gloss:6.2236 gloss1-lgP:10.9477 gloss2gQs:0.4844 gloss3dQs:0.9251 gloss4tgtQ:0.4798 dloss:0.6671 exploreP:0.0100\n",
      "Episode:420 Steps:1000 meanR:0.7369 R:1.0300 gloss:-5.5586 gloss1-lgP:11.3251 gloss2gQs:-0.0983 gloss3dQs:-0.0850 gloss4tgtQ:-0.0975 dloss:0.0732 exploreP:0.0100\n",
      "Episode:421 Steps:1000 meanR:0.7298 R:0.6800 gloss:0.0298 gloss1-lgP:10.1927 gloss2gQs:0.0763 gloss3dQs:0.0640 gloss4tgtQ:0.0760 dloss:0.0339 exploreP:0.0100\n",
      "Episode:422 Steps:1000 meanR:0.7272 R:0.3000 gloss:0.0710 gloss1-lgP:6.9749 gloss2gQs:0.0219 gloss3dQs:0.0299 gloss4tgtQ:0.0214 dloss:0.0195 exploreP:0.0100\n",
      "Episode:423 Steps:1000 meanR:0.7244 R:0.0000 gloss:-0.3805 gloss1-lgP:5.3631 gloss2gQs:-0.0827 gloss3dQs:-0.0681 gloss4tgtQ:-0.0818 dloss:0.0142 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:424 Steps:1000 meanR:0.7124 R:0.4300 gloss:-1.3585 gloss1-lgP:4.9334 gloss2gQs:-0.2669 gloss3dQs:-0.2473 gloss4tgtQ:-0.2635 dloss:0.0101 exploreP:0.0100\n",
      "Episode:425 Steps:1000 meanR:0.7111 R:0.8700 gloss:-4530060.7435 gloss1-lgP:178.0617 gloss2gQs:-7762.4654 gloss3dQs:-429.7463 gloss4tgtQ:-7664.1698 dloss:110818960.1933 exploreP:0.0100\n",
      "Episode:426 Steps:1000 meanR:0.7036 R:0.4400 gloss:-9437694.1851 gloss1-lgP:681.1457 gloss2gQs:-6697.3540 gloss3dQs:-492.2535 gloss4tgtQ:-6612.1132 dloss:61509487.3629 exploreP:0.0100\n",
      "Episode:427 Steps:1000 meanR:0.7022 R:0.2300 gloss:-2641758.2040 gloss1-lgP:554.8315 gloss2gQs:-2555.6513 gloss3dQs:-405.0683 gloss4tgtQ:-2527.4517 dloss:7896045.5252 exploreP:0.0100\n",
      "Episode:428 Steps:1000 meanR:0.6918 R:0.1600 gloss:-513607.0323 gloss1-lgP:423.9400 gloss2gQs:-1109.9803 gloss3dQs:-332.7251 gloss4tgtQ:-1099.3663 dloss:1217430.6506 exploreP:0.0100\n",
      "Episode:429 Steps:1000 meanR:0.6853 R:0.3200 gloss:-110497.3811 gloss1-lgP:469.3274 gloss2gQs:-126.9294 gloss3dQs:-127.2669 gloss4tgtQ:-125.2840 dloss:40511.9577 exploreP:0.0100\n",
      "Episode:430 Steps:1000 meanR:0.6821 R:0.2400 gloss:-150503.8342 gloss1-lgP:539.6867 gloss2gQs:-154.8751 gloss3dQs:-126.6056 gloss4tgtQ:-153.0511 dloss:25233.7497 exploreP:0.0100\n",
      "Episode:431 Steps:1000 meanR:0.6801 R:0.3300 gloss:-94186.2981 gloss1-lgP:617.9168 gloss2gQs:-105.5848 gloss3dQs:-107.5643 gloss4tgtQ:-104.5176 dloss:10474.6843 exploreP:0.0100\n",
      "Episode:432 Steps:1000 meanR:0.6902 R:1.0600 gloss:-16789.1628 gloss1-lgP:592.2404 gloss2gQs:-52.4499 gloss3dQs:-75.1451 gloss4tgtQ:-51.8155 dloss:4656.1553 exploreP:0.0100\n",
      "Episode:433 Steps:1000 meanR:0.6853 R:0.2600 gloss:-4482.7829 gloss1-lgP:492.3225 gloss2gQs:-36.8438 gloss3dQs:-54.4557 gloss4tgtQ:-36.4167 dloss:3350.1605 exploreP:0.0100\n",
      "Episode:434 Steps:1000 meanR:0.6905 R:1.2200 gloss:-8297.0539 gloss1-lgP:361.5871 gloss2gQs:-37.4947 gloss3dQs:-42.8678 gloss4tgtQ:-37.1852 dloss:1441.3496 exploreP:0.0100\n",
      "Episode:435 Steps:1000 meanR:0.6922 R:0.5200 gloss:-20701.4491 gloss1-lgP:421.4674 gloss2gQs:-49.1430 gloss3dQs:-44.8886 gloss4tgtQ:-48.7402 dloss:962.6190 exploreP:0.0100\n",
      "Episode:436 Steps:1000 meanR:0.6911 R:0.7900 gloss:-88111.1991 gloss1-lgP:1086.6778 gloss2gQs:-78.5647 gloss3dQs:-63.0589 gloss4tgtQ:-77.8205 dloss:1246.1159 exploreP:0.0100\n",
      "Episode:437 Steps:1000 meanR:0.6870 R:0.1300 gloss:-240222.8638 gloss1-lgP:2856.5501 gloss2gQs:-84.5826 gloss3dQs:-66.4379 gloss4tgtQ:-83.7190 dloss:1806.2794 exploreP:0.0100\n",
      "Episode:438 Steps:1000 meanR:0.6788 R:0.3200 gloss:-280170.4455 gloss1-lgP:4545.4966 gloss2gQs:-61.5207 gloss3dQs:-54.0194 gloss4tgtQ:-60.8770 dloss:3907.6893 exploreP:0.0100\n",
      "Episode:439 Steps:1000 meanR:0.6645 R:0.1800 gloss:-103257622.0608 gloss1-lgP:6400.2761 gloss2gQs:-35596.4735 gloss3dQs:-22769.6309 gloss4tgtQ:-34935.5876 dloss:9848364511.5200 exploreP:0.0100\n",
      "Episode:440 Steps:1000 meanR:0.6608 R:0.4200 gloss:-61575055809.0117 gloss1-lgP:10743.6623 gloss2gQs:-3137356.5314 gloss3dQs:-1110256.6713 gloss4tgtQ:-3113347.9596 dloss:29447147016517.5625 exploreP:0.0100\n",
      "Episode:441 Steps:1000 meanR:0.6633 R:0.6100 gloss:-2052280865313.5544 gloss1-lgP:21458.4980 gloss2gQs:-30396247.9047 gloss3dQs:-6556932.6951 gloss4tgtQ:-30081618.0550 dloss:1930288610593064.2500 exploreP:0.0100\n",
      "Episode:442 Steps:1000 meanR:0.6589 R:0.1600 gloss:-22594322846654.4414 gloss1-lgP:52152.7447 gloss2gQs:-122235851.8522 gloss3dQs:-17088088.8676 gloss4tgtQ:-120952921.8453 dloss:23561767689451548.0000 exploreP:0.0100\n",
      "Episode:443 Steps:1000 meanR:0.6581 R:0.0000 gloss:-98565854706363.4375 gloss1-lgP:103525.7797 gloss2gQs:-261120506.2015 gloss3dQs:-24465624.4579 gloss4tgtQ:-258724168.4600 dloss:100846663972431888.0000 exploreP:0.0100\n",
      "Episode:444 Steps:1000 meanR:0.6554 R:0.2400 gloss:-221872302824852.9062 gloss1-lgP:171711.6008 gloss2gQs:-354358155.6667 gloss3dQs:-30066871.3306 gloss4tgtQ:-350690961.9962 dloss:174851222794132768.0000 exploreP:0.0100\n",
      "Episode:445 Steps:1000 meanR:0.6552 R:1.0600 gloss:-315977262648213.2500 gloss1-lgP:237132.1791 gloss2gQs:-375382518.4626 gloss3dQs:-39198139.1217 gloss4tgtQ:-371218236.5110 dloss:177878384211203808.0000 exploreP:0.0100\n",
      "Episode:446 Steps:1000 meanR:0.6619 R:0.9300 gloss:-363846534113475.2500 gloss1-lgP:294757.0229 gloss2gQs:-355246615.4439 gloss3dQs:-47290582.8855 gloss4tgtQ:-351641063.7143 dloss:142158730392237200.0000 exploreP:0.0100\n",
      "Episode:447 Steps:1000 meanR:0.6778 R:2.4700 gloss:-337997749644050.6250 gloss1-lgP:314111.6440 gloss2gQs:-319411247.0814 gloss3dQs:-53989170.0723 gloss4tgtQ:-316365632.3361 dloss:101454121188617872.0000 exploreP:0.0100\n",
      "Episode:448 Steps:1000 meanR:0.6676 R:0.1100 gloss:-326743235515245.3125 gloss1-lgP:337296.0138 gloss2gQs:-291777945.2804 gloss3dQs:-61037280.2551 gloss4tgtQ:-288244868.5054 dloss:76054443590618928.0000 exploreP:0.0100\n",
      "Episode:449 Steps:1000 meanR:0.6687 R:0.6300 gloss:-260380117491914.2188 gloss1-lgP:282476.3604 gloss2gQs:-288277802.4699 gloss3dQs:-73512874.9821 gloss4tgtQ:-284882502.8099 dloss:68841284375441064.0000 exploreP:0.0100\n",
      "Episode:450 Steps:1000 meanR:0.6658 R:0.7200 gloss:-174064819140096.1250 gloss1-lgP:203918.3938 gloss2gQs:-334344018.0437 gloss3dQs:-105987463.1189 gloss4tgtQ:-330619162.2438 dloss:79601050543741152.0000 exploreP:0.0100\n",
      "Episode:451 Steps:1000 meanR:0.6540 R:0.4400 gloss:-130339198849963.6250 gloss1-lgP:171409.6190 gloss2gQs:-429614437.6751 gloss3dQs:-164698806.4754 gloss4tgtQ:-424219901.1546 dloss:106111187596582592.0000 exploreP:0.0100\n",
      "Episode:452 Steps:1000 meanR:0.6560 R:0.5400 gloss:-223321074803845.6250 gloss1-lgP:191378.5296 gloss2gQs:-928878473.2663 gloss3dQs:-369367420.6143 gloss4tgtQ:-917750746.1922 dloss:473193651042210048.0000 exploreP:0.0100\n",
      "Episode:453 Steps:1000 meanR:0.6513 R:0.8400 gloss:-2924917320879012.5000 gloss1-lgP:353263.3631 gloss2gQs:-6993761151.6711 gloss3dQs:-2022598336.9564 gloss4tgtQ:-6915193561.6675 dloss:29859514570987044864.0000 exploreP:0.0100\n",
      "Episode:454 Steps:1000 meanR:0.6490 R:0.2000 gloss:-29524827607454272.0000 gloss1-lgP:682431.3832 gloss2gQs:-39977518237.1963 gloss3dQs:-10133296416.3778 gloss4tgtQ:-39553507109.7191 dloss:781595600017518231552.0000 exploreP:0.0100\n",
      "Episode:455 Steps:1000 meanR:0.6432 R:0.2100 gloss:-166387910226200448.0000 gloss1-lgP:1082851.9903 gloss2gQs:-144534564456.8200 gloss3dQs:-34747834910.3077 gloss4tgtQ:-142816864299.8720 dloss:9148514858553303891968.0000 exploreP:0.0100\n",
      "Episode:456 Steps:1000 meanR:0.6387 R:0.8000 gloss:-633161190628909312.0000 gloss1-lgP:1491568.6301 gloss2gQs:-394687887565.4847 gloss3dQs:-92026513670.6761 gloss4tgtQ:-390043507394.4114 dloss:65311089415207958609920.0000 exploreP:0.0100\n",
      "Episode:457 Steps:1000 meanR:0.6417 R:0.6100 gloss:-2033660843122800128.0000 gloss1-lgP:1960734.6067 gloss2gQs:-925504568865.9438 gloss3dQs:-220267645921.2567 gloss4tgtQ:-915322241124.6887 dloss:345487218860644037033984.0000 exploreP:0.0100\n",
      "Episode:458 Steps:1000 meanR:0.6341 R:0.0000 gloss:-5033900473934754816.0000 gloss1-lgP:2297145.6291 gloss2gQs:-1925077564903.7339 gloss3dQs:-491478645428.3672 gloss4tgtQ:-1902098773171.3127 dloss:1409726624750790212845568.0000 exploreP:0.0100\n",
      "Episode:459 Steps:1000 meanR:0.6327 R:0.7300 gloss:-9937766640598874112.0000 gloss1-lgP:2406212.7222 gloss2gQs:-3577832850908.2944 gloss3dQs:-986751266338.8468 gloss4tgtQ:-3534594162680.1060 dloss:4647449505352473263472640.0000 exploreP:0.0100\n",
      "Episode:460 Steps:1000 meanR:0.6305 R:0.2000 gloss:-17867905060671932416.0000 gloss1-lgP:2447749.7514 gloss2gQs:-6218268189799.9043 gloss3dQs:-1865145580315.7883 gloss4tgtQ:-6145883808377.6992 dloss:13410127503563795963314176.0000 exploreP:0.0100\n",
      "Episode:461 Steps:1000 meanR:0.6283 R:0.5700 gloss:-29980793558297235456.0000 gloss1-lgP:2453853.6816 gloss2gQs:-10288403417129.7168 gloss3dQs:-3408702830812.4585 gloss4tgtQ:-10164424329051.6602 dloss:34471795559827169439907840.0000 exploreP:0.0100\n",
      "Episode:462 Steps:1000 meanR:0.6167 R:0.4300 gloss:-48038176424392818688.0000 gloss1-lgP:2434504.9540 gloss2gQs:-16472947150500.0957 gloss3dQs:-5814285862666.9639 gloss4tgtQ:-16276049670948.0488 dloss:85320137962995708652945408.0000 exploreP:0.0100\n",
      "Episode:463 Steps:1000 meanR:0.6229 R:0.9400 gloss:-49361755765765185536.0000 gloss1-lgP:1767321.3681 gloss2gQs:-24723059849780.9844 gloss3dQs:-9710391998168.8184 gloss4tgtQ:-24413640451084.6445 dloss:174865026454152420322705408.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:464 Steps:1000 meanR:0.6205 R:0.1200 gloss:-61172595303055106048.0000 gloss1-lgP:1532043.8402 gloss2gQs:-36470523866575.1797 gloss3dQs:-15702266002203.4414 gloss4tgtQ:-36043292304171.5469 dloss:349677707511588803852632064.0000 exploreP:0.0100\n",
      "Episode:465 Steps:1000 meanR:0.6263 R:1.3000 gloss:-62773817861606694912.0000 gloss1-lgP:1236858.1117 gloss2gQs:-51949836735086.8203 gloss3dQs:-24123847367139.8398 gloss4tgtQ:-51337957877482.7969 dloss:642504667822589929798500352.0000 exploreP:0.0100\n",
      "Episode:466 Steps:1000 meanR:0.6231 R:0.4900 gloss:-61585371975228850176.0000 gloss1-lgP:969141.1595 gloss2gQs:-71496503064679.4062 gloss3dQs:-35715494721588.4766 gloss4tgtQ:-70645178362906.2188 dloss:1075586346540685414551781376.0000 exploreP:0.0100\n",
      "Episode:467 Steps:1000 meanR:0.6131 R:0.5900 gloss:-59236660916680851456.0000 gloss1-lgP:717574.3678 gloss2gQs:-96326773973761.4219 gloss3dQs:-52282841965634.7500 gloss4tgtQ:-95236663841349.8438 dloss:1663186191739746790997491712.0000 exploreP:0.0100\n",
      "Episode:468 Steps:1000 meanR:0.6105 R:0.4200 gloss:-74173283879929708544.0000 gloss1-lgP:655291.0499 gloss2gQs:-126899098125066.9375 gloss3dQs:-71993244024777.2812 gloss4tgtQ:-125402351465825.5781 dloss:2631364587590176081742659584.0000 exploreP:0.0100\n",
      "Episode:469 Steps:1000 meanR:0.6095 R:0.5800 gloss:-73648874970912309248.0000 gloss1-lgP:478303.3797 gloss2gQs:-168481295675670.5625 gloss3dQs:-100119370715714.3281 gloss4tgtQ:-166521585952308.0938 dloss:4076470540537820910503067648.0000 exploreP:0.0100\n",
      "Episode:470 Steps:1000 meanR:0.6171 R:1.5000 gloss:-97540559244968820736.0000 gloss1-lgP:464454.6491 gloss2gQs:-219459198523784.1875 gloss3dQs:-135645284974888.5469 gloss4tgtQ:-216819459205437.0312 dloss:6152972253785148769043480576.0000 exploreP:0.0100\n",
      "Episode:471 Steps:1000 meanR:0.6117 R:0.9200 gloss:-129796459960162418688.0000 gloss1-lgP:462539.1087 gloss2gQs:-281663297533474.2500 gloss3dQs:-176557171823870.2188 gloss4tgtQ:-278319623997657.0625 dloss:9591515878725497415429783552.0000 exploreP:0.0100\n",
      "Episode:472 Steps:1000 meanR:0.6080 R:0.3900 gloss:-181008507584034570240.0000 gloss1-lgP:469183.1116 gloss2gQs:-367079823408026.7500 gloss3dQs:-231105847033723.4062 gloss4tgtQ:-362725922719994.2500 dloss:16098925275198580903787888640.0000 exploreP:0.0100\n",
      "Episode:473 Steps:1000 meanR:0.6076 R:0.6700 gloss:-245413285185018167296.0000 gloss1-lgP:467546.2313 gloss2gQs:-467938678040025.9375 gloss3dQs:-297725182381757.7500 gloss4tgtQ:-462460516239330.0625 dloss:25511563822802244042963812352.0000 exploreP:0.0100\n",
      "Episode:474 Steps:1000 meanR:0.6134 R:0.9000 gloss:-315721629812261650432.0000 gloss1-lgP:475675.4524 gloss2gQs:-578309254304446.7500 gloss3dQs:-376776600151148.5625 gloss4tgtQ:-571279907173268.8750 dloss:36410051876148057690523828224.0000 exploreP:0.0100\n",
      "Episode:475 Steps:1000 meanR:0.6121 R:0.6100 gloss:-344373296022818979840.0000 gloss1-lgP:429066.1079 gloss2gQs:-710914927970027.7500 gloss3dQs:-475885569330720.3750 gloss4tgtQ:-702712590111775.5000 dloss:49852301945016878225472618496.0000 exploreP:0.0100\n",
      "Episode:476 Steps:1000 meanR:0.6053 R:0.2400 gloss:-408624064894645960704.0000 gloss1-lgP:417554.7839 gloss2gQs:-878452721467518.1250 gloss3dQs:-602249750293516.8750 gloss4tgtQ:-867377622393905.8750 dloss:69755388945415080694727573504.0000 exploreP:0.0100\n",
      "Episode:477 Steps:1000 meanR:0.5906 R:0.0000 gloss:-572858311649614954496.0000 gloss1-lgP:458252.3215 gloss2gQs:-1055251655008193.0000 gloss3dQs:-731008035474584.7500 gloss4tgtQ:-1043272046719856.0000 dloss:96901070315676953536008028160.0000 exploreP:0.0100\n",
      "Episode:478 Steps:1000 meanR:0.5797 R:0.4900 gloss:-751158737012286029824.0000 gloss1-lgP:488298.6834 gloss2gQs:-1273237230962019.5000 gloss3dQs:-882908109284635.8750 gloss4tgtQ:-1258918204073517.5000 dloss:140650289999324760429343801344.0000 exploreP:0.0100\n",
      "Episode:479 Steps:1000 meanR:0.5787 R:0.3400 gloss:-931040102584410701824.0000 gloss1-lgP:490009.5674 gloss2gQs:-1534257652002258.5000 gloss3dQs:-1077453284351341.6250 gloss4tgtQ:-1515690100005306.5000 dloss:196021967076661780786775588864.0000 exploreP:0.0100\n",
      "Episode:480 Steps:1000 meanR:0.5814 R:1.4200 gloss:-1191666507069360701440.0000 gloss1-lgP:523208.0958 gloss2gQs:-1842653991429968.0000 gloss3dQs:-1317248229732127.5000 gloss4tgtQ:-1821177182938365.7500 dloss:263055264461927606068776534016.0000 exploreP:0.0100\n",
      "Episode:481 Steps:1000 meanR:0.5711 R:0.0600 gloss:-1755492661105572970496.0000 gloss1-lgP:604454.6132 gloss2gQs:-2220485701290501.0000 gloss3dQs:-1596094737990174.0000 gloss4tgtQ:-2195109627350303.5000 dloss:379296431133007168380304621568.0000 exploreP:0.0100\n",
      "Episode:482 Steps:1000 meanR:0.5679 R:0.1500 gloss:-2391489988520893218816.0000 gloss1-lgP:671135.6730 gloss2gQs:-2683012683846931.0000 gloss3dQs:-1938006685932945.5000 gloss4tgtQ:-2652381713647042.0000 dloss:535556457283620086938528120832.0000 exploreP:0.0100\n",
      "Episode:483 Steps:1000 meanR:0.5595 R:0.0000 gloss:-3115770348810595205120.0000 gloss1-lgP:724069.9559 gloss2gQs:-3175444315267384.0000 gloss3dQs:-2297907297656449.0000 gloss4tgtQ:-3137199944038249.5000 dloss:748746403979147319218054103040.0000 exploreP:0.0100\n",
      "Episode:484 Steps:1000 meanR:0.5582 R:0.4800 gloss:-4265471173886288068608.0000 gloss1-lgP:823716.6811 gloss2gQs:-3741948031606496.5000 gloss3dQs:-2717704361124746.5000 gloss4tgtQ:-3698761959268751.5000 dloss:1032615277103391023961915523072.0000 exploreP:0.0100\n",
      "Episode:485 Steps:1000 meanR:0.5524 R:0.3000 gloss:-5081902890503026769920.0000 gloss1-lgP:834419.6151 gloss2gQs:-4400508791061093.0000 gloss3dQs:-3207918154615106.0000 gloss4tgtQ:-4346856541625616.5000 dloss:1421179178845606861356878790656.0000 exploreP:0.0100\n",
      "Episode:486 Steps:1000 meanR:0.5535 R:0.3100 gloss:-6680390384572937797632.0000 gloss1-lgP:944791.5533 gloss2gQs:-5145850622488493.0000 gloss3dQs:-3786449634873583.5000 gloss4tgtQ:-5086191315218660.0000 dloss:1856487086359673244130306162688.0000 exploreP:0.0100\n",
      "Episode:487 Steps:1000 meanR:0.5573 R:0.5800 gloss:-8142687758577825742848.0000 gloss1-lgP:984162.9436 gloss2gQs:-6114595108995496.0000 gloss3dQs:-4536426820141036.0000 gloss4tgtQ:-6042841865227074.0000 dloss:2521275810333746622478589886464.0000 exploreP:0.0100\n",
      "Episode:488 Steps:1000 meanR:0.5564 R:0.5000 gloss:-10349277864899884613632.0000 gloss1-lgP:1064449.3628 gloss2gQs:-7445086547681325.0000 gloss3dQs:-5538212938504442.0000 gloss4tgtQ:-7357095052556862.0000 dloss:3630409273334277404933796921344.0000 exploreP:0.0100\n",
      "Episode:489 Steps:1000 meanR:0.5506 R:0.5600 gloss:-13786022670725537595392.0000 gloss1-lgP:1182888.2193 gloss2gQs:-8780972196294162.0000 gloss3dQs:-6552623684675838.0000 gloss4tgtQ:-8677480804514743.0000 dloss:4916215433847732374534585581568.0000 exploreP:0.0100\n",
      "Episode:490 Steps:1000 meanR:0.5568 R:1.4100 gloss:-16355514228408169005056.0000 gloss1-lgP:1244415.3742 gloss2gQs:-10298573834666794.0000 gloss3dQs:-7710500432261826.0000 gloss4tgtQ:-10177975445927238.0000 dloss:6481110998770493269744233218048.0000 exploreP:0.0100\n",
      "Episode:491 Steps:1000 meanR:0.5488 R:0.3500 gloss:-18198377436590043561984.0000 gloss1-lgP:1242458.9529 gloss2gQs:-11890248491758566.0000 gloss3dQs:-8913759428692230.0000 gloss4tgtQ:-11753371617304540.0000 dloss:8475685422688769219249125195776.0000 exploreP:0.0100\n",
      "Episode:492 Steps:1000 meanR:0.5469 R:0.2700 gloss:-21775657930852065607680.0000 gloss1-lgP:1265084.2984 gloss2gQs:-13952420493495200.0000 gloss3dQs:-10522133887235012.0000 gloss4tgtQ:-13790435176959222.0000 dloss:11143712667082213331554797617152.0000 exploreP:0.0100\n",
      "Episode:493 Steps:1000 meanR:0.5516 R:0.8100 gloss:-29119889722629657985024.0000 gloss1-lgP:1447233.8917 gloss2gQs:-16149084889181930.0000 gloss3dQs:-12096842589773712.0000 gloss4tgtQ:-15955068392351176.0000 dloss:15571710592697123958574513389568.0000 exploreP:0.0100\n",
      "Episode:494 Steps:1000 meanR:0.5541 R:0.3600 gloss:-39859244260788138934272.0000 gloss1-lgP:1667794.5784 gloss2gQs:-18858458353689964.0000 gloss3dQs:-14033129298869798.0000 gloss4tgtQ:-18643033371561492.0000 dloss:22383414536794430598396208218112.0000 exploreP:0.0100\n",
      "Episode:495 Steps:1000 meanR:0.5487 R:0.3600 gloss:-45518682489322045702144.0000 gloss1-lgP:1656741.6528 gloss2gQs:-21611946828164280.0000 gloss3dQs:-16011540659758918.0000 gloss4tgtQ:-21345483175372888.0000 dloss:30299646039387211542782864785408.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:496 Steps:1000 meanR:0.5486 R:0.3100 gloss:-57078371794124877594624.0000 gloss1-lgP:1766488.3446 gloss2gQs:-24419032208122844.0000 gloss3dQs:-18109829881161100.0000 gloss4tgtQ:-24128788624429884.0000 dloss:39253739489828084327179817582592.0000 exploreP:0.0100\n",
      "Episode:497 Steps:1000 meanR:0.5522 R:0.5100 gloss:-62403369001556862042112.0000 gloss1-lgP:1733960.3832 gloss2gQs:-27809915629088436.0000 gloss3dQs:-20807040070530576.0000 gloss4tgtQ:-27473339613812796.0000 dloss:49580658287707576124750291795968.0000 exploreP:0.0100\n",
      "Episode:498 Steps:1000 meanR:0.5512 R:0.4900 gloss:-71768068995239852900352.0000 gloss1-lgP:1776636.2926 gloss2gQs:-31202195932580092.0000 gloss3dQs:-23599809193760600.0000 gloss4tgtQ:-30831497592582452.0000 dloss:60801269328663570224361710288896.0000 exploreP:0.0100\n",
      "Episode:499 Steps:1000 meanR:0.5452 R:0.2100 gloss:-82836101708176485777408.0000 gloss1-lgP:1828124.9071 gloss2gQs:-35177357805297532.0000 gloss3dQs:-26943005582798560.0000 gloss4tgtQ:-34758292949665784.0000 dloss:73402570006540094304969092497408.0000 exploreP:0.0100\n",
      "Episode:500 Steps:1000 meanR:0.5446 R:0.7900 gloss:-104449494864866953920512.0000 gloss1-lgP:1983009.6758 gloss2gQs:-39598884625667768.0000 gloss3dQs:-30503678719632668.0000 gloss4tgtQ:-39138892189420392.0000 dloss:90458805459209541783260871262208.0000 exploreP:0.0100\n",
      "Episode:501 Steps:1000 meanR:0.5429 R:0.6500 gloss:-115541377894290129158144.0000 gloss1-lgP:1957769.7236 gloss2gQs:-45319554344067984.0000 gloss3dQs:-35175874616139932.0000 gloss4tgtQ:-44805962217996616.0000 dloss:111746667800694646514418618204160.0000 exploreP:0.0100\n",
      "Episode:502 Steps:1000 meanR:0.5357 R:0.0000 gloss:-135903801779432877719552.0000 gloss1-lgP:2137064.4372 gloss2gQs:-51138910437634216.0000 gloss3dQs:-40099738821048776.0000 gloss4tgtQ:-50528795748991696.0000 dloss:134510041884032309136781662486528.0000 exploreP:0.0100\n",
      "Episode:503 Steps:1000 meanR:0.5392 R:0.6700 gloss:-148849162406271261868032.0000 gloss1-lgP:2159137.7137 gloss2gQs:-58366926226129816.0000 gloss3dQs:-45904613626188424.0000 gloss4tgtQ:-57673383319913128.0000 dloss:170383752196838968574864286285824.0000 exploreP:0.0100\n",
      "Episode:504 Steps:1000 meanR:0.5474 R:1.2800 gloss:-156699596715820394217472.0000 gloss1-lgP:2021052.4697 gloss2gQs:-66372757599151224.0000 gloss3dQs:-52551863083587464.0000 gloss4tgtQ:-65605563628258096.0000 dloss:207024872129759297045270928293888.0000 exploreP:0.0100\n",
      "Episode:505 Steps:1000 meanR:0.5463 R:0.8100 gloss:-197448375932124624060416.0000 gloss1-lgP:2249274.8249 gloss2gQs:-76194687333485824.0000 gloss3dQs:-60271496754925296.0000 gloss4tgtQ:-75322786441322336.0000 dloss:273221376427154396271120178216960.0000 exploreP:0.0100\n",
      "Episode:506 Steps:1000 meanR:0.5513 R:0.9700 gloss:-252001619438949569134592.0000 gloss1-lgP:2500063.8896 gloss2gQs:-87971618921771744.0000 gloss3dQs:-69258706826655072.0000 gloss4tgtQ:-86963853342700096.0000 dloss:365956689916694595397613980221440.0000 exploreP:0.0100\n",
      "Episode:507 Steps:1000 meanR:0.5571 R:0.9700 gloss:-280771641858862560051200.0000 gloss1-lgP:2578554.4559 gloss2gQs:-98350508548212992.0000 gloss3dQs:-77370215329799280.0000 gloss4tgtQ:-97174472772503888.0000 dloss:463302596799102367936289922088960.0000 exploreP:0.0100\n",
      "Episode:508 Steps:1000 meanR:0.5553 R:0.4000 gloss:-309085408645217775517696.0000 gloss1-lgP:2473273.8463 gloss2gQs:-110624337804278160.0000 gloss3dQs:-87266875681268480.0000 gloss4tgtQ:-109343710187080608.0000 dloss:571644154729527891574240258818048.0000 exploreP:0.0100\n",
      "Episode:509 Steps:1000 meanR:0.5697 R:1.6000 gloss:-386306415687252404338688.0000 gloss1-lgP:2775276.6042 gloss2gQs:-123398507590210608.0000 gloss3dQs:-97709855015719120.0000 gloss4tgtQ:-121983013943581376.0000 dloss:692986160149011474256533901017088.0000 exploreP:0.0100\n",
      "Episode:510 Steps:1000 meanR:0.5672 R:0.4400 gloss:-444424183670572153044992.0000 gloss1-lgP:2888023.4667 gloss2gQs:-136549313707330384.0000 gloss3dQs:-107463408486362176.0000 gloss4tgtQ:-134901031361062272.0000 dloss:874088727769022050946607453569024.0000 exploreP:0.0100\n",
      "Episode:511 Steps:1000 meanR:0.5668 R:0.2300 gloss:-507498955149677195952128.0000 gloss1-lgP:2987677.2301 gloss2gQs:-150094851558434752.0000 gloss3dQs:-118082918389651328.0000 gloss4tgtQ:-148423186814760192.0000 dloss:1084563318375273100095733989638144.0000 exploreP:0.0100\n",
      "Episode:512 Steps:1000 meanR:0.5579 R:0.5300 gloss:-627665572098193552834560.0000 gloss1-lgP:3190904.1835 gloss2gQs:-167713870202045312.0000 gloss3dQs:-132508320525179264.0000 gloss4tgtQ:-165759376733408096.0000 dloss:1327575302619218619864177316986880.0000 exploreP:0.0100\n",
      "Episode:513 Steps:1000 meanR:0.5544 R:0.2500 gloss:-727207302207391951486976.0000 gloss1-lgP:3258187.5969 gloss2gQs:-185410017012854976.0000 gloss3dQs:-146401096975672736.0000 gloss4tgtQ:-183290032263568512.0000 dloss:1648220749120948674025248083935232.0000 exploreP:0.0100\n",
      "Episode:514 Steps:1000 meanR:0.5608 R:1.3900 gloss:-752026127991764779270144.0000 gloss1-lgP:3114345.0941 gloss2gQs:-203332489881419392.0000 gloss3dQs:-162065090201926144.0000 gloss4tgtQ:-200935547342615456.0000 dloss:1895540213170815810878247014498304.0000 exploreP:0.0100\n",
      "Episode:515 Steps:1000 meanR:0.5628 R:0.8900 gloss:-973023131200357243289600.0000 gloss1-lgP:3678398.6110 gloss2gQs:-223777320825338304.0000 gloss3dQs:-178526852413371808.0000 gloss4tgtQ:-221111826496765312.0000 dloss:2310540788551962649732586676420608.0000 exploreP:0.0100\n",
      "Episode:516 Steps:1000 meanR:0.5536 R:0.1400 gloss:-1184248516803829218410496.0000 gloss1-lgP:3885400.4559 gloss2gQs:-249557571621596768.0000 gloss3dQs:-200103455940773952.0000 gloss4tgtQ:-246705933916953824.0000 dloss:2769469449591179132387223016570880.0000 exploreP:0.0100\n",
      "Episode:517 Steps:1000 meanR:0.5507 R:0.0000 gloss:-1327645106932582480609280.0000 gloss1-lgP:3940244.1219 gloss2gQs:-273545191261630336.0000 gloss3dQs:-219680841575514272.0000 gloss4tgtQ:-270362544185470560.0000 dloss:3272883020909858065992287137038336.0000 exploreP:0.0100\n",
      "Episode:518 Steps:1000 meanR:0.5623 R:1.2000 gloss:-1501489963966392256430080.0000 gloss1-lgP:3975504.6634 gloss2gQs:-301511203734538432.0000 gloss3dQs:-243049631427981280.0000 gloss4tgtQ:-298140165730533568.0000 dloss:3892587838615911151844185782353920.0000 exploreP:0.0100\n",
      "Episode:519 Steps:1000 meanR:0.5584 R:0.2600 gloss:-1709344795466930776440832.0000 gloss1-lgP:4018080.7195 gloss2gQs:-330549080651972416.0000 gloss3dQs:-267758909723944704.0000 gloss4tgtQ:-326782920850908416.0000 dloss:4668930632639061066906361187532800.0000 exploreP:0.0100\n",
      "Episode:520 Steps:1000 meanR:0.5518 R:0.3700 gloss:-1894965701704917358477312.0000 gloss1-lgP:4057025.7126 gloss2gQs:-364627394156663296.0000 gloss3dQs:-296173771942335680.0000 gloss4tgtQ:-360384150500862016.0000 dloss:5612313073327340989556310688137216.0000 exploreP:0.0100\n",
      "Episode:521 Steps:1000 meanR:0.5488 R:0.3800 gloss:-2646251797145125757386752.0000 gloss1-lgP:5165074.7880 gloss2gQs:-403361086576249152.0000 gloss3dQs:-325709808210676544.0000 gloss4tgtQ:-398706649146156288.0000 dloss:7218221815206686888160770922643456.0000 exploreP:0.0100\n",
      "Episode:522 Steps:1000 meanR:0.5595 R:1.3700 gloss:-2598906963222541409189888.0000 gloss1-lgP:4801471.5777 gloss2gQs:-436511599301742336.0000 gloss3dQs:-354063144814113856.0000 gloss4tgtQ:-431571455893365952.0000 dloss:8359650201063381803142554312507392.0000 exploreP:0.0100\n",
      "Episode:523 Steps:1000 meanR:0.5705 R:1.1000 gloss:-3364657121283138606071808.0000 gloss1-lgP:5425464.3783 gloss2gQs:-481799498850678400.0000 gloss3dQs:-391793574887892352.0000 gloss4tgtQ:-476266545335793664.0000 dloss:10321240712828399839678211827957760.0000 exploreP:0.0100\n",
      "Episode:524 Steps:1000 meanR:0.5688 R:0.2600 gloss:-3865004753091717297602560.0000 gloss1-lgP:5775795.3550 gloss2gQs:-528696149402914240.0000 gloss3dQs:-431007530012699904.0000 gloss4tgtQ:-522448617959011648.0000 dloss:12190465786341174944736423027146752.0000 exploreP:0.0100\n",
      "Episode:525 Steps:1000 meanR:0.5676 R:0.7500 gloss:-4343823249423715906093056.0000 gloss1-lgP:5984078.6547 gloss2gQs:-578649200398948480.0000 gloss3dQs:-473316005001025664.0000 gloss4tgtQ:-572050511276290432.0000 dloss:14393867074501485330741106380374016.0000 exploreP:0.0100\n",
      "Episode:526 Steps:1000 meanR:0.5664 R:0.3200 gloss:-4097915781141003329601536.0000 gloss1-lgP:5690822.3559 gloss2gQs:-624321619155766144.0000 gloss3dQs:-514387144454694976.0000 gloss4tgtQ:-617057812180306048.0000 dloss:15933838878957277696272592095150080.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:527 Steps:1000 meanR:0.5658 R:0.1700 gloss:-4570197963924361336324096.0000 gloss1-lgP:5784517.5140 gloss2gQs:-686683570986719104.0000 gloss3dQs:-567945526083252736.0000 gloss4tgtQ:-679094849240891264.0000 dloss:18917258018837394334782242963849216.0000 exploreP:0.0100\n",
      "Episode:528 Steps:1000 meanR:0.5670 R:0.2800 gloss:-5720446688713580954517504.0000 gloss1-lgP:6563813.8983 gloss2gQs:-755437423437328768.0000 gloss3dQs:-625616058353912192.0000 gloss4tgtQ:-746885384289138816.0000 dloss:22442342898435452510137489219911680.0000 exploreP:0.0100\n",
      "Episode:529 Steps:1000 meanR:0.5659 R:0.2100 gloss:-5992420106050872088199168.0000 gloss1-lgP:6369738.9683 gloss2gQs:-813949846114775936.0000 gloss3dQs:-674218896390951552.0000 gloss4tgtQ:-804534468215406464.0000 dloss:25520748345763877992806005061189632.0000 exploreP:0.0100\n",
      "Episode:530 Steps:1000 meanR:0.5728 R:0.9300 gloss:-6369959969266938968801280.0000 gloss1-lgP:6316198.6117 gloss2gQs:-882014489117150848.0000 gloss3dQs:-730355037116145536.0000 gloss4tgtQ:-872607508372788224.0000 dloss:29156380002439663281922770364006400.0000 exploreP:0.0100\n",
      "Episode:531 Steps:1000 meanR:0.5744 R:0.4900 gloss:-7459652022632618422960128.0000 gloss1-lgP:6714256.3154 gloss2gQs:-984533938719338624.0000 gloss3dQs:-817712542814492032.0000 gloss4tgtQ:-973207757184701056.0000 dloss:35602869809104942324621607409025024.0000 exploreP:0.0100\n",
      "Episode:532 Steps:1000 meanR:0.5672 R:0.3400 gloss:-8537833839662129462902784.0000 gloss1-lgP:7015337.1355 gloss2gQs:-1090630701692017024.0000 gloss3dQs:-905311454128792960.0000 gloss4tgtQ:-1077606968360330752.0000 dloss:42730727127532366270910301309239296.0000 exploreP:0.0100\n",
      "Episode:533 Steps:1000 meanR:0.5646 R:0.0000 gloss:-9526711807507113596420096.0000 gloss1-lgP:7352525.5272 gloss2gQs:-1193056979971787776.0000 gloss3dQs:-990382205208081792.0000 gloss4tgtQ:-1179372705251838208.0000 dloss:49986477148629650323120814116831232.0000 exploreP:0.0100\n",
      "Episode:534 Steps:1000 meanR:0.5612 R:0.8800 gloss:-11586851284221975672651776.0000 gloss1-lgP:7739849.8564 gloss2gQs:-1302756906301517568.0000 gloss3dQs:-1080458728604364288.0000 gloss4tgtQ:-1287270731680457728.0000 dloss:60250894671675047598028379769536512.0000 exploreP:0.0100\n",
      "Episode:535 Steps:1000 meanR:0.5617 R:0.5700 gloss:-13280224474999412129529856.0000 gloss1-lgP:8041254.5891 gloss2gQs:-1405008784117026816.0000 gloss3dQs:-1158493663635747328.0000 gloss4tgtQ:-1388822452546103808.0000 dloss:73485309042284335716644358599999488.0000 exploreP:0.0100\n",
      "Episode:536 Steps:1000 meanR:0.5560 R:0.2200 gloss:-18088784454728584506900480.0000 gloss1-lgP:9808056.7420 gloss2gQs:-1519236123241744128.0000 gloss3dQs:-1246389884168538880.0000 gloss4tgtQ:-1500214703548612608.0000 dloss:90574790169018641076515759326232576.0000 exploreP:0.0100\n",
      "Episode:537 Steps:1000 meanR:0.5597 R:0.5000 gloss:-19778954624729517580615680.0000 gloss1-lgP:9246774.7899 gloss2gQs:-1644477517118924544.0000 gloss3dQs:-1349657190248176384.0000 gloss4tgtQ:-1625985755409530624.0000 dloss:105963226967485897515382859880202240.0000 exploreP:0.0100\n",
      "Episode:538 Steps:1000 meanR:0.5583 R:0.1800 gloss:-23164243630055450277314560.0000 gloss1-lgP:10039095.1631 gloss2gQs:-1769782479717560064.0000 gloss3dQs:-1458588263117794816.0000 gloss4tgtQ:-1748753074988371200.0000 dloss:121742742272510222215704735175933952.0000 exploreP:0.0100\n",
      "Episode:539 Steps:1000 meanR:0.5628 R:0.6300 gloss:-28885464767727390039212032.0000 gloss1-lgP:11656049.3690 gloss2gQs:-1947617955371333888.0000 gloss3dQs:-1597317882191078144.0000 gloss4tgtQ:-1925033886600851456.0000 dloss:155601306618596218509806050733981696.0000 exploreP:0.0100\n",
      "Episode:540 Steps:1000 meanR:0.5648 R:0.6200 gloss:-31915091892966432719241216.0000 gloss1-lgP:12028400.2659 gloss2gQs:-2128164760329991424.0000 gloss3dQs:-1743566280433224448.0000 gloss4tgtQ:-2103144601519653632.0000 dloss:184855940733753318030748103055769600.0000 exploreP:0.0100\n",
      "Episode:541 Steps:1000 meanR:0.5612 R:0.2500 gloss:-38271960875602315562713088.0000 gloss1-lgP:12649212.6188 gloss2gQs:-2263291538278324736.0000 gloss3dQs:-1859676768361522944.0000 gloss4tgtQ:-2235869827702543616.0000 dloss:211249095599576337508334563800121344.0000 exploreP:0.0100\n",
      "Episode:542 Steps:1000 meanR:0.5652 R:0.5600 gloss:-43737691880632239807004672.0000 gloss1-lgP:12897818.3118 gloss2gQs:-2433365046386161152.0000 gloss3dQs:-2006893617723244032.0000 gloss4tgtQ:-2403764225591944704.0000 dloss:244840707945217795290556734145298432.0000 exploreP:0.0100\n",
      "Episode:543 Steps:1000 meanR:0.5720 R:0.6800 gloss:-46176673500174483623772160.0000 gloss1-lgP:12889682.5742 gloss2gQs:-2608632754179150848.0000 gloss3dQs:-2162407780928244736.0000 gloss4tgtQ:-2576678516276027392.0000 dloss:277631486308365117698001910924050432.0000 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch, gloss3_batch, gloss4_batch = [], [], [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(1111111111):\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1/1 # 1000 episode length\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p >= np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, gloss1, gloss2, gloss3, gloss4, _, _ = sess.run([model.g_loss, model.d_loss,\n",
    "                                                                           model.g_loss1, model.g_loss2, \n",
    "                                                                           model.g_loss3, model.g_loss4,\n",
    "                                                                           model.g_opt, model.d_opt],\n",
    "                                                                          feed_dict = {model.states: states, \n",
    "                                                                                       model.actions: actions,\n",
    "                                                                                       model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            gloss3_batch.append(gloss3)\n",
    "            gloss4_batch.append(gloss4)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'Steps:{}'.format(num_steps),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'gloss1-lgP:{:.4f}'.format(np.mean(gloss1_batch)), #-logp\n",
    "              'gloss2gQs:{:.4f}'.format(np.mean(gloss2_batch)),#gQs\n",
    "              'gloss3dQs:{:.4f}'.format(np.mean(gloss3_batch)),#dQs\n",
    "              'gloss4tgtQ:{:.4f}'.format(np.mean(gloss4_batch)),#tgtQs\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
