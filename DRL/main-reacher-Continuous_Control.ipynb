{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) 0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(action.shape, reward)\n",
    "        print(done)\n",
    "        break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    # G\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    #actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    actions_labels = tf.nn.sigmoid(actions)\n",
    "    # neg_log_prob_actions = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "    #                                                                   labels=actions_labels)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=actions_labels)\n",
    "    #g_loss = tf.reduce_mean(neg_log_prob_actions * targetQs) # error!\n",
    "    \n",
    "    # D\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * Qs)\n",
    "    return actions_logits, Qs, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:3.0\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "# state_size = 37\n",
    "# state_size_ = (84, 84, 3)\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "gamma = 0.99                   # future reward discount\n",
    "memory_size = 1000            # memory capacity\n",
    "batch_size = 1000             # experience mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0700 R:0.0700 gloss:-0.5387 dloss:0.0776 exploreP:0.9057\n",
      "Episode:1 meanR:0.2200 R:0.3700 gloss:1.4941 dloss:0.0100 exploreP:0.8204\n",
      "Episode:2 meanR:0.4133 R:0.8000 gloss:4.5103 dloss:0.0052 exploreP:0.7432\n",
      "Episode:3 meanR:0.3700 R:0.2400 gloss:7.0446 dloss:0.0052 exploreP:0.6734\n",
      "Episode:4 meanR:0.3920 R:0.4800 gloss:1.7065 dloss:0.0094 exploreP:0.6102\n",
      "Episode:5 meanR:0.4600 R:0.8000 gloss:6.8126 dloss:0.0071 exploreP:0.5530\n",
      "Episode:6 meanR:0.5229 R:0.9000 gloss:5.9324 dloss:0.0056 exploreP:0.5013\n",
      "Episode:7 meanR:0.4925 R:0.2800 gloss:3.6661 dloss:0.0102 exploreP:0.4545\n",
      "Episode:8 meanR:0.4722 R:0.3100 gloss:3.1590 dloss:0.0248 exploreP:0.4121\n",
      "Episode:9 meanR:0.4940 R:0.6900 gloss:3.6598 dloss:0.0309 exploreP:0.3738\n",
      "Episode:10 meanR:0.5064 R:0.6300 gloss:8.2172 dloss:0.0329 exploreP:0.3392\n",
      "Episode:11 meanR:0.4642 R:0.0000 gloss:3.7638 dloss:0.0406 exploreP:0.3078\n",
      "Episode:12 meanR:0.4423 R:0.1800 gloss:1.1668 dloss:0.1349 exploreP:0.2795\n",
      "Episode:13 meanR:0.4271 R:0.2300 gloss:3.2819 dloss:0.0999 exploreP:0.2538\n",
      "Episode:14 meanR:0.4033 R:0.0700 gloss:1.0211 dloss:0.1928 exploreP:0.2306\n",
      "Episode:15 meanR:0.3781 R:0.0000 gloss:-0.2624 dloss:0.1764 exploreP:0.2096\n",
      "Episode:16 meanR:0.3776 R:0.3700 gloss:3.5916 dloss:0.2576 exploreP:0.1905\n",
      "Episode:17 meanR:0.3567 R:0.0000 gloss:0.9269 dloss:0.3977 exploreP:0.1734\n",
      "Episode:18 meanR:0.3379 R:0.0000 gloss:3.2796 dloss:0.2222 exploreP:0.1578\n",
      "Episode:19 meanR:0.3210 R:0.0000 gloss:-0.7460 dloss:0.3180 exploreP:0.1437\n",
      "Episode:20 meanR:0.3238 R:0.3800 gloss:-1.8043 dloss:0.2725 exploreP:0.1310\n",
      "Episode:21 meanR:0.3091 R:0.0000 gloss:1.6349 dloss:0.3729 exploreP:0.1195\n",
      "Episode:22 meanR:0.2983 R:0.0600 gloss:-3.7366 dloss:0.2959 exploreP:0.1090\n",
      "Episode:23 meanR:0.2858 R:0.0000 gloss:-6.2281 dloss:0.5094 exploreP:0.0996\n",
      "Episode:24 meanR:0.2744 R:0.0000 gloss:-5.0623 dloss:0.5148 exploreP:0.0911\n",
      "Episode:25 meanR:0.2638 R:0.0000 gloss:-10.5291 dloss:0.6767 exploreP:0.0833\n",
      "Episode:26 meanR:0.2585 R:0.1200 gloss:-18.2720 dloss:0.5548 exploreP:0.0764\n",
      "Episode:27 meanR:0.2493 R:0.0000 gloss:-5.7896 dloss:0.8868 exploreP:0.0700\n",
      "Episode:28 meanR:0.2407 R:0.0000 gloss:19.4667 dloss:1.6108 exploreP:0.0643\n",
      "Episode:29 meanR:0.2327 R:0.0000 gloss:28.1202 dloss:1.4679 exploreP:0.0591\n",
      "Episode:30 meanR:0.2252 R:0.0000 gloss:-7.4598 dloss:2.0508 exploreP:0.0545\n",
      "Episode:31 meanR:0.2294 R:0.3600 gloss:-24.2588 dloss:2.7670 exploreP:0.0502\n",
      "Episode:32 meanR:0.2282 R:0.1900 gloss:-1.3661 dloss:0.2622 exploreP:0.0464\n",
      "Episode:33 meanR:0.2268 R:0.1800 gloss:2.2006 dloss:0.1697 exploreP:0.0429\n",
      "Episode:34 meanR:0.2460 R:0.9000 gloss:-2.2375 dloss:0.0816 exploreP:0.0398\n",
      "Episode:35 meanR:0.2528 R:0.4900 gloss:1.3834 dloss:0.3860 exploreP:0.0370\n",
      "Episode:36 meanR:0.2722 R:0.9700 gloss:-0.4954 dloss:0.5044 exploreP:0.0344\n",
      "Episode:37 meanR:0.2866 R:0.8200 gloss:-2.9323 dloss:0.7786 exploreP:0.0321\n",
      "Episode:38 meanR:0.2974 R:0.7100 gloss:0.8432 dloss:0.0603 exploreP:0.0300\n",
      "Episode:39 meanR:0.3065 R:0.6600 gloss:0.8730 dloss:0.5006 exploreP:0.0281\n",
      "Episode:40 meanR:0.3132 R:0.5800 gloss:1.1510 dloss:0.4606 exploreP:0.0263\n",
      "Episode:41 meanR:0.3131 R:0.3100 gloss:-0.1120 dloss:0.5813 exploreP:0.0248\n",
      "Episode:42 meanR:0.3223 R:0.7100 gloss:0.1348 dloss:0.4249 exploreP:0.0234\n",
      "Episode:43 meanR:0.3284 R:0.5900 gloss:0.7083 dloss:0.0461 exploreP:0.0221\n",
      "Episode:44 meanR:0.3371 R:0.7200 gloss:1.5184 dloss:0.8033 exploreP:0.0209\n",
      "Episode:45 meanR:0.3500 R:0.9300 gloss:3.0168 dloss:0.7446 exploreP:0.0199\n",
      "Episode:46 meanR:0.3426 R:0.0000 gloss:0.8978 dloss:0.0529 exploreP:0.0190\n",
      "Episode:47 meanR:0.3419 R:0.3100 gloss:0.2950 dloss:0.9994 exploreP:0.0181\n",
      "Episode:48 meanR:0.3476 R:0.6200 gloss:-0.1248 dloss:0.6009 exploreP:0.0173\n",
      "Episode:49 meanR:0.3502 R:0.4800 gloss:1.2147 dloss:0.8698 exploreP:0.0166\n",
      "Episode:50 meanR:0.3469 R:0.1800 gloss:0.8465 dloss:0.2900 exploreP:0.0160\n",
      "Episode:51 meanR:0.3483 R:0.4200 gloss:-0.2369 dloss:0.2327 exploreP:0.0154\n",
      "Episode:52 meanR:0.3428 R:0.0600 gloss:1.7428 dloss:0.7479 exploreP:0.0149\n",
      "Episode:53 meanR:0.3559 R:1.0500 gloss:-0.6615 dloss:0.5127 exploreP:0.0144\n",
      "Episode:54 meanR:0.3515 R:0.1100 gloss:-0.6675 dloss:0.6171 exploreP:0.0140\n",
      "Episode:55 meanR:0.3623 R:0.9600 gloss:-0.8889 dloss:0.9443 exploreP:0.0136\n",
      "Episode:56 meanR:0.3595 R:0.2000 gloss:-0.5883 dloss:0.8172 exploreP:0.0133\n",
      "Episode:57 meanR:0.3816 R:1.6400 gloss:-0.5455 dloss:0.0722 exploreP:0.0130\n",
      "Episode:58 meanR:0.3995 R:1.4400 gloss:0.3004 dloss:0.6990 exploreP:0.0127\n",
      "Episode:59 meanR:0.4028 R:0.6000 gloss:0.1141 dloss:0.2425 exploreP:0.0124\n",
      "Episode:60 meanR:0.4070 R:0.6600 gloss:0.2466 dloss:1.1102 exploreP:0.0122\n",
      "Episode:61 meanR:0.4129 R:0.7700 gloss:0.4567 dloss:0.0532 exploreP:0.0120\n",
      "Episode:62 meanR:0.4160 R:0.6100 gloss:0.1395 dloss:0.7721 exploreP:0.0118\n",
      "Episode:63 meanR:0.4127 R:0.2000 gloss:0.4141 dloss:1.2412 exploreP:0.0116\n",
      "Episode:64 meanR:0.4298 R:1.5300 gloss:-0.2645 dloss:0.8993 exploreP:0.0115\n",
      "Episode:65 meanR:0.4274 R:0.2700 gloss:-0.0657 dloss:0.0595 exploreP:0.0113\n",
      "Episode:66 meanR:0.4384 R:1.1600 gloss:0.7059 dloss:0.7649 exploreP:0.0112\n",
      "Episode:67 meanR:0.4428 R:0.7400 gloss:0.5791 dloss:1.0710 exploreP:0.0111\n",
      "Episode:68 meanR:0.4422 R:0.4000 gloss:-0.0010 dloss:0.0281 exploreP:0.0110\n",
      "Episode:69 meanR:0.4411 R:0.3700 gloss:-0.4883 dloss:0.9635 exploreP:0.0109\n",
      "Episode:70 meanR:0.4499 R:1.0600 gloss:-0.6612 dloss:0.0259 exploreP:0.0108\n",
      "Episode:71 meanR:0.4744 R:2.2200 gloss:-0.1788 dloss:1.3049 exploreP:0.0107\n",
      "Episode:72 meanR:0.4866 R:1.3600 gloss:0.0183 dloss:0.0324 exploreP:0.0107\n",
      "Episode:73 meanR:0.4832 R:0.2400 gloss:0.4425 dloss:0.3156 exploreP:0.0106\n",
      "Episode:74 meanR:0.4869 R:0.7600 gloss:0.3033 dloss:0.0740 exploreP:0.0105\n",
      "Episode:75 meanR:0.4900 R:0.7200 gloss:-0.0217 dloss:0.2887 exploreP:0.0105\n",
      "Episode:76 meanR:0.4869 R:0.2500 gloss:0.0109 dloss:0.2661 exploreP:0.0104\n",
      "Episode:77 meanR:0.4864 R:0.4500 gloss:0.0714 dloss:0.4353 exploreP:0.0104\n",
      "Episode:78 meanR:0.4959 R:1.2400 gloss:0.0928 dloss:0.5167 exploreP:0.0104\n",
      "Episode:79 meanR:0.5049 R:1.2100 gloss:0.2829 dloss:0.0327 exploreP:0.0103\n",
      "Episode:80 meanR:0.5068 R:0.6600 gloss:0.9184 dloss:0.0740 exploreP:0.0103\n",
      "Episode:81 meanR:0.5059 R:0.4300 gloss:1.1008 dloss:2.0667 exploreP:0.0103\n",
      "Episode:82 meanR:0.5039 R:0.3400 gloss:0.0573 dloss:0.6941 exploreP:0.0102\n",
      "Episode:83 meanR:0.5356 R:3.1700 gloss:0.3289 dloss:0.0738 exploreP:0.0102\n",
      "Episode:84 meanR:0.5434 R:1.2000 gloss:0.4467 dloss:0.0261 exploreP:0.0102\n",
      "Episode:85 meanR:0.5394 R:0.2000 gloss:5.8096 dloss:0.1870 exploreP:0.0102\n",
      "Episode:86 meanR:0.5383 R:0.4400 gloss:4.3777 dloss:0.0240 exploreP:0.0102\n",
      "Episode:87 meanR:0.5428 R:0.9400 gloss:0.1252 dloss:0.3309 exploreP:0.0101\n",
      "Episode:88 meanR:0.5419 R:0.4600 gloss:0.0121 dloss:0.2382 exploreP:0.0101\n",
      "Episode:89 meanR:0.5450 R:0.8200 gloss:0.2982 dloss:0.5584 exploreP:0.0101\n",
      "Episode:90 meanR:0.5390 R:0.0000 gloss:-0.4574 dloss:0.5255 exploreP:0.0101\n",
      "Episode:91 meanR:0.5359 R:0.2500 gloss:1.2541 dloss:0.4557 exploreP:0.0101\n",
      "Episode:92 meanR:0.5301 R:0.0000 gloss:-0.7245 dloss:0.2937 exploreP:0.0101\n",
      "Episode:93 meanR:0.5247 R:0.0200 gloss:61.4088 dloss:0.5302 exploreP:0.0101\n",
      "Episode:94 meanR:0.5275 R:0.7900 gloss:0.1268 dloss:0.0248 exploreP:0.0101\n",
      "Episode:95 meanR:0.5240 R:0.1900 gloss:0.0170 dloss:0.0173 exploreP:0.0101\n",
      "Episode:96 meanR:0.5186 R:0.0000 gloss:-0.2305 dloss:0.0588 exploreP:0.0101\n",
      "Episode:97 meanR:0.5139 R:0.0600 gloss:-0.1992 dloss:0.0757 exploreP:0.0101\n",
      "Episode:98 meanR:0.5087 R:0.0000 gloss:-0.0997 dloss:0.1014 exploreP:0.0100\n",
      "Episode:99 meanR:0.5036 R:0.0000 gloss:-0.3153 dloss:0.1113 exploreP:0.0100\n",
      "Episode:100 meanR:0.5029 R:0.0000 gloss:-0.6989 dloss:0.1340 exploreP:0.0100\n",
      "Episode:101 meanR:0.4992 R:0.0000 gloss:-0.6508 dloss:0.0528 exploreP:0.0100\n",
      "Episode:102 meanR:0.4912 R:0.0000 gloss:-0.3701 dloss:0.0292 exploreP:0.0100\n",
      "Episode:103 meanR:0.4893 R:0.0500 gloss:-0.6044 dloss:0.0681 exploreP:0.0100\n",
      "Episode:104 meanR:0.4855 R:0.1000 gloss:-0.1721 dloss:0.0267 exploreP:0.0100\n",
      "Episode:105 meanR:0.4775 R:0.0000 gloss:-0.3923 dloss:0.0297 exploreP:0.0100\n",
      "Episode:106 meanR:0.4685 R:0.0000 gloss:-0.3519 dloss:0.0396 exploreP:0.0100\n",
      "Episode:107 meanR:0.4701 R:0.4400 gloss:-0.3023 dloss:0.0673 exploreP:0.0100\n",
      "Episode:108 meanR:0.4670 R:0.0000 gloss:-1.2958 dloss:0.1122 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:109 meanR:0.4601 R:0.0000 gloss:-0.2024 dloss:0.0470 exploreP:0.0100\n",
      "Episode:110 meanR:0.4538 R:0.0000 gloss:-0.5011 dloss:0.0955 exploreP:0.0100\n",
      "Episode:111 meanR:0.4554 R:0.1600 gloss:15.0748 dloss:0.6286 exploreP:0.0100\n",
      "Episode:112 meanR:0.4574 R:0.3800 gloss:20.3729 dloss:0.2324 exploreP:0.0100\n",
      "Episode:113 meanR:0.4661 R:1.1000 gloss:-0.1174 dloss:0.0832 exploreP:0.0100\n",
      "Episode:114 meanR:0.4654 R:0.0000 gloss:0.0936 dloss:0.0601 exploreP:0.0100\n",
      "Episode:115 meanR:0.4686 R:0.3200 gloss:0.0251 dloss:0.0326 exploreP:0.0100\n",
      "Episode:116 meanR:0.4655 R:0.0600 gloss:-0.0970 dloss:0.0340 exploreP:0.0100\n",
      "Episode:117 meanR:0.4690 R:0.3500 gloss:-0.2038 dloss:0.0606 exploreP:0.0100\n",
      "Episode:118 meanR:0.4774 R:0.8400 gloss:-0.2867 dloss:0.0411 exploreP:0.0100\n",
      "Episode:119 meanR:0.4871 R:0.9700 gloss:0.2189 dloss:0.0361 exploreP:0.0100\n",
      "Episode:120 meanR:0.4899 R:0.6600 gloss:0.0201 dloss:0.0326 exploreP:0.0100\n",
      "Episode:121 meanR:0.4994 R:0.9500 gloss:-0.2218 dloss:0.0341 exploreP:0.0100\n",
      "Episode:122 meanR:0.5102 R:1.1400 gloss:-0.0534 dloss:0.0241 exploreP:0.0100\n",
      "Episode:123 meanR:0.5164 R:0.6200 gloss:0.0184 dloss:0.0431 exploreP:0.0100\n",
      "Episode:124 meanR:0.5202 R:0.3800 gloss:0.7276 dloss:0.1119 exploreP:0.0100\n",
      "Episode:125 meanR:0.5202 R:0.0000 gloss:-0.1228 dloss:0.0859 exploreP:0.0100\n",
      "Episode:126 meanR:0.5229 R:0.3900 gloss:-0.7231 dloss:0.0658 exploreP:0.0100\n",
      "Episode:127 meanR:0.5359 R:1.3000 gloss:-0.1428 dloss:0.0358 exploreP:0.0100\n",
      "Episode:128 meanR:0.5460 R:1.0100 gloss:0.0699 dloss:0.0230 exploreP:0.0100\n",
      "Episode:129 meanR:0.5467 R:0.0700 gloss:-0.0897 dloss:0.0222 exploreP:0.0100\n",
      "Episode:130 meanR:0.5498 R:0.3100 gloss:-0.1337 dloss:0.0168 exploreP:0.0100\n",
      "Episode:131 meanR:0.5542 R:0.8000 gloss:0.0079 dloss:0.0252 exploreP:0.0100\n",
      "Episode:132 meanR:0.5582 R:0.5900 gloss:-0.1018 dloss:0.0226 exploreP:0.0100\n",
      "Episode:133 meanR:0.5582 R:0.1800 gloss:-0.1754 dloss:0.0232 exploreP:0.0100\n",
      "Episode:134 meanR:0.5532 R:0.4000 gloss:0.8266 dloss:0.0682 exploreP:0.0100\n",
      "Episode:135 meanR:0.5532 R:0.4900 gloss:0.0737 dloss:0.0156 exploreP:0.0100\n",
      "Episode:136 meanR:0.5566 R:1.3100 gloss:0.0399 dloss:0.0100 exploreP:0.0100\n",
      "Episode:137 meanR:0.5576 R:0.9200 gloss:0.0135 dloss:0.0148 exploreP:0.0100\n",
      "Episode:138 meanR:0.5613 R:1.0800 gloss:0.0374 dloss:0.0129 exploreP:0.0100\n",
      "Episode:139 meanR:0.5576 R:0.2900 gloss:0.0154 dloss:0.0107 exploreP:0.0100\n",
      "Episode:140 meanR:0.5699 R:1.8100 gloss:0.1065 dloss:0.0108 exploreP:0.0100\n",
      "Episode:141 meanR:0.5749 R:0.8100 gloss:0.1802 dloss:0.0104 exploreP:0.0100\n",
      "Episode:142 meanR:0.5725 R:0.4700 gloss:-0.0333 dloss:0.0097 exploreP:0.0100\n",
      "Episode:143 meanR:0.5760 R:0.9400 gloss:0.0004 dloss:0.0101 exploreP:0.0100\n",
      "Episode:144 meanR:0.5826 R:1.3800 gloss:0.0379 dloss:0.0158 exploreP:0.0100\n",
      "Episode:145 meanR:0.5753 R:0.2000 gloss:0.0291 dloss:0.0077 exploreP:0.0100\n",
      "Episode:146 meanR:0.5892 R:1.3900 gloss:0.1049 dloss:0.0141 exploreP:0.0100\n",
      "Episode:147 meanR:0.5895 R:0.3400 gloss:0.1885 dloss:0.0041 exploreP:0.0100\n",
      "Episode:148 meanR:0.5881 R:0.4800 gloss:0.0168 dloss:0.0126 exploreP:0.0100\n",
      "Episode:149 meanR:0.6010 R:1.7700 gloss:0.0735 dloss:0.0060 exploreP:0.0100\n",
      "Episode:150 meanR:0.6072 R:0.8000 gloss:0.1159 dloss:0.0047 exploreP:0.0100\n",
      "Episode:151 meanR:0.6098 R:0.6800 gloss:0.0989 dloss:0.0072 exploreP:0.0100\n",
      "Episode:152 meanR:0.6231 R:1.3900 gloss:0.0201 dloss:0.0096 exploreP:0.0100\n",
      "Episode:153 meanR:0.6189 R:0.6300 gloss:0.0928 dloss:0.0023 exploreP:0.0100\n",
      "Episode:154 meanR:0.6253 R:0.7500 gloss:0.0780 dloss:0.0041 exploreP:0.0100\n",
      "Episode:155 meanR:0.6192 R:0.3500 gloss:0.0194 dloss:0.0108 exploreP:0.0100\n",
      "Episode:156 meanR:0.6250 R:0.7800 gloss:0.0681 dloss:0.0056 exploreP:0.0100\n",
      "Episode:157 meanR:0.6154 R:0.6800 gloss:0.0998 dloss:0.0022 exploreP:0.0100\n",
      "Episode:158 meanR:0.6098 R:0.8800 gloss:0.1006 dloss:0.0043 exploreP:0.0100\n",
      "Episode:159 meanR:0.6101 R:0.6300 gloss:0.0284 dloss:0.0072 exploreP:0.0100\n",
      "Episode:160 meanR:0.6167 R:1.3200 gloss:0.0413 dloss:0.0075 exploreP:0.0100\n",
      "Episode:161 meanR:0.6117 R:0.2700 gloss:0.0195 dloss:0.0067 exploreP:0.0100\n",
      "Episode:162 meanR:0.6077 R:0.2100 gloss:-0.0240 dloss:0.0049 exploreP:0.0100\n",
      "Episode:163 meanR:0.6175 R:1.1800 gloss:0.0294 dloss:0.0072 exploreP:0.0100\n",
      "Episode:164 meanR:0.6093 R:0.7100 gloss:0.2435 dloss:0.0056 exploreP:0.0100\n",
      "Episode:165 meanR:0.6177 R:1.1100 gloss:0.0292 dloss:0.0028 exploreP:0.0100\n",
      "Episode:166 meanR:0.6186 R:1.2500 gloss:0.0644 dloss:0.0040 exploreP:0.0100\n",
      "Episode:167 meanR:0.6179 R:0.6700 gloss:0.0326 dloss:0.0034 exploreP:0.0100\n",
      "Episode:168 meanR:0.6229 R:0.9000 gloss:0.0266 dloss:0.0028 exploreP:0.0100\n",
      "Episode:169 meanR:0.6257 R:0.6500 gloss:0.0173 dloss:0.0025 exploreP:0.0100\n",
      "Episode:170 meanR:0.6303 R:1.5200 gloss:0.0215 dloss:0.0013 exploreP:0.0100\n",
      "Episode:171 meanR:0.6137 R:0.5600 gloss:0.0571 dloss:0.0022 exploreP:0.0100\n",
      "Episode:172 meanR:0.6083 R:0.8200 gloss:0.0232 dloss:0.0017 exploreP:0.0100\n",
      "Episode:173 meanR:0.6167 R:1.0800 gloss:0.0418 dloss:0.0015 exploreP:0.0100\n",
      "Episode:174 meanR:0.6154 R:0.6300 gloss:0.0518 dloss:0.0012 exploreP:0.0100\n",
      "Episode:175 meanR:0.6244 R:1.6200 gloss:0.0523 dloss:0.0012 exploreP:0.0100\n",
      "Episode:176 meanR:0.6272 R:0.5300 gloss:0.0329 dloss:0.0009 exploreP:0.0100\n",
      "Episode:177 meanR:0.6330 R:1.0300 gloss:0.0163 dloss:0.0009 exploreP:0.0100\n",
      "Episode:178 meanR:0.6250 R:0.4400 gloss:0.0232 dloss:0.0010 exploreP:0.0100\n",
      "Episode:179 meanR:0.6262 R:1.3300 gloss:0.0384 dloss:0.0009 exploreP:0.0100\n",
      "Episode:180 meanR:0.6368 R:1.7200 gloss:0.0224 dloss:0.0006 exploreP:0.0100\n",
      "Episode:181 meanR:0.6342 R:0.1700 gloss:0.0224 dloss:0.0006 exploreP:0.0100\n",
      "Episode:182 meanR:0.6401 R:0.9300 gloss:0.0188 dloss:0.0006 exploreP:0.0100\n",
      "Episode:183 meanR:0.6208 R:1.2400 gloss:0.0418 dloss:0.0005 exploreP:0.0100\n",
      "Episode:184 meanR:0.6202 R:1.1400 gloss:0.0281 dloss:0.0003 exploreP:0.0100\n",
      "Episode:185 meanR:0.6360 R:1.7800 gloss:0.0164 dloss:0.0002 exploreP:0.0100\n",
      "Episode:186 meanR:0.6327 R:0.1100 gloss:0.0380 dloss:0.0003 exploreP:0.0100\n",
      "Episode:187 meanR:0.6295 R:0.6200 gloss:0.0079 dloss:0.0001 exploreP:0.0100\n",
      "Episode:188 meanR:0.6267 R:0.1800 gloss:0.0059 dloss:0.0001 exploreP:0.0100\n",
      "Episode:189 meanR:0.6284 R:0.9900 gloss:0.0142 dloss:0.0001 exploreP:0.0100\n",
      "Episode:190 meanR:0.6323 R:0.3900 gloss:0.0283 dloss:0.0006 exploreP:0.0100\n",
      "Episode:191 meanR:0.6381 R:0.8300 gloss:0.0701 dloss:0.0005 exploreP:0.0100\n",
      "Episode:192 meanR:0.6476 R:0.9500 gloss:0.0191 dloss:0.0004 exploreP:0.0100\n",
      "Episode:193 meanR:0.6591 R:1.1700 gloss:0.0391 dloss:0.0003 exploreP:0.0100\n",
      "Episode:194 meanR:0.6649 R:1.3700 gloss:0.0231 dloss:0.0003 exploreP:0.0100\n",
      "Episode:195 meanR:0.6728 R:0.9800 gloss:0.0277 dloss:0.0003 exploreP:0.0100\n",
      "Episode:196 meanR:0.6925 R:1.9700 gloss:0.0321 dloss:0.0002 exploreP:0.0100\n",
      "Episode:197 meanR:0.7090 R:1.7100 gloss:0.0230 dloss:0.0002 exploreP:0.0100\n",
      "Episode:198 meanR:0.7211 R:1.2100 gloss:0.0262 dloss:0.0001 exploreP:0.0100\n",
      "Episode:199 meanR:0.7234 R:0.2300 gloss:0.0215 dloss:0.0001 exploreP:0.0100\n",
      "Episode:200 meanR:0.7355 R:1.2100 gloss:0.0096 dloss:0.0001 exploreP:0.0100\n",
      "Episode:201 meanR:0.7449 R:0.9400 gloss:0.0212 dloss:0.0001 exploreP:0.0100\n",
      "Episode:202 meanR:0.7682 R:2.3300 gloss:0.0309 dloss:0.0002 exploreP:0.0100\n",
      "Episode:203 meanR:0.7903 R:2.2600 gloss:0.0305 dloss:0.0001 exploreP:0.0100\n",
      "Episode:204 meanR:0.8254 R:3.6100 gloss:0.0481 dloss:0.0002 exploreP:0.0100\n",
      "Episode:205 meanR:0.8445 R:1.9100 gloss:0.0552 dloss:0.0002 exploreP:0.0100\n",
      "Episode:206 meanR:0.8659 R:2.1400 gloss:0.0412 dloss:0.0001 exploreP:0.0100\n",
      "Episode:207 meanR:0.8734 R:1.1900 gloss:0.0345 dloss:0.0001 exploreP:0.0100\n",
      "Episode:208 meanR:0.8956 R:2.2200 gloss:0.0336 dloss:0.0001 exploreP:0.0100\n",
      "Episode:209 meanR:0.9033 R:0.7700 gloss:0.0239 dloss:0.0001 exploreP:0.0100\n",
      "Episode:210 meanR:0.9303 R:2.7000 gloss:0.0260 dloss:0.0001 exploreP:0.0100\n",
      "Episode:211 meanR:0.9338 R:0.5100 gloss:0.0430 dloss:0.0001 exploreP:0.0100\n",
      "Episode:212 meanR:0.9370 R:0.7000 gloss:0.0110 dloss:0.0000 exploreP:0.0100\n",
      "Episode:213 meanR:0.9480 R:2.2000 gloss:0.0370 dloss:0.0001 exploreP:0.0100\n",
      "Episode:214 meanR:0.9539 R:0.5900 gloss:0.0265 dloss:0.0001 exploreP:0.0100\n",
      "Episode:215 meanR:0.9669 R:1.6200 gloss:0.0288 dloss:0.0001 exploreP:0.0100\n",
      "Episode:216 meanR:0.9687 R:0.2400 gloss:0.0213 dloss:0.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:217 meanR:0.9772 R:1.2000 gloss:0.0214 dloss:0.0000 exploreP:0.0100\n",
      "Episode:218 meanR:0.9733 R:0.4500 gloss:0.0182 dloss:0.0000 exploreP:0.0100\n",
      "Episode:219 meanR:0.9701 R:0.6500 gloss:0.0119 dloss:0.0000 exploreP:0.0100\n",
      "Episode:220 meanR:0.9743 R:1.0800 gloss:0.0238 dloss:0.0000 exploreP:0.0100\n",
      "Episode:221 meanR:0.9744 R:0.9600 gloss:0.0290 dloss:0.0001 exploreP:0.0100\n",
      "Episode:222 meanR:0.9721 R:0.9100 gloss:0.0182 dloss:0.0000 exploreP:0.0100\n",
      "Episode:223 meanR:0.9759 R:1.0000 gloss:0.0367 dloss:0.0001 exploreP:0.0100\n",
      "Episode:224 meanR:0.9753 R:0.3200 gloss:0.0106 dloss:0.0000 exploreP:0.0100\n",
      "Episode:225 meanR:0.9940 R:1.8700 gloss:0.0355 dloss:0.0001 exploreP:0.0100\n",
      "Episode:226 meanR:0.9984 R:0.8300 gloss:0.0350 dloss:0.0001 exploreP:0.0100\n",
      "Episode:227 meanR:0.9854 R:0.0000 gloss:0.0082 dloss:0.0000 exploreP:0.0100\n",
      "Episode:228 meanR:0.9805 R:0.5200 gloss:0.0070 dloss:0.0000 exploreP:0.0100\n",
      "Episode:229 meanR:0.9831 R:0.3300 gloss:0.0144 dloss:0.0000 exploreP:0.0100\n",
      "Episode:230 meanR:0.9883 R:0.8300 gloss:0.0145 dloss:0.0000 exploreP:0.0100\n",
      "Episode:231 meanR:0.9942 R:1.3900 gloss:0.0351 dloss:0.0001 exploreP:0.0100\n",
      "Episode:232 meanR:0.9947 R:0.6400 gloss:0.0352 dloss:0.0001 exploreP:0.0100\n",
      "Episode:233 meanR:0.9967 R:0.3800 gloss:0.0186 dloss:0.0000 exploreP:0.0100\n",
      "Episode:234 meanR:0.9944 R:0.1700 gloss:0.0099 dloss:0.0000 exploreP:0.0100\n",
      "Episode:235 meanR:0.9989 R:0.9400 gloss:0.0213 dloss:0.0000 exploreP:0.0100\n",
      "Episode:236 meanR:0.9877 R:0.1900 gloss:0.0367 dloss:0.0001 exploreP:0.0100\n",
      "Episode:237 meanR:0.9875 R:0.9000 gloss:0.0186 dloss:0.0000 exploreP:0.0100\n",
      "Episode:238 meanR:0.9775 R:0.0800 gloss:0.0117 dloss:0.0000 exploreP:0.0100\n",
      "Episode:239 meanR:0.9882 R:1.3600 gloss:0.0277 dloss:0.0001 exploreP:0.0100\n",
      "Episode:240 meanR:0.9868 R:1.6700 gloss:0.0389 dloss:0.0001 exploreP:0.0100\n",
      "Episode:241 meanR:0.9849 R:0.6200 gloss:0.0290 dloss:0.0001 exploreP:0.0100\n",
      "Episode:242 meanR:0.9843 R:0.4100 gloss:0.0051 dloss:0.0000 exploreP:0.0100\n",
      "Episode:243 meanR:0.9982 R:2.3300 gloss:0.0474 dloss:0.0001 exploreP:0.0100\n",
      "Episode:244 meanR:0.9939 R:0.9500 gloss:0.0466 dloss:0.0001 exploreP:0.0100\n",
      "Episode:245 meanR:1.0150 R:2.3100 gloss:0.0411 dloss:0.0001 exploreP:0.0100\n",
      "Episode:246 meanR:1.0105 R:0.9400 gloss:0.0562 dloss:0.0001 exploreP:0.0100\n",
      "Episode:247 meanR:1.0207 R:1.3600 gloss:0.0294 dloss:0.0000 exploreP:0.0100\n",
      "Episode:248 meanR:1.0300 R:1.4100 gloss:0.0517 dloss:0.0001 exploreP:0.0100\n",
      "Episode:249 meanR:1.0323 R:2.0000 gloss:0.0445 dloss:0.0001 exploreP:0.0100\n",
      "Episode:250 meanR:1.0447 R:2.0400 gloss:0.0614 dloss:0.0001 exploreP:0.0100\n",
      "Episode:251 meanR:1.0475 R:0.9600 gloss:0.0446 dloss:0.0001 exploreP:0.0100\n",
      "Episode:252 meanR:1.0336 R:0.0000 gloss:0.0236 dloss:0.0000 exploreP:0.0100\n",
      "Episode:253 meanR:1.0325 R:0.5200 gloss:0.0562 dloss:0.0001 exploreP:0.0100\n",
      "Episode:254 meanR:1.0440 R:1.9000 gloss:0.0231 dloss:0.0000 exploreP:0.0100\n",
      "Episode:255 meanR:1.0540 R:1.3500 gloss:0.0476 dloss:0.0001 exploreP:0.0100\n",
      "Episode:256 meanR:1.0581 R:1.1900 gloss:0.0369 dloss:0.0001 exploreP:0.0100\n",
      "Episode:257 meanR:1.0605 R:0.9200 gloss:0.0410 dloss:0.0001 exploreP:0.0100\n",
      "Episode:258 meanR:1.0654 R:1.3700 gloss:0.0313 dloss:0.0000 exploreP:0.0100\n",
      "Episode:259 meanR:1.0637 R:0.4600 gloss:0.0334 dloss:0.0000 exploreP:0.0100\n",
      "Episode:260 meanR:1.0505 R:0.0000 gloss:0.0086 dloss:0.0000 exploreP:0.0100\n",
      "Episode:261 meanR:1.0501 R:0.2300 gloss:0.0101 dloss:0.0000 exploreP:0.0100\n",
      "Episode:262 meanR:1.0718 R:2.3800 gloss:0.0331 dloss:0.0001 exploreP:0.0100\n",
      "Episode:263 meanR:1.0668 R:0.6800 gloss:0.0562 dloss:0.0001 exploreP:0.0100\n",
      "Episode:264 meanR:1.0682 R:0.8500 gloss:0.0122 dloss:0.0000 exploreP:0.0100\n",
      "Episode:265 meanR:1.0600 R:0.2900 gloss:0.0206 dloss:0.0000 exploreP:0.0100\n",
      "Episode:266 meanR:1.0646 R:1.7100 gloss:0.0199 dloss:0.0000 exploreP:0.0100\n",
      "Episode:267 meanR:1.0714 R:1.3500 gloss:0.0508 dloss:0.0001 exploreP:0.0100\n",
      "Episode:268 meanR:1.0748 R:1.2400 gloss:0.0269 dloss:0.0000 exploreP:0.0100\n",
      "Episode:269 meanR:1.0709 R:0.2600 gloss:0.0240 dloss:0.0001 exploreP:0.0100\n",
      "Episode:270 meanR:1.0764 R:2.0700 gloss:0.0431 dloss:0.0001 exploreP:0.0100\n",
      "Episode:271 meanR:1.1053 R:3.4500 gloss:0.0977 dloss:0.0002 exploreP:0.0100\n",
      "Episode:272 meanR:1.1005 R:0.3400 gloss:0.0534 dloss:0.0001 exploreP:0.0100\n",
      "Episode:273 meanR:1.0982 R:0.8500 gloss:0.0141 dloss:0.0000 exploreP:0.0100\n",
      "Episode:274 meanR:1.1021 R:1.0200 gloss:0.0299 dloss:0.0000 exploreP:0.0100\n",
      "Episode:275 meanR:1.0929 R:0.7000 gloss:0.0372 dloss:0.0000 exploreP:0.0100\n",
      "Episode:276 meanR:1.1020 R:1.4400 gloss:0.0342 dloss:0.0000 exploreP:0.0100\n",
      "Episode:277 meanR:1.1068 R:1.5100 gloss:0.0524 dloss:0.0001 exploreP:0.0100\n",
      "Episode:278 meanR:1.1194 R:1.7000 gloss:0.0598 dloss:0.0001 exploreP:0.0100\n",
      "Episode:279 meanR:1.1422 R:3.6100 gloss:0.0746 dloss:0.0001 exploreP:0.0100\n",
      "Episode:280 meanR:1.1297 R:0.4700 gloss:0.0822 dloss:0.0001 exploreP:0.0100\n",
      "Episode:281 meanR:1.1622 R:3.4200 gloss:0.0933 dloss:0.0001 exploreP:0.0100\n",
      "Episode:282 meanR:1.1711 R:1.8200 gloss:0.0852 dloss:0.0001 exploreP:0.0100\n",
      "Episode:283 meanR:1.1648 R:0.6100 gloss:0.0458 dloss:0.0001 exploreP:0.0100\n",
      "Episode:284 meanR:1.1680 R:1.4600 gloss:0.0309 dloss:0.0000 exploreP:0.0100\n",
      "Episode:285 meanR:1.1558 R:0.5600 gloss:0.0428 dloss:0.0001 exploreP:0.0100\n",
      "Episode:286 meanR:1.1747 R:2.0000 gloss:0.0555 dloss:0.0001 exploreP:0.0100\n",
      "Episode:287 meanR:1.1818 R:1.3300 gloss:0.0480 dloss:0.0001 exploreP:0.0100\n",
      "Episode:288 meanR:1.1915 R:1.1500 gloss:0.0384 dloss:0.0000 exploreP:0.0100\n",
      "Episode:289 meanR:1.1930 R:1.1400 gloss:0.0441 dloss:0.0001 exploreP:0.0100\n",
      "Episode:290 meanR:1.2062 R:1.7100 gloss:0.0530 dloss:0.0001 exploreP:0.0100\n",
      "Episode:291 meanR:1.2045 R:0.6600 gloss:0.0624 dloss:0.0001 exploreP:0.0100\n",
      "Episode:292 meanR:1.2025 R:0.7500 gloss:0.0289 dloss:0.0000 exploreP:0.0100\n",
      "Episode:293 meanR:1.1978 R:0.7000 gloss:0.0182 dloss:0.0000 exploreP:0.0100\n",
      "Episode:294 meanR:1.1983 R:1.4200 gloss:0.0571 dloss:0.0001 exploreP:0.0100\n",
      "Episode:295 meanR:1.1966 R:0.8100 gloss:0.0341 dloss:0.0000 exploreP:0.0100\n",
      "Episode:296 meanR:1.1855 R:0.8600 gloss:0.0418 dloss:0.0000 exploreP:0.0100\n",
      "Episode:297 meanR:1.1864 R:1.8000 gloss:0.0481 dloss:0.0001 exploreP:0.0100\n",
      "Episode:298 meanR:1.1800 R:0.5700 gloss:0.0555 dloss:0.0001 exploreP:0.0100\n",
      "Episode:299 meanR:1.1914 R:1.3700 gloss:0.0409 dloss:0.0000 exploreP:0.0100\n",
      "Episode:300 meanR:1.1916 R:1.2300 gloss:0.0538 dloss:0.0000 exploreP:0.0100\n",
      "Episode:301 meanR:1.2195 R:3.7300 gloss:0.1084 dloss:0.0001 exploreP:0.0100\n",
      "Episode:302 meanR:1.2125 R:1.6300 gloss:0.1272 dloss:0.0002 exploreP:0.0100\n",
      "Episode:303 meanR:1.2043 R:1.4400 gloss:0.0636 dloss:0.0001 exploreP:0.0100\n",
      "Episode:304 meanR:1.1988 R:3.0600 gloss:0.0781 dloss:0.0001 exploreP:0.0100\n",
      "Episode:305 meanR:1.1911 R:1.1400 gloss:0.1326 dloss:0.0002 exploreP:0.0100\n",
      "Episode:306 meanR:1.1865 R:1.6800 gloss:0.0500 dloss:0.0001 exploreP:0.0100\n",
      "Episode:307 meanR:1.1883 R:1.3700 gloss:0.0652 dloss:0.0001 exploreP:0.0100\n",
      "Episode:308 meanR:1.1777 R:1.1600 gloss:0.0703 dloss:0.0001 exploreP:0.0100\n",
      "Episode:309 meanR:1.1790 R:0.9000 gloss:0.0303 dloss:0.0000 exploreP:0.0100\n",
      "Episode:310 meanR:1.1614 R:0.9400 gloss:0.0390 dloss:0.0000 exploreP:0.0100\n",
      "Episode:311 meanR:1.1586 R:0.2300 gloss:0.0325 dloss:0.0000 exploreP:0.0100\n",
      "Episode:312 meanR:1.1571 R:0.5500 gloss:0.0212 dloss:0.0000 exploreP:0.0100\n",
      "Episode:313 meanR:1.1443 R:0.9200 gloss:0.0261 dloss:0.0000 exploreP:0.0100\n",
      "Episode:314 meanR:1.1483 R:0.9900 gloss:0.0473 dloss:0.0001 exploreP:0.0100\n",
      "Episode:315 meanR:1.1340 R:0.1900 gloss:0.0325 dloss:0.0000 exploreP:0.0100\n",
      "Episode:316 meanR:1.1482 R:1.6600 gloss:0.0520 dloss:0.0001 exploreP:0.0100\n",
      "Episode:317 meanR:1.1376 R:0.1400 gloss:0.0286 dloss:0.0000 exploreP:0.0100\n",
      "Episode:318 meanR:1.1375 R:0.4400 gloss:0.0138 dloss:0.0000 exploreP:0.0100\n",
      "Episode:319 meanR:1.1455 R:1.4500 gloss:0.0515 dloss:0.0001 exploreP:0.0100\n",
      "Episode:320 meanR:1.1370 R:0.2300 gloss:0.0285 dloss:0.0000 exploreP:0.0100\n",
      "Episode:321 meanR:1.1517 R:2.4300 gloss:0.0851 dloss:0.0001 exploreP:0.0100\n",
      "Episode:322 meanR:1.1558 R:1.3200 gloss:0.0829 dloss:0.0001 exploreP:0.0100\n",
      "Episode:323 meanR:1.1532 R:0.7400 gloss:0.0242 dloss:0.0000 exploreP:0.0100\n",
      "Episode:324 meanR:1.1769 R:2.6900 gloss:0.1011 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:325 meanR:1.1658 R:0.7600 gloss:0.0810 dloss:0.0001 exploreP:0.0100\n",
      "Episode:326 meanR:1.1671 R:0.9600 gloss:0.0240 dloss:0.0000 exploreP:0.0100\n",
      "Episode:327 meanR:1.1740 R:0.6900 gloss:0.0521 dloss:0.0001 exploreP:0.0100\n",
      "Episode:328 meanR:1.1826 R:1.3800 gloss:0.0278 dloss:0.0000 exploreP:0.0100\n",
      "Episode:329 meanR:1.1966 R:1.7300 gloss:0.0906 dloss:0.0001 exploreP:0.0100\n",
      "Episode:330 meanR:1.2130 R:2.4700 gloss:0.0707 dloss:0.0001 exploreP:0.0100\n",
      "Episode:331 meanR:1.2006 R:0.1500 gloss:0.0818 dloss:0.0001 exploreP:0.0100\n",
      "Episode:332 meanR:1.2036 R:0.9400 gloss:0.0289 dloss:0.0000 exploreP:0.0100\n",
      "Episode:333 meanR:1.2110 R:1.1200 gloss:0.0363 dloss:0.0000 exploreP:0.0100\n",
      "Episode:334 meanR:1.2152 R:0.5900 gloss:0.0501 dloss:0.0000 exploreP:0.0100\n",
      "Episode:335 meanR:1.2197 R:1.3900 gloss:0.0258 dloss:0.0000 exploreP:0.0100\n",
      "Episode:336 meanR:1.2196 R:0.1800 gloss:0.0551 dloss:0.0001 exploreP:0.0100\n",
      "Episode:337 meanR:1.2213 R:1.0700 gloss:0.0182 dloss:0.0000 exploreP:0.0100\n",
      "Episode:338 meanR:1.2328 R:1.2300 gloss:0.0579 dloss:0.0001 exploreP:0.0100\n",
      "Episode:339 meanR:1.2533 R:3.4100 gloss:0.1560 dloss:0.0002 exploreP:0.0100\n",
      "Episode:340 meanR:1.2606 R:2.4000 gloss:0.0899 dloss:0.0001 exploreP:0.0100\n",
      "Episode:341 meanR:1.2694 R:1.5000 gloss:0.0976 dloss:0.0001 exploreP:0.0100\n",
      "Episode:342 meanR:1.2781 R:1.2800 gloss:0.0664 dloss:0.0001 exploreP:0.0100\n",
      "Episode:343 meanR:1.2752 R:2.0400 gloss:0.0887 dloss:0.0001 exploreP:0.0100\n",
      "Episode:344 meanR:1.2711 R:0.5400 gloss:0.0535 dloss:0.0001 exploreP:0.0100\n",
      "Episode:345 meanR:1.2596 R:1.1600 gloss:0.0342 dloss:0.0000 exploreP:0.0100\n",
      "Episode:346 meanR:1.2674 R:1.7200 gloss:0.0714 dloss:0.0001 exploreP:0.0100\n",
      "Episode:347 meanR:1.2554 R:0.1600 gloss:0.0501 dloss:0.0000 exploreP:0.0100\n",
      "Episode:348 meanR:1.2516 R:1.0300 gloss:0.0237 dloss:0.0000 exploreP:0.0100\n",
      "Episode:349 meanR:1.2377 R:0.6100 gloss:0.0395 dloss:0.0000 exploreP:0.0100\n",
      "Episode:350 meanR:1.2271 R:0.9800 gloss:0.0242 dloss:0.0000 exploreP:0.0100\n",
      "Episode:351 meanR:1.2243 R:0.6800 gloss:0.0485 dloss:0.0000 exploreP:0.0100\n",
      "Episode:352 meanR:1.2313 R:0.7000 gloss:0.0350 dloss:0.0000 exploreP:0.0100\n",
      "Episode:353 meanR:1.2356 R:0.9500 gloss:0.0433 dloss:0.0000 exploreP:0.0100\n",
      "Episode:354 meanR:1.2264 R:0.9800 gloss:0.0547 dloss:0.0001 exploreP:0.0100\n",
      "Episode:355 meanR:1.2334 R:2.0500 gloss:0.0736 dloss:0.0001 exploreP:0.0100\n",
      "Episode:356 meanR:1.2278 R:0.6300 gloss:0.0685 dloss:0.0001 exploreP:0.0100\n",
      "Episode:357 meanR:1.2240 R:0.5400 gloss:0.0255 dloss:0.0000 exploreP:0.0100\n",
      "Episode:358 meanR:1.2176 R:0.7300 gloss:0.0342 dloss:0.0000 exploreP:0.0100\n",
      "Episode:359 meanR:1.2214 R:0.8400 gloss:0.0328 dloss:0.0000 exploreP:0.0100\n",
      "Episode:360 meanR:1.2288 R:0.7400 gloss:0.0272 dloss:0.0000 exploreP:0.0100\n",
      "Episode:361 meanR:1.2421 R:1.5600 gloss:0.0497 dloss:0.0000 exploreP:0.0100\n",
      "Episode:362 meanR:1.2251 R:0.6800 gloss:0.0675 dloss:0.0001 exploreP:0.0100\n",
      "Episode:363 meanR:1.2296 R:1.1300 gloss:0.0384 dloss:0.0000 exploreP:0.0100\n",
      "Episode:364 meanR:1.2300 R:0.8900 gloss:0.0599 dloss:0.0001 exploreP:0.0100\n",
      "Episode:365 meanR:1.2368 R:0.9700 gloss:0.0418 dloss:0.0000 exploreP:0.0100\n",
      "Episode:366 meanR:1.2319 R:1.2200 gloss:0.0524 dloss:0.0000 exploreP:0.0100\n",
      "Episode:367 meanR:1.2254 R:0.7000 gloss:0.0517 dloss:0.0000 exploreP:0.0100\n",
      "Episode:368 meanR:1.2192 R:0.6200 gloss:0.0296 dloss:0.0000 exploreP:0.0100\n",
      "Episode:369 meanR:1.2203 R:0.3700 gloss:0.0305 dloss:0.0000 exploreP:0.0100\n",
      "Episode:370 meanR:1.2034 R:0.3800 gloss:0.0221 dloss:0.0000 exploreP:0.0100\n",
      "Episode:371 meanR:1.1788 R:0.9900 gloss:0.0421 dloss:0.0000 exploreP:0.0100\n",
      "Episode:372 meanR:1.1761 R:0.0700 gloss:0.0190 dloss:0.0000 exploreP:0.0100\n",
      "Episode:373 meanR:1.1734 R:0.5800 gloss:0.0201 dloss:0.0000 exploreP:0.0100\n",
      "Episode:374 meanR:1.1675 R:0.4300 gloss:0.0249 dloss:0.0000 exploreP:0.0100\n",
      "Episode:375 meanR:1.1605 R:0.0000 gloss:0.0051 dloss:0.0000 exploreP:0.0100\n",
      "Episode:376 meanR:1.1541 R:0.8000 gloss:0.0260 dloss:0.0000 exploreP:0.0100\n",
      "Episode:377 meanR:1.1471 R:0.8100 gloss:0.0418 dloss:0.0000 exploreP:0.0100\n",
      "Episode:378 meanR:1.1338 R:0.3700 gloss:0.0149 dloss:0.0000 exploreP:0.0100\n",
      "Episode:379 meanR:1.1131 R:1.5400 gloss:0.0633 dloss:0.0001 exploreP:0.0100\n",
      "Episode:380 meanR:1.1131 R:0.4700 gloss:0.0388 dloss:0.0000 exploreP:0.0100\n",
      "Episode:381 meanR:1.0930 R:1.4100 gloss:0.0310 dloss:0.0000 exploreP:0.0100\n",
      "Episode:382 meanR:1.0837 R:0.8900 gloss:0.0654 dloss:0.0001 exploreP:0.0100\n",
      "Episode:383 meanR:1.0791 R:0.1500 gloss:0.0276 dloss:0.0000 exploreP:0.0100\n",
      "Episode:384 meanR:1.0769 R:1.2400 gloss:0.0392 dloss:0.0000 exploreP:0.0100\n",
      "Episode:385 meanR:1.0740 R:0.2700 gloss:0.0278 dloss:0.0000 exploreP:0.0100\n",
      "Episode:386 meanR:1.0705 R:1.6500 gloss:0.0445 dloss:0.0000 exploreP:0.0100\n",
      "Episode:387 meanR:1.0586 R:0.1400 gloss:0.0495 dloss:0.0000 exploreP:0.0100\n",
      "Episode:388 meanR:1.0572 R:1.0100 gloss:0.0314 dloss:0.0000 exploreP:0.0100\n",
      "Episode:389 meanR:1.0644 R:1.8600 gloss:0.0879 dloss:0.0001 exploreP:0.0100\n",
      "Episode:390 meanR:1.0509 R:0.3600 gloss:0.0341 dloss:0.0000 exploreP:0.0100\n",
      "Episode:391 meanR:1.0572 R:1.2900 gloss:0.0430 dloss:0.0000 exploreP:0.0100\n",
      "Episode:392 meanR:1.0655 R:1.5800 gloss:0.0809 dloss:0.0001 exploreP:0.0100\n",
      "Episode:393 meanR:1.0662 R:0.7700 gloss:0.0427 dloss:0.0000 exploreP:0.0100\n",
      "Episode:394 meanR:1.0591 R:0.7100 gloss:0.0502 dloss:0.0000 exploreP:0.0100\n",
      "Episode:395 meanR:1.0535 R:0.2500 gloss:0.0173 dloss:0.0000 exploreP:0.0100\n",
      "Episode:396 meanR:1.0562 R:1.1300 gloss:0.0228 dloss:0.0000 exploreP:0.0100\n",
      "Episode:397 meanR:1.0474 R:0.9200 gloss:0.0617 dloss:0.0001 exploreP:0.0100\n",
      "Episode:398 meanR:1.0531 R:1.1400 gloss:0.0668 dloss:0.0001 exploreP:0.0100\n",
      "Episode:399 meanR:1.0566 R:1.7200 gloss:0.0565 dloss:0.0001 exploreP:0.0100\n",
      "Episode:400 meanR:1.0575 R:1.3200 gloss:0.0857 dloss:0.0001 exploreP:0.0100\n",
      "Episode:401 meanR:1.0305 R:1.0300 gloss:0.0463 dloss:0.0000 exploreP:0.0100\n",
      "Episode:402 meanR:1.0234 R:0.9200 gloss:0.0685 dloss:0.0001 exploreP:0.0100\n",
      "Episode:403 meanR:1.0136 R:0.4600 gloss:0.0291 dloss:0.0000 exploreP:0.0100\n",
      "Episode:404 meanR:0.9965 R:1.3500 gloss:0.0346 dloss:0.0000 exploreP:0.0100\n",
      "Episode:405 meanR:0.9973 R:1.2200 gloss:0.0732 dloss:0.0001 exploreP:0.0100\n",
      "Episode:406 meanR:0.9824 R:0.1900 gloss:0.0393 dloss:0.0000 exploreP:0.0100\n",
      "Episode:407 meanR:0.9767 R:0.8000 gloss:0.0367 dloss:0.0000 exploreP:0.0100\n",
      "Episode:408 meanR:0.9728 R:0.7700 gloss:0.0301 dloss:0.0000 exploreP:0.0100\n",
      "Episode:409 meanR:0.9746 R:1.0800 gloss:0.0328 dloss:0.0000 exploreP:0.0100\n",
      "Episode:410 meanR:0.9727 R:0.7500 gloss:0.0649 dloss:0.0001 exploreP:0.0100\n",
      "Episode:411 meanR:0.9717 R:0.1300 gloss:0.0143 dloss:0.0000 exploreP:0.0100\n",
      "Episode:412 meanR:0.9716 R:0.5400 gloss:0.0254 dloss:0.0000 exploreP:0.0100\n",
      "Episode:413 meanR:0.9725 R:1.0100 gloss:0.0168 dloss:0.0000 exploreP:0.0100\n",
      "Episode:414 meanR:0.9648 R:0.2200 gloss:0.0441 dloss:0.0000 exploreP:0.0100\n",
      "Episode:415 meanR:0.9747 R:1.1800 gloss:0.0268 dloss:0.0000 exploreP:0.0100\n",
      "Episode:416 meanR:0.9720 R:1.3900 gloss:0.0878 dloss:0.0001 exploreP:0.0100\n",
      "Episode:417 meanR:0.9860 R:1.5400 gloss:0.0497 dloss:0.0000 exploreP:0.0100\n",
      "Episode:418 meanR:0.9929 R:1.1300 gloss:0.0760 dloss:0.0001 exploreP:0.0100\n",
      "Episode:419 meanR:0.9973 R:1.8900 gloss:0.0834 dloss:0.0001 exploreP:0.0100\n",
      "Episode:420 meanR:1.0113 R:1.6300 gloss:0.0846 dloss:0.0001 exploreP:0.0100\n",
      "Episode:421 meanR:0.9990 R:1.2000 gloss:0.0736 dloss:0.0001 exploreP:0.0100\n",
      "Episode:422 meanR:0.9972 R:1.1400 gloss:0.0482 dloss:0.0000 exploreP:0.0100\n",
      "Episode:423 meanR:0.9955 R:0.5700 gloss:0.0659 dloss:0.0001 exploreP:0.0100\n",
      "Episode:424 meanR:0.9786 R:1.0000 gloss:0.0292 dloss:0.0000 exploreP:0.0100\n",
      "Episode:425 meanR:0.9772 R:0.6200 gloss:0.0529 dloss:0.0000 exploreP:0.0100\n",
      "Episode:426 meanR:0.9854 R:1.7800 gloss:0.0601 dloss:0.0001 exploreP:0.0100\n",
      "Episode:427 meanR:0.9823 R:0.3800 gloss:0.0586 dloss:0.0001 exploreP:0.0100\n",
      "Episode:428 meanR:0.9685 R:0.0000 gloss:0.0030 dloss:0.0000 exploreP:0.0100\n",
      "Episode:429 meanR:0.9580 R:0.6800 gloss:0.0117 dloss:0.0000 exploreP:0.0100\n",
      "Episode:430 meanR:0.9389 R:0.5600 gloss:0.0191 dloss:0.0000 exploreP:0.0100\n",
      "Episode:431 meanR:0.9515 R:1.4100 gloss:0.0244 dloss:0.0000 exploreP:0.0100\n",
      "Episode:432 meanR:0.9457 R:0.3600 gloss:0.0254 dloss:0.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:433 meanR:0.9399 R:0.5400 gloss:0.0168 dloss:0.0000 exploreP:0.0100\n",
      "Episode:434 meanR:0.9404 R:0.6400 gloss:0.0153 dloss:0.0000 exploreP:0.0100\n",
      "Episode:435 meanR:0.9345 R:0.8000 gloss:0.0224 dloss:0.0000 exploreP:0.0100\n",
      "Episode:436 meanR:0.9452 R:1.2500 gloss:0.0310 dloss:0.0000 exploreP:0.0100\n",
      "Episode:437 meanR:0.9430 R:0.8500 gloss:0.0254 dloss:0.0000 exploreP:0.0100\n",
      "Episode:438 meanR:0.9332 R:0.2500 gloss:0.0282 dloss:0.0000 exploreP:0.0100\n",
      "Episode:439 meanR:0.9081 R:0.9000 gloss:0.0187 dloss:0.0000 exploreP:0.0100\n",
      "Episode:440 meanR:0.8883 R:0.4200 gloss:0.0256 dloss:0.0000 exploreP:0.0100\n",
      "Episode:441 meanR:0.8795 R:0.6200 gloss:0.0200 dloss:0.0000 exploreP:0.0100\n",
      "Episode:442 meanR:0.8789 R:1.2200 gloss:0.0333 dloss:0.0000 exploreP:0.0100\n",
      "Episode:443 meanR:0.8743 R:1.5800 gloss:0.0613 dloss:0.0001 exploreP:0.0100\n",
      "Episode:444 meanR:0.8747 R:0.5800 gloss:0.0186 dloss:0.0000 exploreP:0.0100\n",
      "Episode:445 meanR:0.8727 R:0.9600 gloss:0.0353 dloss:0.0000 exploreP:0.0100\n",
      "Episode:446 meanR:0.8649 R:0.9400 gloss:0.0399 dloss:0.0000 exploreP:0.0100\n",
      "Episode:447 meanR:0.8641 R:0.0800 gloss:0.0182 dloss:0.0000 exploreP:0.0100\n",
      "Episode:448 meanR:0.8673 R:1.3500 gloss:0.0247 dloss:0.0000 exploreP:0.0100\n",
      "Episode:449 meanR:0.8670 R:0.5800 gloss:0.0425 dloss:0.0000 exploreP:0.0100\n",
      "Episode:450 meanR:0.8730 R:1.5800 gloss:0.0472 dloss:0.0001 exploreP:0.0100\n",
      "Episode:451 meanR:0.8735 R:0.7300 gloss:0.0379 dloss:0.0000 exploreP:0.0100\n",
      "Episode:452 meanR:0.8768 R:1.0300 gloss:0.0422 dloss:0.0000 exploreP:0.0100\n",
      "Episode:453 meanR:0.8795 R:1.2200 gloss:0.0416 dloss:0.0000 exploreP:0.0100\n",
      "Episode:454 meanR:0.8778 R:0.8100 gloss:0.0585 dloss:0.0001 exploreP:0.0100\n",
      "Episode:455 meanR:0.8663 R:0.9000 gloss:0.0193 dloss:0.0000 exploreP:0.0100\n",
      "Episode:456 meanR:0.8720 R:1.2000 gloss:0.0495 dloss:0.0001 exploreP:0.0100\n",
      "Episode:457 meanR:0.8770 R:1.0400 gloss:0.0485 dloss:0.0001 exploreP:0.0100\n",
      "Episode:458 meanR:0.8830 R:1.3300 gloss:0.0355 dloss:0.0000 exploreP:0.0100\n",
      "Episode:459 meanR:0.8932 R:1.8600 gloss:0.0703 dloss:0.0001 exploreP:0.0100\n",
      "Episode:460 meanR:0.8921 R:0.6300 gloss:0.0543 dloss:0.0001 exploreP:0.0100\n",
      "Episode:461 meanR:0.8844 R:0.7900 gloss:0.0468 dloss:0.0000 exploreP:0.0100\n",
      "Episode:462 meanR:0.8927 R:1.5100 gloss:0.0335 dloss:0.0000 exploreP:0.0100\n",
      "Episode:463 meanR:0.8895 R:0.8100 gloss:0.0576 dloss:0.0001 exploreP:0.0100\n",
      "Episode:464 meanR:0.8832 R:0.2600 gloss:0.0316 dloss:0.0000 exploreP:0.0100\n",
      "Episode:465 meanR:0.8806 R:0.7100 gloss:0.0273 dloss:0.0000 exploreP:0.0100\n",
      "Episode:466 meanR:0.8753 R:0.6900 gloss:0.0400 dloss:0.0000 exploreP:0.0100\n",
      "Episode:467 meanR:0.8835 R:1.5200 gloss:0.0599 dloss:0.0001 exploreP:0.0100\n",
      "Episode:468 meanR:0.8904 R:1.3100 gloss:0.0608 dloss:0.0001 exploreP:0.0100\n",
      "Episode:469 meanR:0.8982 R:1.1500 gloss:0.0391 dloss:0.0000 exploreP:0.0100\n",
      "Episode:470 meanR:0.9032 R:0.8800 gloss:0.0621 dloss:0.0001 exploreP:0.0100\n",
      "Episode:471 meanR:0.9047 R:1.1400 gloss:0.0429 dloss:0.0000 exploreP:0.0100\n",
      "Episode:472 meanR:0.9080 R:0.4000 gloss:0.0334 dloss:0.0000 exploreP:0.0100\n",
      "Episode:473 meanR:0.9093 R:0.7100 gloss:0.0212 dloss:0.0000 exploreP:0.0100\n",
      "Episode:474 meanR:0.9122 R:0.7200 gloss:0.0288 dloss:0.0000 exploreP:0.0100\n",
      "Episode:475 meanR:0.9148 R:0.2600 gloss:0.0360 dloss:0.0000 exploreP:0.0100\n",
      "Episode:476 meanR:0.9124 R:0.5600 gloss:0.0236 dloss:0.0000 exploreP:0.0100\n",
      "Episode:477 meanR:0.9118 R:0.7500 gloss:0.0304 dloss:0.0000 exploreP:0.0100\n",
      "Episode:478 meanR:0.9127 R:0.4600 gloss:0.0175 dloss:0.0000 exploreP:0.0100\n",
      "Episode:479 meanR:0.9056 R:0.8300 gloss:0.0306 dloss:0.0000 exploreP:0.0100\n",
      "Episode:480 meanR:0.9020 R:0.1100 gloss:0.0240 dloss:0.0000 exploreP:0.0100\n",
      "Episode:481 meanR:0.9003 R:1.2400 gloss:0.0203 dloss:0.0000 exploreP:0.0100\n",
      "Episode:482 meanR:0.9057 R:1.4300 gloss:0.0833 dloss:0.0001 exploreP:0.0100\n",
      "Episode:483 meanR:0.9063 R:0.2100 gloss:0.0303 dloss:0.0000 exploreP:0.0100\n",
      "Episode:484 meanR:0.9004 R:0.6500 gloss:0.0140 dloss:0.0000 exploreP:0.0100\n",
      "Episode:485 meanR:0.8977 R:0.0000 gloss:0.0238 dloss:0.0000 exploreP:0.0100\n",
      "Episode:486 meanR:0.8844 R:0.3200 gloss:0.0019 dloss:0.0000 exploreP:0.0100\n",
      "Episode:487 meanR:0.8846 R:0.1600 gloss:0.0113 dloss:0.0000 exploreP:0.0100\n",
      "Episode:488 meanR:0.8897 R:1.5200 gloss:0.0248 dloss:0.0000 exploreP:0.0100\n",
      "Episode:489 meanR:0.8831 R:1.2000 gloss:0.0441 dloss:0.0001 exploreP:0.0100\n",
      "Episode:490 meanR:0.8810 R:0.1500 gloss:0.0147 dloss:0.0000 exploreP:0.0100\n",
      "Episode:491 meanR:0.8749 R:0.6800 gloss:0.0080 dloss:0.0000 exploreP:0.0100\n",
      "Episode:492 meanR:0.8609 R:0.1800 gloss:0.0193 dloss:0.0000 exploreP:0.0100\n",
      "Episode:493 meanR:0.8616 R:0.8400 gloss:0.0132 dloss:0.0000 exploreP:0.0100\n",
      "Episode:494 meanR:0.8626 R:0.8100 gloss:0.0342 dloss:0.0000 exploreP:0.0100\n",
      "Episode:495 meanR:0.8687 R:0.8600 gloss:0.0264 dloss:0.0000 exploreP:0.0100\n",
      "Episode:496 meanR:0.8689 R:1.1500 gloss:0.0268 dloss:0.0000 exploreP:0.0100\n",
      "Episode:497 meanR:0.8692 R:0.9500 gloss:0.0464 dloss:0.0001 exploreP:0.0100\n",
      "Episode:498 meanR:0.8582 R:0.0400 gloss:0.0126 dloss:0.0000 exploreP:0.0100\n",
      "Episode:499 meanR:0.8471 R:0.6100 gloss:0.0079 dloss:0.0000 exploreP:0.0100\n",
      "Episode:500 meanR:0.8419 R:0.8000 gloss:0.0303 dloss:0.0000 exploreP:0.0100\n",
      "Episode:501 meanR:0.8426 R:1.1000 gloss:0.0408 dloss:0.0000 exploreP:0.0100\n",
      "Episode:502 meanR:0.8457 R:1.2300 gloss:0.0364 dloss:0.0000 exploreP:0.0100\n",
      "Episode:503 meanR:0.8454 R:0.4300 gloss:0.0339 dloss:0.0000 exploreP:0.0100\n",
      "Episode:504 meanR:0.8440 R:1.2100 gloss:0.0257 dloss:0.0000 exploreP:0.0100\n",
      "Episode:505 meanR:0.8359 R:0.4100 gloss:0.0305 dloss:0.0000 exploreP:0.0100\n",
      "Episode:506 meanR:0.8453 R:1.1300 gloss:0.0332 dloss:0.0000 exploreP:0.0100\n",
      "Episode:507 meanR:0.8674 R:3.0100 gloss:0.0831 dloss:0.0001 exploreP:0.0100\n",
      "Episode:508 meanR:0.8650 R:0.5300 gloss:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:509 meanR:0.8570 R:0.2800 gloss:0.0207 dloss:0.0000 exploreP:0.0100\n",
      "Episode:510 meanR:0.8611 R:1.1600 gloss:0.0251 dloss:0.0000 exploreP:0.0100\n",
      "Episode:511 meanR:0.8730 R:1.3200 gloss:0.0563 dloss:0.0001 exploreP:0.0100\n",
      "Episode:512 meanR:0.8711 R:0.3500 gloss:0.0307 dloss:0.0000 exploreP:0.0100\n",
      "Episode:513 meanR:0.8706 R:0.9600 gloss:0.0207 dloss:0.0000 exploreP:0.0100\n",
      "Episode:514 meanR:0.8718 R:0.3400 gloss:0.0258 dloss:0.0000 exploreP:0.0100\n",
      "Episode:515 meanR:0.8600 R:0.0000 gloss:0.0072 dloss:0.0000 exploreP:0.0100\n",
      "Episode:516 meanR:0.8461 R:0.0000 gloss:-0.0000 dloss:0.0000 exploreP:0.0100\n",
      "Episode:517 meanR:0.8330 R:0.2300 gloss:0.0043 dloss:0.0000 exploreP:0.0100\n",
      "Episode:518 meanR:0.8250 R:0.3300 gloss:0.0062 dloss:0.0000 exploreP:0.0100\n",
      "Episode:519 meanR:0.8120 R:0.5900 gloss:0.0121 dloss:0.0000 exploreP:0.0100\n",
      "Episode:520 meanR:0.8024 R:0.6700 gloss:0.0298 dloss:0.0000 exploreP:0.0100\n",
      "Episode:521 meanR:0.7988 R:0.8400 gloss:0.0201 dloss:0.0000 exploreP:0.0100\n",
      "Episode:522 meanR:0.8013 R:1.3900 gloss:0.0424 dloss:0.0001 exploreP:0.0100\n",
      "Episode:523 meanR:0.8054 R:0.9800 gloss:0.0395 dloss:0.0000 exploreP:0.0100\n",
      "Episode:524 meanR:0.8028 R:0.7400 gloss:0.0260 dloss:0.0000 exploreP:0.0100\n",
      "Episode:525 meanR:0.7977 R:0.1100 gloss:0.0246 dloss:0.0000 exploreP:0.0100\n",
      "Episode:526 meanR:0.7928 R:1.2900 gloss:0.0120 dloss:0.0000 exploreP:0.0100\n",
      "Episode:527 meanR:0.8079 R:1.8900 gloss:0.0645 dloss:0.0001 exploreP:0.0100\n",
      "Episode:528 meanR:0.8150 R:0.7100 gloss:0.0591 dloss:0.0001 exploreP:0.0100\n",
      "Episode:529 meanR:0.8088 R:0.0600 gloss:0.0156 dloss:0.0000 exploreP:0.0100\n",
      "Episode:530 meanR:0.8104 R:0.7200 gloss:0.0145 dloss:0.0000 exploreP:0.0100\n",
      "Episode:531 meanR:0.8005 R:0.4200 gloss:0.0204 dloss:0.0000 exploreP:0.0100\n",
      "Episode:532 meanR:0.8119 R:1.5000 gloss:0.0495 dloss:0.0001 exploreP:0.0100\n",
      "Episode:533 meanR:0.8100 R:0.3500 gloss:0.0341 dloss:0.0000 exploreP:0.0100\n",
      "Episode:534 meanR:0.8135 R:0.9900 gloss:0.0382 dloss:0.0000 exploreP:0.0100\n",
      "Episode:535 meanR:0.8093 R:0.3800 gloss:0.0196 dloss:0.0000 exploreP:0.0100\n",
      "Episode:536 meanR:0.8139 R:1.7100 gloss:0.0439 dloss:0.0000 exploreP:0.0100\n",
      "Episode:537 meanR:0.8194 R:1.4000 gloss:0.0656 dloss:0.0001 exploreP:0.0100\n",
      "Episode:538 meanR:0.8272 R:1.0300 gloss:0.0497 dloss:0.0001 exploreP:0.0100\n",
      "Episode:539 meanR:0.8362 R:1.8000 gloss:0.0514 dloss:0.0001 exploreP:0.0100\n",
      "Episode:540 meanR:0.8381 R:0.6100 gloss:0.0520 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:541 meanR:0.8468 R:1.4900 gloss:0.0502 dloss:0.0001 exploreP:0.0100\n",
      "Episode:542 meanR:0.8527 R:1.8100 gloss:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:543 meanR:0.8386 R:0.1700 gloss:0.0493 dloss:0.0001 exploreP:0.0100\n",
      "Episode:544 meanR:0.8443 R:1.1500 gloss:0.0329 dloss:0.0000 exploreP:0.0100\n",
      "Episode:545 meanR:0.8394 R:0.4700 gloss:0.0268 dloss:0.0000 exploreP:0.0100\n",
      "Episode:546 meanR:0.8431 R:1.3100 gloss:0.0332 dloss:0.0000 exploreP:0.0100\n",
      "Episode:547 meanR:0.8556 R:1.3300 gloss:0.0712 dloss:0.0001 exploreP:0.0100\n",
      "Episode:548 meanR:0.8609 R:1.8800 gloss:0.0749 dloss:0.0001 exploreP:0.0100\n",
      "Episode:549 meanR:0.8651 R:1.0000 gloss:0.0617 dloss:0.0001 exploreP:0.0100\n",
      "Episode:550 meanR:0.8542 R:0.4900 gloss:0.0409 dloss:0.0000 exploreP:0.0100\n",
      "Episode:551 meanR:0.8648 R:1.7900 gloss:0.0753 dloss:0.0001 exploreP:0.0100\n",
      "Episode:552 meanR:0.8767 R:2.2200 gloss:0.0688 dloss:0.0001 exploreP:0.0100\n",
      "Episode:553 meanR:0.8822 R:1.7700 gloss:0.0820 dloss:0.0001 exploreP:0.0100\n",
      "Episode:554 meanR:0.8782 R:0.4100 gloss:0.0647 dloss:0.0001 exploreP:0.0100\n",
      "Episode:555 meanR:0.8832 R:1.4000 gloss:0.0367 dloss:0.0000 exploreP:0.0100\n",
      "Episode:556 meanR:0.8834 R:1.2200 gloss:0.0698 dloss:0.0001 exploreP:0.0100\n",
      "Episode:557 meanR:0.8781 R:0.5100 gloss:0.0370 dloss:0.0000 exploreP:0.0100\n",
      "Episode:558 meanR:0.8797 R:1.4900 gloss:0.0222 dloss:0.0000 exploreP:0.0100\n",
      "Episode:559 meanR:0.8844 R:2.3300 gloss:0.1200 dloss:0.0002 exploreP:0.0100\n",
      "Episode:560 meanR:0.8876 R:0.9500 gloss:0.0506 dloss:0.0000 exploreP:0.0100\n",
      "Episode:561 meanR:0.8956 R:1.5900 gloss:0.0638 dloss:0.0001 exploreP:0.0100\n",
      "Episode:562 meanR:0.8926 R:1.2100 gloss:0.0654 dloss:0.0001 exploreP:0.0100\n",
      "Episode:563 meanR:0.9076 R:2.3100 gloss:0.0808 dloss:0.0001 exploreP:0.0100\n",
      "Episode:564 meanR:0.9189 R:1.3900 gloss:0.0928 dloss:0.0001 exploreP:0.0100\n",
      "Episode:565 meanR:0.9177 R:0.5900 gloss:0.0527 dloss:0.0001 exploreP:0.0100\n",
      "Episode:566 meanR:0.9321 R:2.1300 gloss:0.0678 dloss:0.0001 exploreP:0.0100\n",
      "Episode:567 meanR:0.9293 R:1.2400 gloss:0.0720 dloss:0.0001 exploreP:0.0100\n",
      "Episode:568 meanR:0.9289 R:1.2700 gloss:0.0734 dloss:0.0001 exploreP:0.0100\n",
      "Episode:569 meanR:0.9295 R:1.2100 gloss:0.0590 dloss:0.0001 exploreP:0.0100\n",
      "Episode:570 meanR:0.9360 R:1.5300 gloss:0.0790 dloss:0.0001 exploreP:0.0100\n",
      "Episode:571 meanR:0.9292 R:0.4600 gloss:0.0372 dloss:0.0000 exploreP:0.0100\n",
      "Episode:572 meanR:0.9562 R:3.1000 gloss:0.0781 dloss:0.0001 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-reacher-Continuous_Control.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                #print(action.shape)\n",
    "                #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "                #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            #batch = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            #print(targetQs.shape)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-reacher-Continuous_Control.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
