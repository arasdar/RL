{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float64, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float64, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float64, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits,\n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob_actions * targetQs) # DPG\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    dloss = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) #Qs\n",
    "    dloss += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dloss /= 2 # if dQs==gQs\n",
    "    gloss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    gloss2 = tf.reduce_mean(gQs)\n",
    "    gloss3 = tf.reduce_mean(dQs)\n",
    "    gloss4 = tf.reduce_mean(targetQs)\n",
    "    return actions_logits, gQs, gloss, dloss, gloss1, gloss2, gloss3, gloss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        \n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, glearning_rate, dlearning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2, self.g_loss3, self.g_loss4 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                           g_learning_rate=glearning_rate, d_learning_rate=dlearning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.697119985869685\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "glearning_rate = 0.0001         # Q-network learning rate\n",
    "dlearning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100              # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, \n",
    "              glearning_rate=glearning_rate, dlearning_rate=dlearning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "num_steps = 0\n",
    "for _ in range(memory_size):\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Steps:1000 meanR:0.0000 R:0.0000 gloss:0.5825 gloss1-lgP:0.8450 gloss2gQs:0.7646 gloss3dQs:0.7457 gloss4tgtQ:0.7574 dloss:0.0589 exploreP:0.9057\n",
      "Episode:1 Steps:1000 meanR:0.0600 R:0.1200 gloss:0.4479 gloss1-lgP:0.8812 gloss2gQs:0.5601 gloss3dQs:0.5463 gloss4tgtQ:0.5538 dloss:0.0374 exploreP:0.8204\n",
      "Episode:2 Steps:1000 meanR:0.0400 R:0.0000 gloss:0.2329 gloss1-lgP:0.7268 gloss2gQs:0.3202 gloss3dQs:0.3170 gloss4tgtQ:0.3165 dloss:0.0117 exploreP:0.7432\n",
      "Episode:3 Steps:1000 meanR:0.0575 R:0.1100 gloss:0.0395 gloss1-lgP:0.7081 gloss2gQs:0.0574 gloss3dQs:0.0566 gloss4tgtQ:0.0569 dloss:0.0067 exploreP:0.6734\n",
      "Episode:4 Steps:1000 meanR:0.0460 R:0.0000 gloss:0.0389 gloss1-lgP:0.7056 gloss2gQs:0.0566 gloss3dQs:0.0556 gloss4tgtQ:0.0563 dloss:0.0049 exploreP:0.6102\n",
      "Episode:5 Steps:1000 meanR:0.0383 R:0.0000 gloss:0.0188 gloss1-lgP:0.7080 gloss2gQs:0.0281 gloss3dQs:0.0269 gloss4tgtQ:0.0276 dloss:0.0036 exploreP:0.5530\n",
      "Episode:6 Steps:1000 meanR:0.0471 R:0.1000 gloss:0.0236 gloss1-lgP:0.7056 gloss2gQs:0.0344 gloss3dQs:0.0337 gloss4tgtQ:0.0342 dloss:0.0027 exploreP:0.5013\n",
      "Episode:7 Steps:1000 meanR:0.0412 R:0.0000 gloss:0.0186 gloss1-lgP:0.7097 gloss2gQs:0.0273 gloss3dQs:0.0268 gloss4tgtQ:0.0271 dloss:0.0023 exploreP:0.4545\n",
      "Episode:8 Steps:1000 meanR:0.0367 R:0.0000 gloss:0.0256 gloss1-lgP:0.7107 gloss2gQs:0.0372 gloss3dQs:0.0367 gloss4tgtQ:0.0370 dloss:0.0018 exploreP:0.4121\n",
      "Episode:9 Steps:1000 meanR:0.0330 R:0.0000 gloss:0.0230 gloss1-lgP:0.6998 gloss2gQs:0.0338 gloss3dQs:0.0329 gloss4tgtQ:0.0334 dloss:0.0013 exploreP:0.3738\n",
      "Episode:10 Steps:1000 meanR:0.0727 R:0.4700 gloss:0.0303 gloss1-lgP:0.6974 gloss2gQs:0.0441 gloss3dQs:0.0434 gloss4tgtQ:0.0438 dloss:0.0010 exploreP:0.3392\n",
      "Episode:11 Steps:1000 meanR:0.0667 R:0.0000 gloss:0.0281 gloss1-lgP:0.6927 gloss2gQs:0.0410 gloss3dQs:0.0406 gloss4tgtQ:0.0408 dloss:0.0008 exploreP:0.3078\n",
      "Episode:12 Steps:1000 meanR:0.0769 R:0.2000 gloss:0.3627 gloss1-lgP:0.9286 gloss2gQs:0.2201 gloss3dQs:0.1961 gloss4tgtQ:0.2179 dloss:0.0284 exploreP:0.2795\n",
      "Episode:13 Steps:1000 meanR:0.1243 R:0.7400 gloss:0.3490 gloss1-lgP:0.7675 gloss2gQs:0.5031 gloss3dQs:0.5019 gloss4tgtQ:0.4977 dloss:0.0169 exploreP:0.2538\n",
      "Episode:14 Steps:1000 meanR:0.1493 R:0.5000 gloss:0.2458 gloss1-lgP:0.8172 gloss2gQs:0.2699 gloss3dQs:0.2621 gloss4tgtQ:0.2671 dloss:0.0273 exploreP:0.2306\n",
      "Episode:15 Steps:1000 meanR:0.1687 R:0.4600 gloss:0.2080 gloss1-lgP:0.6854 gloss2gQs:0.3140 gloss3dQs:0.3148 gloss4tgtQ:0.3105 dloss:0.0217 exploreP:0.2096\n",
      "Episode:16 Steps:1000 meanR:0.1588 R:0.0000 gloss:0.0397 gloss1-lgP:0.6811 gloss2gQs:0.0610 gloss3dQs:0.0620 gloss4tgtQ:0.0607 dloss:0.0034 exploreP:0.1905\n",
      "Episode:17 Steps:1000 meanR:0.1883 R:0.6900 gloss:0.1967 gloss1-lgP:0.8012 gloss2gQs:0.2660 gloss3dQs:0.2597 gloss4tgtQ:0.2636 dloss:0.0146 exploreP:0.1734\n",
      "Episode:18 Steps:1000 meanR:0.1858 R:0.1400 gloss:0.0475 gloss1-lgP:0.6732 gloss2gQs:0.0728 gloss3dQs:0.0736 gloss4tgtQ:0.0724 dloss:0.0029 exploreP:0.1578\n",
      "Episode:19 Steps:1000 meanR:0.1845 R:0.1600 gloss:0.0438 gloss1-lgP:0.7911 gloss2gQs:0.0514 gloss3dQs:0.0492 gloss4tgtQ:0.0512 dloss:0.0059 exploreP:0.1437\n",
      "Episode:20 Steps:1000 meanR:0.1757 R:0.0000 gloss:0.0268 gloss1-lgP:0.7067 gloss2gQs:0.0417 gloss3dQs:0.0435 gloss4tgtQ:0.0415 dloss:0.0036 exploreP:0.1310\n",
      "Episode:21 Steps:1000 meanR:0.1759 R:0.1800 gloss:0.0419 gloss1-lgP:0.8644 gloss2gQs:0.0548 gloss3dQs:0.0539 gloss4tgtQ:0.0547 dloss:0.0035 exploreP:0.1195\n",
      "Episode:22 Steps:1000 meanR:0.1826 R:0.3300 gloss:0.0493 gloss1-lgP:0.7375 gloss2gQs:0.0692 gloss3dQs:0.0686 gloss4tgtQ:0.0690 dloss:0.0024 exploreP:0.1090\n",
      "Episode:23 Steps:1000 meanR:0.1967 R:0.5200 gloss:0.0103 gloss1-lgP:0.7610 gloss2gQs:0.0121 gloss3dQs:0.0115 gloss4tgtQ:0.0122 dloss:0.0023 exploreP:0.0996\n",
      "Episode:24 Steps:1000 meanR:0.2076 R:0.4700 gloss:0.0329 gloss1-lgP:0.6634 gloss2gQs:0.0499 gloss3dQs:0.0501 gloss4tgtQ:0.0499 dloss:0.0010 exploreP:0.0911\n",
      "Episode:25 Steps:1000 meanR:0.2019 R:0.0600 gloss:0.0099 gloss1-lgP:0.6590 gloss2gQs:0.0159 gloss3dQs:0.0160 gloss4tgtQ:0.0160 dloss:0.0005 exploreP:0.0833\n",
      "Episode:26 Steps:1000 meanR:0.1944 R:0.0000 gloss:0.0200 gloss1-lgP:0.6745 gloss2gQs:0.0304 gloss3dQs:0.0303 gloss4tgtQ:0.0305 dloss:0.0005 exploreP:0.0764\n",
      "Episode:27 Steps:1000 meanR:0.2029 R:0.4300 gloss:0.0082 gloss1-lgP:0.7562 gloss2gQs:0.0135 gloss3dQs:0.0137 gloss4tgtQ:0.0136 dloss:0.0004 exploreP:0.0700\n",
      "Episode:28 Steps:1000 meanR:0.1997 R:0.1100 gloss:0.0191 gloss1-lgP:0.6950 gloss2gQs:0.0256 gloss3dQs:0.0258 gloss4tgtQ:0.0256 dloss:0.0004 exploreP:0.0643\n",
      "Episode:29 Steps:1000 meanR:0.1990 R:0.1800 gloss:0.0058 gloss1-lgP:0.6967 gloss2gQs:0.0084 gloss3dQs:0.0087 gloss4tgtQ:0.0086 dloss:0.0002 exploreP:0.0591\n",
      "Episode:30 Steps:1000 meanR:0.1926 R:0.0000 gloss:0.0132 gloss1-lgP:0.6711 gloss2gQs:0.0201 gloss3dQs:0.0202 gloss4tgtQ:0.0202 dloss:0.0002 exploreP:0.0545\n",
      "Episode:31 Steps:1000 meanR:0.1944 R:0.2500 gloss:0.0100 gloss1-lgP:0.6482 gloss2gQs:0.0154 gloss3dQs:0.0154 gloss4tgtQ:0.0155 dloss:0.0001 exploreP:0.0502\n",
      "Episode:32 Steps:1000 meanR:0.1885 R:0.0000 gloss:0.0119 gloss1-lgP:0.6537 gloss2gQs:0.0184 gloss3dQs:0.0184 gloss4tgtQ:0.0184 dloss:0.0001 exploreP:0.0464\n",
      "Episode:33 Steps:1000 meanR:0.1953 R:0.4200 gloss:0.0136 gloss1-lgP:0.6649 gloss2gQs:0.0209 gloss3dQs:0.0208 gloss4tgtQ:0.0209 dloss:0.0001 exploreP:0.0429\n",
      "Episode:34 Steps:1000 meanR:0.2403 R:1.7700 gloss:0.0135 gloss1-lgP:0.6425 gloss2gQs:0.0210 gloss3dQs:0.0210 gloss4tgtQ:0.0210 dloss:0.0000 exploreP:0.0398\n",
      "Episode:35 Steps:1000 meanR:0.2414 R:0.2800 gloss:0.0229 gloss1-lgP:0.6460 gloss2gQs:0.0356 gloss3dQs:0.0355 gloss4tgtQ:0.0356 dloss:0.0001 exploreP:0.0370\n",
      "Episode:36 Steps:1000 meanR:0.2376 R:0.1000 gloss:0.0217 gloss1-lgP:0.6376 gloss2gQs:0.0339 gloss3dQs:0.0339 gloss4tgtQ:0.0339 dloss:0.0000 exploreP:0.0344\n",
      "Episode:37 Steps:1000 meanR:0.2468 R:0.5900 gloss:0.0229 gloss1-lgP:0.6370 gloss2gQs:0.0357 gloss3dQs:0.0357 gloss4tgtQ:0.0358 dloss:0.0001 exploreP:0.0321\n",
      "Episode:38 Steps:1000 meanR:0.2533 R:0.5000 gloss:0.0258 gloss1-lgP:0.6411 gloss2gQs:0.0399 gloss3dQs:0.0399 gloss4tgtQ:0.0399 dloss:0.0000 exploreP:0.0300\n",
      "Episode:39 Steps:1000 meanR:0.2520 R:0.2000 gloss:0.0243 gloss1-lgP:0.6417 gloss2gQs:0.0376 gloss3dQs:0.0376 gloss4tgtQ:0.0376 dloss:0.0000 exploreP:0.0281\n",
      "Episode:40 Steps:1000 meanR:0.2824 R:1.5000 gloss:0.0295 gloss1-lgP:0.6432 gloss2gQs:0.0456 gloss3dQs:0.0456 gloss4tgtQ:0.0456 dloss:0.0001 exploreP:0.0263\n",
      "Episode:41 Steps:1000 meanR:0.2902 R:0.6100 gloss:0.0329 gloss1-lgP:0.6597 gloss2gQs:0.0503 gloss3dQs:0.0503 gloss4tgtQ:0.0503 dloss:0.0001 exploreP:0.0248\n",
      "Episode:42 Steps:1000 meanR:0.2865 R:0.1300 gloss:0.0361 gloss1-lgP:0.6472 gloss2gQs:0.0555 gloss3dQs:0.0554 gloss4tgtQ:0.0555 dloss:0.0001 exploreP:0.0234\n",
      "Episode:43 Steps:1000 meanR:0.2800 R:0.0000 gloss:0.0337 gloss1-lgP:0.6466 gloss2gQs:0.0518 gloss3dQs:0.0518 gloss4tgtQ:0.0518 dloss:0.0001 exploreP:0.0221\n",
      "Episode:44 Steps:1000 meanR:0.2807 R:0.3100 gloss:0.0276 gloss1-lgP:0.6518 gloss2gQs:0.0424 gloss3dQs:0.0423 gloss4tgtQ:0.0423 dloss:0.0001 exploreP:0.0209\n",
      "Episode:45 Steps:1000 meanR:0.2761 R:0.0700 gloss:0.0255 gloss1-lgP:0.6475 gloss2gQs:0.0392 gloss3dQs:0.0392 gloss4tgtQ:0.0392 dloss:0.0001 exploreP:0.0199\n",
      "Episode:46 Steps:1000 meanR:0.2926 R:1.0500 gloss:0.0560 gloss1-lgP:0.7272 gloss2gQs:0.0596 gloss3dQs:0.0569 gloss4tgtQ:0.0595 dloss:0.0009 exploreP:0.0190\n",
      "Episode:47 Steps:1000 meanR:0.2865 R:0.0000 gloss:0.1326 gloss1-lgP:0.6480 gloss2gQs:0.2064 gloss3dQs:0.2064 gloss4tgtQ:0.2047 dloss:0.0025 exploreP:0.0181\n",
      "Episode:48 Steps:1000 meanR:0.2806 R:0.0000 gloss:0.0279 gloss1-lgP:0.6446 gloss2gQs:0.0433 gloss3dQs:0.0433 gloss4tgtQ:0.0432 dloss:0.0001 exploreP:0.0173\n",
      "Episode:49 Steps:1000 meanR:0.2868 R:0.5900 gloss:0.0227 gloss1-lgP:0.6445 gloss2gQs:0.0349 gloss3dQs:0.0349 gloss4tgtQ:0.0349 dloss:0.0001 exploreP:0.0166\n",
      "Episode:50 Steps:1000 meanR:0.2831 R:0.1000 gloss:0.0240 gloss1-lgP:0.6589 gloss2gQs:0.0368 gloss3dQs:0.0365 gloss4tgtQ:0.0366 dloss:0.0002 exploreP:0.0160\n",
      "Episode:51 Steps:1000 meanR:0.2777 R:0.0000 gloss:0.0190 gloss1-lgP:0.6403 gloss2gQs:0.0294 gloss3dQs:0.0294 gloss4tgtQ:0.0294 dloss:0.0001 exploreP:0.0154\n",
      "Episode:52 Steps:1000 meanR:0.2734 R:0.0500 gloss:0.0196 gloss1-lgP:0.6417 gloss2gQs:0.0304 gloss3dQs:0.0302 gloss4tgtQ:0.0303 dloss:0.0001 exploreP:0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:53 Steps:1000 meanR:0.2852 R:0.9100 gloss:0.0170 gloss1-lgP:0.6387 gloss2gQs:0.0263 gloss3dQs:0.0263 gloss4tgtQ:0.0263 dloss:0.0000 exploreP:0.0144\n",
      "Episode:54 Steps:1000 meanR:0.2800 R:0.0000 gloss:0.0178 gloss1-lgP:0.6459 gloss2gQs:0.0275 gloss3dQs:0.0275 gloss4tgtQ:0.0275 dloss:0.0001 exploreP:0.0140\n",
      "Episode:55 Steps:1000 meanR:0.2750 R:0.0000 gloss:0.0182 gloss1-lgP:0.6449 gloss2gQs:0.0283 gloss3dQs:0.0283 gloss4tgtQ:0.0283 dloss:0.0001 exploreP:0.0136\n",
      "Episode:56 Steps:1000 meanR:0.2853 R:0.8600 gloss:0.1464 gloss1-lgP:0.7928 gloss2gQs:0.1825 gloss3dQs:0.1789 gloss4tgtQ:0.1808 dloss:0.0037 exploreP:0.0133\n",
      "Episode:57 Steps:1000 meanR:0.2803 R:0.0000 gloss:0.0253 gloss1-lgP:0.6332 gloss2gQs:0.0399 gloss3dQs:0.0399 gloss4tgtQ:0.0397 dloss:0.0001 exploreP:0.0130\n",
      "Episode:58 Steps:1000 meanR:0.2834 R:0.4600 gloss:0.0124 gloss1-lgP:0.6323 gloss2gQs:0.0193 gloss3dQs:0.0192 gloss4tgtQ:0.0193 dloss:0.0001 exploreP:0.0127\n",
      "Episode:59 Steps:1000 meanR:0.2880 R:0.5600 gloss:0.0142 gloss1-lgP:0.6335 gloss2gQs:0.0221 gloss3dQs:0.0221 gloss4tgtQ:0.0221 dloss:0.0000 exploreP:0.0124\n",
      "Episode:60 Steps:1000 meanR:0.2833 R:0.0000 gloss:0.0148 gloss1-lgP:0.6348 gloss2gQs:0.0231 gloss3dQs:0.0231 gloss4tgtQ:0.0231 dloss:0.0000 exploreP:0.0122\n",
      "Episode:61 Steps:1000 meanR:0.2877 R:0.5600 gloss:0.0333 gloss1-lgP:0.7098 gloss2gQs:0.0424 gloss3dQs:0.0420 gloss4tgtQ:0.0423 dloss:0.0003 exploreP:0.0120\n",
      "Episode:62 Steps:1000 meanR:0.2933 R:0.6400 gloss:0.0374 gloss1-lgP:0.6654 gloss2gQs:0.0564 gloss3dQs:0.0556 gloss4tgtQ:0.0560 dloss:0.0003 exploreP:0.0118\n",
      "Episode:63 Steps:1000 meanR:0.2902 R:0.0900 gloss:0.0203 gloss1-lgP:0.6380 gloss2gQs:0.0315 gloss3dQs:0.0315 gloss4tgtQ:0.0315 dloss:0.0000 exploreP:0.0116\n",
      "Episode:64 Steps:1000 meanR:0.3120 R:1.7100 gloss:0.0209 gloss1-lgP:0.6382 gloss2gQs:0.0325 gloss3dQs:0.0325 gloss4tgtQ:0.0325 dloss:0.0000 exploreP:0.0115\n",
      "Episode:65 Steps:1000 meanR:0.3132 R:0.3900 gloss:0.0295 gloss1-lgP:0.6421 gloss2gQs:0.0456 gloss3dQs:0.0456 gloss4tgtQ:0.0457 dloss:0.0001 exploreP:0.0113\n",
      "Episode:66 Steps:1000 meanR:0.3107 R:0.1500 gloss:0.0291 gloss1-lgP:0.6434 gloss2gQs:0.0449 gloss3dQs:0.0449 gloss4tgtQ:0.0449 dloss:0.0001 exploreP:0.0112\n",
      "Episode:67 Steps:1000 meanR:0.3101 R:0.2700 gloss:0.0277 gloss1-lgP:0.6432 gloss2gQs:0.0427 gloss3dQs:0.0427 gloss4tgtQ:0.0427 dloss:0.0001 exploreP:0.0111\n",
      "Episode:68 Steps:1000 meanR:0.3128 R:0.4900 gloss:0.0287 gloss1-lgP:0.6450 gloss2gQs:0.0442 gloss3dQs:0.0441 gloss4tgtQ:0.0441 dloss:0.0001 exploreP:0.0110\n",
      "Episode:69 Steps:1000 meanR:0.3150 R:0.4700 gloss:0.0307 gloss1-lgP:0.6596 gloss2gQs:0.0468 gloss3dQs:0.0467 gloss4tgtQ:0.0467 dloss:0.0001 exploreP:0.0109\n",
      "Episode:70 Steps:1000 meanR:0.3106 R:0.0000 gloss:0.0266 gloss1-lgP:0.6402 gloss2gQs:0.0410 gloss3dQs:0.0410 gloss4tgtQ:0.0410 dloss:0.0001 exploreP:0.0108\n",
      "Episode:71 Steps:1000 meanR:0.3160 R:0.7000 gloss:0.0261 gloss1-lgP:0.6418 gloss2gQs:0.0402 gloss3dQs:0.0403 gloss4tgtQ:0.0403 dloss:0.0001 exploreP:0.0107\n",
      "Episode:72 Steps:1000 meanR:0.3160 R:0.3200 gloss:0.0288 gloss1-lgP:0.6427 gloss2gQs:0.0443 gloss3dQs:0.0443 gloss4tgtQ:0.0443 dloss:0.0001 exploreP:0.0107\n",
      "Episode:73 Steps:1000 meanR:0.3126 R:0.0600 gloss:0.0276 gloss1-lgP:0.6375 gloss2gQs:0.0426 gloss3dQs:0.0426 gloss4tgtQ:0.0426 dloss:0.0001 exploreP:0.0106\n",
      "Episode:74 Steps:1000 meanR:0.3112 R:0.2100 gloss:0.0891 gloss1-lgP:0.7683 gloss2gQs:0.0999 gloss3dQs:0.0975 gloss4tgtQ:0.0991 dloss:0.0011 exploreP:0.0105\n",
      "Episode:75 Steps:1000 meanR:0.3071 R:0.0000 gloss:0.0496 gloss1-lgP:0.6372 gloss2gQs:0.0783 gloss3dQs:0.0783 gloss4tgtQ:0.0778 dloss:0.0003 exploreP:0.0105\n",
      "Episode:76 Steps:1000 meanR:0.3044 R:0.1000 gloss:0.0184 gloss1-lgP:0.6353 gloss2gQs:0.0286 gloss3dQs:0.0286 gloss4tgtQ:0.0285 dloss:0.0000 exploreP:0.0104\n",
      "Episode:77 Steps:1000 meanR:0.3035 R:0.2300 gloss:0.0147 gloss1-lgP:0.6351 gloss2gQs:0.0229 gloss3dQs:0.0228 gloss4tgtQ:0.0228 dloss:0.0000 exploreP:0.0104\n",
      "Episode:78 Steps:1000 meanR:0.3010 R:0.1100 gloss:0.0131 gloss1-lgP:0.6347 gloss2gQs:0.0203 gloss3dQs:0.0203 gloss4tgtQ:0.0203 dloss:0.0000 exploreP:0.0104\n",
      "Episode:79 Steps:1000 meanR:0.3045 R:0.5800 gloss:0.0134 gloss1-lgP:0.6340 gloss2gQs:0.0208 gloss3dQs:0.0208 gloss4tgtQ:0.0208 dloss:0.0000 exploreP:0.0103\n",
      "Episode:80 Steps:1000 meanR:0.3164 R:1.2700 gloss:0.0134 gloss1-lgP:0.6343 gloss2gQs:0.0207 gloss3dQs:0.0207 gloss4tgtQ:0.0207 dloss:0.0000 exploreP:0.0103\n",
      "Episode:81 Steps:1000 meanR:0.3216 R:0.7400 gloss:0.0208 gloss1-lgP:0.6361 gloss2gQs:0.0321 gloss3dQs:0.0321 gloss4tgtQ:0.0322 dloss:0.0001 exploreP:0.0103\n",
      "Episode:82 Steps:1000 meanR:0.3408 R:1.9200 gloss:-0.0094 gloss1-lgP:0.6746 gloss2gQs:0.0250 gloss3dQs:0.0268 gloss4tgtQ:0.0252 dloss:0.0004 exploreP:0.0102\n",
      "Episode:83 Steps:1000 meanR:0.3379 R:0.0900 gloss:1.1813 gloss1-lgP:1.0242 gloss2gQs:1.0180 gloss3dQs:0.9324 gloss4tgtQ:1.0078 dloss:0.1799 exploreP:0.0102\n",
      "Episode:84 Steps:1000 meanR:0.3339 R:0.0000 gloss:0.2852 gloss1-lgP:0.6365 gloss2gQs:0.4571 gloss3dQs:0.4564 gloss4tgtQ:0.4532 dloss:0.0029 exploreP:0.0102\n",
      "Episode:85 Steps:1000 meanR:0.3328 R:0.2400 gloss:0.1432 gloss1-lgP:0.6346 gloss2gQs:0.2286 gloss3dQs:0.2284 gloss4tgtQ:0.2265 dloss:0.0015 exploreP:0.0102\n",
      "Episode:86 Steps:1000 meanR:0.3290 R:0.0000 gloss:0.0654 gloss1-lgP:0.6342 gloss2gQs:0.1026 gloss3dQs:0.1028 gloss4tgtQ:0.1021 dloss:0.0011 exploreP:0.0102\n",
      "Episode:87 Steps:1000 meanR:0.3312 R:0.5300 gloss:0.0477 gloss1-lgP:0.6351 gloss2gQs:0.0740 gloss3dQs:0.0741 gloss4tgtQ:0.0737 dloss:0.0008 exploreP:0.0101\n",
      "Episode:88 Steps:1000 meanR:0.3338 R:0.5600 gloss:0.0413 gloss1-lgP:0.6977 gloss2gQs:0.0612 gloss3dQs:0.0613 gloss4tgtQ:0.0612 dloss:0.0017 exploreP:0.0101\n",
      "Episode:89 Steps:1000 meanR:0.3318 R:0.1500 gloss:0.0403 gloss1-lgP:0.6384 gloss2gQs:0.0623 gloss3dQs:0.0624 gloss4tgtQ:0.0622 dloss:0.0007 exploreP:0.0101\n",
      "Episode:90 Steps:1000 meanR:0.3281 R:0.0000 gloss:0.0315 gloss1-lgP:0.6349 gloss2gQs:0.0488 gloss3dQs:0.0488 gloss4tgtQ:0.0488 dloss:0.0004 exploreP:0.0101\n",
      "Episode:91 Steps:1000 meanR:0.3336 R:0.8300 gloss:0.0519 gloss1-lgP:0.6697 gloss2gQs:0.0779 gloss3dQs:0.0771 gloss4tgtQ:0.0775 dloss:0.0019 exploreP:0.0101\n",
      "Episode:92 Steps:1000 meanR:0.3347 R:0.4400 gloss:0.0202 gloss1-lgP:0.6343 gloss2gQs:0.0313 gloss3dQs:0.0315 gloss4tgtQ:0.0313 dloss:0.0002 exploreP:0.0101\n",
      "Episode:93 Steps:1000 meanR:0.3320 R:0.0800 gloss:0.0164 gloss1-lgP:0.6396 gloss2gQs:0.0253 gloss3dQs:0.0253 gloss4tgtQ:0.0253 dloss:0.0001 exploreP:0.0101\n",
      "Episode:94 Steps:1000 meanR:0.3345 R:0.5700 gloss:0.0266 gloss1-lgP:0.6500 gloss2gQs:0.0410 gloss3dQs:0.0409 gloss4tgtQ:0.0410 dloss:0.0002 exploreP:0.0101\n",
      "Episode:95 Steps:1000 meanR:0.3310 R:0.0000 gloss:0.0200 gloss1-lgP:0.6440 gloss2gQs:0.0307 gloss3dQs:0.0307 gloss4tgtQ:0.0307 dloss:0.0001 exploreP:0.0101\n",
      "Episode:96 Steps:1000 meanR:0.3276 R:0.0000 gloss:0.0172 gloss1-lgP:0.6414 gloss2gQs:0.0264 gloss3dQs:0.0264 gloss4tgtQ:0.0264 dloss:0.0001 exploreP:0.0101\n",
      "Episode:97 Steps:1000 meanR:0.3279 R:0.3500 gloss:-0.4377 gloss1-lgP:2.7197 gloss2gQs:-0.0664 gloss3dQs:-0.1325 gloss4tgtQ:-0.0656 dloss:0.0851 exploreP:0.0101\n",
      "Episode:98 Steps:1000 meanR:0.3306 R:0.6000 gloss:4.8400 gloss1-lgP:4.3816 gloss2gQs:0.6715 gloss3dQs:0.6725 gloss4tgtQ:0.6644 dloss:0.1198 exploreP:0.0100\n",
      "Episode:99 Steps:1000 meanR:0.3344 R:0.7100 gloss:0.2889 gloss1-lgP:0.6710 gloss2gQs:0.4315 gloss3dQs:0.4301 gloss4tgtQ:0.4273 dloss:0.0025 exploreP:0.0100\n",
      "Episode:100 Steps:1000 meanR:0.3344 R:0.0000 gloss:0.1326 gloss1-lgP:0.6470 gloss2gQs:0.2076 gloss3dQs:0.2074 gloss4tgtQ:0.2054 dloss:0.0011 exploreP:0.0100\n",
      "Episode:101 Steps:1000 meanR:0.3387 R:0.5500 gloss:0.0502 gloss1-lgP:0.6411 gloss2gQs:0.0799 gloss3dQs:0.0805 gloss4tgtQ:0.0794 dloss:0.0008 exploreP:0.0100\n",
      "Episode:102 Steps:1000 meanR:0.3387 R:0.0000 gloss:0.0284 gloss1-lgP:0.7190 gloss2gQs:0.0488 gloss3dQs:0.0485 gloss4tgtQ:0.0486 dloss:0.0006 exploreP:0.0100\n",
      "Episode:103 Steps:1000 meanR:0.3407 R:0.3100 gloss:0.0305 gloss1-lgP:0.7288 gloss2gQs:0.0508 gloss3dQs:0.0508 gloss4tgtQ:0.0507 dloss:0.0003 exploreP:0.0100\n",
      "Episode:104 Steps:1000 meanR:0.3462 R:0.5500 gloss:0.0217 gloss1-lgP:0.7207 gloss2gQs:0.0372 gloss3dQs:0.0372 gloss4tgtQ:0.0370 dloss:0.0002 exploreP:0.0100\n",
      "Episode:105 Steps:1000 meanR:0.3567 R:1.0500 gloss:0.0195 gloss1-lgP:0.7749 gloss2gQs:0.0306 gloss3dQs:0.0306 gloss4tgtQ:0.0306 dloss:0.0004 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:106 Steps:1000 meanR:0.3633 R:0.7600 gloss:0.0171 gloss1-lgP:0.6578 gloss2gQs:0.0289 gloss3dQs:0.0291 gloss4tgtQ:0.0291 dloss:0.0001 exploreP:0.0100\n",
      "Episode:107 Steps:1000 meanR:0.3745 R:1.1200 gloss:0.0237 gloss1-lgP:0.5881 gloss2gQs:0.0403 gloss3dQs:0.0404 gloss4tgtQ:0.0404 dloss:0.0001 exploreP:0.0100\n",
      "Episode:108 Steps:1000 meanR:0.3875 R:1.3000 gloss:0.0280 gloss1-lgP:0.5935 gloss2gQs:0.0473 gloss3dQs:0.0475 gloss4tgtQ:0.0474 dloss:0.0002 exploreP:0.0100\n",
      "Episode:109 Steps:1000 meanR:0.3997 R:1.2200 gloss:0.0323 gloss1-lgP:0.5870 gloss2gQs:0.0547 gloss3dQs:0.0547 gloss4tgtQ:0.0548 dloss:0.0001 exploreP:0.0100\n",
      "Episode:110 Steps:1000 meanR:0.4087 R:1.3700 gloss:0.0416 gloss1-lgP:0.5956 gloss2gQs:0.0712 gloss3dQs:0.0711 gloss4tgtQ:0.0712 dloss:0.0001 exploreP:0.0100\n",
      "Episode:111 Steps:1000 meanR:0.4189 R:1.0200 gloss:0.0442 gloss1-lgP:0.5768 gloss2gQs:0.0764 gloss3dQs:0.0764 gloss4tgtQ:0.0765 dloss:0.0001 exploreP:0.0100\n",
      "Episode:112 Steps:1000 meanR:0.4275 R:1.0600 gloss:0.0503 gloss1-lgP:0.5738 gloss2gQs:0.0877 gloss3dQs:0.0877 gloss4tgtQ:0.0877 dloss:0.0001 exploreP:0.0100\n",
      "Episode:113 Steps:1000 meanR:0.4298 R:0.9700 gloss:0.0499 gloss1-lgP:0.5712 gloss2gQs:0.0875 gloss3dQs:0.0875 gloss4tgtQ:0.0875 dloss:0.0001 exploreP:0.0100\n",
      "Episode:114 Steps:1000 meanR:0.4346 R:0.9800 gloss:0.0520 gloss1-lgP:0.5724 gloss2gQs:0.0911 gloss3dQs:0.0911 gloss4tgtQ:0.0911 dloss:0.0001 exploreP:0.0100\n",
      "Episode:115 Steps:1000 meanR:0.4367 R:0.6700 gloss:0.0553 gloss1-lgP:0.5739 gloss2gQs:0.0962 gloss3dQs:0.0962 gloss4tgtQ:0.0962 dloss:0.0001 exploreP:0.0100\n",
      "Episode:116 Steps:1000 meanR:0.4401 R:0.3400 gloss:0.0565 gloss1-lgP:0.5790 gloss2gQs:0.0982 gloss3dQs:0.0982 gloss4tgtQ:0.0982 dloss:0.0001 exploreP:0.0100\n",
      "Episode:117 Steps:1000 meanR:0.4387 R:0.5500 gloss:0.0497 gloss1-lgP:0.5804 gloss2gQs:0.0858 gloss3dQs:0.0857 gloss4tgtQ:0.0858 dloss:0.0001 exploreP:0.0100\n",
      "Episode:118 Steps:1000 meanR:0.4406 R:0.3300 gloss:0.0495 gloss1-lgP:0.5799 gloss2gQs:0.0858 gloss3dQs:0.0858 gloss4tgtQ:0.0858 dloss:0.0001 exploreP:0.0100\n",
      "Episode:119 Steps:1000 meanR:0.4436 R:0.4600 gloss:0.0454 gloss1-lgP:0.5784 gloss2gQs:0.0789 gloss3dQs:0.0788 gloss4tgtQ:0.0788 dloss:0.0001 exploreP:0.0100\n",
      "Episode:120 Steps:1000 meanR:0.4555 R:1.1900 gloss:0.0362 gloss1-lgP:0.5798 gloss2gQs:0.0625 gloss3dQs:0.0625 gloss4tgtQ:0.0625 dloss:0.0001 exploreP:0.0100\n",
      "Episode:121 Steps:1000 meanR:0.4631 R:0.9400 gloss:0.0406 gloss1-lgP:0.5739 gloss2gQs:0.0707 gloss3dQs:0.0707 gloss4tgtQ:0.0707 dloss:0.0001 exploreP:0.0100\n",
      "Episode:122 Steps:1000 meanR:0.4734 R:1.3600 gloss:0.0406 gloss1-lgP:0.5739 gloss2gQs:0.0704 gloss3dQs:0.0704 gloss4tgtQ:0.0704 dloss:0.0001 exploreP:0.0100\n",
      "Episode:123 Steps:1000 meanR:0.4725 R:0.4300 gloss:0.0404 gloss1-lgP:0.5758 gloss2gQs:0.0702 gloss3dQs:0.0702 gloss4tgtQ:0.0701 dloss:0.0001 exploreP:0.0100\n",
      "Episode:124 Steps:1000 meanR:0.4849 R:1.7100 gloss:0.0393 gloss1-lgP:0.5797 gloss2gQs:0.0675 gloss3dQs:0.0675 gloss4tgtQ:0.0676 dloss:0.0001 exploreP:0.0100\n",
      "Episode:125 Steps:1000 meanR:0.4943 R:1.0000 gloss:0.0424 gloss1-lgP:0.5795 gloss2gQs:0.0731 gloss3dQs:0.0730 gloss4tgtQ:0.0731 dloss:0.0001 exploreP:0.0100\n",
      "Episode:126 Steps:1000 meanR:0.5027 R:0.8400 gloss:0.0444 gloss1-lgP:0.5818 gloss2gQs:0.0761 gloss3dQs:0.0761 gloss4tgtQ:0.0761 dloss:0.0001 exploreP:0.0100\n",
      "Episode:127 Steps:1000 meanR:0.5009 R:0.2500 gloss:0.0464 gloss1-lgP:0.5831 gloss2gQs:0.0794 gloss3dQs:0.0794 gloss4tgtQ:0.0794 dloss:0.0001 exploreP:0.0100\n",
      "Episode:128 Steps:1000 meanR:0.5039 R:0.4100 gloss:0.0462 gloss1-lgP:0.5823 gloss2gQs:0.0791 gloss3dQs:0.0791 gloss4tgtQ:0.0791 dloss:0.0001 exploreP:0.0100\n",
      "Episode:129 Steps:1000 meanR:0.5061 R:0.4000 gloss:0.0484 gloss1-lgP:0.5840 gloss2gQs:0.0829 gloss3dQs:0.0828 gloss4tgtQ:0.0828 dloss:0.0001 exploreP:0.0100\n",
      "Episode:130 Steps:1000 meanR:0.5072 R:0.1100 gloss:0.0475 gloss1-lgP:0.5877 gloss2gQs:0.0809 gloss3dQs:0.0809 gloss4tgtQ:0.0809 dloss:0.0001 exploreP:0.0100\n",
      "Episode:131 Steps:1000 meanR:0.5127 R:0.8000 gloss:0.0441 gloss1-lgP:0.5931 gloss2gQs:0.0747 gloss3dQs:0.0746 gloss4tgtQ:0.0746 dloss:0.0001 exploreP:0.0100\n",
      "Episode:132 Steps:1000 meanR:0.5205 R:0.7800 gloss:0.0404 gloss1-lgP:0.5946 gloss2gQs:0.0683 gloss3dQs:0.0683 gloss4tgtQ:0.0683 dloss:0.0001 exploreP:0.0100\n",
      "Episode:133 Steps:1000 meanR:0.5236 R:0.7300 gloss:0.0378 gloss1-lgP:0.5941 gloss2gQs:0.0640 gloss3dQs:0.0640 gloss4tgtQ:0.0639 dloss:0.0001 exploreP:0.0100\n",
      "Episode:134 Steps:1000 meanR:0.5121 R:0.6200 gloss:0.0356 gloss1-lgP:0.5901 gloss2gQs:0.0607 gloss3dQs:0.0607 gloss4tgtQ:0.0607 dloss:0.0001 exploreP:0.0100\n",
      "Episode:135 Steps:1000 meanR:0.5095 R:0.0200 gloss:0.0283 gloss1-lgP:0.5897 gloss2gQs:0.0483 gloss3dQs:0.0483 gloss4tgtQ:0.0483 dloss:0.0001 exploreP:0.0100\n",
      "Episode:136 Steps:1000 meanR:0.5238 R:1.5300 gloss:0.0283 gloss1-lgP:0.5889 gloss2gQs:0.0484 gloss3dQs:0.0484 gloss4tgtQ:0.0484 dloss:0.0001 exploreP:0.0100\n",
      "Episode:137 Steps:1000 meanR:0.5259 R:0.8000 gloss:0.0318 gloss1-lgP:0.5883 gloss2gQs:0.0545 gloss3dQs:0.0546 gloss4tgtQ:0.0545 dloss:0.0001 exploreP:0.0100\n",
      "Episode:138 Steps:1000 meanR:0.5298 R:0.8900 gloss:0.0321 gloss1-lgP:0.5891 gloss2gQs:0.0548 gloss3dQs:0.0548 gloss4tgtQ:0.0549 dloss:0.0001 exploreP:0.0100\n",
      "Episode:139 Steps:1000 meanR:0.5305 R:0.2700 gloss:0.0328 gloss1-lgP:0.5883 gloss2gQs:0.0560 gloss3dQs:0.0560 gloss4tgtQ:0.0560 dloss:0.0001 exploreP:0.0100\n",
      "Episode:140 Steps:1000 meanR:0.5261 R:1.0600 gloss:0.0368 gloss1-lgP:0.5862 gloss2gQs:0.0630 gloss3dQs:0.0630 gloss4tgtQ:0.0630 dloss:0.0001 exploreP:0.0100\n",
      "Episode:141 Steps:1000 meanR:0.5264 R:0.6400 gloss:0.0355 gloss1-lgP:0.5822 gloss2gQs:0.0607 gloss3dQs:0.0607 gloss4tgtQ:0.0607 dloss:0.0001 exploreP:0.0100\n",
      "Episode:142 Steps:1000 meanR:0.5316 R:0.6500 gloss:0.0378 gloss1-lgP:0.5825 gloss2gQs:0.0645 gloss3dQs:0.0645 gloss4tgtQ:0.0645 dloss:0.0001 exploreP:0.0100\n",
      "Episode:143 Steps:1000 meanR:0.5407 R:0.9100 gloss:0.0394 gloss1-lgP:0.5836 gloss2gQs:0.0671 gloss3dQs:0.0671 gloss4tgtQ:0.0671 dloss:0.0001 exploreP:0.0100\n",
      "Episode:144 Steps:1000 meanR:0.5475 R:0.9900 gloss:0.0451 gloss1-lgP:0.6412 gloss2gQs:0.0740 gloss3dQs:0.0739 gloss4tgtQ:0.0740 dloss:0.0001 exploreP:0.0100\n",
      "Episode:145 Steps:1000 meanR:0.5508 R:0.4000 gloss:0.0436 gloss1-lgP:0.5879 gloss2gQs:0.0736 gloss3dQs:0.0736 gloss4tgtQ:0.0736 dloss:0.0001 exploreP:0.0100\n",
      "Episode:146 Steps:1000 meanR:0.5486 R:0.8300 gloss:0.0411 gloss1-lgP:0.5886 gloss2gQs:0.0693 gloss3dQs:0.0694 gloss4tgtQ:0.0694 dloss:0.0001 exploreP:0.0100\n",
      "Episode:147 Steps:1000 meanR:0.5562 R:0.7600 gloss:0.0406 gloss1-lgP:0.5880 gloss2gQs:0.0687 gloss3dQs:0.0687 gloss4tgtQ:0.0687 dloss:0.0001 exploreP:0.0100\n",
      "Episode:148 Steps:1000 meanR:0.5593 R:0.3100 gloss:0.0379 gloss1-lgP:0.5892 gloss2gQs:0.0640 gloss3dQs:0.0640 gloss4tgtQ:0.0640 dloss:0.0001 exploreP:0.0100\n",
      "Episode:149 Steps:1000 meanR:0.5614 R:0.8000 gloss:0.0369 gloss1-lgP:0.5890 gloss2gQs:0.0621 gloss3dQs:0.0621 gloss4tgtQ:0.0621 dloss:0.0001 exploreP:0.0100\n",
      "Episode:150 Steps:1000 meanR:0.5699 R:0.9500 gloss:0.0385 gloss1-lgP:0.5891 gloss2gQs:0.0648 gloss3dQs:0.0648 gloss4tgtQ:0.0648 dloss:0.0001 exploreP:0.0100\n",
      "Episode:151 Steps:1000 meanR:0.5800 R:1.0100 gloss:0.0390 gloss1-lgP:0.5912 gloss2gQs:0.0655 gloss3dQs:0.0656 gloss4tgtQ:0.0656 dloss:0.0001 exploreP:0.0100\n",
      "Episode:152 Steps:1000 meanR:0.5861 R:0.6600 gloss:0.0394 gloss1-lgP:0.5918 gloss2gQs:0.0664 gloss3dQs:0.0665 gloss4tgtQ:0.0664 dloss:0.0001 exploreP:0.0100\n",
      "Episode:153 Steps:1000 meanR:0.5801 R:0.3100 gloss:0.0402 gloss1-lgP:0.5923 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:154 Steps:1000 meanR:0.5822 R:0.2100 gloss:0.0401 gloss1-lgP:0.5928 gloss2gQs:0.0677 gloss3dQs:0.0677 gloss4tgtQ:0.0677 dloss:0.0001 exploreP:0.0100\n",
      "Episode:155 Steps:1000 meanR:0.5883 R:0.6100 gloss:0.0336 gloss1-lgP:0.5914 gloss2gQs:0.0569 gloss3dQs:0.0569 gloss4tgtQ:0.0569 dloss:0.0001 exploreP:0.0100\n",
      "Episode:156 Steps:1000 meanR:0.5897 R:1.0000 gloss:0.0353 gloss1-lgP:0.5920 gloss2gQs:0.0596 gloss3dQs:0.0597 gloss4tgtQ:0.0596 dloss:0.0001 exploreP:0.0100\n",
      "Episode:157 Steps:1000 meanR:0.5934 R:0.3700 gloss:0.0354 gloss1-lgP:0.5937 gloss2gQs:0.0597 gloss3dQs:0.0597 gloss4tgtQ:0.0597 dloss:0.0001 exploreP:0.0100\n",
      "Episode:158 Steps:1000 meanR:0.5929 R:0.4100 gloss:0.0373 gloss1-lgP:0.5929 gloss2gQs:0.0628 gloss3dQs:0.0629 gloss4tgtQ:0.0629 dloss:0.0001 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:159 Steps:1000 meanR:0.5935 R:0.6200 gloss:0.0332 gloss1-lgP:0.5950 gloss2gQs:0.0560 gloss3dQs:0.0560 gloss4tgtQ:0.0560 dloss:0.0001 exploreP:0.0100\n",
      "Episode:160 Steps:1000 meanR:0.5949 R:0.1400 gloss:0.0339 gloss1-lgP:0.5950 gloss2gQs:0.0572 gloss3dQs:0.0572 gloss4tgtQ:0.0572 dloss:0.0001 exploreP:0.0100\n",
      "Episode:161 Steps:1000 meanR:0.5926 R:0.3300 gloss:0.0281 gloss1-lgP:0.5936 gloss2gQs:0.0474 gloss3dQs:0.0474 gloss4tgtQ:0.0474 dloss:0.0001 exploreP:0.0100\n",
      "Episode:162 Steps:1000 meanR:0.5877 R:0.1500 gloss:0.0247 gloss1-lgP:0.5930 gloss2gQs:0.0416 gloss3dQs:0.0416 gloss4tgtQ:0.0416 dloss:0.0000 exploreP:0.0100\n",
      "Episode:163 Steps:1000 meanR:0.5929 R:0.6100 gloss:0.0217 gloss1-lgP:0.6012 gloss2gQs:0.0366 gloss3dQs:0.0365 gloss4tgtQ:0.0365 dloss:0.0000 exploreP:0.0100\n",
      "Episode:164 Steps:1000 meanR:0.5836 R:0.7800 gloss:0.0244 gloss1-lgP:0.5941 gloss2gQs:0.0411 gloss3dQs:0.0410 gloss4tgtQ:0.0411 dloss:0.0000 exploreP:0.0100\n",
      "Episode:165 Steps:1000 meanR:0.5836 R:0.3900 gloss:0.0281 gloss1-lgP:0.5950 gloss2gQs:0.0474 gloss3dQs:0.0473 gloss4tgtQ:0.0474 dloss:0.0000 exploreP:0.0100\n",
      "Episode:166 Steps:1000 meanR:0.5941 R:1.2000 gloss:0.0280 gloss1-lgP:0.5943 gloss2gQs:0.0472 gloss3dQs:0.0472 gloss4tgtQ:0.0472 dloss:0.0000 exploreP:0.0100\n",
      "Episode:167 Steps:1000 meanR:0.6047 R:1.3300 gloss:0.0304 gloss1-lgP:0.5984 gloss2gQs:0.0508 gloss3dQs:0.0508 gloss4tgtQ:0.0508 dloss:0.0001 exploreP:0.0100\n",
      "Episode:168 Steps:1000 meanR:0.6052 R:0.5400 gloss:0.0313 gloss1-lgP:0.5957 gloss2gQs:0.0523 gloss3dQs:0.0524 gloss4tgtQ:0.0524 dloss:0.0001 exploreP:0.0100\n",
      "Episode:169 Steps:1000 meanR:0.6063 R:0.5800 gloss:0.0341 gloss1-lgP:0.5934 gloss2gQs:0.0573 gloss3dQs:0.0573 gloss4tgtQ:0.0573 dloss:0.0001 exploreP:0.0100\n",
      "Episode:170 Steps:1000 meanR:0.6128 R:0.6500 gloss:0.0322 gloss1-lgP:0.5964 gloss2gQs:0.0536 gloss3dQs:0.0536 gloss4tgtQ:0.0536 dloss:0.0001 exploreP:0.0100\n",
      "Episode:171 Steps:1000 meanR:0.6092 R:0.3400 gloss:0.8284 gloss1-lgP:1.8369 gloss2gQs:0.1252 gloss3dQs:0.0860 gloss4tgtQ:0.1247 dloss:0.0288 exploreP:0.0100\n",
      "Episode:172 Steps:1000 meanR:0.6136 R:0.7600 gloss:0.3159 gloss1-lgP:0.6581 gloss2gQs:0.4475 gloss3dQs:0.4511 gloss4tgtQ:0.4435 dloss:0.0039 exploreP:0.0100\n",
      "Episode:173 Steps:1000 meanR:0.6173 R:0.4300 gloss:0.0835 gloss1-lgP:0.6002 gloss2gQs:0.1409 gloss3dQs:0.1408 gloss4tgtQ:0.1401 dloss:0.0004 exploreP:0.0100\n",
      "Episode:174 Steps:1000 meanR:0.6289 R:1.3700 gloss:0.0520 gloss1-lgP:0.5997 gloss2gQs:0.0865 gloss3dQs:0.0866 gloss4tgtQ:0.0864 dloss:0.0002 exploreP:0.0100\n",
      "Episode:175 Steps:1000 meanR:0.6422 R:1.3300 gloss:0.0460 gloss1-lgP:0.6014 gloss2gQs:0.0766 gloss3dQs:0.0766 gloss4tgtQ:0.0765 dloss:0.0002 exploreP:0.0100\n",
      "Episode:176 Steps:1000 meanR:0.6455 R:0.4300 gloss:0.0455 gloss1-lgP:0.6004 gloss2gQs:0.0759 gloss3dQs:0.0759 gloss4tgtQ:0.0759 dloss:0.0001 exploreP:0.0100\n",
      "Episode:177 Steps:1000 meanR:0.6492 R:0.6000 gloss:0.0410 gloss1-lgP:0.5990 gloss2gQs:0.0690 gloss3dQs:0.0690 gloss4tgtQ:0.0689 dloss:0.0001 exploreP:0.0100\n",
      "Episode:178 Steps:1000 meanR:0.6552 R:0.7100 gloss:0.0393 gloss1-lgP:0.5980 gloss2gQs:0.0663 gloss3dQs:0.0662 gloss4tgtQ:0.0662 dloss:0.0001 exploreP:0.0100\n",
      "Episode:179 Steps:1000 meanR:0.6503 R:0.0900 gloss:0.0360 gloss1-lgP:0.5986 gloss2gQs:0.0602 gloss3dQs:0.0602 gloss4tgtQ:0.0602 dloss:0.0001 exploreP:0.0100\n",
      "Episode:180 Steps:1000 meanR:0.6481 R:1.0500 gloss:0.0360 gloss1-lgP:0.5960 gloss2gQs:0.0605 gloss3dQs:0.0605 gloss4tgtQ:0.0605 dloss:0.0001 exploreP:0.0100\n",
      "Episode:181 Steps:1000 meanR:0.6455 R:0.4800 gloss:0.0390 gloss1-lgP:0.5978 gloss2gQs:0.0652 gloss3dQs:0.0652 gloss4tgtQ:0.0652 dloss:0.0001 exploreP:0.0100\n",
      "Episode:182 Steps:1000 meanR:0.6322 R:0.5900 gloss:0.0406 gloss1-lgP:0.6023 gloss2gQs:0.0674 gloss3dQs:0.0674 gloss4tgtQ:0.0674 dloss:0.0001 exploreP:0.0100\n",
      "Episode:183 Steps:1000 meanR:0.6411 R:0.9800 gloss:0.0415 gloss1-lgP:0.6041 gloss2gQs:0.0687 gloss3dQs:0.0687 gloss4tgtQ:0.0688 dloss:0.0001 exploreP:0.0100\n",
      "Episode:184 Steps:1000 meanR:0.6453 R:0.4200 gloss:0.0403 gloss1-lgP:0.6050 gloss2gQs:0.0666 gloss3dQs:0.0666 gloss4tgtQ:0.0666 dloss:0.0001 exploreP:0.0100\n",
      "Episode:185 Steps:1000 meanR:0.6498 R:0.6900 gloss:0.0340 gloss1-lgP:0.6056 gloss2gQs:0.0561 gloss3dQs:0.0561 gloss4tgtQ:0.0561 dloss:0.0001 exploreP:0.0100\n",
      "Episode:186 Steps:1000 meanR:0.6625 R:1.2700 gloss:0.0329 gloss1-lgP:0.6047 gloss2gQs:0.0543 gloss3dQs:0.0543 gloss4tgtQ:0.0543 dloss:0.0001 exploreP:0.0100\n",
      "Episode:187 Steps:1000 meanR:0.6647 R:0.7500 gloss:0.0380 gloss1-lgP:0.6033 gloss2gQs:0.0626 gloss3dQs:0.0626 gloss4tgtQ:0.0626 dloss:0.0001 exploreP:0.0100\n",
      "Episode:188 Steps:1000 meanR:0.6606 R:0.1500 gloss:0.0363 gloss1-lgP:0.6053 gloss2gQs:0.0594 gloss3dQs:0.0594 gloss4tgtQ:0.0594 dloss:0.0001 exploreP:0.0100\n",
      "Episode:189 Steps:1000 meanR:0.6624 R:0.3300 gloss:0.0374 gloss1-lgP:0.6081 gloss2gQs:0.0611 gloss3dQs:0.0611 gloss4tgtQ:0.0611 dloss:0.0001 exploreP:0.0100\n",
      "Episode:190 Steps:1000 meanR:0.6734 R:1.1000 gloss:0.0357 gloss1-lgP:0.6097 gloss2gQs:0.0582 gloss3dQs:0.0582 gloss4tgtQ:0.0582 dloss:0.0001 exploreP:0.0100\n",
      "Episode:191 Steps:1000 meanR:0.6788 R:1.3700 gloss:0.0380 gloss1-lgP:0.6076 gloss2gQs:0.0622 gloss3dQs:0.0623 gloss4tgtQ:0.0623 dloss:0.0001 exploreP:0.0100\n",
      "Episode:192 Steps:1000 meanR:0.6838 R:0.9400 gloss:0.0401 gloss1-lgP:0.6067 gloss2gQs:0.0658 gloss3dQs:0.0658 gloss4tgtQ:0.0658 dloss:0.0001 exploreP:0.0100\n",
      "Episode:193 Steps:1000 meanR:0.6890 R:0.6000 gloss:0.0412 gloss1-lgP:0.6056 gloss2gQs:0.0679 gloss3dQs:0.0679 gloss4tgtQ:0.0679 dloss:0.0001 exploreP:0.0100\n",
      "Episode:194 Steps:1000 meanR:0.6905 R:0.7200 gloss:0.0435 gloss1-lgP:0.6050 gloss2gQs:0.0718 gloss3dQs:0.0718 gloss4tgtQ:0.0718 dloss:0.0001 exploreP:0.0100\n",
      "Episode:195 Steps:1000 meanR:0.7019 R:1.1400 gloss:0.0430 gloss1-lgP:0.6057 gloss2gQs:0.0708 gloss3dQs:0.0708 gloss4tgtQ:0.0708 dloss:0.0001 exploreP:0.0100\n",
      "Episode:196 Steps:1000 meanR:0.7098 R:0.7900 gloss:0.0472 gloss1-lgP:0.6084 gloss2gQs:0.0773 gloss3dQs:0.0773 gloss4tgtQ:0.0773 dloss:0.0001 exploreP:0.0100\n",
      "Episode:197 Steps:1000 meanR:0.7162 R:0.9900 gloss:0.0430 gloss1-lgP:0.6089 gloss2gQs:0.0704 gloss3dQs:0.0704 gloss4tgtQ:0.0704 dloss:0.0001 exploreP:0.0100\n",
      "Episode:198 Steps:1000 meanR:0.7129 R:0.2700 gloss:0.0437 gloss1-lgP:0.6090 gloss2gQs:0.0715 gloss3dQs:0.0715 gloss4tgtQ:0.0716 dloss:0.0001 exploreP:0.0100\n",
      "Episode:199 Steps:1000 meanR:0.7135 R:0.7700 gloss:0.0501 gloss1-lgP:0.6090 gloss2gQs:0.0822 gloss3dQs:0.0822 gloss4tgtQ:0.0822 dloss:0.0001 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch, gloss3_batch, gloss4_batch = [], [], [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(1111111111):\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1/1 # 1000 episode length\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p >= np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, gloss1, gloss2, gloss3, gloss4, _, _ = sess.run([model.g_loss, model.d_loss,\n",
    "                                                                           model.g_loss1, model.g_loss2, \n",
    "                                                                           model.g_loss3, model.g_loss4,\n",
    "                                                                           model.g_opt, model.d_opt],\n",
    "                                                                          feed_dict = {model.states: states, \n",
    "                                                                                       model.actions: actions,\n",
    "                                                                                       model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            gloss3_batch.append(gloss3)\n",
    "            gloss4_batch.append(gloss4)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'Steps:{}'.format(num_steps),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'gloss1-lgP:{:.4f}'.format(np.mean(gloss1_batch)), #-logp\n",
    "              'gloss2gQs:{:.4f}'.format(np.mean(gloss2_batch)),#gQs\n",
    "              'gloss3dQs:{:.4f}'.format(np.mean(gloss3_batch)),#dQs\n",
    "              'gloss4tgtQ:{:.4f}'.format(np.mean(gloss4_batch)),#tgtQs\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
