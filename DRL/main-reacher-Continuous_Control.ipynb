{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) 0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(action.shape, reward)\n",
    "        print(done)\n",
    "        break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    #dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    #g_loss = tf.reduce_mean(neg_log_prob_actions * dQs)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * gQs)\n",
    "    #d_loss = tf.reduce_mean(tf.square(dQs - targetQs))\n",
    "    d_loss = tf.reduce_mean(tf.square(gQs - targetQs))\n",
    "    return actions_logits, gQs, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.1205030921368464\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exploration parameters\n",
    "# explore_start = 1.0            # exploration probability at start\n",
    "# explore_stop = 0.01            # minimum exploration probability \n",
    "# decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# # Network parameters\n",
    "# # state_size = 37\n",
    "# # state_size_ = (84, 84, 3)\n",
    "# state_size = 33\n",
    "# action_size = 4\n",
    "# hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "# learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# # Memory parameters\n",
    "# gamma = 0.99                   # future reward discount\n",
    "# memory_size = 1000            # memory capacity\n",
    "# batch_size = 1000             # experience mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.3000 R:0.3000 gloss:-1.6677 dloss:0.1148 exploreP:0.9057\n",
      "Episode:1 meanR:0.7300 R:1.1600 gloss:0.4104 dloss:0.0219 exploreP:0.8204\n",
      "Episode:2 meanR:0.6567 R:0.5100 gloss:-4.5833 dloss:0.0920 exploreP:0.7432\n",
      "Episode:3 meanR:0.4925 R:0.0000 gloss:-2.9944 dloss:0.0303 exploreP:0.6734\n",
      "Episode:4 meanR:0.3940 R:0.0000 gloss:0.5764 dloss:0.0535 exploreP:0.6102\n",
      "Episode:5 meanR:0.3683 R:0.2400 gloss:-0.3082 dloss:0.0480 exploreP:0.5530\n",
      "Episode:6 meanR:0.3600 R:0.3100 gloss:-0.6370 dloss:0.1392 exploreP:0.5013\n",
      "Episode:7 meanR:0.3975 R:0.6600 gloss:3.3918 dloss:0.0265 exploreP:0.4545\n",
      "Episode:8 meanR:0.4522 R:0.8900 gloss:3.2454 dloss:0.0670 exploreP:0.4121\n",
      "Episode:9 meanR:0.4190 R:0.1200 gloss:5.2227 dloss:0.0954 exploreP:0.3738\n",
      "Episode:10 meanR:0.3991 R:0.2000 gloss:-0.0050 dloss:0.0883 exploreP:0.3392\n",
      "Episode:11 meanR:0.3858 R:0.2400 gloss:-0.0050 dloss:0.0939 exploreP:0.3078\n",
      "Episode:12 meanR:0.3608 R:0.0600 gloss:-0.0147 dloss:0.1237 exploreP:0.2795\n",
      "Episode:13 meanR:0.3464 R:0.1600 gloss:-0.0139 dloss:0.0471 exploreP:0.2538\n",
      "Episode:14 meanR:0.3467 R:0.3500 gloss:-0.0239 dloss:0.0537 exploreP:0.2306\n",
      "Episode:15 meanR:0.3250 R:0.0000 gloss:-0.0077 dloss:0.1109 exploreP:0.2096\n",
      "Episode:16 meanR:0.3659 R:1.0200 gloss:0.0115 dloss:0.0278 exploreP:0.1905\n",
      "Episode:17 meanR:0.3994 R:0.9700 gloss:0.0023 dloss:0.0859 exploreP:0.1734\n",
      "Episode:18 meanR:0.4221 R:0.8300 gloss:-0.0090 dloss:0.0554 exploreP:0.1578\n",
      "Episode:19 meanR:0.4140 R:0.2600 gloss:-0.0345 dloss:0.1021 exploreP:0.1437\n",
      "Episode:20 meanR:0.4190 R:0.5200 gloss:-0.0203 dloss:0.0654 exploreP:0.1310\n",
      "Episode:21 meanR:0.4264 R:0.5800 gloss:0.0001 dloss:0.0820 exploreP:0.1195\n",
      "Episode:22 meanR:0.4778 R:1.6100 gloss:-0.0007 dloss:0.1202 exploreP:0.1090\n",
      "Episode:23 meanR:0.4625 R:0.1100 gloss:0.0005 dloss:0.0210 exploreP:0.0996\n",
      "Episode:24 meanR:0.4868 R:1.0700 gloss:0.0010 dloss:0.3241 exploreP:0.0911\n",
      "Episode:25 meanR:0.5127 R:1.1600 gloss:0.0001 dloss:0.0185 exploreP:0.0833\n",
      "Episode:26 meanR:0.5185 R:0.6700 gloss:0.0001 dloss:0.0584 exploreP:0.0764\n",
      "Episode:27 meanR:0.5182 R:0.5100 gloss:-0.0005 dloss:0.0860 exploreP:0.0700\n",
      "Episode:28 meanR:0.5128 R:0.3600 gloss:-0.0005 dloss:0.0514 exploreP:0.0643\n",
      "Episode:29 meanR:0.4957 R:0.0000 gloss:-0.0008 dloss:0.0849 exploreP:0.0591\n",
      "Episode:30 meanR:0.4826 R:0.0900 gloss:-0.0003 dloss:0.0415 exploreP:0.0545\n",
      "Episode:31 meanR:0.4941 R:0.8500 gloss:-0.0014 dloss:0.0666 exploreP:0.0502\n",
      "Episode:32 meanR:0.4897 R:0.3500 gloss:-0.0006 dloss:0.0758 exploreP:0.0464\n",
      "Episode:33 meanR:0.4924 R:0.5800 gloss:-0.0008 dloss:0.0751 exploreP:0.0429\n",
      "Episode:34 meanR:0.4826 R:0.1500 gloss:-0.0007 dloss:0.0321 exploreP:0.0398\n",
      "Episode:35 meanR:0.4828 R:0.4900 gloss:-0.0004 dloss:0.0448 exploreP:0.0370\n",
      "Episode:36 meanR:0.4751 R:0.2000 gloss:0.0001 dloss:0.0584 exploreP:0.0344\n",
      "Episode:37 meanR:0.4626 R:0.0000 gloss:0.0003 dloss:0.0509 exploreP:0.0321\n",
      "Episode:38 meanR:0.4610 R:0.4000 gloss:0.0000 dloss:0.0567 exploreP:0.0300\n",
      "Episode:39 meanR:0.4710 R:0.8600 gloss:-0.0027 dloss:0.0714 exploreP:0.0281\n",
      "Episode:40 meanR:0.4712 R:0.4800 gloss:-0.0017 dloss:0.0740 exploreP:0.0263\n",
      "Episode:41 meanR:0.4871 R:1.1400 gloss:0.0027 dloss:0.0524 exploreP:0.0248\n",
      "Episode:42 meanR:0.4928 R:0.7300 gloss:-0.0022 dloss:0.0558 exploreP:0.0234\n",
      "Episode:43 meanR:0.4818 R:0.0100 gloss:-0.0056 dloss:0.0897 exploreP:0.0221\n",
      "Episode:44 meanR:0.4711 R:0.0000 gloss:-0.0103 dloss:0.1156 exploreP:0.0209\n",
      "Episode:45 meanR:0.4787 R:0.8200 gloss:-0.0003 dloss:0.0228 exploreP:0.0199\n",
      "Episode:46 meanR:0.4864 R:0.8400 gloss:-0.0129 dloss:0.0888 exploreP:0.0190\n",
      "Episode:47 meanR:0.4771 R:0.0400 gloss:-0.0072 dloss:0.0817 exploreP:0.0181\n",
      "Episode:48 meanR:0.4743 R:0.3400 gloss:-0.0015 dloss:0.0567 exploreP:0.0173\n",
      "Episode:49 meanR:0.4720 R:0.3600 gloss:-0.0004 dloss:0.0718 exploreP:0.0166\n",
      "Episode:50 meanR:0.4824 R:1.0000 gloss:0.0007 dloss:0.0708 exploreP:0.0160\n",
      "Episode:51 meanR:0.4850 R:0.6200 gloss:0.0007 dloss:0.0766 exploreP:0.0154\n",
      "Episode:52 meanR:0.4808 R:0.2600 gloss:0.0003 dloss:0.0541 exploreP:0.0149\n",
      "Episode:53 meanR:0.4746 R:0.1500 gloss:-0.0009 dloss:0.0693 exploreP:0.0144\n",
      "Episode:54 meanR:0.4724 R:0.3500 gloss:-0.0009 dloss:0.0614 exploreP:0.0140\n",
      "Episode:55 meanR:0.4700 R:0.3400 gloss:0.0002 dloss:0.0910 exploreP:0.0136\n",
      "Episode:56 meanR:0.4747 R:0.7400 gloss:-0.0006 dloss:0.0568 exploreP:0.0133\n",
      "Episode:57 meanR:0.4671 R:0.0300 gloss:0.0003 dloss:0.0468 exploreP:0.0130\n",
      "Episode:58 meanR:0.4592 R:0.0000 gloss:-0.0006 dloss:0.1113 exploreP:0.0127\n",
      "Episode:59 meanR:0.4640 R:0.7500 gloss:-0.0024 dloss:0.0489 exploreP:0.0124\n",
      "Episode:60 meanR:0.4564 R:0.0000 gloss:-0.0186 dloss:0.1113 exploreP:0.0122\n",
      "Episode:61 meanR:0.4568 R:0.4800 gloss:-0.0512 dloss:0.0868 exploreP:0.0120\n",
      "Episode:62 meanR:0.4762 R:1.6800 gloss:0.0137 dloss:0.0551 exploreP:0.0118\n",
      "Episode:63 meanR:0.4803 R:0.7400 gloss:0.0050 dloss:0.0760 exploreP:0.0116\n",
      "Episode:64 meanR:0.4823 R:0.6100 gloss:0.0052 dloss:0.1028 exploreP:0.0115\n",
      "Episode:65 meanR:0.4809 R:0.3900 gloss:0.0011 dloss:0.0874 exploreP:0.0113\n",
      "Episode:66 meanR:0.4737 R:0.0000 gloss:-0.0014 dloss:0.0680 exploreP:0.0112\n",
      "Episode:67 meanR:0.4725 R:0.3900 gloss:0.0023 dloss:0.0759 exploreP:0.0111\n",
      "Episode:68 meanR:0.4742 R:0.5900 gloss:0.0275 dloss:0.0713 exploreP:0.0110\n",
      "Episode:69 meanR:0.4716 R:0.2900 gloss:0.0320 dloss:0.0901 exploreP:0.0109\n",
      "Episode:70 meanR:0.4656 R:0.0500 gloss:0.0212 dloss:0.0577 exploreP:0.0108\n",
      "Episode:71 meanR:0.4701 R:0.7900 gloss:0.0191 dloss:0.0897 exploreP:0.0107\n",
      "Episode:72 meanR:0.4649 R:0.0900 gloss:0.0090 dloss:0.0660 exploreP:0.0107\n",
      "Episode:73 meanR:0.4661 R:0.5500 gloss:0.0048 dloss:0.1943 exploreP:0.0106\n",
      "Episode:74 meanR:0.4664 R:0.4900 gloss:-0.0012 dloss:0.0109 exploreP:0.0105\n",
      "Episode:75 meanR:0.4672 R:0.5300 gloss:-0.0004 dloss:0.0544 exploreP:0.0105\n",
      "Episode:76 meanR:0.4692 R:0.6200 gloss:0.0018 dloss:0.1060 exploreP:0.0104\n",
      "Episode:77 meanR:0.4712 R:0.6200 gloss:-0.0017 dloss:0.0765 exploreP:0.0104\n",
      "Episode:78 meanR:0.4770 R:0.9300 gloss:-0.0072 dloss:0.0658 exploreP:0.0104\n",
      "Episode:79 meanR:0.4806 R:0.7700 gloss:-0.0040 dloss:0.0655 exploreP:0.0103\n",
      "Episode:80 meanR:0.4856 R:0.8800 gloss:0.0033 dloss:0.1239 exploreP:0.0103\n",
      "Episode:81 meanR:0.4796 R:0.0000 gloss:-0.0090 dloss:0.0513 exploreP:0.0103\n",
      "Episode:82 meanR:0.4798 R:0.4900 gloss:0.0060 dloss:0.0993 exploreP:0.0102\n",
      "Episode:83 meanR:0.4824 R:0.7000 gloss:-0.0057 dloss:0.0668 exploreP:0.0102\n",
      "Episode:84 meanR:0.4826 R:0.5000 gloss:-0.0047 dloss:0.1053 exploreP:0.0102\n",
      "Episode:85 meanR:0.4901 R:1.1300 gloss:-0.0043 dloss:0.0718 exploreP:0.0102\n",
      "Episode:86 meanR:0.4926 R:0.7100 gloss:-0.0067 dloss:0.0475 exploreP:0.0102\n",
      "Episode:87 meanR:0.4931 R:0.5300 gloss:0.0000 dloss:0.0947 exploreP:0.0101\n",
      "Episode:88 meanR:0.4979 R:0.9200 gloss:-0.0005 dloss:0.0296 exploreP:0.0101\n",
      "Episode:89 meanR:0.4983 R:0.5400 gloss:-0.0009 dloss:0.0723 exploreP:0.0101\n",
      "Episode:90 meanR:0.4951 R:0.2000 gloss:-0.0014 dloss:0.1458 exploreP:0.0101\n",
      "Episode:91 meanR:0.4977 R:0.7400 gloss:0.0005 dloss:0.0843 exploreP:0.0101\n",
      "Episode:92 meanR:0.4956 R:0.3000 gloss:0.0002 dloss:0.0319 exploreP:0.0101\n",
      "Episode:93 meanR:0.4930 R:0.2500 gloss:0.0004 dloss:0.0660 exploreP:0.0101\n",
      "Episode:94 meanR:0.4899 R:0.2000 gloss:0.0011 dloss:0.1595 exploreP:0.0101\n",
      "Episode:95 meanR:0.4852 R:0.0400 gloss:0.0002 dloss:0.0135 exploreP:0.0101\n",
      "Episode:96 meanR:0.4946 R:1.4000 gloss:-0.0009 dloss:0.1773 exploreP:0.0101\n",
      "Episode:97 meanR:0.4943 R:0.4600 gloss:0.0002 dloss:0.0149 exploreP:0.0101\n",
      "Episode:98 meanR:0.4944 R:0.5100 gloss:0.0007 dloss:0.0342 exploreP:0.0100\n",
      "Episode:99 meanR:0.4998 R:1.0300 gloss:0.0028 dloss:0.0438 exploreP:0.0100\n",
      "Episode:100 meanR:0.5022 R:0.5400 gloss:0.0020 dloss:0.1157 exploreP:0.0100\n",
      "Episode:101 meanR:0.4953 R:0.4700 gloss:0.0003 dloss:0.0341 exploreP:0.0100\n",
      "Episode:102 meanR:0.4956 R:0.5400 gloss:0.0014 dloss:0.0738 exploreP:0.0100\n",
      "Episode:103 meanR:0.4995 R:0.3900 gloss:-0.0012 dloss:0.0781 exploreP:0.0100\n",
      "Episode:104 meanR:0.5151 R:1.5600 gloss:-0.0018 dloss:0.1198 exploreP:0.0100\n",
      "Episode:105 meanR:0.5195 R:0.6800 gloss:0.0000 dloss:0.0150 exploreP:0.0100\n",
      "Episode:106 meanR:0.5235 R:0.7100 gloss:-0.0020 dloss:0.0697 exploreP:0.0100\n",
      "Episode:107 meanR:0.5200 R:0.3100 gloss:-0.0002 dloss:0.0679 exploreP:0.0100\n",
      "Episode:108 meanR:0.5140 R:0.2900 gloss:-0.0015 dloss:0.3490 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:109 meanR:0.5276 R:1.4800 gloss:-0.0009 dloss:0.0145 exploreP:0.0100\n",
      "Episode:110 meanR:0.5308 R:0.5200 gloss:-0.0028 dloss:0.0653 exploreP:0.0100\n",
      "Episode:111 meanR:0.5338 R:0.5400 gloss:0.0001 dloss:0.0470 exploreP:0.0100\n",
      "Episode:112 meanR:0.5378 R:0.4600 gloss:-0.0008 dloss:0.1920 exploreP:0.0100\n",
      "Episode:113 meanR:0.5371 R:0.0900 gloss:0.0010 dloss:0.0228 exploreP:0.0100\n",
      "Episode:114 meanR:0.5345 R:0.0900 gloss:0.0003 dloss:0.0840 exploreP:0.0100\n",
      "Episode:115 meanR:0.5390 R:0.4500 gloss:0.0012 dloss:0.0605 exploreP:0.0100\n",
      "Episode:116 meanR:0.5299 R:0.1100 gloss:0.0005 dloss:0.1626 exploreP:0.0100\n",
      "Episode:117 meanR:0.5366 R:1.6400 gloss:0.0000 dloss:0.0495 exploreP:0.0100\n",
      "Episode:118 meanR:0.5333 R:0.5000 gloss:0.0004 dloss:0.0474 exploreP:0.0100\n",
      "Episode:119 meanR:0.5375 R:0.6800 gloss:0.0007 dloss:0.0644 exploreP:0.0100\n",
      "Episode:120 meanR:0.5352 R:0.2900 gloss:0.0007 dloss:0.0802 exploreP:0.0100\n",
      "Episode:121 meanR:0.5347 R:0.5300 gloss:-0.0004 dloss:0.0570 exploreP:0.0100\n",
      "Episode:122 meanR:0.5353 R:1.6700 gloss:-0.0725 dloss:0.2010 exploreP:0.0100\n",
      "Episode:123 meanR:0.5377 R:0.3500 gloss:-0.0191 dloss:0.0582 exploreP:0.0100\n",
      "Episode:124 meanR:0.5322 R:0.5200 gloss:-0.0177 dloss:0.0706 exploreP:0.0100\n",
      "Episode:125 meanR:0.5228 R:0.2200 gloss:0.0057 dloss:0.1164 exploreP:0.0100\n",
      "Episode:126 meanR:0.5166 R:0.0500 gloss:-0.0088 dloss:0.0792 exploreP:0.0100\n",
      "Episode:127 meanR:0.5115 R:0.0000 gloss:-0.0168 dloss:0.1087 exploreP:0.0100\n",
      "Episode:128 meanR:0.5103 R:0.2400 gloss:-0.0282 dloss:0.1032 exploreP:0.0100\n",
      "Episode:129 meanR:0.5162 R:0.5900 gloss:0.0154 dloss:0.1120 exploreP:0.0100\n",
      "Episode:130 meanR:0.5197 R:0.4400 gloss:-0.0663 dloss:0.0756 exploreP:0.0100\n",
      "Episode:131 meanR:0.5156 R:0.4400 gloss:0.0221 dloss:0.0907 exploreP:0.0100\n",
      "Episode:132 meanR:0.5154 R:0.3300 gloss:-0.0174 dloss:0.0642 exploreP:0.0100\n",
      "Episode:133 meanR:0.5265 R:1.6900 gloss:-0.0375 dloss:0.0870 exploreP:0.0100\n",
      "Episode:134 meanR:0.5287 R:0.3700 gloss:-0.0143 dloss:0.0738 exploreP:0.0100\n",
      "Episode:135 meanR:0.5251 R:0.1300 gloss:-0.0345 dloss:0.0727 exploreP:0.0100\n",
      "Episode:136 meanR:0.5272 R:0.4100 gloss:-0.0063 dloss:0.0668 exploreP:0.0100\n",
      "Episode:137 meanR:0.5427 R:1.5500 gloss:0.0003 dloss:0.0512 exploreP:0.0100\n",
      "Episode:138 meanR:0.5443 R:0.5600 gloss:-0.0007 dloss:0.1880 exploreP:0.0100\n",
      "Episode:139 meanR:0.5385 R:0.2800 gloss:0.0009 dloss:0.0207 exploreP:0.0100\n",
      "Episode:140 meanR:0.5397 R:0.6000 gloss:-0.0004 dloss:0.0605 exploreP:0.0100\n",
      "Episode:141 meanR:0.5310 R:0.2700 gloss:-0.0028 dloss:0.1180 exploreP:0.0100\n",
      "Episode:142 meanR:0.5237 R:0.0000 gloss:-0.0077 dloss:0.1160 exploreP:0.0100\n",
      "Episode:143 meanR:0.5256 R:0.2000 gloss:0.0056 dloss:0.0588 exploreP:0.0100\n",
      "Episode:144 meanR:0.5274 R:0.1800 gloss:-0.0000 dloss:0.1389 exploreP:0.0100\n",
      "Episode:145 meanR:0.5285 R:0.9300 gloss:-0.0002 dloss:0.0355 exploreP:0.0100\n",
      "Episode:146 meanR:0.5201 R:0.0000 gloss:-0.0058 dloss:0.0670 exploreP:0.0100\n",
      "Episode:147 meanR:0.5197 R:0.0000 gloss:-0.0056 dloss:0.1535 exploreP:0.0100\n",
      "Episode:148 meanR:0.5237 R:0.7400 gloss:0.0420 dloss:0.0636 exploreP:0.0100\n",
      "Episode:149 meanR:0.5212 R:0.1100 gloss:0.0047 dloss:0.0703 exploreP:0.0100\n",
      "Episode:150 meanR:0.5144 R:0.3200 gloss:0.0474 dloss:0.2435 exploreP:0.0100\n",
      "Episode:151 meanR:0.5119 R:0.3700 gloss:-0.0130 dloss:0.0255 exploreP:0.0100\n",
      "Episode:152 meanR:0.5164 R:0.7100 gloss:0.0174 dloss:0.0620 exploreP:0.0100\n",
      "Episode:153 meanR:0.5159 R:0.1000 gloss:0.0008 dloss:0.1300 exploreP:0.0100\n",
      "Episode:154 meanR:0.5315 R:1.9100 gloss:-0.0077 dloss:0.0842 exploreP:0.0100\n",
      "Episode:155 meanR:0.5340 R:0.5900 gloss:-0.0134 dloss:0.0504 exploreP:0.0100\n",
      "Episode:156 meanR:0.5313 R:0.4700 gloss:0.0056 dloss:0.1236 exploreP:0.0100\n",
      "Episode:157 meanR:0.5310 R:0.0000 gloss:0.0041 dloss:0.0545 exploreP:0.0100\n",
      "Episode:158 meanR:0.5360 R:0.5000 gloss:-0.0007 dloss:0.1010 exploreP:0.0100\n",
      "Episode:159 meanR:0.5285 R:0.0000 gloss:0.0045 dloss:0.1097 exploreP:0.0100\n",
      "Episode:160 meanR:0.5430 R:1.4500 gloss:0.0009 dloss:0.0549 exploreP:0.0100\n",
      "Episode:161 meanR:0.5419 R:0.3700 gloss:0.0077 dloss:0.0878 exploreP:0.0100\n",
      "Episode:162 meanR:0.5271 R:0.2000 gloss:0.0020 dloss:0.0580 exploreP:0.0100\n",
      "Episode:163 meanR:0.5267 R:0.7000 gloss:0.0019 dloss:0.0764 exploreP:0.0100\n",
      "Episode:164 meanR:0.5206 R:0.0000 gloss:0.0042 dloss:0.0886 exploreP:0.0100\n",
      "Episode:165 meanR:0.5293 R:1.2600 gloss:0.0019 dloss:0.0608 exploreP:0.0100\n",
      "Episode:166 meanR:0.5364 R:0.7100 gloss:0.0010 dloss:0.1116 exploreP:0.0100\n",
      "Episode:167 meanR:0.5432 R:1.0700 gloss:0.0012 dloss:0.0626 exploreP:0.0100\n",
      "Episode:168 meanR:0.5379 R:0.0600 gloss:0.0017 dloss:0.0875 exploreP:0.0100\n",
      "Episode:169 meanR:0.5436 R:0.8600 gloss:0.0008 dloss:0.0909 exploreP:0.0100\n",
      "Episode:170 meanR:0.5431 R:0.0000 gloss:0.0027 dloss:0.1327 exploreP:0.0100\n",
      "Episode:171 meanR:0.5365 R:0.1300 gloss:0.0012 dloss:0.0592 exploreP:0.0100\n",
      "Episode:172 meanR:0.5397 R:0.4100 gloss:0.0023 dloss:0.1168 exploreP:0.0100\n",
      "Episode:173 meanR:0.5342 R:0.0000 gloss:0.0016 dloss:0.0767 exploreP:0.0100\n",
      "Episode:174 meanR:0.5349 R:0.5600 gloss:0.0005 dloss:0.0754 exploreP:0.0100\n",
      "Episode:175 meanR:0.5346 R:0.5000 gloss:0.0036 dloss:0.0846 exploreP:0.0100\n",
      "Episode:176 meanR:0.5362 R:0.7800 gloss:0.0004 dloss:0.0832 exploreP:0.0100\n",
      "Episode:177 meanR:0.5378 R:0.7800 gloss:0.0022 dloss:0.0868 exploreP:0.0100\n",
      "Episode:178 meanR:0.5339 R:0.5400 gloss:-0.0011 dloss:0.0720 exploreP:0.0100\n",
      "Episode:179 meanR:0.5283 R:0.2100 gloss:0.0008 dloss:0.0755 exploreP:0.0100\n",
      "Episode:180 meanR:0.5252 R:0.5700 gloss:0.0024 dloss:0.0573 exploreP:0.0100\n",
      "Episode:181 meanR:0.5252 R:0.0000 gloss:0.0005 dloss:0.0708 exploreP:0.0100\n",
      "Episode:182 meanR:0.5246 R:0.4300 gloss:0.0055 dloss:0.1201 exploreP:0.0100\n",
      "Episode:183 meanR:0.5245 R:0.6900 gloss:0.0031 dloss:0.0925 exploreP:0.0100\n",
      "Episode:184 meanR:0.5225 R:0.3000 gloss:0.0019 dloss:0.0487 exploreP:0.0100\n",
      "Episode:185 meanR:0.5222 R:1.1000 gloss:0.0012 dloss:0.0882 exploreP:0.0100\n",
      "Episode:186 meanR:0.5235 R:0.8400 gloss:0.0003 dloss:0.0228 exploreP:0.0100\n",
      "Episode:187 meanR:0.5246 R:0.6400 gloss:0.0024 dloss:0.0885 exploreP:0.0100\n",
      "Episode:188 meanR:0.5182 R:0.2800 gloss:0.0007 dloss:0.0741 exploreP:0.0100\n",
      "Episode:189 meanR:0.5255 R:1.2700 gloss:0.0009 dloss:0.0352 exploreP:0.0100\n",
      "Episode:190 meanR:0.5241 R:0.0600 gloss:0.0012 dloss:0.1094 exploreP:0.0100\n",
      "Episode:191 meanR:0.5244 R:0.7700 gloss:-0.0005 dloss:0.0831 exploreP:0.0100\n",
      "Episode:192 meanR:0.5235 R:0.2100 gloss:0.0049 dloss:0.0579 exploreP:0.0100\n",
      "Episode:193 meanR:0.5287 R:0.7700 gloss:0.0025 dloss:0.0649 exploreP:0.0100\n",
      "Episode:194 meanR:0.5315 R:0.4800 gloss:0.0023 dloss:0.1035 exploreP:0.0100\n",
      "Episode:195 meanR:0.5363 R:0.5200 gloss:0.0011 dloss:0.0670 exploreP:0.0100\n",
      "Episode:196 meanR:0.5317 R:0.9400 gloss:0.0029 dloss:0.0713 exploreP:0.0100\n",
      "Episode:197 meanR:0.5354 R:0.8300 gloss:-0.0007 dloss:0.0741 exploreP:0.0100\n",
      "Episode:198 meanR:0.5335 R:0.3200 gloss:0.0016 dloss:0.0620 exploreP:0.0100\n",
      "Episode:199 meanR:0.5264 R:0.3200 gloss:0.0012 dloss:0.0460 exploreP:0.0100\n",
      "Episode:200 meanR:0.5285 R:0.7500 gloss:0.0021 dloss:0.0747 exploreP:0.0100\n",
      "Episode:201 meanR:0.5253 R:0.1500 gloss:0.0016 dloss:0.0472 exploreP:0.0100\n",
      "Episode:202 meanR:0.5286 R:0.8700 gloss:0.0006 dloss:0.0370 exploreP:0.0100\n",
      "Episode:203 meanR:0.5348 R:1.0100 gloss:0.0020 dloss:0.0593 exploreP:0.0100\n",
      "Episode:204 meanR:0.5273 R:0.8100 gloss:0.0011 dloss:0.0426 exploreP:0.0100\n",
      "Episode:205 meanR:0.5241 R:0.3600 gloss:0.0020 dloss:0.0637 exploreP:0.0100\n",
      "Episode:206 meanR:0.5225 R:0.5500 gloss:0.0009 dloss:0.0551 exploreP:0.0100\n",
      "Episode:207 meanR:0.5281 R:0.8700 gloss:0.0023 dloss:0.0507 exploreP:0.0100\n",
      "Episode:208 meanR:0.5337 R:0.8500 gloss:0.0046 dloss:0.1163 exploreP:0.0100\n",
      "Episode:209 meanR:0.5189 R:0.0000 gloss:0.0183 dloss:0.0289 exploreP:0.0100\n",
      "Episode:210 meanR:0.5137 R:0.0000 gloss:0.0115 dloss:0.0699 exploreP:0.0100\n",
      "Episode:211 meanR:0.5173 R:0.9000 gloss:0.0047 dloss:0.0385 exploreP:0.0100\n",
      "Episode:212 meanR:0.5173 R:0.4600 gloss:0.0071 dloss:0.0485 exploreP:0.0100\n",
      "Episode:213 meanR:0.5221 R:0.5700 gloss:0.0092 dloss:0.0523 exploreP:0.0100\n",
      "Episode:214 meanR:0.5294 R:0.8200 gloss:0.0072 dloss:0.0805 exploreP:0.0100\n",
      "Episode:215 meanR:0.5298 R:0.4900 gloss:0.0044 dloss:0.0276 exploreP:0.0100\n",
      "Episode:216 meanR:0.5431 R:1.4400 gloss:0.0025 dloss:0.0461 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:217 meanR:0.5267 R:0.0000 gloss:0.0024 dloss:0.0642 exploreP:0.0100\n",
      "Episode:218 meanR:0.5264 R:0.4700 gloss:0.0012 dloss:0.0282 exploreP:0.0100\n",
      "Episode:219 meanR:0.5245 R:0.4900 gloss:0.0022 dloss:0.0504 exploreP:0.0100\n",
      "Episode:220 meanR:0.5222 R:0.0600 gloss:0.0011 dloss:0.0312 exploreP:0.0100\n",
      "Episode:221 meanR:0.5260 R:0.9100 gloss:0.0005 dloss:0.0465 exploreP:0.0100\n",
      "Episode:222 meanR:0.5093 R:0.0000 gloss:0.0010 dloss:0.0477 exploreP:0.0100\n",
      "Episode:223 meanR:0.5062 R:0.0400 gloss:0.0000 dloss:0.0442 exploreP:0.0100\n",
      "Episode:224 meanR:0.5010 R:0.0000 gloss:0.0016 dloss:0.0794 exploreP:0.0100\n",
      "Episode:225 meanR:0.5006 R:0.1800 gloss:0.0008 dloss:0.0303 exploreP:0.0100\n",
      "Episode:226 meanR:0.5090 R:0.8900 gloss:0.0006 dloss:0.0576 exploreP:0.0100\n",
      "Episode:227 meanR:0.5090 R:0.0000 gloss:0.0005 dloss:0.0365 exploreP:0.0100\n",
      "Episode:228 meanR:0.5127 R:0.6100 gloss:0.0011 dloss:0.0442 exploreP:0.0100\n",
      "Episode:229 meanR:0.5131 R:0.6300 gloss:0.0012 dloss:0.0225 exploreP:0.0100\n",
      "Episode:230 meanR:0.5087 R:0.0000 gloss:0.0008 dloss:0.0201 exploreP:0.0100\n",
      "Episode:231 meanR:0.5189 R:1.4600 gloss:0.0005 dloss:0.0504 exploreP:0.0100\n",
      "Episode:232 meanR:0.5208 R:0.5200 gloss:-0.0000 dloss:0.0676 exploreP:0.0100\n",
      "Episode:233 meanR:0.5039 R:0.0000 gloss:0.0005 dloss:0.0213 exploreP:0.0100\n",
      "Episode:234 meanR:0.5002 R:0.0000 gloss:0.0069 dloss:0.0218 exploreP:0.0100\n",
      "Episode:235 meanR:0.5108 R:1.1900 gloss:0.0013 dloss:0.0460 exploreP:0.0100\n",
      "Episode:236 meanR:0.5067 R:0.0000 gloss:0.0022 dloss:0.0424 exploreP:0.0100\n",
      "Episode:237 meanR:0.4931 R:0.1900 gloss:0.0003 dloss:0.0280 exploreP:0.0100\n",
      "Episode:238 meanR:0.4939 R:0.6400 gloss:0.0015 dloss:0.0534 exploreP:0.0100\n",
      "Episode:239 meanR:0.4991 R:0.8000 gloss:0.0011 dloss:0.0195 exploreP:0.0100\n",
      "Episode:240 meanR:0.4931 R:0.0000 gloss:0.0001 dloss:0.0443 exploreP:0.0100\n",
      "Episode:241 meanR:0.4956 R:0.5200 gloss:0.0015 dloss:0.0296 exploreP:0.0100\n",
      "Episode:242 meanR:0.4956 R:0.0000 gloss:0.0017 dloss:0.0307 exploreP:0.0100\n",
      "Episode:243 meanR:0.4936 R:0.0000 gloss:-0.0002 dloss:0.0349 exploreP:0.0100\n",
      "Episode:244 meanR:0.4973 R:0.5500 gloss:0.0001 dloss:0.0440 exploreP:0.0100\n",
      "Episode:245 meanR:0.4925 R:0.4500 gloss:-0.0002 dloss:0.0194 exploreP:0.0100\n",
      "Episode:246 meanR:0.5000 R:0.7500 gloss:0.0004 dloss:0.0373 exploreP:0.0100\n",
      "Episode:247 meanR:0.5020 R:0.2000 gloss:0.0013 dloss:0.0218 exploreP:0.0100\n",
      "Episode:248 meanR:0.5065 R:1.1900 gloss:0.0020 dloss:0.0386 exploreP:0.0100\n",
      "Episode:249 meanR:0.5200 R:1.4600 gloss:0.0011 dloss:0.0761 exploreP:0.0100\n",
      "Episode:250 meanR:0.5273 R:1.0500 gloss:0.0006 dloss:0.0069 exploreP:0.0100\n",
      "Episode:251 meanR:0.5312 R:0.7600 gloss:0.0004 dloss:0.0334 exploreP:0.0100\n",
      "Episode:252 meanR:0.5254 R:0.1300 gloss:0.0019 dloss:0.0469 exploreP:0.0100\n",
      "Episode:253 meanR:0.5244 R:0.0000 gloss:0.0011 dloss:0.0243 exploreP:0.0100\n",
      "Episode:254 meanR:0.5053 R:0.0000 gloss:0.0010 dloss:0.0244 exploreP:0.0100\n",
      "Episode:255 meanR:0.4994 R:0.0000 gloss:0.0002 dloss:0.0286 exploreP:0.0100\n",
      "Episode:256 meanR:0.4967 R:0.2000 gloss:0.0005 dloss:0.0234 exploreP:0.0100\n",
      "Episode:257 meanR:0.5093 R:1.2600 gloss:0.0012 dloss:0.0290 exploreP:0.0100\n",
      "Episode:258 meanR:0.5056 R:0.1300 gloss:0.0005 dloss:0.0319 exploreP:0.0100\n",
      "Episode:259 meanR:0.5109 R:0.5300 gloss:-0.0000 dloss:0.0224 exploreP:0.0100\n",
      "Episode:260 meanR:0.5049 R:0.8500 gloss:0.0000 dloss:0.0290 exploreP:0.0100\n",
      "Episode:261 meanR:0.5058 R:0.4600 gloss:0.0005 dloss:0.0263 exploreP:0.0100\n",
      "Episode:262 meanR:0.5205 R:1.6700 gloss:0.0004 dloss:0.0135 exploreP:0.0100\n",
      "Episode:263 meanR:0.5198 R:0.6300 gloss:0.0029 dloss:0.0247 exploreP:0.0100\n",
      "Episode:264 meanR:0.5198 R:0.0000 gloss:0.0006 dloss:0.0425 exploreP:0.0100\n",
      "Episode:265 meanR:0.5072 R:0.0000 gloss:0.0008 dloss:0.0112 exploreP:0.0100\n",
      "Episode:266 meanR:0.5023 R:0.2200 gloss:0.0002 dloss:0.0183 exploreP:0.0100\n",
      "Episode:267 meanR:0.4960 R:0.4400 gloss:0.0007 dloss:0.0469 exploreP:0.0100\n",
      "Episode:268 meanR:0.4954 R:0.0000 gloss:0.0002 dloss:0.0135 exploreP:0.0100\n",
      "Episode:269 meanR:0.5032 R:1.6400 gloss:0.0002 dloss:0.0307 exploreP:0.0100\n",
      "Episode:270 meanR:0.5062 R:0.3000 gloss:0.0005 dloss:0.0222 exploreP:0.0100\n",
      "Episode:271 meanR:0.5060 R:0.1100 gloss:0.0012 dloss:0.0285 exploreP:0.0100\n",
      "Episode:272 meanR:0.5020 R:0.0100 gloss:0.0001 dloss:0.0447 exploreP:0.0100\n",
      "Episode:273 meanR:0.5071 R:0.5100 gloss:0.0006 dloss:0.0273 exploreP:0.0100\n",
      "Episode:274 meanR:0.5076 R:0.6100 gloss:0.0004 dloss:0.0215 exploreP:0.0100\n",
      "Episode:275 meanR:0.5108 R:0.8200 gloss:0.0007 dloss:0.0272 exploreP:0.0100\n",
      "Episode:276 meanR:0.5088 R:0.5800 gloss:0.0005 dloss:0.0397 exploreP:0.0100\n",
      "Episode:277 meanR:0.5010 R:0.0000 gloss:0.0005 dloss:0.0133 exploreP:0.0100\n",
      "Episode:278 meanR:0.4956 R:0.0000 gloss:0.0009 dloss:0.0216 exploreP:0.0100\n",
      "Episode:279 meanR:0.5001 R:0.6600 gloss:0.0008 dloss:0.0652 exploreP:0.0100\n",
      "Episode:280 meanR:0.5034 R:0.9000 gloss:0.0002 dloss:0.0044 exploreP:0.0100\n",
      "Episode:281 meanR:0.5075 R:0.4100 gloss:0.0005 dloss:0.0160 exploreP:0.0100\n",
      "Episode:282 meanR:0.5126 R:0.9400 gloss:0.0006 dloss:0.0328 exploreP:0.0100\n",
      "Episode:283 meanR:0.5083 R:0.2600 gloss:0.0003 dloss:0.0234 exploreP:0.0100\n",
      "Episode:284 meanR:0.5160 R:1.0700 gloss:0.0002 dloss:0.0148 exploreP:0.0100\n",
      "Episode:285 meanR:0.5155 R:1.0500 gloss:0.0002 dloss:0.0114 exploreP:0.0100\n",
      "Episode:286 meanR:0.5123 R:0.5200 gloss:0.0003 dloss:0.0183 exploreP:0.0100\n",
      "Episode:287 meanR:0.5198 R:1.3900 gloss:0.0014 dloss:0.0464 exploreP:0.0100\n",
      "Episode:288 meanR:0.5220 R:0.5000 gloss:0.0003 dloss:0.0141 exploreP:0.0100\n",
      "Episode:289 meanR:0.5127 R:0.3400 gloss:0.0003 dloss:0.0176 exploreP:0.0100\n",
      "Episode:290 meanR:0.5192 R:0.7100 gloss:0.0006 dloss:0.0286 exploreP:0.0100\n",
      "Episode:291 meanR:0.5143 R:0.2800 gloss:0.0003 dloss:0.0192 exploreP:0.0100\n",
      "Episode:292 meanR:0.5244 R:1.2200 gloss:0.0003 dloss:0.0153 exploreP:0.0100\n",
      "Episode:293 meanR:0.5369 R:2.0200 gloss:0.0006 dloss:0.0240 exploreP:0.0100\n",
      "Episode:294 meanR:0.5369 R:0.4800 gloss:0.0005 dloss:0.0202 exploreP:0.0100\n",
      "Episode:295 meanR:0.5355 R:0.3800 gloss:0.0004 dloss:0.0100 exploreP:0.0100\n",
      "Episode:296 meanR:0.5290 R:0.2900 gloss:0.0003 dloss:0.0185 exploreP:0.0100\n",
      "Episode:297 meanR:0.5259 R:0.5200 gloss:0.0006 dloss:0.0691 exploreP:0.0100\n",
      "Episode:298 meanR:0.5244 R:0.1700 gloss:0.0002 dloss:0.0041 exploreP:0.0100\n",
      "Episode:299 meanR:0.5212 R:0.0000 gloss:0.0001 dloss:0.0354 exploreP:0.0100\n",
      "Episode:300 meanR:0.5218 R:0.8100 gloss:0.0040 dloss:0.0064 exploreP:0.0100\n",
      "Episode:301 meanR:0.5209 R:0.0600 gloss:0.0008 dloss:0.0214 exploreP:0.0100\n",
      "Episode:302 meanR:0.5240 R:1.1800 gloss:0.0008 dloss:0.0118 exploreP:0.0100\n",
      "Episode:303 meanR:0.5268 R:1.2900 gloss:-0.0001 dloss:0.0231 exploreP:0.0100\n",
      "Episode:304 meanR:0.5269 R:0.8200 gloss:0.0009 dloss:0.0167 exploreP:0.0100\n",
      "Episode:305 meanR:0.5308 R:0.7500 gloss:0.0005 dloss:0.0146 exploreP:0.0100\n",
      "Episode:306 meanR:0.5286 R:0.3300 gloss:0.0000 dloss:0.0315 exploreP:0.0100\n",
      "Episode:307 meanR:0.5278 R:0.7900 gloss:0.0003 dloss:0.0100 exploreP:0.0100\n",
      "Episode:308 meanR:0.5251 R:0.5800 gloss:0.0003 dloss:0.0186 exploreP:0.0100\n",
      "Episode:309 meanR:0.5341 R:0.9000 gloss:0.0011 dloss:0.0152 exploreP:0.0100\n",
      "Episode:310 meanR:0.5341 R:0.0000 gloss:0.0008 dloss:0.0155 exploreP:0.0100\n",
      "Episode:311 meanR:0.5345 R:0.9400 gloss:0.0005 dloss:0.0153 exploreP:0.0100\n",
      "Episode:312 meanR:0.5409 R:1.1000 gloss:0.0006 dloss:0.0145 exploreP:0.0100\n",
      "Episode:313 meanR:0.5352 R:0.0000 gloss:0.0002 dloss:0.0290 exploreP:0.0100\n",
      "Episode:314 meanR:0.5282 R:0.1200 gloss:0.0003 dloss:0.0049 exploreP:0.0100\n",
      "Episode:315 meanR:0.5273 R:0.4000 gloss:0.0003 dloss:0.0227 exploreP:0.0100\n",
      "Episode:316 meanR:0.5166 R:0.3700 gloss:0.0003 dloss:0.0137 exploreP:0.0100\n",
      "Episode:317 meanR:0.5244 R:0.7800 gloss:0.0002 dloss:0.0191 exploreP:0.0100\n",
      "Episode:318 meanR:0.5284 R:0.8700 gloss:0.0001 dloss:0.0070 exploreP:0.0100\n",
      "Episode:319 meanR:0.5245 R:0.1000 gloss:0.0003 dloss:0.0277 exploreP:0.0100\n",
      "Episode:320 meanR:0.5271 R:0.3200 gloss:0.0001 dloss:0.0082 exploreP:0.0100\n",
      "Episode:321 meanR:0.5180 R:0.0000 gloss:0.0007 dloss:0.0168 exploreP:0.0100\n",
      "Episode:322 meanR:0.5180 R:0.0000 gloss:0.0004 dloss:0.0143 exploreP:0.0100\n",
      "Episode:323 meanR:0.5299 R:1.2300 gloss:0.0002 dloss:0.0192 exploreP:0.0100\n",
      "Episode:324 meanR:0.5408 R:1.0900 gloss:0.0004 dloss:0.0135 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:325 meanR:0.5390 R:0.0000 gloss:0.0002 dloss:0.0272 exploreP:0.0100\n",
      "Episode:326 meanR:0.5349 R:0.4800 gloss:0.0002 dloss:0.0140 exploreP:0.0100\n",
      "Episode:327 meanR:0.5424 R:0.7500 gloss:0.0003 dloss:0.0142 exploreP:0.0100\n",
      "Episode:328 meanR:0.5420 R:0.5700 gloss:0.0005 dloss:0.0103 exploreP:0.0100\n",
      "Episode:329 meanR:0.5441 R:0.8400 gloss:0.0002 dloss:0.0106 exploreP:0.0100\n",
      "Episode:330 meanR:0.5441 R:0.0000 gloss:0.0000 dloss:0.0206 exploreP:0.0100\n",
      "Episode:331 meanR:0.5336 R:0.4100 gloss:0.0003 dloss:0.0090 exploreP:0.0100\n",
      "Episode:332 meanR:0.5295 R:0.1100 gloss:0.0002 dloss:0.0179 exploreP:0.0100\n",
      "Episode:333 meanR:0.5349 R:0.5400 gloss:0.0003 dloss:0.0121 exploreP:0.0100\n",
      "Episode:334 meanR:0.5385 R:0.3600 gloss:0.0001 dloss:0.0121 exploreP:0.0100\n",
      "Episode:335 meanR:0.5311 R:0.4500 gloss:0.0004 dloss:0.0249 exploreP:0.0100\n",
      "Episode:336 meanR:0.5391 R:0.8000 gloss:0.0001 dloss:0.0043 exploreP:0.0100\n",
      "Episode:337 meanR:0.5389 R:0.1700 gloss:0.0004 dloss:0.0187 exploreP:0.0100\n",
      "Episode:338 meanR:0.5371 R:0.4600 gloss:0.0002 dloss:0.0150 exploreP:0.0100\n",
      "Episode:339 meanR:0.5339 R:0.4800 gloss:-0.0017 dloss:0.0154 exploreP:0.0100\n",
      "Episode:340 meanR:0.5395 R:0.5600 gloss:0.0052 dloss:0.0385 exploreP:0.0100\n",
      "Episode:341 meanR:0.5484 R:1.4100 gloss:0.0008 dloss:0.0052 exploreP:0.0100\n",
      "Episode:342 meanR:0.5522 R:0.3800 gloss:0.0007 dloss:0.0080 exploreP:0.0100\n",
      "Episode:343 meanR:0.5596 R:0.7400 gloss:0.0014 dloss:0.0145 exploreP:0.0100\n",
      "Episode:344 meanR:0.5605 R:0.6400 gloss:0.0011 dloss:0.0215 exploreP:0.0100\n",
      "Episode:345 meanR:0.5663 R:1.0300 gloss:0.0004 dloss:0.0087 exploreP:0.0100\n",
      "Episode:346 meanR:0.5670 R:0.8200 gloss:0.0004 dloss:0.0115 exploreP:0.0100\n",
      "Episode:347 meanR:0.5650 R:0.0000 gloss:0.0005 dloss:0.0064 exploreP:0.0100\n",
      "Episode:348 meanR:0.5567 R:0.3600 gloss:0.0005 dloss:0.0155 exploreP:0.0100\n",
      "Episode:349 meanR:0.5511 R:0.9000 gloss:0.0007 dloss:0.0130 exploreP:0.0100\n",
      "Episode:350 meanR:0.5414 R:0.0800 gloss:0.0002 dloss:0.0083 exploreP:0.0100\n",
      "Episode:351 meanR:0.5406 R:0.6800 gloss:0.0003 dloss:0.0083 exploreP:0.0100\n",
      "Episode:352 meanR:0.5480 R:0.8700 gloss:0.0004 dloss:0.0197 exploreP:0.0100\n",
      "Episode:353 meanR:0.5507 R:0.2700 gloss:0.0002 dloss:0.0155 exploreP:0.0100\n",
      "Episode:354 meanR:0.5558 R:0.5100 gloss:0.0007 dloss:0.0202 exploreP:0.0100\n",
      "Episode:355 meanR:0.5626 R:0.6800 gloss:0.0002 dloss:0.0063 exploreP:0.0100\n",
      "Episode:356 meanR:0.5737 R:1.3100 gloss:0.0006 dloss:0.0172 exploreP:0.0100\n",
      "Episode:357 meanR:0.5716 R:1.0500 gloss:0.0004 dloss:0.0131 exploreP:0.0100\n",
      "Episode:358 meanR:0.5754 R:0.5100 gloss:0.0005 dloss:0.0073 exploreP:0.0100\n",
      "Episode:359 meanR:0.5784 R:0.8300 gloss:0.0004 dloss:0.0212 exploreP:0.0100\n",
      "Episode:360 meanR:0.5765 R:0.6600 gloss:0.0009 dloss:0.0096 exploreP:0.0100\n",
      "Episode:361 meanR:0.5796 R:0.7700 gloss:0.0007 dloss:0.0149 exploreP:0.0100\n",
      "Episode:362 meanR:0.5629 R:0.0000 gloss:0.0006 dloss:0.0085 exploreP:0.0100\n",
      "Episode:363 meanR:0.5600 R:0.3400 gloss:0.0004 dloss:0.0123 exploreP:0.0100\n",
      "Episode:364 meanR:0.5627 R:0.2700 gloss:0.0005 dloss:0.0245 exploreP:0.0100\n",
      "Episode:365 meanR:0.5627 R:0.0000 gloss:0.0001 dloss:0.0054 exploreP:0.0100\n",
      "Episode:366 meanR:0.5605 R:0.0000 gloss:0.0006 dloss:0.0106 exploreP:0.0100\n",
      "Episode:367 meanR:0.5595 R:0.3400 gloss:0.0002 dloss:0.0100 exploreP:0.0100\n",
      "Episode:368 meanR:0.5756 R:1.6100 gloss:0.0004 dloss:0.0092 exploreP:0.0100\n",
      "Episode:369 meanR:0.5625 R:0.3300 gloss:0.0002 dloss:0.0189 exploreP:0.0100\n",
      "Episode:370 meanR:0.5653 R:0.5800 gloss:0.0003 dloss:0.0110 exploreP:0.0100\n",
      "Episode:371 meanR:0.5646 R:0.0400 gloss:0.0009 dloss:0.0274 exploreP:0.0100\n",
      "Episode:372 meanR:0.5645 R:0.0000 gloss:0.0001 dloss:0.0059 exploreP:0.0100\n",
      "Episode:373 meanR:0.5618 R:0.2400 gloss:-0.0002 dloss:0.0162 exploreP:0.0100\n",
      "Episode:374 meanR:0.5706 R:1.4900 gloss:0.0017 dloss:0.0242 exploreP:0.0100\n",
      "Episode:375 meanR:0.5711 R:0.8700 gloss:0.4266 dloss:0.0629 exploreP:0.0100\n",
      "Episode:376 meanR:0.5741 R:0.8800 gloss:-0.0202 dloss:0.0264 exploreP:0.0100\n",
      "Episode:377 meanR:0.5760 R:0.1900 gloss:-0.1616 dloss:0.0514 exploreP:0.0100\n",
      "Episode:378 meanR:0.5820 R:0.6000 gloss:0.1686 dloss:0.0445 exploreP:0.0100\n",
      "Episode:379 meanR:0.5761 R:0.0700 gloss:0.2605 dloss:0.0209 exploreP:0.0100\n",
      "Episode:380 meanR:0.5756 R:0.8500 gloss:0.1474 dloss:0.0265 exploreP:0.0100\n",
      "Episode:381 meanR:0.5747 R:0.3200 gloss:0.1205 dloss:0.0238 exploreP:0.0100\n",
      "Episode:382 meanR:0.5766 R:1.1300 gloss:0.0172 dloss:0.0241 exploreP:0.0100\n",
      "Episode:383 meanR:0.5763 R:0.2300 gloss:0.0412 dloss:0.0182 exploreP:0.0100\n",
      "Episode:384 meanR:0.5761 R:1.0500 gloss:0.0198 dloss:0.0188 exploreP:0.0100\n",
      "Episode:385 meanR:0.5677 R:0.2100 gloss:0.0059 dloss:0.0195 exploreP:0.0100\n",
      "Episode:386 meanR:0.5657 R:0.3200 gloss:0.0052 dloss:0.0182 exploreP:0.0100\n",
      "Episode:387 meanR:0.5565 R:0.4700 gloss:0.0025 dloss:0.0131 exploreP:0.0100\n",
      "Episode:388 meanR:0.5573 R:0.5800 gloss:0.0026 dloss:0.0277 exploreP:0.0100\n",
      "Episode:389 meanR:0.5547 R:0.0800 gloss:0.0011 dloss:0.0109 exploreP:0.0100\n",
      "Episode:390 meanR:0.5601 R:1.2500 gloss:0.0004 dloss:0.0155 exploreP:0.0100\n",
      "Episode:391 meanR:0.5689 R:1.1600 gloss:0.0004 dloss:0.0146 exploreP:0.0100\n",
      "Episode:392 meanR:0.5629 R:0.6200 gloss:0.0005 dloss:0.0204 exploreP:0.0100\n",
      "Episode:393 meanR:0.5498 R:0.7100 gloss:0.0013 dloss:0.0187 exploreP:0.0100\n",
      "Episode:394 meanR:0.5450 R:0.0000 gloss:0.0006 dloss:0.0098 exploreP:0.0100\n",
      "Episode:395 meanR:0.5481 R:0.6900 gloss:0.0011 dloss:0.0239 exploreP:0.0100\n",
      "Episode:396 meanR:0.5531 R:0.7900 gloss:0.0008 dloss:0.0173 exploreP:0.0100\n",
      "Episode:397 meanR:0.5479 R:0.0000 gloss:0.0009 dloss:0.0140 exploreP:0.0100\n",
      "Episode:398 meanR:0.5526 R:0.6400 gloss:0.0002 dloss:0.0093 exploreP:0.0100\n",
      "Episode:399 meanR:0.5611 R:0.8500 gloss:0.0000 dloss:0.0159 exploreP:0.0100\n",
      "Episode:400 meanR:0.5573 R:0.4300 gloss:0.0002 dloss:0.0105 exploreP:0.0100\n",
      "Episode:401 meanR:0.5630 R:0.6300 gloss:0.0004 dloss:0.0110 exploreP:0.0100\n",
      "Episode:402 meanR:0.5564 R:0.5200 gloss:0.0003 dloss:0.0119 exploreP:0.0100\n",
      "Episode:403 meanR:0.5552 R:1.1700 gloss:-0.0001 dloss:0.0051 exploreP:0.0100\n",
      "Episode:404 meanR:0.5508 R:0.3800 gloss:-0.0005 dloss:0.0121 exploreP:0.0100\n",
      "Episode:405 meanR:0.5451 R:0.1800 gloss:0.0008 dloss:0.0107 exploreP:0.0100\n",
      "Episode:406 meanR:0.5485 R:0.6700 gloss:-0.0000 dloss:0.0089 exploreP:0.0100\n",
      "Episode:407 meanR:0.5489 R:0.8300 gloss:-0.0003 dloss:0.0095 exploreP:0.0100\n",
      "Episode:408 meanR:0.5431 R:0.0000 gloss:0.0005 dloss:0.0105 exploreP:0.0100\n",
      "Episode:409 meanR:0.5370 R:0.2900 gloss:0.0001 dloss:0.0076 exploreP:0.0100\n",
      "Episode:410 meanR:0.5424 R:0.5400 gloss:0.0002 dloss:0.0124 exploreP:0.0100\n",
      "Episode:411 meanR:0.5350 R:0.2000 gloss:0.0003 dloss:0.0117 exploreP:0.0100\n",
      "Episode:412 meanR:0.5267 R:0.2700 gloss:0.0001 dloss:0.0045 exploreP:0.0100\n",
      "Episode:413 meanR:0.5276 R:0.0900 gloss:-0.0067 dloss:0.0114 exploreP:0.0100\n",
      "Episode:414 meanR:0.5385 R:1.2100 gloss:-0.0028 dloss:0.0137 exploreP:0.0100\n",
      "Episode:415 meanR:0.5412 R:0.6700 gloss:0.0062 dloss:0.0191 exploreP:0.0100\n",
      "Episode:416 meanR:0.5433 R:0.5800 gloss:-0.0425 dloss:0.0207 exploreP:0.0100\n",
      "Episode:417 meanR:0.5398 R:0.4300 gloss:-0.0134 dloss:0.0067 exploreP:0.0100\n",
      "Episode:418 meanR:0.5360 R:0.4900 gloss:0.0001 dloss:0.0121 exploreP:0.0100\n",
      "Episode:419 meanR:0.5350 R:0.0000 gloss:0.0006 dloss:0.0082 exploreP:0.0100\n",
      "Episode:420 meanR:0.5346 R:0.2800 gloss:-0.0154 dloss:0.0177 exploreP:0.0100\n",
      "Episode:421 meanR:0.5355 R:0.0900 gloss:-0.0117 dloss:0.0049 exploreP:0.0100\n",
      "Episode:422 meanR:0.5389 R:0.3400 gloss:0.0037 dloss:0.0100 exploreP:0.0100\n",
      "Episode:423 meanR:0.5279 R:0.1300 gloss:0.0022 dloss:0.0078 exploreP:0.0100\n",
      "Episode:424 meanR:0.5219 R:0.4900 gloss:0.0004 dloss:0.0103 exploreP:0.0100\n",
      "Episode:425 meanR:0.5276 R:0.5700 gloss:0.0001 dloss:0.0029 exploreP:0.0100\n",
      "Episode:426 meanR:0.5228 R:0.0000 gloss:-0.0002 dloss:0.0064 exploreP:0.0100\n",
      "Episode:427 meanR:0.5195 R:0.4200 gloss:0.0008 dloss:0.0045 exploreP:0.0100\n",
      "Episode:428 meanR:0.5198 R:0.6000 gloss:0.0005 dloss:0.0095 exploreP:0.0100\n",
      "Episode:429 meanR:0.5186 R:0.7200 gloss:-0.0000 dloss:0.0055 exploreP:0.0100\n",
      "Episode:430 meanR:0.5241 R:0.5500 gloss:-0.0003 dloss:0.0056 exploreP:0.0100\n",
      "Episode:431 meanR:0.5269 R:0.6900 gloss:-0.0123 dloss:0.0167 exploreP:0.0100\n",
      "Episode:432 meanR:0.5320 R:0.6200 gloss:0.0205 dloss:0.0046 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:433 meanR:0.5337 R:0.7100 gloss:0.0092 dloss:0.0060 exploreP:0.0100\n",
      "Episode:434 meanR:0.5315 R:0.1400 gloss:0.0071 dloss:0.0071 exploreP:0.0100\n",
      "Episode:435 meanR:0.5318 R:0.4800 gloss:0.0132 dloss:0.0186 exploreP:0.0100\n",
      "Episode:436 meanR:0.5238 R:0.0000 gloss:0.0018 dloss:0.0033 exploreP:0.0100\n",
      "Episode:437 meanR:0.5234 R:0.1300 gloss:0.0108 dloss:0.0082 exploreP:0.0100\n",
      "Episode:438 meanR:0.5205 R:0.1700 gloss:0.0015 dloss:0.0081 exploreP:0.0100\n",
      "Episode:439 meanR:0.5242 R:0.8500 gloss:-0.0012 dloss:0.0052 exploreP:0.0100\n",
      "Episode:440 meanR:0.5262 R:0.7600 gloss:0.0017 dloss:0.0111 exploreP:0.0100\n",
      "Episode:441 meanR:0.5193 R:0.7200 gloss:0.0013 dloss:0.0036 exploreP:0.0100\n",
      "Episode:442 meanR:0.5205 R:0.5000 gloss:0.0002 dloss:0.0075 exploreP:0.0100\n",
      "Episode:443 meanR:0.5233 R:1.0200 gloss:0.0008 dloss:0.0045 exploreP:0.0100\n",
      "Episode:444 meanR:0.5220 R:0.5100 gloss:0.0003 dloss:0.0097 exploreP:0.0100\n",
      "Episode:445 meanR:0.5184 R:0.6700 gloss:0.0003 dloss:0.0056 exploreP:0.0100\n",
      "Episode:446 meanR:0.5102 R:0.0000 gloss:0.0005 dloss:0.0060 exploreP:0.0100\n",
      "Episode:447 meanR:0.5249 R:1.4700 gloss:0.0002 dloss:0.0051 exploreP:0.0100\n",
      "Episode:448 meanR:0.5293 R:0.8000 gloss:0.0007 dloss:0.0159 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-reacher-Continuous_Control.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            # if explore_p > np.random.rand():\n",
    "            #     #action = env.action_space.sample()\n",
    "            #     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            # else:\n",
    "            action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            #print(action.shape)\n",
    "            #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "            #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            #batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #print(actions.shape, actions.dtype)\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            #print(nextQs_logits.shape, targetQs.shape)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-reacher-Continuous_Control.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
