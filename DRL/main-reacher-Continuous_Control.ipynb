{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1099999975413084\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits,\n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    g_loss = -tf.reduce_mean(Qs)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    g_loss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    #g_loss2 = tf.reduce_mean(Qs)\n",
    "    g_loss2 = tf.reduce_mean(neg_log_prob_actions * Qs)\n",
    "    g_loss3 = tf.reduce_mean(targetQs)\n",
    "    return actions_logits, Qs, g_loss, d_loss, g_loss1, g_loss2, g_loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2, self.g_loss3 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.623481828158697\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100               # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(action, -2, 2)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.2000 R:0.2000 gloss:-0.3280 gloss1:3.3649 gloss2:0.8398 gloss3:0.3255 dloss:0.1699\n",
      "Episode:1 meanR:0.1850 R:0.1700 gloss:-0.0242 gloss1:6.0981 gloss2:0.1357 gloss3:0.0243 dloss:0.0349\n",
      "Episode:2 meanR:0.3467 R:0.6700 gloss:-0.0145 gloss1:6.4561 gloss2:0.0896 gloss3:0.0149 dloss:0.0229\n",
      "Episode:3 meanR:0.4700 R:0.8400 gloss:-0.0235 gloss1:5.3198 gloss2:0.1169 gloss3:0.0236 dloss:0.0125\n",
      "Episode:4 meanR:0.6100 R:1.1700 gloss:-0.0179 gloss1:4.9781 gloss2:0.0815 gloss3:0.0179 dloss:0.0085\n",
      "Episode:5 meanR:0.5083 R:0.0000 gloss:-0.0343 gloss1:4.7296 gloss2:0.1629 gloss3:0.0345 dloss:0.0063\n",
      "Episode:6 meanR:0.4929 R:0.4000 gloss:-0.0355 gloss1:4.8527 gloss2:0.1728 gloss3:0.0354 dloss:0.0047\n",
      "Episode:7 meanR:0.4762 R:0.3600 gloss:-0.0401 gloss1:4.6651 gloss2:0.1869 gloss3:0.0402 dloss:0.0033\n",
      "Episode:8 meanR:0.4822 R:0.5300 gloss:-0.0443 gloss1:4.8626 gloss2:0.2117 gloss3:0.0444 dloss:0.0024\n",
      "Episode:9 meanR:0.4740 R:0.4000 gloss:-0.0335 gloss1:3.1941 gloss2:0.1053 gloss3:0.0335 dloss:0.0021\n",
      "Episode:10 meanR:0.5318 R:1.1100 gloss:-0.0452 gloss1:1.8818 gloss2:0.0834 gloss3:0.0453 dloss:0.0020\n",
      "Episode:11 meanR:0.5392 R:0.6200 gloss:-0.0581 gloss1:3.0567 gloss2:0.1667 gloss3:0.0580 dloss:0.0017\n",
      "Episode:12 meanR:0.5492 R:0.6700 gloss:-0.0523 gloss1:4.3802 gloss2:0.2299 gloss3:0.0524 dloss:0.0016\n",
      "Episode:13 meanR:0.5879 R:1.0900 gloss:-0.0664 gloss1:5.4922 gloss2:0.3667 gloss3:0.0664 dloss:0.0014\n",
      "Episode:14 meanR:0.6393 R:1.3600 gloss:-0.0657 gloss1:6.7061 gloss2:0.4403 gloss3:0.0657 dloss:0.0014\n",
      "Episode:15 meanR:0.6769 R:1.2400 gloss:-0.0841 gloss1:5.2964 gloss2:0.4331 gloss3:0.0842 dloss:0.0012\n",
      "Episode:16 meanR:0.6435 R:0.1100 gloss:-0.0737 gloss1:3.3919 gloss2:0.2515 gloss3:0.0737 dloss:0.0012\n",
      "Episode:17 meanR:0.6611 R:0.9600 gloss:-0.0642 gloss1:1.4485 gloss2:0.0951 gloss3:0.0643 dloss:0.0013\n",
      "Episode:18 meanR:0.6395 R:0.2500 gloss:-0.0711 gloss1:3.8644 gloss2:0.2752 gloss3:0.0710 dloss:0.0010\n",
      "Episode:19 meanR:0.6130 R:0.1100 gloss:-0.0702 gloss1:2.6940 gloss2:0.1890 gloss3:0.0702 dloss:0.0008\n",
      "Episode:20 meanR:0.5838 R:0.0000 gloss:-0.0668 gloss1:2.2241 gloss2:0.1503 gloss3:0.0668 dloss:0.0007\n",
      "Episode:21 meanR:0.5832 R:0.5700 gloss:-0.0540 gloss1:1.4055 gloss2:0.0762 gloss3:0.0541 dloss:0.0006\n",
      "Episode:22 meanR:0.5713 R:0.3100 gloss:-0.0525 gloss1:0.8087 gloss2:0.0430 gloss3:0.0525 dloss:0.0005\n",
      "Episode:23 meanR:0.5867 R:0.9400 gloss:-0.0518 gloss1:0.7501 gloss2:0.0390 gloss3:0.0518 dloss:0.0005\n",
      "Episode:24 meanR:0.5792 R:0.4000 gloss:-0.0448 gloss1:0.6198 gloss2:0.0280 gloss3:0.0448 dloss:0.0004\n",
      "Episode:25 meanR:0.5569 R:0.0000 gloss:-0.0303 gloss1:0.5335 gloss2:0.0162 gloss3:0.0303 dloss:0.0005\n",
      "Episode:26 meanR:0.5507 R:0.3900 gloss:-0.0259 gloss1:1.4017 gloss2:0.0372 gloss3:0.0259 dloss:0.0004\n",
      "Episode:27 meanR:0.5625 R:0.8800 gloss:-0.0301 gloss1:3.4355 gloss2:0.1042 gloss3:0.0301 dloss:0.0004\n",
      "Episode:28 meanR:0.5479 R:0.1400 gloss:-0.0394 gloss1:4.7461 gloss2:0.1881 gloss3:0.0394 dloss:0.0004\n",
      "Episode:29 meanR:0.5440 R:0.4300 gloss:-0.0370 gloss1:5.1573 gloss2:0.1912 gloss3:0.0370 dloss:0.0004\n",
      "Episode:30 meanR:0.5529 R:0.8200 gloss:-0.0399 gloss1:5.1702 gloss2:0.2067 gloss3:0.0399 dloss:0.0005\n",
      "Episode:31 meanR:0.5825 R:1.5000 gloss:-0.0407 gloss1:4.9366 gloss2:0.2002 gloss3:0.0407 dloss:0.0004\n",
      "Episode:32 meanR:0.6173 R:1.7300 gloss:-0.0591 gloss1:4.8940 gloss2:0.2885 gloss3:0.0591 dloss:0.0005\n",
      "Episode:33 meanR:0.6253 R:0.8900 gloss:-0.0599 gloss1:4.2140 gloss2:0.2519 gloss3:0.0599 dloss:0.0006\n",
      "Episode:34 meanR:0.6140 R:0.2300 gloss:-0.0611 gloss1:2.5900 gloss2:0.1549 gloss3:0.0611 dloss:0.0007\n",
      "Episode:35 meanR:0.6033 R:0.2300 gloss:-0.0688 gloss1:1.1115 gloss2:0.0757 gloss3:0.0688 dloss:0.0007\n",
      "Episode:36 meanR:0.5949 R:0.2900 gloss:-0.0699 gloss1:0.4280 gloss2:0.0301 gloss3:0.0699 dloss:0.0009\n",
      "Episode:37 meanR:0.6461 R:2.5400 gloss:-0.0715 gloss1:0.7102 gloss2:0.0530 gloss3:0.0715 dloss:0.0014\n",
      "Episode:38 meanR:0.6549 R:0.9900 gloss:-0.0812 gloss1:7.2153 gloss2:0.5933 gloss3:0.0812 dloss:0.0020\n",
      "Episode:39 meanR:0.6507 R:0.4900 gloss:-0.0888 gloss1:12.1793 gloss2:1.0805 gloss3:0.0888 dloss:0.0015\n",
      "Episode:40 meanR:0.6649 R:1.2300 gloss:-0.0927 gloss1:16.0164 gloss2:1.4923 gloss3:0.0927 dloss:0.0016\n",
      "Episode:41 meanR:0.6552 R:0.2600 gloss:-0.0924 gloss1:15.7870 gloss2:1.4551 gloss3:0.0924 dloss:0.0014\n",
      "Episode:42 meanR:0.6442 R:0.1800 gloss:-0.0755 gloss1:17.2793 gloss2:1.3131 gloss3:0.0755 dloss:0.0015\n",
      "Episode:43 meanR:0.6355 R:0.2600 gloss:-0.0653 gloss1:13.4996 gloss2:0.8807 gloss3:0.0653 dloss:0.0012\n",
      "Episode:44 meanR:0.6236 R:0.1000 gloss:-0.0614 gloss1:12.1104 gloss2:0.7434 gloss3:0.0614 dloss:0.0018\n",
      "Episode:45 meanR:0.6237 R:0.6300 gloss:-0.0556 gloss1:9.1532 gloss2:0.5027 gloss3:0.0556 dloss:0.0014\n",
      "Episode:46 meanR:0.6343 R:1.1200 gloss:-0.0695 gloss1:5.0182 gloss2:0.3422 gloss3:0.0695 dloss:0.0009\n",
      "Episode:47 meanR:0.6283 R:0.3500 gloss:-0.0690 gloss1:9.0190 gloss2:0.5720 gloss3:0.0690 dloss:0.0007\n",
      "Episode:48 meanR:0.6371 R:1.0600 gloss:-0.0466 gloss1:31.5737 gloss2:1.4642 gloss3:0.0466 dloss:0.0012\n",
      "Episode:49 meanR:0.6268 R:0.1200 gloss:-0.0461 gloss1:27.8745 gloss2:1.2855 gloss3:0.0461 dloss:0.0024\n",
      "Episode:50 meanR:0.6200 R:0.2800 gloss:-0.0445 gloss1:16.4818 gloss2:0.7539 gloss3:0.0445 dloss:0.0022\n",
      "Episode:51 meanR:0.6250 R:0.8800 gloss:-0.0436 gloss1:3.3011 gloss2:0.1456 gloss3:0.0436 dloss:0.0040\n",
      "Episode:52 meanR:0.6175 R:0.2300 gloss:-0.0539 gloss1:11.1287 gloss2:0.5942 gloss3:0.0538 dloss:0.0008\n",
      "Episode:53 meanR:0.6061 R:0.0000 gloss:-0.0450 gloss1:15.6709 gloss2:0.7082 gloss3:0.0450 dloss:0.0028\n",
      "Episode:54 meanR:0.6045 R:0.5200 gloss:-0.0444 gloss1:21.7099 gloss2:0.9593 gloss3:0.0444 dloss:0.0068\n",
      "Episode:55 meanR:0.6036 R:0.5500 gloss:-0.0489 gloss1:25.1432 gloss2:1.2328 gloss3:0.0489 dloss:0.0002\n",
      "Episode:56 meanR:0.5951 R:0.1200 gloss:-0.0437 gloss1:27.5839 gloss2:1.2195 gloss3:0.0437 dloss:0.0013\n",
      "Episode:57 meanR:0.5848 R:0.0000 gloss:-0.0328 gloss1:22.4912 gloss2:0.7484 gloss3:0.0328 dloss:0.0063\n",
      "Episode:58 meanR:0.5776 R:0.1600 gloss:-0.0258 gloss1:11.3047 gloss2:0.3037 gloss3:0.0257 dloss:0.0008\n",
      "Episode:59 meanR:0.5758 R:0.4700 gloss:-0.0277 gloss1:6.0720 gloss2:0.1601 gloss3:0.0277 dloss:0.0041\n",
      "Episode:60 meanR:0.5749 R:0.5200 gloss:-0.0285 gloss1:3.4318 gloss2:0.0947 gloss3:0.0284 dloss:0.0021\n",
      "Episode:61 meanR:0.5702 R:0.2800 gloss:-0.0314 gloss1:4.6385 gloss2:0.1316 gloss3:0.0315 dloss:0.0038\n",
      "Episode:62 meanR:0.5670 R:0.3700 gloss:-0.0339 gloss1:9.8062 gloss2:0.3024 gloss3:0.0339 dloss:0.0046\n",
      "Episode:63 meanR:0.5766 R:1.1800 gloss:-0.0288 gloss1:11.1656 gloss2:0.3090 gloss3:0.0288 dloss:0.0050\n",
      "Episode:64 meanR:0.5677 R:0.0000 gloss:-0.0416 gloss1:10.9643 gloss2:0.4546 gloss3:0.0416 dloss:0.0036\n",
      "Episode:65 meanR:0.5617 R:0.1700 gloss:-0.0291 gloss1:10.6787 gloss2:0.2947 gloss3:0.0291 dloss:0.0053\n",
      "Episode:66 meanR:0.5693 R:1.0700 gloss:-0.0309 gloss1:7.6658 gloss2:0.2221 gloss3:0.0309 dloss:0.0036\n",
      "Episode:67 meanR:0.5766 R:1.0700 gloss:-0.0356 gloss1:7.8751 gloss2:0.2790 gloss3:0.0356 dloss:0.0046\n",
      "Episode:68 meanR:0.5835 R:1.0500 gloss:-0.0478 gloss1:19.5341 gloss2:0.9195 gloss3:0.0479 dloss:0.0024\n",
      "Episode:69 meanR:0.5900 R:1.0400 gloss:-0.0557 gloss1:31.0207 gloss2:1.7161 gloss3:0.0557 dloss:0.0016\n",
      "Episode:70 meanR:0.5817 R:0.0000 gloss:-0.0550 gloss1:53.1355 gloss2:2.9298 gloss3:0.0550 dloss:0.0022\n",
      "Episode:71 meanR:0.5790 R:0.3900 gloss:-0.0495 gloss1:53.8290 gloss2:2.6272 gloss3:0.0495 dloss:0.0004\n",
      "Episode:72 meanR:0.5789 R:0.5700 gloss:-0.0632 gloss1:37.4804 gloss2:2.3845 gloss3:0.0632 dloss:0.0015\n",
      "Episode:73 meanR:0.5845 R:0.9900 gloss:-0.0576 gloss1:24.1193 gloss2:1.3870 gloss3:0.0576 dloss:0.0008\n",
      "Episode:74 meanR:0.5893 R:0.9500 gloss:-0.0652 gloss1:14.6832 gloss2:0.9214 gloss3:0.0652 dloss:0.0029\n",
      "Episode:75 meanR:0.5989 R:1.3200 gloss:-0.0717 gloss1:8.4338 gloss2:0.6059 gloss3:0.0717 dloss:0.0002\n",
      "Episode:76 meanR:0.5929 R:0.1300 gloss:-0.0716 gloss1:4.1492 gloss2:0.2950 gloss3:0.0716 dloss:0.0012\n",
      "Episode:77 meanR:0.6064 R:1.6500 gloss:-0.0758 gloss1:0.2056 gloss2:0.0153 gloss3:0.0758 dloss:0.0026\n",
      "Episode:78 meanR:0.6209 R:1.7500 gloss:-0.0843 gloss1:0.1048 gloss2:0.0089 gloss3:0.0843 dloss:0.0007\n",
      "Episode:79 meanR:0.6206 R:0.6000 gloss:-0.0826 gloss1:1.1492 gloss2:0.0801 gloss3:0.0826 dloss:0.0018\n",
      "Episode:80 meanR:0.6159 R:0.2400 gloss:-0.0783 gloss1:13.9302 gloss2:1.1127 gloss3:0.0784 dloss:0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:81 meanR:0.6123 R:0.3200 gloss:-0.0777 gloss1:15.9349 gloss2:1.2352 gloss3:0.0777 dloss:0.0016\n",
      "Episode:82 meanR:0.6140 R:0.7500 gloss:-0.0820 gloss1:11.2129 gloss2:0.9171 gloss3:0.0820 dloss:0.0008\n",
      "Episode:83 meanR:0.6067 R:0.0000 gloss:-0.0793 gloss1:10.5717 gloss2:0.8385 gloss3:0.0793 dloss:0.0031\n",
      "Episode:84 meanR:0.6044 R:0.4100 gloss:-0.0666 gloss1:14.0677 gloss2:0.9302 gloss3:0.0666 dloss:0.0010\n",
      "Episode:85 meanR:0.6029 R:0.4800 gloss:-0.0647 gloss1:12.3328 gloss2:0.7965 gloss3:0.0647 dloss:0.0004\n",
      "Episode:86 meanR:0.6021 R:0.5300 gloss:-0.0539 gloss1:8.0795 gloss2:0.4317 gloss3:0.0540 dloss:0.0008\n",
      "Episode:87 meanR:0.6025 R:0.6400 gloss:-0.0585 gloss1:3.5957 gloss2:0.2171 gloss3:0.0585 dloss:0.0018\n",
      "Episode:88 meanR:0.6012 R:0.4900 gloss:-0.0494 gloss1:0.8990 gloss2:0.0457 gloss3:0.0494 dloss:0.0002\n",
      "Episode:89 meanR:0.5984 R:0.3500 gloss:-0.0415 gloss1:1.2411 gloss2:0.0529 gloss3:0.0415 dloss:0.0011\n",
      "Episode:90 meanR:0.5959 R:0.3700 gloss:-0.0387 gloss1:0.8946 gloss2:0.0357 gloss3:0.0386 dloss:0.0007\n",
      "Episode:91 meanR:0.5913 R:0.1700 gloss:-0.0409 gloss1:1.3289 gloss2:0.0560 gloss3:0.0409 dloss:0.0008\n",
      "Episode:92 meanR:0.5963 R:1.0600 gloss:-0.0332 gloss1:1.1891 gloss2:0.0371 gloss3:0.0332 dloss:0.0011\n",
      "Episode:93 meanR:0.6006 R:1.0000 gloss:-0.0479 gloss1:0.8345 gloss2:0.0384 gloss3:0.0479 dloss:0.0004\n",
      "Episode:94 meanR:0.6014 R:0.6700 gloss:-0.0525 gloss1:0.5195 gloss2:0.0267 gloss3:0.0524 dloss:0.0010\n",
      "Episode:95 meanR:0.6006 R:0.5300 gloss:-0.0528 gloss1:2.3444 gloss2:0.1235 gloss3:0.0528 dloss:0.0008\n",
      "Episode:96 meanR:0.6078 R:1.3000 gloss:-0.0587 gloss1:5.4388 gloss2:0.3236 gloss3:0.0587 dloss:0.0012\n",
      "Episode:97 meanR:0.6056 R:0.3900 gloss:-0.0657 gloss1:11.7163 gloss2:0.7796 gloss3:0.0658 dloss:0.0021\n",
      "Episode:98 meanR:0.6164 R:1.6700 gloss:-0.0657 gloss1:21.8754 gloss2:1.4334 gloss3:0.0657 dloss:0.0025\n",
      "Episode:99 meanR:0.6109 R:0.0700 gloss:-0.0549 gloss1:23.8714 gloss2:1.2859 gloss3:0.0550 dloss:0.0072\n",
      "Episode:100 meanR:0.6196 R:1.0700 gloss:-0.0668 gloss1:30.0307 gloss2:1.9790 gloss3:0.0669 dloss:0.0028\n",
      "Episode:101 meanR:0.6179 R:0.0000 gloss:-0.0696 gloss1:44.1550 gloss2:3.0108 gloss3:0.0696 dloss:0.0053\n",
      "Episode:102 meanR:0.6145 R:0.3300 gloss:-0.0712 gloss1:44.0478 gloss2:3.0903 gloss3:0.0712 dloss:0.0003\n",
      "Episode:103 meanR:0.6084 R:0.2300 gloss:-0.0623 gloss1:31.6313 gloss2:2.0144 gloss3:0.0623 dloss:0.0035\n",
      "Episode:104 meanR:0.6036 R:0.6900 gloss:-0.0597 gloss1:27.8899 gloss2:1.6131 gloss3:0.0598 dloss:0.0010\n",
      "Episode:105 meanR:0.6115 R:0.7900 gloss:-0.0547 gloss1:27.4035 gloss2:1.5083 gloss3:0.0548 dloss:0.0029\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    #total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch, gloss3_batch = [], [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            #action = np.clip(action_logits, -2, 2)                  # all actions between -1 and 1\n",
    "            #print(action.shape)\n",
    "            #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "            #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, gloss1, gloss2, gloss3, _, _ = sess.run([model.g_loss, model.d_loss,\n",
    "                                                                   model.g_loss1, model.g_loss2, model.g_loss3,\n",
    "                                                                   model.g_opt, model.d_opt],\n",
    "                                                                  feed_dict = {model.states: states, \n",
    "                                                                               model.actions: actions,\n",
    "                                                                               model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            gloss3_batch.append(gloss3)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'gloss1:{:.4f}'.format(np.mean(gloss1_batch)),\n",
    "              'gloss2:{:.4f}'.format(np.mean(gloss2_batch)),\n",
    "              'gloss3:{:.4f}'.format(np.mean(gloss3_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
