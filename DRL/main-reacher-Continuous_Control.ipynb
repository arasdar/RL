{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.029999999329447746\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.029999999329447746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float64, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float64, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float64, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits,\n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob_actions * targetQs) # DPG\n",
    "    #dloss = tf.reduce_mean(tf.square(Qs - targetQs)) # DQN\n",
    "    dloss = tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs,\n",
    "                                                    labels=tf.nn.sigmoid(targetQs))\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    dloss += tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs,\n",
    "                                                     labels=tf.nn.sigmoid(targetQs))\n",
    "    gloss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    gloss2 = tf.reduce_mean(Qs)\n",
    "    gloss3 = tf.reduce_mean(dQs)\n",
    "    gloss4 = tf.reduce_mean(targetQs)\n",
    "    return actions_logits, Qs, gloss, dloss, gloss1, gloss2, gloss3, gloss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2, self.g_loss3, self.g_loss4 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.717474771838839\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 1000              # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "num_steps = 0\n",
    "for _ in range(memory_size):\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Steps:1000 meanR:0.0000 R:0.0000 gloss:117.3085 gloss1-lgP:15.1408 gloss2-gQs:5.5072 gloss3-dQs:-0.1235 gloss4-tgtQ:5.4524 dloss:0.2486 exploreP:0.9057\n",
      "Episode:1 Steps:1000 meanR:0.0000 R:0.0000 gloss:13.0174 gloss1-lgP:1.8064 gloss2-gQs:6.0176 gloss3-dQs:5.7520 gloss4-tgtQ:5.9534 dloss:0.0913 exploreP:0.8204\n",
      "Episode:2 Steps:1000 meanR:0.1200 R:0.3600 gloss:0.8876 gloss1-lgP:0.7789 gloss2-gQs:1.1433 gloss3-dQs:1.1618 gloss4-tgtQ:1.1321 dloss:1.0546 exploreP:0.7432\n",
      "Episode:3 Steps:1000 meanR:0.2100 R:0.4800 gloss:0.0972 gloss1-lgP:0.7640 gloss2-gQs:0.1285 gloss3-dQs:0.1290 gloss4-tgtQ:0.1280 dloss:1.3615 exploreP:0.6734\n",
      "Episode:4 Steps:1000 meanR:0.2340 R:0.3300 gloss:0.0773 gloss1-lgP:0.7548 gloss2-gQs:0.1056 gloss3-dQs:0.1038 gloss4-tgtQ:0.1049 dloss:1.3689 exploreP:0.6102\n",
      "Episode:5 Steps:1000 meanR:0.2983 R:0.6200 gloss:0.0799 gloss1-lgP:0.7462 gloss2-gQs:0.1107 gloss3-dQs:0.1082 gloss4-tgtQ:0.1093 dloss:1.3729 exploreP:0.5530\n",
      "Episode:6 Steps:1000 meanR:0.2557 R:0.0000 gloss:0.0810 gloss1-lgP:0.7382 gloss2-gQs:0.1124 gloss3-dQs:0.1115 gloss4-tgtQ:0.1119 dloss:1.3752 exploreP:0.5013\n",
      "Episode:7 Steps:1000 meanR:0.2287 R:0.0400 gloss:0.0219 gloss1-lgP:0.7619 gloss2-gQs:0.0330 gloss3-dQs:0.0332 gloss4-tgtQ:0.0328 dloss:1.3767 exploreP:0.4545\n",
      "Episode:8 Steps:1000 meanR:0.2667 R:0.5700 gloss:0.0328 gloss1-lgP:0.7901 gloss2-gQs:0.0477 gloss3-dQs:0.0471 gloss4-tgtQ:0.0475 dloss:1.3779 exploreP:0.4121\n",
      "Episode:9 Steps:1000 meanR:0.3070 R:0.6700 gloss:0.0441 gloss1-lgP:0.7963 gloss2-gQs:0.0639 gloss3-dQs:0.0632 gloss4-tgtQ:0.0633 dloss:1.3774 exploreP:0.3738\n",
      "Episode:10 Steps:1000 meanR:0.2791 R:0.0000 gloss:0.0495 gloss1-lgP:0.7253 gloss2-gQs:0.0712 gloss3-dQs:0.0702 gloss4-tgtQ:0.0707 dloss:1.3803 exploreP:0.3392\n",
      "Episode:11 Steps:1000 meanR:0.3042 R:0.5800 gloss:0.2322 gloss1-lgP:0.8919 gloss2-gQs:0.2549 gloss3-dQs:0.2498 gloss4-tgtQ:0.2521 dloss:1.3148 exploreP:0.3078\n",
      "Episode:12 Steps:1000 meanR:0.3485 R:0.8800 gloss:0.8798 gloss1-lgP:1.1775 gloss2-gQs:0.7396 gloss3-dQs:0.7273 gloss4-tgtQ:0.7316 dloss:1.1688 exploreP:0.2795\n",
      "Episode:13 Steps:1000 meanR:0.3950 R:1.0000 gloss:0.0645 gloss1-lgP:0.6871 gloss2-gQs:0.0951 gloss3-dQs:0.0938 gloss4-tgtQ:0.0943 dloss:1.3811 exploreP:0.2538\n",
      "Episode:14 Steps:1000 meanR:0.4600 R:1.3700 gloss:0.0622 gloss1-lgP:0.6834 gloss2-gQs:0.0920 gloss3-dQs:0.0912 gloss4-tgtQ:0.0915 dloss:1.3827 exploreP:0.2306\n",
      "Episode:15 Steps:1000 meanR:0.4481 R:0.2700 gloss:0.0467 gloss1-lgP:0.6798 gloss2-gQs:0.0694 gloss3-dQs:0.0689 gloss4-tgtQ:0.0691 dloss:1.3835 exploreP:0.2096\n",
      "Episode:16 Steps:1000 meanR:0.5235 R:1.7300 gloss:0.0485 gloss1-lgP:0.6759 gloss2-gQs:0.0722 gloss3-dQs:0.0719 gloss4-tgtQ:0.0720 dloss:1.3836 exploreP:0.1905\n",
      "Episode:17 Steps:1000 meanR:0.5178 R:0.4200 gloss:0.0601 gloss1-lgP:0.6707 gloss2-gQs:0.0899 gloss3-dQs:0.0897 gloss4-tgtQ:0.0898 dloss:1.3830 exploreP:0.1734\n",
      "Episode:18 Steps:1000 meanR:0.4905 R:0.0000 gloss:0.0451 gloss1-lgP:0.6689 gloss2-gQs:0.0677 gloss3-dQs:0.0678 gloss4-tgtQ:0.0677 dloss:1.3836 exploreP:0.1578\n",
      "Episode:19 Steps:1000 meanR:0.4760 R:0.2000 gloss:0.0450 gloss1-lgP:0.6675 gloss2-gQs:0.0678 gloss3-dQs:0.0677 gloss4-tgtQ:0.0677 dloss:1.3839 exploreP:0.1437\n",
      "Episode:20 Steps:1000 meanR:0.4595 R:0.1300 gloss:0.0434 gloss1-lgP:0.6676 gloss2-gQs:0.0655 gloss3-dQs:0.0654 gloss4-tgtQ:0.0654 dloss:1.3838 exploreP:0.1310\n",
      "Episode:21 Steps:1000 meanR:0.4786 R:0.8800 gloss:28.4048 gloss1-lgP:25.2360 gloss2-gQs:0.4822 gloss3-dQs:0.0806 gloss4-tgtQ:0.4789 dloss:1.0529 exploreP:0.1195\n",
      "Episode:22 Steps:1000 meanR:0.5004 R:0.9800 gloss:3.3123 gloss1-lgP:18.9608 gloss2-gQs:0.1756 gloss3-dQs:0.1725 gloss4-tgtQ:0.1744 dloss:1.3760 exploreP:0.1090\n",
      "Episode:23 Steps:1000 meanR:0.5233 R:1.0500 gloss:2.4125 gloss1-lgP:9.4432 gloss2-gQs:0.3244 gloss3-dQs:0.3127 gloss4-tgtQ:0.3216 dloss:1.3487 exploreP:0.0996\n",
      "Episode:24 Steps:1000 meanR:0.5296 R:0.6800 gloss:0.3523 gloss1-lgP:1.6324 gloss2-gQs:0.2240 gloss3-dQs:0.2222 gloss4-tgtQ:0.2221 dloss:1.3621 exploreP:0.0911\n",
      "Episode:25 Steps:1000 meanR:0.5215 R:0.3200 gloss:0.6945 gloss1-lgP:2.6119 gloss2-gQs:0.3997 gloss3-dQs:0.3869 gloss4-tgtQ:0.3959 dloss:1.2848 exploreP:0.0833\n",
      "Episode:26 Steps:1000 meanR:0.5137 R:0.3100 gloss:-0.1075 gloss1-lgP:1.2686 gloss2-gQs:-0.0525 gloss3-dQs:-0.0475 gloss4-tgtQ:-0.0516 dloss:1.3801 exploreP:0.0764\n",
      "Episode:27 Steps:1000 meanR:0.5143 R:0.5300 gloss:-0.4782 gloss1-lgP:7.2120 gloss2-gQs:-0.0925 gloss3-dQs:-0.0917 gloss4-tgtQ:-0.0911 dloss:1.3698 exploreP:0.0700\n",
      "Episode:28 Steps:1000 meanR:0.5200 R:0.6800 gloss:0.2047 gloss1-lgP:4.9276 gloss2-gQs:0.0341 gloss3-dQs:0.0353 gloss4-tgtQ:0.0342 dloss:1.3794 exploreP:0.0643\n",
      "Episode:29 Steps:1000 meanR:0.5427 R:1.2000 gloss:0.0134 gloss1-lgP:4.8557 gloss2-gQs:0.0089 gloss3-dQs:0.0094 gloss4-tgtQ:0.0094 dloss:1.3848 exploreP:0.0591\n",
      "Episode:30 Steps:1000 meanR:0.5694 R:1.3700 gloss:0.0596 gloss1-lgP:1.9127 gloss2-gQs:0.0301 gloss3-dQs:0.0306 gloss4-tgtQ:0.0305 dloss:1.3838 exploreP:0.0545\n",
      "Episode:31 Steps:1000 meanR:0.6103 R:1.8800 gloss:0.0487 gloss1-lgP:0.6199 gloss2-gQs:0.0791 gloss3-dQs:0.0792 gloss4-tgtQ:0.0794 dloss:1.3797 exploreP:0.0502\n",
      "Episode:32 Steps:1000 meanR:0.6594 R:2.2300 gloss:0.0391 gloss1-lgP:0.4046 gloss2-gQs:0.1068 gloss3-dQs:0.1062 gloss4-tgtQ:0.1067 dloss:1.3800 exploreP:0.0464\n",
      "Episode:33 Steps:1000 meanR:0.6585 R:0.6300 gloss:0.0213 gloss1-lgP:0.3548 gloss2-gQs:0.0637 gloss3-dQs:0.0641 gloss4-tgtQ:0.0639 dloss:1.3837 exploreP:0.0429\n",
      "Episode:34 Steps:1000 meanR:0.6529 R:0.4600 gloss:0.0264 gloss1-lgP:0.3421 gloss2-gQs:0.0827 gloss3-dQs:0.0825 gloss4-tgtQ:0.0827 dloss:1.3830 exploreP:0.0398\n",
      "Episode:35 Steps:1000 meanR:0.6658 R:1.1200 gloss:0.0305 gloss1-lgP:0.4068 gloss2-gQs:0.0925 gloss3-dQs:0.0923 gloss4-tgtQ:0.0925 dloss:1.3818 exploreP:0.0370\n",
      "Episode:36 Steps:1000 meanR:0.6632 R:0.5700 gloss:0.0850 gloss1-lgP:0.6735 gloss2-gQs:0.1158 gloss3-dQs:0.1153 gloss4-tgtQ:0.1155 dloss:1.3783 exploreP:0.0344\n",
      "Episode:37 Steps:1000 meanR:0.6611 R:0.5800 gloss:0.0222 gloss1-lgP:0.3056 gloss2-gQs:0.0751 gloss3-dQs:0.0755 gloss4-tgtQ:0.0754 dloss:1.3839 exploreP:0.0321\n",
      "Episode:38 Steps:1000 meanR:0.6659 R:0.8500 gloss:0.0328 gloss1-lgP:0.3108 gloss2-gQs:0.1102 gloss3-dQs:0.1102 gloss4-tgtQ:0.1102 dloss:1.3817 exploreP:0.0300\n",
      "Episode:39 Steps:1000 meanR:0.6850 R:1.4300 gloss:0.0300 gloss1-lgP:0.3248 gloss2-gQs:0.0965 gloss3-dQs:0.0966 gloss4-tgtQ:0.0966 dloss:1.3829 exploreP:0.0281\n",
      "Episode:40 Steps:1000 meanR:0.6893 R:0.8600 gloss:0.0307 gloss1-lgP:0.3332 gloss2-gQs:0.0961 gloss3-dQs:0.0962 gloss4-tgtQ:0.0961 dloss:1.3828 exploreP:0.0263\n",
      "Episode:41 Steps:1000 meanR:0.6819 R:0.3800 gloss:0.0254 gloss1-lgP:0.3338 gloss2-gQs:0.0791 gloss3-dQs:0.0792 gloss4-tgtQ:0.0792 dloss:1.3837 exploreP:0.0248\n",
      "Episode:42 Steps:1000 meanR:0.6847 R:0.8000 gloss:0.0206 gloss1-lgP:0.3509 gloss2-gQs:0.0613 gloss3-dQs:0.0614 gloss4-tgtQ:0.0613 dloss:1.3843 exploreP:0.0234\n",
      "Episode:43 Steps:1000 meanR:0.6852 R:0.7100 gloss:0.0209 gloss1-lgP:0.3402 gloss2-gQs:0.0612 gloss3-dQs:0.0612 gloss4-tgtQ:0.0612 dloss:1.3847 exploreP:0.0221\n",
      "Episode:44 Steps:1000 meanR:0.6796 R:0.4300 gloss:0.0248 gloss1-lgP:0.3507 gloss2-gQs:0.0727 gloss3-dQs:0.0726 gloss4-tgtQ:0.0727 dloss:1.3842 exploreP:0.0209\n",
      "Episode:45 Steps:1000 meanR:0.6724 R:0.3500 gloss:0.0245 gloss1-lgP:0.3331 gloss2-gQs:0.0749 gloss3-dQs:0.0748 gloss4-tgtQ:0.0748 dloss:1.3842 exploreP:0.0199\n",
      "Episode:46 Steps:1000 meanR:0.6717 R:0.6400 gloss:0.0223 gloss1-lgP:0.3299 gloss2-gQs:0.0689 gloss3-dQs:0.0688 gloss4-tgtQ:0.0688 dloss:1.3846 exploreP:0.0190\n",
      "Episode:47 Steps:1000 meanR:0.6835 R:1.2400 gloss:0.0194 gloss1-lgP:0.3363 gloss2-gQs:0.0578 gloss3-dQs:0.0577 gloss4-tgtQ:0.0577 dloss:1.3849 exploreP:0.0181\n",
      "Episode:48 Steps:1000 meanR:0.6845 R:0.7300 gloss:0.0249 gloss1-lgP:0.3388 gloss2-gQs:0.0741 gloss3-dQs:0.0739 gloss4-tgtQ:0.0740 dloss:1.3844 exploreP:0.0173\n",
      "Episode:49 Steps:1000 meanR:0.6968 R:1.3000 gloss:0.0233 gloss1-lgP:0.3429 gloss2-gQs:0.0679 gloss3-dQs:0.0678 gloss4-tgtQ:0.0679 dloss:1.3844 exploreP:0.0166\n",
      "Episode:50 Steps:1000 meanR:0.6892 R:0.3100 gloss:0.0217 gloss1-lgP:0.3477 gloss2-gQs:0.0616 gloss3-dQs:0.0615 gloss4-tgtQ:0.0615 dloss:1.3848 exploreP:0.0160\n",
      "Episode:51 Steps:1000 meanR:0.6944 R:0.9600 gloss:0.0232 gloss1-lgP:0.3539 gloss2-gQs:0.0650 gloss3-dQs:0.0650 gloss4-tgtQ:0.0650 dloss:1.3846 exploreP:0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:52 Steps:1000 meanR:0.6923 R:0.5800 gloss:0.0226 gloss1-lgP:0.3516 gloss2-gQs:0.0646 gloss3-dQs:0.0645 gloss4-tgtQ:0.0645 dloss:1.3847 exploreP:0.0149\n",
      "Episode:53 Steps:1000 meanR:0.7157 R:1.9600 gloss:0.0263 gloss1-lgP:0.3499 gloss2-gQs:0.0762 gloss3-dQs:0.0761 gloss4-tgtQ:0.0762 dloss:1.3841 exploreP:0.0144\n",
      "Episode:54 Steps:1000 meanR:0.7067 R:0.2200 gloss:0.0252 gloss1-lgP:0.3498 gloss2-gQs:0.0730 gloss3-dQs:0.0729 gloss4-tgtQ:0.0730 dloss:1.3843 exploreP:0.0140\n",
      "Episode:55 Steps:1000 meanR:0.7136 R:1.0900 gloss:0.0228 gloss1-lgP:0.3555 gloss2-gQs:0.0654 gloss3-dQs:0.0653 gloss4-tgtQ:0.0653 dloss:1.3845 exploreP:0.0136\n",
      "Episode:56 Steps:1000 meanR:0.7161 R:0.8600 gloss:0.0281 gloss1-lgP:0.3609 gloss2-gQs:0.0790 gloss3-dQs:0.0789 gloss4-tgtQ:0.0790 dloss:1.3839 exploreP:0.0133\n",
      "Episode:57 Steps:1000 meanR:0.7150 R:0.6500 gloss:0.0307 gloss1-lgP:0.3613 gloss2-gQs:0.0862 gloss3-dQs:0.0861 gloss4-tgtQ:0.0861 dloss:1.3836 exploreP:0.0130\n",
      "Episode:58 Steps:1000 meanR:0.7239 R:1.2400 gloss:0.0277 gloss1-lgP:0.3598 gloss2-gQs:0.0782 gloss3-dQs:0.0781 gloss4-tgtQ:0.0781 dloss:1.3839 exploreP:0.0127\n",
      "Episode:59 Steps:1000 meanR:0.7290 R:1.0300 gloss:0.0295 gloss1-lgP:0.3598 gloss2-gQs:0.0825 gloss3-dQs:0.0825 gloss4-tgtQ:0.0825 dloss:1.3839 exploreP:0.0124\n",
      "Episode:60 Steps:1000 meanR:0.7285 R:0.7000 gloss:0.0267 gloss1-lgP:0.3595 gloss2-gQs:0.0749 gloss3-dQs:0.0750 gloss4-tgtQ:0.0750 dloss:1.3842 exploreP:0.0122\n",
      "Episode:61 Steps:1000 meanR:0.7437 R:1.6700 gloss:0.0310 gloss1-lgP:0.3579 gloss2-gQs:0.0869 gloss3-dQs:0.0869 gloss4-tgtQ:0.0869 dloss:1.3836 exploreP:0.0120\n",
      "Episode:62 Steps:1000 meanR:0.7459 R:0.8800 gloss:0.0318 gloss1-lgP:0.3627 gloss2-gQs:0.0878 gloss3-dQs:0.0879 gloss4-tgtQ:0.0878 dloss:1.3837 exploreP:0.0118\n",
      "Episode:63 Steps:1000 meanR:0.7355 R:0.0800 gloss:0.0292 gloss1-lgP:0.3722 gloss2-gQs:0.0789 gloss3-dQs:0.0790 gloss4-tgtQ:0.0790 dloss:1.3840 exploreP:0.0116\n",
      "Episode:64 Steps:1000 meanR:0.7328 R:0.5600 gloss:0.0260 gloss1-lgP:0.3774 gloss2-gQs:0.0692 gloss3-dQs:0.0692 gloss4-tgtQ:0.0692 dloss:1.3844 exploreP:0.0115\n",
      "Episode:65 Steps:1000 meanR:0.7309 R:0.6100 gloss:0.0290 gloss1-lgP:0.3789 gloss2-gQs:0.0773 gloss3-dQs:0.0774 gloss4-tgtQ:0.0774 dloss:1.3841 exploreP:0.0113\n",
      "Episode:66 Steps:1000 meanR:0.7282 R:0.5500 gloss:0.0256 gloss1-lgP:0.3788 gloss2-gQs:0.0678 gloss3-dQs:0.0679 gloss4-tgtQ:0.0678 dloss:1.3846 exploreP:0.0112\n",
      "Episode:67 Steps:1000 meanR:0.7372 R:1.3400 gloss:0.0274 gloss1-lgP:0.3832 gloss2-gQs:0.0722 gloss3-dQs:0.0723 gloss4-tgtQ:0.0723 dloss:1.3845 exploreP:0.0111\n",
      "Episode:68 Steps:1000 meanR:0.7430 R:1.1400 gloss:0.0284 gloss1-lgP:0.3867 gloss2-gQs:0.0740 gloss3-dQs:0.0740 gloss4-tgtQ:0.0740 dloss:1.3843 exploreP:0.0110\n",
      "Episode:69 Steps:1000 meanR:0.7419 R:0.6600 gloss:0.0260 gloss1-lgP:0.3884 gloss2-gQs:0.0683 gloss3-dQs:0.0682 gloss4-tgtQ:0.0683 dloss:1.3844 exploreP:0.0109\n",
      "Episode:70 Steps:1000 meanR:0.7439 R:0.8900 gloss:0.0283 gloss1-lgP:0.3901 gloss2-gQs:0.0738 gloss3-dQs:0.0738 gloss4-tgtQ:0.0738 dloss:1.3843 exploreP:0.0108\n",
      "Episode:71 Steps:1000 meanR:0.7414 R:0.5600 gloss:0.0283 gloss1-lgP:0.3941 gloss2-gQs:0.0725 gloss3-dQs:0.0725 gloss4-tgtQ:0.0725 dloss:1.3843 exploreP:0.0107\n",
      "Episode:72 Steps:1000 meanR:0.7363 R:0.3700 gloss:0.0257 gloss1-lgP:0.3944 gloss2-gQs:0.0654 gloss3-dQs:0.0653 gloss4-tgtQ:0.0654 dloss:1.3846 exploreP:0.0107\n",
      "Episode:73 Steps:1000 meanR:0.7396 R:0.9800 gloss:0.0258 gloss1-lgP:0.3887 gloss2-gQs:0.0668 gloss3-dQs:0.0668 gloss4-tgtQ:0.0668 dloss:1.3844 exploreP:0.0106\n",
      "Episode:74 Steps:1000 meanR:0.7408 R:0.8300 gloss:0.0257 gloss1-lgP:0.3895 gloss2-gQs:0.0669 gloss3-dQs:0.0669 gloss4-tgtQ:0.0669 dloss:1.3844 exploreP:0.0105\n",
      "Episode:75 Steps:1000 meanR:0.7358 R:0.3600 gloss:0.0277 gloss1-lgP:0.3897 gloss2-gQs:0.0714 gloss3-dQs:0.0714 gloss4-tgtQ:0.0714 dloss:1.3842 exploreP:0.0105\n",
      "Episode:76 Steps:1000 meanR:0.7439 R:1.3600 gloss:0.0284 gloss1-lgP:0.3918 gloss2-gQs:0.0725 gloss3-dQs:0.0725 gloss4-tgtQ:0.0725 dloss:1.3842 exploreP:0.0104\n",
      "Episode:77 Steps:1000 meanR:0.7438 R:0.7400 gloss:0.0325 gloss1-lgP:0.3945 gloss2-gQs:0.0825 gloss3-dQs:0.0824 gloss4-tgtQ:0.0825 dloss:1.3838 exploreP:0.0104\n",
      "Episode:78 Steps:1000 meanR:0.7527 R:1.4400 gloss:0.0285 gloss1-lgP:0.3963 gloss2-gQs:0.0726 gloss3-dQs:0.0726 gloss4-tgtQ:0.0726 dloss:1.3844 exploreP:0.0104\n",
      "Episode:79 Steps:1000 meanR:0.7510 R:0.6200 gloss:0.0298 gloss1-lgP:0.3996 gloss2-gQs:0.0755 gloss3-dQs:0.0755 gloss4-tgtQ:0.0755 dloss:1.3842 exploreP:0.0103\n",
      "Episode:80 Steps:1000 meanR:0.7431 R:0.1100 gloss:0.0294 gloss1-lgP:0.4033 gloss2-gQs:0.0740 gloss3-dQs:0.0740 gloss4-tgtQ:0.0740 dloss:1.3842 exploreP:0.0103\n",
      "Episode:81 Steps:1000 meanR:0.7405 R:0.5300 gloss:0.0280 gloss1-lgP:0.4051 gloss2-gQs:0.0699 gloss3-dQs:0.0699 gloss4-tgtQ:0.0699 dloss:1.3843 exploreP:0.0103\n",
      "Episode:82 Steps:1000 meanR:0.7353 R:0.3100 gloss:0.0267 gloss1-lgP:0.4055 gloss2-gQs:0.0668 gloss3-dQs:0.0668 gloss4-tgtQ:0.0668 dloss:1.3845 exploreP:0.0102\n",
      "Episode:83 Steps:1000 meanR:0.7346 R:0.6800 gloss:0.0255 gloss1-lgP:0.4045 gloss2-gQs:0.0635 gloss3-dQs:0.0635 gloss4-tgtQ:0.0635 dloss:1.3846 exploreP:0.0102\n",
      "Episode:84 Steps:1000 meanR:0.7378 R:1.0000 gloss:0.0246 gloss1-lgP:0.4039 gloss2-gQs:0.0614 gloss3-dQs:0.0614 gloss4-tgtQ:0.0614 dloss:1.3847 exploreP:0.0102\n",
      "Episode:85 Steps:1000 meanR:0.7497 R:1.7600 gloss:0.0305 gloss1-lgP:0.4053 gloss2-gQs:0.0756 gloss3-dQs:0.0756 gloss4-tgtQ:0.0756 dloss:1.3839 exploreP:0.0102\n",
      "Episode:86 Steps:1000 meanR:0.7532 R:1.0600 gloss:0.0307 gloss1-lgP:0.4074 gloss2-gQs:0.0760 gloss3-dQs:0.0759 gloss4-tgtQ:0.0760 dloss:1.3840 exploreP:0.0102\n",
      "Episode:87 Steps:1000 meanR:0.7607 R:1.4100 gloss:0.0292 gloss1-lgP:0.4034 gloss2-gQs:0.0728 gloss3-dQs:0.0729 gloss4-tgtQ:0.0729 dloss:1.3838 exploreP:0.0101\n",
      "Episode:88 Steps:1000 meanR:0.7576 R:0.4900 gloss:0.0294 gloss1-lgP:0.4048 gloss2-gQs:0.0726 gloss3-dQs:0.0726 gloss4-tgtQ:0.0726 dloss:1.3842 exploreP:0.0101\n",
      "Episode:89 Steps:1000 meanR:0.7620 R:1.1500 gloss:0.0253 gloss1-lgP:0.4051 gloss2-gQs:0.0625 gloss3-dQs:0.0625 gloss4-tgtQ:0.0625 dloss:1.3849 exploreP:0.0101\n",
      "Episode:90 Steps:1000 meanR:0.7598 R:0.5600 gloss:0.0305 gloss1-lgP:0.4045 gloss2-gQs:0.0751 gloss3-dQs:0.0751 gloss4-tgtQ:0.0752 dloss:1.3843 exploreP:0.0101\n",
      "Episode:91 Steps:1000 meanR:0.7733 R:2.0000 gloss:0.0341 gloss1-lgP:0.4022 gloss2-gQs:0.0839 gloss3-dQs:0.0839 gloss4-tgtQ:0.0839 dloss:1.3838 exploreP:0.0101\n",
      "Episode:92 Steps:1000 meanR:0.7765 R:1.0700 gloss:0.0365 gloss1-lgP:0.3994 gloss2-gQs:0.0899 gloss3-dQs:0.0898 gloss4-tgtQ:0.0899 dloss:1.3834 exploreP:0.0101\n",
      "Episode:93 Steps:1000 meanR:0.7721 R:0.3700 gloss:0.0391 gloss1-lgP:0.4022 gloss2-gQs:0.0961 gloss3-dQs:0.0961 gloss4-tgtQ:0.0961 dloss:1.3831 exploreP:0.0101\n",
      "Episode:94 Steps:1000 meanR:0.7841 R:1.9100 gloss:0.0394 gloss1-lgP:0.4066 gloss2-gQs:0.0956 gloss3-dQs:0.0955 gloss4-tgtQ:0.0956 dloss:1.3832 exploreP:0.0101\n",
      "Episode:95 Steps:1000 meanR:0.7924 R:1.5800 gloss:0.0400 gloss1-lgP:0.4059 gloss2-gQs:0.0970 gloss3-dQs:0.0970 gloss4-tgtQ:0.0970 dloss:1.3831 exploreP:0.0101\n",
      "Episode:96 Steps:1000 meanR:0.7926 R:0.8100 gloss:0.0390 gloss1-lgP:0.4061 gloss2-gQs:0.0953 gloss3-dQs:0.0953 gloss4-tgtQ:0.0953 dloss:1.3833 exploreP:0.0101\n",
      "Episode:97 Steps:1000 meanR:0.7946 R:0.9900 gloss:0.0389 gloss1-lgP:0.4110 gloss2-gQs:0.0942 gloss3-dQs:0.0942 gloss4-tgtQ:0.0942 dloss:1.3835 exploreP:0.0101\n",
      "Episode:98 Steps:1000 meanR:0.8015 R:1.4800 gloss:0.0407 gloss1-lgP:0.4111 gloss2-gQs:0.0986 gloss3-dQs:0.0986 gloss4-tgtQ:0.0986 dloss:1.3832 exploreP:0.0100\n",
      "Episode:99 Steps:1000 meanR:0.8063 R:1.2800 gloss:0.0462 gloss1-lgP:0.4087 gloss2-gQs:0.1133 gloss3-dQs:0.1133 gloss4-tgtQ:0.1133 dloss:1.3822 exploreP:0.0100\n",
      "Episode:100 Steps:1000 meanR:0.8132 R:0.6900 gloss:0.0438 gloss1-lgP:0.4068 gloss2-gQs:0.1077 gloss3-dQs:0.1077 gloss4-tgtQ:0.1077 dloss:1.3827 exploreP:0.0100\n",
      "Episode:101 Steps:1000 meanR:0.8232 R:1.0000 gloss:0.0392 gloss1-lgP:0.4059 gloss2-gQs:0.0975 gloss3-dQs:0.0975 gloss4-tgtQ:0.0975 dloss:1.3833 exploreP:0.0100\n",
      "Episode:102 Steps:1000 meanR:0.8326 R:1.3000 gloss:0.0423 gloss1-lgP:0.4110 gloss2-gQs:0.1034 gloss3-dQs:0.1034 gloss4-tgtQ:0.1034 dloss:1.3830 exploreP:0.0100\n",
      "Episode:103 Steps:1000 meanR:0.8377 R:0.9900 gloss:0.0422 gloss1-lgP:0.4113 gloss2-gQs:0.1030 gloss3-dQs:0.1030 gloss4-tgtQ:0.1030 dloss:1.3830 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:104 Steps:1000 meanR:0.8481 R:1.3700 gloss:0.0406 gloss1-lgP:0.4126 gloss2-gQs:0.0990 gloss3-dQs:0.0989 gloss4-tgtQ:0.0990 dloss:1.3831 exploreP:0.0100\n",
      "Episode:105 Steps:1000 meanR:0.8480 R:0.6100 gloss:0.0418 gloss1-lgP:0.4141 gloss2-gQs:0.1006 gloss3-dQs:0.1005 gloss4-tgtQ:0.1005 dloss:1.3829 exploreP:0.0100\n",
      "Episode:106 Steps:1000 meanR:0.8524 R:0.4400 gloss:0.0390 gloss1-lgP:0.4146 gloss2-gQs:0.0940 gloss3-dQs:0.0940 gloss4-tgtQ:0.0940 dloss:1.3834 exploreP:0.0100\n",
      "Episode:107 Steps:1000 meanR:0.8610 R:0.9000 gloss:0.0403 gloss1-lgP:0.4125 gloss2-gQs:0.0980 gloss3-dQs:0.0979 gloss4-tgtQ:0.0980 dloss:1.3830 exploreP:0.0100\n",
      "Episode:108 Steps:1000 meanR:0.8588 R:0.3500 gloss:0.0368 gloss1-lgP:0.4146 gloss2-gQs:0.0882 gloss3-dQs:0.0882 gloss4-tgtQ:0.0882 dloss:1.3834 exploreP:0.0100\n",
      "Episode:109 Steps:1000 meanR:0.8592 R:0.7100 gloss:0.0318 gloss1-lgP:0.4207 gloss2-gQs:0.0752 gloss3-dQs:0.0752 gloss4-tgtQ:0.0752 dloss:1.3841 exploreP:0.0100\n",
      "Episode:110 Steps:1000 meanR:0.8704 R:1.1200 gloss:0.0315 gloss1-lgP:0.4230 gloss2-gQs:0.0744 gloss3-dQs:0.0743 gloss4-tgtQ:0.0744 dloss:1.3840 exploreP:0.0100\n",
      "Episode:111 Steps:1000 meanR:0.8700 R:0.5400 gloss:0.0342 gloss1-lgP:0.4281 gloss2-gQs:0.0796 gloss3-dQs:0.0796 gloss4-tgtQ:0.0796 dloss:1.3838 exploreP:0.0100\n",
      "Episode:112 Steps:1000 meanR:0.8689 R:0.7700 gloss:0.0312 gloss1-lgP:0.4298 gloss2-gQs:0.0722 gloss3-dQs:0.0722 gloss4-tgtQ:0.0722 dloss:1.3842 exploreP:0.0100\n",
      "Episode:113 Steps:1000 meanR:0.8664 R:0.7500 gloss:0.0293 gloss1-lgP:0.4287 gloss2-gQs:0.0679 gloss3-dQs:0.0678 gloss4-tgtQ:0.0679 dloss:1.3845 exploreP:0.0100\n",
      "Episode:114 Steps:1000 meanR:0.8740 R:2.1300 gloss:0.0301 gloss1-lgP:0.4284 gloss2-gQs:0.0698 gloss3-dQs:0.0698 gloss4-tgtQ:0.0698 dloss:1.3845 exploreP:0.0100\n",
      "Episode:115 Steps:1000 meanR:0.8759 R:0.4600 gloss:0.0322 gloss1-lgP:0.4305 gloss2-gQs:0.0742 gloss3-dQs:0.0742 gloss4-tgtQ:0.0743 dloss:1.3845 exploreP:0.0100\n",
      "Episode:116 Steps:1000 meanR:0.8679 R:0.9300 gloss:0.0346 gloss1-lgP:0.4319 gloss2-gQs:0.0799 gloss3-dQs:0.0799 gloss4-tgtQ:0.0799 dloss:1.3842 exploreP:0.0100\n",
      "Episode:117 Steps:1000 meanR:0.8750 R:1.1300 gloss:0.0293 gloss1-lgP:0.4330 gloss2-gQs:0.0679 gloss3-dQs:0.0679 gloss4-tgtQ:0.0679 dloss:1.3847 exploreP:0.0100\n",
      "Episode:118 Steps:1000 meanR:0.8890 R:1.4000 gloss:0.0362 gloss1-lgP:0.4343 gloss2-gQs:0.0835 gloss3-dQs:0.0835 gloss4-tgtQ:0.0835 dloss:1.3841 exploreP:0.0100\n",
      "Episode:119 Steps:1000 meanR:0.8956 R:0.8600 gloss:0.0374 gloss1-lgP:0.4342 gloss2-gQs:0.0860 gloss3-dQs:0.0860 gloss4-tgtQ:0.0860 dloss:1.3840 exploreP:0.0100\n",
      "Episode:120 Steps:1000 meanR:0.9010 R:0.6700 gloss:0.0384 gloss1-lgP:0.4372 gloss2-gQs:0.0877 gloss3-dQs:0.0876 gloss4-tgtQ:0.0877 dloss:1.3838 exploreP:0.0100\n",
      "Episode:121 Steps:1000 meanR:0.8988 R:0.6600 gloss:0.0397 gloss1-lgP:0.4396 gloss2-gQs:0.0903 gloss3-dQs:0.0903 gloss4-tgtQ:0.0903 dloss:1.3837 exploreP:0.0100\n",
      "Episode:122 Steps:1000 meanR:0.9018 R:1.2800 gloss:0.0403 gloss1-lgP:0.4400 gloss2-gQs:0.0911 gloss3-dQs:0.0911 gloss4-tgtQ:0.0911 dloss:1.3835 exploreP:0.0100\n",
      "Episode:123 Steps:1000 meanR:0.9050 R:1.3700 gloss:0.0414 gloss1-lgP:0.4435 gloss2-gQs:0.0928 gloss3-dQs:0.0928 gloss4-tgtQ:0.0928 dloss:1.3834 exploreP:0.0100\n",
      "Episode:124 Steps:1000 meanR:0.9072 R:0.9000 gloss:0.0411 gloss1-lgP:0.4460 gloss2-gQs:0.0917 gloss3-dQs:0.0917 gloss4-tgtQ:0.0917 dloss:1.3835 exploreP:0.0100\n",
      "Episode:125 Steps:1000 meanR:0.9111 R:0.7100 gloss:0.0386 gloss1-lgP:0.4463 gloss2-gQs:0.0860 gloss3-dQs:0.0860 gloss4-tgtQ:0.0861 dloss:1.3837 exploreP:0.0100\n",
      "Episode:126 Steps:1000 meanR:0.9171 R:0.9100 gloss:0.0403 gloss1-lgP:0.4502 gloss2-gQs:0.0893 gloss3-dQs:0.0893 gloss4-tgtQ:0.0893 dloss:1.3836 exploreP:0.0100\n",
      "Episode:127 Steps:1000 meanR:0.9224 R:1.0600 gloss:0.0406 gloss1-lgP:0.4515 gloss2-gQs:0.0902 gloss3-dQs:0.0902 gloss4-tgtQ:0.0902 dloss:1.3834 exploreP:0.0100\n",
      "Episode:128 Steps:1000 meanR:0.9248 R:0.9200 gloss:0.0384 gloss1-lgP:0.4501 gloss2-gQs:0.0855 gloss3-dQs:0.0855 gloss4-tgtQ:0.0855 dloss:1.3837 exploreP:0.0100\n",
      "Episode:129 Steps:1000 meanR:0.9225 R:0.9700 gloss:0.0399 gloss1-lgP:0.4491 gloss2-gQs:0.0887 gloss3-dQs:0.0887 gloss4-tgtQ:0.0887 dloss:1.3832 exploreP:0.0100\n",
      "Episode:130 Steps:1000 meanR:0.9144 R:0.5600 gloss:0.0392 gloss1-lgP:0.4468 gloss2-gQs:0.0873 gloss3-dQs:0.0872 gloss4-tgtQ:0.0873 dloss:1.3834 exploreP:0.0100\n",
      "Episode:131 Steps:1000 meanR:0.9037 R:0.8100 gloss:0.0396 gloss1-lgP:0.4447 gloss2-gQs:0.0882 gloss3-dQs:0.0882 gloss4-tgtQ:0.0882 dloss:1.3835 exploreP:0.0100\n",
      "Episode:132 Steps:1000 meanR:0.8825 R:0.1100 gloss:0.0383 gloss1-lgP:0.4490 gloss2-gQs:0.0849 gloss3-dQs:0.0849 gloss4-tgtQ:0.0849 dloss:1.3838 exploreP:0.0100\n",
      "Episode:133 Steps:1000 meanR:0.8811 R:0.4900 gloss:0.0318 gloss1-lgP:0.4502 gloss2-gQs:0.0704 gloss3-dQs:0.0705 gloss4-tgtQ:0.0704 dloss:1.3843 exploreP:0.0100\n",
      "Episode:134 Steps:1000 meanR:0.8870 R:1.0500 gloss:0.0296 gloss1-lgP:0.4535 gloss2-gQs:0.0650 gloss3-dQs:0.0650 gloss4-tgtQ:0.0650 dloss:1.3846 exploreP:0.0100\n",
      "Episode:135 Steps:1000 meanR:0.8778 R:0.2000 gloss:0.0299 gloss1-lgP:0.4536 gloss2-gQs:0.0657 gloss3-dQs:0.0657 gloss4-tgtQ:0.0658 dloss:1.3844 exploreP:0.0100\n",
      "Episode:136 Steps:1000 meanR:0.8786 R:0.6500 gloss:0.0303 gloss1-lgP:0.4514 gloss2-gQs:0.0664 gloss3-dQs:0.0664 gloss4-tgtQ:0.0664 dloss:1.3843 exploreP:0.0100\n",
      "Episode:137 Steps:1000 meanR:0.8763 R:0.3500 gloss:0.0260 gloss1-lgP:0.4498 gloss2-gQs:0.0571 gloss3-dQs:0.0571 gloss4-tgtQ:0.0571 dloss:1.3850 exploreP:0.0100\n",
      "Episode:138 Steps:1000 meanR:0.8831 R:1.5300 gloss:0.0270 gloss1-lgP:0.4560 gloss2-gQs:0.0579 gloss3-dQs:0.0579 gloss4-tgtQ:0.0579 dloss:1.3850 exploreP:0.0100\n",
      "Episode:139 Steps:1000 meanR:0.8816 R:1.2800 gloss:0.0317 gloss1-lgP:0.4593 gloss2-gQs:0.0674 gloss3-dQs:0.0674 gloss4-tgtQ:0.0674 dloss:1.3844 exploreP:0.0100\n",
      "Episode:140 Steps:1000 meanR:0.8894 R:1.6400 gloss:0.0297 gloss1-lgP:0.4625 gloss2-gQs:0.0626 gloss3-dQs:0.0626 gloss4-tgtQ:0.0626 dloss:1.3846 exploreP:0.0100\n",
      "Episode:141 Steps:1000 meanR:0.8977 R:1.2100 gloss:0.0327 gloss1-lgP:0.4649 gloss2-gQs:0.0692 gloss3-dQs:0.0692 gloss4-tgtQ:0.0692 dloss:1.3844 exploreP:0.0100\n",
      "Episode:142 Steps:1000 meanR:0.9073 R:1.7600 gloss:0.0351 gloss1-lgP:0.4634 gloss2-gQs:0.0744 gloss3-dQs:0.0744 gloss4-tgtQ:0.0744 dloss:1.3841 exploreP:0.0100\n",
      "Episode:143 Steps:1000 meanR:0.9019 R:0.1700 gloss:0.0430 gloss1-lgP:0.4608 gloss2-gQs:0.0923 gloss3-dQs:0.0924 gloss4-tgtQ:0.0924 dloss:1.3830 exploreP:0.0100\n",
      "Episode:144 Steps:1000 meanR:0.9034 R:0.5800 gloss:0.0420 gloss1-lgP:0.4601 gloss2-gQs:0.0899 gloss3-dQs:0.0900 gloss4-tgtQ:0.0900 dloss:1.3834 exploreP:0.0100\n",
      "Episode:145 Steps:1000 meanR:0.9113 R:1.1400 gloss:0.0435 gloss1-lgP:0.4605 gloss2-gQs:0.0931 gloss3-dQs:0.0931 gloss4-tgtQ:0.0931 dloss:1.3832 exploreP:0.0100\n",
      "Episode:146 Steps:1000 meanR:0.9154 R:1.0500 gloss:0.0457 gloss1-lgP:0.4619 gloss2-gQs:0.0980 gloss3-dQs:0.0980 gloss4-tgtQ:0.0980 dloss:1.3829 exploreP:0.0100\n",
      "Episode:147 Steps:1000 meanR:0.9081 R:0.5100 gloss:0.0469 gloss1-lgP:0.4608 gloss2-gQs:0.1007 gloss3-dQs:0.1007 gloss4-tgtQ:0.1007 dloss:1.3826 exploreP:0.0100\n",
      "Episode:148 Steps:1000 meanR:0.9091 R:0.8300 gloss:0.0407 gloss1-lgP:0.4609 gloss2-gQs:0.0885 gloss3-dQs:0.0885 gloss4-tgtQ:0.0885 dloss:1.3833 exploreP:0.0100\n",
      "Episode:149 Steps:1000 meanR:0.9042 R:0.8100 gloss:0.0408 gloss1-lgP:0.4601 gloss2-gQs:0.0893 gloss3-dQs:0.0893 gloss4-tgtQ:0.0893 dloss:1.3834 exploreP:0.0100\n",
      "Episode:150 Steps:1000 meanR:0.9135 R:1.2400 gloss:0.0379 gloss1-lgP:0.4600 gloss2-gQs:0.0830 gloss3-dQs:0.0829 gloss4-tgtQ:0.0830 dloss:1.3837 exploreP:0.0100\n",
      "Episode:151 Steps:1000 meanR:0.9047 R:0.0800 gloss:0.0387 gloss1-lgP:0.4627 gloss2-gQs:0.0839 gloss3-dQs:0.0839 gloss4-tgtQ:0.0839 dloss:1.3837 exploreP:0.0100\n",
      "Episode:152 Steps:1000 meanR:0.9089 R:1.0000 gloss:0.0370 gloss1-lgP:0.4672 gloss2-gQs:0.0793 gloss3-dQs:0.0793 gloss4-tgtQ:0.0793 dloss:1.3840 exploreP:0.0100\n",
      "Episode:153 Steps:1000 meanR:0.8962 R:0.6900 gloss:0.0338 gloss1-lgP:0.4700 gloss2-gQs:0.0719 gloss3-dQs:0.0718 gloss4-tgtQ:0.0718 dloss:1.3845 exploreP:0.0100\n",
      "Episode:154 Steps:1000 meanR:0.8975 R:0.3500 gloss:0.0353 gloss1-lgP:0.4714 gloss2-gQs:0.0750 gloss3-dQs:0.0750 gloss4-tgtQ:0.0750 dloss:1.3843 exploreP:0.0100\n",
      "Episode:155 Steps:1000 meanR:0.8991 R:1.2500 gloss:0.0354 gloss1-lgP:0.4709 gloss2-gQs:0.0752 gloss3-dQs:0.0753 gloss4-tgtQ:0.0753 dloss:1.3843 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:156 Steps:1000 meanR:0.8986 R:0.8100 gloss:0.0351 gloss1-lgP:0.4732 gloss2-gQs:0.0739 gloss3-dQs:0.0739 gloss4-tgtQ:0.0739 dloss:1.3844 exploreP:0.0100\n",
      "Episode:157 Steps:1000 meanR:0.9018 R:0.9700 gloss:0.0336 gloss1-lgP:0.4767 gloss2-gQs:0.0699 gloss3-dQs:0.0699 gloss4-tgtQ:0.0699 dloss:1.3845 exploreP:0.0100\n",
      "Episode:158 Steps:1000 meanR:0.9073 R:1.7900 gloss:0.0377 gloss1-lgP:0.4763 gloss2-gQs:0.0787 gloss3-dQs:0.0787 gloss4-tgtQ:0.0787 dloss:1.3843 exploreP:0.0100\n",
      "Episode:159 Steps:1000 meanR:0.9144 R:1.7400 gloss:0.0364 gloss1-lgP:0.4766 gloss2-gQs:0.0761 gloss3-dQs:0.0761 gloss4-tgtQ:0.0762 dloss:1.3843 exploreP:0.0100\n",
      "Episode:160 Steps:1000 meanR:0.9145 R:0.7100 gloss:0.0402 gloss1-lgP:0.4788 gloss2-gQs:0.0840 gloss3-dQs:0.0840 gloss4-tgtQ:0.0840 dloss:1.3836 exploreP:0.0100\n",
      "Episode:161 Steps:1000 meanR:0.9096 R:1.1800 gloss:0.0400 gloss1-lgP:0.4764 gloss2-gQs:0.0836 gloss3-dQs:0.0836 gloss4-tgtQ:0.0836 dloss:1.3839 exploreP:0.0100\n",
      "Episode:162 Steps:1000 meanR:0.9144 R:1.3600 gloss:0.0425 gloss1-lgP:0.4709 gloss2-gQs:0.0903 gloss3-dQs:0.0903 gloss4-tgtQ:0.0903 dloss:1.3834 exploreP:0.0100\n",
      "Episode:163 Steps:1000 meanR:0.9164 R:0.2800 gloss:0.0403 gloss1-lgP:0.4723 gloss2-gQs:0.0851 gloss3-dQs:0.0851 gloss4-tgtQ:0.0851 dloss:1.3836 exploreP:0.0100\n",
      "Episode:164 Steps:1000 meanR:0.9187 R:0.7900 gloss:0.0465 gloss1-lgP:0.4758 gloss2-gQs:0.0974 gloss3-dQs:0.0974 gloss4-tgtQ:0.0975 dloss:1.3830 exploreP:0.0100\n",
      "Episode:165 Steps:1000 meanR:0.9256 R:1.3000 gloss:0.0449 gloss1-lgP:0.4767 gloss2-gQs:0.0940 gloss3-dQs:0.0940 gloss4-tgtQ:0.0940 dloss:1.3831 exploreP:0.0100\n",
      "Episode:166 Steps:1000 meanR:0.9241 R:0.4000 gloss:0.0435 gloss1-lgP:0.4759 gloss2-gQs:0.0912 gloss3-dQs:0.0912 gloss4-tgtQ:0.0913 dloss:1.3831 exploreP:0.0100\n",
      "Episode:167 Steps:1000 meanR:0.9217 R:1.1000 gloss:0.0476 gloss1-lgP:0.4755 gloss2-gQs:0.0997 gloss3-dQs:0.0998 gloss4-tgtQ:0.0998 dloss:1.3825 exploreP:0.0100\n",
      "Episode:168 Steps:1000 meanR:0.9156 R:0.5300 gloss:0.0408 gloss1-lgP:0.4750 gloss2-gQs:0.0859 gloss3-dQs:0.0860 gloss4-tgtQ:0.0859 dloss:1.3835 exploreP:0.0100\n",
      "Episode:169 Steps:1000 meanR:0.9212 R:1.2200 gloss:0.0424 gloss1-lgP:0.4759 gloss2-gQs:0.0887 gloss3-dQs:0.0887 gloss4-tgtQ:0.0887 dloss:1.3834 exploreP:0.0100\n",
      "Episode:170 Steps:1000 meanR:0.9148 R:0.2500 gloss:0.0376 gloss1-lgP:0.4787 gloss2-gQs:0.0781 gloss3-dQs:0.0781 gloss4-tgtQ:0.0781 dloss:1.3839 exploreP:0.0100\n",
      "Episode:171 Steps:1000 meanR:0.9143 R:0.5100 gloss:0.0348 gloss1-lgP:0.4838 gloss2-gQs:0.0721 gloss3-dQs:0.0721 gloss4-tgtQ:0.0721 dloss:1.3840 exploreP:0.0100\n",
      "Episode:172 Steps:1000 meanR:0.9170 R:0.6400 gloss:0.0325 gloss1-lgP:0.4868 gloss2-gQs:0.0669 gloss3-dQs:0.0670 gloss4-tgtQ:0.0669 dloss:1.3843 exploreP:0.0100\n",
      "Episode:173 Steps:1000 meanR:0.9157 R:0.8500 gloss:0.0314 gloss1-lgP:0.4877 gloss2-gQs:0.0644 gloss3-dQs:0.0644 gloss4-tgtQ:0.0644 dloss:1.3845 exploreP:0.0100\n",
      "Episode:174 Steps:1000 meanR:0.9214 R:1.4000 gloss:0.0344 gloss1-lgP:0.4868 gloss2-gQs:0.0708 gloss3-dQs:0.0708 gloss4-tgtQ:0.0708 dloss:1.3843 exploreP:0.0100\n",
      "Episode:175 Steps:1000 meanR:0.9352 R:1.7400 gloss:0.0392 gloss1-lgP:0.4869 gloss2-gQs:0.0803 gloss3-dQs:0.0803 gloss4-tgtQ:0.0803 dloss:1.3839 exploreP:0.0100\n",
      "Episode:176 Steps:1000 meanR:0.9328 R:1.1200 gloss:0.0398 gloss1-lgP:0.4884 gloss2-gQs:0.0813 gloss3-dQs:0.0813 gloss4-tgtQ:0.0813 dloss:1.3837 exploreP:0.0100\n",
      "Episode:177 Steps:1000 meanR:0.9327 R:0.7300 gloss:0.0406 gloss1-lgP:0.4898 gloss2-gQs:0.0825 gloss3-dQs:0.0825 gloss4-tgtQ:0.0825 dloss:1.3838 exploreP:0.0100\n",
      "Episode:178 Steps:1000 meanR:0.9351 R:1.6800 gloss:0.0487 gloss1-lgP:0.4916 gloss2-gQs:0.0991 gloss3-dQs:0.0991 gloss4-tgtQ:0.0991 dloss:1.3829 exploreP:0.0100\n",
      "Episode:179 Steps:1000 meanR:0.9406 R:1.1700 gloss:0.0476 gloss1-lgP:0.4928 gloss2-gQs:0.0967 gloss3-dQs:0.0967 gloss4-tgtQ:0.0967 dloss:1.3828 exploreP:0.0100\n",
      "Episode:180 Steps:1000 meanR:0.9490 R:0.9500 gloss:0.0463 gloss1-lgP:0.4925 gloss2-gQs:0.0939 gloss3-dQs:0.0939 gloss4-tgtQ:0.0940 dloss:1.3829 exploreP:0.0100\n",
      "Episode:181 Steps:1000 meanR:0.9584 R:1.4700 gloss:0.0511 gloss1-lgP:0.4908 gloss2-gQs:0.1040 gloss3-dQs:0.1040 gloss4-tgtQ:0.1041 dloss:1.3819 exploreP:0.0100\n",
      "Episode:182 Steps:1000 meanR:0.9680 R:1.2700 gloss:0.0514 gloss1-lgP:0.4908 gloss2-gQs:0.1050 gloss3-dQs:0.1050 gloss4-tgtQ:0.1050 dloss:1.3815 exploreP:0.0100\n",
      "Episode:183 Steps:1000 meanR:0.9775 R:1.6300 gloss:0.0607 gloss1-lgP:0.4910 gloss2-gQs:0.1242 gloss3-dQs:0.1242 gloss4-tgtQ:0.1242 dloss:1.3801 exploreP:0.0100\n",
      "Episode:184 Steps:1000 meanR:0.9720 R:0.4500 gloss:0.0570 gloss1-lgP:0.4912 gloss2-gQs:0.1165 gloss3-dQs:0.1165 gloss4-tgtQ:0.1165 dloss:1.3807 exploreP:0.0100\n",
      "Episode:185 Steps:1000 meanR:0.9660 R:1.1600 gloss:0.0523 gloss1-lgP:0.4921 gloss2-gQs:0.1066 gloss3-dQs:0.1066 gloss4-tgtQ:0.1067 dloss:1.3812 exploreP:0.0100\n",
      "Episode:186 Steps:1000 meanR:0.9685 R:1.3100 gloss:0.0505 gloss1-lgP:0.4944 gloss2-gQs:0.1029 gloss3-dQs:0.1029 gloss4-tgtQ:0.1029 dloss:1.3817 exploreP:0.0100\n",
      "Episode:187 Steps:1000 meanR:0.9725 R:1.8100 gloss:0.0519 gloss1-lgP:0.4968 gloss2-gQs:0.1058 gloss3-dQs:0.1059 gloss4-tgtQ:0.1059 dloss:1.3815 exploreP:0.0100\n",
      "Episode:188 Steps:1000 meanR:0.9798 R:1.2200 gloss:0.0526 gloss1-lgP:0.4978 gloss2-gQs:0.1064 gloss3-dQs:0.1064 gloss4-tgtQ:0.1064 dloss:1.3819 exploreP:0.0100\n",
      "Episode:189 Steps:1000 meanR:0.9832 R:1.4900 gloss:0.0500 gloss1-lgP:0.4968 gloss2-gQs:0.1011 gloss3-dQs:0.1011 gloss4-tgtQ:0.1012 dloss:1.3825 exploreP:0.0100\n",
      "Episode:190 Steps:1000 meanR:0.9889 R:1.1300 gloss:0.0577 gloss1-lgP:0.4959 gloss2-gQs:0.1172 gloss3-dQs:0.1172 gloss4-tgtQ:0.1172 dloss:1.3816 exploreP:0.0100\n",
      "Episode:191 Steps:1000 meanR:0.9757 R:0.6800 gloss:0.0553 gloss1-lgP:0.4971 gloss2-gQs:0.1120 gloss3-dQs:0.1119 gloss4-tgtQ:0.1120 dloss:1.3820 exploreP:0.0100\n",
      "Episode:192 Steps:1000 meanR:0.9721 R:0.7100 gloss:0.0525 gloss1-lgP:0.4987 gloss2-gQs:0.1064 gloss3-dQs:0.1063 gloss4-tgtQ:0.1064 dloss:1.3824 exploreP:0.0100\n",
      "Episode:193 Steps:1000 meanR:0.9753 R:0.6900 gloss:0.0481 gloss1-lgP:0.5016 gloss2-gQs:0.0972 gloss3-dQs:0.0972 gloss4-tgtQ:0.0972 dloss:1.3829 exploreP:0.0100\n",
      "Episode:194 Steps:1000 meanR:0.9685 R:1.2300 gloss:0.0489 gloss1-lgP:0.5047 gloss2-gQs:0.0982 gloss3-dQs:0.0982 gloss4-tgtQ:0.0982 dloss:1.3830 exploreP:0.0100\n",
      "Episode:195 Steps:1000 meanR:0.9608 R:0.8100 gloss:0.0487 gloss1-lgP:0.5064 gloss2-gQs:0.0976 gloss3-dQs:0.0975 gloss4-tgtQ:0.0976 dloss:1.3830 exploreP:0.0100\n",
      "Episode:196 Steps:1000 meanR:0.9648 R:1.2100 gloss:0.0496 gloss1-lgP:0.5082 gloss2-gQs:0.0987 gloss3-dQs:0.0987 gloss4-tgtQ:0.0987 dloss:1.3829 exploreP:0.0100\n",
      "Episode:197 Steps:1000 meanR:0.9593 R:0.4400 gloss:0.0463 gloss1-lgP:0.5103 gloss2-gQs:0.0918 gloss3-dQs:0.0918 gloss4-tgtQ:0.0918 dloss:1.3834 exploreP:0.0100\n",
      "Episode:198 Steps:1000 meanR:0.9512 R:0.6700 gloss:0.0435 gloss1-lgP:0.5107 gloss2-gQs:0.0862 gloss3-dQs:0.0862 gloss4-tgtQ:0.0862 dloss:1.3837 exploreP:0.0100\n",
      "Episode:199 Steps:1000 meanR:0.9533 R:1.4900 gloss:0.0365 gloss1-lgP:0.5113 gloss2-gQs:0.0720 gloss3-dQs:0.0720 gloss4-tgtQ:0.0720 dloss:1.3843 exploreP:0.0100\n",
      "Episode:200 Steps:1000 meanR:0.9782 R:3.1800 gloss:0.0476 gloss1-lgP:0.5125 gloss2-gQs:0.0935 gloss3-dQs:0.0935 gloss4-tgtQ:0.0935 dloss:1.3833 exploreP:0.0100\n",
      "Episode:201 Steps:1000 meanR:0.9785 R:1.0300 gloss:0.0509 gloss1-lgP:0.5131 gloss2-gQs:0.0996 gloss3-dQs:0.0996 gloss4-tgtQ:0.0996 dloss:1.3826 exploreP:0.0100\n",
      "Episode:202 Steps:1000 meanR:0.9767 R:1.1200 gloss:0.0553 gloss1-lgP:0.5143 gloss2-gQs:0.1080 gloss3-dQs:0.1080 gloss4-tgtQ:0.1080 dloss:1.3818 exploreP:0.0100\n",
      "Episode:203 Steps:1000 meanR:0.9799 R:1.3100 gloss:0.0532 gloss1-lgP:0.5106 gloss2-gQs:0.1045 gloss3-dQs:0.1045 gloss4-tgtQ:0.1045 dloss:1.3817 exploreP:0.0100\n",
      "Episode:204 Steps:1000 meanR:0.9877 R:2.1500 gloss:0.0590 gloss1-lgP:0.5077 gloss2-gQs:0.1161 gloss3-dQs:0.1161 gloss4-tgtQ:0.1162 dloss:1.3810 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch, gloss3_batch, gloss4_batch = [], [], [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(1111111111):\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                #action = np.clip(action_logits, -1, 1)                  # all actions between -1 and 1\n",
    "            #print(action.shape)\n",
    "            #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "            #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, gloss1, gloss2, gloss3, gloss4, _, _ = sess.run([model.g_loss, model.d_loss,\n",
    "                                                                           model.g_loss1, model.g_loss2, \n",
    "                                                                           model.g_loss3, model.g_loss4,\n",
    "                                                                           model.g_opt, model.d_opt],\n",
    "                                                                          feed_dict = {model.states: states, \n",
    "                                                                                       model.actions: actions,\n",
    "                                                                                       model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            gloss3_batch.append(gloss3)\n",
    "            gloss4_batch.append(gloss4)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'Steps:{}'.format(num_steps),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'gloss1-lgP:{:.4f}'.format(np.mean(gloss1_batch)), #-logp\n",
    "              'gloss2-gQs:{:.4f}'.format(np.mean(gloss2_batch)),#gQs\n",
    "              'gloss3-dQs:{:.4f}'.format(np.mean(gloss3_batch)),#dQs\n",
    "              'gloss4-tgtQ:{:.4f}'.format(np.mean(gloss4_batch)),#tgtQs\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
