{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.6899999845772982\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.6899999845772982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #print(action)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(action)\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(action.shape, reward)\n",
    "        #print(done)\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float64, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float64, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float64, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits,\n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states) # nextQs\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob_actions * targetQs) # DPG\n",
    "    gloss += tf.reduce_mean(neg_log_prob_actions * Qs) # DPG\n",
    "    #dloss = tf.reduce_mean(tf.square(Qs - targetQs)) # DQN\n",
    "    dloss = tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs,\n",
    "                                                    labels=tf.nn.sigmoid(targetQs))\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    gloss += tf.reduce_mean(neg_log_prob_actions * dQs) # DPG\n",
    "    #dloss += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dloss += tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs,\n",
    "                                                     labels=tf.nn.sigmoid(targetQs))\n",
    "    gloss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    gloss2 = tf.reduce_mean(Qs)\n",
    "    gloss3 = tf.reduce_mean(dQs)\n",
    "    gloss4 = tf.reduce_mean(targetQs)\n",
    "    return actions_logits, Qs, gloss, dloss, gloss1, gloss2, gloss3, gloss4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2, self.g_loss3, self.g_loss4 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.1692914121018836\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 1000              # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "num_steps = 0\n",
    "for _ in range(memory_size):\n",
    "    num_steps += 1\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        #print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Steps:1000 meanR:0.0000 R:0.0000 gloss:5.2309 gloss1-lgP:1.4400 gloss2-gQs:1.8578 gloss3-dQs:1.6747 gloss4-tgtQ:1.8389 dloss:0.5913 exploreP:0.9057\n",
      "Episode:1 Steps:1000 meanR:0.0000 R:0.0000 gloss:2.9245 gloss1-lgP:0.9497 gloss2-gQs:1.2185 gloss3-dQs:1.1901 gloss4-tgtQ:1.2060 dloss:0.9182 exploreP:0.8204\n",
      "Episode:2 Steps:1000 meanR:0.2000 R:0.6000 gloss:0.0069 gloss1-lgP:0.7891 gloss2-gQs:0.0177 gloss3-dQs:0.0239 gloss4-tgtQ:0.0174 dloss:1.3317 exploreP:0.7432\n",
      "Episode:3 Steps:1000 meanR:0.2825 R:0.5300 gloss:0.9092 gloss1-lgP:1.0711 gloss2-gQs:0.3321 gloss3-dQs:0.3228 gloss4-tgtQ:0.3284 dloss:1.2028 exploreP:0.6734\n",
      "Episode:4 Steps:1000 meanR:0.2260 R:0.0000 gloss:0.2180 gloss1-lgP:0.8727 gloss2-gQs:0.0938 gloss3-dQs:0.0893 gloss4-tgtQ:0.0928 dloss:1.3069 exploreP:0.6102\n",
      "Episode:5 Steps:1000 meanR:0.6500 R:2.7700 gloss:0.7695 gloss1-lgP:0.9097 gloss2-gQs:0.3342 gloss3-dQs:0.3292 gloss4-tgtQ:0.3309 dloss:1.2346 exploreP:0.5530\n",
      "Episode:6 Steps:1000 meanR:0.5729 R:0.1100 gloss:0.3274 gloss1-lgP:0.7882 gloss2-gQs:0.1427 gloss3-dQs:0.1414 gloss4-tgtQ:0.1417 dloss:1.3308 exploreP:0.5013\n",
      "Episode:7 Steps:1000 meanR:0.6637 R:1.3000 gloss:0.0109 gloss1-lgP:0.9548 gloss2-gQs:0.0019 gloss3-dQs:0.0031 gloss4-tgtQ:0.0025 dloss:1.3668 exploreP:0.4545\n",
      "Episode:8 Steps:1000 meanR:0.5978 R:0.0700 gloss:0.0802 gloss1-lgP:0.7802 gloss2-gQs:0.0330 gloss3-dQs:0.0339 gloss4-tgtQ:0.0333 dloss:1.3666 exploreP:0.4121\n",
      "Episode:9 Steps:1000 meanR:0.6100 R:0.7200 gloss:0.0418 gloss1-lgP:0.9910 gloss2-gQs:0.0095 gloss3-dQs:0.0089 gloss4-tgtQ:0.0099 dloss:1.3742 exploreP:0.3738\n",
      "Episode:10 Steps:1000 meanR:0.6464 R:1.0100 gloss:0.1633 gloss1-lgP:0.8681 gloss2-gQs:0.0724 gloss3-dQs:0.0727 gloss4-tgtQ:0.0722 dloss:1.3730 exploreP:0.3392\n",
      "Episode:11 Steps:1000 meanR:0.6950 R:1.2300 gloss:-0.0153 gloss1-lgP:0.8129 gloss2-gQs:-0.0039 gloss3-dQs:-0.0023 gloss4-tgtQ:-0.0032 dloss:1.3848 exploreP:0.3078\n",
      "Episode:12 Steps:1000 meanR:0.7469 R:1.3700 gloss:0.0534 gloss1-lgP:0.8853 gloss2-gQs:0.0232 gloss3-dQs:0.0242 gloss4-tgtQ:0.0237 dloss:1.3835 exploreP:0.2795\n",
      "Episode:13 Steps:1000 meanR:0.7150 R:0.3000 gloss:0.0519 gloss1-lgP:0.7801 gloss2-gQs:0.0239 gloss3-dQs:0.0249 gloss4-tgtQ:0.0244 dloss:1.3844 exploreP:0.2538\n",
      "Episode:14 Steps:1000 meanR:0.7153 R:0.7200 gloss:0.1072 gloss1-lgP:0.8023 gloss2-gQs:0.0425 gloss3-dQs:0.0427 gloss4-tgtQ:0.0427 dloss:1.3809 exploreP:0.2306\n",
      "Episode:15 Steps:1000 meanR:0.7044 R:0.5400 gloss:0.0798 gloss1-lgP:0.6999 gloss2-gQs:0.0384 gloss3-dQs:0.0390 gloss4-tgtQ:0.0387 dloss:1.3843 exploreP:0.2096\n",
      "Episode:16 Steps:1000 meanR:0.7318 R:1.1700 gloss:0.0435 gloss1-lgP:0.7374 gloss2-gQs:0.0212 gloss3-dQs:0.0221 gloss4-tgtQ:0.0217 dloss:1.3852 exploreP:0.1905\n",
      "Episode:17 Steps:1000 meanR:0.7606 R:1.2500 gloss:0.0583 gloss1-lgP:0.7039 gloss2-gQs:0.0280 gloss3-dQs:0.0288 gloss4-tgtQ:0.0285 dloss:1.3852 exploreP:0.1734\n",
      "Episode:18 Steps:1000 meanR:0.7595 R:0.7400 gloss:0.0866 gloss1-lgP:0.6831 gloss2-gQs:0.0425 gloss3-dQs:0.0430 gloss4-tgtQ:0.0428 dloss:1.3852 exploreP:0.1578\n",
      "Episode:19 Steps:1000 meanR:0.7380 R:0.3300 gloss:0.0985 gloss1-lgP:0.6767 gloss2-gQs:0.0487 gloss3-dQs:0.0490 gloss4-tgtQ:0.0489 dloss:1.3852 exploreP:0.1437\n",
      "Episode:20 Steps:1000 meanR:0.7124 R:0.2000 gloss:0.0964 gloss1-lgP:0.6765 gloss2-gQs:0.0477 gloss3-dQs:0.0481 gloss4-tgtQ:0.0479 dloss:1.3851 exploreP:0.1310\n",
      "Episode:21 Steps:1000 meanR:0.7173 R:0.8200 gloss:0.1040 gloss1-lgP:0.6745 gloss2-gQs:0.0516 gloss3-dQs:0.0519 gloss4-tgtQ:0.0518 dloss:1.3851 exploreP:0.1195\n",
      "Episode:22 Steps:1000 meanR:0.6983 R:0.2800 gloss:0.0996 gloss1-lgP:0.6739 gloss2-gQs:0.0495 gloss3-dQs:0.0497 gloss4-tgtQ:0.0496 dloss:1.3852 exploreP:0.1090\n",
      "Episode:23 Steps:1000 meanR:0.6800 R:0.2600 gloss:0.1033 gloss1-lgP:0.6724 gloss2-gQs:0.0513 gloss3-dQs:0.0516 gloss4-tgtQ:0.0515 dloss:1.3850 exploreP:0.0996\n",
      "Episode:24 Steps:1000 meanR:0.6644 R:0.2900 gloss:0.0954 gloss1-lgP:0.6726 gloss2-gQs:0.0474 gloss3-dQs:0.0477 gloss4-tgtQ:0.0476 dloss:1.3852 exploreP:0.0911\n",
      "Episode:25 Steps:1000 meanR:0.6427 R:0.1000 gloss:0.0958 gloss1-lgP:0.6844 gloss2-gQs:0.0474 gloss3-dQs:0.0476 gloss4-tgtQ:0.0475 dloss:1.3852 exploreP:0.0833\n",
      "Episode:26 Steps:1000 meanR:0.6433 R:0.6600 gloss:0.0951 gloss1-lgP:0.6683 gloss2-gQs:0.0475 gloss3-dQs:0.0477 gloss4-tgtQ:0.0476 dloss:1.3853 exploreP:0.0764\n",
      "Episode:27 Steps:1000 meanR:0.6204 R:0.0000 gloss:0.0940 gloss1-lgP:0.6826 gloss2-gQs:0.0466 gloss3-dQs:0.0469 gloss4-tgtQ:0.0468 dloss:1.3852 exploreP:0.0700\n",
      "Episode:28 Steps:1000 meanR:0.6066 R:0.2200 gloss:0.0989 gloss1-lgP:0.6914 gloss2-gQs:0.0483 gloss3-dQs:0.0484 gloss4-tgtQ:0.0483 dloss:1.3851 exploreP:0.0643\n",
      "Episode:29 Steps:1000 meanR:0.6010 R:0.4400 gloss:0.0952 gloss1-lgP:0.6665 gloss2-gQs:0.0476 gloss3-dQs:0.0477 gloss4-tgtQ:0.0477 dloss:1.3853 exploreP:0.0591\n",
      "Episode:30 Steps:1000 meanR:0.6010 R:0.6000 gloss:0.0909 gloss1-lgP:0.6664 gloss2-gQs:0.0454 gloss3-dQs:0.0456 gloss4-tgtQ:0.0455 dloss:1.3853 exploreP:0.0545\n",
      "Episode:31 Steps:1000 meanR:0.6072 R:0.8000 gloss:0.0900 gloss1-lgP:0.6658 gloss2-gQs:0.0450 gloss3-dQs:0.0452 gloss4-tgtQ:0.0451 dloss:1.3853 exploreP:0.0502\n",
      "Episode:32 Steps:1000 meanR:0.6036 R:0.4900 gloss:0.0839 gloss1-lgP:0.6663 gloss2-gQs:0.0419 gloss3-dQs:0.0422 gloss4-tgtQ:0.0420 dloss:1.3854 exploreP:0.0464\n",
      "Episode:33 Steps:1000 meanR:0.6209 R:1.1900 gloss:0.0848 gloss1-lgP:0.6679 gloss2-gQs:0.0424 gloss3-dQs:0.0426 gloss4-tgtQ:0.0425 dloss:1.3854 exploreP:0.0429\n",
      "Episode:34 Steps:1000 meanR:0.6071 R:0.1400 gloss:0.0903 gloss1-lgP:0.6654 gloss2-gQs:0.0452 gloss3-dQs:0.0454 gloss4-tgtQ:0.0453 dloss:1.3853 exploreP:0.0398\n",
      "Episode:35 Steps:1000 meanR:0.5936 R:0.1200 gloss:0.0890 gloss1-lgP:0.6760 gloss2-gQs:0.0441 gloss3-dQs:0.0443 gloss4-tgtQ:0.0442 dloss:1.3853 exploreP:0.0370\n",
      "Episode:36 Steps:1000 meanR:0.5922 R:0.5400 gloss:0.0917 gloss1-lgP:0.6646 gloss2-gQs:0.0460 gloss3-dQs:0.0462 gloss4-tgtQ:0.0461 dloss:1.3853 exploreP:0.0344\n",
      "Episode:37 Steps:1000 meanR:0.6297 R:2.0200 gloss:0.0895 gloss1-lgP:0.6643 gloss2-gQs:0.0449 gloss3-dQs:0.0451 gloss4-tgtQ:0.0450 dloss:1.3853 exploreP:0.0321\n",
      "Episode:38 Steps:1000 meanR:0.6136 R:0.0000 gloss:0.0883 gloss1-lgP:0.6656 gloss2-gQs:0.0441 gloss3-dQs:0.0444 gloss4-tgtQ:0.0443 dloss:1.3853 exploreP:0.0300\n",
      "Episode:39 Steps:1000 meanR:0.6140 R:0.6300 gloss:0.0856 gloss1-lgP:0.6670 gloss2-gQs:0.0428 gloss3-dQs:0.0431 gloss4-tgtQ:0.0429 dloss:1.3854 exploreP:0.0281\n",
      "Episode:40 Steps:1000 meanR:0.6446 R:1.8700 gloss:0.0877 gloss1-lgP:0.6640 gloss2-gQs:0.0439 gloss3-dQs:0.0441 gloss4-tgtQ:0.0440 dloss:1.3854 exploreP:0.0263\n",
      "Episode:41 Steps:1000 meanR:0.6476 R:0.7700 gloss:0.0895 gloss1-lgP:0.6694 gloss2-gQs:0.0447 gloss3-dQs:0.0450 gloss4-tgtQ:0.0448 dloss:1.3853 exploreP:0.0248\n",
      "Episode:42 Steps:1000 meanR:0.6370 R:0.1900 gloss:0.0905 gloss1-lgP:0.6750 gloss2-gQs:0.0451 gloss3-dQs:0.0454 gloss4-tgtQ:0.0452 dloss:1.3852 exploreP:0.0234\n",
      "Episode:43 Steps:1000 meanR:0.6248 R:0.1000 gloss:0.0909 gloss1-lgP:0.6683 gloss2-gQs:0.0454 gloss3-dQs:0.0457 gloss4-tgtQ:0.0456 dloss:1.3852 exploreP:0.0221\n",
      "Episode:44 Steps:1000 meanR:0.6331 R:1.0000 gloss:0.0884 gloss1-lgP:0.6636 gloss2-gQs:0.0442 gloss3-dQs:0.0445 gloss4-tgtQ:0.0443 dloss:1.3853 exploreP:0.0209\n",
      "Episode:45 Steps:1000 meanR:0.6193 R:0.0000 gloss:0.0912 gloss1-lgP:0.6635 gloss2-gQs:0.0457 gloss3-dQs:0.0459 gloss4-tgtQ:0.0458 dloss:1.3853 exploreP:0.0199\n",
      "Episode:46 Steps:1000 meanR:0.6191 R:0.6100 gloss:0.0910 gloss1-lgP:0.6699 gloss2-gQs:0.0454 gloss3-dQs:0.0456 gloss4-tgtQ:0.0455 dloss:1.3852 exploreP:0.0190\n",
      "Episode:47 Steps:1000 meanR:0.6104 R:0.2000 gloss:0.0850 gloss1-lgP:0.6646 gloss2-gQs:0.0425 gloss3-dQs:0.0428 gloss4-tgtQ:0.0427 dloss:1.3853 exploreP:0.0181\n",
      "Episode:48 Steps:1000 meanR:0.6047 R:0.3300 gloss:0.0867 gloss1-lgP:0.6658 gloss2-gQs:0.0434 gloss3-dQs:0.0436 gloss4-tgtQ:0.0435 dloss:1.3853 exploreP:0.0173\n",
      "Episode:49 Steps:1000 meanR:0.6136 R:1.0500 gloss:0.0891 gloss1-lgP:0.6622 gloss2-gQs:0.0447 gloss3-dQs:0.0449 gloss4-tgtQ:0.0448 dloss:1.3853 exploreP:0.0166\n",
      "Episode:50 Steps:1000 meanR:0.6131 R:0.5900 gloss:0.1328 gloss1-lgP:0.7348 gloss2-gQs:0.0560 gloss3-dQs:0.0560 gloss4-tgtQ:0.0560 dloss:1.3840 exploreP:0.0160\n",
      "Episode:51 Steps:1000 meanR:0.6087 R:0.3800 gloss:0.0966 gloss1-lgP:0.6643 gloss2-gQs:0.0483 gloss3-dQs:0.0484 gloss4-tgtQ:0.0484 dloss:1.3853 exploreP:0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:52 Steps:1000 meanR:0.5972 R:0.0000 gloss:0.0949 gloss1-lgP:0.6628 gloss2-gQs:0.0475 gloss3-dQs:0.0477 gloss4-tgtQ:0.0476 dloss:1.3853 exploreP:0.0149\n",
      "Episode:53 Steps:1000 meanR:0.5896 R:0.1900 gloss:0.0888 gloss1-lgP:0.6621 gloss2-gQs:0.0445 gloss3-dQs:0.0447 gloss4-tgtQ:0.0446 dloss:1.3853 exploreP:0.0144\n",
      "Episode:54 Steps:1000 meanR:0.5813 R:0.1300 gloss:0.0835 gloss1-lgP:0.6620 gloss2-gQs:0.0418 gloss3-dQs:0.0420 gloss4-tgtQ:0.0419 dloss:1.3854 exploreP:0.0140\n",
      "Episode:55 Steps:1000 meanR:0.5734 R:0.1400 gloss:0.0840 gloss1-lgP:0.6616 gloss2-gQs:0.0421 gloss3-dQs:0.0423 gloss4-tgtQ:0.0422 dloss:1.3854 exploreP:0.0136\n",
      "Episode:56 Steps:1000 meanR:0.5739 R:0.6000 gloss:0.0742 gloss1-lgP:0.6730 gloss2-gQs:0.0370 gloss3-dQs:0.0373 gloss4-tgtQ:0.0371 dloss:1.3854 exploreP:0.0133\n",
      "Episode:57 Steps:1000 meanR:0.5640 R:0.0000 gloss:0.0805 gloss1-lgP:0.6617 gloss2-gQs:0.0403 gloss3-dQs:0.0406 gloss4-tgtQ:0.0405 dloss:1.3854 exploreP:0.0130\n",
      "Episode:58 Steps:1000 meanR:0.5722 R:1.0500 gloss:0.0787 gloss1-lgP:0.6918 gloss2-gQs:0.0384 gloss3-dQs:0.0386 gloss4-tgtQ:0.0385 dloss:1.3853 exploreP:0.0127\n",
      "Episode:59 Steps:1000 meanR:0.5712 R:0.5100 gloss:0.0827 gloss1-lgP:0.6612 gloss2-gQs:0.0414 gloss3-dQs:0.0417 gloss4-tgtQ:0.0415 dloss:1.3854 exploreP:0.0124\n",
      "Episode:60 Steps:1000 meanR:0.5618 R:0.0000 gloss:0.0749 gloss1-lgP:0.6624 gloss2-gQs:0.0375 gloss3-dQs:0.0378 gloss4-tgtQ:0.0376 dloss:1.3855 exploreP:0.0122\n",
      "Episode:61 Steps:1000 meanR:0.5558 R:0.1900 gloss:0.0757 gloss1-lgP:0.6666 gloss2-gQs:0.0378 gloss3-dQs:0.0381 gloss4-tgtQ:0.0379 dloss:1.3855 exploreP:0.0120\n",
      "Episode:62 Steps:1000 meanR:0.5600 R:0.8200 gloss:0.0758 gloss1-lgP:0.6738 gloss2-gQs:0.0375 gloss3-dQs:0.0378 gloss4-tgtQ:0.0376 dloss:1.3854 exploreP:0.0118\n",
      "Episode:63 Steps:1000 meanR:0.5569 R:0.3600 gloss:0.0745 gloss1-lgP:0.6612 gloss2-gQs:0.0373 gloss3-dQs:0.0375 gloss4-tgtQ:0.0374 dloss:1.3855 exploreP:0.0116\n",
      "Episode:64 Steps:1000 meanR:0.5483 R:0.0000 gloss:0.0722 gloss1-lgP:0.6618 gloss2-gQs:0.0361 gloss3-dQs:0.0364 gloss4-tgtQ:0.0363 dloss:1.3855 exploreP:0.0115\n",
      "Episode:65 Steps:1000 meanR:0.5477 R:0.5100 gloss:0.0631 gloss1-lgP:0.6700 gloss2-gQs:0.0314 gloss3-dQs:0.0317 gloss4-tgtQ:0.0316 dloss:1.3856 exploreP:0.0113\n",
      "Episode:66 Steps:1000 meanR:0.5410 R:0.1000 gloss:0.0716 gloss1-lgP:0.6597 gloss2-gQs:0.0359 gloss3-dQs:0.0362 gloss4-tgtQ:0.0360 dloss:1.3856 exploreP:0.0112\n",
      "Episode:67 Steps:1000 meanR:0.5349 R:0.1200 gloss:0.0701 gloss1-lgP:0.6742 gloss2-gQs:0.0347 gloss3-dQs:0.0349 gloss4-tgtQ:0.0348 dloss:1.3855 exploreP:0.0111\n",
      "Episode:68 Steps:1000 meanR:0.5317 R:0.3200 gloss:0.0672 gloss1-lgP:0.6632 gloss2-gQs:0.0335 gloss3-dQs:0.0338 gloss4-tgtQ:0.0337 dloss:1.3855 exploreP:0.0110\n",
      "Episode:69 Steps:1000 meanR:0.5257 R:0.1100 gloss:0.0637 gloss1-lgP:0.6623 gloss2-gQs:0.0319 gloss3-dQs:0.0322 gloss4-tgtQ:0.0321 dloss:1.3856 exploreP:0.0109\n",
      "Episode:70 Steps:1000 meanR:0.5241 R:0.4100 gloss:0.0802 gloss1-lgP:0.6823 gloss2-gQs:0.0392 gloss3-dQs:0.0394 gloss4-tgtQ:0.0393 dloss:1.3853 exploreP:0.0108\n",
      "Episode:71 Steps:1000 meanR:0.5265 R:0.7000 gloss:0.0740 gloss1-lgP:0.6567 gloss2-gQs:0.0372 gloss3-dQs:0.0374 gloss4-tgtQ:0.0373 dloss:1.3856 exploreP:0.0107\n",
      "Episode:72 Steps:1000 meanR:0.5203 R:0.0700 gloss:0.0728 gloss1-lgP:0.6560 gloss2-gQs:0.0366 gloss3-dQs:0.0369 gloss4-tgtQ:0.0367 dloss:1.3855 exploreP:0.0107\n",
      "Episode:73 Steps:1000 meanR:0.5280 R:1.0900 gloss:0.0690 gloss1-lgP:0.6582 gloss2-gQs:0.0346 gloss3-dQs:0.0349 gloss4-tgtQ:0.0348 dloss:1.3856 exploreP:0.0106\n",
      "Episode:74 Steps:1000 meanR:0.5232 R:0.1700 gloss:0.0665 gloss1-lgP:0.6676 gloss2-gQs:0.0331 gloss3-dQs:0.0334 gloss4-tgtQ:0.0333 dloss:1.3856 exploreP:0.0105\n",
      "Episode:75 Steps:1000 meanR:0.5163 R:0.0000 gloss:0.0712 gloss1-lgP:0.6543 gloss2-gQs:0.0359 gloss3-dQs:0.0361 gloss4-tgtQ:0.0360 dloss:1.3856 exploreP:0.0105\n",
      "Episode:76 Steps:1000 meanR:0.5129 R:0.2500 gloss:0.0755 gloss1-lgP:0.6770 gloss2-gQs:0.0363 gloss3-dQs:0.0365 gloss4-tgtQ:0.0364 dloss:1.3853 exploreP:0.0104\n",
      "Episode:77 Steps:1000 meanR:0.5063 R:0.0000 gloss:0.0697 gloss1-lgP:0.6540 gloss2-gQs:0.0351 gloss3-dQs:0.0353 gloss4-tgtQ:0.0352 dloss:1.3856 exploreP:0.0104\n",
      "Episode:78 Steps:1000 meanR:0.4999 R:0.0000 gloss:0.0693 gloss1-lgP:0.6578 gloss2-gQs:0.0349 gloss3-dQs:0.0351 gloss4-tgtQ:0.0350 dloss:1.3855 exploreP:0.0104\n",
      "Episode:79 Steps:1000 meanR:0.4936 R:0.0000 gloss:0.0713 gloss1-lgP:0.6518 gloss2-gQs:0.0360 gloss3-dQs:0.0362 gloss4-tgtQ:0.0361 dloss:1.3856 exploreP:0.0103\n",
      "Episode:80 Steps:1000 meanR:0.4880 R:0.0400 gloss:9.8540 gloss1-lgP:2.2572 gloss2-gQs:1.4859 gloss3-dQs:1.3091 gloss4-tgtQ:1.4706 dloss:0.9769 exploreP:0.0103\n",
      "Episode:81 Steps:1000 meanR:0.4821 R:0.0000 gloss:0.9071 gloss1-lgP:0.6658 gloss2-gQs:0.4625 gloss3-dQs:0.4639 gloss4-tgtQ:0.4580 dloss:1.3058 exploreP:0.0103\n",
      "Episode:82 Steps:1000 meanR:0.4800 R:0.3100 gloss:7.1614 gloss1-lgP:2.4506 gloss2-gQs:0.4850 gloss3-dQs:0.2724 gloss4-tgtQ:0.4808 dloss:1.2124 exploreP:0.0102\n",
      "Episode:83 Steps:1000 meanR:0.4942 R:1.6700 gloss:3.0837 gloss1-lgP:0.6843 gloss2-gQs:1.5233 gloss3-dQs:1.5324 gloss4-tgtQ:1.5076 dloss:0.9676 exploreP:0.0102\n",
      "Episode:84 Steps:1000 meanR:0.4929 R:0.3900 gloss:0.6515 gloss1-lgP:0.6736 gloss2-gQs:0.3279 gloss3-dQs:0.3286 gloss4-tgtQ:0.3249 dloss:1.3485 exploreP:0.0102\n",
      "Episode:85 Steps:1000 meanR:0.4888 R:0.1400 gloss:0.1123 gloss1-lgP:0.6730 gloss2-gQs:0.0567 gloss3-dQs:0.0571 gloss4-tgtQ:0.0566 dloss:1.3829 exploreP:0.0102\n",
      "Episode:86 Steps:1000 meanR:0.4869 R:0.3200 gloss:0.0178 gloss1-lgP:0.8252 gloss2-gQs:0.0098 gloss3-dQs:0.0092 gloss4-tgtQ:0.0101 dloss:1.3828 exploreP:0.0102\n",
      "Episode:87 Steps:1000 meanR:0.4818 R:0.0400 gloss:0.0757 gloss1-lgP:0.7999 gloss2-gQs:0.0348 gloss3-dQs:0.0352 gloss4-tgtQ:0.0349 dloss:1.3823 exploreP:0.0101\n",
      "Episode:88 Steps:1000 meanR:0.4811 R:0.4200 gloss:0.1029 gloss1-lgP:1.0159 gloss2-gQs:0.0395 gloss3-dQs:0.0391 gloss4-tgtQ:0.0395 dloss:1.3808 exploreP:0.0101\n",
      "Episode:89 Steps:1000 meanR:0.4762 R:0.0400 gloss:0.5475 gloss1-lgP:1.3150 gloss2-gQs:0.1560 gloss3-dQs:0.1526 gloss4-tgtQ:0.1548 dloss:1.3501 exploreP:0.0101\n",
      "Episode:90 Steps:1000 meanR:0.4762 R:0.4700 gloss:0.1864 gloss1-lgP:0.7279 gloss2-gQs:0.0951 gloss3-dQs:0.0956 gloss4-tgtQ:0.0945 dloss:1.3734 exploreP:0.0101\n",
      "Episode:91 Steps:1000 meanR:0.4847 R:1.2600 gloss:0.0174 gloss1-lgP:0.8013 gloss2-gQs:0.0102 gloss3-dQs:0.0108 gloss4-tgtQ:0.0106 dloss:1.3858 exploreP:0.0101\n",
      "Episode:92 Steps:1000 meanR:0.4795 R:0.0000 gloss:0.0194 gloss1-lgP:1.0562 gloss2-gQs:0.0132 gloss3-dQs:0.0138 gloss4-tgtQ:0.0136 dloss:1.3860 exploreP:0.0101\n",
      "Episode:93 Steps:1000 meanR:0.4787 R:0.4100 gloss:0.0854 gloss1-lgP:1.1916 gloss2-gQs:0.0268 gloss3-dQs:0.0271 gloss4-tgtQ:0.0269 dloss:1.3838 exploreP:0.0101\n",
      "Episode:94 Steps:1000 meanR:0.4737 R:0.0000 gloss:-0.0036 gloss1-lgP:2.2979 gloss2-gQs:0.0031 gloss3-dQs:0.0037 gloss4-tgtQ:0.0035 dloss:1.3838 exploreP:0.0101\n",
      "Episode:95 Steps:1000 meanR:0.4819 R:1.2600 gloss:0.2827 gloss1-lgP:2.1081 gloss2-gQs:0.0378 gloss3-dQs:0.0380 gloss4-tgtQ:0.0379 dloss:1.3843 exploreP:0.0101\n",
      "Episode:96 Steps:1000 meanR:0.4858 R:0.8600 gloss:0.0417 gloss1-lgP:0.9250 gloss2-gQs:0.0175 gloss3-dQs:0.0180 gloss4-tgtQ:0.0178 dloss:1.3857 exploreP:0.0101\n",
      "Episode:97 Steps:1000 meanR:0.4892 R:0.8200 gloss:0.0629 gloss1-lgP:0.6819 gloss2-gQs:0.0305 gloss3-dQs:0.0307 gloss4-tgtQ:0.0306 dloss:1.3859 exploreP:0.0101\n",
      "Episode:98 Steps:1000 meanR:0.4842 R:0.0000 gloss:0.0776 gloss1-lgP:0.6572 gloss2-gQs:0.0389 gloss3-dQs:0.0389 gloss4-tgtQ:0.0389 dloss:1.3856 exploreP:0.0100\n",
      "Episode:99 Steps:1000 meanR:0.4902 R:1.0800 gloss:0.0791 gloss1-lgP:0.6519 gloss2-gQs:0.0397 gloss3-dQs:0.0397 gloss4-tgtQ:0.0398 dloss:1.3855 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch, gloss3_batch, gloss4_batch = [], [], [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(1111111111):\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                #action = np.clip(action_logits, -1, 1)                  # all actions between -1 and 1\n",
    "            #print(action.shape)\n",
    "            #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "            #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, gloss1, gloss2, gloss3, gloss4, _, _ = sess.run([model.g_loss, model.d_loss,\n",
    "                                                                           model.g_loss1, model.g_loss2, \n",
    "                                                                           model.g_loss3, model.g_loss4,\n",
    "                                                                           model.g_opt, model.d_opt],\n",
    "                                                                          feed_dict = {model.states: states, \n",
    "                                                                                       model.actions: actions,\n",
    "                                                                                       model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            gloss3_batch.append(gloss3)\n",
    "            gloss4_batch.append(gloss4)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'Steps:{}'.format(num_steps),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'gloss1-lgP:{:.4f}'.format(np.mean(gloss1_batch)), #-logp\n",
    "              'gloss2-gQs:{:.4f}'.format(np.mean(gloss2_batch)),#gQs\n",
    "              'gloss3-dQs:{:.4f}'.format(np.mean(gloss3_batch)),#dQs\n",
    "              'gloss4-tgtQ:{:.4f}'.format(np.mean(gloss4_batch)),#tgtQs\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
