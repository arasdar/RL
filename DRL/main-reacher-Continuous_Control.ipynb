{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) 0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Testing the train mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    #scores += env_info.rewards                         # update the score (for each agent)\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(action.shape, reward)\n",
    "        print(done)\n",
    "        break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusion/merge states and actions/ SA/ SM\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    neg_log_prob_actions = tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=tf.nn.sigmoid(actions))\n",
    "    Qs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    #Qs = discriminator(actions=actions, hidden_size=hidden_size, states=states)\n",
    "    g_loss = tf.reduce_mean(neg_log_prob_actions * Qs)\n",
    "    g_loss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "    g_loss2 = tf.reduce_mean(Qs)\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, Qs, g_loss, d_loss, g_loss1, g_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_loss, self.g_loss1, self.g_loss2 = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1, 33) actions:(1, 4)\n",
      "action size:2.90378975973461\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print('action size:{}'.format(np.max(actions) - np.min(actions)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    #action = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "    #print(state.shape, action.reshape([-1]).shape, reward, float(done))\n",
    "    state = next_state                               # roll over states to next time step\n",
    "    if done is True:                                  # exit loop if episode finished\n",
    "        print(done)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(memory.buffer), memory.buffer[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.5000 R:0.5000 gloss:-0.0060 dloss:0.1196 gloss1-As:1.6245 gloss2-Qs:0.0216 exploreP:0.9057\n",
      "Episode:1 meanR:0.4450 R:0.3900 gloss:-0.0767 dloss:0.0343 gloss1-As:2.0516 gloss2-Qs:-0.0346 exploreP:0.8204\n",
      "Episode:2 meanR:0.4467 R:0.4500 gloss:0.0787 dloss:0.0272 gloss1-As:2.5895 gloss2-Qs:0.0337 exploreP:0.7432\n",
      "Episode:3 meanR:0.4125 R:0.3100 gloss:0.0825 dloss:0.0175 gloss1-As:2.0652 gloss2-Qs:0.0450 exploreP:0.6734\n",
      "Episode:4 meanR:0.4260 R:0.4800 gloss:0.0464 dloss:0.0130 gloss1-As:1.4764 gloss2-Qs:0.0327 exploreP:0.6102\n",
      "Episode:5 meanR:0.4117 R:0.3400 gloss:0.0109 dloss:0.0100 gloss1-As:1.3720 gloss2-Qs:0.0155 exploreP:0.5530\n",
      "Episode:6 meanR:0.4286 R:0.5300 gloss:0.0176 dloss:0.0087 gloss1-As:1.1610 gloss2-Qs:0.0215 exploreP:0.5013\n",
      "Episode:7 meanR:0.4587 R:0.6700 gloss:-0.0107 dloss:0.0075 gloss1-As:1.2199 gloss2-Qs:-0.0034 exploreP:0.4545\n",
      "Episode:8 meanR:0.5244 R:1.0500 gloss:0.0079 dloss:0.0089 gloss1-As:1.9174 gloss2-Qs:0.0065 exploreP:0.4121\n",
      "Episode:9 meanR:0.5440 R:0.7200 gloss:0.0339 dloss:0.0085 gloss1-As:3.1875 gloss2-Qs:0.0126 exploreP:0.3738\n",
      "Episode:10 meanR:0.6073 R:1.2400 gloss:0.0835 dloss:0.0060 gloss1-As:3.2360 gloss2-Qs:0.0257 exploreP:0.3392\n",
      "Episode:11 meanR:0.7033 R:1.7600 gloss:0.1414 dloss:0.0059 gloss1-As:4.0354 gloss2-Qs:0.0332 exploreP:0.3078\n",
      "Episode:12 meanR:0.7146 R:0.8500 gloss:0.1778 dloss:0.0051 gloss1-As:4.2285 gloss2-Qs:0.0424 exploreP:0.2795\n",
      "Episode:13 meanR:0.7107 R:0.6600 gloss:0.1900 dloss:0.0049 gloss1-As:3.9486 gloss2-Qs:0.0463 exploreP:0.2538\n",
      "Episode:14 meanR:0.7227 R:0.8900 gloss:0.2164 dloss:0.0038 gloss1-As:3.0455 gloss2-Qs:0.0683 exploreP:0.2306\n",
      "Episode:15 meanR:0.7400 R:1.0000 gloss:0.1659 dloss:0.0041 gloss1-As:1.7843 gloss2-Qs:0.0903 exploreP:0.2096\n",
      "Episode:16 meanR:0.7471 R:0.8600 gloss:0.0658 dloss:0.0029 gloss1-As:0.7770 gloss2-Qs:0.0860 exploreP:0.1905\n",
      "Episode:17 meanR:0.7339 R:0.5100 gloss:0.0260 dloss:0.0024 gloss1-As:0.3296 gloss2-Qs:0.0808 exploreP:0.1734\n",
      "Episode:18 meanR:0.7489 R:1.0200 gloss:0.0155 dloss:0.0038 gloss1-As:0.1937 gloss2-Qs:0.0815 exploreP:0.1578\n",
      "Episode:19 meanR:0.7650 R:1.0700 gloss:0.0117 dloss:0.0021 gloss1-As:0.1714 gloss2-Qs:0.0692 exploreP:0.1437\n",
      "Episode:20 meanR:0.7614 R:0.6900 gloss:0.0126 dloss:0.0016 gloss1-As:0.1631 gloss2-Qs:0.0783 exploreP:0.1310\n",
      "Episode:21 meanR:0.7827 R:1.2300 gloss:0.0137 dloss:0.0018 gloss1-As:0.1550 gloss2-Qs:0.0889 exploreP:0.1195\n",
      "Episode:22 meanR:0.8039 R:1.2700 gloss:0.0111 dloss:0.0018 gloss1-As:0.1526 gloss2-Qs:0.0739 exploreP:0.1090\n",
      "Episode:23 meanR:0.8042 R:0.8100 gloss:0.0107 dloss:0.0020 gloss1-As:0.1497 gloss2-Qs:0.0723 exploreP:0.0996\n",
      "Episode:24 meanR:0.8008 R:0.7200 gloss:0.0096 dloss:0.0015 gloss1-As:0.1491 gloss2-Qs:0.0653 exploreP:0.0911\n",
      "Episode:25 meanR:0.7962 R:0.6800 gloss:0.0120 dloss:0.0020 gloss1-As:0.1465 gloss2-Qs:0.0829 exploreP:0.0833\n",
      "Episode:26 meanR:0.8048 R:1.0300 gloss:0.0127 dloss:0.0015 gloss1-As:0.1403 gloss2-Qs:0.0910 exploreP:0.0764\n",
      "Episode:27 meanR:0.7979 R:0.6100 gloss:0.0110 dloss:0.0016 gloss1-As:0.1412 gloss2-Qs:0.0780 exploreP:0.0700\n",
      "Episode:28 meanR:0.7959 R:0.7400 gloss:0.0091 dloss:0.0022 gloss1-As:0.1420 gloss2-Qs:0.0647 exploreP:0.0643\n",
      "Episode:29 meanR:0.8093 R:1.2000 gloss:0.0106 dloss:0.0024 gloss1-As:0.1411 gloss2-Qs:0.0755 exploreP:0.0591\n",
      "Episode:30 meanR:0.8652 R:2.5400 gloss:0.0105 dloss:0.0012 gloss1-As:0.1399 gloss2-Qs:0.0751 exploreP:0.0545\n",
      "Episode:31 meanR:0.8981 R:1.9200 gloss:0.0144 dloss:0.0057 gloss1-As:0.1401 gloss2-Qs:0.1023 exploreP:0.0502\n",
      "Episode:32 meanR:0.8967 R:0.8500 gloss:0.0142 dloss:0.0008 gloss1-As:0.1376 gloss2-Qs:0.1035 exploreP:0.0464\n",
      "Episode:33 meanR:0.8824 R:0.4100 gloss:0.0127 dloss:0.0015 gloss1-As:0.1340 gloss2-Qs:0.0949 exploreP:0.0429\n",
      "Episode:34 meanR:0.8880 R:1.0800 gloss:0.0118 dloss:0.0027 gloss1-As:0.1330 gloss2-Qs:0.0884 exploreP:0.0398\n",
      "Episode:35 meanR:0.8811 R:0.6400 gloss:0.0135 dloss:0.0010 gloss1-As:0.1317 gloss2-Qs:0.1030 exploreP:0.0370\n",
      "Episode:36 meanR:0.9146 R:2.1200 gloss:0.0150 dloss:0.0021 gloss1-As:0.1305 gloss2-Qs:0.1150 exploreP:0.0344\n",
      "Episode:37 meanR:0.9434 R:2.0100 gloss:0.0136 dloss:0.0024 gloss1-As:0.1299 gloss2-Qs:0.1045 exploreP:0.0321\n",
      "Episode:38 meanR:0.9621 R:1.6700 gloss:0.0150 dloss:0.0019 gloss1-As:0.1277 gloss2-Qs:0.1179 exploreP:0.0300\n",
      "Episode:39 meanR:0.9622 R:0.9700 gloss:0.0160 dloss:0.0032 gloss1-As:0.1286 gloss2-Qs:0.1247 exploreP:0.0281\n",
      "Episode:40 meanR:0.9661 R:1.1200 gloss:0.0166 dloss:0.0020 gloss1-As:0.1245 gloss2-Qs:0.1328 exploreP:0.0263\n",
      "Episode:41 meanR:0.9593 R:0.6800 gloss:0.0166 dloss:0.0041 gloss1-As:0.1254 gloss2-Qs:0.1323 exploreP:0.0248\n",
      "Episode:42 meanR:0.9640 R:1.1600 gloss:0.0132 dloss:0.0013 gloss1-As:0.1264 gloss2-Qs:0.1043 exploreP:0.0234\n",
      "Episode:43 meanR:0.9723 R:1.3300 gloss:0.0136 dloss:0.0020 gloss1-As:0.1270 gloss2-Qs:0.1068 exploreP:0.0221\n",
      "Episode:44 meanR:0.9569 R:0.2800 gloss:0.0152 dloss:0.0016 gloss1-As:0.1273 gloss2-Qs:0.1191 exploreP:0.0209\n",
      "Episode:45 meanR:0.9780 R:1.9300 gloss:0.0151 dloss:0.0027 gloss1-As:0.1266 gloss2-Qs:0.1198 exploreP:0.0199\n",
      "Episode:46 meanR:0.9898 R:1.5300 gloss:0.0153 dloss:0.0049 gloss1-As:0.1303 gloss2-Qs:0.1172 exploreP:0.0190\n",
      "Episode:47 meanR:0.9890 R:0.9500 gloss:0.0157 dloss:0.0029 gloss1-As:0.1323 gloss2-Qs:0.1185 exploreP:0.0181\n",
      "Episode:48 meanR:0.9765 R:0.3800 gloss:0.0132 dloss:0.0018 gloss1-As:0.1320 gloss2-Qs:0.0999 exploreP:0.0173\n",
      "Episode:49 meanR:0.9834 R:1.3200 gloss:0.0124 dloss:0.0039 gloss1-As:0.1324 gloss2-Qs:0.0935 exploreP:0.0166\n",
      "Episode:50 meanR:0.9780 R:0.7100 gloss:0.0135 dloss:0.0050 gloss1-As:0.1342 gloss2-Qs:0.1007 exploreP:0.0160\n",
      "Episode:51 meanR:0.9837 R:1.2700 gloss:0.0134 dloss:0.0008 gloss1-As:0.1302 gloss2-Qs:0.1030 exploreP:0.0154\n",
      "Episode:52 meanR:0.9942 R:1.5400 gloss:0.0120 dloss:0.0038 gloss1-As:0.1321 gloss2-Qs:0.0901 exploreP:0.0149\n",
      "Episode:53 meanR:0.9946 R:1.0200 gloss:0.0150 dloss:0.0023 gloss1-As:0.1347 gloss2-Qs:0.1120 exploreP:0.0144\n",
      "Episode:54 meanR:1.0102 R:1.8500 gloss:0.0145 dloss:0.0158 gloss1-As:0.1335 gloss2-Qs:0.1078 exploreP:0.0140\n",
      "Episode:55 meanR:1.0105 R:1.0300 gloss:0.0145 dloss:0.0004 gloss1-As:0.1312 gloss2-Qs:0.1114 exploreP:0.0136\n",
      "Episode:56 meanR:1.0040 R:0.6400 gloss:0.0146 dloss:0.0022 gloss1-As:0.1309 gloss2-Qs:0.1112 exploreP:0.0133\n",
      "Episode:57 meanR:1.0086 R:1.2700 gloss:0.0163 dloss:0.0008 gloss1-As:0.1313 gloss2-Qs:0.1239 exploreP:0.0130\n",
      "Episode:58 meanR:1.0059 R:0.8500 gloss:0.0155 dloss:0.0052 gloss1-As:0.1317 gloss2-Qs:0.1166 exploreP:0.0127\n",
      "Episode:59 meanR:0.9957 R:0.3900 gloss:0.0149 dloss:0.0005 gloss1-As:0.1323 gloss2-Qs:0.1130 exploreP:0.0124\n",
      "Episode:60 meanR:0.9944 R:0.9200 gloss:0.0125 dloss:0.0045 gloss1-As:0.1332 gloss2-Qs:0.0941 exploreP:0.0122\n",
      "Episode:61 meanR:1.0069 R:1.7700 gloss:0.0128 dloss:0.0046 gloss1-As:0.1363 gloss2-Qs:0.0927 exploreP:0.0120\n",
      "Episode:62 meanR:1.0087 R:1.1200 gloss:0.0143 dloss:0.0006 gloss1-As:0.1336 gloss2-Qs:0.1076 exploreP:0.0118\n",
      "Episode:63 meanR:1.0098 R:1.0800 gloss:0.0163 dloss:0.0012 gloss1-As:0.1360 gloss2-Qs:0.1203 exploreP:0.0116\n",
      "Episode:64 meanR:0.9986 R:0.2800 gloss:0.0140 dloss:0.0043 gloss1-As:0.1376 gloss2-Qs:0.1015 exploreP:0.0115\n",
      "Episode:65 meanR:0.9888 R:0.3500 gloss:0.0120 dloss:0.0077 gloss1-As:0.1373 gloss2-Qs:0.0871 exploreP:0.0113\n",
      "Episode:66 meanR:0.9857 R:0.7800 gloss:0.0096 dloss:0.0004 gloss1-As:0.1360 gloss2-Qs:0.0708 exploreP:0.0112\n",
      "Episode:67 meanR:0.9893 R:1.2300 gloss:0.0097 dloss:0.0022 gloss1-As:0.1399 gloss2-Qs:0.0699 exploreP:0.0111\n",
      "Episode:68 meanR:0.9851 R:0.7000 gloss:0.0094 dloss:0.0019 gloss1-As:0.1357 gloss2-Qs:0.0695 exploreP:0.0110\n",
      "Episode:69 meanR:0.9780 R:0.4900 gloss:0.0100 dloss:0.0016 gloss1-As:0.1333 gloss2-Qs:0.0746 exploreP:0.0109\n",
      "Episode:70 meanR:0.9744 R:0.7200 gloss:0.0109 dloss:0.0109 gloss1-As:0.1342 gloss2-Qs:0.0786 exploreP:0.0108\n",
      "Episode:71 meanR:0.9644 R:0.2600 gloss:0.0090 dloss:0.0005 gloss1-As:0.1329 gloss2-Qs:0.0677 exploreP:0.0107\n",
      "Episode:72 meanR:0.9618 R:0.7700 gloss:0.0089 dloss:0.0022 gloss1-As:0.1316 gloss2-Qs:0.0674 exploreP:0.0107\n",
      "Episode:73 meanR:0.9580 R:0.6800 gloss:0.0084 dloss:0.0020 gloss1-As:0.1295 gloss2-Qs:0.0649 exploreP:0.0106\n",
      "Episode:74 meanR:0.9545 R:0.7000 gloss:0.0078 dloss:0.0018 gloss1-As:0.1281 gloss2-Qs:0.0614 exploreP:0.0105\n",
      "Episode:75 meanR:0.9514 R:0.7200 gloss:0.0075 dloss:0.0013 gloss1-As:0.1289 gloss2-Qs:0.0586 exploreP:0.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:76 meanR:0.9483 R:0.7100 gloss:0.0076 dloss:0.0025 gloss1-As:0.1300 gloss2-Qs:0.0581 exploreP:0.0104\n",
      "Episode:77 meanR:0.9605 R:1.9000 gloss:0.0076 dloss:0.0018 gloss1-As:0.1258 gloss2-Qs:0.0605 exploreP:0.0104\n",
      "Episode:78 meanR:0.9716 R:1.8400 gloss:0.0100 dloss:0.0035 gloss1-As:0.1281 gloss2-Qs:0.0768 exploreP:0.0104\n",
      "Episode:79 meanR:0.9675 R:0.6400 gloss:0.0102 dloss:0.0012 gloss1-As:0.1258 gloss2-Qs:0.0814 exploreP:0.0103\n",
      "Episode:80 meanR:0.9622 R:0.5400 gloss:0.0093 dloss:0.0029 gloss1-As:0.1254 gloss2-Qs:0.0730 exploreP:0.0103\n",
      "Episode:81 meanR:0.9705 R:1.6400 gloss:0.0092 dloss:0.0011 gloss1-As:0.1270 gloss2-Qs:0.0724 exploreP:0.0103\n",
      "Episode:82 meanR:0.9653 R:0.5400 gloss:0.0127 dloss:0.0019 gloss1-As:0.1295 gloss2-Qs:0.0977 exploreP:0.0102\n",
      "Episode:83 meanR:0.9565 R:0.2300 gloss:0.0115 dloss:0.0011 gloss1-As:0.1267 gloss2-Qs:0.0906 exploreP:0.0102\n",
      "Episode:84 meanR:0.9486 R:0.2800 gloss:0.0106 dloss:0.0025 gloss1-As:0.1263 gloss2-Qs:0.0843 exploreP:0.0102\n",
      "Episode:85 meanR:0.9478 R:0.8800 gloss:0.0106 dloss:0.0024 gloss1-As:0.1290 gloss2-Qs:0.0821 exploreP:0.0102\n",
      "Episode:86 meanR:0.9537 R:1.4600 gloss:0.0090 dloss:0.0022 gloss1-As:0.1246 gloss2-Qs:0.0720 exploreP:0.0102\n",
      "Episode:87 meanR:0.9574 R:1.2800 gloss:0.0109 dloss:0.0034 gloss1-As:0.1249 gloss2-Qs:0.0865 exploreP:0.0101\n",
      "Episode:88 meanR:0.9681 R:1.9100 gloss:0.0113 dloss:0.0012 gloss1-As:0.1226 gloss2-Qs:0.0921 exploreP:0.0101\n",
      "Episode:89 meanR:0.9578 R:0.0400 gloss:0.0122 dloss:0.0028 gloss1-As:0.1236 gloss2-Qs:0.0977 exploreP:0.0101\n",
      "Episode:90 meanR:0.9631 R:1.4400 gloss:0.0102 dloss:0.0031 gloss1-As:0.1224 gloss2-Qs:0.0830 exploreP:0.0101\n",
      "Episode:91 meanR:0.9627 R:0.9300 gloss:0.0098 dloss:0.0007 gloss1-As:0.1191 gloss2-Qs:0.0824 exploreP:0.0101\n",
      "Episode:92 meanR:0.9551 R:0.2500 gloss:0.0095 dloss:0.0018 gloss1-As:0.1206 gloss2-Qs:0.0782 exploreP:0.0101\n",
      "Episode:93 meanR:0.9648 R:1.8700 gloss:0.0092 dloss:0.0023 gloss1-As:0.1172 gloss2-Qs:0.0783 exploreP:0.0101\n",
      "Episode:94 meanR:0.9597 R:0.4800 gloss:0.0113 dloss:0.0010 gloss1-As:0.1179 gloss2-Qs:0.0960 exploreP:0.0101\n",
      "Episode:95 meanR:0.9579 R:0.7900 gloss:0.0116 dloss:0.0020 gloss1-As:0.1189 gloss2-Qs:0.0973 exploreP:0.0101\n",
      "Episode:96 meanR:0.9672 R:1.8600 gloss:0.0104 dloss:0.0021 gloss1-As:0.1155 gloss2-Qs:0.0896 exploreP:0.0101\n",
      "Episode:97 meanR:0.9681 R:1.0500 gloss:0.0119 dloss:0.0069 gloss1-As:0.1162 gloss2-Qs:0.1004 exploreP:0.0101\n",
      "Episode:98 meanR:0.9749 R:1.6500 gloss:0.0108 dloss:0.0003 gloss1-As:0.1127 gloss2-Qs:0.0959 exploreP:0.0100\n",
      "Episode:99 meanR:0.9703 R:0.5100 gloss:0.0103 dloss:0.0017 gloss1-As:0.1123 gloss2-Qs:0.0919 exploreP:0.0100\n",
      "Episode:100 meanR:0.9673 R:0.2000 gloss:0.0109 dloss:0.0026 gloss1-As:0.1137 gloss2-Qs:0.0945 exploreP:0.0100\n",
      "Episode:101 meanR:0.9677 R:0.4300 gloss:0.0102 dloss:0.0003 gloss1-As:0.1103 gloss2-Qs:0.0926 exploreP:0.0100\n",
      "Episode:102 meanR:0.9676 R:0.4400 gloss:0.0093 dloss:0.0012 gloss1-As:0.1080 gloss2-Qs:0.0863 exploreP:0.0100\n",
      "Episode:103 meanR:0.9804 R:1.5900 gloss:0.0091 dloss:0.0011 gloss1-As:0.1073 gloss2-Qs:0.0848 exploreP:0.0100\n",
      "Episode:104 meanR:0.9861 R:1.0500 gloss:0.0093 dloss:0.0023 gloss1-As:0.1084 gloss2-Qs:0.0856 exploreP:0.0100\n",
      "Episode:105 meanR:0.9933 R:1.0600 gloss:0.0097 dloss:0.0012 gloss1-As:0.1092 gloss2-Qs:0.0888 exploreP:0.0100\n",
      "Episode:106 meanR:0.9925 R:0.4500 gloss:0.0106 dloss:0.0022 gloss1-As:0.1126 gloss2-Qs:0.0942 exploreP:0.0100\n",
      "Episode:107 meanR:0.9990 R:1.3200 gloss:0.0084 dloss:0.0002 gloss1-As:0.1098 gloss2-Qs:0.0767 exploreP:0.0100\n",
      "Episode:108 meanR:0.9952 R:0.6700 gloss:0.0087 dloss:0.0006 gloss1-As:0.1111 gloss2-Qs:0.0781 exploreP:0.0100\n",
      "Episode:109 meanR:0.9967 R:0.8700 gloss:0.0069 dloss:0.0018 gloss1-As:0.1125 gloss2-Qs:0.0615 exploreP:0.0100\n",
      "Episode:110 meanR:1.0049 R:2.0600 gloss:0.0085 dloss:0.0041 gloss1-As:0.1110 gloss2-Qs:0.0752 exploreP:0.0100\n",
      "Episode:111 meanR:0.9990 R:1.1700 gloss:0.0097 dloss:0.0003 gloss1-As:0.1097 gloss2-Qs:0.0882 exploreP:0.0100\n",
      "Episode:112 meanR:0.9977 R:0.7200 gloss:0.0106 dloss:0.0006 gloss1-As:0.1141 gloss2-Qs:0.0932 exploreP:0.0100\n",
      "Episode:113 meanR:0.9967 R:0.5600 gloss:0.0122 dloss:0.0011 gloss1-As:0.1172 gloss2-Qs:0.1041 exploreP:0.0100\n",
      "Episode:114 meanR:0.9932 R:0.5400 gloss:0.0132 dloss:0.0075 gloss1-As:0.1201 gloss2-Qs:0.1036 exploreP:0.0100\n",
      "Episode:115 meanR:0.9878 R:0.4600 gloss:0.0107 dloss:0.0004 gloss1-As:0.1147 gloss2-Qs:0.0932 exploreP:0.0100\n",
      "Episode:116 meanR:0.9880 R:0.8800 gloss:0.0100 dloss:0.0009 gloss1-As:0.1161 gloss2-Qs:0.0865 exploreP:0.0100\n",
      "Episode:117 meanR:0.9998 R:1.6900 gloss:0.0114 dloss:0.0011 gloss1-As:0.1155 gloss2-Qs:0.0992 exploreP:0.0100\n",
      "Episode:118 meanR:1.0052 R:1.5600 gloss:0.0106 dloss:0.0015 gloss1-As:0.1152 gloss2-Qs:0.0920 exploreP:0.0100\n",
      "Episode:119 meanR:1.0093 R:1.4800 gloss:0.0112 dloss:0.0019 gloss1-As:0.1156 gloss2-Qs:0.0967 exploreP:0.0100\n",
      "Episode:120 meanR:1.0287 R:2.6300 gloss:0.0106 dloss:0.0008 gloss1-As:0.1140 gloss2-Qs:0.0927 exploreP:0.0100\n",
      "Episode:121 meanR:1.0366 R:2.0200 gloss:0.0110 dloss:0.0022 gloss1-As:0.1126 gloss2-Qs:0.0966 exploreP:0.0100\n",
      "Episode:122 meanR:1.0350 R:1.1100 gloss:0.0118 dloss:0.0013 gloss1-As:0.1078 gloss2-Qs:0.1095 exploreP:0.0100\n",
      "Episode:123 meanR:1.0353 R:0.8400 gloss:0.0116 dloss:0.0034 gloss1-As:0.1103 gloss2-Qs:0.1046 exploreP:0.0100\n",
      "Episode:124 meanR:1.0443 R:1.6200 gloss:0.0127 dloss:0.0022 gloss1-As:0.1083 gloss2-Qs:0.1172 exploreP:0.0100\n",
      "Episode:125 meanR:1.0554 R:1.7900 gloss:0.0144 dloss:0.0005 gloss1-As:0.1080 gloss2-Qs:0.1330 exploreP:0.0100\n",
      "Episode:126 meanR:1.0573 R:1.2200 gloss:0.0152 dloss:0.0032 gloss1-As:0.1088 gloss2-Qs:0.1376 exploreP:0.0100\n",
      "Episode:127 meanR:1.0644 R:1.3200 gloss:0.0163 dloss:0.0003 gloss1-As:0.1077 gloss2-Qs:0.1517 exploreP:0.0100\n",
      "Episode:128 meanR:1.0746 R:1.7600 gloss:0.0156 dloss:0.0009 gloss1-As:0.1093 gloss2-Qs:0.1431 exploreP:0.0100\n",
      "Episode:129 meanR:1.0716 R:0.9000 gloss:0.0165 dloss:0.0019 gloss1-As:0.1078 gloss2-Qs:0.1529 exploreP:0.0100\n",
      "Episode:130 meanR:1.0524 R:0.6200 gloss:0.0137 dloss:0.0027 gloss1-As:0.1077 gloss2-Qs:0.1266 exploreP:0.0100\n",
      "Episode:131 meanR:1.0403 R:0.7100 gloss:0.0142 dloss:0.0012 gloss1-As:0.1125 gloss2-Qs:0.1276 exploreP:0.0100\n",
      "Episode:132 meanR:1.0390 R:0.7200 gloss:0.0129 dloss:0.0004 gloss1-As:0.1153 gloss2-Qs:0.1125 exploreP:0.0100\n",
      "Episode:133 meanR:1.0369 R:0.2000 gloss:0.0120 dloss:0.0004 gloss1-As:0.1145 gloss2-Qs:0.1055 exploreP:0.0100\n",
      "Episode:134 meanR:1.0406 R:1.4500 gloss:0.0108 dloss:0.0009 gloss1-As:0.1110 gloss2-Qs:0.0979 exploreP:0.0100\n",
      "Episode:135 meanR:1.0456 R:1.1400 gloss:0.0098 dloss:0.0018 gloss1-As:0.1115 gloss2-Qs:0.0870 exploreP:0.0100\n",
      "Episode:136 meanR:1.0429 R:1.8500 gloss:0.0097 dloss:0.0003 gloss1-As:0.1107 gloss2-Qs:0.0874 exploreP:0.0100\n",
      "Episode:137 meanR:1.0319 R:0.9100 gloss:0.0110 dloss:0.0007 gloss1-As:0.1092 gloss2-Qs:0.1011 exploreP:0.0100\n",
      "Episode:138 meanR:1.0277 R:1.2500 gloss:0.0104 dloss:0.0009 gloss1-As:0.1095 gloss2-Qs:0.0951 exploreP:0.0100\n",
      "Episode:139 meanR:1.0288 R:1.0800 gloss:0.0093 dloss:0.0004 gloss1-As:0.1086 gloss2-Qs:0.0862 exploreP:0.0100\n",
      "Episode:140 meanR:1.0228 R:0.5200 gloss:0.0102 dloss:0.0013 gloss1-As:0.1111 gloss2-Qs:0.0915 exploreP:0.0100\n",
      "Episode:141 meanR:1.0310 R:1.5000 gloss:0.0090 dloss:0.0004 gloss1-As:0.1100 gloss2-Qs:0.0823 exploreP:0.0100\n",
      "Episode:142 meanR:1.0323 R:1.2900 gloss:0.0102 dloss:0.0003 gloss1-As:0.1110 gloss2-Qs:0.0922 exploreP:0.0100\n",
      "Episode:143 meanR:1.0282 R:0.9200 gloss:0.0106 dloss:0.0004 gloss1-As:0.1086 gloss2-Qs:0.0980 exploreP:0.0100\n",
      "Episode:144 meanR:1.0451 R:1.9700 gloss:0.0121 dloss:0.0009 gloss1-As:0.1092 gloss2-Qs:0.1111 exploreP:0.0100\n",
      "Episode:145 meanR:1.0335 R:0.7700 gloss:0.0118 dloss:0.0012 gloss1-As:0.1088 gloss2-Qs:0.1088 exploreP:0.0100\n",
      "Episode:146 meanR:1.0222 R:0.4000 gloss:0.0111 dloss:0.0003 gloss1-As:0.1075 gloss2-Qs:0.1029 exploreP:0.0100\n",
      "Episode:147 meanR:1.0211 R:0.8400 gloss:0.0111 dloss:0.0007 gloss1-As:0.1094 gloss2-Qs:0.1010 exploreP:0.0100\n",
      "Episode:148 meanR:1.0300 R:1.2700 gloss:0.0106 dloss:0.0002 gloss1-As:0.1070 gloss2-Qs:0.0996 exploreP:0.0100\n",
      "Episode:149 meanR:1.0282 R:1.1400 gloss:0.0097 dloss:0.0005 gloss1-As:0.1057 gloss2-Qs:0.0920 exploreP:0.0100\n",
      "Episode:150 meanR:1.0296 R:0.8500 gloss:0.0096 dloss:0.0005 gloss1-As:0.1059 gloss2-Qs:0.0906 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:151 meanR:1.0269 R:1.0000 gloss:0.0105 dloss:0.0007 gloss1-As:0.1060 gloss2-Qs:0.0990 exploreP:0.0100\n",
      "Episode:152 meanR:1.0324 R:2.0900 gloss:0.0110 dloss:0.0005 gloss1-As:0.1077 gloss2-Qs:0.1017 exploreP:0.0100\n",
      "Episode:153 meanR:1.0385 R:1.6300 gloss:0.0110 dloss:0.0004 gloss1-As:0.1073 gloss2-Qs:0.1029 exploreP:0.0100\n",
      "Episode:154 meanR:1.0361 R:1.6100 gloss:0.0121 dloss:0.0016 gloss1-As:0.1083 gloss2-Qs:0.1109 exploreP:0.0100\n",
      "Episode:155 meanR:1.0379 R:1.2100 gloss:0.0118 dloss:0.0003 gloss1-As:0.1055 gloss2-Qs:0.1123 exploreP:0.0100\n",
      "Episode:156 meanR:1.0450 R:1.3500 gloss:0.0114 dloss:0.0002 gloss1-As:0.1050 gloss2-Qs:0.1087 exploreP:0.0100\n",
      "Episode:157 meanR:1.0435 R:1.1200 gloss:0.0113 dloss:0.0002 gloss1-As:0.1034 gloss2-Qs:0.1098 exploreP:0.0100\n",
      "Episode:158 meanR:1.0448 R:0.9800 gloss:0.0122 dloss:0.0006 gloss1-As:0.1068 gloss2-Qs:0.1142 exploreP:0.0100\n",
      "Episode:159 meanR:1.0499 R:0.9000 gloss:0.0124 dloss:0.0001 gloss1-As:0.1058 gloss2-Qs:0.1176 exploreP:0.0100\n",
      "Episode:160 meanR:1.0438 R:0.3100 gloss:0.0129 dloss:0.0003 gloss1-As:0.1075 gloss2-Qs:0.1203 exploreP:0.0100\n",
      "Episode:161 meanR:1.0356 R:0.9500 gloss:0.0124 dloss:0.0005 gloss1-As:0.1074 gloss2-Qs:0.1148 exploreP:0.0100\n",
      "Episode:162 meanR:1.0290 R:0.4600 gloss:0.0121 dloss:0.0001 gloss1-As:0.1087 gloss2-Qs:0.1117 exploreP:0.0100\n",
      "Episode:163 meanR:1.0263 R:0.8100 gloss:0.0110 dloss:0.0002 gloss1-As:0.1083 gloss2-Qs:0.1020 exploreP:0.0100\n",
      "Episode:164 meanR:1.0331 R:0.9600 gloss:0.0097 dloss:0.0004 gloss1-As:0.1109 gloss2-Qs:0.0873 exploreP:0.0100\n",
      "Episode:165 meanR:1.0457 R:1.6100 gloss:0.0096 dloss:0.0001 gloss1-As:0.1112 gloss2-Qs:0.0861 exploreP:0.0100\n",
      "Episode:166 meanR:1.0504 R:1.2500 gloss:0.0096 dloss:0.0002 gloss1-As:0.1120 gloss2-Qs:0.0858 exploreP:0.0100\n",
      "Episode:167 meanR:1.0466 R:0.8500 gloss:0.0093 dloss:0.0001 gloss1-As:0.1128 gloss2-Qs:0.0828 exploreP:0.0100\n",
      "Episode:168 meanR:1.0501 R:1.0500 gloss:0.0093 dloss:0.0001 gloss1-As:0.1134 gloss2-Qs:0.0819 exploreP:0.0100\n",
      "Episode:169 meanR:1.0467 R:0.1500 gloss:0.0092 dloss:0.0002 gloss1-As:0.1145 gloss2-Qs:0.0804 exploreP:0.0100\n",
      "Episode:170 meanR:1.0550 R:1.5500 gloss:0.0097 dloss:0.0001 gloss1-As:0.1146 gloss2-Qs:0.0845 exploreP:0.0100\n",
      "Episode:171 meanR:1.0657 R:1.3300 gloss:0.0098 dloss:0.0002 gloss1-As:0.1130 gloss2-Qs:0.0871 exploreP:0.0100\n",
      "Episode:172 meanR:1.0618 R:0.3800 gloss:0.0110 dloss:0.0001 gloss1-As:0.1139 gloss2-Qs:0.0963 exploreP:0.0100\n",
      "Episode:173 meanR:1.0579 R:0.2900 gloss:0.0098 dloss:0.0002 gloss1-As:0.1124 gloss2-Qs:0.0875 exploreP:0.0100\n",
      "Episode:174 meanR:1.0602 R:0.9300 gloss:0.0094 dloss:0.0001 gloss1-As:0.1113 gloss2-Qs:0.0844 exploreP:0.0100\n",
      "Episode:175 meanR:1.0716 R:1.8600 gloss:0.0102 dloss:0.0001 gloss1-As:0.1122 gloss2-Qs:0.0912 exploreP:0.0100\n",
      "Episode:176 meanR:1.0691 R:0.4600 gloss:0.0099 dloss:0.0001 gloss1-As:0.1117 gloss2-Qs:0.0885 exploreP:0.0100\n",
      "Episode:177 meanR:1.0647 R:1.4600 gloss:0.0093 dloss:0.0001 gloss1-As:0.1108 gloss2-Qs:0.0836 exploreP:0.0100\n",
      "Episode:178 meanR:1.0550 R:0.8700 gloss:0.0101 dloss:0.0001 gloss1-As:0.1127 gloss2-Qs:0.0897 exploreP:0.0100\n",
      "Episode:179 meanR:1.0589 R:1.0300 gloss:0.0103 dloss:0.0001 gloss1-As:0.1134 gloss2-Qs:0.0909 exploreP:0.0100\n",
      "Episode:180 meanR:1.0675 R:1.4000 gloss:0.0097 dloss:0.0002 gloss1-As:0.1115 gloss2-Qs:0.0872 exploreP:0.0100\n",
      "Episode:181 meanR:1.0575 R:0.6400 gloss:0.0106 dloss:0.0001 gloss1-As:0.1127 gloss2-Qs:0.0943 exploreP:0.0100\n",
      "Episode:182 meanR:1.0613 R:0.9200 gloss:0.0105 dloss:0.0001 gloss1-As:0.1147 gloss2-Qs:0.0920 exploreP:0.0100\n",
      "Episode:183 meanR:1.0700 R:1.1000 gloss:0.0113 dloss:0.0001 gloss1-As:0.1142 gloss2-Qs:0.0988 exploreP:0.0100\n",
      "Episode:184 meanR:1.0831 R:1.5900 gloss:0.0114 dloss:0.0002 gloss1-As:0.1137 gloss2-Qs:0.1007 exploreP:0.0100\n",
      "Episode:185 meanR:1.0955 R:2.1200 gloss:0.0120 dloss:0.0002 gloss1-As:0.1164 gloss2-Qs:0.1028 exploreP:0.0100\n",
      "Episode:186 meanR:1.0910 R:1.0100 gloss:0.0119 dloss:0.0001 gloss1-As:0.1153 gloss2-Qs:0.1032 exploreP:0.0100\n",
      "Episode:187 meanR:1.0809 R:0.2700 gloss:0.0120 dloss:0.0001 gloss1-As:0.1147 gloss2-Qs:0.1048 exploreP:0.0100\n",
      "Episode:188 meanR:1.0752 R:1.3400 gloss:0.0115 dloss:0.0001 gloss1-As:0.1140 gloss2-Qs:0.1013 exploreP:0.0100\n",
      "Episode:189 meanR:1.0840 R:0.9200 gloss:0.0114 dloss:0.0001 gloss1-As:0.1160 gloss2-Qs:0.0982 exploreP:0.0100\n",
      "Episode:190 meanR:1.0784 R:0.8800 gloss:0.0116 dloss:0.0001 gloss1-As:0.1155 gloss2-Qs:0.1007 exploreP:0.0100\n",
      "Episode:191 meanR:1.0756 R:0.6500 gloss:0.0110 dloss:0.0001 gloss1-As:0.1133 gloss2-Qs:0.0972 exploreP:0.0100\n",
      "Episode:192 meanR:1.0835 R:1.0400 gloss:0.0109 dloss:0.0001 gloss1-As:0.1131 gloss2-Qs:0.0966 exploreP:0.0100\n",
      "Episode:193 meanR:1.0768 R:1.2000 gloss:0.0106 dloss:0.0001 gloss1-As:0.1119 gloss2-Qs:0.0951 exploreP:0.0100\n",
      "Episode:194 meanR:1.0803 R:0.8300 gloss:0.0110 dloss:0.0001 gloss1-As:0.1123 gloss2-Qs:0.0977 exploreP:0.0100\n",
      "Episode:195 meanR:1.0949 R:2.2500 gloss:0.0105 dloss:0.0001 gloss1-As:0.1114 gloss2-Qs:0.0939 exploreP:0.0100\n",
      "Episode:196 meanR:1.0791 R:0.2800 gloss:0.0099 dloss:0.0001 gloss1-As:0.1122 gloss2-Qs:0.0878 exploreP:0.0100\n",
      "Episode:197 meanR:1.0930 R:2.4400 gloss:0.0097 dloss:0.0001 gloss1-As:0.1108 gloss2-Qs:0.0873 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        gloss1_batch, gloss2_batch = [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            # if explore_p > np.random.rand():\n",
    "            #     #action = env.action_space.sample()\n",
    "            #     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            # else:\n",
    "            action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            #print(action.shape)\n",
    "            #action = np.reshape(action_logits, [-1]) # For continuous action space\n",
    "            #action = np.argmax(action_logits) # For discrete action space\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done)])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            #batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #print(actions.shape, actions.dtype)\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones)\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            #print(nextQs_logits.shape, targetQs.shape)\n",
    "            gloss, dloss, gloss1, gloss2, _, _ = sess.run([model.g_loss, model.d_loss, \n",
    "                                                           model.g_loss1, model.g_loss2,\n",
    "                                                           model.g_opt, model.d_opt],\n",
    "                                                          feed_dict = {model.states: states, \n",
    "                                                                       model.actions: actions,\n",
    "                                                                       model.targetQs: targetQs})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            gloss1_batch.append(gloss1)\n",
    "            gloss2_batch.append(gloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "                # g_loss1 = tf.reduce_mean(neg_log_prob_actions)\n",
    "                # g_loss2 = tf.reduce_mean(Qs)\n",
    "              'gloss1-As:{:.4f}'.format(np.mean(gloss1_batch)),\n",
    "              'gloss2-Qs:{:.4f}'.format(np.mean(gloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
