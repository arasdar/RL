{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with rated memory replay\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state = env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    #     print('state, action, reward, done, info:', \n",
    "    #           state, action, reward, done, info)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 0.04101413, -0.00918651,  0.03434713, -0.04816723]),\n",
       "  0,\n",
       "  array([ 0.0408304 , -0.2047837 ,  0.03338379,  0.25515159]),\n",
       "  1.0,\n",
       "  0.0],\n",
       " (4,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 float64\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.6282315236610856 -2.599334347538798\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, targetQs, action_size, hidden_size):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    #Qs = tf.reduce_max(actions_logits, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(batch_size, ListArr):\n",
    "    idx = np.random.choice(np.arange(len(ListArr)), \n",
    "                           size=batch_size, \n",
    "                           replace=True)\n",
    "    return [ListArr[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 24*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the memory with the pool of random exploration of the env.\n",
    "goal = 500 # env-based, the total reward required for reaching the goal G\n",
    "state = env.reset() # env-based\n",
    "total_reward = 0 # episode R\n",
    "num_step = 0 # episode steps/ length based on number of steps\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample() # exploring the env action space/ random action/ explore\n",
    "    next_state, reward, done, _ = env.step(action) # exploring the env state, reward, and done/end\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward # R += r\n",
    "    state = next_state # update the state for next episode\n",
    "    if done is True: # end of this episode\n",
    "        state = env.reset() # reset for next episode\n",
    "        rate = total_reward/goal # the actual sucess rate of the played sequence\n",
    "        total_reward = 0 # reset for next episode\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "        num_step = 0 # reset for the next episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = np.array(memory.buffer)[:, 4]\n",
    "rates = np.array(memory.buffer)[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones[-27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates[-27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = np.array(memory.buffer)[:, 5]\n",
    "rated_mem = np.array(memory.buffer)[rates >= (max(rates)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = np.array(rated_mem)[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.058, 0.058, 0.058, ..., 0.04, 0.04, 0.04], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dones = np.array(memory.buffer)[:, 4]\n",
    "rates = np.array(memory.buffer)[:, 5]\n",
    "rated_mem = np.array(memory.buffer)[rates >= (max(rates)*0.1)]\n",
    "batch = sample(ListArr=rated_mem, batch_size=batch_size)\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4),\n",
       " dtype('float64'),\n",
       " (100,),\n",
       " dtype('int64'),\n",
       " (100, 4),\n",
       " dtype('float64'),\n",
       " (100,),\n",
       " dtype('float64'),\n",
       " (100,),\n",
       " dtype('float64'),\n",
       " (100,),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, states.dtype, actions.shape, actions.dtype, next_states.shape, next_states.dtype, \\\n",
    "rewards.shape, rewards.dtype, dones.shape, dones.dtype, rates.shape, rates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.064, 0.048, 0.042, 0.094, 0.05 , 0.052, 0.032, 0.1  , 0.032,\n",
       "       0.068, 0.052, 0.088, 0.26 , 0.048, 0.056, 0.12 , 0.074, 0.038,\n",
       "       0.06 , 0.052, 0.03 , 0.04 , 0.042, 0.044, 0.102, 0.05 , 0.106,\n",
       "       0.052, 0.052, 0.052, 0.054, 0.076, 0.106, 0.062, 0.038, 0.05 ,\n",
       "       0.116, 0.03 , 0.082, 0.082, 0.062, 0.058, 0.078, 0.094, 0.056,\n",
       "       0.034, 0.044, 0.056, 0.036, 0.098, 0.088, 0.04 , 0.044, 0.064,\n",
       "       0.044, 0.032, 0.036, 0.092, 0.032, 0.05 , 0.066, 0.028, 0.034,\n",
       "       0.066, 0.07 , 0.06 , 0.076, 0.078, 0.068, 0.078, 0.052, 0.068,\n",
       "       0.034, 0.036, 0.06 , 0.06 , 0.08 , 0.06 , 0.052, 0.038, 0.11 ,\n",
       "       0.05 , 0.044, 0.06 , 0.074, 0.09 , 0.112, 0.084, 0.048, 0.072,\n",
       "       0.04 , 0.052, 0.064, 0.048, 0.056, 0.112, 0.072, 0.034, 0.052,\n",
       "       0.158])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:23.0000 R:23.0 rate:0.0460 loss:1.0343 exploreP:0.9977\n",
      "Episode:1 meanR:34.0000 R:45.0 rate:0.0900 loss:1.1496 exploreP:0.9933\n",
      "Episode:2 meanR:27.0000 R:13.0 rate:0.0260 loss:1.2681 exploreP:0.9920\n",
      "Episode:3 meanR:23.5000 R:13.0 rate:0.0260 loss:1.3459 exploreP:0.9907\n",
      "Episode:4 meanR:22.0000 R:16.0 rate:0.0320 loss:1.4108 exploreP:0.9892\n",
      "Episode:5 meanR:25.1667 R:41.0 rate:0.0820 loss:1.5974 exploreP:0.9852\n",
      "Episode:6 meanR:23.8571 R:16.0 rate:0.0320 loss:1.7620 exploreP:0.9836\n",
      "Episode:7 meanR:24.8750 R:32.0 rate:0.0640 loss:2.0377 exploreP:0.9805\n",
      "Episode:8 meanR:24.1111 R:18.0 rate:0.0360 loss:2.3423 exploreP:0.9787\n",
      "Episode:9 meanR:23.0000 R:13.0 rate:0.0260 loss:2.6117 exploreP:0.9775\n",
      "Episode:10 meanR:22.0000 R:12.0 rate:0.0240 loss:2.7479 exploreP:0.9763\n",
      "Episode:11 meanR:21.9167 R:21.0 rate:0.0420 loss:3.0209 exploreP:0.9743\n",
      "Episode:12 meanR:21.4615 R:16.0 rate:0.0320 loss:3.0101 exploreP:0.9728\n",
      "Episode:13 meanR:20.7857 R:12.0 rate:0.0240 loss:3.0180 exploreP:0.9716\n",
      "Episode:14 meanR:21.8000 R:36.0 rate:0.0720 loss:2.9925 exploreP:0.9682\n",
      "Episode:15 meanR:21.6875 R:20.0 rate:0.0400 loss:2.9744 exploreP:0.9662\n",
      "Episode:16 meanR:22.9412 R:43.0 rate:0.0860 loss:3.0690 exploreP:0.9621\n",
      "Episode:17 meanR:23.4444 R:32.0 rate:0.0640 loss:3.2416 exploreP:0.9591\n",
      "Episode:18 meanR:23.6316 R:27.0 rate:0.0540 loss:3.4552 exploreP:0.9565\n",
      "Episode:19 meanR:23.4500 R:20.0 rate:0.0400 loss:3.6924 exploreP:0.9546\n",
      "Episode:20 meanR:24.3333 R:42.0 rate:0.0840 loss:4.0264 exploreP:0.9507\n",
      "Episode:21 meanR:23.7273 R:11.0 rate:0.0220 loss:4.4870 exploreP:0.9496\n",
      "Episode:22 meanR:23.3478 R:15.0 rate:0.0300 loss:4.4594 exploreP:0.9482\n",
      "Episode:23 meanR:22.7917 R:10.0 rate:0.0200 loss:4.6500 exploreP:0.9473\n",
      "Episode:24 meanR:22.4400 R:14.0 rate:0.0280 loss:4.8056 exploreP:0.9460\n",
      "Episode:25 meanR:22.2308 R:17.0 rate:0.0340 loss:5.3016 exploreP:0.9444\n",
      "Episode:26 meanR:21.9630 R:15.0 rate:0.0300 loss:6.1605 exploreP:0.9430\n",
      "Episode:27 meanR:21.6786 R:14.0 rate:0.0280 loss:7.1209 exploreP:0.9417\n",
      "Episode:28 meanR:22.0000 R:31.0 rate:0.0620 loss:6.8587 exploreP:0.9388\n",
      "Episode:29 meanR:22.7333 R:44.0 rate:0.0880 loss:8.2160 exploreP:0.9347\n",
      "Episode:30 meanR:22.8065 R:25.0 rate:0.0500 loss:10.2316 exploreP:0.9324\n",
      "Episode:31 meanR:22.5625 R:15.0 rate:0.0300 loss:9.9864 exploreP:0.9310\n",
      "Episode:32 meanR:22.9697 R:36.0 rate:0.0720 loss:13.5205 exploreP:0.9277\n",
      "Episode:33 meanR:22.8235 R:18.0 rate:0.0360 loss:15.5505 exploreP:0.9261\n",
      "Episode:34 meanR:23.1429 R:34.0 rate:0.0680 loss:15.6387 exploreP:0.9230\n",
      "Episode:35 meanR:23.8056 R:47.0 rate:0.0940 loss:17.9170 exploreP:0.9187\n",
      "Episode:36 meanR:23.8108 R:24.0 rate:0.0480 loss:22.1246 exploreP:0.9165\n",
      "Episode:37 meanR:23.4737 R:11.0 rate:0.0220 loss:21.0882 exploreP:0.9155\n",
      "Episode:38 meanR:24.6410 R:69.0 rate:0.1380 loss:29.7446 exploreP:0.9093\n",
      "Episode:39 meanR:24.4500 R:17.0 rate:0.0340 loss:35.4146 exploreP:0.9078\n",
      "Episode:40 meanR:24.1707 R:13.0 rate:0.0260 loss:38.3168 exploreP:0.9066\n",
      "Episode:41 meanR:24.5714 R:41.0 rate:0.0820 loss:37.0590 exploreP:0.9029\n",
      "Episode:42 meanR:24.6512 R:28.0 rate:0.0560 loss:43.5335 exploreP:0.9004\n",
      "Episode:43 meanR:24.4545 R:16.0 rate:0.0320 loss:46.8993 exploreP:0.8990\n",
      "Episode:44 meanR:24.2667 R:16.0 rate:0.0320 loss:42.5796 exploreP:0.8976\n",
      "Episode:45 meanR:24.0870 R:16.0 rate:0.0320 loss:43.3160 exploreP:0.8962\n",
      "Episode:46 meanR:24.5319 R:45.0 rate:0.0900 loss:58.0949 exploreP:0.8922\n",
      "Episode:47 meanR:24.2917 R:13.0 rate:0.0260 loss:64.5327 exploreP:0.8910\n",
      "Episode:48 meanR:24.0612 R:13.0 rate:0.0260 loss:48.4091 exploreP:0.8899\n",
      "Episode:49 meanR:23.7800 R:10.0 rate:0.0200 loss:60.6248 exploreP:0.8890\n",
      "Episode:50 meanR:23.7647 R:23.0 rate:0.0460 loss:55.7671 exploreP:0.8870\n",
      "Episode:51 meanR:23.8462 R:28.0 rate:0.0560 loss:62.3069 exploreP:0.8845\n",
      "Episode:52 meanR:23.7358 R:18.0 rate:0.0360 loss:73.4951 exploreP:0.8830\n",
      "Episode:53 meanR:24.7963 R:81.0 rate:0.1620 loss:76.1930 exploreP:0.8759\n",
      "Episode:54 meanR:24.6364 R:16.0 rate:0.0320 loss:58.7930 exploreP:0.8745\n",
      "Episode:55 meanR:24.5000 R:17.0 rate:0.0340 loss:58.7218 exploreP:0.8731\n",
      "Episode:56 meanR:24.3684 R:17.0 rate:0.0340 loss:53.0579 exploreP:0.8716\n",
      "Episode:57 meanR:24.5345 R:34.0 rate:0.0680 loss:89.7947 exploreP:0.8687\n",
      "Episode:58 meanR:24.3220 R:12.0 rate:0.0240 loss:80.9546 exploreP:0.8677\n",
      "Episode:59 meanR:24.4833 R:34.0 rate:0.0680 loss:71.7733 exploreP:0.8647\n",
      "Episode:60 meanR:24.2787 R:12.0 rate:0.0240 loss:82.9759 exploreP:0.8637\n",
      "Episode:61 meanR:24.3548 R:29.0 rate:0.0580 loss:64.9449 exploreP:0.8612\n",
      "Episode:62 meanR:24.2540 R:18.0 rate:0.0360 loss:69.8692 exploreP:0.8597\n",
      "Episode:63 meanR:24.1875 R:20.0 rate:0.0400 loss:91.9849 exploreP:0.8580\n",
      "Episode:64 meanR:23.9692 R:10.0 rate:0.0200 loss:75.8486 exploreP:0.8572\n",
      "Episode:65 meanR:23.9394 R:22.0 rate:0.0440 loss:74.8963 exploreP:0.8553\n",
      "Episode:66 meanR:23.9104 R:22.0 rate:0.0440 loss:79.6485 exploreP:0.8535\n",
      "Episode:67 meanR:23.9412 R:26.0 rate:0.0520 loss:75.9046 exploreP:0.8513\n",
      "Episode:68 meanR:24.1884 R:41.0 rate:0.0820 loss:68.4219 exploreP:0.8478\n",
      "Episode:69 meanR:24.0286 R:13.0 rate:0.0260 loss:72.0163 exploreP:0.8467\n",
      "Episode:70 meanR:23.8873 R:14.0 rate:0.0280 loss:52.4020 exploreP:0.8456\n",
      "Episode:71 meanR:23.8472 R:21.0 rate:0.0420 loss:65.6046 exploreP:0.8438\n",
      "Episode:72 meanR:23.9589 R:32.0 rate:0.0640 loss:62.7473 exploreP:0.8411\n",
      "Episode:73 meanR:23.8378 R:15.0 rate:0.0300 loss:66.8645 exploreP:0.8399\n",
      "Episode:74 meanR:23.7067 R:14.0 rate:0.0280 loss:56.5895 exploreP:0.8387\n",
      "Episode:75 meanR:23.5789 R:14.0 rate:0.0280 loss:64.5765 exploreP:0.8376\n",
      "Episode:76 meanR:23.6234 R:27.0 rate:0.0540 loss:79.7061 exploreP:0.8353\n",
      "Episode:77 meanR:23.5641 R:19.0 rate:0.0380 loss:74.5042 exploreP:0.8338\n",
      "Episode:78 meanR:23.4051 R:11.0 rate:0.0220 loss:64.2548 exploreP:0.8329\n",
      "Episode:79 meanR:23.4125 R:24.0 rate:0.0480 loss:53.1687 exploreP:0.8309\n",
      "Episode:80 meanR:23.2963 R:14.0 rate:0.0280 loss:57.3305 exploreP:0.8298\n",
      "Episode:81 meanR:23.1341 R:10.0 rate:0.0200 loss:49.4833 exploreP:0.8289\n",
      "Episode:82 meanR:23.3253 R:39.0 rate:0.0780 loss:59.9235 exploreP:0.8257\n",
      "Episode:83 meanR:23.4405 R:33.0 rate:0.0660 loss:54.4329 exploreP:0.8231\n",
      "Episode:84 meanR:23.3882 R:19.0 rate:0.0380 loss:46.6987 exploreP:0.8215\n",
      "Episode:85 meanR:23.3837 R:23.0 rate:0.0460 loss:65.7169 exploreP:0.8197\n",
      "Episode:86 meanR:23.2644 R:13.0 rate:0.0260 loss:46.8666 exploreP:0.8186\n",
      "Episode:87 meanR:23.1136 R:10.0 rate:0.0200 loss:59.4631 exploreP:0.8178\n",
      "Episode:88 meanR:23.0674 R:19.0 rate:0.0380 loss:58.2213 exploreP:0.8163\n",
      "Episode:89 meanR:22.9778 R:15.0 rate:0.0300 loss:49.1217 exploreP:0.8151\n",
      "Episode:90 meanR:22.8571 R:12.0 rate:0.0240 loss:59.5478 exploreP:0.8141\n",
      "Episode:91 meanR:22.7283 R:11.0 rate:0.0220 loss:48.5345 exploreP:0.8132\n",
      "Episode:92 meanR:22.5806 R:9.0 rate:0.0180 loss:52.8830 exploreP:0.8125\n",
      "Episode:93 meanR:22.5957 R:24.0 rate:0.0480 loss:48.5749 exploreP:0.8106\n",
      "Episode:94 meanR:22.6316 R:26.0 rate:0.0520 loss:45.3634 exploreP:0.8085\n",
      "Episode:95 meanR:22.5208 R:12.0 rate:0.0240 loss:54.6969 exploreP:0.8075\n",
      "Episode:96 meanR:22.3814 R:9.0 rate:0.0180 loss:46.5659 exploreP:0.8068\n",
      "Episode:97 meanR:22.2755 R:12.0 rate:0.0240 loss:50.9082 exploreP:0.8058\n",
      "Episode:98 meanR:22.3131 R:26.0 rate:0.0520 loss:48.1989 exploreP:0.8038\n",
      "Episode:99 meanR:22.4400 R:35.0 rate:0.0700 loss:46.7967 exploreP:0.8010\n",
      "Episode:100 meanR:22.3100 R:10.0 rate:0.0200 loss:44.4144 exploreP:0.8002\n",
      "Episode:101 meanR:21.9900 R:13.0 rate:0.0260 loss:46.9068 exploreP:0.7992\n",
      "Episode:102 meanR:22.0900 R:23.0 rate:0.0460 loss:43.2111 exploreP:0.7974\n",
      "Episode:103 meanR:22.1100 R:15.0 rate:0.0300 loss:37.7742 exploreP:0.7962\n",
      "Episode:104 meanR:22.0800 R:13.0 rate:0.0260 loss:27.1714 exploreP:0.7952\n",
      "Episode:105 meanR:22.0100 R:34.0 rate:0.0680 loss:38.4419 exploreP:0.7925\n",
      "Episode:106 meanR:22.0800 R:23.0 rate:0.0460 loss:40.0138 exploreP:0.7907\n",
      "Episode:107 meanR:22.0400 R:28.0 rate:0.0560 loss:37.3590 exploreP:0.7885\n",
      "Episode:108 meanR:21.9500 R:9.0 rate:0.0180 loss:35.7759 exploreP:0.7878\n",
      "Episode:109 meanR:21.9700 R:15.0 rate:0.0300 loss:38.1089 exploreP:0.7867\n",
      "Episode:110 meanR:21.9400 R:9.0 rate:0.0180 loss:31.9119 exploreP:0.7860\n",
      "Episode:111 meanR:21.8700 R:14.0 rate:0.0280 loss:28.5204 exploreP:0.7849\n",
      "Episode:112 meanR:21.8400 R:13.0 rate:0.0260 loss:36.9478 exploreP:0.7839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:113 meanR:21.8700 R:15.0 rate:0.0300 loss:35.9635 exploreP:0.7827\n",
      "Episode:114 meanR:21.7100 R:20.0 rate:0.0400 loss:32.1920 exploreP:0.7812\n",
      "Episode:115 meanR:21.7100 R:20.0 rate:0.0400 loss:38.6111 exploreP:0.7796\n",
      "Episode:116 meanR:21.4500 R:17.0 rate:0.0340 loss:37.4513 exploreP:0.7783\n",
      "Episode:117 meanR:21.3200 R:19.0 rate:0.0380 loss:34.9844 exploreP:0.7769\n",
      "Episode:118 meanR:21.3600 R:31.0 rate:0.0620 loss:29.2651 exploreP:0.7745\n",
      "Episode:119 meanR:21.2800 R:12.0 rate:0.0240 loss:25.3651 exploreP:0.7736\n",
      "Episode:120 meanR:21.0500 R:19.0 rate:0.0380 loss:26.1590 exploreP:0.7721\n",
      "Episode:121 meanR:21.1200 R:18.0 rate:0.0360 loss:33.3844 exploreP:0.7708\n",
      "Episode:122 meanR:21.2100 R:24.0 rate:0.0480 loss:29.5622 exploreP:0.7689\n",
      "Episode:123 meanR:21.2500 R:14.0 rate:0.0280 loss:28.0663 exploreP:0.7679\n",
      "Episode:124 meanR:21.3700 R:26.0 rate:0.0520 loss:27.1437 exploreP:0.7659\n",
      "Episode:125 meanR:21.6500 R:45.0 rate:0.0900 loss:27.3250 exploreP:0.7625\n",
      "Episode:126 meanR:21.7300 R:23.0 rate:0.0460 loss:26.1274 exploreP:0.7608\n",
      "Episode:127 meanR:21.7300 R:14.0 rate:0.0280 loss:28.1481 exploreP:0.7597\n",
      "Episode:128 meanR:21.6400 R:22.0 rate:0.0440 loss:26.0777 exploreP:0.7581\n",
      "Episode:129 meanR:21.3300 R:13.0 rate:0.0260 loss:28.0988 exploreP:0.7571\n",
      "Episode:130 meanR:21.2900 R:21.0 rate:0.0420 loss:19.6071 exploreP:0.7555\n",
      "Episode:131 meanR:21.4900 R:35.0 rate:0.0700 loss:25.4206 exploreP:0.7529\n",
      "Episode:132 meanR:21.3800 R:25.0 rate:0.0500 loss:25.1966 exploreP:0.7511\n",
      "Episode:133 meanR:21.4400 R:24.0 rate:0.0480 loss:25.8458 exploreP:0.7493\n",
      "Episode:134 meanR:21.2900 R:19.0 rate:0.0380 loss:20.4611 exploreP:0.7479\n",
      "Episode:135 meanR:21.0800 R:26.0 rate:0.0520 loss:27.3404 exploreP:0.7460\n",
      "Episode:136 meanR:21.0100 R:17.0 rate:0.0340 loss:27.1783 exploreP:0.7447\n",
      "Episode:137 meanR:21.2700 R:37.0 rate:0.0740 loss:25.1947 exploreP:0.7420\n",
      "Episode:138 meanR:20.7800 R:20.0 rate:0.0400 loss:23.3272 exploreP:0.7406\n",
      "Episode:139 meanR:20.7500 R:14.0 rate:0.0280 loss:19.8311 exploreP:0.7395\n",
      "Episode:140 meanR:20.8100 R:19.0 rate:0.0380 loss:16.1864 exploreP:0.7381\n",
      "Episode:141 meanR:20.7700 R:37.0 rate:0.0740 loss:21.2199 exploreP:0.7355\n",
      "Episode:142 meanR:20.8800 R:39.0 rate:0.0780 loss:23.1210 exploreP:0.7326\n",
      "Episode:143 meanR:20.9200 R:20.0 rate:0.0400 loss:20.1584 exploreP:0.7312\n",
      "Episode:144 meanR:20.9300 R:17.0 rate:0.0340 loss:15.5484 exploreP:0.7300\n",
      "Episode:145 meanR:20.9100 R:14.0 rate:0.0280 loss:19.8153 exploreP:0.7290\n",
      "Episode:146 meanR:20.8400 R:38.0 rate:0.0760 loss:21.9138 exploreP:0.7262\n",
      "Episode:147 meanR:20.9500 R:24.0 rate:0.0480 loss:23.6136 exploreP:0.7245\n",
      "Episode:148 meanR:21.0100 R:19.0 rate:0.0380 loss:19.9859 exploreP:0.7232\n",
      "Episode:149 meanR:21.1900 R:28.0 rate:0.0560 loss:19.9682 exploreP:0.7212\n",
      "Episode:150 meanR:21.0900 R:13.0 rate:0.0260 loss:23.7242 exploreP:0.7202\n",
      "Episode:151 meanR:21.4300 R:62.0 rate:0.1240 loss:19.3607 exploreP:0.7159\n",
      "Episode:152 meanR:21.4100 R:16.0 rate:0.0320 loss:20.3885 exploreP:0.7147\n",
      "Episode:153 meanR:21.1200 R:52.0 rate:0.1040 loss:20.1080 exploreP:0.7111\n",
      "Episode:154 meanR:21.2600 R:30.0 rate:0.0600 loss:17.5066 exploreP:0.7090\n",
      "Episode:155 meanR:21.3900 R:30.0 rate:0.0600 loss:21.2811 exploreP:0.7069\n",
      "Episode:156 meanR:21.6600 R:44.0 rate:0.0880 loss:18.9983 exploreP:0.7038\n",
      "Episode:157 meanR:21.7100 R:39.0 rate:0.0780 loss:17.2980 exploreP:0.7011\n",
      "Episode:158 meanR:21.7600 R:17.0 rate:0.0340 loss:21.8865 exploreP:0.6999\n",
      "Episode:159 meanR:21.5900 R:17.0 rate:0.0340 loss:17.6740 exploreP:0.6988\n",
      "Episode:160 meanR:21.6400 R:17.0 rate:0.0340 loss:19.4887 exploreP:0.6976\n",
      "Episode:161 meanR:21.7500 R:40.0 rate:0.0800 loss:20.6600 exploreP:0.6949\n",
      "Episode:162 meanR:22.1500 R:58.0 rate:0.1160 loss:18.0938 exploreP:0.6909\n",
      "Episode:163 meanR:22.3300 R:38.0 rate:0.0760 loss:18.4334 exploreP:0.6883\n",
      "Episode:164 meanR:22.5100 R:28.0 rate:0.0560 loss:18.1069 exploreP:0.6864\n",
      "Episode:165 meanR:22.7500 R:46.0 rate:0.0920 loss:21.5736 exploreP:0.6833\n",
      "Episode:166 meanR:23.3300 R:80.0 rate:0.1600 loss:18.5961 exploreP:0.6779\n",
      "Episode:167 meanR:23.6500 R:58.0 rate:0.1160 loss:19.5358 exploreP:0.6741\n",
      "Episode:168 meanR:23.7900 R:55.0 rate:0.1100 loss:20.0824 exploreP:0.6704\n",
      "Episode:169 meanR:24.1000 R:44.0 rate:0.0880 loss:19.3433 exploreP:0.6675\n",
      "Episode:170 meanR:24.4100 R:45.0 rate:0.0900 loss:20.3612 exploreP:0.6646\n",
      "Episode:171 meanR:24.3400 R:14.0 rate:0.0280 loss:21.1854 exploreP:0.6637\n",
      "Episode:172 meanR:24.1500 R:13.0 rate:0.0260 loss:21.1610 exploreP:0.6628\n",
      "Episode:173 meanR:24.3800 R:38.0 rate:0.0760 loss:22.1747 exploreP:0.6603\n",
      "Episode:174 meanR:24.9400 R:70.0 rate:0.1400 loss:19.6202 exploreP:0.6558\n",
      "Episode:175 meanR:25.0200 R:22.0 rate:0.0440 loss:17.2823 exploreP:0.6544\n",
      "Episode:176 meanR:24.9700 R:22.0 rate:0.0440 loss:21.1980 exploreP:0.6530\n",
      "Episode:177 meanR:25.3200 R:54.0 rate:0.1080 loss:22.3012 exploreP:0.6495\n",
      "Episode:178 meanR:25.8000 R:59.0 rate:0.1180 loss:18.4582 exploreP:0.6457\n",
      "Episode:179 meanR:25.9200 R:36.0 rate:0.0720 loss:21.7004 exploreP:0.6435\n",
      "Episode:180 meanR:25.8900 R:11.0 rate:0.0220 loss:19.5041 exploreP:0.6428\n",
      "Episode:181 meanR:25.9900 R:20.0 rate:0.0400 loss:15.9609 exploreP:0.6415\n",
      "Episode:182 meanR:25.7500 R:15.0 rate:0.0300 loss:22.5913 exploreP:0.6406\n",
      "Episode:183 meanR:25.7500 R:33.0 rate:0.0660 loss:18.9540 exploreP:0.6385\n",
      "Episode:184 meanR:25.6800 R:12.0 rate:0.0240 loss:20.2562 exploreP:0.6377\n",
      "Episode:185 meanR:26.0700 R:62.0 rate:0.1240 loss:23.9144 exploreP:0.6338\n",
      "Episode:186 meanR:26.4800 R:54.0 rate:0.1080 loss:25.5824 exploreP:0.6305\n",
      "Episode:187 meanR:26.9000 R:52.0 rate:0.1040 loss:22.7330 exploreP:0.6273\n",
      "Episode:188 meanR:27.0200 R:31.0 rate:0.0620 loss:24.4479 exploreP:0.6254\n",
      "Episode:189 meanR:27.2100 R:34.0 rate:0.0680 loss:21.9341 exploreP:0.6233\n",
      "Episode:190 meanR:27.2900 R:20.0 rate:0.0400 loss:24.4106 exploreP:0.6220\n",
      "Episode:191 meanR:27.4000 R:22.0 rate:0.0440 loss:23.5088 exploreP:0.6207\n",
      "Episode:192 meanR:27.8600 R:55.0 rate:0.1100 loss:21.5801 exploreP:0.6173\n",
      "Episode:193 meanR:28.1900 R:57.0 rate:0.1140 loss:24.4870 exploreP:0.6139\n",
      "Episode:194 meanR:28.2500 R:32.0 rate:0.0640 loss:22.3854 exploreP:0.6120\n",
      "Episode:195 meanR:28.9000 R:77.0 rate:0.1540 loss:23.7938 exploreP:0.6074\n",
      "Episode:196 meanR:28.9900 R:18.0 rate:0.0360 loss:26.4340 exploreP:0.6063\n",
      "Episode:197 meanR:29.1100 R:24.0 rate:0.0480 loss:28.7629 exploreP:0.6048\n",
      "Episode:198 meanR:29.5000 R:65.0 rate:0.1300 loss:26.3133 exploreP:0.6010\n",
      "Episode:199 meanR:29.6000 R:45.0 rate:0.0900 loss:27.2959 exploreP:0.5983\n",
      "Episode:200 meanR:29.9900 R:49.0 rate:0.0980 loss:26.6527 exploreP:0.5955\n",
      "Episode:201 meanR:30.8100 R:95.0 rate:0.1900 loss:26.0775 exploreP:0.5899\n",
      "Episode:202 meanR:30.9900 R:41.0 rate:0.0820 loss:28.3905 exploreP:0.5876\n",
      "Episode:203 meanR:31.0700 R:23.0 rate:0.0460 loss:30.7445 exploreP:0.5862\n",
      "Episode:204 meanR:31.2200 R:28.0 rate:0.0560 loss:22.2995 exploreP:0.5846\n",
      "Episode:205 meanR:31.1200 R:24.0 rate:0.0480 loss:25.1518 exploreP:0.5832\n",
      "Episode:206 meanR:32.5100 R:162.0 rate:0.3240 loss:29.5897 exploreP:0.5740\n",
      "Episode:207 meanR:32.4800 R:25.0 rate:0.0500 loss:29.3036 exploreP:0.5726\n",
      "Episode:208 meanR:32.8600 R:47.0 rate:0.0940 loss:27.7316 exploreP:0.5700\n",
      "Episode:209 meanR:33.3800 R:67.0 rate:0.1340 loss:32.2713 exploreP:0.5662\n",
      "Episode:210 meanR:33.7600 R:47.0 rate:0.0940 loss:29.6242 exploreP:0.5636\n",
      "Episode:211 meanR:33.9900 R:37.0 rate:0.0740 loss:28.5907 exploreP:0.5616\n",
      "Episode:212 meanR:34.2700 R:41.0 rate:0.0820 loss:35.2138 exploreP:0.5593\n",
      "Episode:213 meanR:34.3100 R:19.0 rate:0.0380 loss:30.7523 exploreP:0.5583\n",
      "Episode:214 meanR:34.7600 R:65.0 rate:0.1300 loss:30.4085 exploreP:0.5547\n",
      "Episode:215 meanR:34.9400 R:38.0 rate:0.0760 loss:33.9422 exploreP:0.5527\n",
      "Episode:216 meanR:35.3500 R:58.0 rate:0.1160 loss:28.1110 exploreP:0.5495\n",
      "Episode:217 meanR:35.8800 R:72.0 rate:0.1440 loss:36.4826 exploreP:0.5457\n",
      "Episode:218 meanR:36.4100 R:84.0 rate:0.1680 loss:34.5450 exploreP:0.5412\n",
      "Episode:219 meanR:36.9600 R:67.0 rate:0.1340 loss:34.7223 exploreP:0.5376\n",
      "Episode:220 meanR:37.2400 R:47.0 rate:0.0940 loss:34.6560 exploreP:0.5352\n",
      "Episode:221 meanR:38.0700 R:101.0 rate:0.2020 loss:36.3346 exploreP:0.5299\n",
      "Episode:222 meanR:38.1300 R:30.0 rate:0.0600 loss:35.6170 exploreP:0.5283\n",
      "Episode:223 meanR:38.9800 R:99.0 rate:0.1980 loss:39.2511 exploreP:0.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:224 meanR:39.2000 R:48.0 rate:0.0960 loss:37.6460 exploreP:0.5208\n",
      "Episode:225 meanR:39.0700 R:32.0 rate:0.0640 loss:37.0249 exploreP:0.5191\n",
      "Episode:226 meanR:39.0400 R:20.0 rate:0.0400 loss:29.2746 exploreP:0.5181\n",
      "Episode:227 meanR:39.8400 R:94.0 rate:0.1880 loss:36.7479 exploreP:0.5134\n",
      "Episode:228 meanR:39.8700 R:25.0 rate:0.0500 loss:38.6711 exploreP:0.5121\n",
      "Episode:229 meanR:40.0800 R:34.0 rate:0.0680 loss:43.5493 exploreP:0.5104\n",
      "Episode:230 meanR:41.4000 R:153.0 rate:0.3060 loss:41.2304 exploreP:0.5028\n",
      "Episode:231 meanR:41.4100 R:36.0 rate:0.0720 loss:39.7982 exploreP:0.5010\n",
      "Episode:232 meanR:41.6300 R:47.0 rate:0.0940 loss:42.9969 exploreP:0.4987\n",
      "Episode:233 meanR:42.6800 R:129.0 rate:0.2580 loss:41.5443 exploreP:0.4925\n",
      "Episode:234 meanR:42.7200 R:23.0 rate:0.0460 loss:36.8471 exploreP:0.4914\n",
      "Episode:235 meanR:43.8700 R:141.0 rate:0.2820 loss:40.7908 exploreP:0.4846\n",
      "Episode:236 meanR:43.8700 R:17.0 rate:0.0340 loss:39.4979 exploreP:0.4838\n",
      "Episode:237 meanR:44.3700 R:87.0 rate:0.1740 loss:46.7372 exploreP:0.4797\n",
      "Episode:238 meanR:44.9400 R:77.0 rate:0.1540 loss:40.9171 exploreP:0.4761\n",
      "Episode:239 meanR:45.3800 R:58.0 rate:0.1160 loss:47.9055 exploreP:0.4734\n",
      "Episode:240 meanR:45.6200 R:43.0 rate:0.0860 loss:53.3717 exploreP:0.4714\n",
      "Episode:241 meanR:45.6500 R:40.0 rate:0.0800 loss:40.4001 exploreP:0.4696\n",
      "Episode:242 meanR:46.4100 R:115.0 rate:0.2300 loss:53.9999 exploreP:0.4643\n",
      "Episode:243 meanR:46.4100 R:20.0 rate:0.0400 loss:46.4135 exploreP:0.4634\n",
      "Episode:244 meanR:47.0100 R:77.0 rate:0.1540 loss:50.3553 exploreP:0.4599\n",
      "Episode:245 meanR:47.5900 R:72.0 rate:0.1440 loss:51.4397 exploreP:0.4567\n",
      "Episode:246 meanR:47.3100 R:10.0 rate:0.0200 loss:57.5092 exploreP:0.4563\n",
      "Episode:247 meanR:47.8600 R:79.0 rate:0.1580 loss:52.8722 exploreP:0.4527\n",
      "Episode:248 meanR:48.6000 R:93.0 rate:0.1860 loss:56.0211 exploreP:0.4487\n",
      "Episode:249 meanR:49.9600 R:164.0 rate:0.3280 loss:57.1082 exploreP:0.4415\n",
      "Episode:250 meanR:50.5000 R:67.0 rate:0.1340 loss:53.8795 exploreP:0.4386\n",
      "Episode:251 meanR:50.7700 R:89.0 rate:0.1780 loss:55.7913 exploreP:0.4348\n",
      "Episode:252 meanR:50.9800 R:37.0 rate:0.0740 loss:55.0376 exploreP:0.4333\n",
      "Episode:253 meanR:51.2800 R:82.0 rate:0.1640 loss:61.2190 exploreP:0.4298\n",
      "Episode:254 meanR:51.8700 R:89.0 rate:0.1780 loss:62.6007 exploreP:0.4261\n",
      "Episode:255 meanR:52.2100 R:64.0 rate:0.1280 loss:54.6120 exploreP:0.4234\n",
      "Episode:256 meanR:52.1500 R:38.0 rate:0.0760 loss:61.0469 exploreP:0.4219\n",
      "Episode:257 meanR:52.4400 R:68.0 rate:0.1360 loss:52.6303 exploreP:0.4191\n",
      "Episode:258 meanR:52.8400 R:57.0 rate:0.1140 loss:68.4189 exploreP:0.4168\n",
      "Episode:259 meanR:53.0000 R:33.0 rate:0.0660 loss:57.7701 exploreP:0.4154\n",
      "Episode:260 meanR:53.5200 R:69.0 rate:0.1380 loss:72.9050 exploreP:0.4126\n",
      "Episode:261 meanR:53.7200 R:60.0 rate:0.1200 loss:61.9832 exploreP:0.4102\n",
      "Episode:262 meanR:53.8800 R:74.0 rate:0.1480 loss:62.6852 exploreP:0.4073\n",
      "Episode:263 meanR:54.2200 R:72.0 rate:0.1440 loss:62.9305 exploreP:0.4044\n",
      "Episode:264 meanR:54.3900 R:45.0 rate:0.0900 loss:67.6993 exploreP:0.4026\n",
      "Episode:265 meanR:54.4900 R:56.0 rate:0.1120 loss:73.4629 exploreP:0.4005\n",
      "Episode:266 meanR:55.1400 R:145.0 rate:0.2900 loss:68.4425 exploreP:0.3948\n",
      "Episode:267 meanR:55.5300 R:97.0 rate:0.1940 loss:74.2461 exploreP:0.3911\n",
      "Episode:268 meanR:55.6000 R:62.0 rate:0.1240 loss:76.5799 exploreP:0.3888\n",
      "Episode:269 meanR:55.9000 R:74.0 rate:0.1480 loss:75.1104 exploreP:0.3860\n",
      "Episode:270 meanR:56.2000 R:75.0 rate:0.1500 loss:71.6369 exploreP:0.3832\n",
      "Episode:271 meanR:56.6400 R:58.0 rate:0.1160 loss:70.0661 exploreP:0.3810\n",
      "Episode:272 meanR:57.5100 R:100.0 rate:0.2000 loss:73.5455 exploreP:0.3773\n",
      "Episode:273 meanR:59.0800 R:195.0 rate:0.3900 loss:77.1148 exploreP:0.3702\n",
      "Episode:274 meanR:59.1200 R:74.0 rate:0.1480 loss:75.7451 exploreP:0.3676\n",
      "Episode:275 meanR:59.4200 R:52.0 rate:0.1040 loss:70.2224 exploreP:0.3657\n",
      "Episode:276 meanR:59.8400 R:64.0 rate:0.1280 loss:79.7702 exploreP:0.3634\n",
      "Episode:277 meanR:60.0800 R:78.0 rate:0.1560 loss:71.5675 exploreP:0.3607\n",
      "Episode:278 meanR:61.3200 R:183.0 rate:0.3660 loss:80.1842 exploreP:0.3543\n",
      "Episode:279 meanR:61.8800 R:92.0 rate:0.1840 loss:96.8868 exploreP:0.3512\n",
      "Episode:280 meanR:61.9500 R:18.0 rate:0.0360 loss:88.2870 exploreP:0.3506\n",
      "Episode:281 meanR:62.6600 R:91.0 rate:0.1820 loss:93.8983 exploreP:0.3475\n",
      "Episode:282 meanR:64.1900 R:168.0 rate:0.3360 loss:90.4221 exploreP:0.3419\n",
      "Episode:283 meanR:65.3900 R:153.0 rate:0.3060 loss:90.9634 exploreP:0.3368\n",
      "Episode:284 meanR:66.0800 R:81.0 rate:0.1620 loss:90.2795 exploreP:0.3342\n",
      "Episode:285 meanR:66.7500 R:129.0 rate:0.2580 loss:94.2648 exploreP:0.3300\n",
      "Episode:286 meanR:67.1800 R:97.0 rate:0.1940 loss:101.6349 exploreP:0.3269\n",
      "Episode:287 meanR:68.1600 R:150.0 rate:0.3000 loss:94.2429 exploreP:0.3222\n",
      "Episode:288 meanR:68.6100 R:76.0 rate:0.1520 loss:97.4117 exploreP:0.3199\n",
      "Episode:289 meanR:68.9800 R:71.0 rate:0.1420 loss:97.2859 exploreP:0.3177\n",
      "Episode:290 meanR:70.0400 R:126.0 rate:0.2520 loss:105.0156 exploreP:0.3138\n",
      "Episode:291 meanR:70.6600 R:84.0 rate:0.1680 loss:98.6300 exploreP:0.3113\n",
      "Episode:292 meanR:72.3200 R:221.0 rate:0.4420 loss:103.8151 exploreP:0.3047\n",
      "Episode:293 meanR:72.6500 R:90.0 rate:0.1800 loss:106.8028 exploreP:0.3020\n",
      "Episode:294 meanR:73.2300 R:90.0 rate:0.1800 loss:107.4663 exploreP:0.2994\n",
      "Episode:295 meanR:73.9300 R:147.0 rate:0.2940 loss:113.5221 exploreP:0.2952\n",
      "Episode:296 meanR:75.8500 R:210.0 rate:0.4200 loss:110.8582 exploreP:0.2893\n",
      "Episode:297 meanR:77.1400 R:153.0 rate:0.3060 loss:119.4262 exploreP:0.2850\n",
      "Episode:298 meanR:77.7300 R:124.0 rate:0.2480 loss:114.3053 exploreP:0.2816\n",
      "Episode:299 meanR:78.4600 R:118.0 rate:0.2360 loss:124.1479 exploreP:0.2785\n",
      "Episode:300 meanR:78.8300 R:86.0 rate:0.1720 loss:139.9577 exploreP:0.2762\n",
      "Episode:301 meanR:79.5400 R:166.0 rate:0.3320 loss:128.9227 exploreP:0.2718\n",
      "Episode:302 meanR:80.9300 R:180.0 rate:0.3600 loss:122.1891 exploreP:0.2671\n",
      "Episode:303 meanR:83.8400 R:314.0 rate:0.6280 loss:135.7111 exploreP:0.2592\n",
      "Episode:304 meanR:85.4600 R:190.0 rate:0.3800 loss:117.3632 exploreP:0.2545\n",
      "Episode:305 meanR:86.6100 R:139.0 rate:0.2780 loss:127.4593 exploreP:0.2511\n",
      "Episode:306 meanR:86.0600 R:107.0 rate:0.2140 loss:119.4282 exploreP:0.2485\n",
      "Episode:307 meanR:87.6700 R:186.0 rate:0.3720 loss:128.8586 exploreP:0.2441\n",
      "Episode:308 meanR:90.1800 R:298.0 rate:0.5960 loss:148.0254 exploreP:0.2373\n",
      "Episode:309 meanR:91.0100 R:150.0 rate:0.3000 loss:138.4832 exploreP:0.2339\n",
      "Episode:310 meanR:92.9200 R:238.0 rate:0.4760 loss:153.7958 exploreP:0.2286\n",
      "Episode:311 meanR:94.2600 R:171.0 rate:0.3420 loss:157.5719 exploreP:0.2249\n",
      "Episode:312 meanR:95.9700 R:212.0 rate:0.4240 loss:145.7289 exploreP:0.2204\n",
      "Episode:313 meanR:97.5300 R:175.0 rate:0.3500 loss:175.5615 exploreP:0.2167\n",
      "Episode:314 meanR:99.1700 R:229.0 rate:0.4580 loss:151.7656 exploreP:0.2121\n",
      "Episode:315 meanR:100.5200 R:173.0 rate:0.3460 loss:167.7958 exploreP:0.2086\n",
      "Episode:316 meanR:104.9000 R:496.0 rate:0.9920 loss:171.5308 exploreP:0.1990\n",
      "Episode:317 meanR:105.7200 R:154.0 rate:0.3080 loss:134.7696 exploreP:0.1961\n",
      "Episode:318 meanR:106.7500 R:187.0 rate:0.3740 loss:124.5085 exploreP:0.1927\n",
      "Episode:319 meanR:108.0300 R:195.0 rate:0.3900 loss:144.4841 exploreP:0.1891\n",
      "Episode:320 meanR:112.5600 R:500.0 rate:1.0000 loss:138.0520 exploreP:0.1804\n",
      "Episode:321 meanR:114.1100 R:256.0 rate:0.5120 loss:129.8647 exploreP:0.1761\n",
      "Episode:322 meanR:115.8600 R:205.0 rate:0.4100 loss:147.7549 exploreP:0.1727\n",
      "Episode:323 meanR:117.1200 R:225.0 rate:0.4500 loss:120.0743 exploreP:0.1691\n",
      "Episode:324 meanR:118.6100 R:197.0 rate:0.3940 loss:143.4208 exploreP:0.1660\n",
      "Episode:325 meanR:120.3500 R:206.0 rate:0.4120 loss:143.8573 exploreP:0.1628\n",
      "Episode:326 meanR:123.3100 R:316.0 rate:0.6320 loss:140.3734 exploreP:0.1581\n",
      "Episode:327 meanR:127.3700 R:500.0 rate:1.0000 loss:124.2233 exploreP:0.1508\n",
      "Episode:328 meanR:129.7500 R:263.0 rate:0.5260 loss:145.3938 exploreP:0.1472\n",
      "Episode:329 meanR:132.0700 R:266.0 rate:0.5320 loss:126.6397 exploreP:0.1436\n",
      "Episode:330 meanR:133.1600 R:262.0 rate:0.5240 loss:142.2985 exploreP:0.1401\n",
      "Episode:331 meanR:135.1300 R:233.0 rate:0.4660 loss:129.4164 exploreP:0.1371\n",
      "Episode:332 meanR:139.1300 R:447.0 rate:0.8940 loss:129.0019 exploreP:0.1316\n",
      "Episode:333 meanR:140.4400 R:260.0 rate:0.5200 loss:135.8932 exploreP:0.1285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:334 meanR:145.2100 R:500.0 rate:1.0000 loss:142.1698 exploreP:0.1227\n",
      "Episode:335 meanR:148.6400 R:484.0 rate:0.9680 loss:133.6314 exploreP:0.1174\n",
      "Episode:336 meanR:151.6500 R:318.0 rate:0.6360 loss:141.8802 exploreP:0.1140\n",
      "Episode:337 meanR:154.5100 R:373.0 rate:0.7460 loss:137.5766 exploreP:0.1102\n",
      "Episode:338 meanR:156.6800 R:294.0 rate:0.5880 loss:143.7679 exploreP:0.1073\n",
      "Episode:339 meanR:159.2600 R:316.0 rate:0.6320 loss:128.4631 exploreP:0.1043\n",
      "Episode:340 meanR:163.4200 R:459.0 rate:0.9180 loss:139.1361 exploreP:0.1000\n",
      "Episode:341 meanR:165.7700 R:275.0 rate:0.5500 loss:120.8211 exploreP:0.0976\n",
      "Episode:342 meanR:168.1600 R:354.0 rate:0.7080 loss:134.5438 exploreP:0.0945\n",
      "Episode:343 meanR:171.8700 R:391.0 rate:0.7820 loss:134.4934 exploreP:0.0913\n",
      "Episode:344 meanR:174.6100 R:351.0 rate:0.7020 loss:134.0343 exploreP:0.0885\n",
      "Episode:345 meanR:177.2500 R:336.0 rate:0.6720 loss:135.8453 exploreP:0.0859\n",
      "Episode:346 meanR:180.4600 R:331.0 rate:0.6620 loss:130.4309 exploreP:0.0834\n",
      "Episode:347 meanR:182.8300 R:316.0 rate:0.6320 loss:127.2129 exploreP:0.0811\n",
      "Episode:348 meanR:185.0300 R:313.0 rate:0.6260 loss:122.7723 exploreP:0.0790\n",
      "Episode:349 meanR:186.6600 R:327.0 rate:0.6540 loss:115.4944 exploreP:0.0767\n",
      "Episode:350 meanR:188.6200 R:263.0 rate:0.5260 loss:127.1309 exploreP:0.0750\n",
      "Episode:351 meanR:191.7000 R:397.0 rate:0.7940 loss:120.0783 exploreP:0.0725\n",
      "Episode:352 meanR:194.6600 R:333.0 rate:0.6660 loss:104.9304 exploreP:0.0704\n",
      "Episode:353 meanR:196.6900 R:285.0 rate:0.5700 loss:112.7838 exploreP:0.0687\n",
      "Episode:354 meanR:198.6100 R:281.0 rate:0.5620 loss:93.8074 exploreP:0.0671\n",
      "Episode:355 meanR:201.7500 R:378.0 rate:0.7560 loss:101.3481 exploreP:0.0650\n",
      "Episode:356 meanR:204.9400 R:357.0 rate:0.7140 loss:95.7356 exploreP:0.0631\n",
      "Episode:357 meanR:207.5000 R:324.0 rate:0.6480 loss:103.3196 exploreP:0.0614\n",
      "Episode:358 meanR:210.6900 R:376.0 rate:0.7520 loss:104.3838 exploreP:0.0595\n",
      "Episode:359 meanR:213.3700 R:301.0 rate:0.6020 loss:94.4946 exploreP:0.0580\n",
      "Episode:360 meanR:215.9200 R:324.0 rate:0.6480 loss:91.4698 exploreP:0.0565\n",
      "Episode:361 meanR:218.7100 R:339.0 rate:0.6780 loss:102.7581 exploreP:0.0549\n",
      "Episode:362 meanR:221.0400 R:307.0 rate:0.6140 loss:92.7334 exploreP:0.0536\n",
      "Episode:363 meanR:224.1800 R:386.0 rate:0.7720 loss:86.8864 exploreP:0.0519\n",
      "Episode:364 meanR:227.2800 R:355.0 rate:0.7100 loss:93.7506 exploreP:0.0505\n",
      "Episode:365 meanR:229.6900 R:297.0 rate:0.5940 loss:104.3978 exploreP:0.0493\n",
      "Episode:366 meanR:231.9600 R:372.0 rate:0.7440 loss:118.6713 exploreP:0.0478\n",
      "Episode:367 meanR:234.6700 R:368.0 rate:0.7360 loss:133.8206 exploreP:0.0465\n",
      "Episode:368 meanR:237.4200 R:337.0 rate:0.6740 loss:164.6769 exploreP:0.0453\n",
      "Episode:369 meanR:240.6300 R:395.0 rate:0.7900 loss:165.9063 exploreP:0.0439\n",
      "Episode:370 meanR:242.5400 R:266.0 rate:0.5320 loss:193.9210 exploreP:0.0430\n",
      "Episode:371 meanR:244.9500 R:299.0 rate:0.5980 loss:214.1904 exploreP:0.0420\n",
      "Episode:372 meanR:246.9300 R:298.0 rate:0.5960 loss:307.5281 exploreP:0.0411\n",
      "Episode:373 meanR:248.2000 R:322.0 rate:0.6440 loss:394.2877 exploreP:0.0401\n",
      "Episode:374 meanR:251.1700 R:371.0 rate:0.7420 loss:555.5537 exploreP:0.0390\n",
      "Episode:375 meanR:254.7200 R:407.0 rate:0.8140 loss:668.4905 exploreP:0.0379\n",
      "Episode:376 meanR:258.0600 R:398.0 rate:0.7960 loss:916.8282 exploreP:0.0368\n",
      "Episode:377 meanR:261.0700 R:379.0 rate:0.7580 loss:1112.0391 exploreP:0.0358\n",
      "Episode:378 meanR:263.0200 R:378.0 rate:0.7560 loss:1238.5251 exploreP:0.0348\n",
      "Episode:379 meanR:265.6300 R:353.0 rate:0.7060 loss:1519.3948 exploreP:0.0340\n",
      "Episode:380 meanR:270.2400 R:479.0 rate:0.9580 loss:1934.5873 exploreP:0.0328\n",
      "Episode:381 meanR:273.5900 R:426.0 rate:0.8520 loss:2386.5464 exploreP:0.0319\n",
      "Episode:382 meanR:274.8700 R:296.0 rate:0.5920 loss:2765.7039 exploreP:0.0312\n",
      "Episode:383 meanR:276.9700 R:363.0 rate:0.7260 loss:3032.3220 exploreP:0.0305\n",
      "Episode:384 meanR:280.4200 R:426.0 rate:0.8520 loss:3452.6807 exploreP:0.0296\n",
      "Episode:385 meanR:284.1300 R:500.0 rate:1.0000 loss:3844.8311 exploreP:0.0287\n",
      "Episode:386 meanR:286.7800 R:362.0 rate:0.7240 loss:4231.8086 exploreP:0.0280\n",
      "Episode:387 meanR:288.9300 R:365.0 rate:0.7300 loss:4733.1689 exploreP:0.0274\n",
      "Episode:388 meanR:293.1700 R:500.0 rate:1.0000 loss:5160.3379 exploreP:0.0265\n",
      "Episode:389 meanR:295.1700 R:271.0 rate:0.5420 loss:5735.4448 exploreP:0.0261\n",
      "Episode:390 meanR:298.9100 R:500.0 rate:1.0000 loss:6241.1362 exploreP:0.0253\n",
      "Episode:391 meanR:302.2100 R:414.0 rate:0.8280 loss:6275.7173 exploreP:0.0247\n",
      "Episode:392 meanR:305.0000 R:500.0 rate:1.0000 loss:7281.3247 exploreP:0.0240\n",
      "Episode:393 meanR:307.4900 R:339.0 rate:0.6780 loss:7627.8726 exploreP:0.0235\n",
      "Episode:394 meanR:309.4800 R:289.0 rate:0.5780 loss:7777.2871 exploreP:0.0231\n",
      "Episode:395 meanR:312.3300 R:432.0 rate:0.8640 loss:8063.5947 exploreP:0.0226\n",
      "Episode:396 meanR:313.6600 R:343.0 rate:0.6860 loss:7640.8804 exploreP:0.0221\n",
      "Episode:397 meanR:317.1300 R:500.0 rate:1.0000 loss:8021.0869 exploreP:0.0215\n",
      "Episode:398 meanR:318.7300 R:284.0 rate:0.5680 loss:7803.6187 exploreP:0.0212\n",
      "Episode:399 meanR:320.2900 R:274.0 rate:0.5480 loss:6824.4233 exploreP:0.0209\n",
      "Episode:400 meanR:322.4700 R:304.0 rate:0.6080 loss:6701.5112 exploreP:0.0206\n",
      "Episode:401 meanR:324.6800 R:387.0 rate:0.7740 loss:6969.7559 exploreP:0.0202\n",
      "Episode:402 meanR:325.9500 R:307.0 rate:0.6140 loss:6202.7852 exploreP:0.0199\n",
      "Episode:403 meanR:325.8700 R:306.0 rate:0.6120 loss:6825.3335 exploreP:0.0196\n",
      "Episode:404 meanR:327.6900 R:372.0 rate:0.7440 loss:6901.9424 exploreP:0.0192\n",
      "Episode:405 meanR:329.2800 R:298.0 rate:0.5960 loss:6929.4756 exploreP:0.0190\n",
      "Episode:406 meanR:331.2700 R:306.0 rate:0.6120 loss:7220.1445 exploreP:0.0187\n",
      "Episode:407 meanR:333.4400 R:403.0 rate:0.8060 loss:7574.8530 exploreP:0.0183\n",
      "Episode:408 meanR:334.2000 R:374.0 rate:0.7480 loss:7382.1201 exploreP:0.0180\n",
      "Episode:409 meanR:335.9800 R:328.0 rate:0.6560 loss:7699.0112 exploreP:0.0178\n",
      "Episode:410 meanR:337.9200 R:432.0 rate:0.8640 loss:8096.1665 exploreP:0.0174\n",
      "Episode:411 meanR:339.6300 R:342.0 rate:0.6840 loss:8296.0264 exploreP:0.0172\n",
      "Episode:412 meanR:341.3700 R:386.0 rate:0.7720 loss:7890.4824 exploreP:0.0169\n",
      "Episode:413 meanR:344.3000 R:468.0 rate:0.9360 loss:8756.6367 exploreP:0.0166\n",
      "Episode:414 meanR:347.0100 R:500.0 rate:1.0000 loss:8465.9443 exploreP:0.0163\n",
      "Episode:415 meanR:350.2800 R:500.0 rate:1.0000 loss:9884.4678 exploreP:0.0160\n",
      "Episode:416 meanR:350.3200 R:500.0 rate:1.0000 loss:10588.9688 exploreP:0.0157\n",
      "Episode:417 meanR:353.7800 R:500.0 rate:1.0000 loss:9810.0078 exploreP:0.0154\n",
      "Episode:418 meanR:356.9100 R:500.0 rate:1.0000 loss:6484.3745 exploreP:0.0151\n",
      "Episode:419 meanR:359.9600 R:500.0 rate:1.0000 loss:6878.8970 exploreP:0.0149\n",
      "Episode:420 meanR:359.9600 R:500.0 rate:1.0000 loss:6632.9287 exploreP:0.0147\n",
      "Episode:421 meanR:362.4000 R:500.0 rate:1.0000 loss:5306.9580 exploreP:0.0144\n",
      "Episode:422 meanR:365.3500 R:500.0 rate:1.0000 loss:4210.4526 exploreP:0.0142\n",
      "Episode:423 meanR:368.1000 R:500.0 rate:1.0000 loss:3716.6895 exploreP:0.0140\n",
      "Episode:424 meanR:371.1300 R:500.0 rate:1.0000 loss:3784.8867 exploreP:0.0138\n",
      "Episode:425 meanR:374.0700 R:500.0 rate:1.0000 loss:5313.5122 exploreP:0.0136\n",
      "Episode:426 meanR:375.9100 R:500.0 rate:1.0000 loss:6436.5327 exploreP:0.0135\n",
      "Episode:427 meanR:375.9100 R:500.0 rate:1.0000 loss:6398.4009 exploreP:0.0133\n",
      "Episode:428 meanR:378.2800 R:500.0 rate:1.0000 loss:6211.7271 exploreP:0.0131\n",
      "Episode:429 meanR:380.6200 R:500.0 rate:1.0000 loss:6480.0371 exploreP:0.0130\n",
      "Episode:430 meanR:383.0000 R:500.0 rate:1.0000 loss:6409.2656 exploreP:0.0128\n",
      "Episode:431 meanR:385.6700 R:500.0 rate:1.0000 loss:6724.6743 exploreP:0.0127\n",
      "Episode:432 meanR:386.2000 R:500.0 rate:1.0000 loss:6629.6382 exploreP:0.0126\n",
      "Episode:433 meanR:387.6300 R:403.0 rate:0.8060 loss:6369.2983 exploreP:0.0125\n",
      "Episode:434 meanR:386.1800 R:355.0 rate:0.7100 loss:5939.4702 exploreP:0.0124\n",
      "Episode:435 meanR:384.9200 R:358.0 rate:0.7160 loss:5612.6221 exploreP:0.0123\n",
      "Episode:436 meanR:385.1200 R:338.0 rate:0.6760 loss:5651.5542 exploreP:0.0122\n",
      "Episode:437 meanR:384.9600 R:357.0 rate:0.7140 loss:5801.1709 exploreP:0.0121\n",
      "Episode:438 meanR:385.4500 R:343.0 rate:0.6860 loss:4981.9248 exploreP:0.0121\n",
      "Episode:439 meanR:385.5100 R:322.0 rate:0.6440 loss:5198.4634 exploreP:0.0120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:440 meanR:384.1000 R:318.0 rate:0.6360 loss:5196.6714 exploreP:0.0119\n",
      "Episode:441 meanR:384.2300 R:288.0 rate:0.5760 loss:5102.3535 exploreP:0.0119\n",
      "Episode:442 meanR:383.5500 R:286.0 rate:0.5720 loss:5353.8452 exploreP:0.0118\n",
      "Episode:443 meanR:382.8300 R:319.0 rate:0.6380 loss:4983.0508 exploreP:0.0118\n",
      "Episode:444 meanR:382.3100 R:299.0 rate:0.5980 loss:5171.4214 exploreP:0.0117\n",
      "Episode:445 meanR:382.2200 R:327.0 rate:0.6540 loss:5001.4434 exploreP:0.0117\n",
      "Episode:446 meanR:381.9300 R:302.0 rate:0.6040 loss:4895.3330 exploreP:0.0116\n",
      "Episode:447 meanR:381.7300 R:296.0 rate:0.5920 loss:5177.1079 exploreP:0.0116\n",
      "Episode:448 meanR:381.4600 R:286.0 rate:0.5720 loss:4781.8374 exploreP:0.0115\n",
      "Episode:449 meanR:381.3100 R:312.0 rate:0.6240 loss:4722.6738 exploreP:0.0115\n",
      "Episode:450 meanR:381.6000 R:292.0 rate:0.5840 loss:4786.5044 exploreP:0.0114\n",
      "Episode:451 meanR:380.3200 R:269.0 rate:0.5380 loss:4599.6992 exploreP:0.0114\n",
      "Episode:452 meanR:379.5400 R:255.0 rate:0.5100 loss:4908.1685 exploreP:0.0114\n",
      "Episode:453 meanR:379.8300 R:314.0 rate:0.6280 loss:4896.1641 exploreP:0.0113\n",
      "Episode:454 meanR:380.1800 R:316.0 rate:0.6320 loss:5076.0142 exploreP:0.0113\n",
      "Episode:455 meanR:379.0900 R:269.0 rate:0.5380 loss:4989.1069 exploreP:0.0112\n",
      "Episode:456 meanR:378.5700 R:305.0 rate:0.6100 loss:4698.3696 exploreP:0.0112\n",
      "Episode:457 meanR:378.0000 R:267.0 rate:0.5340 loss:4532.1348 exploreP:0.0112\n",
      "Episode:458 meanR:376.7200 R:248.0 rate:0.4960 loss:4273.3442 exploreP:0.0111\n",
      "Episode:459 meanR:376.2600 R:255.0 rate:0.5100 loss:4400.2354 exploreP:0.0111\n",
      "Episode:460 meanR:375.8800 R:286.0 rate:0.5720 loss:4387.6782 exploreP:0.0111\n",
      "Episode:461 meanR:375.0900 R:260.0 rate:0.5200 loss:3876.3203 exploreP:0.0111\n",
      "Episode:462 meanR:374.8500 R:283.0 rate:0.5660 loss:4389.5493 exploreP:0.0110\n",
      "Episode:463 meanR:373.8200 R:283.0 rate:0.5660 loss:3995.4819 exploreP:0.0110\n",
      "Episode:464 meanR:373.1700 R:290.0 rate:0.5800 loss:4129.1040 exploreP:0.0110\n",
      "Episode:465 meanR:372.9200 R:272.0 rate:0.5440 loss:4225.0010 exploreP:0.0109\n",
      "Episode:466 meanR:371.3900 R:219.0 rate:0.4380 loss:4161.0073 exploreP:0.0109\n",
      "Episode:467 meanR:370.2400 R:253.0 rate:0.5060 loss:4120.5938 exploreP:0.0109\n",
      "Episode:468 meanR:369.2800 R:241.0 rate:0.4820 loss:4112.8843 exploreP:0.0109\n",
      "Episode:469 meanR:367.7900 R:246.0 rate:0.4920 loss:3975.2600 exploreP:0.0109\n",
      "Episode:470 meanR:368.0500 R:292.0 rate:0.5840 loss:4021.5359 exploreP:0.0108\n",
      "Episode:471 meanR:367.9800 R:292.0 rate:0.5840 loss:4430.0991 exploreP:0.0108\n",
      "Episode:472 meanR:367.1900 R:219.0 rate:0.4380 loss:4132.2646 exploreP:0.0108\n",
      "Episode:473 meanR:366.4800 R:251.0 rate:0.5020 loss:4317.4844 exploreP:0.0108\n",
      "Episode:474 meanR:365.0600 R:229.0 rate:0.4580 loss:4114.6045 exploreP:0.0108\n",
      "Episode:475 meanR:363.5400 R:255.0 rate:0.5100 loss:4282.3750 exploreP:0.0107\n",
      "Episode:476 meanR:361.6100 R:205.0 rate:0.4100 loss:4068.7336 exploreP:0.0107\n",
      "Episode:477 meanR:360.6100 R:279.0 rate:0.5580 loss:4225.2480 exploreP:0.0107\n",
      "Episode:478 meanR:359.2900 R:246.0 rate:0.4920 loss:4212.7817 exploreP:0.0107\n",
      "Episode:479 meanR:358.4300 R:267.0 rate:0.5340 loss:4124.0396 exploreP:0.0107\n",
      "Episode:480 meanR:356.1300 R:249.0 rate:0.4980 loss:4033.0928 exploreP:0.0106\n",
      "Episode:481 meanR:354.1700 R:230.0 rate:0.4600 loss:4095.2717 exploreP:0.0106\n",
      "Episode:482 meanR:353.6600 R:245.0 rate:0.4900 loss:4082.5491 exploreP:0.0106\n",
      "Episode:483 meanR:352.5000 R:247.0 rate:0.4940 loss:4397.2583 exploreP:0.0106\n",
      "Episode:484 meanR:351.0300 R:279.0 rate:0.5580 loss:4345.3779 exploreP:0.0106\n",
      "Episode:485 meanR:348.5900 R:256.0 rate:0.5120 loss:4197.5249 exploreP:0.0106\n",
      "Episode:486 meanR:347.1100 R:214.0 rate:0.4280 loss:4480.9058 exploreP:0.0106\n",
      "Episode:487 meanR:346.2300 R:277.0 rate:0.5540 loss:4229.5210 exploreP:0.0105\n",
      "Episode:488 meanR:343.7500 R:252.0 rate:0.5040 loss:4332.7988 exploreP:0.0105\n",
      "Episode:489 meanR:344.3400 R:330.0 rate:0.6600 loss:4523.2896 exploreP:0.0105\n",
      "Episode:490 meanR:341.5700 R:223.0 rate:0.4460 loss:4955.1670 exploreP:0.0105\n",
      "Episode:491 meanR:340.3400 R:291.0 rate:0.5820 loss:4309.2788 exploreP:0.0105\n",
      "Episode:492 meanR:337.6400 R:230.0 rate:0.4600 loss:4346.7900 exploreP:0.0105\n",
      "Episode:493 meanR:336.8100 R:256.0 rate:0.5120 loss:4472.9814 exploreP:0.0105\n",
      "Episode:494 meanR:336.4000 R:248.0 rate:0.4960 loss:4394.0527 exploreP:0.0105\n",
      "Episode:495 meanR:334.5100 R:243.0 rate:0.4860 loss:4728.1074 exploreP:0.0104\n",
      "Episode:496 meanR:333.2500 R:217.0 rate:0.4340 loss:4472.8486 exploreP:0.0104\n",
      "Episode:497 meanR:331.1700 R:292.0 rate:0.5840 loss:4501.5493 exploreP:0.0104\n",
      "Episode:498 meanR:330.7300 R:240.0 rate:0.4800 loss:4370.3672 exploreP:0.0104\n",
      "Episode:499 meanR:330.5800 R:259.0 rate:0.5180 loss:4661.8086 exploreP:0.0104\n",
      "Episode:500 meanR:330.2700 R:273.0 rate:0.5460 loss:4788.0391 exploreP:0.0104\n",
      "Episode:501 meanR:329.2900 R:289.0 rate:0.5780 loss:4760.7007 exploreP:0.0104\n",
      "Episode:502 meanR:328.4300 R:221.0 rate:0.4420 loss:4697.2739 exploreP:0.0104\n",
      "Episode:503 meanR:328.2000 R:283.0 rate:0.5660 loss:5004.7998 exploreP:0.0104\n",
      "Episode:504 meanR:326.7000 R:222.0 rate:0.4440 loss:4546.1782 exploreP:0.0104\n",
      "Episode:505 meanR:326.1100 R:239.0 rate:0.4780 loss:4924.1172 exploreP:0.0103\n",
      "Episode:506 meanR:325.5500 R:250.0 rate:0.5000 loss:4788.2041 exploreP:0.0103\n",
      "Episode:507 meanR:324.2800 R:276.0 rate:0.5520 loss:5008.1865 exploreP:0.0103\n",
      "Episode:508 meanR:322.9200 R:238.0 rate:0.4760 loss:5219.4331 exploreP:0.0103\n",
      "Episode:509 meanR:321.8800 R:224.0 rate:0.4480 loss:5139.9492 exploreP:0.0103\n",
      "Episode:510 meanR:319.9900 R:243.0 rate:0.4860 loss:5335.0557 exploreP:0.0103\n",
      "Episode:511 meanR:319.1300 R:256.0 rate:0.5120 loss:5023.3369 exploreP:0.0103\n",
      "Episode:512 meanR:318.1700 R:290.0 rate:0.5800 loss:5210.8545 exploreP:0.0103\n",
      "Episode:513 meanR:316.2600 R:277.0 rate:0.5540 loss:5333.3691 exploreP:0.0103\n",
      "Episode:514 meanR:313.7200 R:246.0 rate:0.4920 loss:5128.4048 exploreP:0.0103\n",
      "Episode:515 meanR:311.2100 R:249.0 rate:0.4980 loss:5363.9209 exploreP:0.0103\n",
      "Episode:516 meanR:308.7800 R:257.0 rate:0.5140 loss:5227.2197 exploreP:0.0103\n",
      "Episode:517 meanR:306.1000 R:232.0 rate:0.4640 loss:5770.8018 exploreP:0.0103\n",
      "Episode:518 meanR:303.3700 R:227.0 rate:0.4540 loss:5555.9956 exploreP:0.0102\n",
      "Episode:519 meanR:301.3100 R:294.0 rate:0.5880 loss:5832.8779 exploreP:0.0102\n",
      "Episode:520 meanR:299.0300 R:272.0 rate:0.5440 loss:5798.4688 exploreP:0.0102\n",
      "Episode:521 meanR:296.8300 R:280.0 rate:0.5600 loss:5775.0918 exploreP:0.0102\n",
      "Episode:522 meanR:294.7900 R:296.0 rate:0.5920 loss:6326.9175 exploreP:0.0102\n",
      "Episode:523 meanR:293.0800 R:329.0 rate:0.6580 loss:6605.0503 exploreP:0.0102\n",
      "Episode:524 meanR:290.8400 R:276.0 rate:0.5520 loss:6460.2988 exploreP:0.0102\n",
      "Episode:525 meanR:289.0900 R:325.0 rate:0.6500 loss:6611.5200 exploreP:0.0102\n",
      "Episode:526 meanR:286.6500 R:256.0 rate:0.5120 loss:7045.5703 exploreP:0.0102\n",
      "Episode:527 meanR:284.3500 R:270.0 rate:0.5400 loss:8020.3398 exploreP:0.0102\n",
      "Episode:528 meanR:283.8700 R:452.0 rate:0.9040 loss:7704.4258 exploreP:0.0102\n",
      "Episode:529 meanR:283.2000 R:433.0 rate:0.8660 loss:8385.8359 exploreP:0.0102\n",
      "Episode:530 meanR:283.2000 R:500.0 rate:1.0000 loss:8459.1826 exploreP:0.0102\n",
      "Episode:531 meanR:281.2600 R:306.0 rate:0.6120 loss:8808.4492 exploreP:0.0102\n",
      "Episode:532 meanR:281.2600 R:500.0 rate:1.0000 loss:8611.3301 exploreP:0.0102\n",
      "Episode:533 meanR:282.2300 R:500.0 rate:1.0000 loss:8598.5703 exploreP:0.0101\n",
      "Episode:534 meanR:283.1500 R:447.0 rate:0.8940 loss:8912.4980 exploreP:0.0101\n",
      "Episode:535 meanR:283.2000 R:363.0 rate:0.7260 loss:8671.2930 exploreP:0.0101\n",
      "Episode:536 meanR:283.2500 R:343.0 rate:0.6860 loss:8632.1797 exploreP:0.0101\n",
      "Episode:537 meanR:284.1800 R:450.0 rate:0.9000 loss:8923.3291 exploreP:0.0101\n",
      "Episode:538 meanR:284.1300 R:338.0 rate:0.6760 loss:8816.8506 exploreP:0.0101\n",
      "Episode:539 meanR:285.0500 R:414.0 rate:0.8280 loss:8117.1699 exploreP:0.0101\n",
      "Episode:540 meanR:286.3100 R:444.0 rate:0.8880 loss:7773.9463 exploreP:0.0101\n",
      "Episode:541 meanR:287.4300 R:400.0 rate:0.8000 loss:7343.5630 exploreP:0.0101\n",
      "Episode:542 meanR:288.6000 R:403.0 rate:0.8060 loss:7022.8164 exploreP:0.0101\n",
      "Episode:543 meanR:289.0600 R:365.0 rate:0.7300 loss:6615.6211 exploreP:0.0101\n",
      "Episode:544 meanR:289.5700 R:350.0 rate:0.7000 loss:6341.7358 exploreP:0.0101\n",
      "Episode:545 meanR:289.3000 R:300.0 rate:0.6000 loss:5564.3643 exploreP:0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:546 meanR:289.5300 R:325.0 rate:0.6500 loss:5442.2295 exploreP:0.0101\n",
      "Episode:547 meanR:289.8300 R:326.0 rate:0.6520 loss:4879.4746 exploreP:0.0101\n",
      "Episode:548 meanR:290.6800 R:371.0 rate:0.7420 loss:5238.1440 exploreP:0.0101\n",
      "Episode:549 meanR:291.9400 R:438.0 rate:0.8760 loss:5159.9282 exploreP:0.0101\n",
      "Episode:550 meanR:292.0600 R:304.0 rate:0.6080 loss:5314.1045 exploreP:0.0101\n",
      "Episode:551 meanR:292.5000 R:313.0 rate:0.6260 loss:5037.0933 exploreP:0.0101\n",
      "Episode:552 meanR:292.8300 R:288.0 rate:0.5760 loss:5089.6147 exploreP:0.0101\n",
      "Episode:553 meanR:292.9400 R:325.0 rate:0.6500 loss:5199.1177 exploreP:0.0101\n",
      "Episode:554 meanR:293.1700 R:339.0 rate:0.6780 loss:5191.4497 exploreP:0.0101\n",
      "Episode:555 meanR:295.4800 R:500.0 rate:1.0000 loss:5335.9141 exploreP:0.0101\n",
      "Episode:556 meanR:296.3900 R:396.0 rate:0.7920 loss:5425.3970 exploreP:0.0101\n",
      "Episode:557 meanR:297.1500 R:343.0 rate:0.6860 loss:5336.5503 exploreP:0.0101\n",
      "Episode:558 meanR:298.0300 R:336.0 rate:0.6720 loss:5629.7256 exploreP:0.0101\n",
      "Episode:559 meanR:300.4800 R:500.0 rate:1.0000 loss:5451.4741 exploreP:0.0101\n",
      "Episode:560 meanR:302.6200 R:500.0 rate:1.0000 loss:5351.3521 exploreP:0.0101\n",
      "Episode:561 meanR:304.4900 R:447.0 rate:0.8940 loss:5317.6284 exploreP:0.0101\n",
      "Episode:562 meanR:305.3200 R:366.0 rate:0.7320 loss:5388.3730 exploreP:0.0100\n",
      "Episode:563 meanR:307.4900 R:500.0 rate:1.0000 loss:5418.3418 exploreP:0.0100\n",
      "Episode:564 meanR:308.3400 R:375.0 rate:0.7500 loss:5327.3540 exploreP:0.0100\n",
      "Episode:565 meanR:310.6200 R:500.0 rate:1.0000 loss:4764.5298 exploreP:0.0100\n",
      "Episode:566 meanR:308.5300 R:10.0 rate:0.0200 loss:4294.5693 exploreP:0.0100\n",
      "Episode:567 meanR:306.1100 R:11.0 rate:0.0220 loss:4494.9800 exploreP:0.0100\n",
      "Episode:568 meanR:303.7900 R:9.0 rate:0.0180 loss:4370.1763 exploreP:0.0100\n",
      "Episode:569 meanR:301.4300 R:10.0 rate:0.0200 loss:3423.9824 exploreP:0.0100\n",
      "Episode:570 meanR:298.6100 R:10.0 rate:0.0200 loss:2950.2427 exploreP:0.0100\n",
      "Episode:571 meanR:295.7800 R:9.0 rate:0.0180 loss:2175.8074 exploreP:0.0100\n",
      "Episode:572 meanR:293.6800 R:9.0 rate:0.0180 loss:3766.7747 exploreP:0.0100\n",
      "Episode:573 meanR:291.2700 R:10.0 rate:0.0200 loss:3663.4519 exploreP:0.0100\n",
      "Episode:574 meanR:289.0700 R:9.0 rate:0.0180 loss:4230.0562 exploreP:0.0100\n",
      "Episode:575 meanR:286.6200 R:10.0 rate:0.0200 loss:3455.9382 exploreP:0.0100\n",
      "Episode:576 meanR:284.6700 R:10.0 rate:0.0200 loss:2631.0366 exploreP:0.0100\n",
      "Episode:577 meanR:281.9700 R:9.0 rate:0.0180 loss:2525.8325 exploreP:0.0100\n",
      "Episode:578 meanR:279.6000 R:9.0 rate:0.0180 loss:3612.9431 exploreP:0.0100\n",
      "Episode:579 meanR:277.0200 R:9.0 rate:0.0180 loss:3728.9631 exploreP:0.0100\n",
      "Episode:580 meanR:274.6200 R:9.0 rate:0.0180 loss:3097.3940 exploreP:0.0100\n",
      "Episode:581 meanR:272.4200 R:10.0 rate:0.0200 loss:2397.7019 exploreP:0.0100\n",
      "Episode:582 meanR:270.0500 R:8.0 rate:0.0160 loss:2424.7524 exploreP:0.0100\n",
      "Episode:583 meanR:267.6800 R:10.0 rate:0.0200 loss:2861.0347 exploreP:0.0100\n",
      "Episode:584 meanR:264.9800 R:9.0 rate:0.0180 loss:3435.6902 exploreP:0.0100\n",
      "Episode:585 meanR:262.5100 R:9.0 rate:0.0180 loss:2406.6863 exploreP:0.0100\n",
      "Episode:586 meanR:260.4600 R:9.0 rate:0.0180 loss:2747.7673 exploreP:0.0100\n",
      "Episode:587 meanR:257.7800 R:9.0 rate:0.0180 loss:2754.2366 exploreP:0.0100\n",
      "Episode:588 meanR:255.3500 R:9.0 rate:0.0180 loss:2247.1208 exploreP:0.0100\n",
      "Episode:589 meanR:252.1400 R:9.0 rate:0.0180 loss:2198.3901 exploreP:0.0100\n",
      "Episode:590 meanR:249.9900 R:8.0 rate:0.0160 loss:2962.4456 exploreP:0.0100\n",
      "Episode:591 meanR:247.1800 R:10.0 rate:0.0200 loss:3687.0039 exploreP:0.0100\n",
      "Episode:592 meanR:244.9900 R:11.0 rate:0.0220 loss:2904.6633 exploreP:0.0100\n",
      "Episode:593 meanR:242.5300 R:10.0 rate:0.0200 loss:3046.2329 exploreP:0.0100\n",
      "Episode:594 meanR:240.1500 R:10.0 rate:0.0200 loss:4138.2056 exploreP:0.0100\n",
      "Episode:595 meanR:237.8100 R:9.0 rate:0.0180 loss:2267.6155 exploreP:0.0100\n",
      "Episode:596 meanR:235.7400 R:10.0 rate:0.0200 loss:1747.2679 exploreP:0.0100\n",
      "Episode:597 meanR:232.9200 R:10.0 rate:0.0200 loss:3937.7173 exploreP:0.0100\n",
      "Episode:598 meanR:230.6200 R:10.0 rate:0.0200 loss:4786.4590 exploreP:0.0100\n",
      "Episode:599 meanR:228.1100 R:8.0 rate:0.0160 loss:2792.0811 exploreP:0.0100\n",
      "Episode:600 meanR:225.4700 R:9.0 rate:0.0180 loss:2410.4229 exploreP:0.0100\n",
      "Episode:601 meanR:222.6700 R:9.0 rate:0.0180 loss:4507.2637 exploreP:0.0100\n",
      "Episode:602 meanR:220.5500 R:9.0 rate:0.0180 loss:4117.7920 exploreP:0.0100\n",
      "Episode:603 meanR:217.8200 R:10.0 rate:0.0200 loss:3728.8906 exploreP:0.0100\n",
      "Episode:604 meanR:215.6800 R:8.0 rate:0.0160 loss:3938.8394 exploreP:0.0100\n",
      "Episode:605 meanR:213.3900 R:10.0 rate:0.0200 loss:7596.7656 exploreP:0.0100\n",
      "Episode:606 meanR:210.9800 R:9.0 rate:0.0180 loss:5111.9189 exploreP:0.0100\n",
      "Episode:607 meanR:208.3200 R:10.0 rate:0.0200 loss:3379.8547 exploreP:0.0100\n",
      "Episode:608 meanR:206.0300 R:9.0 rate:0.0180 loss:2960.7295 exploreP:0.0100\n",
      "Episode:609 meanR:203.8800 R:9.0 rate:0.0180 loss:4614.1680 exploreP:0.0100\n",
      "Episode:610 meanR:201.5300 R:8.0 rate:0.0160 loss:2108.0637 exploreP:0.0100\n",
      "Episode:611 meanR:199.0700 R:10.0 rate:0.0200 loss:4153.8892 exploreP:0.0100\n",
      "Episode:612 meanR:196.2700 R:10.0 rate:0.0200 loss:4560.3369 exploreP:0.0100\n",
      "Episode:613 meanR:193.5900 R:9.0 rate:0.0180 loss:3802.9231 exploreP:0.0100\n",
      "Episode:614 meanR:191.2100 R:8.0 rate:0.0160 loss:1100.7734 exploreP:0.0100\n",
      "Episode:615 meanR:188.8100 R:9.0 rate:0.0180 loss:7197.7432 exploreP:0.0100\n",
      "Episode:616 meanR:186.3400 R:10.0 rate:0.0200 loss:6641.0728 exploreP:0.0100\n",
      "Episode:617 meanR:184.1000 R:8.0 rate:0.0160 loss:993.8035 exploreP:0.0100\n",
      "Episode:618 meanR:181.9300 R:10.0 rate:0.0200 loss:6753.4385 exploreP:0.0100\n",
      "Episode:619 meanR:179.0700 R:8.0 rate:0.0160 loss:7299.7236 exploreP:0.0100\n",
      "Episode:620 meanR:176.4400 R:9.0 rate:0.0180 loss:3277.1455 exploreP:0.0100\n",
      "Episode:621 meanR:173.7300 R:9.0 rate:0.0180 loss:2593.6956 exploreP:0.0100\n",
      "Episode:622 meanR:170.8700 R:10.0 rate:0.0200 loss:7356.1377 exploreP:0.0100\n",
      "Episode:623 meanR:167.6800 R:10.0 rate:0.0200 loss:4634.5654 exploreP:0.0100\n",
      "Episode:624 meanR:165.0100 R:9.0 rate:0.0180 loss:3348.0908 exploreP:0.0100\n",
      "Episode:625 meanR:161.8600 R:10.0 rate:0.0200 loss:6217.1729 exploreP:0.0100\n",
      "Episode:626 meanR:159.3800 R:8.0 rate:0.0160 loss:2408.3286 exploreP:0.0100\n",
      "Episode:627 meanR:156.7700 R:9.0 rate:0.0180 loss:4224.0596 exploreP:0.0100\n",
      "Episode:628 meanR:152.3400 R:9.0 rate:0.0180 loss:6964.7676 exploreP:0.0100\n",
      "Episode:629 meanR:148.1200 R:11.0 rate:0.0220 loss:12454.2900 exploreP:0.0100\n",
      "Episode:630 meanR:143.2100 R:9.0 rate:0.0180 loss:1423.7373 exploreP:0.0100\n",
      "Episode:631 meanR:140.2400 R:9.0 rate:0.0180 loss:2586.6514 exploreP:0.0100\n",
      "Episode:632 meanR:135.3200 R:8.0 rate:0.0160 loss:19358.6016 exploreP:0.0100\n",
      "Episode:633 meanR:130.4100 R:9.0 rate:0.0180 loss:8688.3506 exploreP:0.0100\n",
      "Episode:634 meanR:126.0200 R:8.0 rate:0.0160 loss:2108.9607 exploreP:0.0100\n",
      "Episode:635 meanR:122.4800 R:9.0 rate:0.0180 loss:21659.5938 exploreP:0.0100\n",
      "Episode:636 meanR:119.1400 R:9.0 rate:0.0180 loss:2961.7444 exploreP:0.0100\n",
      "Episode:637 meanR:114.7400 R:10.0 rate:0.0200 loss:4741.2061 exploreP:0.0100\n",
      "Episode:638 meanR:111.4600 R:10.0 rate:0.0200 loss:7498.2905 exploreP:0.0100\n",
      "Episode:639 meanR:107.4100 R:9.0 rate:0.0180 loss:6091.3594 exploreP:0.0100\n",
      "Episode:640 meanR:103.0500 R:8.0 rate:0.0160 loss:12581.3047 exploreP:0.0100\n",
      "Episode:641 meanR:99.1500 R:10.0 rate:0.0200 loss:17754.6074 exploreP:0.0100\n",
      "Episode:642 meanR:95.2100 R:9.0 rate:0.0180 loss:6803.9727 exploreP:0.0100\n",
      "Episode:643 meanR:91.6500 R:9.0 rate:0.0180 loss:11584.7012 exploreP:0.0100\n",
      "Episode:644 meanR:88.2300 R:8.0 rate:0.0160 loss:11935.6094 exploreP:0.0100\n",
      "Episode:645 meanR:85.3300 R:10.0 rate:0.0200 loss:904.3245 exploreP:0.0100\n",
      "Episode:646 meanR:82.1700 R:9.0 rate:0.0180 loss:9275.7988 exploreP:0.0100\n",
      "Episode:647 meanR:79.0100 R:10.0 rate:0.0200 loss:19747.5098 exploreP:0.0100\n",
      "Episode:648 meanR:75.4000 R:10.0 rate:0.0200 loss:9790.4805 exploreP:0.0100\n",
      "Episode:649 meanR:71.1100 R:9.0 rate:0.0180 loss:1225.7670 exploreP:0.0100\n",
      "Episode:650 meanR:68.1700 R:10.0 rate:0.0200 loss:12213.2656 exploreP:0.0100\n",
      "Episode:651 meanR:65.1300 R:9.0 rate:0.0180 loss:18846.1309 exploreP:0.0100\n",
      "Episode:652 meanR:62.3400 R:9.0 rate:0.0180 loss:21449.3535 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:653 meanR:59.1900 R:10.0 rate:0.0200 loss:12265.5449 exploreP:0.0100\n",
      "Episode:654 meanR:55.9000 R:10.0 rate:0.0200 loss:12871.3643 exploreP:0.0100\n",
      "Episode:655 meanR:50.9900 R:9.0 rate:0.0180 loss:15637.7988 exploreP:0.0100\n",
      "Episode:656 meanR:47.1300 R:10.0 rate:0.0200 loss:14391.4014 exploreP:0.0100\n",
      "Episode:657 meanR:43.8000 R:10.0 rate:0.0200 loss:11895.4668 exploreP:0.0100\n",
      "Episode:658 meanR:40.5500 R:11.0 rate:0.0220 loss:16086.5186 exploreP:0.0100\n",
      "Episode:659 meanR:35.6400 R:9.0 rate:0.0180 loss:24298.2422 exploreP:0.0100\n",
      "Episode:660 meanR:30.7400 R:10.0 rate:0.0200 loss:28414.1934 exploreP:0.0100\n",
      "Episode:661 meanR:26.3600 R:9.0 rate:0.0180 loss:10490.6143 exploreP:0.0100\n",
      "Episode:662 meanR:22.8000 R:10.0 rate:0.0200 loss:14925.8877 exploreP:0.0100\n",
      "Episode:663 meanR:17.8800 R:8.0 rate:0.0160 loss:54883.5430 exploreP:0.0100\n",
      "Episode:664 meanR:14.2300 R:10.0 rate:0.0200 loss:9056.1533 exploreP:0.0100\n",
      "Episode:665 meanR:9.3300 R:10.0 rate:0.0200 loss:31971.5898 exploreP:0.0100\n",
      "Episode:666 meanR:9.3300 R:10.0 rate:0.0200 loss:55621.1055 exploreP:0.0100\n",
      "Episode:667 meanR:9.3200 R:10.0 rate:0.0200 loss:21337.6113 exploreP:0.0100\n",
      "Episode:668 meanR:9.3200 R:9.0 rate:0.0180 loss:18198.9668 exploreP:0.0100\n",
      "Episode:669 meanR:9.3200 R:10.0 rate:0.0200 loss:22741.3535 exploreP:0.0100\n",
      "Episode:670 meanR:9.3200 R:10.0 rate:0.0200 loss:28208.5000 exploreP:0.0100\n",
      "Episode:671 meanR:9.3300 R:10.0 rate:0.0200 loss:33736.8711 exploreP:0.0100\n",
      "Episode:672 meanR:9.3300 R:9.0 rate:0.0180 loss:12988.2041 exploreP:0.0100\n",
      "Episode:673 meanR:9.3400 R:11.0 rate:0.0220 loss:12158.5195 exploreP:0.0100\n",
      "Episode:674 meanR:9.3400 R:9.0 rate:0.0180 loss:16983.5566 exploreP:0.0100\n",
      "Episode:675 meanR:9.3300 R:9.0 rate:0.0180 loss:39376.1133 exploreP:0.0100\n",
      "Episode:676 meanR:9.3200 R:9.0 rate:0.0180 loss:41487.4570 exploreP:0.0100\n",
      "Episode:677 meanR:9.3300 R:10.0 rate:0.0200 loss:17931.9980 exploreP:0.0100\n",
      "Episode:678 meanR:9.3400 R:10.0 rate:0.0200 loss:32316.5820 exploreP:0.0100\n",
      "Episode:679 meanR:9.3500 R:10.0 rate:0.0200 loss:33058.4492 exploreP:0.0100\n",
      "Episode:680 meanR:9.3600 R:10.0 rate:0.0200 loss:20653.3086 exploreP:0.0100\n",
      "Episode:681 meanR:9.3600 R:10.0 rate:0.0200 loss:28104.2285 exploreP:0.0100\n",
      "Episode:682 meanR:9.3800 R:10.0 rate:0.0200 loss:28828.4121 exploreP:0.0100\n",
      "Episode:683 meanR:9.3800 R:10.0 rate:0.0200 loss:14025.2705 exploreP:0.0100\n",
      "Episode:684 meanR:9.3900 R:10.0 rate:0.0200 loss:59764.7109 exploreP:0.0100\n",
      "Episode:685 meanR:9.4000 R:10.0 rate:0.0200 loss:48636.4453 exploreP:0.0100\n",
      "Episode:686 meanR:9.4100 R:10.0 rate:0.0200 loss:42748.7461 exploreP:0.0100\n",
      "Episode:687 meanR:9.4400 R:12.0 rate:0.0240 loss:24782.8105 exploreP:0.0100\n",
      "Episode:688 meanR:9.4600 R:11.0 rate:0.0220 loss:49205.2227 exploreP:0.0100\n",
      "Episode:689 meanR:9.4700 R:10.0 rate:0.0200 loss:60073.6875 exploreP:0.0100\n",
      "Episode:690 meanR:9.5000 R:11.0 rate:0.0220 loss:32439.8242 exploreP:0.0100\n",
      "Episode:691 meanR:9.4900 R:9.0 rate:0.0180 loss:35009.2891 exploreP:0.0100\n",
      "Episode:692 meanR:9.4800 R:10.0 rate:0.0200 loss:21601.4023 exploreP:0.0100\n",
      "Episode:693 meanR:9.4800 R:10.0 rate:0.0200 loss:32510.3438 exploreP:0.0100\n",
      "Episode:694 meanR:9.4800 R:10.0 rate:0.0200 loss:65090.7578 exploreP:0.0100\n",
      "Episode:695 meanR:9.4900 R:10.0 rate:0.0200 loss:34188.6367 exploreP:0.0100\n",
      "Episode:696 meanR:9.4700 R:8.0 rate:0.0160 loss:3663.4602 exploreP:0.0100\n",
      "Episode:697 meanR:9.4700 R:10.0 rate:0.0200 loss:37611.8203 exploreP:0.0100\n",
      "Episode:698 meanR:9.4700 R:10.0 rate:0.0200 loss:97939.3047 exploreP:0.0100\n",
      "Episode:699 meanR:9.4800 R:9.0 rate:0.0180 loss:110085.5625 exploreP:0.0100\n",
      "Episode:700 meanR:9.4800 R:9.0 rate:0.0180 loss:56137.9648 exploreP:0.0100\n",
      "Episode:701 meanR:9.4800 R:9.0 rate:0.0180 loss:22790.9746 exploreP:0.0100\n",
      "Episode:702 meanR:9.4800 R:9.0 rate:0.0180 loss:4545.1504 exploreP:0.0100\n",
      "Episode:703 meanR:9.4800 R:10.0 rate:0.0200 loss:7773.7471 exploreP:0.0100\n",
      "Episode:704 meanR:9.5000 R:10.0 rate:0.0200 loss:19983.0664 exploreP:0.0100\n",
      "Episode:705 meanR:9.5000 R:10.0 rate:0.0200 loss:26535.0430 exploreP:0.0100\n",
      "Episode:706 meanR:9.4900 R:8.0 rate:0.0160 loss:4744.7563 exploreP:0.0100\n",
      "Episode:707 meanR:9.4700 R:8.0 rate:0.0160 loss:75615.6719 exploreP:0.0100\n",
      "Episode:708 meanR:9.4700 R:9.0 rate:0.0180 loss:37928.6602 exploreP:0.0100\n",
      "Episode:709 meanR:9.4600 R:8.0 rate:0.0160 loss:30513.1934 exploreP:0.0100\n",
      "Episode:710 meanR:9.4700 R:9.0 rate:0.0180 loss:5353.0469 exploreP:0.0100\n",
      "Episode:711 meanR:9.4700 R:10.0 rate:0.0200 loss:96843.4766 exploreP:0.0100\n",
      "Episode:712 meanR:9.4600 R:9.0 rate:0.0180 loss:88402.0000 exploreP:0.0100\n",
      "Episode:713 meanR:9.4500 R:8.0 rate:0.0160 loss:32847.6172 exploreP:0.0100\n",
      "Episode:714 meanR:9.4800 R:11.0 rate:0.0220 loss:77334.8750 exploreP:0.0100\n",
      "Episode:715 meanR:9.4900 R:10.0 rate:0.0200 loss:8186.2891 exploreP:0.0100\n",
      "Episode:716 meanR:9.4800 R:9.0 rate:0.0180 loss:5833.6919 exploreP:0.0100\n",
      "Episode:717 meanR:9.4900 R:9.0 rate:0.0180 loss:6044.9004 exploreP:0.0100\n",
      "Episode:718 meanR:9.4700 R:8.0 rate:0.0160 loss:51825.4648 exploreP:0.0100\n",
      "Episode:719 meanR:9.4800 R:9.0 rate:0.0180 loss:151786.7656 exploreP:0.0100\n",
      "Episode:720 meanR:9.4900 R:10.0 rate:0.0200 loss:139700.3281 exploreP:0.0100\n",
      "Episode:721 meanR:9.4800 R:8.0 rate:0.0160 loss:38888.0430 exploreP:0.0100\n",
      "Episode:722 meanR:9.4800 R:10.0 rate:0.0200 loss:67627.5859 exploreP:0.0100\n",
      "Episode:723 meanR:9.4700 R:9.0 rate:0.0180 loss:79556.3750 exploreP:0.0100\n",
      "Episode:724 meanR:9.4700 R:9.0 rate:0.0180 loss:190696.0938 exploreP:0.0100\n",
      "Episode:725 meanR:9.4500 R:8.0 rate:0.0160 loss:44919.4141 exploreP:0.0100\n",
      "Episode:726 meanR:9.4800 R:11.0 rate:0.0220 loss:150305.5938 exploreP:0.0100\n",
      "Episode:727 meanR:9.4800 R:9.0 rate:0.0180 loss:88802.5312 exploreP:0.0100\n",
      "Episode:728 meanR:9.4900 R:10.0 rate:0.0200 loss:37628.3086 exploreP:0.0100\n",
      "Episode:729 meanR:9.4700 R:9.0 rate:0.0180 loss:42218.9141 exploreP:0.0100\n",
      "Episode:730 meanR:9.4700 R:9.0 rate:0.0180 loss:91196.9766 exploreP:0.0100\n",
      "Episode:731 meanR:9.4800 R:10.0 rate:0.0200 loss:160920.8906 exploreP:0.0100\n",
      "Episode:732 meanR:9.4900 R:9.0 rate:0.0180 loss:121956.0547 exploreP:0.0100\n",
      "Episode:733 meanR:9.4900 R:9.0 rate:0.0180 loss:144163.3906 exploreP:0.0100\n",
      "Episode:734 meanR:9.5100 R:10.0 rate:0.0200 loss:82639.5469 exploreP:0.0100\n",
      "Episode:735 meanR:9.5000 R:8.0 rate:0.0160 loss:49063.6562 exploreP:0.0100\n",
      "Episode:736 meanR:9.5100 R:10.0 rate:0.0200 loss:69751.5469 exploreP:0.0100\n",
      "Episode:737 meanR:9.5000 R:9.0 rate:0.0180 loss:48520.5977 exploreP:0.0100\n",
      "Episode:738 meanR:9.4800 R:8.0 rate:0.0160 loss:110257.1094 exploreP:0.0100\n",
      "Episode:739 meanR:9.4700 R:8.0 rate:0.0160 loss:53834.7578 exploreP:0.0100\n",
      "Episode:740 meanR:9.4900 R:10.0 rate:0.0200 loss:101020.1641 exploreP:0.0100\n",
      "Episode:741 meanR:9.4900 R:10.0 rate:0.0200 loss:66958.7656 exploreP:0.0100\n",
      "Episode:742 meanR:9.4900 R:9.0 rate:0.0180 loss:134593.6875 exploreP:0.0100\n",
      "Episode:743 meanR:9.5000 R:10.0 rate:0.0200 loss:111432.1406 exploreP:0.0100\n",
      "Episode:744 meanR:9.5100 R:9.0 rate:0.0180 loss:178386.8281 exploreP:0.0100\n",
      "Episode:745 meanR:9.5100 R:10.0 rate:0.0200 loss:50539.8438 exploreP:0.0100\n",
      "Episode:746 meanR:9.5500 R:13.0 rate:0.0260 loss:168302.9688 exploreP:0.0100\n",
      "Episode:747 meanR:9.5400 R:9.0 rate:0.0180 loss:62023.7422 exploreP:0.0100\n",
      "Episode:748 meanR:9.5400 R:10.0 rate:0.0200 loss:161136.1719 exploreP:0.0100\n",
      "Episode:749 meanR:9.5500 R:10.0 rate:0.0200 loss:110900.3594 exploreP:0.0100\n",
      "Episode:750 meanR:9.5500 R:10.0 rate:0.0200 loss:27749.6816 exploreP:0.0100\n",
      "Episode:751 meanR:9.5600 R:10.0 rate:0.0200 loss:61348.9062 exploreP:0.0100\n",
      "Episode:752 meanR:9.5600 R:9.0 rate:0.0180 loss:41530.3125 exploreP:0.0100\n",
      "Episode:753 meanR:9.5400 R:8.0 rate:0.0160 loss:151517.9531 exploreP:0.0100\n",
      "Episode:754 meanR:9.5300 R:9.0 rate:0.0180 loss:55181.1836 exploreP:0.0100\n",
      "Episode:755 meanR:9.5400 R:10.0 rate:0.0200 loss:12920.2578 exploreP:0.0100\n",
      "Episode:756 meanR:9.5300 R:9.0 rate:0.0180 loss:13466.1592 exploreP:0.0100\n",
      "Episode:757 meanR:9.5100 R:8.0 rate:0.0160 loss:50877.9648 exploreP:0.0100\n",
      "Episode:758 meanR:9.4900 R:9.0 rate:0.0180 loss:132205.4531 exploreP:0.0100\n",
      "Episode:759 meanR:9.4900 R:9.0 rate:0.0180 loss:14936.6855 exploreP:0.0100\n",
      "Episode:760 meanR:9.4800 R:9.0 rate:0.0180 loss:141279.6875 exploreP:0.0100\n",
      "Episode:761 meanR:9.4800 R:9.0 rate:0.0180 loss:174669.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:762 meanR:9.4800 R:10.0 rate:0.0200 loss:75602.4609 exploreP:0.0100\n",
      "Episode:763 meanR:9.5000 R:10.0 rate:0.0200 loss:207744.7656 exploreP:0.0100\n",
      "Episode:764 meanR:9.5000 R:10.0 rate:0.0200 loss:231674.6719 exploreP:0.0100\n",
      "Episode:765 meanR:9.5000 R:10.0 rate:0.0200 loss:117280.4219 exploreP:0.0100\n",
      "Episode:766 meanR:9.4900 R:9.0 rate:0.0180 loss:277219.9375 exploreP:0.0100\n",
      "Episode:767 meanR:9.4900 R:10.0 rate:0.0200 loss:127234.2031 exploreP:0.0100\n",
      "Episode:768 meanR:9.4800 R:8.0 rate:0.0160 loss:230889.1875 exploreP:0.0100\n",
      "Episode:769 meanR:9.4800 R:10.0 rate:0.0200 loss:220382.3438 exploreP:0.0100\n",
      "Episode:770 meanR:9.4700 R:9.0 rate:0.0180 loss:270522.0938 exploreP:0.0100\n",
      "Episode:771 meanR:9.4600 R:9.0 rate:0.0180 loss:324363.8750 exploreP:0.0100\n",
      "Episode:772 meanR:9.4700 R:10.0 rate:0.0200 loss:332834.8125 exploreP:0.0100\n",
      "Episode:773 meanR:9.4600 R:10.0 rate:0.0200 loss:212197.1719 exploreP:0.0100\n",
      "Episode:774 meanR:9.4500 R:8.0 rate:0.0160 loss:193031.1562 exploreP:0.0100\n",
      "Episode:775 meanR:9.4400 R:8.0 rate:0.0160 loss:172537.0156 exploreP:0.0100\n",
      "Episode:776 meanR:9.4300 R:8.0 rate:0.0160 loss:187491.5000 exploreP:0.0100\n",
      "Episode:777 meanR:9.4200 R:9.0 rate:0.0180 loss:431242.5938 exploreP:0.0100\n",
      "Episode:778 meanR:9.4200 R:10.0 rate:0.0200 loss:261332.2812 exploreP:0.0100\n",
      "Episode:779 meanR:9.4000 R:8.0 rate:0.0160 loss:51289.8984 exploreP:0.0100\n",
      "Episode:780 meanR:9.3900 R:9.0 rate:0.0180 loss:207395.6094 exploreP:0.0100\n",
      "Episode:781 meanR:9.3800 R:9.0 rate:0.0180 loss:173195.3438 exploreP:0.0100\n",
      "Episode:782 meanR:9.3800 R:10.0 rate:0.0200 loss:348876.2188 exploreP:0.0100\n",
      "Episode:783 meanR:9.3700 R:9.0 rate:0.0180 loss:270915.8750 exploreP:0.0100\n",
      "Episode:784 meanR:9.3700 R:10.0 rate:0.0200 loss:275107.7188 exploreP:0.0100\n",
      "Episode:785 meanR:9.3600 R:9.0 rate:0.0180 loss:51340.0000 exploreP:0.0100\n",
      "Episode:786 meanR:9.3600 R:10.0 rate:0.0200 loss:102271.5781 exploreP:0.0100\n",
      "Episode:787 meanR:9.3400 R:10.0 rate:0.0200 loss:117897.3750 exploreP:0.0100\n",
      "Episode:788 meanR:9.3200 R:9.0 rate:0.0180 loss:22594.4961 exploreP:0.0100\n",
      "Episode:789 meanR:9.3100 R:9.0 rate:0.0180 loss:22913.7129 exploreP:0.0100\n",
      "Episode:790 meanR:9.3000 R:10.0 rate:0.0200 loss:296791.9688 exploreP:0.0100\n",
      "Episode:791 meanR:9.3100 R:10.0 rate:0.0200 loss:291992.6875 exploreP:0.0100\n",
      "Episode:792 meanR:9.3100 R:10.0 rate:0.0200 loss:442006.1875 exploreP:0.0100\n",
      "Episode:793 meanR:9.3000 R:9.0 rate:0.0180 loss:720124.2500 exploreP:0.0100\n",
      "Episode:794 meanR:9.3000 R:10.0 rate:0.0200 loss:278151.0625 exploreP:0.0100\n",
      "Episode:795 meanR:9.3000 R:10.0 rate:0.0200 loss:304968.2188 exploreP:0.0100\n",
      "Episode:796 meanR:9.3100 R:9.0 rate:0.0180 loss:657885.3750 exploreP:0.0100\n",
      "Episode:797 meanR:9.3100 R:10.0 rate:0.0200 loss:319627.3438 exploreP:0.0100\n",
      "Episode:798 meanR:9.3100 R:10.0 rate:0.0200 loss:396769.1562 exploreP:0.0100\n",
      "Episode:799 meanR:9.3200 R:10.0 rate:0.0200 loss:371563.6562 exploreP:0.0100\n",
      "Episode:800 meanR:9.3200 R:9.0 rate:0.0180 loss:243254.2500 exploreP:0.0100\n",
      "Episode:801 meanR:9.3300 R:10.0 rate:0.0200 loss:208507.1875 exploreP:0.0100\n",
      "Episode:802 meanR:9.3400 R:10.0 rate:0.0200 loss:122231.3516 exploreP:0.0100\n",
      "Episode:803 meanR:9.3400 R:10.0 rate:0.0200 loss:205925.5938 exploreP:0.0100\n",
      "Episode:804 meanR:9.3400 R:10.0 rate:0.0200 loss:133461.4688 exploreP:0.0100\n",
      "Episode:805 meanR:9.3400 R:10.0 rate:0.0200 loss:46230.4336 exploreP:0.0100\n",
      "Episode:806 meanR:9.3400 R:8.0 rate:0.0160 loss:581903.2500 exploreP:0.0100\n",
      "Episode:807 meanR:9.3600 R:10.0 rate:0.0200 loss:29855.5664 exploreP:0.0100\n",
      "Episode:808 meanR:9.3600 R:9.0 rate:0.0180 loss:491278.0000 exploreP:0.0100\n",
      "Episode:809 meanR:9.3700 R:9.0 rate:0.0180 loss:151147.2969 exploreP:0.0100\n",
      "Episode:810 meanR:9.3800 R:10.0 rate:0.0200 loss:253314.5938 exploreP:0.0100\n",
      "Episode:811 meanR:9.3700 R:9.0 rate:0.0180 loss:280860.8125 exploreP:0.0100\n",
      "Episode:812 meanR:9.3800 R:10.0 rate:0.0200 loss:142838.6719 exploreP:0.0100\n",
      "Episode:813 meanR:9.3900 R:9.0 rate:0.0180 loss:515265.5625 exploreP:0.0100\n",
      "Episode:814 meanR:9.3800 R:10.0 rate:0.0200 loss:231206.5469 exploreP:0.0100\n",
      "Episode:815 meanR:9.3700 R:9.0 rate:0.0180 loss:404168.6250 exploreP:0.0100\n",
      "Episode:816 meanR:9.3600 R:8.0 rate:0.0160 loss:101731.1250 exploreP:0.0100\n",
      "Episode:817 meanR:9.3600 R:9.0 rate:0.0180 loss:282366.1250 exploreP:0.0100\n",
      "Episode:818 meanR:9.3700 R:9.0 rate:0.0180 loss:396161.5312 exploreP:0.0100\n",
      "Episode:819 meanR:9.3800 R:10.0 rate:0.0200 loss:244697.9688 exploreP:0.0100\n",
      "Episode:820 meanR:9.3800 R:10.0 rate:0.0200 loss:535462.3125 exploreP:0.0100\n",
      "Episode:821 meanR:9.3800 R:8.0 rate:0.0160 loss:598808.2500 exploreP:0.0100\n",
      "Episode:822 meanR:9.3700 R:9.0 rate:0.0180 loss:315688.8750 exploreP:0.0100\n",
      "Episode:823 meanR:9.3600 R:8.0 rate:0.0160 loss:498655.3750 exploreP:0.0100\n",
      "Episode:824 meanR:9.3700 R:10.0 rate:0.0200 loss:166844.2812 exploreP:0.0100\n",
      "Episode:825 meanR:9.4000 R:11.0 rate:0.0220 loss:405377.4688 exploreP:0.0100\n",
      "Episode:826 meanR:9.3800 R:9.0 rate:0.0180 loss:230628.5469 exploreP:0.0100\n",
      "Episode:827 meanR:9.3700 R:8.0 rate:0.0160 loss:421261.4688 exploreP:0.0100\n",
      "Episode:828 meanR:9.3700 R:10.0 rate:0.0200 loss:199472.5469 exploreP:0.0100\n",
      "Episode:829 meanR:9.3800 R:10.0 rate:0.0200 loss:619074.6875 exploreP:0.0100\n",
      "Episode:830 meanR:9.3800 R:9.0 rate:0.0180 loss:679089.1875 exploreP:0.0100\n",
      "Episode:831 meanR:9.3700 R:9.0 rate:0.0180 loss:43938.8828 exploreP:0.0100\n",
      "Episode:832 meanR:9.3800 R:10.0 rate:0.0200 loss:584269.1875 exploreP:0.0100\n",
      "Episode:833 meanR:9.4000 R:11.0 rate:0.0220 loss:602412.3125 exploreP:0.0100\n",
      "Episode:834 meanR:9.3900 R:9.0 rate:0.0180 loss:825331.1250 exploreP:0.0100\n",
      "Episode:835 meanR:9.4100 R:10.0 rate:0.0200 loss:479370.9375 exploreP:0.0100\n",
      "Episode:836 meanR:9.3900 R:8.0 rate:0.0160 loss:361398.6250 exploreP:0.0100\n",
      "Episode:837 meanR:9.4000 R:10.0 rate:0.0200 loss:573402.8750 exploreP:0.0100\n",
      "Episode:838 meanR:9.4200 R:10.0 rate:0.0200 loss:452465.3125 exploreP:0.0100\n",
      "Episode:839 meanR:9.4400 R:10.0 rate:0.0200 loss:47942.8125 exploreP:0.0100\n",
      "Episode:840 meanR:9.4400 R:10.0 rate:0.0200 loss:619715.1250 exploreP:0.0100\n",
      "Episode:841 meanR:9.4400 R:10.0 rate:0.0200 loss:394362.6562 exploreP:0.0100\n",
      "Episode:842 meanR:9.4600 R:11.0 rate:0.0220 loss:875121.2500 exploreP:0.0100\n",
      "Episode:843 meanR:9.4500 R:9.0 rate:0.0180 loss:50582.3828 exploreP:0.0100\n",
      "Episode:844 meanR:9.4600 R:10.0 rate:0.0200 loss:499610.8438 exploreP:0.0100\n",
      "Episode:845 meanR:9.4500 R:9.0 rate:0.0180 loss:1034534.6875 exploreP:0.0100\n",
      "Episode:846 meanR:9.4200 R:10.0 rate:0.0200 loss:52511.2891 exploreP:0.0100\n",
      "Episode:847 meanR:9.4300 R:10.0 rate:0.0200 loss:643527.6250 exploreP:0.0100\n",
      "Episode:848 meanR:9.4300 R:10.0 rate:0.0200 loss:132115.2812 exploreP:0.0100\n",
      "Episode:849 meanR:9.4300 R:10.0 rate:0.0200 loss:690451.8125 exploreP:0.0100\n",
      "Episode:850 meanR:9.4300 R:10.0 rate:0.0200 loss:631596.3750 exploreP:0.0100\n",
      "Episode:851 meanR:9.4200 R:9.0 rate:0.0180 loss:718946.2500 exploreP:0.0100\n",
      "Episode:852 meanR:9.4200 R:9.0 rate:0.0180 loss:850752.6875 exploreP:0.0100\n",
      "Episode:853 meanR:9.4400 R:10.0 rate:0.0200 loss:376722.8125 exploreP:0.0100\n",
      "Episode:854 meanR:9.4500 R:10.0 rate:0.0200 loss:439821.0625 exploreP:0.0100\n",
      "Episode:855 meanR:9.4400 R:9.0 rate:0.0180 loss:651540.8750 exploreP:0.0100\n",
      "Episode:856 meanR:9.4500 R:10.0 rate:0.0200 loss:413553.7188 exploreP:0.0100\n",
      "Episode:857 meanR:9.4500 R:8.0 rate:0.0160 loss:301304.4688 exploreP:0.0100\n",
      "Episode:858 meanR:9.4600 R:10.0 rate:0.0200 loss:460882.0000 exploreP:0.0100\n",
      "Episode:859 meanR:9.4600 R:9.0 rate:0.0180 loss:523349.6250 exploreP:0.0100\n",
      "Episode:860 meanR:9.4700 R:10.0 rate:0.0200 loss:255241.5469 exploreP:0.0100\n",
      "Episode:861 meanR:9.4700 R:9.0 rate:0.0180 loss:1021908.2500 exploreP:0.0100\n",
      "Episode:862 meanR:9.4600 R:9.0 rate:0.0180 loss:324007.4375 exploreP:0.0100\n",
      "Episode:863 meanR:9.4600 R:10.0 rate:0.0200 loss:754150.3750 exploreP:0.0100\n",
      "Episode:864 meanR:9.4600 R:10.0 rate:0.0200 loss:316637.1875 exploreP:0.0100\n",
      "Episode:865 meanR:9.4600 R:10.0 rate:0.0200 loss:1089359.7500 exploreP:0.0100\n",
      "Episode:866 meanR:9.4600 R:9.0 rate:0.0180 loss:371871.0000 exploreP:0.0100\n",
      "Episode:867 meanR:9.4700 R:11.0 rate:0.0220 loss:375396.4062 exploreP:0.0100\n",
      "Episode:868 meanR:9.4800 R:9.0 rate:0.0180 loss:380562.8438 exploreP:0.0100\n",
      "Episode:869 meanR:9.4800 R:10.0 rate:0.0200 loss:936813.1875 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:870 meanR:9.4900 R:10.0 rate:0.0200 loss:238598.7031 exploreP:0.0100\n",
      "Episode:871 meanR:9.4900 R:9.0 rate:0.0180 loss:822924.6875 exploreP:0.0100\n",
      "Episode:872 meanR:9.4800 R:9.0 rate:0.0180 loss:352673.1250 exploreP:0.0100\n",
      "Episode:873 meanR:9.4700 R:9.0 rate:0.0180 loss:324561.5312 exploreP:0.0100\n",
      "Episode:874 meanR:9.4900 R:10.0 rate:0.0200 loss:537158.2500 exploreP:0.0100\n",
      "Episode:875 meanR:9.5000 R:9.0 rate:0.0180 loss:2105020.5000 exploreP:0.0100\n",
      "Episode:876 meanR:9.5400 R:12.0 rate:0.0240 loss:1175356.3750 exploreP:0.0100\n",
      "Episode:877 meanR:9.5400 R:9.0 rate:0.0180 loss:315446.0938 exploreP:0.0100\n",
      "Episode:878 meanR:9.5300 R:9.0 rate:0.0180 loss:760331.1250 exploreP:0.0100\n",
      "Episode:879 meanR:9.5500 R:10.0 rate:0.0200 loss:364850.4688 exploreP:0.0100\n",
      "Episode:880 meanR:9.5600 R:10.0 rate:0.0200 loss:360032.4062 exploreP:0.0100\n",
      "Episode:881 meanR:9.5700 R:10.0 rate:0.0200 loss:793123.8750 exploreP:0.0100\n",
      "Episode:882 meanR:9.5700 R:10.0 rate:0.0200 loss:337552.8750 exploreP:0.0100\n",
      "Episode:883 meanR:9.5700 R:9.0 rate:0.0180 loss:670016.7500 exploreP:0.0100\n",
      "Episode:884 meanR:9.5500 R:8.0 rate:0.0160 loss:841079.1250 exploreP:0.0100\n",
      "Episode:885 meanR:9.5500 R:9.0 rate:0.0180 loss:349275.0625 exploreP:0.0100\n",
      "Episode:886 meanR:9.5500 R:10.0 rate:0.0200 loss:1120238.5000 exploreP:0.0100\n",
      "Episode:887 meanR:9.5300 R:8.0 rate:0.0160 loss:1248944.2500 exploreP:0.0100\n",
      "Episode:888 meanR:9.5200 R:8.0 rate:0.0160 loss:814362.7500 exploreP:0.0100\n",
      "Episode:889 meanR:9.5400 R:11.0 rate:0.0220 loss:503716.6875 exploreP:0.0100\n",
      "Episode:890 meanR:9.5300 R:9.0 rate:0.0180 loss:837975.8750 exploreP:0.0100\n",
      "Episode:891 meanR:9.5300 R:10.0 rate:0.0200 loss:1358462.2500 exploreP:0.0100\n",
      "Episode:892 meanR:9.5200 R:9.0 rate:0.0180 loss:1611757.5000 exploreP:0.0100\n",
      "Episode:893 meanR:9.5300 R:10.0 rate:0.0200 loss:1506067.2500 exploreP:0.0100\n",
      "Episode:894 meanR:9.5200 R:9.0 rate:0.0180 loss:518525.6562 exploreP:0.0100\n",
      "Episode:895 meanR:9.5100 R:9.0 rate:0.0180 loss:84155.3203 exploreP:0.0100\n",
      "Episode:896 meanR:9.5100 R:9.0 rate:0.0180 loss:183207.4531 exploreP:0.0100\n",
      "Episode:897 meanR:9.5000 R:9.0 rate:0.0180 loss:1208418.6250 exploreP:0.0100\n",
      "Episode:898 meanR:9.4800 R:8.0 rate:0.0160 loss:1892692.0000 exploreP:0.0100\n",
      "Episode:899 meanR:9.4600 R:8.0 rate:0.0160 loss:1089416.1250 exploreP:0.0100\n",
      "Episode:900 meanR:9.4500 R:8.0 rate:0.0160 loss:1135781.0000 exploreP:0.0100\n",
      "Episode:901 meanR:9.4300 R:8.0 rate:0.0160 loss:1490443.7500 exploreP:0.0100\n",
      "Episode:902 meanR:9.4200 R:9.0 rate:0.0180 loss:980606.4375 exploreP:0.0100\n",
      "Episode:903 meanR:9.4100 R:9.0 rate:0.0180 loss:2407214.7500 exploreP:0.0100\n",
      "Episode:904 meanR:9.4100 R:10.0 rate:0.0200 loss:431658.8125 exploreP:0.0100\n",
      "Episode:905 meanR:9.3900 R:8.0 rate:0.0160 loss:321233.2500 exploreP:0.0100\n",
      "Episode:906 meanR:9.4000 R:9.0 rate:0.0180 loss:959713.1250 exploreP:0.0100\n",
      "Episode:907 meanR:9.3800 R:8.0 rate:0.0160 loss:592334.6875 exploreP:0.0100\n",
      "Episode:908 meanR:9.3700 R:8.0 rate:0.0160 loss:712917.1875 exploreP:0.0100\n",
      "Episode:909 meanR:9.3700 R:9.0 rate:0.0180 loss:1136956.5000 exploreP:0.0100\n",
      "Episode:910 meanR:9.3600 R:9.0 rate:0.0180 loss:172443.7656 exploreP:0.0100\n",
      "Episode:911 meanR:9.3600 R:9.0 rate:0.0180 loss:1792063.0000 exploreP:0.0100\n",
      "Episode:912 meanR:9.3500 R:9.0 rate:0.0180 loss:754598.0625 exploreP:0.0100\n",
      "Episode:913 meanR:9.3400 R:8.0 rate:0.0160 loss:861752.2500 exploreP:0.0100\n",
      "Episode:914 meanR:9.3400 R:10.0 rate:0.0200 loss:1447291.7500 exploreP:0.0100\n",
      "Episode:915 meanR:9.3400 R:9.0 rate:0.0180 loss:1469919.6250 exploreP:0.0100\n",
      "Episode:916 meanR:9.3600 R:10.0 rate:0.0200 loss:1315534.0000 exploreP:0.0100\n",
      "Episode:917 meanR:9.3700 R:10.0 rate:0.0200 loss:993611.5000 exploreP:0.0100\n",
      "Episode:918 meanR:9.3800 R:10.0 rate:0.0200 loss:1998321.2500 exploreP:0.0100\n",
      "Episode:919 meanR:9.3800 R:10.0 rate:0.0200 loss:700973.5625 exploreP:0.0100\n",
      "Episode:920 meanR:9.3800 R:10.0 rate:0.0200 loss:911436.6250 exploreP:0.0100\n",
      "Episode:921 meanR:9.4000 R:10.0 rate:0.0200 loss:556140.1875 exploreP:0.0100\n",
      "Episode:922 meanR:9.4000 R:9.0 rate:0.0180 loss:1940419.7500 exploreP:0.0100\n",
      "Episode:923 meanR:9.4200 R:10.0 rate:0.0200 loss:578931.4375 exploreP:0.0100\n",
      "Episode:924 meanR:9.4400 R:12.0 rate:0.0240 loss:1076873.5000 exploreP:0.0100\n",
      "Episode:925 meanR:9.4100 R:8.0 rate:0.0160 loss:116733.8516 exploreP:0.0100\n",
      "Episode:926 meanR:9.4000 R:8.0 rate:0.0160 loss:604612.4375 exploreP:0.0100\n",
      "Episode:927 meanR:9.4100 R:9.0 rate:0.0180 loss:125465.4609 exploreP:0.0100\n",
      "Episode:928 meanR:9.4100 R:10.0 rate:0.0200 loss:1949225.3750 exploreP:0.0100\n",
      "Episode:929 meanR:9.4100 R:10.0 rate:0.0200 loss:1857327.3750 exploreP:0.0100\n",
      "Episode:930 meanR:9.4200 R:10.0 rate:0.0200 loss:673895.0000 exploreP:0.0100\n",
      "Episode:931 meanR:9.4200 R:9.0 rate:0.0180 loss:628508.3125 exploreP:0.0100\n",
      "Episode:932 meanR:9.4100 R:9.0 rate:0.0180 loss:122042.1250 exploreP:0.0100\n",
      "Episode:933 meanR:9.3900 R:9.0 rate:0.0180 loss:1339428.5000 exploreP:0.0100\n",
      "Episode:934 meanR:9.3900 R:9.0 rate:0.0180 loss:1260285.8750 exploreP:0.0100\n",
      "Episode:935 meanR:9.3900 R:10.0 rate:0.0200 loss:1496555.8750 exploreP:0.0100\n",
      "Episode:936 meanR:9.4100 R:10.0 rate:0.0200 loss:2540071.7500 exploreP:0.0100\n",
      "Episode:937 meanR:9.4000 R:9.0 rate:0.0180 loss:1842377.3750 exploreP:0.0100\n",
      "Episode:938 meanR:9.3900 R:9.0 rate:0.0180 loss:3366100.7500 exploreP:0.0100\n",
      "Episode:939 meanR:9.3900 R:10.0 rate:0.0200 loss:2080556.3750 exploreP:0.0100\n",
      "Episode:940 meanR:9.3900 R:10.0 rate:0.0200 loss:1109250.8750 exploreP:0.0100\n",
      "Episode:941 meanR:9.3800 R:9.0 rate:0.0180 loss:2482849.2500 exploreP:0.0100\n",
      "Episode:942 meanR:9.3700 R:10.0 rate:0.0200 loss:1160561.2500 exploreP:0.0100\n",
      "Episode:943 meanR:9.3700 R:9.0 rate:0.0180 loss:1304551.2500 exploreP:0.0100\n",
      "Episode:944 meanR:9.3600 R:9.0 rate:0.0180 loss:1625638.3750 exploreP:0.0100\n",
      "Episode:945 meanR:9.3600 R:9.0 rate:0.0180 loss:463162.8125 exploreP:0.0100\n",
      "Episode:946 meanR:9.3600 R:10.0 rate:0.0200 loss:1522673.2500 exploreP:0.0100\n",
      "Episode:947 meanR:9.3600 R:10.0 rate:0.0200 loss:1498723.5000 exploreP:0.0100\n",
      "Episode:948 meanR:9.3400 R:8.0 rate:0.0160 loss:678464.6875 exploreP:0.0100\n",
      "Episode:949 meanR:9.3400 R:10.0 rate:0.0200 loss:2238023.7500 exploreP:0.0100\n",
      "Episode:950 meanR:9.3400 R:10.0 rate:0.0200 loss:1277767.7500 exploreP:0.0100\n",
      "Episode:951 meanR:9.3500 R:10.0 rate:0.0200 loss:267137.9688 exploreP:0.0100\n",
      "Episode:952 meanR:9.3600 R:10.0 rate:0.0200 loss:2280212.7500 exploreP:0.0100\n",
      "Episode:953 meanR:9.3500 R:9.0 rate:0.0180 loss:877896.4375 exploreP:0.0100\n",
      "Episode:954 meanR:9.3500 R:10.0 rate:0.0200 loss:420912.6875 exploreP:0.0100\n",
      "Episode:955 meanR:9.3500 R:9.0 rate:0.0180 loss:890241.9375 exploreP:0.0100\n",
      "Episode:956 meanR:9.3300 R:8.0 rate:0.0160 loss:1041229.9375 exploreP:0.0100\n",
      "Episode:957 meanR:9.3500 R:10.0 rate:0.0200 loss:160686.3906 exploreP:0.0100\n",
      "Episode:958 meanR:9.3400 R:9.0 rate:0.0180 loss:1181697.1250 exploreP:0.0100\n",
      "Episode:959 meanR:9.3600 R:11.0 rate:0.0220 loss:714939.7500 exploreP:0.0100\n",
      "Episode:960 meanR:9.3600 R:10.0 rate:0.0200 loss:1635545.0000 exploreP:0.0100\n",
      "Episode:961 meanR:9.3700 R:10.0 rate:0.0200 loss:731228.8125 exploreP:0.0100\n",
      "Episode:962 meanR:9.3700 R:9.0 rate:0.0180 loss:2505518.7500 exploreP:0.0100\n",
      "Episode:963 meanR:9.3500 R:8.0 rate:0.0160 loss:957296.8125 exploreP:0.0100\n",
      "Episode:964 meanR:9.3400 R:9.0 rate:0.0180 loss:1950781.5000 exploreP:0.0100\n",
      "Episode:965 meanR:9.3300 R:9.0 rate:0.0180 loss:230203.0938 exploreP:0.0100\n",
      "Episode:966 meanR:9.3200 R:8.0 rate:0.0160 loss:2920194.7500 exploreP:0.0100\n",
      "Episode:967 meanR:9.3000 R:9.0 rate:0.0180 loss:2889959.0000 exploreP:0.0100\n",
      "Episode:968 meanR:9.3000 R:9.0 rate:0.0180 loss:931054.4375 exploreP:0.0100\n",
      "Episode:969 meanR:9.2900 R:9.0 rate:0.0180 loss:834434.3125 exploreP:0.0100\n",
      "Episode:970 meanR:9.2800 R:9.0 rate:0.0180 loss:1601317.0000 exploreP:0.0100\n",
      "Episode:971 meanR:9.2700 R:8.0 rate:0.0160 loss:2021643.5000 exploreP:0.0100\n",
      "Episode:972 meanR:9.2800 R:10.0 rate:0.0200 loss:1598838.7500 exploreP:0.0100\n",
      "Episode:973 meanR:9.2800 R:9.0 rate:0.0180 loss:1814026.7500 exploreP:0.0100\n",
      "Episode:974 meanR:9.2700 R:9.0 rate:0.0180 loss:1148977.8750 exploreP:0.0100\n",
      "Episode:975 meanR:9.2800 R:10.0 rate:0.0200 loss:2127824.5000 exploreP:0.0100\n",
      "Episode:976 meanR:9.2600 R:10.0 rate:0.0200 loss:1296707.7500 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:977 meanR:9.2500 R:8.0 rate:0.0160 loss:1661964.8750 exploreP:0.0100\n",
      "Episode:978 meanR:9.2500 R:9.0 rate:0.0180 loss:3525825.0000 exploreP:0.0100\n",
      "Episode:979 meanR:9.2500 R:10.0 rate:0.0200 loss:2105830.2500 exploreP:0.0100\n",
      "Episode:980 meanR:9.2400 R:9.0 rate:0.0180 loss:2078941.3750 exploreP:0.0100\n",
      "Episode:981 meanR:9.2400 R:10.0 rate:0.0200 loss:2713848.5000 exploreP:0.0100\n",
      "Episode:982 meanR:9.2400 R:10.0 rate:0.0200 loss:1666684.3750 exploreP:0.0100\n",
      "Episode:983 meanR:9.2400 R:9.0 rate:0.0180 loss:4201303.5000 exploreP:0.0100\n",
      "Episode:984 meanR:9.2400 R:8.0 rate:0.0160 loss:212553.9531 exploreP:0.0100\n",
      "Episode:985 meanR:9.2500 R:10.0 rate:0.0200 loss:2680211.0000 exploreP:0.0100\n",
      "Episode:986 meanR:9.2500 R:10.0 rate:0.0200 loss:5072659.0000 exploreP:0.0100\n",
      "Episode:987 meanR:9.2700 R:10.0 rate:0.0200 loss:2601763.0000 exploreP:0.0100\n",
      "Episode:988 meanR:9.2700 R:8.0 rate:0.0160 loss:2111675.5000 exploreP:0.0100\n",
      "Episode:989 meanR:9.2500 R:9.0 rate:0.0180 loss:1930681.5000 exploreP:0.0100\n",
      "Episode:990 meanR:9.2600 R:10.0 rate:0.0200 loss:1459891.2500 exploreP:0.0100\n",
      "Episode:991 meanR:9.2700 R:11.0 rate:0.0220 loss:2658661.7500 exploreP:0.0100\n",
      "Episode:992 meanR:9.2800 R:10.0 rate:0.0200 loss:2218853.5000 exploreP:0.0100\n",
      "Episode:993 meanR:9.2700 R:9.0 rate:0.0180 loss:3744518.7500 exploreP:0.0100\n",
      "Episode:994 meanR:9.2700 R:9.0 rate:0.0180 loss:226281.1094 exploreP:0.0100\n",
      "Episode:995 meanR:9.2700 R:9.0 rate:0.0180 loss:2229545.0000 exploreP:0.0100\n",
      "Episode:996 meanR:9.2700 R:9.0 rate:0.0180 loss:2186550.0000 exploreP:0.0100\n",
      "Episode:997 meanR:9.2700 R:9.0 rate:0.0180 loss:1183646.1250 exploreP:0.0100\n",
      "Episode:998 meanR:9.2900 R:10.0 rate:0.0200 loss:1994750.6250 exploreP:0.0100\n",
      "Episode:999 meanR:9.3000 R:9.0 rate:0.0180 loss:5620394.0000 exploreP:0.0100\n",
      "Episode:1000 meanR:9.3100 R:9.0 rate:0.0180 loss:1000820.3125 exploreP:0.0100\n",
      "Episode:1001 meanR:9.3200 R:9.0 rate:0.0180 loss:2299145.2500 exploreP:0.0100\n",
      "Episode:1002 meanR:9.3100 R:8.0 rate:0.0160 loss:4257559.0000 exploreP:0.0100\n",
      "Episode:1003 meanR:9.3000 R:8.0 rate:0.0160 loss:252259.2812 exploreP:0.0100\n",
      "Episode:1004 meanR:9.3000 R:10.0 rate:0.0200 loss:4021263.5000 exploreP:0.0100\n",
      "Episode:1005 meanR:9.3200 R:10.0 rate:0.0200 loss:3410092.0000 exploreP:0.0100\n",
      "Episode:1006 meanR:9.3200 R:9.0 rate:0.0180 loss:2364564.5000 exploreP:0.0100\n",
      "Episode:1007 meanR:9.3400 R:10.0 rate:0.0200 loss:4631930.0000 exploreP:0.0100\n",
      "Episode:1008 meanR:9.3500 R:9.0 rate:0.0180 loss:3174352.7500 exploreP:0.0100\n",
      "Episode:1009 meanR:9.3500 R:9.0 rate:0.0180 loss:2552306.5000 exploreP:0.0100\n",
      "Episode:1010 meanR:9.3600 R:10.0 rate:0.0200 loss:3077647.5000 exploreP:0.0100\n",
      "Episode:1011 meanR:9.3600 R:9.0 rate:0.0180 loss:5173650.0000 exploreP:0.0100\n",
      "Episode:1012 meanR:9.3600 R:9.0 rate:0.0180 loss:297558.5625 exploreP:0.0100\n",
      "Episode:1013 meanR:9.3700 R:9.0 rate:0.0180 loss:3022938.2500 exploreP:0.0100\n",
      "Episode:1014 meanR:9.3500 R:8.0 rate:0.0160 loss:2173923.2500 exploreP:0.0100\n",
      "Episode:1015 meanR:9.3500 R:9.0 rate:0.0180 loss:1496169.2500 exploreP:0.0100\n",
      "Episode:1016 meanR:9.3300 R:8.0 rate:0.0160 loss:1872520.7500 exploreP:0.0100\n",
      "Episode:1017 meanR:9.3200 R:9.0 rate:0.0180 loss:6021490.0000 exploreP:0.0100\n",
      "Episode:1018 meanR:9.3100 R:9.0 rate:0.0180 loss:3416543.2500 exploreP:0.0100\n",
      "Episode:1019 meanR:9.3100 R:10.0 rate:0.0200 loss:1472347.1250 exploreP:0.0100\n",
      "Episode:1020 meanR:9.3100 R:10.0 rate:0.0200 loss:1543504.6250 exploreP:0.0100\n",
      "Episode:1021 meanR:9.3100 R:10.0 rate:0.0200 loss:2825300.2500 exploreP:0.0100\n",
      "Episode:1022 meanR:9.3100 R:9.0 rate:0.0180 loss:2032498.0000 exploreP:0.0100\n",
      "Episode:1023 meanR:9.3100 R:10.0 rate:0.0200 loss:5241623.0000 exploreP:0.0100\n",
      "Episode:1024 meanR:9.2800 R:9.0 rate:0.0180 loss:1830625.7500 exploreP:0.0100\n",
      "Episode:1025 meanR:9.3000 R:10.0 rate:0.0200 loss:5216541.5000 exploreP:0.0100\n",
      "Episode:1026 meanR:9.3300 R:11.0 rate:0.0220 loss:2251362.2500 exploreP:0.0100\n",
      "Episode:1027 meanR:9.3300 R:9.0 rate:0.0180 loss:6870826.5000 exploreP:0.0100\n",
      "Episode:1028 meanR:9.3300 R:10.0 rate:0.0200 loss:3175289.7500 exploreP:0.0100\n",
      "Episode:1029 meanR:9.3200 R:9.0 rate:0.0180 loss:1662785.6250 exploreP:0.0100\n",
      "Episode:1030 meanR:9.3300 R:11.0 rate:0.0220 loss:5018168.0000 exploreP:0.0100\n",
      "Episode:1031 meanR:9.3400 R:10.0 rate:0.0200 loss:3999180.7500 exploreP:0.0100\n",
      "Episode:1032 meanR:9.3500 R:10.0 rate:0.0200 loss:3901130.0000 exploreP:0.0100\n",
      "Episode:1033 meanR:9.3600 R:10.0 rate:0.0200 loss:4224492.5000 exploreP:0.0100\n",
      "Episode:1034 meanR:9.3700 R:10.0 rate:0.0200 loss:5340126.0000 exploreP:0.0100\n",
      "Episode:1035 meanR:9.3700 R:10.0 rate:0.0200 loss:4686263.0000 exploreP:0.0100\n",
      "Episode:1036 meanR:9.3600 R:9.0 rate:0.0180 loss:359081.8750 exploreP:0.0100\n",
      "Episode:1037 meanR:9.3600 R:9.0 rate:0.0180 loss:3311743.7500 exploreP:0.0100\n",
      "Episode:1038 meanR:9.3600 R:9.0 rate:0.0180 loss:2734143.5000 exploreP:0.0100\n",
      "Episode:1039 meanR:9.3500 R:9.0 rate:0.0180 loss:6391362.5000 exploreP:0.0100\n",
      "Episode:1040 meanR:9.3500 R:10.0 rate:0.0200 loss:1562523.2500 exploreP:0.0100\n",
      "Episode:1041 meanR:9.3600 R:10.0 rate:0.0200 loss:4288449.0000 exploreP:0.0100\n",
      "Episode:1042 meanR:9.3500 R:9.0 rate:0.0180 loss:3316339.2500 exploreP:0.0100\n",
      "Episode:1043 meanR:9.3600 R:10.0 rate:0.0200 loss:2238013.7500 exploreP:0.0100\n",
      "Episode:1044 meanR:9.3600 R:9.0 rate:0.0180 loss:4217954.0000 exploreP:0.0100\n",
      "Episode:1045 meanR:9.3700 R:10.0 rate:0.0200 loss:3113241.5000 exploreP:0.0100\n",
      "Episode:1046 meanR:9.3500 R:8.0 rate:0.0160 loss:5724303.0000 exploreP:0.0100\n",
      "Episode:1047 meanR:9.3400 R:9.0 rate:0.0180 loss:3016183.5000 exploreP:0.0100\n",
      "Episode:1048 meanR:9.3600 R:10.0 rate:0.0200 loss:3575262.7500 exploreP:0.0100\n",
      "Episode:1049 meanR:9.3400 R:8.0 rate:0.0160 loss:6268216.0000 exploreP:0.0100\n",
      "Episode:1050 meanR:9.3300 R:9.0 rate:0.0180 loss:1069904.8750 exploreP:0.0100\n",
      "Episode:1051 meanR:9.3200 R:9.0 rate:0.0180 loss:5952744.0000 exploreP:0.0100\n",
      "Episode:1052 meanR:9.3200 R:10.0 rate:0.0200 loss:5916207.0000 exploreP:0.0100\n",
      "Episode:1053 meanR:9.3200 R:9.0 rate:0.0180 loss:1998715.3750 exploreP:0.0100\n",
      "Episode:1054 meanR:9.3200 R:10.0 rate:0.0200 loss:13309266.0000 exploreP:0.0100\n",
      "Episode:1055 meanR:9.3300 R:10.0 rate:0.0200 loss:4085972.7500 exploreP:0.0100\n",
      "Episode:1056 meanR:9.3500 R:10.0 rate:0.0200 loss:3058240.0000 exploreP:0.0100\n",
      "Episode:1057 meanR:9.3400 R:9.0 rate:0.0180 loss:3464652.7500 exploreP:0.0100\n",
      "Episode:1058 meanR:9.3500 R:10.0 rate:0.0200 loss:5104629.0000 exploreP:0.0100\n",
      "Episode:1059 meanR:9.3200 R:8.0 rate:0.0160 loss:8320748.0000 exploreP:0.0100\n",
      "Episode:1060 meanR:9.3400 R:12.0 rate:0.0240 loss:2009359.8750 exploreP:0.0100\n",
      "Episode:1061 meanR:9.3400 R:10.0 rate:0.0200 loss:5557039.0000 exploreP:0.0100\n",
      "Episode:1062 meanR:9.3500 R:10.0 rate:0.0200 loss:5599260.0000 exploreP:0.0100\n",
      "Episode:1063 meanR:9.3600 R:9.0 rate:0.0180 loss:4219306.5000 exploreP:0.0100\n",
      "Episode:1064 meanR:9.3700 R:10.0 rate:0.0200 loss:5387581.5000 exploreP:0.0100\n",
      "Episode:1065 meanR:9.3700 R:9.0 rate:0.0180 loss:2244447.5000 exploreP:0.0100\n",
      "Episode:1066 meanR:9.3900 R:10.0 rate:0.0200 loss:7159818.5000 exploreP:0.0100\n",
      "Episode:1067 meanR:9.4000 R:10.0 rate:0.0200 loss:2056828.6250 exploreP:0.0100\n",
      "Episode:1068 meanR:9.4000 R:9.0 rate:0.0180 loss:3959176.5000 exploreP:0.0100\n",
      "Episode:1069 meanR:9.4000 R:9.0 rate:0.0180 loss:886004.0000 exploreP:0.0100\n",
      "Episode:1070 meanR:9.4000 R:9.0 rate:0.0180 loss:7051347.5000 exploreP:0.0100\n",
      "Episode:1071 meanR:9.4100 R:9.0 rate:0.0180 loss:7964143.0000 exploreP:0.0100\n",
      "Episode:1072 meanR:9.4100 R:10.0 rate:0.0200 loss:7150508.0000 exploreP:0.0100\n",
      "Episode:1073 meanR:9.4100 R:9.0 rate:0.0180 loss:7241603.5000 exploreP:0.0100\n",
      "Episode:1074 meanR:9.4100 R:9.0 rate:0.0180 loss:6834432.0000 exploreP:0.0100\n",
      "Episode:1075 meanR:9.4000 R:9.0 rate:0.0180 loss:3275476.7500 exploreP:0.0100\n",
      "Episode:1076 meanR:9.3800 R:8.0 rate:0.0160 loss:3923680.5000 exploreP:0.0100\n",
      "Episode:1077 meanR:9.4000 R:10.0 rate:0.0200 loss:3831068.0000 exploreP:0.0100\n",
      "Episode:1078 meanR:9.4000 R:9.0 rate:0.0180 loss:8461003.0000 exploreP:0.0100\n",
      "Episode:1079 meanR:9.3900 R:9.0 rate:0.0180 loss:1651278.7500 exploreP:0.0100\n",
      "Episode:1080 meanR:9.4000 R:10.0 rate:0.0200 loss:10482509.0000 exploreP:0.0100\n",
      "Episode:1081 meanR:9.4000 R:10.0 rate:0.0200 loss:10184082.0000 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1082 meanR:9.4000 R:10.0 rate:0.0200 loss:4575794.0000 exploreP:0.0100\n",
      "Episode:1083 meanR:9.4100 R:10.0 rate:0.0200 loss:3186307.2500 exploreP:0.0100\n",
      "Episode:1084 meanR:9.4300 R:10.0 rate:0.0200 loss:10977310.0000 exploreP:0.0100\n",
      "Episode:1085 meanR:9.4200 R:9.0 rate:0.0180 loss:2820195.7500 exploreP:0.0100\n",
      "Episode:1086 meanR:9.4100 R:9.0 rate:0.0180 loss:4689466.5000 exploreP:0.0100\n",
      "Episode:1087 meanR:9.4100 R:10.0 rate:0.0200 loss:2546262.2500 exploreP:0.0100\n",
      "Episode:1088 meanR:9.4300 R:10.0 rate:0.0200 loss:9643470.0000 exploreP:0.0100\n",
      "Episode:1089 meanR:9.4400 R:10.0 rate:0.0200 loss:5444814.0000 exploreP:0.0100\n",
      "Episode:1090 meanR:9.4400 R:10.0 rate:0.0200 loss:3942244.7500 exploreP:0.0100\n",
      "Episode:1091 meanR:9.4300 R:10.0 rate:0.0200 loss:7302964.0000 exploreP:0.0100\n",
      "Episode:1092 meanR:9.4100 R:8.0 rate:0.0160 loss:10423755.0000 exploreP:0.0100\n",
      "Episode:1093 meanR:9.4100 R:9.0 rate:0.0180 loss:3093316.7500 exploreP:0.0100\n",
      "Episode:1094 meanR:9.4200 R:10.0 rate:0.0200 loss:2175077.0000 exploreP:0.0100\n",
      "Episode:1095 meanR:9.4300 R:10.0 rate:0.0200 loss:5269377.5000 exploreP:0.0100\n",
      "Episode:1096 meanR:9.4400 R:10.0 rate:0.0200 loss:5214954.0000 exploreP:0.0100\n",
      "Episode:1097 meanR:9.4300 R:8.0 rate:0.0160 loss:6283243.0000 exploreP:0.0100\n",
      "Episode:1098 meanR:9.4100 R:8.0 rate:0.0160 loss:11473450.0000 exploreP:0.0100\n",
      "Episode:1099 meanR:9.4000 R:8.0 rate:0.0160 loss:4143012.5000 exploreP:0.0100\n",
      "Episode:1100 meanR:9.3900 R:8.0 rate:0.0160 loss:11629073.0000 exploreP:0.0100\n",
      "Episode:1101 meanR:9.3800 R:8.0 rate:0.0160 loss:2538128.0000 exploreP:0.0100\n",
      "Episode:1102 meanR:9.4000 R:10.0 rate:0.0200 loss:7416999.0000 exploreP:0.0100\n",
      "Episode:1103 meanR:9.4100 R:9.0 rate:0.0180 loss:8137061.5000 exploreP:0.0100\n",
      "Episode:1104 meanR:9.3900 R:8.0 rate:0.0160 loss:1783704.3750 exploreP:0.0100\n",
      "Episode:1105 meanR:9.3800 R:9.0 rate:0.0180 loss:3951418.2500 exploreP:0.0100\n",
      "Episode:1106 meanR:9.4000 R:11.0 rate:0.0220 loss:7834318.0000 exploreP:0.0100\n",
      "Episode:1107 meanR:9.3900 R:9.0 rate:0.0180 loss:12200928.0000 exploreP:0.0100\n",
      "Episode:1108 meanR:9.4000 R:10.0 rate:0.0200 loss:16021576.0000 exploreP:0.0100\n",
      "Episode:1109 meanR:9.4000 R:9.0 rate:0.0180 loss:4578700.0000 exploreP:0.0100\n",
      "Episode:1110 meanR:9.3900 R:9.0 rate:0.0180 loss:13389236.0000 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "                        \n",
    "            # Rating and training the memory\n",
    "            rates = np.array(memory.buffer)[:, 5]\n",
    "            rated_mem = np.array(memory.buffer)[rates >= (max(rates)*0.1)]\n",
    "            #rated_mem = np.array(memory.buffer)\n",
    "            batch = sample(ListArr=rated_mem, batch_size=batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            #rates = np.array([each[5] for each in batch])\n",
    "            #nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Creating a gym env\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# A training graph session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "# Close the env at the end\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
