{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/aras/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "num_steps = 0\n",
    "while True: # infinite number of steps\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1.        , 0.        , 0.        , 0.        , 0.20790455,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.01327663,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.1713129 ,\n",
       "         0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.01027161,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.02920904,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.00962341,\n",
       "         0.        , 0.        ]),\n",
       "  3,\n",
       "  array([1.        , 0.        , 0.        , 0.        , 0.16192973,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.59092385,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.00976282,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.39982799,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.01006025,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.44728744,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.0180034 ,\n",
       "         0.        , 0.        ]),\n",
       "  0.0,\n",
       "  0.0],\n",
       " 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.        , 0.        , 0.        , 0.20790455,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.01327663,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.1713129 ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.01027161,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.02920904,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.00962341,\n",
       "        0.        , 0.        ]),\n",
       " 3,\n",
       " array([1.        , 0.        , 0.        , 0.        , 0.16192973,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.59092385,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.00976282,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.39982799,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.01006025,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.44728744,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.0180034 ,\n",
       "        0.        , 0.        ]),\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[1] for each in batch])\n",
    "actions = np.array([each[0] for each in batch])\n",
    "next_states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 37) (300,) (300, 37) (300,)\n",
      "float64 int64 float64 float64\n",
      "9.81694221496582 -10.490506172180176 21.307448387145996\n",
      "9.81694221496582 -10.490506172180176\n",
      "3 0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return states, actions, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "action_size = 4\n",
    "state_size = 37\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 64            # memory capacity\n",
    "batch_size = 64             # experience mini-batch size\n",
    "gamma = 0.99                 # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37) (?, 74)\n",
      "(1, ?, 74) (1, 74)\n",
      "(1, ?, 74) (1, 74)\n",
      "(?, 74)\n",
      "(?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.states.append(np.zeros([1, hidden_size])) # initial_states for rnn/mem\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 74)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial_states = memory.states\n",
    "memory.states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.0 Steps:299 loss:0.0292\n",
      "Episode:1 meanR:-1.0000 R:-2.0 Steps:299 loss:0.0176\n",
      "Episode:2 meanR:-1.0000 R:-1.0 Steps:299 loss:0.0139\n",
      "Episode:3 meanR:-0.7500 R:0.0 Steps:299 loss:0.0239\n",
      "Episode:4 meanR:-0.8000 R:-1.0 Steps:299 loss:0.0170\n",
      "Episode:5 meanR:-0.8333 R:-1.0 Steps:299 loss:0.0079\n",
      "Episode:6 meanR:-0.4286 R:2.0 Steps:299 loss:0.0143\n",
      "Episode:7 meanR:-0.3750 R:0.0 Steps:299 loss:0.0275\n",
      "Episode:8 meanR:0.1111 R:4.0 Steps:299 loss:0.0398\n",
      "Episode:9 meanR:0.2000 R:1.0 Steps:299 loss:0.0271\n",
      "Episode:10 meanR:0.4545 R:3.0 Steps:299 loss:0.0275\n",
      "Episode:11 meanR:0.4167 R:0.0 Steps:299 loss:0.0266\n",
      "Episode:12 meanR:0.2308 R:-2.0 Steps:299 loss:0.0287\n",
      "Episode:13 meanR:0.2857 R:1.0 Steps:299 loss:0.0135\n",
      "Episode:14 meanR:0.2667 R:0.0 Steps:299 loss:0.0075\n",
      "Episode:15 meanR:0.1250 R:-2.0 Steps:299 loss:0.0150\n",
      "Episode:16 meanR:0.1765 R:1.0 Steps:299 loss:0.0088\n",
      "Episode:17 meanR:0.1667 R:0.0 Steps:299 loss:0.0057\n",
      "Episode:18 meanR:0.1053 R:-1.0 Steps:299 loss:0.0113\n",
      "Episode:19 meanR:0.0500 R:-1.0 Steps:299 loss:0.0130\n",
      "Episode:20 meanR:-0.0476 R:-2.0 Steps:299 loss:0.0095\n",
      "Episode:21 meanR:-0.0909 R:-1.0 Steps:299 loss:0.0082\n",
      "Episode:22 meanR:-0.0435 R:1.0 Steps:299 loss:0.0084\n",
      "Episode:23 meanR:-0.0417 R:0.0 Steps:299 loss:0.0062\n",
      "Episode:24 meanR:-0.0800 R:-1.0 Steps:299 loss:0.0046\n",
      "Episode:25 meanR:-0.0769 R:0.0 Steps:299 loss:0.0020\n",
      "Episode:26 meanR:-0.1111 R:-1.0 Steps:299 loss:0.0039\n",
      "Episode:27 meanR:-0.1429 R:-1.0 Steps:299 loss:0.0069\n",
      "Episode:28 meanR:-0.1379 R:0.0 Steps:299 loss:0.0031\n",
      "Episode:29 meanR:-0.1333 R:0.0 Steps:299 loss:0.0057\n",
      "Episode:30 meanR:-0.0968 R:1.0 Steps:299 loss:0.0326\n",
      "Episode:31 meanR:-0.0938 R:0.0 Steps:299 loss:0.0279\n",
      "Episode:32 meanR:-0.1818 R:-3.0 Steps:299 loss:0.0203\n",
      "Episode:33 meanR:-0.2353 R:-2.0 Steps:299 loss:0.0321\n",
      "Episode:34 meanR:-0.2000 R:1.0 Steps:299 loss:0.0180\n",
      "Episode:35 meanR:-0.1111 R:3.0 Steps:299 loss:0.0433\n",
      "Episode:36 meanR:-0.1351 R:-1.0 Steps:299 loss:0.0728\n",
      "Episode:37 meanR:-0.0789 R:2.0 Steps:299 loss:0.0366\n",
      "Episode:38 meanR:0.0256 R:4.0 Steps:299 loss:0.0401\n",
      "Episode:39 meanR:0.0000 R:-1.0 Steps:299 loss:0.0512\n",
      "Episode:40 meanR:0.2195 R:9.0 Steps:299 loss:0.0386\n",
      "Episode:41 meanR:0.1905 R:-1.0 Steps:299 loss:0.0825\n",
      "Episode:42 meanR:0.1628 R:-1.0 Steps:299 loss:0.0444\n",
      "Episode:43 meanR:0.2045 R:2.0 Steps:299 loss:0.0194\n",
      "Episode:44 meanR:0.2667 R:3.0 Steps:299 loss:0.0214\n",
      "Episode:45 meanR:0.2609 R:0.0 Steps:299 loss:0.0205\n",
      "Episode:46 meanR:0.2553 R:0.0 Steps:299 loss:0.0088\n",
      "Episode:47 meanR:0.3542 R:5.0 Steps:299 loss:0.0203\n",
      "Episode:48 meanR:0.5918 R:12.0 Steps:299 loss:0.0403\n",
      "Episode:49 meanR:0.6800 R:5.0 Steps:299 loss:0.0527\n",
      "Episode:50 meanR:0.7843 R:6.0 Steps:299 loss:0.0603\n",
      "Episode:51 meanR:0.9615 R:10.0 Steps:299 loss:0.0624\n",
      "Episode:52 meanR:1.1509 R:11.0 Steps:299 loss:0.0673\n",
      "Episode:53 meanR:1.2407 R:6.0 Steps:299 loss:0.0712\n",
      "Episode:54 meanR:1.3455 R:7.0 Steps:299 loss:0.0883\n",
      "Episode:55 meanR:1.5179 R:11.0 Steps:299 loss:0.0499\n",
      "Episode:56 meanR:1.5789 R:5.0 Steps:299 loss:0.0618\n",
      "Episode:57 meanR:1.7931 R:14.0 Steps:299 loss:0.0500\n",
      "Episode:58 meanR:1.9831 R:13.0 Steps:299 loss:0.0666\n",
      "Episode:59 meanR:2.1667 R:13.0 Steps:299 loss:0.0814\n",
      "Episode:60 meanR:2.1967 R:4.0 Steps:299 loss:0.0806\n",
      "Episode:61 meanR:2.3226 R:10.0 Steps:299 loss:0.0425\n",
      "Episode:62 meanR:2.4921 R:13.0 Steps:299 loss:0.0961\n",
      "Episode:63 meanR:2.5781 R:8.0 Steps:299 loss:0.0702\n",
      "Episode:64 meanR:2.6923 R:10.0 Steps:299 loss:0.0615\n",
      "Episode:65 meanR:2.7879 R:9.0 Steps:299 loss:0.0773\n",
      "Episode:66 meanR:2.8507 R:7.0 Steps:299 loss:0.0613\n",
      "Episode:67 meanR:2.8824 R:5.0 Steps:299 loss:0.0650\n",
      "Episode:68 meanR:2.8696 R:2.0 Steps:299 loss:0.0241\n",
      "Episode:69 meanR:2.9000 R:5.0 Steps:299 loss:0.0338\n",
      "Episode:70 meanR:2.9014 R:3.0 Steps:299 loss:0.0353\n",
      "Episode:71 meanR:2.9583 R:7.0 Steps:299 loss:0.0353\n",
      "Episode:72 meanR:2.9589 R:3.0 Steps:299 loss:0.0338\n",
      "Episode:73 meanR:3.0270 R:8.0 Steps:299 loss:0.0459\n",
      "Episode:74 meanR:3.1200 R:10.0 Steps:299 loss:0.0391\n",
      "Episode:75 meanR:3.1974 R:9.0 Steps:299 loss:0.0523\n",
      "Episode:76 meanR:3.2338 R:6.0 Steps:299 loss:0.0672\n",
      "Episode:77 meanR:3.3333 R:11.0 Steps:299 loss:0.0678\n",
      "Episode:78 meanR:3.4684 R:14.0 Steps:299 loss:0.0617\n",
      "Episode:79 meanR:3.5750 R:12.0 Steps:299 loss:0.1078\n",
      "Episode:80 meanR:3.6049 R:6.0 Steps:299 loss:0.0672\n",
      "Episode:81 meanR:3.6707 R:9.0 Steps:299 loss:0.0556\n",
      "Episode:82 meanR:3.7831 R:13.0 Steps:299 loss:0.0802\n",
      "Episode:83 meanR:3.7857 R:4.0 Steps:299 loss:0.0660\n",
      "Episode:84 meanR:3.8824 R:12.0 Steps:299 loss:0.0836\n",
      "Episode:85 meanR:4.0349 R:17.0 Steps:299 loss:0.0681\n",
      "Episode:86 meanR:4.0230 R:3.0 Steps:299 loss:0.0821\n",
      "Episode:87 meanR:3.9886 R:1.0 Steps:299 loss:0.0408\n",
      "Episode:88 meanR:3.9551 R:1.0 Steps:299 loss:0.0242\n",
      "Episode:89 meanR:3.9778 R:6.0 Steps:299 loss:0.0244\n",
      "Episode:90 meanR:4.0440 R:10.0 Steps:299 loss:0.0361\n",
      "Episode:91 meanR:4.0435 R:4.0 Steps:299 loss:0.0573\n",
      "Episode:92 meanR:4.0215 R:2.0 Steps:299 loss:0.0251\n",
      "Episode:93 meanR:4.0745 R:9.0 Steps:299 loss:0.0534\n",
      "Episode:94 meanR:4.1368 R:10.0 Steps:299 loss:0.0746\n",
      "Episode:95 meanR:4.1458 R:5.0 Steps:299 loss:0.0504\n",
      "Episode:96 meanR:4.1753 R:7.0 Steps:299 loss:0.0572\n",
      "Episode:97 meanR:4.2755 R:14.0 Steps:299 loss:0.0904\n",
      "Episode:98 meanR:4.3434 R:11.0 Steps:299 loss:0.0644\n",
      "Episode:99 meanR:4.3400 R:4.0 Steps:299 loss:0.0473\n",
      "Episode:100 meanR:4.4400 R:10.0 Steps:299 loss:0.0515\n",
      "Episode:101 meanR:4.4700 R:1.0 Steps:299 loss:0.0334\n",
      "Episode:102 meanR:4.5100 R:3.0 Steps:299 loss:0.0261\n",
      "Episode:103 meanR:4.6000 R:9.0 Steps:299 loss:0.0533\n",
      "Episode:104 meanR:4.7100 R:10.0 Steps:299 loss:0.0511\n",
      "Episode:105 meanR:4.7900 R:7.0 Steps:299 loss:0.0469\n",
      "Episode:106 meanR:4.8600 R:9.0 Steps:299 loss:0.0521\n",
      "Episode:107 meanR:4.9500 R:9.0 Steps:299 loss:0.0317\n",
      "Episode:108 meanR:4.9400 R:3.0 Steps:299 loss:0.0278\n",
      "Episode:109 meanR:5.0100 R:8.0 Steps:299 loss:0.0397\n",
      "Episode:110 meanR:5.1200 R:14.0 Steps:299 loss:0.0567\n",
      "Episode:111 meanR:5.1900 R:7.0 Steps:299 loss:0.0897\n",
      "Episode:112 meanR:5.3100 R:10.0 Steps:299 loss:0.0835\n",
      "Episode:113 meanR:5.3500 R:5.0 Steps:299 loss:0.0446\n",
      "Episode:114 meanR:5.3900 R:4.0 Steps:299 loss:0.0527\n",
      "Episode:115 meanR:5.4600 R:5.0 Steps:299 loss:0.0249\n",
      "Episode:116 meanR:5.4500 R:0.0 Steps:299 loss:0.0411\n",
      "Episode:117 meanR:5.4500 R:0.0 Steps:299 loss:0.0321\n",
      "Episode:118 meanR:5.5000 R:4.0 Steps:299 loss:0.0293\n",
      "Episode:119 meanR:5.5200 R:1.0 Steps:299 loss:0.0450\n",
      "Episode:120 meanR:5.6000 R:6.0 Steps:299 loss:0.0452\n",
      "Episode:121 meanR:5.7200 R:11.0 Steps:299 loss:0.0290\n",
      "Episode:122 meanR:5.7600 R:5.0 Steps:299 loss:0.0412\n",
      "Episode:123 meanR:5.8700 R:11.0 Steps:299 loss:0.0585\n",
      "Episode:124 meanR:5.9900 R:11.0 Steps:299 loss:0.0481\n",
      "Episode:125 meanR:6.0300 R:4.0 Steps:299 loss:0.0763\n",
      "Episode:126 meanR:6.1400 R:10.0 Steps:299 loss:0.0402\n",
      "Episode:127 meanR:6.2000 R:5.0 Steps:299 loss:0.0439\n",
      "Episode:128 meanR:6.3400 R:14.0 Steps:299 loss:0.0647\n",
      "Episode:129 meanR:6.4200 R:8.0 Steps:299 loss:0.0629\n",
      "Episode:130 meanR:6.4100 R:0.0 Steps:299 loss:0.0737\n",
      "Episode:131 meanR:6.4900 R:8.0 Steps:299 loss:0.0480\n",
      "Episode:132 meanR:6.6200 R:10.0 Steps:299 loss:0.0469\n",
      "Episode:133 meanR:6.7700 R:13.0 Steps:299 loss:0.0595\n",
      "Episode:134 meanR:6.7900 R:3.0 Steps:299 loss:0.0745\n",
      "Episode:135 meanR:6.8300 R:7.0 Steps:299 loss:0.0300\n",
      "Episode:136 meanR:6.9300 R:9.0 Steps:299 loss:0.0493\n",
      "Episode:137 meanR:7.0800 R:17.0 Steps:299 loss:0.0594\n",
      "Episode:138 meanR:7.1100 R:7.0 Steps:299 loss:0.0318\n",
      "Episode:139 meanR:7.2100 R:9.0 Steps:299 loss:0.0663\n",
      "Episode:140 meanR:7.1500 R:3.0 Steps:299 loss:0.0654\n",
      "Episode:141 meanR:7.2300 R:7.0 Steps:299 loss:0.0420\n",
      "Episode:142 meanR:7.2900 R:5.0 Steps:299 loss:0.0433\n",
      "Episode:143 meanR:7.3000 R:3.0 Steps:299 loss:0.0492\n",
      "Episode:144 meanR:7.3300 R:6.0 Steps:299 loss:0.0589\n",
      "Episode:145 meanR:7.3600 R:3.0 Steps:299 loss:0.0654\n",
      "Episode:146 meanR:7.4000 R:4.0 Steps:299 loss:0.0633\n",
      "Episode:147 meanR:7.3500 R:0.0 Steps:299 loss:0.0385\n",
      "Episode:148 meanR:7.2700 R:4.0 Steps:299 loss:0.0338\n",
      "Episode:149 meanR:7.2800 R:6.0 Steps:299 loss:0.0344\n",
      "Episode:150 meanR:7.2500 R:3.0 Steps:299 loss:0.0488\n",
      "Episode:151 meanR:7.2900 R:14.0 Steps:299 loss:0.0520\n",
      "Episode:152 meanR:7.2700 R:9.0 Steps:299 loss:0.0736\n",
      "Episode:153 meanR:7.3200 R:11.0 Steps:299 loss:0.0652\n",
      "Episode:154 meanR:7.3700 R:12.0 Steps:299 loss:0.0497\n",
      "Episode:155 meanR:7.3000 R:4.0 Steps:299 loss:0.0704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:156 meanR:7.3200 R:7.0 Steps:299 loss:0.0578\n",
      "Episode:157 meanR:7.3300 R:15.0 Steps:299 loss:0.0550\n",
      "Episode:158 meanR:7.2700 R:7.0 Steps:299 loss:0.0957\n",
      "Episode:159 meanR:7.2400 R:10.0 Steps:299 loss:0.0450\n",
      "Episode:160 meanR:7.2400 R:4.0 Steps:299 loss:0.0851\n",
      "Episode:161 meanR:7.2200 R:8.0 Steps:299 loss:0.0679\n",
      "Episode:162 meanR:7.1600 R:7.0 Steps:299 loss:0.0450\n",
      "Episode:163 meanR:7.2100 R:13.0 Steps:299 loss:0.0608\n",
      "Episode:164 meanR:7.2600 R:15.0 Steps:299 loss:0.1260\n",
      "Episode:165 meanR:7.2500 R:8.0 Steps:299 loss:0.0868\n",
      "Episode:166 meanR:7.2000 R:2.0 Steps:299 loss:0.0674\n",
      "Episode:167 meanR:7.2500 R:10.0 Steps:299 loss:0.0705\n",
      "Episode:168 meanR:7.3300 R:10.0 Steps:299 loss:0.0600\n",
      "Episode:169 meanR:7.3300 R:5.0 Steps:299 loss:0.0620\n",
      "Episode:170 meanR:7.3500 R:5.0 Steps:299 loss:0.0500\n",
      "Episode:171 meanR:7.4200 R:14.0 Steps:299 loss:0.0360\n",
      "Episode:172 meanR:7.4800 R:9.0 Steps:299 loss:0.0616\n",
      "Episode:173 meanR:7.5100 R:11.0 Steps:299 loss:0.0628\n",
      "Episode:174 meanR:7.4700 R:6.0 Steps:299 loss:0.0565\n",
      "Episode:175 meanR:7.5200 R:14.0 Steps:299 loss:0.0895\n",
      "Episode:176 meanR:7.5600 R:10.0 Steps:299 loss:0.0531\n",
      "Episode:177 meanR:7.5100 R:6.0 Steps:299 loss:0.0683\n",
      "Episode:178 meanR:7.4800 R:11.0 Steps:299 loss:0.0458\n",
      "Episode:179 meanR:7.4200 R:6.0 Steps:299 loss:0.0632\n",
      "Episode:180 meanR:7.5200 R:16.0 Steps:299 loss:0.0479\n",
      "Episode:181 meanR:7.4900 R:6.0 Steps:299 loss:0.1071\n",
      "Episode:182 meanR:7.4400 R:8.0 Steps:299 loss:0.0767\n",
      "Episode:183 meanR:7.5100 R:11.0 Steps:299 loss:0.0922\n",
      "Episode:184 meanR:7.4700 R:8.0 Steps:299 loss:0.0687\n",
      "Episode:185 meanR:7.4000 R:10.0 Steps:299 loss:0.0579\n",
      "Episode:186 meanR:7.4300 R:6.0 Steps:299 loss:0.0545\n",
      "Episode:187 meanR:7.4900 R:7.0 Steps:299 loss:0.0650\n",
      "Episode:188 meanR:7.5200 R:4.0 Steps:299 loss:0.0578\n",
      "Episode:189 meanR:7.4600 R:0.0 Steps:299 loss:0.0268\n",
      "Episode:190 meanR:7.4500 R:9.0 Steps:299 loss:0.0281\n",
      "Episode:191 meanR:7.5200 R:11.0 Steps:299 loss:0.0595\n",
      "Episode:192 meanR:7.5800 R:8.0 Steps:299 loss:0.0658\n",
      "Episode:193 meanR:7.5800 R:9.0 Steps:299 loss:0.0766\n",
      "Episode:194 meanR:7.5400 R:6.0 Steps:299 loss:0.0817\n",
      "Episode:195 meanR:7.6500 R:16.0 Steps:299 loss:0.0635\n",
      "Episode:196 meanR:7.6400 R:6.0 Steps:299 loss:0.0555\n",
      "Episode:197 meanR:7.5600 R:6.0 Steps:299 loss:0.0573\n",
      "Episode:198 meanR:7.5200 R:7.0 Steps:299 loss:0.0740\n",
      "Episode:199 meanR:7.6200 R:14.0 Steps:299 loss:0.0396\n",
      "Episode:200 meanR:7.5800 R:6.0 Steps:299 loss:0.0552\n",
      "Episode:201 meanR:7.6700 R:10.0 Steps:299 loss:0.0582\n",
      "Episode:202 meanR:7.6900 R:5.0 Steps:299 loss:0.0455\n",
      "Episode:203 meanR:7.6800 R:8.0 Steps:299 loss:0.0572\n",
      "Episode:204 meanR:7.6800 R:10.0 Steps:299 loss:0.0651\n",
      "Episode:205 meanR:7.7000 R:9.0 Steps:299 loss:0.0488\n",
      "Episode:206 meanR:7.7000 R:9.0 Steps:299 loss:0.0914\n",
      "Episode:207 meanR:7.6600 R:5.0 Steps:299 loss:0.0719\n",
      "Episode:208 meanR:7.7100 R:8.0 Steps:299 loss:0.0446\n",
      "Episode:209 meanR:7.7500 R:12.0 Steps:299 loss:0.0722\n",
      "Episode:210 meanR:7.6600 R:5.0 Steps:299 loss:0.0496\n",
      "Episode:211 meanR:7.6100 R:2.0 Steps:299 loss:0.0397\n",
      "Episode:212 meanR:7.6100 R:10.0 Steps:299 loss:0.0335\n",
      "Episode:213 meanR:7.6000 R:4.0 Steps:299 loss:0.0447\n",
      "Episode:214 meanR:7.5900 R:3.0 Steps:299 loss:0.0545\n",
      "Episode:215 meanR:7.6100 R:7.0 Steps:299 loss:0.0378\n",
      "Episode:216 meanR:7.6900 R:8.0 Steps:299 loss:0.0384\n",
      "Episode:217 meanR:7.8100 R:12.0 Steps:299 loss:0.0530\n",
      "Episode:218 meanR:7.8700 R:10.0 Steps:299 loss:0.0693\n",
      "Episode:219 meanR:7.8800 R:2.0 Steps:299 loss:0.0522\n",
      "Episode:220 meanR:7.8200 R:0.0 Steps:299 loss:0.0410\n",
      "Episode:221 meanR:7.7700 R:6.0 Steps:299 loss:0.0299\n",
      "Episode:222 meanR:7.9000 R:18.0 Steps:299 loss:0.0599\n",
      "Episode:223 meanR:7.8700 R:8.0 Steps:299 loss:0.0415\n",
      "Episode:224 meanR:7.8600 R:10.0 Steps:299 loss:0.0428\n",
      "Episode:225 meanR:7.9600 R:14.0 Steps:299 loss:0.0778\n",
      "Episode:226 meanR:7.9400 R:8.0 Steps:299 loss:0.0794\n",
      "Episode:227 meanR:8.0600 R:17.0 Steps:299 loss:0.0779\n",
      "Episode:228 meanR:8.0500 R:13.0 Steps:299 loss:0.1030\n",
      "Episode:229 meanR:8.0400 R:7.0 Steps:299 loss:0.1001\n",
      "Episode:230 meanR:8.1800 R:14.0 Steps:299 loss:0.0601\n",
      "Episode:231 meanR:8.1500 R:5.0 Steps:299 loss:0.0637\n",
      "Episode:232 meanR:8.0900 R:4.0 Steps:299 loss:0.0523\n",
      "Episode:233 meanR:8.0400 R:8.0 Steps:299 loss:0.0397\n",
      "Episode:234 meanR:8.1200 R:11.0 Steps:299 loss:0.0446\n",
      "Episode:235 meanR:8.1100 R:6.0 Steps:299 loss:0.0777\n",
      "Episode:236 meanR:8.0700 R:5.0 Steps:299 loss:0.0540\n",
      "Episode:237 meanR:8.0000 R:10.0 Steps:299 loss:0.0444\n",
      "Episode:238 meanR:8.0700 R:14.0 Steps:299 loss:0.0761\n",
      "Episode:239 meanR:8.0200 R:4.0 Steps:299 loss:0.0969\n",
      "Episode:240 meanR:8.0700 R:8.0 Steps:299 loss:0.0340\n",
      "Episode:241 meanR:8.0900 R:9.0 Steps:299 loss:0.0441\n",
      "Episode:242 meanR:8.1900 R:15.0 Steps:299 loss:0.0717\n",
      "Episode:243 meanR:8.2400 R:8.0 Steps:299 loss:0.0572\n",
      "Episode:244 meanR:8.2600 R:8.0 Steps:299 loss:0.0592\n",
      "Episode:245 meanR:8.2800 R:5.0 Steps:299 loss:0.0553\n",
      "Episode:246 meanR:8.2800 R:4.0 Steps:299 loss:0.0475\n",
      "Episode:247 meanR:8.4000 R:12.0 Steps:299 loss:0.0633\n",
      "Episode:248 meanR:8.4600 R:10.0 Steps:299 loss:0.0803\n",
      "Episode:249 meanR:8.5400 R:14.0 Steps:299 loss:0.0722\n",
      "Episode:250 meanR:8.6100 R:10.0 Steps:299 loss:0.0749\n",
      "Episode:251 meanR:8.6200 R:15.0 Steps:299 loss:0.0978\n",
      "Episode:252 meanR:8.5600 R:3.0 Steps:299 loss:0.0859\n",
      "Episode:253 meanR:8.5100 R:6.0 Steps:299 loss:0.0460\n",
      "Episode:254 meanR:8.4700 R:8.0 Steps:299 loss:0.0377\n",
      "Episode:255 meanR:8.5700 R:14.0 Steps:299 loss:0.0500\n",
      "Episode:256 meanR:8.5000 R:0.0 Steps:299 loss:0.0440\n",
      "Episode:257 meanR:8.4400 R:9.0 Steps:299 loss:0.0563\n",
      "Episode:258 meanR:8.3900 R:2.0 Steps:299 loss:0.0624\n",
      "Episode:259 meanR:8.4000 R:11.0 Steps:299 loss:0.0452\n",
      "Episode:260 meanR:8.4800 R:12.0 Steps:299 loss:0.0619\n",
      "Episode:261 meanR:8.5100 R:11.0 Steps:299 loss:0.0797\n",
      "Episode:262 meanR:8.5300 R:9.0 Steps:299 loss:0.0586\n",
      "Episode:263 meanR:8.4600 R:6.0 Steps:299 loss:0.0553\n",
      "Episode:264 meanR:8.3200 R:1.0 Steps:299 loss:0.0743\n",
      "Episode:265 meanR:8.2700 R:3.0 Steps:299 loss:0.0340\n",
      "Episode:266 meanR:8.3200 R:7.0 Steps:299 loss:0.0405\n",
      "Episode:267 meanR:8.3200 R:10.0 Steps:299 loss:0.0580\n",
      "Episode:268 meanR:8.2900 R:7.0 Steps:299 loss:0.0528\n",
      "Episode:269 meanR:8.2600 R:2.0 Steps:299 loss:0.0434\n",
      "Episode:270 meanR:8.2000 R:-1.0 Steps:299 loss:0.0149\n",
      "Episode:271 meanR:8.1100 R:5.0 Steps:299 loss:0.0336\n",
      "Episode:272 meanR:8.0600 R:4.0 Steps:299 loss:0.0560\n",
      "Episode:273 meanR:7.9900 R:4.0 Steps:299 loss:0.0248\n",
      "Episode:274 meanR:8.0600 R:13.0 Steps:299 loss:0.0482\n",
      "Episode:275 meanR:8.0000 R:8.0 Steps:299 loss:0.0708\n",
      "Episode:276 meanR:8.0100 R:11.0 Steps:299 loss:0.0740\n",
      "Episode:277 meanR:8.0600 R:11.0 Steps:299 loss:0.0470\n",
      "Episode:278 meanR:8.0900 R:14.0 Steps:299 loss:0.0559\n",
      "Episode:279 meanR:8.2000 R:17.0 Steps:299 loss:0.0983\n",
      "Episode:280 meanR:8.1400 R:10.0 Steps:299 loss:0.0652\n",
      "Episode:281 meanR:8.1500 R:7.0 Steps:299 loss:0.0665\n",
      "Episode:282 meanR:8.1800 R:11.0 Steps:299 loss:0.0325\n",
      "Episode:283 meanR:8.1600 R:9.0 Steps:299 loss:0.0472\n",
      "Episode:284 meanR:8.1800 R:10.0 Steps:299 loss:0.0470\n",
      "Episode:285 meanR:8.2100 R:13.0 Steps:299 loss:0.0450\n",
      "Episode:286 meanR:8.2000 R:5.0 Steps:299 loss:0.0585\n",
      "Episode:287 meanR:8.2000 R:7.0 Steps:299 loss:0.0497\n",
      "Episode:288 meanR:8.2200 R:6.0 Steps:299 loss:0.0757\n",
      "Episode:289 meanR:8.2700 R:5.0 Steps:299 loss:0.0486\n",
      "Episode:290 meanR:8.2600 R:8.0 Steps:299 loss:0.0427\n",
      "Episode:291 meanR:8.2100 R:6.0 Steps:299 loss:0.0387\n",
      "Episode:292 meanR:8.1600 R:3.0 Steps:299 loss:0.0463\n",
      "Episode:293 meanR:8.1800 R:11.0 Steps:299 loss:0.0606\n",
      "Episode:294 meanR:8.2600 R:14.0 Steps:299 loss:0.0489\n",
      "Episode:295 meanR:8.1900 R:9.0 Steps:299 loss:0.0705\n",
      "Episode:296 meanR:8.2600 R:13.0 Steps:299 loss:0.0544\n",
      "Episode:297 meanR:8.2400 R:4.0 Steps:299 loss:0.0794\n",
      "Episode:298 meanR:8.3000 R:13.0 Steps:299 loss:0.0562\n",
      "Episode:299 meanR:8.3200 R:16.0 Steps:299 loss:0.0729\n",
      "Episode:300 meanR:8.2900 R:3.0 Steps:299 loss:0.0885\n",
      "Episode:301 meanR:8.3300 R:14.0 Steps:299 loss:0.0544\n",
      "Episode:302 meanR:8.3800 R:10.0 Steps:299 loss:0.0607\n",
      "Episode:303 meanR:8.3700 R:7.0 Steps:299 loss:0.0621\n",
      "Episode:304 meanR:8.4100 R:14.0 Steps:299 loss:0.0560\n",
      "Episode:305 meanR:8.4100 R:9.0 Steps:299 loss:0.0578\n",
      "Episode:306 meanR:8.3600 R:4.0 Steps:299 loss:0.0646\n",
      "Episode:307 meanR:8.4200 R:11.0 Steps:299 loss:0.0372\n",
      "Episode:308 meanR:8.4700 R:13.0 Steps:299 loss:0.0376\n",
      "Episode:309 meanR:8.3600 R:1.0 Steps:299 loss:0.0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:310 meanR:8.4500 R:14.0 Steps:299 loss:0.0552\n",
      "Episode:311 meanR:8.5000 R:7.0 Steps:299 loss:0.0681\n",
      "Episode:312 meanR:8.4900 R:9.0 Steps:299 loss:0.0588\n",
      "Episode:313 meanR:8.5100 R:6.0 Steps:299 loss:0.0567\n",
      "Episode:314 meanR:8.5700 R:9.0 Steps:299 loss:0.0513\n",
      "Episode:315 meanR:8.5000 R:0.0 Steps:299 loss:0.0443\n",
      "Episode:316 meanR:8.4000 R:-2.0 Steps:299 loss:0.0352\n",
      "Episode:317 meanR:8.4600 R:18.0 Steps:299 loss:0.0487\n",
      "Episode:318 meanR:8.4700 R:11.0 Steps:299 loss:0.0930\n",
      "Episode:319 meanR:8.6100 R:16.0 Steps:299 loss:0.0689\n",
      "Episode:320 meanR:8.7000 R:9.0 Steps:299 loss:0.0805\n",
      "Episode:321 meanR:8.7200 R:8.0 Steps:299 loss:0.0558\n",
      "Episode:322 meanR:8.5800 R:4.0 Steps:299 loss:0.0445\n",
      "Episode:323 meanR:8.5300 R:3.0 Steps:299 loss:0.0461\n",
      "Episode:324 meanR:8.5900 R:16.0 Steps:299 loss:0.0537\n",
      "Episode:325 meanR:8.4900 R:4.0 Steps:299 loss:0.0577\n",
      "Episode:326 meanR:8.5100 R:10.0 Steps:299 loss:0.0674\n",
      "Episode:327 meanR:8.4300 R:9.0 Steps:299 loss:0.0497\n",
      "Episode:328 meanR:8.3400 R:4.0 Steps:299 loss:0.0428\n",
      "Episode:329 meanR:8.3400 R:7.0 Steps:299 loss:0.0592\n",
      "Episode:330 meanR:8.3600 R:16.0 Steps:299 loss:0.0621\n",
      "Episode:331 meanR:8.4500 R:14.0 Steps:299 loss:0.0616\n",
      "Episode:332 meanR:8.5800 R:17.0 Steps:299 loss:0.0974\n",
      "Episode:333 meanR:8.6100 R:11.0 Steps:299 loss:0.1275\n",
      "Episode:334 meanR:8.5000 R:0.0 Steps:299 loss:0.0785\n",
      "Episode:335 meanR:8.4600 R:2.0 Steps:299 loss:0.0317\n",
      "Episode:336 meanR:8.5100 R:10.0 Steps:299 loss:0.0706\n",
      "Episode:337 meanR:8.5200 R:11.0 Steps:299 loss:0.0827\n",
      "Episode:338 meanR:8.5100 R:13.0 Steps:299 loss:0.0628\n",
      "Episode:339 meanR:8.5800 R:11.0 Steps:299 loss:0.0605\n",
      "Episode:340 meanR:8.6400 R:14.0 Steps:299 loss:0.0889\n",
      "Episode:341 meanR:8.6500 R:10.0 Steps:299 loss:0.0563\n",
      "Episode:342 meanR:8.5300 R:3.0 Steps:299 loss:0.0544\n",
      "Episode:343 meanR:8.5800 R:13.0 Steps:299 loss:0.0614\n",
      "Episode:344 meanR:8.5200 R:2.0 Steps:299 loss:0.0660\n",
      "Episode:345 meanR:8.5100 R:4.0 Steps:299 loss:0.0436\n",
      "Episode:346 meanR:8.5700 R:10.0 Steps:299 loss:0.0373\n",
      "Episode:347 meanR:8.5400 R:9.0 Steps:299 loss:0.0701\n",
      "Episode:348 meanR:8.5300 R:9.0 Steps:299 loss:0.0721\n",
      "Episode:349 meanR:8.4900 R:10.0 Steps:299 loss:0.0554\n",
      "Episode:350 meanR:8.4600 R:7.0 Steps:299 loss:0.0480\n",
      "Episode:351 meanR:8.3900 R:8.0 Steps:299 loss:0.0354\n",
      "Episode:352 meanR:8.4100 R:5.0 Steps:299 loss:0.0463\n",
      "Episode:353 meanR:8.4200 R:7.0 Steps:299 loss:0.0408\n",
      "Episode:354 meanR:8.3600 R:2.0 Steps:299 loss:0.0649\n",
      "Episode:355 meanR:8.2500 R:3.0 Steps:299 loss:0.0328\n",
      "Episode:356 meanR:8.2600 R:1.0 Steps:299 loss:0.0196\n",
      "Episode:357 meanR:8.2000 R:3.0 Steps:299 loss:0.0282\n",
      "Episode:358 meanR:8.1900 R:1.0 Steps:299 loss:0.0257\n",
      "Episode:359 meanR:8.1600 R:8.0 Steps:299 loss:0.0448\n",
      "Episode:360 meanR:8.1200 R:8.0 Steps:299 loss:0.0514\n",
      "Episode:361 meanR:8.0600 R:5.0 Steps:299 loss:0.0525\n",
      "Episode:362 meanR:7.9900 R:2.0 Steps:299 loss:0.0330\n",
      "Episode:363 meanR:7.9400 R:1.0 Steps:299 loss:0.0204\n",
      "Episode:364 meanR:7.9800 R:5.0 Steps:299 loss:0.0388\n",
      "Episode:365 meanR:7.9700 R:2.0 Steps:299 loss:0.0345\n",
      "Episode:366 meanR:7.9100 R:1.0 Steps:299 loss:0.0392\n",
      "Episode:367 meanR:7.8800 R:7.0 Steps:299 loss:0.0624\n",
      "Episode:368 meanR:7.9600 R:15.0 Steps:299 loss:0.0613\n",
      "Episode:369 meanR:8.0700 R:13.0 Steps:299 loss:0.0796\n",
      "Episode:370 meanR:8.1200 R:4.0 Steps:299 loss:0.0565\n",
      "Episode:371 meanR:8.2600 R:19.0 Steps:299 loss:0.0749\n",
      "Episode:372 meanR:8.3400 R:12.0 Steps:299 loss:0.0759\n",
      "Episode:373 meanR:8.4100 R:11.0 Steps:299 loss:0.0543\n",
      "Episode:374 meanR:8.4000 R:12.0 Steps:299 loss:0.0739\n",
      "Episode:375 meanR:8.4200 R:10.0 Steps:299 loss:0.0747\n",
      "Episode:376 meanR:8.4600 R:15.0 Steps:299 loss:0.0800\n",
      "Episode:377 meanR:8.4100 R:6.0 Steps:299 loss:0.0597\n",
      "Episode:378 meanR:8.3900 R:12.0 Steps:299 loss:0.0527\n",
      "Episode:379 meanR:8.2400 R:2.0 Steps:299 loss:0.0538\n",
      "Episode:380 meanR:8.2500 R:11.0 Steps:299 loss:0.0519\n",
      "Episode:381 meanR:8.2800 R:10.0 Steps:299 loss:0.0515\n",
      "Episode:382 meanR:8.1700 R:0.0 Steps:299 loss:0.0499\n",
      "Episode:383 meanR:8.1900 R:11.0 Steps:299 loss:0.0466\n",
      "Episode:384 meanR:8.1900 R:10.0 Steps:299 loss:0.0650\n",
      "Episode:385 meanR:8.1000 R:4.0 Steps:299 loss:0.0544\n",
      "Episode:386 meanR:8.1100 R:6.0 Steps:299 loss:0.0455\n",
      "Episode:387 meanR:8.0900 R:5.0 Steps:299 loss:0.0803\n",
      "Episode:388 meanR:8.0300 R:0.0 Steps:299 loss:0.0272\n",
      "Episode:389 meanR:8.0600 R:8.0 Steps:299 loss:0.0518\n",
      "Episode:390 meanR:8.0300 R:5.0 Steps:299 loss:0.0506\n",
      "Episode:391 meanR:8.1000 R:13.0 Steps:299 loss:0.0413\n",
      "Episode:392 meanR:8.1700 R:10.0 Steps:299 loss:0.0613\n",
      "Episode:393 meanR:8.1700 R:11.0 Steps:299 loss:0.0692\n",
      "Episode:394 meanR:8.0800 R:5.0 Steps:299 loss:0.0553\n",
      "Episode:395 meanR:8.0600 R:7.0 Steps:299 loss:0.0364\n",
      "Episode:396 meanR:8.0600 R:13.0 Steps:299 loss:0.0555\n",
      "Episode:397 meanR:8.0900 R:7.0 Steps:299 loss:0.0676\n",
      "Episode:398 meanR:7.9600 R:0.0 Steps:299 loss:0.0251\n",
      "Episode:399 meanR:7.8500 R:5.0 Steps:299 loss:0.0291\n",
      "Episode:400 meanR:7.9100 R:9.0 Steps:299 loss:0.0377\n",
      "Episode:401 meanR:7.7700 R:0.0 Steps:299 loss:0.0451\n",
      "Episode:402 meanR:7.7100 R:4.0 Steps:299 loss:0.0312\n",
      "Episode:403 meanR:7.7100 R:7.0 Steps:299 loss:0.0423\n",
      "Episode:404 meanR:7.6100 R:4.0 Steps:299 loss:0.0457\n",
      "Episode:405 meanR:7.6100 R:9.0 Steps:299 loss:0.0471\n",
      "Episode:406 meanR:7.6900 R:12.0 Steps:299 loss:0.0656\n",
      "Episode:407 meanR:7.6300 R:5.0 Steps:299 loss:0.0667\n",
      "Episode:408 meanR:7.6300 R:13.0 Steps:299 loss:0.0598\n",
      "Episode:409 meanR:7.6900 R:7.0 Steps:299 loss:0.0672\n",
      "Episode:410 meanR:7.6200 R:7.0 Steps:299 loss:0.0692\n",
      "Episode:411 meanR:7.5900 R:4.0 Steps:299 loss:0.0481\n",
      "Episode:412 meanR:7.5600 R:6.0 Steps:299 loss:0.0428\n",
      "Episode:413 meanR:7.6300 R:13.0 Steps:299 loss:0.0878\n",
      "Episode:414 meanR:7.6200 R:8.0 Steps:299 loss:0.0841\n",
      "Episode:415 meanR:7.7500 R:13.0 Steps:299 loss:0.0670\n",
      "Episode:416 meanR:7.8500 R:8.0 Steps:299 loss:0.0789\n",
      "Episode:417 meanR:7.7800 R:11.0 Steps:299 loss:0.0489\n",
      "Episode:418 meanR:7.6700 R:0.0 Steps:299 loss:0.0670\n",
      "Episode:419 meanR:7.5800 R:7.0 Steps:299 loss:0.0394\n",
      "Episode:420 meanR:7.6000 R:11.0 Steps:299 loss:0.0594\n",
      "Episode:421 meanR:7.7000 R:18.0 Steps:299 loss:0.0757\n",
      "Episode:422 meanR:7.8200 R:16.0 Steps:299 loss:0.0923\n",
      "Episode:423 meanR:7.8900 R:10.0 Steps:299 loss:0.0783\n",
      "Episode:424 meanR:7.8900 R:16.0 Steps:299 loss:0.0797\n",
      "Episode:425 meanR:7.9700 R:12.0 Steps:299 loss:0.0648\n",
      "Episode:426 meanR:8.1000 R:23.0 Steps:299 loss:0.1148\n",
      "Episode:427 meanR:8.1800 R:17.0 Steps:299 loss:0.0828\n",
      "Episode:428 meanR:8.2300 R:9.0 Steps:299 loss:0.1303\n",
      "Episode:429 meanR:8.2400 R:8.0 Steps:299 loss:0.0804\n",
      "Episode:430 meanR:8.2000 R:12.0 Steps:299 loss:0.0595\n",
      "Episode:431 meanR:8.1300 R:7.0 Steps:299 loss:0.0537\n",
      "Episode:432 meanR:8.1300 R:17.0 Steps:299 loss:0.0621\n",
      "Episode:433 meanR:8.1500 R:13.0 Steps:299 loss:0.0923\n",
      "Episode:434 meanR:8.2800 R:13.0 Steps:299 loss:0.0844\n",
      "Episode:435 meanR:8.2900 R:3.0 Steps:299 loss:0.0793\n",
      "Episode:436 meanR:8.2700 R:8.0 Steps:299 loss:0.0429\n",
      "Episode:437 meanR:8.2500 R:9.0 Steps:299 loss:0.0553\n",
      "Episode:438 meanR:8.1600 R:4.0 Steps:299 loss:0.0876\n",
      "Episode:439 meanR:8.0900 R:4.0 Steps:299 loss:0.0535\n",
      "Episode:440 meanR:7.9900 R:4.0 Steps:299 loss:0.0484\n",
      "Episode:441 meanR:7.9900 R:10.0 Steps:299 loss:0.0505\n",
      "Episode:442 meanR:8.0900 R:13.0 Steps:299 loss:0.0761\n",
      "Episode:443 meanR:8.0400 R:8.0 Steps:299 loss:0.0608\n",
      "Episode:444 meanR:8.0800 R:6.0 Steps:299 loss:0.0402\n",
      "Episode:445 meanR:8.1400 R:10.0 Steps:299 loss:0.0714\n",
      "Episode:446 meanR:8.0600 R:2.0 Steps:299 loss:0.0697\n",
      "Episode:447 meanR:8.0700 R:10.0 Steps:299 loss:0.0540\n",
      "Episode:448 meanR:7.9800 R:0.0 Steps:299 loss:0.0426\n",
      "Episode:449 meanR:7.9400 R:6.0 Steps:299 loss:0.0506\n",
      "Episode:450 meanR:8.0000 R:13.0 Steps:299 loss:0.0613\n",
      "Episode:451 meanR:8.0000 R:8.0 Steps:299 loss:0.0646\n",
      "Episode:452 meanR:8.0400 R:9.0 Steps:299 loss:0.0628\n",
      "Episode:453 meanR:8.1100 R:14.0 Steps:299 loss:0.0743\n",
      "Episode:454 meanR:8.1400 R:5.0 Steps:299 loss:0.0812\n",
      "Episode:455 meanR:8.2200 R:11.0 Steps:299 loss:0.0511\n",
      "Episode:456 meanR:8.3300 R:12.0 Steps:299 loss:0.0540\n",
      "Episode:457 meanR:8.4100 R:11.0 Steps:299 loss:0.0654\n",
      "Episode:458 meanR:8.5000 R:10.0 Steps:299 loss:0.0647\n",
      "Episode:459 meanR:8.5200 R:10.0 Steps:299 loss:0.0726\n",
      "Episode:460 meanR:8.5200 R:8.0 Steps:299 loss:0.0647\n",
      "Episode:461 meanR:8.6100 R:14.0 Steps:299 loss:0.0532\n",
      "Episode:462 meanR:8.6400 R:5.0 Steps:299 loss:0.0379\n",
      "Episode:463 meanR:8.7400 R:11.0 Steps:299 loss:0.0708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:464 meanR:8.8400 R:15.0 Steps:299 loss:0.0570\n",
      "Episode:465 meanR:8.8800 R:6.0 Steps:299 loss:0.0549\n",
      "Episode:466 meanR:8.9000 R:3.0 Steps:299 loss:0.0423\n",
      "Episode:467 meanR:8.9500 R:12.0 Steps:299 loss:0.0583\n",
      "Episode:468 meanR:8.9600 R:16.0 Steps:299 loss:0.0544\n",
      "Episode:469 meanR:8.8700 R:4.0 Steps:299 loss:0.0520\n",
      "Episode:470 meanR:8.8400 R:1.0 Steps:299 loss:0.0533\n",
      "Episode:471 meanR:8.7200 R:7.0 Steps:299 loss:0.0506\n",
      "Episode:472 meanR:8.6600 R:6.0 Steps:299 loss:0.0404\n",
      "Episode:473 meanR:8.6200 R:7.0 Steps:299 loss:0.0449\n",
      "Episode:474 meanR:8.6600 R:16.0 Steps:299 loss:0.0750\n",
      "Episode:475 meanR:8.7300 R:17.0 Steps:299 loss:0.0799\n",
      "Episode:476 meanR:8.6400 R:6.0 Steps:299 loss:0.0718\n",
      "Episode:477 meanR:8.7400 R:16.0 Steps:299 loss:0.0698\n",
      "Episode:478 meanR:8.7000 R:8.0 Steps:299 loss:0.0871\n",
      "Episode:479 meanR:8.7900 R:11.0 Steps:299 loss:0.0740\n",
      "Episode:480 meanR:8.8100 R:13.0 Steps:299 loss:0.0741\n",
      "Episode:481 meanR:8.7900 R:8.0 Steps:299 loss:0.0794\n",
      "Episode:482 meanR:8.8600 R:7.0 Steps:299 loss:0.0688\n",
      "Episode:483 meanR:8.8200 R:7.0 Steps:299 loss:0.0514\n",
      "Episode:484 meanR:8.8100 R:9.0 Steps:299 loss:0.0701\n",
      "Episode:485 meanR:8.9300 R:16.0 Steps:299 loss:0.0801\n",
      "Episode:486 meanR:9.0000 R:13.0 Steps:299 loss:0.0871\n",
      "Episode:487 meanR:9.0000 R:5.0 Steps:299 loss:0.0637\n",
      "Episode:488 meanR:8.9900 R:-1.0 Steps:299 loss:0.0321\n",
      "Episode:489 meanR:8.9500 R:4.0 Steps:299 loss:0.0348\n",
      "Episode:490 meanR:8.9900 R:9.0 Steps:299 loss:0.0596\n",
      "Episode:491 meanR:9.0400 R:18.0 Steps:299 loss:0.0602\n",
      "Episode:492 meanR:9.0200 R:8.0 Steps:299 loss:0.0781\n",
      "Episode:493 meanR:9.0000 R:9.0 Steps:299 loss:0.0432\n",
      "Episode:494 meanR:9.0200 R:7.0 Steps:299 loss:0.0819\n",
      "Episode:495 meanR:9.0200 R:7.0 Steps:299 loss:0.0340\n",
      "Episode:496 meanR:8.9500 R:6.0 Steps:299 loss:0.0411\n",
      "Episode:497 meanR:9.0300 R:15.0 Steps:299 loss:0.0597\n",
      "Episode:498 meanR:9.2100 R:18.0 Steps:299 loss:0.0957\n",
      "Episode:499 meanR:9.2400 R:8.0 Steps:299 loss:0.0722\n",
      "Episode:500 meanR:9.3400 R:19.0 Steps:299 loss:0.0797\n",
      "Episode:501 meanR:9.4500 R:11.0 Steps:299 loss:0.1165\n",
      "Episode:502 meanR:9.5100 R:10.0 Steps:299 loss:0.0790\n",
      "Episode:503 meanR:9.5600 R:12.0 Steps:299 loss:0.1143\n",
      "Episode:504 meanR:9.6100 R:9.0 Steps:299 loss:0.0541\n",
      "Episode:505 meanR:9.6100 R:9.0 Steps:299 loss:0.0774\n",
      "Episode:506 meanR:9.5300 R:4.0 Steps:299 loss:0.0554\n",
      "Episode:507 meanR:9.5200 R:4.0 Steps:299 loss:0.0351\n",
      "Episode:508 meanR:9.3800 R:-1.0 Steps:299 loss:0.0364\n",
      "Episode:509 meanR:9.4200 R:11.0 Steps:299 loss:0.0341\n",
      "Episode:510 meanR:9.5000 R:15.0 Steps:299 loss:0.0725\n",
      "Episode:511 meanR:9.5900 R:13.0 Steps:299 loss:0.0906\n",
      "Episode:512 meanR:9.5400 R:1.0 Steps:299 loss:0.0761\n",
      "Episode:513 meanR:9.4300 R:2.0 Steps:299 loss:0.0384\n",
      "Episode:514 meanR:9.4300 R:8.0 Steps:299 loss:0.0438\n",
      "Episode:515 meanR:9.3600 R:6.0 Steps:299 loss:0.0604\n",
      "Episode:516 meanR:9.3800 R:10.0 Steps:299 loss:0.0530\n",
      "Episode:517 meanR:9.4300 R:16.0 Steps:299 loss:0.0709\n",
      "Episode:518 meanR:9.5100 R:8.0 Steps:299 loss:0.0779\n",
      "Episode:519 meanR:9.5200 R:8.0 Steps:299 loss:0.0614\n",
      "Episode:520 meanR:9.4900 R:8.0 Steps:299 loss:0.0888\n",
      "Episode:521 meanR:9.3900 R:8.0 Steps:299 loss:0.0657\n",
      "Episode:522 meanR:9.3800 R:15.0 Steps:299 loss:0.0604\n",
      "Episode:523 meanR:9.3700 R:9.0 Steps:299 loss:0.0821\n",
      "Episode:524 meanR:9.3100 R:10.0 Steps:299 loss:0.0781\n",
      "Episode:525 meanR:9.3100 R:12.0 Steps:299 loss:0.0663\n",
      "Episode:526 meanR:9.2500 R:17.0 Steps:299 loss:0.0680\n",
      "Episode:527 meanR:9.1900 R:11.0 Steps:299 loss:0.0585\n",
      "Episode:528 meanR:9.1700 R:7.0 Steps:299 loss:0.0770\n",
      "Episode:529 meanR:9.1800 R:9.0 Steps:299 loss:0.0598\n",
      "Episode:530 meanR:9.1600 R:10.0 Steps:299 loss:0.0579\n",
      "Episode:531 meanR:9.2000 R:11.0 Steps:299 loss:0.0832\n",
      "Episode:532 meanR:9.1800 R:15.0 Steps:299 loss:0.0934\n",
      "Episode:533 meanR:9.1500 R:10.0 Steps:299 loss:0.0609\n",
      "Episode:534 meanR:9.1700 R:15.0 Steps:299 loss:0.0560\n",
      "Episode:535 meanR:9.2700 R:13.0 Steps:299 loss:0.1186\n",
      "Episode:536 meanR:9.2100 R:2.0 Steps:299 loss:0.0845\n",
      "Episode:537 meanR:9.2300 R:11.0 Steps:299 loss:0.0730\n",
      "Episode:538 meanR:9.3000 R:11.0 Steps:299 loss:0.0498\n",
      "Episode:539 meanR:9.3500 R:9.0 Steps:299 loss:0.0587\n",
      "Episode:540 meanR:9.4300 R:12.0 Steps:299 loss:0.0599\n",
      "Episode:541 meanR:9.3900 R:6.0 Steps:299 loss:0.0622\n",
      "Episode:542 meanR:9.3900 R:13.0 Steps:299 loss:0.0666\n",
      "Episode:543 meanR:9.4300 R:12.0 Steps:299 loss:0.0801\n",
      "Episode:544 meanR:9.4600 R:9.0 Steps:299 loss:0.0840\n",
      "Episode:545 meanR:9.4700 R:11.0 Steps:299 loss:0.0844\n",
      "Episode:546 meanR:9.5400 R:9.0 Steps:299 loss:0.0726\n",
      "Episode:547 meanR:9.5200 R:8.0 Steps:299 loss:0.0412\n",
      "Episode:548 meanR:9.5800 R:6.0 Steps:299 loss:0.0503\n",
      "Episode:549 meanR:9.6100 R:9.0 Steps:299 loss:0.0570\n",
      "Episode:550 meanR:9.5600 R:8.0 Steps:299 loss:0.0508\n",
      "Episode:551 meanR:9.5700 R:9.0 Steps:299 loss:0.0411\n",
      "Episode:552 meanR:9.5900 R:11.0 Steps:299 loss:0.0532\n",
      "Episode:553 meanR:9.5000 R:5.0 Steps:299 loss:0.0555\n",
      "Episode:554 meanR:9.4700 R:2.0 Steps:299 loss:0.0267\n",
      "Episode:555 meanR:9.4900 R:13.0 Steps:299 loss:0.0461\n",
      "Episode:556 meanR:9.4400 R:7.0 Steps:299 loss:0.0339\n",
      "Episode:557 meanR:9.4300 R:10.0 Steps:299 loss:0.0491\n",
      "Episode:558 meanR:9.4800 R:15.0 Steps:299 loss:0.0526\n",
      "Episode:559 meanR:9.5300 R:15.0 Steps:299 loss:0.0854\n",
      "Episode:560 meanR:9.5700 R:12.0 Steps:299 loss:0.0711\n",
      "Episode:561 meanR:9.5400 R:11.0 Steps:299 loss:0.0870\n",
      "Episode:562 meanR:9.6000 R:11.0 Steps:299 loss:0.0522\n",
      "Episode:563 meanR:9.6800 R:19.0 Steps:299 loss:0.0784\n",
      "Episode:564 meanR:9.7000 R:17.0 Steps:299 loss:0.0681\n",
      "Episode:565 meanR:9.7800 R:14.0 Steps:299 loss:0.0795\n",
      "Episode:566 meanR:9.8000 R:5.0 Steps:299 loss:0.0899\n",
      "Episode:567 meanR:9.7600 R:8.0 Steps:299 loss:0.0819\n",
      "Episode:568 meanR:9.7100 R:11.0 Steps:299 loss:0.0714\n",
      "Episode:569 meanR:9.8200 R:15.0 Steps:299 loss:0.0693\n",
      "Episode:570 meanR:9.9400 R:13.0 Steps:299 loss:0.0750\n",
      "Episode:571 meanR:10.0000 R:13.0 Steps:299 loss:0.0534\n",
      "Episode:572 meanR:10.0400 R:10.0 Steps:299 loss:0.0521\n",
      "Episode:573 meanR:10.0800 R:11.0 Steps:299 loss:0.0509\n",
      "Episode:574 meanR:10.0600 R:14.0 Steps:299 loss:0.0672\n",
      "Episode:575 meanR:9.9400 R:5.0 Steps:299 loss:0.0723\n",
      "Episode:576 meanR:10.0400 R:16.0 Steps:299 loss:0.0623\n",
      "Episode:577 meanR:10.0000 R:12.0 Steps:299 loss:0.0496\n",
      "Episode:578 meanR:10.0400 R:12.0 Steps:299 loss:0.0611\n",
      "Episode:579 meanR:10.0100 R:8.0 Steps:299 loss:0.0512\n",
      "Episode:580 meanR:10.0300 R:15.0 Steps:299 loss:0.0503\n",
      "Episode:581 meanR:10.0600 R:11.0 Steps:299 loss:0.0957\n",
      "Episode:582 meanR:10.0800 R:9.0 Steps:299 loss:0.0638\n",
      "Episode:583 meanR:10.0900 R:8.0 Steps:299 loss:0.0567\n",
      "Episode:584 meanR:10.1100 R:11.0 Steps:299 loss:0.0885\n",
      "Episode:585 meanR:10.0400 R:9.0 Steps:299 loss:0.0486\n",
      "Episode:586 meanR:10.0300 R:12.0 Steps:299 loss:0.0637\n",
      "Episode:587 meanR:10.0800 R:10.0 Steps:299 loss:0.0805\n",
      "Episode:588 meanR:10.1800 R:9.0 Steps:299 loss:0.0833\n",
      "Episode:589 meanR:10.1900 R:5.0 Steps:299 loss:0.0552\n",
      "Episode:590 meanR:10.2000 R:10.0 Steps:299 loss:0.0506\n",
      "Episode:591 meanR:10.1100 R:9.0 Steps:299 loss:0.0699\n",
      "Episode:592 meanR:10.0600 R:3.0 Steps:299 loss:0.0463\n",
      "Episode:593 meanR:10.0600 R:9.0 Steps:299 loss:0.0464\n",
      "Episode:594 meanR:9.9900 R:0.0 Steps:299 loss:0.0219\n",
      "Episode:595 meanR:10.0200 R:10.0 Steps:299 loss:0.0425\n",
      "Episode:596 meanR:10.0200 R:6.0 Steps:299 loss:0.0482\n",
      "Episode:597 meanR:9.9500 R:8.0 Steps:299 loss:0.0543\n",
      "Episode:598 meanR:9.8900 R:12.0 Steps:299 loss:0.0398\n",
      "Episode:599 meanR:9.9000 R:9.0 Steps:299 loss:0.0728\n",
      "Episode:600 meanR:9.8200 R:11.0 Steps:299 loss:0.0465\n",
      "Episode:601 meanR:9.7200 R:1.0 Steps:299 loss:0.0511\n",
      "Episode:602 meanR:9.7300 R:11.0 Steps:299 loss:0.0467\n",
      "Episode:603 meanR:9.7200 R:11.0 Steps:299 loss:0.0533\n",
      "Episode:604 meanR:9.7700 R:14.0 Steps:299 loss:0.0679\n",
      "Episode:605 meanR:9.8100 R:13.0 Steps:299 loss:0.0580\n",
      "Episode:606 meanR:9.8100 R:4.0 Steps:299 loss:0.0479\n",
      "Episode:607 meanR:9.8500 R:8.0 Steps:299 loss:0.0364\n",
      "Episode:608 meanR:9.9900 R:13.0 Steps:299 loss:0.0634\n",
      "Episode:609 meanR:9.9400 R:6.0 Steps:299 loss:0.0629\n",
      "Episode:610 meanR:9.9000 R:11.0 Steps:299 loss:0.0492\n",
      "Episode:611 meanR:9.8700 R:10.0 Steps:299 loss:0.0473\n",
      "Episode:612 meanR:10.0000 R:14.0 Steps:299 loss:0.0558\n",
      "Episode:613 meanR:10.1000 R:12.0 Steps:299 loss:0.0677\n",
      "Episode:614 meanR:10.1100 R:9.0 Steps:299 loss:0.0835\n",
      "Episode:615 meanR:10.0800 R:3.0 Steps:299 loss:0.0664\n",
      "Episode:616 meanR:10.0400 R:6.0 Steps:299 loss:0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:617 meanR:9.9600 R:8.0 Steps:299 loss:0.0793\n",
      "Episode:618 meanR:10.0600 R:18.0 Steps:299 loss:0.0603\n",
      "Episode:619 meanR:10.0900 R:11.0 Steps:299 loss:0.0903\n",
      "Episode:620 meanR:10.1100 R:10.0 Steps:299 loss:0.0497\n",
      "Episode:621 meanR:10.0700 R:4.0 Steps:299 loss:0.0555\n",
      "Episode:622 meanR:10.0000 R:8.0 Steps:299 loss:0.0318\n",
      "Episode:623 meanR:10.0400 R:13.0 Steps:299 loss:0.0446\n",
      "Episode:624 meanR:10.0600 R:12.0 Steps:299 loss:0.0626\n",
      "Episode:625 meanR:10.0400 R:10.0 Steps:299 loss:0.0517\n",
      "Episode:626 meanR:9.9800 R:11.0 Steps:299 loss:0.0481\n",
      "Episode:627 meanR:9.9500 R:8.0 Steps:299 loss:0.0569\n",
      "Episode:628 meanR:10.0400 R:16.0 Steps:299 loss:0.0557\n",
      "Episode:629 meanR:10.0400 R:9.0 Steps:299 loss:0.0788\n",
      "Episode:630 meanR:10.0400 R:10.0 Steps:299 loss:0.0377\n",
      "Episode:631 meanR:9.9900 R:6.0 Steps:299 loss:0.0498\n",
      "Episode:632 meanR:9.8600 R:2.0 Steps:299 loss:0.0264\n",
      "Episode:633 meanR:9.8900 R:13.0 Steps:299 loss:0.0529\n",
      "Episode:634 meanR:9.8200 R:8.0 Steps:299 loss:0.0572\n",
      "Episode:635 meanR:9.8200 R:13.0 Steps:299 loss:0.0515\n",
      "Episode:636 meanR:9.8900 R:9.0 Steps:299 loss:0.0645\n",
      "Episode:637 meanR:9.8500 R:7.0 Steps:299 loss:0.0452\n",
      "Episode:638 meanR:9.9200 R:18.0 Steps:299 loss:0.0646\n",
      "Episode:639 meanR:10.0200 R:19.0 Steps:299 loss:0.0603\n",
      "Episode:640 meanR:10.0400 R:14.0 Steps:299 loss:0.0675\n",
      "Episode:641 meanR:10.0800 R:10.0 Steps:299 loss:0.0545\n",
      "Episode:642 meanR:10.0300 R:8.0 Steps:299 loss:0.0612\n",
      "Episode:643 meanR:10.0300 R:12.0 Steps:299 loss:0.0570\n",
      "Episode:644 meanR:10.0500 R:11.0 Steps:299 loss:0.0499\n",
      "Episode:645 meanR:10.0200 R:8.0 Steps:299 loss:0.0649\n",
      "Episode:646 meanR:10.0400 R:11.0 Steps:299 loss:0.0531\n",
      "Episode:647 meanR:10.0800 R:12.0 Steps:299 loss:0.0531\n",
      "Episode:648 meanR:10.1000 R:8.0 Steps:299 loss:0.0723\n",
      "Episode:649 meanR:10.1200 R:11.0 Steps:299 loss:0.0660\n",
      "Episode:650 meanR:10.1800 R:14.0 Steps:299 loss:0.0608\n",
      "Episode:651 meanR:10.2100 R:12.0 Steps:299 loss:0.0818\n",
      "Episode:652 meanR:10.2200 R:12.0 Steps:299 loss:0.0387\n",
      "Episode:653 meanR:10.3100 R:14.0 Steps:299 loss:0.0695\n",
      "Episode:654 meanR:10.3900 R:10.0 Steps:299 loss:0.0432\n",
      "Episode:655 meanR:10.3300 R:7.0 Steps:299 loss:0.0600\n",
      "Episode:656 meanR:10.3500 R:9.0 Steps:299 loss:0.0665\n",
      "Episode:657 meanR:10.3500 R:10.0 Steps:299 loss:0.0617\n",
      "Episode:658 meanR:10.3200 R:12.0 Steps:299 loss:0.0490\n",
      "Episode:659 meanR:10.2200 R:5.0 Steps:299 loss:0.0483\n",
      "Episode:660 meanR:10.1200 R:2.0 Steps:299 loss:0.0520\n",
      "Episode:661 meanR:10.0100 R:0.0 Steps:299 loss:0.0609\n",
      "Episode:662 meanR:9.9900 R:9.0 Steps:299 loss:0.0501\n",
      "Episode:663 meanR:9.8300 R:3.0 Steps:299 loss:0.0552\n",
      "Episode:664 meanR:9.7500 R:9.0 Steps:299 loss:0.0435\n",
      "Episode:665 meanR:9.7300 R:12.0 Steps:299 loss:0.0554\n",
      "Episode:666 meanR:9.9300 R:25.0 Steps:299 loss:0.0751\n",
      "Episode:667 meanR:9.9800 R:13.0 Steps:299 loss:0.1152\n",
      "Episode:668 meanR:9.9500 R:8.0 Steps:299 loss:0.0723\n",
      "Episode:669 meanR:9.9100 R:11.0 Steps:299 loss:0.0743\n",
      "Episode:670 meanR:9.8800 R:10.0 Steps:299 loss:0.0600\n",
      "Episode:671 meanR:9.9200 R:17.0 Steps:299 loss:0.0622\n",
      "Episode:672 meanR:9.9600 R:14.0 Steps:299 loss:0.1072\n",
      "Episode:673 meanR:9.9900 R:14.0 Steps:299 loss:0.0766\n",
      "Episode:674 meanR:9.9700 R:12.0 Steps:299 loss:0.1007\n",
      "Episode:675 meanR:10.0800 R:16.0 Steps:299 loss:0.0729\n",
      "Episode:676 meanR:10.0200 R:10.0 Steps:299 loss:0.0502\n",
      "Episode:677 meanR:10.0000 R:10.0 Steps:299 loss:0.0574\n",
      "Episode:678 meanR:10.0100 R:13.0 Steps:299 loss:0.0652\n",
      "Episode:679 meanR:10.0800 R:15.0 Steps:299 loss:0.0464\n",
      "Episode:680 meanR:10.0100 R:8.0 Steps:299 loss:0.0689\n",
      "Episode:681 meanR:9.9800 R:8.0 Steps:299 loss:0.0720\n",
      "Episode:682 meanR:10.0400 R:15.0 Steps:299 loss:0.0351\n",
      "Episode:683 meanR:10.1200 R:16.0 Steps:299 loss:0.0769\n",
      "Episode:684 meanR:10.1400 R:13.0 Steps:299 loss:0.0783\n",
      "Episode:685 meanR:10.1100 R:6.0 Steps:299 loss:0.0819\n",
      "Episode:686 meanR:10.0200 R:3.0 Steps:299 loss:0.0589\n",
      "Episode:687 meanR:9.9400 R:2.0 Steps:299 loss:0.0593\n",
      "Episode:688 meanR:9.8500 R:0.0 Steps:299 loss:0.0183\n",
      "Episode:689 meanR:9.9400 R:14.0 Steps:299 loss:0.0602\n",
      "Episode:690 meanR:10.0000 R:16.0 Steps:299 loss:0.0775\n",
      "Episode:691 meanR:10.0100 R:10.0 Steps:299 loss:0.0775\n",
      "Episode:692 meanR:10.0500 R:7.0 Steps:299 loss:0.0520\n",
      "Episode:693 meanR:10.0300 R:7.0 Steps:299 loss:0.0585\n",
      "Episode:694 meanR:10.1400 R:11.0 Steps:299 loss:0.0699\n",
      "Episode:695 meanR:10.1600 R:12.0 Steps:299 loss:0.0873\n",
      "Episode:696 meanR:10.1900 R:9.0 Steps:299 loss:0.0780\n",
      "Episode:697 meanR:10.2200 R:11.0 Steps:299 loss:0.0762\n",
      "Episode:698 meanR:10.1300 R:3.0 Steps:299 loss:0.0690\n",
      "Episode:699 meanR:10.1100 R:7.0 Steps:299 loss:0.0470\n",
      "Episode:700 meanR:10.0500 R:5.0 Steps:299 loss:0.0465\n",
      "Episode:701 meanR:10.0800 R:4.0 Steps:299 loss:0.0582\n",
      "Episode:702 meanR:10.0100 R:4.0 Steps:299 loss:0.0554\n",
      "Episode:703 meanR:9.9100 R:1.0 Steps:299 loss:0.0465\n",
      "Episode:704 meanR:9.8200 R:5.0 Steps:299 loss:0.0388\n",
      "Episode:705 meanR:9.7600 R:7.0 Steps:299 loss:0.0320\n",
      "Episode:706 meanR:9.8400 R:12.0 Steps:299 loss:0.0581\n",
      "Episode:707 meanR:9.8300 R:7.0 Steps:299 loss:0.0543\n",
      "Episode:708 meanR:9.7300 R:3.0 Steps:299 loss:0.0300\n",
      "Episode:709 meanR:9.7600 R:9.0 Steps:299 loss:0.0406\n",
      "Episode:710 meanR:9.7300 R:8.0 Steps:299 loss:0.0469\n",
      "Episode:711 meanR:9.7400 R:11.0 Steps:299 loss:0.0615\n",
      "Episode:712 meanR:9.7000 R:10.0 Steps:299 loss:0.0583\n",
      "Episode:713 meanR:9.6700 R:9.0 Steps:299 loss:0.0374\n",
      "Episode:714 meanR:9.7100 R:13.0 Steps:299 loss:0.0468\n",
      "Episode:715 meanR:9.7600 R:8.0 Steps:299 loss:0.0695\n",
      "Episode:716 meanR:9.8000 R:10.0 Steps:299 loss:0.0496\n",
      "Episode:717 meanR:9.7500 R:3.0 Steps:299 loss:0.0703\n",
      "Episode:718 meanR:9.6700 R:10.0 Steps:299 loss:0.0423\n",
      "Episode:719 meanR:9.7100 R:15.0 Steps:299 loss:0.0505\n",
      "Episode:720 meanR:9.7300 R:12.0 Steps:299 loss:0.0664\n",
      "Episode:721 meanR:9.8400 R:15.0 Steps:299 loss:0.0501\n",
      "Episode:722 meanR:9.9200 R:16.0 Steps:299 loss:0.0519\n",
      "Episode:723 meanR:9.9200 R:13.0 Steps:299 loss:0.0626\n",
      "Episode:724 meanR:9.8700 R:7.0 Steps:299 loss:0.0663\n",
      "Episode:725 meanR:9.8800 R:11.0 Steps:299 loss:0.0716\n",
      "Episode:726 meanR:9.8700 R:10.0 Steps:299 loss:0.0621\n",
      "Episode:727 meanR:9.8900 R:10.0 Steps:299 loss:0.0668\n",
      "Episode:728 meanR:9.7400 R:1.0 Steps:299 loss:0.0650\n",
      "Episode:729 meanR:9.7300 R:8.0 Steps:299 loss:0.0532\n",
      "Episode:730 meanR:9.6800 R:5.0 Steps:299 loss:0.0812\n",
      "Episode:731 meanR:9.7000 R:8.0 Steps:299 loss:0.0543\n",
      "Episode:732 meanR:9.6800 R:0.0 Steps:299 loss:0.0320\n",
      "Episode:733 meanR:9.6300 R:8.0 Steps:299 loss:0.0368\n",
      "Episode:734 meanR:9.6700 R:12.0 Steps:299 loss:0.0628\n",
      "Episode:735 meanR:9.6700 R:13.0 Steps:299 loss:0.0795\n",
      "Episode:736 meanR:9.6700 R:9.0 Steps:299 loss:0.0398\n",
      "Episode:737 meanR:9.6900 R:9.0 Steps:299 loss:0.0871\n",
      "Episode:738 meanR:9.5800 R:7.0 Steps:299 loss:0.0808\n",
      "Episode:739 meanR:9.5000 R:11.0 Steps:299 loss:0.0543\n",
      "Episode:740 meanR:9.4800 R:12.0 Steps:299 loss:0.0468\n",
      "Episode:741 meanR:9.4300 R:5.0 Steps:299 loss:0.0747\n",
      "Episode:742 meanR:9.4000 R:5.0 Steps:299 loss:0.0377\n",
      "Episode:743 meanR:9.3600 R:8.0 Steps:299 loss:0.0414\n",
      "Episode:744 meanR:9.3100 R:6.0 Steps:299 loss:0.0289\n",
      "Episode:745 meanR:9.2900 R:6.0 Steps:299 loss:0.0527\n",
      "Episode:746 meanR:9.2600 R:8.0 Steps:299 loss:0.0363\n",
      "Episode:747 meanR:9.2000 R:6.0 Steps:299 loss:0.0427\n",
      "Episode:748 meanR:9.2000 R:8.0 Steps:299 loss:0.0388\n",
      "Episode:749 meanR:9.1800 R:9.0 Steps:299 loss:0.0400\n",
      "Episode:750 meanR:9.1100 R:7.0 Steps:299 loss:0.0527\n",
      "Episode:751 meanR:9.0400 R:5.0 Steps:299 loss:0.0491\n",
      "Episode:752 meanR:8.9700 R:5.0 Steps:299 loss:0.0327\n",
      "Episode:753 meanR:8.8700 R:4.0 Steps:299 loss:0.0441\n",
      "Episode:754 meanR:8.8100 R:4.0 Steps:299 loss:0.0410\n",
      "Episode:755 meanR:8.7800 R:4.0 Steps:299 loss:0.0333\n",
      "Episode:756 meanR:8.8400 R:15.0 Steps:299 loss:0.0528\n",
      "Episode:757 meanR:8.7400 R:0.0 Steps:299 loss:0.0674\n",
      "Episode:758 meanR:8.7200 R:10.0 Steps:299 loss:0.0445\n",
      "Episode:759 meanR:8.7500 R:8.0 Steps:299 loss:0.0607\n",
      "Episode:760 meanR:8.8700 R:14.0 Steps:299 loss:0.0848\n",
      "Episode:761 meanR:8.9500 R:8.0 Steps:299 loss:0.0878\n",
      "Episode:762 meanR:9.0300 R:17.0 Steps:299 loss:0.0819\n",
      "Episode:763 meanR:9.0700 R:7.0 Steps:299 loss:0.1055\n",
      "Episode:764 meanR:9.0400 R:6.0 Steps:299 loss:0.0436\n",
      "Episode:765 meanR:9.0300 R:11.0 Steps:299 loss:0.0837\n",
      "Episode:766 meanR:8.8200 R:4.0 Steps:299 loss:0.0523\n",
      "Episode:767 meanR:8.7100 R:2.0 Steps:299 loss:0.0315\n",
      "Episode:768 meanR:8.7600 R:13.0 Steps:299 loss:0.0526\n",
      "Episode:769 meanR:8.7900 R:14.0 Steps:299 loss:0.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:770 meanR:8.7700 R:8.0 Steps:299 loss:0.0645\n",
      "Episode:771 meanR:8.7300 R:13.0 Steps:299 loss:0.0671\n",
      "Episode:772 meanR:8.6800 R:9.0 Steps:299 loss:0.0806\n",
      "Episode:773 meanR:8.5700 R:3.0 Steps:299 loss:0.0367\n",
      "Episode:774 meanR:8.5500 R:10.0 Steps:299 loss:0.0496\n",
      "Episode:775 meanR:8.4500 R:6.0 Steps:299 loss:0.0632\n",
      "Episode:776 meanR:8.4800 R:13.0 Steps:299 loss:0.0878\n",
      "Episode:777 meanR:8.3900 R:1.0 Steps:299 loss:0.0714\n",
      "Episode:778 meanR:8.3000 R:4.0 Steps:299 loss:0.0409\n",
      "Episode:779 meanR:8.2300 R:8.0 Steps:299 loss:0.0480\n",
      "Episode:780 meanR:8.2700 R:12.0 Steps:299 loss:0.0717\n",
      "Episode:781 meanR:8.2800 R:9.0 Steps:299 loss:0.0800\n",
      "Episode:782 meanR:8.2200 R:9.0 Steps:299 loss:0.0544\n",
      "Episode:783 meanR:8.1600 R:10.0 Steps:299 loss:0.0590\n",
      "Episode:784 meanR:8.0700 R:4.0 Steps:299 loss:0.0562\n",
      "Episode:785 meanR:8.1000 R:9.0 Steps:299 loss:0.0533\n",
      "Episode:786 meanR:8.2200 R:15.0 Steps:299 loss:0.0861\n",
      "Episode:787 meanR:8.3000 R:10.0 Steps:299 loss:0.0536\n",
      "Episode:788 meanR:8.4300 R:13.0 Steps:299 loss:0.0833\n",
      "Episode:789 meanR:8.4100 R:12.0 Steps:299 loss:0.0838\n",
      "Episode:790 meanR:8.3100 R:6.0 Steps:299 loss:0.0644\n",
      "Episode:791 meanR:8.2700 R:6.0 Steps:299 loss:0.0552\n",
      "Episode:792 meanR:8.3200 R:12.0 Steps:299 loss:0.0530\n",
      "Episode:793 meanR:8.3800 R:13.0 Steps:299 loss:0.0541\n",
      "Episode:794 meanR:8.4200 R:15.0 Steps:299 loss:0.0749\n",
      "Episode:795 meanR:8.3900 R:9.0 Steps:299 loss:0.0973\n",
      "Episode:796 meanR:8.4200 R:12.0 Steps:299 loss:0.0667\n",
      "Episode:797 meanR:8.4100 R:10.0 Steps:299 loss:0.0760\n",
      "Episode:798 meanR:8.4900 R:11.0 Steps:299 loss:0.0585\n",
      "Episode:799 meanR:8.4900 R:7.0 Steps:299 loss:0.0618\n",
      "Episode:800 meanR:8.5100 R:7.0 Steps:299 loss:0.0505\n",
      "Episode:801 meanR:8.6100 R:14.0 Steps:299 loss:0.0356\n",
      "Episode:802 meanR:8.7200 R:15.0 Steps:299 loss:0.0567\n",
      "Episode:803 meanR:8.8100 R:10.0 Steps:299 loss:0.0694\n",
      "Episode:804 meanR:8.7800 R:2.0 Steps:299 loss:0.0609\n",
      "Episode:805 meanR:8.7500 R:4.0 Steps:299 loss:0.0344\n",
      "Episode:806 meanR:8.6700 R:4.0 Steps:299 loss:0.0294\n",
      "Episode:807 meanR:8.7000 R:10.0 Steps:299 loss:0.0470\n",
      "Episode:808 meanR:8.7500 R:8.0 Steps:299 loss:0.0934\n",
      "Episode:809 meanR:8.7200 R:6.0 Steps:299 loss:0.0577\n",
      "Episode:810 meanR:8.7200 R:8.0 Steps:299 loss:0.0518\n",
      "Episode:811 meanR:8.7000 R:9.0 Steps:299 loss:0.0371\n",
      "Episode:812 meanR:8.7800 R:18.0 Steps:299 loss:0.0692\n",
      "Episode:813 meanR:8.8400 R:15.0 Steps:299 loss:0.0817\n",
      "Episode:814 meanR:8.7800 R:7.0 Steps:299 loss:0.0637\n",
      "Episode:815 meanR:8.7900 R:9.0 Steps:299 loss:0.0701\n",
      "Episode:816 meanR:8.8000 R:11.0 Steps:299 loss:0.0700\n",
      "Episode:817 meanR:8.8400 R:7.0 Steps:299 loss:0.0810\n",
      "Episode:818 meanR:8.8200 R:8.0 Steps:299 loss:0.0541\n",
      "Episode:819 meanR:8.7300 R:6.0 Steps:299 loss:0.0564\n",
      "Episode:820 meanR:8.6600 R:5.0 Steps:299 loss:0.0516\n",
      "Episode:821 meanR:8.5800 R:7.0 Steps:299 loss:0.0415\n",
      "Episode:822 meanR:8.5600 R:14.0 Steps:299 loss:0.0574\n",
      "Episode:823 meanR:8.5500 R:12.0 Steps:299 loss:0.0609\n",
      "Episode:824 meanR:8.5100 R:3.0 Steps:299 loss:0.0616\n",
      "Episode:825 meanR:8.4800 R:8.0 Steps:299 loss:0.0565\n",
      "Episode:826 meanR:8.4600 R:8.0 Steps:299 loss:0.0652\n",
      "Episode:827 meanR:8.5200 R:16.0 Steps:299 loss:0.0614\n",
      "Episode:828 meanR:8.5800 R:7.0 Steps:299 loss:0.0733\n",
      "Episode:829 meanR:8.6300 R:13.0 Steps:299 loss:0.0625\n",
      "Episode:830 meanR:8.7100 R:13.0 Steps:299 loss:0.1013\n",
      "Episode:831 meanR:8.7700 R:14.0 Steps:299 loss:0.1085\n",
      "Episode:832 meanR:8.7800 R:1.0 Steps:299 loss:0.0760\n",
      "Episode:833 meanR:8.8000 R:10.0 Steps:299 loss:0.0640\n",
      "Episode:834 meanR:8.8000 R:12.0 Steps:299 loss:0.0611\n",
      "Episode:835 meanR:8.8000 R:13.0 Steps:299 loss:0.0557\n",
      "Episode:836 meanR:8.7900 R:8.0 Steps:299 loss:0.0666\n",
      "Episode:837 meanR:8.8600 R:16.0 Steps:299 loss:0.0407\n",
      "Episode:838 meanR:8.8700 R:8.0 Steps:299 loss:0.0877\n",
      "Episode:839 meanR:8.7900 R:3.0 Steps:299 loss:0.0667\n",
      "Episode:840 meanR:8.8300 R:16.0 Steps:299 loss:0.0832\n",
      "Episode:841 meanR:8.8500 R:7.0 Steps:299 loss:0.0533\n",
      "Episode:842 meanR:8.8100 R:1.0 Steps:299 loss:0.0756\n",
      "Episode:843 meanR:8.8000 R:7.0 Steps:299 loss:0.0274\n",
      "Episode:844 meanR:8.7800 R:4.0 Steps:299 loss:0.0447\n",
      "Episode:845 meanR:8.8900 R:17.0 Steps:299 loss:0.0576\n",
      "Episode:846 meanR:8.9700 R:16.0 Steps:299 loss:0.0722\n",
      "Episode:847 meanR:9.1000 R:19.0 Steps:299 loss:0.0858\n",
      "Episode:848 meanR:9.1400 R:12.0 Steps:299 loss:0.0723\n",
      "Episode:849 meanR:9.1600 R:11.0 Steps:299 loss:0.0949\n",
      "Episode:850 meanR:9.2000 R:11.0 Steps:299 loss:0.0785\n",
      "Episode:851 meanR:9.2600 R:11.0 Steps:299 loss:0.0687\n",
      "Episode:852 meanR:9.3700 R:16.0 Steps:299 loss:0.0853\n",
      "Episode:853 meanR:9.5200 R:19.0 Steps:299 loss:0.0749\n",
      "Episode:854 meanR:9.6000 R:12.0 Steps:299 loss:0.0975\n",
      "Episode:855 meanR:9.6700 R:11.0 Steps:299 loss:0.0564\n",
      "Episode:856 meanR:9.6100 R:9.0 Steps:299 loss:0.0876\n",
      "Episode:857 meanR:9.6900 R:8.0 Steps:299 loss:0.0694\n",
      "Episode:858 meanR:9.7500 R:16.0 Steps:299 loss:0.0910\n",
      "Episode:859 meanR:9.7400 R:7.0 Steps:299 loss:0.0558\n",
      "Episode:860 meanR:9.7200 R:12.0 Steps:299 loss:0.0703\n",
      "Episode:861 meanR:9.7700 R:13.0 Steps:299 loss:0.0770\n",
      "Episode:862 meanR:9.7600 R:16.0 Steps:299 loss:0.0856\n",
      "Episode:863 meanR:9.8000 R:11.0 Steps:299 loss:0.0867\n",
      "Episode:864 meanR:9.8600 R:12.0 Steps:299 loss:0.0711\n",
      "Episode:865 meanR:9.8400 R:9.0 Steps:299 loss:0.0870\n",
      "Episode:866 meanR:9.9500 R:15.0 Steps:299 loss:0.0693\n",
      "Episode:867 meanR:10.0500 R:12.0 Steps:299 loss:0.0672\n",
      "Episode:868 meanR:9.9800 R:6.0 Steps:299 loss:0.0823\n",
      "Episode:869 meanR:9.9000 R:6.0 Steps:299 loss:0.0754\n",
      "Episode:870 meanR:9.9200 R:10.0 Steps:299 loss:0.0755\n",
      "Episode:871 meanR:9.8800 R:9.0 Steps:299 loss:0.0737\n",
      "Episode:872 meanR:9.9800 R:19.0 Steps:299 loss:0.0765\n",
      "Episode:873 meanR:10.0500 R:10.0 Steps:299 loss:0.0848\n",
      "Episode:874 meanR:10.0400 R:9.0 Steps:299 loss:0.0989\n",
      "Episode:875 meanR:10.0500 R:7.0 Steps:299 loss:0.0448\n",
      "Episode:876 meanR:10.0100 R:9.0 Steps:299 loss:0.0518\n",
      "Episode:877 meanR:10.0600 R:6.0 Steps:299 loss:0.0631\n",
      "Episode:878 meanR:10.1100 R:9.0 Steps:299 loss:0.0384\n",
      "Episode:879 meanR:10.0900 R:6.0 Steps:299 loss:0.0355\n",
      "Episode:880 meanR:10.0700 R:10.0 Steps:299 loss:0.0596\n",
      "Episode:881 meanR:10.0500 R:7.0 Steps:299 loss:0.0750\n",
      "Episode:882 meanR:9.9600 R:0.0 Steps:299 loss:0.0520\n",
      "Episode:883 meanR:9.9700 R:11.0 Steps:299 loss:0.0377\n",
      "Episode:884 meanR:10.0600 R:13.0 Steps:299 loss:0.0455\n",
      "Episode:885 meanR:10.0800 R:11.0 Steps:299 loss:0.0497\n",
      "Episode:886 meanR:10.0100 R:8.0 Steps:299 loss:0.0466\n",
      "Episode:887 meanR:10.0300 R:12.0 Steps:299 loss:0.0629\n",
      "Episode:888 meanR:10.0100 R:11.0 Steps:299 loss:0.0565\n",
      "Episode:889 meanR:9.9700 R:8.0 Steps:299 loss:0.0644\n",
      "Episode:890 meanR:10.0000 R:9.0 Steps:299 loss:0.0534\n",
      "Episode:891 meanR:10.0600 R:12.0 Steps:299 loss:0.0628\n",
      "Episode:892 meanR:10.0600 R:12.0 Steps:299 loss:0.0828\n",
      "Episode:893 meanR:10.0500 R:12.0 Steps:299 loss:0.0313\n",
      "Episode:894 meanR:10.0100 R:11.0 Steps:299 loss:0.0551\n",
      "Episode:895 meanR:9.9800 R:6.0 Steps:299 loss:0.0571\n",
      "Episode:896 meanR:9.9500 R:9.0 Steps:299 loss:0.0428\n",
      "Episode:897 meanR:9.9600 R:11.0 Steps:299 loss:0.0499\n",
      "Episode:898 meanR:9.9100 R:6.0 Steps:299 loss:0.0444\n",
      "Episode:899 meanR:9.8600 R:2.0 Steps:299 loss:0.0531\n",
      "Episode:900 meanR:9.8200 R:3.0 Steps:299 loss:0.0312\n",
      "Episode:901 meanR:9.7800 R:10.0 Steps:299 loss:0.0283\n",
      "Episode:902 meanR:9.6800 R:5.0 Steps:299 loss:0.0480\n",
      "Episode:903 meanR:9.7100 R:13.0 Steps:299 loss:0.0686\n",
      "Episode:904 meanR:9.7100 R:2.0 Steps:299 loss:0.0508\n",
      "Episode:905 meanR:9.7900 R:12.0 Steps:299 loss:0.0311\n",
      "Episode:906 meanR:9.8300 R:8.0 Steps:299 loss:0.0412\n",
      "Episode:907 meanR:9.8400 R:11.0 Steps:299 loss:0.0352\n",
      "Episode:908 meanR:9.8400 R:8.0 Steps:299 loss:0.0637\n",
      "Episode:909 meanR:9.8400 R:6.0 Steps:299 loss:0.0620\n",
      "Episode:910 meanR:9.8500 R:9.0 Steps:299 loss:0.0408\n",
      "Episode:911 meanR:9.8800 R:12.0 Steps:299 loss:0.0610\n",
      "Episode:912 meanR:9.7300 R:3.0 Steps:299 loss:0.0433\n",
      "Episode:913 meanR:9.5800 R:0.0 Steps:299 loss:0.0238\n",
      "Episode:914 meanR:9.5300 R:2.0 Steps:299 loss:0.0197\n",
      "Episode:915 meanR:9.5100 R:7.0 Steps:299 loss:0.0524\n",
      "Episode:916 meanR:9.4800 R:8.0 Steps:299 loss:0.0436\n",
      "Episode:917 meanR:9.4500 R:4.0 Steps:299 loss:0.0623\n",
      "Episode:918 meanR:9.4500 R:8.0 Steps:299 loss:0.0526\n",
      "Episode:919 meanR:9.4400 R:5.0 Steps:299 loss:0.0573\n",
      "Episode:920 meanR:9.3800 R:-1.0 Steps:299 loss:0.0372\n",
      "Episode:921 meanR:9.3100 R:0.0 Steps:299 loss:0.0331\n",
      "Episode:922 meanR:9.2800 R:11.0 Steps:299 loss:0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:923 meanR:9.2300 R:7.0 Steps:299 loss:0.0690\n",
      "Episode:924 meanR:9.2100 R:1.0 Steps:299 loss:0.0640\n",
      "Episode:925 meanR:9.2300 R:10.0 Steps:299 loss:0.0378\n",
      "Episode:926 meanR:9.2500 R:10.0 Steps:299 loss:0.0491\n",
      "Episode:927 meanR:9.1200 R:3.0 Steps:299 loss:0.0426\n",
      "Episode:928 meanR:9.1400 R:9.0 Steps:299 loss:0.0371\n",
      "Episode:929 meanR:9.2000 R:19.0 Steps:299 loss:0.0579\n",
      "Episode:930 meanR:9.1500 R:8.0 Steps:299 loss:0.0675\n",
      "Episode:931 meanR:9.0700 R:6.0 Steps:299 loss:0.0659\n",
      "Episode:932 meanR:9.1800 R:12.0 Steps:299 loss:0.0398\n",
      "Episode:933 meanR:9.0800 R:0.0 Steps:299 loss:0.0355\n",
      "Episode:934 meanR:9.0700 R:11.0 Steps:299 loss:0.0512\n",
      "Episode:935 meanR:9.0300 R:9.0 Steps:299 loss:0.0713\n",
      "Episode:936 meanR:9.0900 R:14.0 Steps:299 loss:0.0650\n",
      "Episode:937 meanR:9.0500 R:12.0 Steps:299 loss:0.0657\n",
      "Episode:938 meanR:9.0500 R:8.0 Steps:299 loss:0.0673\n",
      "Episode:939 meanR:9.0800 R:6.0 Steps:299 loss:0.0618\n",
      "Episode:940 meanR:9.0800 R:16.0 Steps:299 loss:0.0562\n",
      "Episode:941 meanR:9.0900 R:8.0 Steps:299 loss:0.0766\n",
      "Episode:942 meanR:9.1300 R:5.0 Steps:299 loss:0.0489\n",
      "Episode:943 meanR:9.1700 R:11.0 Steps:299 loss:0.0533\n",
      "Episode:944 meanR:9.3300 R:20.0 Steps:299 loss:0.0635\n",
      "Episode:945 meanR:9.2800 R:12.0 Steps:299 loss:0.0913\n",
      "Episode:946 meanR:9.2100 R:9.0 Steps:299 loss:0.0924\n",
      "Episode:947 meanR:9.1400 R:12.0 Steps:299 loss:0.0593\n",
      "Episode:948 meanR:9.0700 R:5.0 Steps:299 loss:0.0630\n",
      "Episode:949 meanR:9.0900 R:13.0 Steps:299 loss:0.0451\n",
      "Episode:950 meanR:9.1400 R:16.0 Steps:299 loss:0.0726\n",
      "Episode:951 meanR:9.1600 R:13.0 Steps:299 loss:0.0415\n",
      "Episode:952 meanR:9.1300 R:13.0 Steps:299 loss:0.0469\n",
      "Episode:953 meanR:9.0800 R:14.0 Steps:299 loss:0.0544\n",
      "Episode:954 meanR:9.0200 R:6.0 Steps:299 loss:0.0561\n",
      "Episode:955 meanR:9.0200 R:11.0 Steps:299 loss:0.0590\n",
      "Episode:956 meanR:8.9700 R:4.0 Steps:299 loss:0.0571\n",
      "Episode:957 meanR:8.9300 R:4.0 Steps:299 loss:0.0424\n",
      "Episode:958 meanR:8.9000 R:13.0 Steps:299 loss:0.0464\n",
      "Episode:959 meanR:8.9200 R:9.0 Steps:299 loss:0.0535\n",
      "Episode:960 meanR:8.9000 R:10.0 Steps:299 loss:0.0345\n",
      "Episode:961 meanR:8.9000 R:13.0 Steps:299 loss:0.0422\n",
      "Episode:962 meanR:8.8700 R:13.0 Steps:299 loss:0.0643\n",
      "Episode:963 meanR:8.8400 R:8.0 Steps:299 loss:0.0366\n",
      "Episode:964 meanR:8.8300 R:11.0 Steps:299 loss:0.0553\n",
      "Episode:965 meanR:8.8800 R:14.0 Steps:299 loss:0.0435\n",
      "Episode:966 meanR:8.8700 R:14.0 Steps:299 loss:0.0914\n",
      "Episode:967 meanR:8.8600 R:11.0 Steps:299 loss:0.0923\n",
      "Episode:968 meanR:8.9100 R:11.0 Steps:299 loss:0.0876\n",
      "Episode:969 meanR:8.9100 R:6.0 Steps:299 loss:0.0736\n",
      "Episode:970 meanR:8.9400 R:13.0 Steps:299 loss:0.0516\n",
      "Episode:971 meanR:8.9100 R:6.0 Steps:299 loss:0.0533\n",
      "Episode:972 meanR:8.7400 R:2.0 Steps:299 loss:0.0683\n",
      "Episode:973 meanR:8.6600 R:2.0 Steps:299 loss:0.0474\n",
      "Episode:974 meanR:8.5800 R:1.0 Steps:299 loss:0.0304\n",
      "Episode:975 meanR:8.5700 R:6.0 Steps:299 loss:0.0444\n",
      "Episode:976 meanR:8.5700 R:9.0 Steps:299 loss:0.0577\n",
      "Episode:977 meanR:8.6500 R:14.0 Steps:299 loss:0.0542\n",
      "Episode:978 meanR:8.6300 R:7.0 Steps:299 loss:0.0353\n",
      "Episode:979 meanR:8.6300 R:6.0 Steps:299 loss:0.0511\n",
      "Episode:980 meanR:8.5900 R:6.0 Steps:299 loss:0.0407\n",
      "Episode:981 meanR:8.7000 R:18.0 Steps:299 loss:0.0615\n",
      "Episode:982 meanR:8.7900 R:9.0 Steps:299 loss:0.0839\n",
      "Episode:983 meanR:8.8400 R:16.0 Steps:299 loss:0.0791\n",
      "Episode:984 meanR:8.8400 R:13.0 Steps:299 loss:0.0574\n",
      "Episode:985 meanR:8.8900 R:16.0 Steps:299 loss:0.0804\n",
      "Episode:986 meanR:8.8500 R:4.0 Steps:299 loss:0.0847\n",
      "Episode:987 meanR:8.8200 R:9.0 Steps:299 loss:0.0497\n",
      "Episode:988 meanR:8.8000 R:9.0 Steps:299 loss:0.0801\n",
      "Episode:989 meanR:8.7900 R:7.0 Steps:299 loss:0.0465\n",
      "Episode:990 meanR:8.7600 R:6.0 Steps:299 loss:0.0421\n",
      "Episode:991 meanR:8.8300 R:19.0 Steps:299 loss:0.0531\n",
      "Episode:992 meanR:8.8000 R:9.0 Steps:299 loss:0.0851\n",
      "Episode:993 meanR:8.8400 R:16.0 Steps:299 loss:0.0581\n",
      "Episode:994 meanR:8.7400 R:1.0 Steps:299 loss:0.0753\n",
      "Episode:995 meanR:8.8400 R:16.0 Steps:299 loss:0.0399\n",
      "Episode:996 meanR:8.8300 R:8.0 Steps:299 loss:0.0828\n",
      "Episode:997 meanR:8.8100 R:9.0 Steps:299 loss:0.0635\n",
      "Episode:998 meanR:8.8800 R:13.0 Steps:299 loss:0.0399\n",
      "Episode:999 meanR:8.9900 R:13.0 Steps:299 loss:0.0569\n",
      "Episode:1000 meanR:9.1200 R:16.0 Steps:299 loss:0.0767\n",
      "Episode:1001 meanR:9.1700 R:15.0 Steps:299 loss:0.0826\n",
      "Episode:1002 meanR:9.2500 R:13.0 Steps:299 loss:0.0671\n",
      "Episode:1003 meanR:9.2800 R:16.0 Steps:299 loss:0.0778\n",
      "Episode:1004 meanR:9.3500 R:9.0 Steps:299 loss:0.0658\n",
      "Episode:1005 meanR:9.2800 R:5.0 Steps:299 loss:0.0643\n",
      "Episode:1006 meanR:9.2700 R:7.0 Steps:299 loss:0.0408\n",
      "Episode:1007 meanR:9.1800 R:2.0 Steps:299 loss:0.0435\n",
      "Episode:1008 meanR:9.1800 R:8.0 Steps:299 loss:0.0281\n",
      "Episode:1009 meanR:9.1700 R:5.0 Steps:299 loss:0.0255\n",
      "Episode:1010 meanR:9.1600 R:8.0 Steps:299 loss:0.0567\n",
      "Episode:1011 meanR:9.1900 R:15.0 Steps:299 loss:0.0667\n",
      "Episode:1012 meanR:9.2800 R:12.0 Steps:299 loss:0.0631\n",
      "Episode:1013 meanR:9.2900 R:1.0 Steps:299 loss:0.0438\n",
      "Episode:1014 meanR:9.3000 R:3.0 Steps:299 loss:0.0337\n",
      "Episode:1015 meanR:9.2600 R:3.0 Steps:299 loss:0.0275\n",
      "Episode:1016 meanR:9.2200 R:4.0 Steps:299 loss:0.0327\n",
      "Episode:1017 meanR:9.3100 R:13.0 Steps:299 loss:0.0423\n",
      "Episode:1018 meanR:9.3500 R:12.0 Steps:299 loss:0.0516\n",
      "Episode:1019 meanR:9.3900 R:9.0 Steps:299 loss:0.0636\n",
      "Episode:1020 meanR:9.4900 R:9.0 Steps:299 loss:0.0475\n",
      "Episode:1021 meanR:9.5900 R:10.0 Steps:299 loss:0.0406\n",
      "Episode:1022 meanR:9.5800 R:10.0 Steps:299 loss:0.0576\n",
      "Episode:1023 meanR:9.5900 R:8.0 Steps:299 loss:0.0661\n",
      "Episode:1024 meanR:9.6200 R:4.0 Steps:299 loss:0.0352\n",
      "Episode:1025 meanR:9.6400 R:12.0 Steps:299 loss:0.0397\n",
      "Episode:1026 meanR:9.7000 R:16.0 Steps:299 loss:0.0432\n",
      "Episode:1027 meanR:9.7800 R:11.0 Steps:299 loss:0.0738\n",
      "Episode:1028 meanR:9.7300 R:4.0 Steps:299 loss:0.0672\n",
      "Episode:1029 meanR:9.7300 R:19.0 Steps:299 loss:0.0576\n",
      "Episode:1030 meanR:9.7700 R:12.0 Steps:299 loss:0.0767\n",
      "Episode:1031 meanR:9.7900 R:8.0 Steps:299 loss:0.0723\n",
      "Episode:1032 meanR:9.8000 R:13.0 Steps:299 loss:0.0355\n",
      "Episode:1033 meanR:9.9100 R:11.0 Steps:299 loss:0.0434\n",
      "Episode:1034 meanR:9.9400 R:14.0 Steps:299 loss:0.0887\n",
      "Episode:1035 meanR:9.9400 R:9.0 Steps:299 loss:0.0643\n",
      "Episode:1036 meanR:9.8800 R:8.0 Steps:299 loss:0.0727\n",
      "Episode:1037 meanR:9.8200 R:6.0 Steps:299 loss:0.0346\n",
      "Episode:1038 meanR:9.8400 R:10.0 Steps:299 loss:0.0435\n",
      "Episode:1039 meanR:9.9200 R:14.0 Steps:299 loss:0.0739\n",
      "Episode:1040 meanR:9.8900 R:13.0 Steps:299 loss:0.0790\n",
      "Episode:1041 meanR:9.9400 R:13.0 Steps:299 loss:0.0463\n",
      "Episode:1042 meanR:10.0100 R:12.0 Steps:299 loss:0.0729\n",
      "Episode:1043 meanR:10.0100 R:11.0 Steps:299 loss:0.0369\n",
      "Episode:1044 meanR:9.9300 R:12.0 Steps:299 loss:0.0584\n",
      "Episode:1045 meanR:9.8800 R:7.0 Steps:299 loss:0.0437\n",
      "Episode:1046 meanR:9.8600 R:7.0 Steps:299 loss:0.0731\n",
      "Episode:1047 meanR:9.8700 R:13.0 Steps:299 loss:0.0558\n",
      "Episode:1048 meanR:9.9600 R:14.0 Steps:299 loss:0.0580\n",
      "Episode:1049 meanR:10.0100 R:18.0 Steps:299 loss:0.0478\n",
      "Episode:1050 meanR:10.0100 R:16.0 Steps:299 loss:0.0919\n",
      "Episode:1051 meanR:10.0300 R:15.0 Steps:299 loss:0.0802\n",
      "Episode:1052 meanR:10.0100 R:11.0 Steps:299 loss:0.0628\n",
      "Episode:1053 meanR:9.9800 R:11.0 Steps:299 loss:0.0780\n",
      "Episode:1054 meanR:10.0400 R:12.0 Steps:299 loss:0.0391\n",
      "Episode:1055 meanR:10.1000 R:17.0 Steps:299 loss:0.0672\n",
      "Episode:1056 meanR:10.1600 R:10.0 Steps:299 loss:0.0802\n",
      "Episode:1057 meanR:10.2100 R:9.0 Steps:299 loss:0.0491\n",
      "Episode:1058 meanR:10.1900 R:11.0 Steps:299 loss:0.0513\n",
      "Episode:1059 meanR:10.2500 R:15.0 Steps:299 loss:0.0619\n",
      "Episode:1060 meanR:10.2600 R:11.0 Steps:299 loss:0.0487\n",
      "Episode:1061 meanR:10.2900 R:16.0 Steps:299 loss:0.0691\n",
      "Episode:1062 meanR:10.2700 R:11.0 Steps:299 loss:0.0679\n",
      "Episode:1063 meanR:10.3000 R:11.0 Steps:299 loss:0.0642\n",
      "Episode:1064 meanR:10.2500 R:6.0 Steps:299 loss:0.0654\n",
      "Episode:1065 meanR:10.1800 R:7.0 Steps:299 loss:0.0466\n",
      "Episode:1066 meanR:10.1000 R:6.0 Steps:299 loss:0.0338\n",
      "Episode:1067 meanR:10.0700 R:8.0 Steps:299 loss:0.0516\n",
      "Episode:1068 meanR:10.0800 R:12.0 Steps:299 loss:0.0591\n",
      "Episode:1069 meanR:10.1300 R:11.0 Steps:299 loss:0.0754\n",
      "Episode:1070 meanR:10.0800 R:8.0 Steps:299 loss:0.0589\n",
      "Episode:1071 meanR:10.0800 R:6.0 Steps:299 loss:0.0559\n",
      "Episode:1072 meanR:10.1700 R:11.0 Steps:299 loss:0.0517\n",
      "Episode:1073 meanR:10.1900 R:4.0 Steps:299 loss:0.0441\n",
      "Episode:1074 meanR:10.3600 R:18.0 Steps:299 loss:0.0521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1075 meanR:10.3000 R:0.0 Steps:299 loss:0.0391\n",
      "Episode:1076 meanR:10.3100 R:10.0 Steps:299 loss:0.0552\n",
      "Episode:1077 meanR:10.3200 R:15.0 Steps:299 loss:0.0805\n",
      "Episode:1078 meanR:10.3400 R:9.0 Steps:299 loss:0.0829\n",
      "Episode:1079 meanR:10.4100 R:13.0 Steps:299 loss:0.0536\n",
      "Episode:1080 meanR:10.5100 R:16.0 Steps:299 loss:0.0922\n",
      "Episode:1081 meanR:10.5000 R:17.0 Steps:299 loss:0.0718\n",
      "Episode:1082 meanR:10.5200 R:11.0 Steps:299 loss:0.0773\n",
      "Episode:1083 meanR:10.4200 R:6.0 Steps:299 loss:0.0578\n",
      "Episode:1084 meanR:10.3600 R:7.0 Steps:299 loss:0.0508\n",
      "Episode:1085 meanR:10.2800 R:8.0 Steps:299 loss:0.0403\n",
      "Episode:1086 meanR:10.3300 R:9.0 Steps:299 loss:0.0608\n",
      "Episode:1087 meanR:10.3600 R:12.0 Steps:299 loss:0.0618\n",
      "Episode:1088 meanR:10.3500 R:8.0 Steps:299 loss:0.0540\n",
      "Episode:1089 meanR:10.3900 R:11.0 Steps:299 loss:0.0535\n",
      "Episode:1090 meanR:10.3900 R:6.0 Steps:299 loss:0.0520\n",
      "Episode:1091 meanR:10.2900 R:9.0 Steps:299 loss:0.0500\n",
      "Episode:1092 meanR:10.2500 R:5.0 Steps:299 loss:0.0432\n",
      "Episode:1093 meanR:10.1800 R:9.0 Steps:299 loss:0.0480\n",
      "Episode:1094 meanR:10.3100 R:14.0 Steps:299 loss:0.0521\n",
      "Episode:1095 meanR:10.2300 R:8.0 Steps:299 loss:0.0838\n",
      "Episode:1096 meanR:10.2100 R:6.0 Steps:299 loss:0.0587\n",
      "Episode:1097 meanR:10.2200 R:10.0 Steps:299 loss:0.0534\n",
      "Episode:1098 meanR:10.1500 R:6.0 Steps:299 loss:0.0483\n",
      "Episode:1099 meanR:10.1700 R:15.0 Steps:299 loss:0.0582\n",
      "Episode:1100 meanR:10.0700 R:6.0 Steps:299 loss:0.0491\n",
      "Episode:1101 meanR:10.0300 R:11.0 Steps:299 loss:0.0621\n",
      "Episode:1102 meanR:10.0400 R:14.0 Steps:299 loss:0.0777\n",
      "Episode:1103 meanR:9.9500 R:7.0 Steps:299 loss:0.0605\n",
      "Episode:1104 meanR:9.9500 R:9.0 Steps:299 loss:0.0582\n",
      "Episode:1105 meanR:10.0100 R:11.0 Steps:299 loss:0.0321\n",
      "Episode:1106 meanR:10.0700 R:13.0 Steps:299 loss:0.0714\n",
      "Episode:1107 meanR:10.1500 R:10.0 Steps:299 loss:0.0825\n",
      "Episode:1108 meanR:10.1300 R:6.0 Steps:299 loss:0.0430\n",
      "Episode:1109 meanR:10.1200 R:4.0 Steps:299 loss:0.0289\n",
      "Episode:1110 meanR:10.1400 R:10.0 Steps:299 loss:0.0302\n",
      "Episode:1111 meanR:10.0900 R:10.0 Steps:299 loss:0.0332\n",
      "Episode:1112 meanR:10.0200 R:5.0 Steps:299 loss:0.0678\n",
      "Episode:1113 meanR:10.1400 R:13.0 Steps:299 loss:0.0430\n",
      "Episode:1114 meanR:10.1700 R:6.0 Steps:299 loss:0.0558\n",
      "Episode:1115 meanR:10.2300 R:9.0 Steps:299 loss:0.0286\n",
      "Episode:1116 meanR:10.3600 R:17.0 Steps:299 loss:0.0509\n",
      "Episode:1117 meanR:10.3200 R:9.0 Steps:299 loss:0.0717\n",
      "Episode:1118 meanR:10.3100 R:11.0 Steps:299 loss:0.0631\n",
      "Episode:1119 meanR:10.3900 R:17.0 Steps:299 loss:0.0688\n",
      "Episode:1120 meanR:10.3600 R:6.0 Steps:299 loss:0.0774\n",
      "Episode:1121 meanR:10.3300 R:7.0 Steps:299 loss:0.0478\n",
      "Episode:1122 meanR:10.3400 R:11.0 Steps:299 loss:0.0484\n",
      "Episode:1123 meanR:10.4100 R:15.0 Steps:299 loss:0.0417\n",
      "Episode:1124 meanR:10.4500 R:8.0 Steps:299 loss:0.0453\n",
      "Episode:1125 meanR:10.3400 R:1.0 Steps:299 loss:0.0452\n",
      "Episode:1126 meanR:10.3200 R:14.0 Steps:299 loss:0.0540\n",
      "Episode:1127 meanR:10.3400 R:13.0 Steps:299 loss:0.0534\n",
      "Episode:1128 meanR:10.4000 R:10.0 Steps:299 loss:0.0845\n",
      "Episode:1129 meanR:10.3100 R:10.0 Steps:299 loss:0.0493\n",
      "Episode:1130 meanR:10.1800 R:-1.0 Steps:299 loss:0.0320\n",
      "Episode:1131 meanR:10.1800 R:8.0 Steps:299 loss:0.0352\n",
      "Episode:1132 meanR:10.2000 R:15.0 Steps:299 loss:0.0479\n",
      "Episode:1133 meanR:10.1900 R:10.0 Steps:299 loss:0.0615\n",
      "Episode:1134 meanR:10.1200 R:7.0 Steps:299 loss:0.0454\n",
      "Episode:1135 meanR:10.2000 R:17.0 Steps:299 loss:0.0585\n",
      "Episode:1136 meanR:10.1800 R:6.0 Steps:299 loss:0.0702\n",
      "Episode:1137 meanR:10.1700 R:5.0 Steps:299 loss:0.0419\n",
      "Episode:1138 meanR:10.1900 R:12.0 Steps:299 loss:0.0550\n",
      "Episode:1139 meanR:10.1200 R:7.0 Steps:299 loss:0.0638\n",
      "Episode:1140 meanR:9.9900 R:0.0 Steps:299 loss:0.0590\n",
      "Episode:1141 meanR:9.9000 R:4.0 Steps:299 loss:0.0321\n",
      "Episode:1142 meanR:9.9000 R:12.0 Steps:299 loss:0.0568\n",
      "Episode:1143 meanR:9.9000 R:11.0 Steps:299 loss:0.0708\n",
      "Episode:1144 meanR:9.8400 R:6.0 Steps:299 loss:0.0765\n",
      "Episode:1145 meanR:9.8200 R:5.0 Steps:299 loss:0.0518\n",
      "Episode:1146 meanR:9.8300 R:8.0 Steps:299 loss:0.0758\n",
      "Episode:1147 meanR:9.8300 R:13.0 Steps:299 loss:0.0521\n",
      "Episode:1148 meanR:9.7900 R:10.0 Steps:299 loss:0.0733\n",
      "Episode:1149 meanR:9.6900 R:8.0 Steps:299 loss:0.0424\n",
      "Episode:1150 meanR:9.5800 R:5.0 Steps:299 loss:0.0700\n",
      "Episode:1151 meanR:9.5000 R:7.0 Steps:299 loss:0.0563\n",
      "Episode:1152 meanR:9.4600 R:7.0 Steps:299 loss:0.0414\n",
      "Episode:1153 meanR:9.4300 R:8.0 Steps:299 loss:0.0407\n",
      "Episode:1154 meanR:9.4600 R:15.0 Steps:299 loss:0.0752\n",
      "Episode:1155 meanR:9.3900 R:10.0 Steps:299 loss:0.0563\n",
      "Episode:1156 meanR:9.4100 R:12.0 Steps:299 loss:0.0743\n",
      "Episode:1157 meanR:9.3600 R:4.0 Steps:299 loss:0.0738\n",
      "Episode:1158 meanR:9.2700 R:2.0 Steps:299 loss:0.0243\n",
      "Episode:1159 meanR:9.2500 R:13.0 Steps:299 loss:0.0451\n",
      "Episode:1160 meanR:9.1800 R:4.0 Steps:299 loss:0.0464\n",
      "Episode:1161 meanR:9.0600 R:4.0 Steps:299 loss:0.0326\n",
      "Episode:1162 meanR:9.0000 R:5.0 Steps:299 loss:0.0339\n",
      "Episode:1163 meanR:8.9500 R:6.0 Steps:299 loss:0.0598\n",
      "Episode:1164 meanR:9.0100 R:12.0 Steps:299 loss:0.0448\n",
      "Episode:1165 meanR:9.0700 R:13.0 Steps:299 loss:0.0453\n",
      "Episode:1166 meanR:9.1000 R:9.0 Steps:299 loss:0.0464\n",
      "Episode:1167 meanR:9.0900 R:7.0 Steps:299 loss:0.0460\n",
      "Episode:1168 meanR:9.0500 R:8.0 Steps:299 loss:0.0520\n",
      "Episode:1169 meanR:9.0500 R:11.0 Steps:299 loss:0.0350\n",
      "Episode:1170 meanR:9.1500 R:18.0 Steps:299 loss:0.0654\n",
      "Episode:1171 meanR:9.2000 R:11.0 Steps:299 loss:0.0869\n",
      "Episode:1172 meanR:9.1100 R:2.0 Steps:299 loss:0.0738\n",
      "Episode:1173 meanR:9.2000 R:13.0 Steps:299 loss:0.0473\n",
      "Episode:1174 meanR:9.1800 R:16.0 Steps:299 loss:0.0989\n",
      "Episode:1175 meanR:9.3300 R:15.0 Steps:299 loss:0.0948\n",
      "Episode:1176 meanR:9.3200 R:9.0 Steps:299 loss:0.0679\n",
      "Episode:1177 meanR:9.2400 R:7.0 Steps:299 loss:0.0651\n",
      "Episode:1178 meanR:9.2700 R:12.0 Steps:299 loss:0.0417\n",
      "Episode:1179 meanR:9.2400 R:10.0 Steps:299 loss:0.0512\n",
      "Episode:1180 meanR:9.2000 R:12.0 Steps:299 loss:0.0443\n",
      "Episode:1181 meanR:9.1300 R:10.0 Steps:299 loss:0.0405\n",
      "Episode:1182 meanR:9.1900 R:17.0 Steps:299 loss:0.0544\n",
      "Episode:1183 meanR:9.2500 R:12.0 Steps:299 loss:0.0517\n",
      "Episode:1184 meanR:9.2800 R:10.0 Steps:299 loss:0.0802\n",
      "Episode:1185 meanR:9.2800 R:8.0 Steps:299 loss:0.0427\n",
      "Episode:1186 meanR:9.2600 R:7.0 Steps:299 loss:0.0416\n",
      "Episode:1187 meanR:9.2900 R:15.0 Steps:299 loss:0.0643\n",
      "Episode:1188 meanR:9.2600 R:5.0 Steps:299 loss:0.0692\n",
      "Episode:1189 meanR:9.1800 R:3.0 Steps:299 loss:0.0601\n",
      "Episode:1190 meanR:9.1800 R:6.0 Steps:299 loss:0.0368\n",
      "Episode:1191 meanR:9.3100 R:22.0 Steps:299 loss:0.0497\n",
      "Episode:1192 meanR:9.3800 R:12.0 Steps:299 loss:0.0761\n",
      "Episode:1193 meanR:9.4200 R:13.0 Steps:299 loss:0.0609\n",
      "Episode:1194 meanR:9.3900 R:11.0 Steps:299 loss:0.0433\n",
      "Episode:1195 meanR:9.4000 R:9.0 Steps:299 loss:0.0425\n",
      "Episode:1196 meanR:9.4400 R:10.0 Steps:299 loss:0.0517\n",
      "Episode:1197 meanR:9.4300 R:9.0 Steps:299 loss:0.0428\n",
      "Episode:1198 meanR:9.4700 R:10.0 Steps:299 loss:0.0594\n",
      "Episode:1199 meanR:9.4400 R:12.0 Steps:299 loss:0.0568\n",
      "Episode:1200 meanR:9.4600 R:8.0 Steps:299 loss:0.0608\n",
      "Episode:1201 meanR:9.4400 R:9.0 Steps:299 loss:0.0327\n",
      "Episode:1202 meanR:9.4700 R:17.0 Steps:299 loss:0.0554\n",
      "Episode:1203 meanR:9.5600 R:16.0 Steps:299 loss:0.0636\n",
      "Episode:1204 meanR:9.5900 R:12.0 Steps:299 loss:0.0672\n",
      "Episode:1205 meanR:9.5600 R:8.0 Steps:299 loss:0.0557\n",
      "Episode:1206 meanR:9.5800 R:15.0 Steps:299 loss:0.0469\n",
      "Episode:1207 meanR:9.6300 R:15.0 Steps:299 loss:0.0629\n",
      "Episode:1208 meanR:9.5800 R:1.0 Steps:299 loss:0.0701\n",
      "Episode:1209 meanR:9.6800 R:14.0 Steps:299 loss:0.0461\n",
      "Episode:1210 meanR:9.6300 R:5.0 Steps:299 loss:0.0458\n",
      "Episode:1211 meanR:9.6500 R:12.0 Steps:299 loss:0.0456\n",
      "Episode:1212 meanR:9.7600 R:16.0 Steps:299 loss:0.0733\n",
      "Episode:1213 meanR:9.7700 R:14.0 Steps:299 loss:0.0884\n",
      "Episode:1214 meanR:9.8100 R:10.0 Steps:299 loss:0.0739\n",
      "Episode:1215 meanR:9.8400 R:12.0 Steps:299 loss:0.0660\n",
      "Episode:1216 meanR:9.7200 R:5.0 Steps:299 loss:0.0410\n",
      "Episode:1217 meanR:9.7700 R:14.0 Steps:299 loss:0.0516\n",
      "Episode:1218 meanR:9.6900 R:3.0 Steps:299 loss:0.0492\n",
      "Episode:1219 meanR:9.6600 R:14.0 Steps:299 loss:0.0510\n",
      "Episode:1220 meanR:9.7200 R:12.0 Steps:299 loss:0.0604\n",
      "Episode:1221 meanR:9.7600 R:11.0 Steps:299 loss:0.0721\n",
      "Episode:1222 meanR:9.8000 R:15.0 Steps:299 loss:0.0583\n",
      "Episode:1223 meanR:9.7900 R:14.0 Steps:299 loss:0.0531\n",
      "Episode:1224 meanR:9.7800 R:7.0 Steps:299 loss:0.0866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1225 meanR:9.8800 R:11.0 Steps:299 loss:0.0526\n",
      "Episode:1226 meanR:9.7800 R:4.0 Steps:299 loss:0.0681\n",
      "Episode:1227 meanR:9.7600 R:11.0 Steps:299 loss:0.0575\n",
      "Episode:1228 meanR:9.7300 R:7.0 Steps:299 loss:0.0462\n",
      "Episode:1229 meanR:9.7100 R:8.0 Steps:299 loss:0.0489\n",
      "Episode:1230 meanR:9.8100 R:9.0 Steps:299 loss:0.0574\n",
      "Episode:1231 meanR:9.9100 R:18.0 Steps:299 loss:0.0708\n",
      "Episode:1232 meanR:9.8500 R:9.0 Steps:299 loss:0.0692\n",
      "Episode:1233 meanR:9.8400 R:9.0 Steps:299 loss:0.0635\n",
      "Episode:1234 meanR:9.8500 R:8.0 Steps:299 loss:0.0662\n",
      "Episode:1235 meanR:9.7800 R:10.0 Steps:299 loss:0.0490\n",
      "Episode:1236 meanR:9.8400 R:12.0 Steps:299 loss:0.0564\n",
      "Episode:1237 meanR:9.8200 R:3.0 Steps:299 loss:0.0782\n",
      "Episode:1238 meanR:9.8200 R:12.0 Steps:299 loss:0.0361\n",
      "Episode:1239 meanR:9.8600 R:11.0 Steps:299 loss:0.0576\n",
      "Episode:1240 meanR:10.0100 R:15.0 Steps:299 loss:0.0646\n",
      "Episode:1241 meanR:10.1200 R:15.0 Steps:299 loss:0.0578\n",
      "Episode:1242 meanR:10.1000 R:10.0 Steps:299 loss:0.0685\n",
      "Episode:1243 meanR:10.1100 R:12.0 Steps:299 loss:0.0613\n",
      "Episode:1244 meanR:10.1800 R:13.0 Steps:299 loss:0.0506\n",
      "Episode:1245 meanR:10.1900 R:6.0 Steps:299 loss:0.0435\n",
      "Episode:1246 meanR:10.2500 R:14.0 Steps:299 loss:0.0459\n",
      "Episode:1247 meanR:10.2100 R:9.0 Steps:299 loss:0.0641\n",
      "Episode:1248 meanR:10.2300 R:12.0 Steps:299 loss:0.0418\n",
      "Episode:1249 meanR:10.2500 R:10.0 Steps:299 loss:0.0641\n",
      "Episode:1250 meanR:10.3300 R:13.0 Steps:299 loss:0.0464\n",
      "Episode:1251 meanR:10.3400 R:8.0 Steps:299 loss:0.0517\n",
      "Episode:1252 meanR:10.3300 R:6.0 Steps:299 loss:0.0711\n",
      "Episode:1253 meanR:10.4100 R:16.0 Steps:299 loss:0.0715\n",
      "Episode:1254 meanR:10.4000 R:14.0 Steps:299 loss:0.0886\n",
      "Episode:1255 meanR:10.4500 R:15.0 Steps:299 loss:0.0628\n",
      "Episode:1256 meanR:10.3900 R:6.0 Steps:299 loss:0.0672\n",
      "Episode:1257 meanR:10.4500 R:10.0 Steps:299 loss:0.0610\n",
      "Episode:1258 meanR:10.5200 R:9.0 Steps:299 loss:0.0551\n",
      "Episode:1259 meanR:10.4600 R:7.0 Steps:299 loss:0.0523\n",
      "Episode:1260 meanR:10.5500 R:13.0 Steps:299 loss:0.0534\n",
      "Episode:1261 meanR:10.5600 R:5.0 Steps:299 loss:0.0560\n",
      "Episode:1262 meanR:10.6000 R:9.0 Steps:299 loss:0.0505\n",
      "Episode:1263 meanR:10.5700 R:3.0 Steps:299 loss:0.0375\n",
      "Episode:1264 meanR:10.5200 R:7.0 Steps:299 loss:0.0443\n",
      "Episode:1265 meanR:10.5300 R:14.0 Steps:299 loss:0.0259\n",
      "Episode:1266 meanR:10.6300 R:19.0 Steps:299 loss:0.0350\n",
      "Episode:1267 meanR:10.5900 R:3.0 Steps:299 loss:0.0483\n",
      "Episode:1268 meanR:10.6200 R:11.0 Steps:299 loss:0.0516\n",
      "Episode:1269 meanR:10.6500 R:14.0 Steps:299 loss:0.0761\n",
      "Episode:1270 meanR:10.6200 R:15.0 Steps:299 loss:0.0885\n",
      "Episode:1271 meanR:10.6500 R:14.0 Steps:299 loss:0.0469\n",
      "Episode:1272 meanR:10.7300 R:10.0 Steps:299 loss:0.0892\n",
      "Episode:1273 meanR:10.7200 R:12.0 Steps:299 loss:0.0698\n",
      "Episode:1274 meanR:10.6300 R:7.0 Steps:299 loss:0.0579\n",
      "Episode:1275 meanR:10.6000 R:12.0 Steps:299 loss:0.0415\n",
      "Episode:1276 meanR:10.6300 R:12.0 Steps:299 loss:0.0468\n",
      "Episode:1277 meanR:10.6700 R:11.0 Steps:299 loss:0.0763\n",
      "Episode:1278 meanR:10.6400 R:9.0 Steps:299 loss:0.0627\n",
      "Episode:1279 meanR:10.6300 R:9.0 Steps:299 loss:0.0518\n",
      "Episode:1280 meanR:10.6300 R:12.0 Steps:299 loss:0.0769\n",
      "Episode:1281 meanR:10.7000 R:17.0 Steps:299 loss:0.0984\n",
      "Episode:1282 meanR:10.6700 R:14.0 Steps:299 loss:0.0567\n",
      "Episode:1283 meanR:10.6900 R:14.0 Steps:299 loss:0.0527\n",
      "Episode:1284 meanR:10.6500 R:6.0 Steps:299 loss:0.0608\n",
      "Episode:1285 meanR:10.7200 R:15.0 Steps:299 loss:0.0692\n",
      "Episode:1286 meanR:10.7800 R:13.0 Steps:299 loss:0.0686\n",
      "Episode:1287 meanR:10.7800 R:15.0 Steps:299 loss:0.0831\n",
      "Episode:1288 meanR:10.8000 R:7.0 Steps:299 loss:0.0786\n",
      "Episode:1289 meanR:10.9500 R:18.0 Steps:299 loss:0.0732\n",
      "Episode:1290 meanR:10.9600 R:7.0 Steps:299 loss:0.1074\n",
      "Episode:1291 meanR:10.8200 R:8.0 Steps:299 loss:0.0687\n",
      "Episode:1292 meanR:10.8200 R:12.0 Steps:299 loss:0.0658\n",
      "Episode:1293 meanR:10.8100 R:12.0 Steps:299 loss:0.0915\n",
      "Episode:1294 meanR:10.7500 R:5.0 Steps:299 loss:0.0636\n",
      "Episode:1295 meanR:10.7600 R:10.0 Steps:299 loss:0.0852\n",
      "Episode:1296 meanR:10.7300 R:7.0 Steps:299 loss:0.0507\n",
      "Episode:1297 meanR:10.7600 R:12.0 Steps:299 loss:0.0561\n",
      "Episode:1298 meanR:10.7400 R:8.0 Steps:299 loss:0.0509\n",
      "Episode:1299 meanR:10.6700 R:5.0 Steps:299 loss:0.0444\n",
      "Episode:1300 meanR:10.7400 R:15.0 Steps:299 loss:0.0495\n",
      "Episode:1301 meanR:10.7400 R:9.0 Steps:299 loss:0.0575\n",
      "Episode:1302 meanR:10.6400 R:7.0 Steps:299 loss:0.0456\n",
      "Episode:1303 meanR:10.6000 R:12.0 Steps:299 loss:0.0597\n",
      "Episode:1304 meanR:10.6200 R:14.0 Steps:299 loss:0.0648\n",
      "Episode:1305 meanR:10.6800 R:14.0 Steps:299 loss:0.0537\n",
      "Episode:1306 meanR:10.5800 R:5.0 Steps:299 loss:0.0513\n",
      "Episode:1307 meanR:10.4500 R:2.0 Steps:299 loss:0.0418\n",
      "Episode:1308 meanR:10.4800 R:4.0 Steps:299 loss:0.0343\n",
      "Episode:1309 meanR:10.4300 R:9.0 Steps:299 loss:0.0428\n",
      "Episode:1310 meanR:10.5000 R:12.0 Steps:299 loss:0.0441\n",
      "Episode:1311 meanR:10.4300 R:5.0 Steps:299 loss:0.0346\n",
      "Episode:1312 meanR:10.4100 R:14.0 Steps:299 loss:0.0310\n",
      "Episode:1313 meanR:10.4000 R:13.0 Steps:299 loss:0.0553\n",
      "Episode:1314 meanR:10.3500 R:5.0 Steps:299 loss:0.0815\n",
      "Episode:1315 meanR:10.4200 R:19.0 Steps:299 loss:0.0961\n",
      "Episode:1316 meanR:10.3800 R:1.0 Steps:299 loss:0.1081\n",
      "Episode:1317 meanR:10.2600 R:2.0 Steps:299 loss:0.0690\n",
      "Episode:1318 meanR:10.3900 R:16.0 Steps:299 loss:0.0502\n",
      "Episode:1319 meanR:10.3300 R:8.0 Steps:299 loss:0.0584\n",
      "Episode:1320 meanR:10.3300 R:12.0 Steps:299 loss:0.0454\n",
      "Episode:1321 meanR:10.3500 R:13.0 Steps:299 loss:0.0465\n",
      "Episode:1322 meanR:10.3600 R:16.0 Steps:299 loss:0.0595\n",
      "Episode:1323 meanR:10.2800 R:6.0 Steps:299 loss:0.0703\n",
      "Episode:1324 meanR:10.3300 R:12.0 Steps:299 loss:0.0702\n",
      "Episode:1325 meanR:10.2600 R:4.0 Steps:299 loss:0.0532\n",
      "Episode:1326 meanR:10.3400 R:12.0 Steps:299 loss:0.0562\n",
      "Episode:1327 meanR:10.3400 R:11.0 Steps:299 loss:0.0583\n",
      "Episode:1328 meanR:10.4700 R:20.0 Steps:299 loss:0.0793\n",
      "Episode:1329 meanR:10.5400 R:15.0 Steps:299 loss:0.0778\n",
      "Episode:1330 meanR:10.6300 R:18.0 Steps:299 loss:0.0633\n",
      "Episode:1331 meanR:10.6100 R:16.0 Steps:299 loss:0.0888\n",
      "Episode:1332 meanR:10.6300 R:11.0 Steps:299 loss:0.0797\n",
      "Episode:1333 meanR:10.6800 R:14.0 Steps:299 loss:0.0597\n",
      "Episode:1334 meanR:10.6600 R:6.0 Steps:299 loss:0.0786\n",
      "Episode:1335 meanR:10.6800 R:12.0 Steps:299 loss:0.0395\n",
      "Episode:1336 meanR:10.6800 R:12.0 Steps:299 loss:0.0686\n",
      "Episode:1337 meanR:10.7600 R:11.0 Steps:299 loss:0.1045\n",
      "Episode:1338 meanR:10.7300 R:9.0 Steps:299 loss:0.0575\n",
      "Episode:1339 meanR:10.7600 R:14.0 Steps:299 loss:0.0658\n",
      "Episode:1340 meanR:10.7500 R:14.0 Steps:299 loss:0.0640\n",
      "Episode:1341 meanR:10.7400 R:14.0 Steps:299 loss:0.0636\n",
      "Episode:1342 meanR:10.7300 R:9.0 Steps:299 loss:0.0504\n",
      "Episode:1343 meanR:10.6600 R:5.0 Steps:299 loss:0.0607\n",
      "Episode:1344 meanR:10.6600 R:13.0 Steps:299 loss:0.0653\n",
      "Episode:1345 meanR:10.7500 R:15.0 Steps:299 loss:0.0549\n",
      "Episode:1346 meanR:10.7100 R:10.0 Steps:299 loss:0.0557\n",
      "Episode:1347 meanR:10.7400 R:12.0 Steps:299 loss:0.0561\n",
      "Episode:1348 meanR:10.7700 R:15.0 Steps:299 loss:0.0926\n",
      "Episode:1349 meanR:10.7800 R:11.0 Steps:299 loss:0.1094\n",
      "Episode:1350 meanR:10.8300 R:18.0 Steps:299 loss:0.0468\n",
      "Episode:1351 meanR:10.8400 R:9.0 Steps:299 loss:0.0761\n",
      "Episode:1352 meanR:10.9200 R:14.0 Steps:299 loss:0.0508\n",
      "Episode:1353 meanR:10.8400 R:8.0 Steps:299 loss:0.0580\n",
      "Episode:1354 meanR:10.7400 R:4.0 Steps:299 loss:0.0355\n",
      "Episode:1355 meanR:10.7000 R:11.0 Steps:299 loss:0.0415\n",
      "Episode:1356 meanR:10.7900 R:15.0 Steps:299 loss:0.0425\n",
      "Episode:1357 meanR:10.8300 R:14.0 Steps:299 loss:0.0615\n",
      "Episode:1358 meanR:10.9200 R:18.0 Steps:299 loss:0.0645\n",
      "Episode:1359 meanR:10.9200 R:7.0 Steps:299 loss:0.0674\n",
      "Episode:1360 meanR:10.8800 R:9.0 Steps:299 loss:0.0792\n",
      "Episode:1361 meanR:10.9100 R:8.0 Steps:299 loss:0.0532\n",
      "Episode:1362 meanR:10.8100 R:-1.0 Steps:299 loss:0.0487\n",
      "Episode:1363 meanR:10.8900 R:11.0 Steps:299 loss:0.0500\n",
      "Episode:1364 meanR:10.9800 R:16.0 Steps:299 loss:0.0693\n",
      "Episode:1365 meanR:10.9800 R:14.0 Steps:299 loss:0.0815\n",
      "Episode:1366 meanR:10.9000 R:11.0 Steps:299 loss:0.0858\n",
      "Episode:1367 meanR:11.0200 R:15.0 Steps:299 loss:0.0564\n",
      "Episode:1368 meanR:11.0400 R:13.0 Steps:299 loss:0.0772\n",
      "Episode:1369 meanR:11.0200 R:12.0 Steps:299 loss:0.0749\n",
      "Episode:1370 meanR:10.9500 R:8.0 Steps:299 loss:0.0687\n",
      "Episode:1371 meanR:10.9400 R:13.0 Steps:299 loss:0.0996\n",
      "Episode:1372 meanR:10.9600 R:12.0 Steps:299 loss:0.0770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1373 meanR:10.9700 R:13.0 Steps:299 loss:0.0586\n",
      "Episode:1374 meanR:11.0000 R:10.0 Steps:299 loss:0.0765\n",
      "Episode:1375 meanR:11.0600 R:18.0 Steps:299 loss:0.0708\n",
      "Episode:1376 meanR:10.9700 R:3.0 Steps:299 loss:0.0798\n",
      "Episode:1377 meanR:10.9400 R:8.0 Steps:299 loss:0.0412\n",
      "Episode:1378 meanR:10.9900 R:14.0 Steps:299 loss:0.0606\n",
      "Episode:1379 meanR:11.0100 R:11.0 Steps:299 loss:0.0621\n",
      "Episode:1380 meanR:11.0200 R:13.0 Steps:299 loss:0.0546\n",
      "Episode:1381 meanR:10.9800 R:13.0 Steps:299 loss:0.0853\n",
      "Episode:1382 meanR:10.9800 R:14.0 Steps:299 loss:0.0644\n",
      "Episode:1383 meanR:10.9400 R:10.0 Steps:299 loss:0.0616\n",
      "Episode:1384 meanR:11.0000 R:12.0 Steps:299 loss:0.0532\n",
      "Episode:1385 meanR:10.9900 R:14.0 Steps:299 loss:0.0482\n",
      "Episode:1386 meanR:11.0400 R:18.0 Steps:299 loss:0.0540\n",
      "Episode:1387 meanR:10.9800 R:9.0 Steps:299 loss:0.0699\n",
      "Episode:1388 meanR:11.0000 R:9.0 Steps:299 loss:0.0549\n",
      "Episode:1389 meanR:10.9100 R:9.0 Steps:299 loss:0.0667\n",
      "Episode:1390 meanR:10.9700 R:13.0 Steps:299 loss:0.0795\n",
      "Episode:1391 meanR:11.0100 R:12.0 Steps:299 loss:0.0764\n",
      "Episode:1392 meanR:11.0400 R:15.0 Steps:299 loss:0.0903\n",
      "Episode:1393 meanR:11.0100 R:9.0 Steps:299 loss:0.0832\n",
      "Episode:1394 meanR:11.0500 R:9.0 Steps:299 loss:0.0776\n",
      "Episode:1395 meanR:10.9900 R:4.0 Steps:299 loss:0.0573\n",
      "Episode:1396 meanR:11.0700 R:15.0 Steps:299 loss:0.0624\n",
      "Episode:1397 meanR:11.0800 R:13.0 Steps:299 loss:0.0738\n",
      "Episode:1398 meanR:11.1300 R:13.0 Steps:299 loss:0.0742\n",
      "Episode:1399 meanR:11.1800 R:10.0 Steps:299 loss:0.0851\n",
      "Episode:1400 meanR:11.1300 R:10.0 Steps:299 loss:0.0734\n",
      "Episode:1401 meanR:11.1000 R:6.0 Steps:299 loss:0.0652\n",
      "Episode:1402 meanR:11.1400 R:11.0 Steps:299 loss:0.0752\n",
      "Episode:1403 meanR:11.1700 R:15.0 Steps:299 loss:0.0501\n",
      "Episode:1404 meanR:11.1200 R:9.0 Steps:299 loss:0.0472\n",
      "Episode:1405 meanR:11.1400 R:16.0 Steps:299 loss:0.0622\n",
      "Episode:1406 meanR:11.1400 R:5.0 Steps:299 loss:0.0896\n",
      "Episode:1407 meanR:11.1600 R:4.0 Steps:299 loss:0.0655\n",
      "Episode:1408 meanR:11.2400 R:12.0 Steps:299 loss:0.0334\n",
      "Episode:1409 meanR:11.2400 R:9.0 Steps:299 loss:0.0470\n",
      "Episode:1410 meanR:11.2300 R:11.0 Steps:299 loss:0.0698\n",
      "Episode:1411 meanR:11.2300 R:5.0 Steps:299 loss:0.0516\n",
      "Episode:1412 meanR:11.1800 R:9.0 Steps:299 loss:0.0427\n",
      "Episode:1413 meanR:11.1600 R:11.0 Steps:299 loss:0.0602\n",
      "Episode:1414 meanR:11.1700 R:6.0 Steps:299 loss:0.0537\n",
      "Episode:1415 meanR:11.0600 R:8.0 Steps:299 loss:0.0730\n",
      "Episode:1416 meanR:11.1700 R:12.0 Steps:299 loss:0.0710\n",
      "Episode:1417 meanR:11.2000 R:5.0 Steps:299 loss:0.0674\n",
      "Episode:1418 meanR:11.1800 R:14.0 Steps:299 loss:0.0623\n",
      "Episode:1419 meanR:11.1800 R:8.0 Steps:299 loss:0.0481\n",
      "Episode:1420 meanR:11.2300 R:17.0 Steps:299 loss:0.0719\n",
      "Episode:1421 meanR:11.1700 R:7.0 Steps:299 loss:0.1009\n",
      "Episode:1422 meanR:11.0700 R:6.0 Steps:299 loss:0.0817\n",
      "Episode:1423 meanR:11.1000 R:9.0 Steps:299 loss:0.0496\n",
      "Episode:1424 meanR:11.1200 R:14.0 Steps:299 loss:0.0683\n",
      "Episode:1425 meanR:11.2000 R:12.0 Steps:299 loss:0.0665\n",
      "Episode:1426 meanR:11.2400 R:16.0 Steps:299 loss:0.0662\n",
      "Episode:1427 meanR:11.1800 R:5.0 Steps:299 loss:0.0697\n",
      "Episode:1428 meanR:11.1100 R:13.0 Steps:299 loss:0.0622\n",
      "Episode:1429 meanR:11.0500 R:9.0 Steps:299 loss:0.0527\n",
      "Episode:1430 meanR:10.8700 R:0.0 Steps:299 loss:0.0528\n",
      "Episode:1431 meanR:10.7600 R:5.0 Steps:299 loss:0.0389\n",
      "Episode:1432 meanR:10.7700 R:12.0 Steps:299 loss:0.0455\n",
      "Episode:1433 meanR:10.7400 R:11.0 Steps:299 loss:0.0757\n",
      "Episode:1434 meanR:10.7900 R:11.0 Steps:299 loss:0.0523\n",
      "Episode:1435 meanR:10.7700 R:10.0 Steps:299 loss:0.0467\n",
      "Episode:1436 meanR:10.7800 R:13.0 Steps:299 loss:0.0645\n",
      "Episode:1437 meanR:10.7400 R:7.0 Steps:299 loss:0.0343\n",
      "Episode:1438 meanR:10.7500 R:10.0 Steps:299 loss:0.0697\n",
      "Episode:1439 meanR:10.7600 R:15.0 Steps:299 loss:0.0619\n",
      "Episode:1440 meanR:10.6700 R:5.0 Steps:299 loss:0.0446\n",
      "Episode:1441 meanR:10.5900 R:6.0 Steps:299 loss:0.0468\n",
      "Episode:1442 meanR:10.5900 R:9.0 Steps:299 loss:0.0499\n",
      "Episode:1443 meanR:10.6300 R:9.0 Steps:299 loss:0.0388\n",
      "Episode:1444 meanR:10.5800 R:8.0 Steps:299 loss:0.0479\n",
      "Episode:1445 meanR:10.5300 R:10.0 Steps:299 loss:0.0556\n",
      "Episode:1446 meanR:10.5100 R:8.0 Steps:299 loss:0.0319\n",
      "Episode:1447 meanR:10.4200 R:3.0 Steps:299 loss:0.0546\n",
      "Episode:1448 meanR:10.3600 R:9.0 Steps:299 loss:0.0432\n",
      "Episode:1449 meanR:10.3100 R:6.0 Steps:299 loss:0.0624\n",
      "Episode:1450 meanR:10.2000 R:7.0 Steps:299 loss:0.0412\n",
      "Episode:1451 meanR:10.1100 R:0.0 Steps:299 loss:0.0171\n",
      "Episode:1452 meanR:10.0400 R:7.0 Steps:299 loss:0.0529\n",
      "Episode:1453 meanR:10.0300 R:7.0 Steps:299 loss:0.0637\n",
      "Episode:1454 meanR:10.1600 R:17.0 Steps:299 loss:0.0749\n",
      "Episode:1455 meanR:10.2200 R:17.0 Steps:299 loss:0.0689\n",
      "Episode:1456 meanR:10.1200 R:5.0 Steps:299 loss:0.0647\n",
      "Episode:1457 meanR:10.0200 R:4.0 Steps:299 loss:0.0418\n",
      "Episode:1458 meanR:9.9200 R:8.0 Steps:299 loss:0.0431\n",
      "Episode:1459 meanR:9.8900 R:4.0 Steps:299 loss:0.0474\n",
      "Episode:1460 meanR:9.8100 R:1.0 Steps:299 loss:0.0270\n",
      "Episode:1461 meanR:9.8200 R:9.0 Steps:299 loss:0.0414\n",
      "Episode:1462 meanR:9.9600 R:13.0 Steps:299 loss:0.0556\n",
      "Episode:1463 meanR:9.9300 R:8.0 Steps:299 loss:0.0506\n",
      "Episode:1464 meanR:9.8700 R:10.0 Steps:299 loss:0.0476\n",
      "Episode:1465 meanR:9.8300 R:10.0 Steps:299 loss:0.0692\n",
      "Episode:1466 meanR:9.7600 R:4.0 Steps:299 loss:0.0524\n",
      "Episode:1467 meanR:9.7200 R:11.0 Steps:299 loss:0.0377\n",
      "Episode:1468 meanR:9.6800 R:9.0 Steps:299 loss:0.0473\n",
      "Episode:1469 meanR:9.7300 R:17.0 Steps:299 loss:0.0724\n",
      "Episode:1470 meanR:9.7100 R:6.0 Steps:299 loss:0.0572\n",
      "Episode:1471 meanR:9.7100 R:13.0 Steps:299 loss:0.0582\n",
      "Episode:1472 meanR:9.7400 R:15.0 Steps:299 loss:0.0629\n",
      "Episode:1473 meanR:9.6500 R:4.0 Steps:299 loss:0.0865\n",
      "Episode:1474 meanR:9.6500 R:10.0 Steps:299 loss:0.0538\n",
      "Episode:1475 meanR:9.6600 R:19.0 Steps:299 loss:0.0873\n",
      "Episode:1476 meanR:9.8100 R:18.0 Steps:299 loss:0.0983\n",
      "Episode:1477 meanR:9.8100 R:8.0 Steps:299 loss:0.0838\n",
      "Episode:1478 meanR:9.7000 R:3.0 Steps:299 loss:0.0718\n",
      "Episode:1479 meanR:9.6900 R:10.0 Steps:299 loss:0.0549\n",
      "Episode:1480 meanR:9.7100 R:15.0 Steps:299 loss:0.0850\n",
      "Episode:1481 meanR:9.7200 R:14.0 Steps:299 loss:0.0699\n",
      "Episode:1482 meanR:9.7200 R:14.0 Steps:299 loss:0.0556\n",
      "Episode:1483 meanR:9.7700 R:15.0 Steps:299 loss:0.0798\n",
      "Episode:1484 meanR:9.7500 R:10.0 Steps:299 loss:0.0820\n",
      "Episode:1485 meanR:9.6500 R:4.0 Steps:299 loss:0.0599\n",
      "Episode:1486 meanR:9.5700 R:10.0 Steps:299 loss:0.0408\n",
      "Episode:1487 meanR:9.6200 R:14.0 Steps:299 loss:0.0666\n",
      "Episode:1488 meanR:9.6200 R:9.0 Steps:299 loss:0.0805\n",
      "Episode:1489 meanR:9.6400 R:11.0 Steps:299 loss:0.0526\n",
      "Episode:1490 meanR:9.6000 R:9.0 Steps:299 loss:0.0532\n",
      "Episode:1491 meanR:9.6300 R:15.0 Steps:299 loss:0.0610\n",
      "Episode:1492 meanR:9.6500 R:17.0 Steps:299 loss:0.0729\n",
      "Episode:1493 meanR:9.6900 R:13.0 Steps:299 loss:0.0831\n",
      "Episode:1494 meanR:9.7100 R:11.0 Steps:299 loss:0.0579\n",
      "Episode:1495 meanR:9.6700 R:0.0 Steps:299 loss:0.0599\n",
      "Episode:1496 meanR:9.5200 R:0.0 Steps:299 loss:0.0490\n",
      "Episode:1497 meanR:9.4800 R:9.0 Steps:299 loss:0.0485\n",
      "Episode:1498 meanR:9.4900 R:14.0 Steps:299 loss:0.0623\n",
      "Episode:1499 meanR:9.4700 R:8.0 Steps:299 loss:0.0499\n",
      "Episode:1500 meanR:9.5100 R:14.0 Steps:299 loss:0.0577\n",
      "Episode:1501 meanR:9.5800 R:13.0 Steps:299 loss:0.0530\n",
      "Episode:1502 meanR:9.6400 R:17.0 Steps:299 loss:0.0583\n",
      "Episode:1503 meanR:9.5400 R:5.0 Steps:299 loss:0.0431\n",
      "Episode:1504 meanR:9.6000 R:15.0 Steps:299 loss:0.0754\n",
      "Episode:1505 meanR:9.5100 R:7.0 Steps:299 loss:0.0794\n",
      "Episode:1506 meanR:9.5100 R:5.0 Steps:299 loss:0.0599\n",
      "Episode:1507 meanR:9.5700 R:10.0 Steps:299 loss:0.0632\n",
      "Episode:1508 meanR:9.5800 R:13.0 Steps:299 loss:0.0491\n",
      "Episode:1509 meanR:9.5200 R:3.0 Steps:299 loss:0.0640\n",
      "Episode:1510 meanR:9.5200 R:11.0 Steps:299 loss:0.0520\n",
      "Episode:1511 meanR:9.5800 R:11.0 Steps:299 loss:0.0606\n",
      "Episode:1512 meanR:9.6400 R:15.0 Steps:299 loss:0.0643\n",
      "Episode:1513 meanR:9.6100 R:8.0 Steps:299 loss:0.0682\n",
      "Episode:1514 meanR:9.6100 R:6.0 Steps:299 loss:0.0609\n",
      "Episode:1515 meanR:9.6700 R:14.0 Steps:299 loss:0.0649\n",
      "Episode:1516 meanR:9.6100 R:6.0 Steps:299 loss:0.0783\n",
      "Episode:1517 meanR:9.7100 R:15.0 Steps:299 loss:0.0693\n",
      "Episode:1518 meanR:9.6000 R:3.0 Steps:299 loss:0.0701\n",
      "Episode:1519 meanR:9.6500 R:13.0 Steps:299 loss:0.0535\n",
      "Episode:1520 meanR:9.6300 R:15.0 Steps:299 loss:0.0647\n",
      "Episode:1521 meanR:9.7100 R:15.0 Steps:299 loss:0.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1522 meanR:9.7400 R:9.0 Steps:299 loss:0.0618\n",
      "Episode:1523 meanR:9.7800 R:13.0 Steps:299 loss:0.0694\n",
      "Episode:1524 meanR:9.7900 R:15.0 Steps:299 loss:0.0585\n",
      "Episode:1525 meanR:9.8600 R:19.0 Steps:299 loss:0.0961\n",
      "Episode:1526 meanR:9.8500 R:15.0 Steps:299 loss:0.1210\n",
      "Episode:1527 meanR:9.9400 R:14.0 Steps:299 loss:0.0909\n",
      "Episode:1528 meanR:9.9400 R:13.0 Steps:299 loss:0.0629\n",
      "Episode:1529 meanR:10.0100 R:16.0 Steps:299 loss:0.0460\n",
      "Episode:1530 meanR:10.0700 R:6.0 Steps:299 loss:0.0899\n",
      "Episode:1531 meanR:10.1300 R:11.0 Steps:299 loss:0.0504\n",
      "Episode:1532 meanR:10.0600 R:5.0 Steps:299 loss:0.0549\n",
      "Episode:1533 meanR:10.0800 R:13.0 Steps:299 loss:0.0693\n",
      "Episode:1534 meanR:10.0600 R:9.0 Steps:299 loss:0.0868\n",
      "Episode:1535 meanR:10.0200 R:6.0 Steps:299 loss:0.0686\n",
      "Episode:1536 meanR:10.0000 R:11.0 Steps:299 loss:0.0445\n",
      "Episode:1537 meanR:10.0600 R:13.0 Steps:299 loss:0.0542\n",
      "Episode:1538 meanR:10.1400 R:18.0 Steps:299 loss:0.0705\n",
      "Episode:1539 meanR:10.0500 R:6.0 Steps:299 loss:0.0421\n",
      "Episode:1540 meanR:10.0900 R:9.0 Steps:299 loss:0.0621\n",
      "Episode:1541 meanR:10.0900 R:6.0 Steps:299 loss:0.0718\n",
      "Episode:1542 meanR:10.1400 R:14.0 Steps:299 loss:0.0688\n",
      "Episode:1543 meanR:10.0900 R:4.0 Steps:299 loss:0.0572\n",
      "Episode:1544 meanR:10.0700 R:6.0 Steps:299 loss:0.0532\n",
      "Episode:1545 meanR:10.1200 R:15.0 Steps:299 loss:0.0454\n",
      "Episode:1546 meanR:10.1000 R:6.0 Steps:299 loss:0.0503\n",
      "Episode:1547 meanR:10.1600 R:9.0 Steps:299 loss:0.0844\n",
      "Episode:1548 meanR:10.1600 R:9.0 Steps:299 loss:0.0607\n",
      "Episode:1549 meanR:10.2300 R:13.0 Steps:299 loss:0.0540\n",
      "Episode:1550 meanR:10.3200 R:16.0 Steps:299 loss:0.0796\n",
      "Episode:1551 meanR:10.4700 R:15.0 Steps:299 loss:0.0892\n",
      "Episode:1552 meanR:10.5100 R:11.0 Steps:299 loss:0.0734\n",
      "Episode:1553 meanR:10.5100 R:7.0 Steps:299 loss:0.0684\n",
      "Episode:1554 meanR:10.4200 R:8.0 Steps:299 loss:0.0549\n",
      "Episode:1555 meanR:10.3200 R:7.0 Steps:299 loss:0.0850\n",
      "Episode:1556 meanR:10.4100 R:14.0 Steps:299 loss:0.0562\n",
      "Episode:1557 meanR:10.4400 R:7.0 Steps:299 loss:0.0576\n",
      "Episode:1558 meanR:10.4600 R:10.0 Steps:299 loss:0.0421\n",
      "Episode:1559 meanR:10.5300 R:11.0 Steps:299 loss:0.0580\n",
      "Episode:1560 meanR:10.5900 R:7.0 Steps:299 loss:0.0739\n",
      "Episode:1561 meanR:10.6000 R:10.0 Steps:299 loss:0.0427\n",
      "Episode:1562 meanR:10.4900 R:2.0 Steps:299 loss:0.0392\n",
      "Episode:1563 meanR:10.5100 R:10.0 Steps:299 loss:0.0498\n",
      "Episode:1564 meanR:10.4600 R:5.0 Steps:299 loss:0.0806\n",
      "Episode:1565 meanR:10.4800 R:12.0 Steps:299 loss:0.0427\n",
      "Episode:1566 meanR:10.5300 R:9.0 Steps:299 loss:0.0493\n",
      "Episode:1567 meanR:10.4300 R:1.0 Steps:299 loss:0.0370\n",
      "Episode:1568 meanR:10.3600 R:2.0 Steps:299 loss:0.0305\n",
      "Episode:1569 meanR:10.2400 R:5.0 Steps:299 loss:0.0383\n",
      "Episode:1570 meanR:10.2400 R:6.0 Steps:299 loss:0.0247\n",
      "Episode:1571 meanR:10.2300 R:12.0 Steps:299 loss:0.0320\n",
      "Episode:1572 meanR:10.2100 R:13.0 Steps:299 loss:0.0535\n",
      "Episode:1573 meanR:10.3000 R:13.0 Steps:299 loss:0.0627\n",
      "Episode:1574 meanR:10.2800 R:8.0 Steps:299 loss:0.0733\n",
      "Episode:1575 meanR:10.1900 R:10.0 Steps:299 loss:0.0760\n",
      "Episode:1576 meanR:10.1500 R:14.0 Steps:299 loss:0.0462\n",
      "Episode:1577 meanR:10.2100 R:14.0 Steps:299 loss:0.0741\n",
      "Episode:1578 meanR:10.3200 R:14.0 Steps:299 loss:0.0469\n",
      "Episode:1579 meanR:10.3700 R:15.0 Steps:299 loss:0.0609\n",
      "Episode:1580 meanR:10.3800 R:16.0 Steps:299 loss:0.0614\n",
      "Episode:1581 meanR:10.2800 R:4.0 Steps:299 loss:0.0649\n",
      "Episode:1582 meanR:10.2400 R:10.0 Steps:299 loss:0.0527\n",
      "Episode:1583 meanR:10.1300 R:4.0 Steps:299 loss:0.0438\n",
      "Episode:1584 meanR:10.1600 R:13.0 Steps:299 loss:0.0442\n",
      "Episode:1585 meanR:10.1800 R:6.0 Steps:299 loss:0.0475\n",
      "Episode:1586 meanR:10.1700 R:9.0 Steps:299 loss:0.0645\n",
      "Episode:1587 meanR:10.1600 R:13.0 Steps:299 loss:0.0587\n",
      "Episode:1588 meanR:10.2300 R:16.0 Steps:299 loss:0.0591\n",
      "Episode:1589 meanR:10.3000 R:18.0 Steps:299 loss:0.0715\n",
      "Episode:1590 meanR:10.3500 R:14.0 Steps:299 loss:0.0755\n",
      "Episode:1591 meanR:10.3400 R:14.0 Steps:299 loss:0.0627\n",
      "Episode:1592 meanR:10.3100 R:14.0 Steps:299 loss:0.0791\n",
      "Episode:1593 meanR:10.2900 R:11.0 Steps:299 loss:0.1034\n",
      "Episode:1594 meanR:10.3300 R:15.0 Steps:299 loss:0.0810\n",
      "Episode:1595 meanR:10.3400 R:1.0 Steps:299 loss:0.0291\n",
      "Episode:1596 meanR:10.4200 R:8.0 Steps:299 loss:0.0608\n",
      "Episode:1597 meanR:10.4600 R:13.0 Steps:299 loss:0.1003\n",
      "Episode:1598 meanR:10.3800 R:6.0 Steps:299 loss:0.0772\n",
      "Episode:1599 meanR:10.3300 R:3.0 Steps:299 loss:0.0643\n",
      "Episode:1600 meanR:10.2500 R:6.0 Steps:299 loss:0.0593\n",
      "Episode:1601 meanR:10.2500 R:13.0 Steps:299 loss:0.0334\n",
      "Episode:1602 meanR:10.2000 R:12.0 Steps:299 loss:0.0531\n",
      "Episode:1603 meanR:10.2900 R:14.0 Steps:299 loss:0.0762\n",
      "Episode:1604 meanR:10.2700 R:13.0 Steps:299 loss:0.0518\n",
      "Episode:1605 meanR:10.3200 R:12.0 Steps:299 loss:0.0513\n",
      "Episode:1606 meanR:10.3100 R:4.0 Steps:299 loss:0.0581\n",
      "Episode:1607 meanR:10.3500 R:14.0 Steps:299 loss:0.0476\n",
      "Episode:1608 meanR:10.2800 R:6.0 Steps:299 loss:0.0569\n",
      "Episode:1609 meanR:10.3900 R:14.0 Steps:299 loss:0.0725\n",
      "Episode:1610 meanR:10.3600 R:8.0 Steps:299 loss:0.0755\n",
      "Episode:1611 meanR:10.3900 R:14.0 Steps:299 loss:0.0508\n",
      "Episode:1612 meanR:10.3400 R:10.0 Steps:299 loss:0.0409\n",
      "Episode:1613 meanR:10.3700 R:11.0 Steps:299 loss:0.0787\n",
      "Episode:1614 meanR:10.4600 R:15.0 Steps:299 loss:0.0732\n",
      "Episode:1615 meanR:10.4300 R:11.0 Steps:299 loss:0.0689\n",
      "Episode:1616 meanR:10.4900 R:12.0 Steps:299 loss:0.0652\n",
      "Episode:1617 meanR:10.4500 R:11.0 Steps:299 loss:0.0618\n",
      "Episode:1618 meanR:10.5400 R:12.0 Steps:299 loss:0.0563\n",
      "Episode:1619 meanR:10.5600 R:15.0 Steps:299 loss:0.0651\n",
      "Episode:1620 meanR:10.5500 R:14.0 Steps:299 loss:0.0568\n",
      "Episode:1621 meanR:10.5600 R:16.0 Steps:299 loss:0.0702\n",
      "Episode:1622 meanR:10.5400 R:7.0 Steps:299 loss:0.0608\n",
      "Episode:1623 meanR:10.5300 R:12.0 Steps:299 loss:0.0410\n",
      "Episode:1624 meanR:10.4000 R:2.0 Steps:299 loss:0.0509\n",
      "Episode:1625 meanR:10.2800 R:7.0 Steps:299 loss:0.0387\n",
      "Episode:1626 meanR:10.2100 R:8.0 Steps:299 loss:0.0584\n",
      "Episode:1627 meanR:10.1500 R:8.0 Steps:299 loss:0.0656\n",
      "Episode:1628 meanR:10.0300 R:1.0 Steps:299 loss:0.0536\n",
      "Episode:1629 meanR:9.8800 R:1.0 Steps:299 loss:0.0256\n",
      "Episode:1630 meanR:9.9600 R:14.0 Steps:299 loss:0.0414\n",
      "Episode:1631 meanR:9.9500 R:10.0 Steps:299 loss:0.0472\n",
      "Episode:1632 meanR:9.9500 R:5.0 Steps:299 loss:0.0346\n",
      "Episode:1633 meanR:9.8700 R:5.0 Steps:299 loss:0.0349\n",
      "Episode:1634 meanR:9.8800 R:10.0 Steps:299 loss:0.0425\n",
      "Episode:1635 meanR:9.9100 R:9.0 Steps:299 loss:0.0737\n",
      "Episode:1636 meanR:9.8900 R:9.0 Steps:299 loss:0.0568\n",
      "Episode:1637 meanR:9.8700 R:11.0 Steps:299 loss:0.0743\n",
      "Episode:1638 meanR:9.7300 R:4.0 Steps:299 loss:0.0863\n",
      "Episode:1639 meanR:9.7800 R:11.0 Steps:299 loss:0.0547\n",
      "Episode:1640 meanR:9.8500 R:16.0 Steps:299 loss:0.0712\n",
      "Episode:1641 meanR:9.9700 R:18.0 Steps:299 loss:0.0657\n",
      "Episode:1642 meanR:9.9100 R:8.0 Steps:299 loss:0.0496\n",
      "Episode:1643 meanR:9.9000 R:3.0 Steps:299 loss:0.0426\n",
      "Episode:1644 meanR:9.9600 R:12.0 Steps:299 loss:0.0611\n",
      "Episode:1645 meanR:9.8600 R:5.0 Steps:299 loss:0.0491\n",
      "Episode:1646 meanR:9.9100 R:11.0 Steps:299 loss:0.0401\n",
      "Episode:1647 meanR:9.9400 R:12.0 Steps:299 loss:0.0533\n",
      "Episode:1648 meanR:10.0300 R:18.0 Steps:299 loss:0.0476\n",
      "Episode:1649 meanR:10.0600 R:16.0 Steps:299 loss:0.0835\n",
      "Episode:1650 meanR:10.0000 R:10.0 Steps:299 loss:0.0879\n",
      "Episode:1651 meanR:9.9300 R:8.0 Steps:299 loss:0.0562\n",
      "Episode:1652 meanR:9.8300 R:1.0 Steps:299 loss:0.0533\n",
      "Episode:1653 meanR:9.8700 R:11.0 Steps:299 loss:0.0419\n",
      "Episode:1654 meanR:9.9200 R:13.0 Steps:299 loss:0.0779\n",
      "Episode:1655 meanR:9.9400 R:9.0 Steps:299 loss:0.0871\n",
      "Episode:1656 meanR:9.8500 R:5.0 Steps:299 loss:0.0621\n",
      "Episode:1657 meanR:9.8500 R:7.0 Steps:299 loss:0.0439\n",
      "Episode:1658 meanR:9.8700 R:12.0 Steps:299 loss:0.0435\n",
      "Episode:1659 meanR:9.8600 R:10.0 Steps:299 loss:0.0472\n",
      "Episode:1660 meanR:9.9100 R:12.0 Steps:299 loss:0.0700\n",
      "Episode:1661 meanR:9.9200 R:11.0 Steps:299 loss:0.0840\n",
      "Episode:1662 meanR:10.0100 R:11.0 Steps:299 loss:0.0655\n",
      "Episode:1663 meanR:10.0500 R:14.0 Steps:299 loss:0.0696\n",
      "Episode:1664 meanR:10.1400 R:14.0 Steps:299 loss:0.0766\n",
      "Episode:1665 meanR:10.0700 R:5.0 Steps:299 loss:0.0870\n",
      "Episode:1666 meanR:10.0800 R:10.0 Steps:299 loss:0.0816\n",
      "Episode:1667 meanR:10.1600 R:9.0 Steps:299 loss:0.0604\n",
      "Episode:1668 meanR:10.1800 R:4.0 Steps:299 loss:0.0305\n",
      "Episode:1669 meanR:10.1800 R:5.0 Steps:299 loss:0.0597\n",
      "Episode:1670 meanR:10.2500 R:13.0 Steps:299 loss:0.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1671 meanR:10.1900 R:6.0 Steps:299 loss:0.0499\n",
      "Episode:1672 meanR:10.0800 R:2.0 Steps:299 loss:0.0409\n",
      "Episode:1673 meanR:10.0800 R:13.0 Steps:299 loss:0.0528\n",
      "Episode:1674 meanR:10.1400 R:14.0 Steps:299 loss:0.0569\n",
      "Episode:1675 meanR:10.1600 R:12.0 Steps:299 loss:0.0664\n",
      "Episode:1676 meanR:10.0600 R:4.0 Steps:299 loss:0.0960\n",
      "Episode:1677 meanR:9.9700 R:5.0 Steps:299 loss:0.0454\n",
      "Episode:1678 meanR:9.9600 R:13.0 Steps:299 loss:0.0388\n",
      "Episode:1679 meanR:9.9300 R:12.0 Steps:299 loss:0.0396\n",
      "Episode:1680 meanR:9.8500 R:8.0 Steps:299 loss:0.0774\n",
      "Episode:1681 meanR:9.8900 R:8.0 Steps:299 loss:0.0432\n",
      "Episode:1682 meanR:9.9000 R:11.0 Steps:299 loss:0.0402\n",
      "Episode:1683 meanR:10.0200 R:16.0 Steps:299 loss:0.0811\n",
      "Episode:1684 meanR:10.0100 R:12.0 Steps:299 loss:0.0486\n",
      "Episode:1685 meanR:10.1100 R:16.0 Steps:299 loss:0.0620\n",
      "Episode:1686 meanR:10.0400 R:2.0 Steps:299 loss:0.0630\n",
      "Episode:1687 meanR:10.0400 R:13.0 Steps:299 loss:0.0564\n",
      "Episode:1688 meanR:9.9200 R:4.0 Steps:299 loss:0.0654\n",
      "Episode:1689 meanR:9.7500 R:1.0 Steps:299 loss:0.0377\n",
      "Episode:1690 meanR:9.7500 R:14.0 Steps:299 loss:0.0570\n",
      "Episode:1691 meanR:9.7600 R:15.0 Steps:299 loss:0.0775\n",
      "Episode:1692 meanR:9.7700 R:15.0 Steps:299 loss:0.0707\n",
      "Episode:1693 meanR:9.7100 R:5.0 Steps:299 loss:0.0684\n",
      "Episode:1694 meanR:9.7000 R:14.0 Steps:299 loss:0.0791\n",
      "Episode:1695 meanR:9.8500 R:16.0 Steps:299 loss:0.0750\n",
      "Episode:1696 meanR:9.9100 R:14.0 Steps:299 loss:0.0613\n",
      "Episode:1697 meanR:9.9000 R:12.0 Steps:299 loss:0.0807\n",
      "Episode:1698 meanR:10.0000 R:16.0 Steps:299 loss:0.0760\n",
      "Episode:1699 meanR:10.0600 R:9.0 Steps:299 loss:0.0955\n",
      "Episode:1700 meanR:10.0600 R:6.0 Steps:299 loss:0.0406\n",
      "Episode:1701 meanR:10.0300 R:10.0 Steps:299 loss:0.0924\n",
      "Episode:1702 meanR:10.0000 R:9.0 Steps:299 loss:0.0524\n",
      "Episode:1703 meanR:9.9300 R:7.0 Steps:299 loss:0.0353\n",
      "Episode:1704 meanR:9.8600 R:6.0 Steps:299 loss:0.0627\n",
      "Episode:1705 meanR:9.8500 R:11.0 Steps:299 loss:0.0454\n",
      "Episode:1706 meanR:9.9400 R:13.0 Steps:299 loss:0.0525\n",
      "Episode:1707 meanR:9.8600 R:6.0 Steps:299 loss:0.0733\n",
      "Episode:1708 meanR:9.8600 R:6.0 Steps:299 loss:0.0711\n",
      "Episode:1709 meanR:9.8700 R:15.0 Steps:299 loss:0.0542\n",
      "Episode:1710 meanR:9.9100 R:12.0 Steps:299 loss:0.0821\n",
      "Episode:1711 meanR:9.9100 R:14.0 Steps:299 loss:0.0489\n",
      "Episode:1712 meanR:9.8200 R:1.0 Steps:299 loss:0.0555\n",
      "Episode:1713 meanR:9.8200 R:11.0 Steps:299 loss:0.0391\n",
      "Episode:1714 meanR:9.8300 R:16.0 Steps:299 loss:0.0535\n",
      "Episode:1715 meanR:9.7700 R:5.0 Steps:299 loss:0.0632\n",
      "Episode:1716 meanR:9.7600 R:11.0 Steps:299 loss:0.0616\n",
      "Episode:1717 meanR:9.8000 R:15.0 Steps:299 loss:0.0539\n",
      "Episode:1718 meanR:9.8000 R:12.0 Steps:299 loss:0.0674\n",
      "Episode:1719 meanR:9.7600 R:11.0 Steps:299 loss:0.1117\n",
      "Episode:1720 meanR:9.7200 R:10.0 Steps:299 loss:0.0499\n",
      "Episode:1721 meanR:9.6800 R:12.0 Steps:299 loss:0.0569\n",
      "Episode:1722 meanR:9.7800 R:17.0 Steps:299 loss:0.0717\n",
      "Episode:1723 meanR:9.7100 R:5.0 Steps:299 loss:0.0476\n",
      "Episode:1724 meanR:9.7600 R:7.0 Steps:299 loss:0.0502\n",
      "Episode:1725 meanR:9.8500 R:16.0 Steps:299 loss:0.0704\n",
      "Episode:1726 meanR:9.7900 R:2.0 Steps:299 loss:0.0472\n",
      "Episode:1727 meanR:9.8000 R:9.0 Steps:299 loss:0.0379\n",
      "Episode:1728 meanR:9.8800 R:9.0 Steps:299 loss:0.0564\n",
      "Episode:1729 meanR:9.9800 R:11.0 Steps:299 loss:0.0618\n",
      "Episode:1730 meanR:9.8600 R:2.0 Steps:299 loss:0.0454\n",
      "Episode:1731 meanR:9.8300 R:7.0 Steps:299 loss:0.0389\n",
      "Episode:1732 meanR:9.8600 R:8.0 Steps:299 loss:0.0511\n",
      "Episode:1733 meanR:9.9600 R:15.0 Steps:299 loss:0.0479\n",
      "Episode:1734 meanR:9.9400 R:8.0 Steps:299 loss:0.0482\n",
      "Episode:1735 meanR:9.9900 R:14.0 Steps:299 loss:0.0449\n",
      "Episode:1736 meanR:10.0500 R:15.0 Steps:299 loss:0.0551\n",
      "Episode:1737 meanR:10.0500 R:11.0 Steps:299 loss:0.0633\n",
      "Episode:1738 meanR:10.0900 R:8.0 Steps:299 loss:0.0584\n",
      "Episode:1739 meanR:10.0500 R:7.0 Steps:299 loss:0.0565\n",
      "Episode:1740 meanR:9.9700 R:8.0 Steps:299 loss:0.0451\n",
      "Episode:1741 meanR:9.8400 R:5.0 Steps:299 loss:0.0394\n",
      "Episode:1742 meanR:9.8900 R:13.0 Steps:299 loss:0.0458\n",
      "Episode:1743 meanR:9.8600 R:0.0 Steps:299 loss:0.0390\n",
      "Episode:1744 meanR:9.8000 R:6.0 Steps:299 loss:0.0511\n",
      "Episode:1745 meanR:9.8600 R:11.0 Steps:299 loss:0.0361\n",
      "Episode:1746 meanR:9.8200 R:7.0 Steps:299 loss:0.0410\n",
      "Episode:1747 meanR:9.8000 R:10.0 Steps:299 loss:0.0592\n",
      "Episode:1748 meanR:9.7400 R:12.0 Steps:299 loss:0.0518\n",
      "Episode:1749 meanR:9.6700 R:9.0 Steps:299 loss:0.0615\n",
      "Episode:1750 meanR:9.5700 R:0.0 Steps:299 loss:0.0439\n",
      "Episode:1751 meanR:9.6200 R:13.0 Steps:299 loss:0.0516\n",
      "Episode:1752 meanR:9.6700 R:6.0 Steps:299 loss:0.0702\n",
      "Episode:1753 meanR:9.6300 R:7.0 Steps:299 loss:0.0414\n",
      "Episode:1754 meanR:9.5900 R:9.0 Steps:299 loss:0.0503\n",
      "Episode:1755 meanR:9.6300 R:13.0 Steps:299 loss:0.0469\n",
      "Episode:1756 meanR:9.6900 R:11.0 Steps:299 loss:0.0657\n",
      "Episode:1757 meanR:9.6900 R:7.0 Steps:299 loss:0.0797\n",
      "Episode:1758 meanR:9.6700 R:10.0 Steps:299 loss:0.0552\n",
      "Episode:1759 meanR:9.6200 R:5.0 Steps:299 loss:0.0691\n",
      "Episode:1760 meanR:9.5500 R:5.0 Steps:299 loss:0.0479\n",
      "Episode:1761 meanR:9.5100 R:7.0 Steps:299 loss:0.0371\n",
      "Episode:1762 meanR:9.4700 R:7.0 Steps:299 loss:0.0286\n",
      "Episode:1763 meanR:9.4500 R:12.0 Steps:299 loss:0.0455\n",
      "Episode:1764 meanR:9.4100 R:10.0 Steps:299 loss:0.0601\n",
      "Episode:1765 meanR:9.4500 R:9.0 Steps:299 loss:0.0605\n",
      "Episode:1766 meanR:9.5000 R:15.0 Steps:299 loss:0.0617\n",
      "Episode:1767 meanR:9.6000 R:19.0 Steps:299 loss:0.0668\n",
      "Episode:1768 meanR:9.6300 R:7.0 Steps:299 loss:0.0738\n",
      "Episode:1769 meanR:9.7800 R:20.0 Steps:299 loss:0.0591\n",
      "Episode:1770 meanR:9.7900 R:14.0 Steps:299 loss:0.0955\n",
      "Episode:1771 meanR:9.8500 R:12.0 Steps:299 loss:0.0625\n",
      "Episode:1772 meanR:9.9600 R:13.0 Steps:299 loss:0.0656\n",
      "Episode:1773 meanR:9.9800 R:15.0 Steps:299 loss:0.0718\n",
      "Episode:1774 meanR:9.9400 R:10.0 Steps:299 loss:0.0607\n",
      "Episode:1775 meanR:9.9400 R:12.0 Steps:299 loss:0.0687\n",
      "Episode:1776 meanR:10.0000 R:10.0 Steps:299 loss:0.0775\n",
      "Episode:1777 meanR:10.0200 R:7.0 Steps:299 loss:0.0629\n",
      "Episode:1778 meanR:9.9700 R:8.0 Steps:299 loss:0.0436\n",
      "Episode:1779 meanR:9.9700 R:12.0 Steps:299 loss:0.0564\n",
      "Episode:1780 meanR:9.9700 R:8.0 Steps:299 loss:0.0599\n",
      "Episode:1781 meanR:10.0200 R:13.0 Steps:299 loss:0.0525\n",
      "Episode:1782 meanR:10.0600 R:15.0 Steps:299 loss:0.0558\n",
      "Episode:1783 meanR:10.0000 R:10.0 Steps:299 loss:0.0592\n",
      "Episode:1784 meanR:10.0300 R:15.0 Steps:299 loss:0.0448\n",
      "Episode:1785 meanR:9.9600 R:9.0 Steps:299 loss:0.0790\n",
      "Episode:1786 meanR:10.0500 R:11.0 Steps:299 loss:0.0858\n",
      "Episode:1787 meanR:10.0800 R:16.0 Steps:299 loss:0.0398\n",
      "Episode:1788 meanR:10.0900 R:5.0 Steps:299 loss:0.0561\n",
      "Episode:1789 meanR:10.1000 R:2.0 Steps:299 loss:0.0360\n",
      "Episode:1790 meanR:10.0200 R:6.0 Steps:299 loss:0.0380\n",
      "Episode:1791 meanR:9.9400 R:7.0 Steps:299 loss:0.0393\n",
      "Episode:1792 meanR:9.9600 R:17.0 Steps:299 loss:0.0533\n",
      "Episode:1793 meanR:10.0300 R:12.0 Steps:299 loss:0.0738\n",
      "Episode:1794 meanR:9.9900 R:10.0 Steps:299 loss:0.0615\n",
      "Episode:1795 meanR:9.9100 R:8.0 Steps:299 loss:0.0664\n",
      "Episode:1796 meanR:9.8400 R:7.0 Steps:299 loss:0.0598\n",
      "Episode:1797 meanR:9.8000 R:8.0 Steps:299 loss:0.0413\n",
      "Episode:1798 meanR:9.7400 R:10.0 Steps:299 loss:0.0459\n",
      "Episode:1799 meanR:9.7700 R:12.0 Steps:299 loss:0.0453\n",
      "Episode:1800 meanR:9.9000 R:19.0 Steps:299 loss:0.0823\n",
      "Episode:1801 meanR:9.9400 R:14.0 Steps:299 loss:0.0553\n",
      "Episode:1802 meanR:9.9900 R:14.0 Steps:299 loss:0.0602\n",
      "Episode:1803 meanR:10.0400 R:12.0 Steps:299 loss:0.0679\n",
      "Episode:1804 meanR:10.0800 R:10.0 Steps:299 loss:0.0699\n",
      "Episode:1805 meanR:10.0100 R:4.0 Steps:299 loss:0.0827\n",
      "Episode:1806 meanR:10.0600 R:18.0 Steps:299 loss:0.0655\n",
      "Episode:1807 meanR:10.1800 R:18.0 Steps:299 loss:0.1007\n",
      "Episode:1808 meanR:10.2000 R:8.0 Steps:299 loss:0.1225\n",
      "Episode:1809 meanR:10.0800 R:3.0 Steps:299 loss:0.0519\n",
      "Episode:1810 meanR:10.0800 R:12.0 Steps:299 loss:0.0546\n",
      "Episode:1811 meanR:10.0500 R:11.0 Steps:299 loss:0.0722\n",
      "Episode:1812 meanR:10.1000 R:6.0 Steps:299 loss:0.0494\n",
      "Episode:1813 meanR:10.1000 R:11.0 Steps:299 loss:0.0464\n",
      "Episode:1814 meanR:10.0900 R:15.0 Steps:299 loss:0.0616\n",
      "Episode:1815 meanR:10.1500 R:11.0 Steps:299 loss:0.0372\n",
      "Episode:1816 meanR:10.1900 R:15.0 Steps:299 loss:0.0557\n",
      "Episode:1817 meanR:10.1300 R:9.0 Steps:299 loss:0.0684\n",
      "Episode:1818 meanR:10.1200 R:11.0 Steps:299 loss:0.0722\n",
      "Episode:1819 meanR:10.1800 R:17.0 Steps:299 loss:0.0558\n",
      "Episode:1820 meanR:10.1800 R:10.0 Steps:299 loss:0.0772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1821 meanR:10.1500 R:9.0 Steps:299 loss:0.0682\n",
      "Episode:1822 meanR:10.0800 R:10.0 Steps:299 loss:0.0599\n",
      "Episode:1823 meanR:10.1200 R:9.0 Steps:299 loss:0.0759\n",
      "Episode:1824 meanR:10.1300 R:8.0 Steps:299 loss:0.0500\n",
      "Episode:1825 meanR:10.0900 R:12.0 Steps:299 loss:0.0529\n",
      "Episode:1826 meanR:10.1300 R:6.0 Steps:299 loss:0.0582\n",
      "Episode:1827 meanR:10.0800 R:4.0 Steps:299 loss:0.0593\n",
      "Episode:1828 meanR:10.1400 R:15.0 Steps:299 loss:0.0418\n",
      "Episode:1829 meanR:10.0800 R:5.0 Steps:299 loss:0.0382\n",
      "Episode:1830 meanR:10.2100 R:15.0 Steps:299 loss:0.0562\n",
      "Episode:1831 meanR:10.2100 R:7.0 Steps:299 loss:0.0667\n",
      "Episode:1832 meanR:10.2600 R:13.0 Steps:299 loss:0.0429\n",
      "Episode:1833 meanR:10.1600 R:5.0 Steps:299 loss:0.0706\n",
      "Episode:1834 meanR:10.1200 R:4.0 Steps:299 loss:0.0303\n",
      "Episode:1835 meanR:10.0700 R:9.0 Steps:299 loss:0.0418\n",
      "Episode:1836 meanR:10.0600 R:14.0 Steps:299 loss:0.0566\n",
      "Episode:1837 meanR:10.0600 R:11.0 Steps:299 loss:0.0694\n",
      "Episode:1838 meanR:10.0700 R:9.0 Steps:299 loss:0.0484\n",
      "Episode:1839 meanR:10.1300 R:13.0 Steps:299 loss:0.0677\n",
      "Episode:1840 meanR:10.2200 R:17.0 Steps:299 loss:0.0657\n",
      "Episode:1841 meanR:10.1800 R:1.0 Steps:299 loss:0.0665\n",
      "Episode:1842 meanR:10.0900 R:4.0 Steps:299 loss:0.0428\n",
      "Episode:1843 meanR:10.1700 R:8.0 Steps:299 loss:0.0706\n",
      "Episode:1844 meanR:10.2300 R:12.0 Steps:299 loss:0.0508\n",
      "Episode:1845 meanR:10.2200 R:10.0 Steps:299 loss:0.0676\n",
      "Episode:1846 meanR:10.2800 R:13.0 Steps:299 loss:0.0601\n",
      "Episode:1847 meanR:10.3200 R:14.0 Steps:299 loss:0.0925\n",
      "Episode:1848 meanR:10.3200 R:12.0 Steps:299 loss:0.0654\n",
      "Episode:1849 meanR:10.3200 R:9.0 Steps:299 loss:0.0885\n",
      "Episode:1850 meanR:10.3900 R:7.0 Steps:299 loss:0.0491\n",
      "Episode:1851 meanR:10.3700 R:11.0 Steps:299 loss:0.0668\n",
      "Episode:1852 meanR:10.4100 R:10.0 Steps:299 loss:0.0828\n",
      "Episode:1853 meanR:10.4700 R:13.0 Steps:299 loss:0.0575\n",
      "Episode:1854 meanR:10.4800 R:10.0 Steps:299 loss:0.0767\n",
      "Episode:1855 meanR:10.4000 R:5.0 Steps:299 loss:0.0493\n",
      "Episode:1856 meanR:10.3400 R:5.0 Steps:299 loss:0.0600\n",
      "Episode:1857 meanR:10.3400 R:7.0 Steps:299 loss:0.0426\n",
      "Episode:1858 meanR:10.3600 R:12.0 Steps:299 loss:0.0672\n",
      "Episode:1859 meanR:10.3900 R:8.0 Steps:299 loss:0.0704\n",
      "Episode:1860 meanR:10.3600 R:2.0 Steps:299 loss:0.0308\n",
      "Episode:1861 meanR:10.3300 R:4.0 Steps:299 loss:0.0548\n",
      "Episode:1862 meanR:10.3000 R:4.0 Steps:299 loss:0.0305\n",
      "Episode:1863 meanR:10.2900 R:11.0 Steps:299 loss:0.0611\n",
      "Episode:1864 meanR:10.3100 R:12.0 Steps:299 loss:0.0664\n",
      "Episode:1865 meanR:10.3700 R:15.0 Steps:299 loss:0.0902\n",
      "Episode:1866 meanR:10.3100 R:9.0 Steps:299 loss:0.0705\n",
      "Episode:1867 meanR:10.2000 R:8.0 Steps:299 loss:0.0884\n",
      "Episode:1868 meanR:10.2000 R:7.0 Steps:299 loss:0.0916\n",
      "Episode:1869 meanR:10.1300 R:13.0 Steps:299 loss:0.0732\n",
      "Episode:1870 meanR:10.1300 R:14.0 Steps:299 loss:0.0873\n",
      "Episode:1871 meanR:10.0800 R:7.0 Steps:299 loss:0.0819\n",
      "Episode:1872 meanR:10.0300 R:8.0 Steps:299 loss:0.0325\n",
      "Episode:1873 meanR:9.9400 R:6.0 Steps:299 loss:0.0573\n",
      "Episode:1874 meanR:9.9000 R:6.0 Steps:299 loss:0.0576\n",
      "Episode:1875 meanR:9.9100 R:13.0 Steps:299 loss:0.0719\n",
      "Episode:1876 meanR:9.9500 R:14.0 Steps:299 loss:0.0748\n",
      "Episode:1877 meanR:10.0000 R:12.0 Steps:299 loss:0.0782\n",
      "Episode:1878 meanR:10.0300 R:11.0 Steps:299 loss:0.0459\n",
      "Episode:1879 meanR:10.0100 R:10.0 Steps:299 loss:0.0743\n",
      "Episode:1880 meanR:10.0300 R:10.0 Steps:299 loss:0.0565\n",
      "Episode:1881 meanR:10.0100 R:11.0 Steps:299 loss:0.0672\n",
      "Episode:1882 meanR:9.9200 R:6.0 Steps:299 loss:0.0449\n",
      "Episode:1883 meanR:9.9300 R:11.0 Steps:299 loss:0.0688\n",
      "Episode:1884 meanR:9.8600 R:8.0 Steps:299 loss:0.0441\n",
      "Episode:1885 meanR:9.8300 R:6.0 Steps:299 loss:0.0464\n",
      "Episode:1886 meanR:9.8000 R:8.0 Steps:299 loss:0.0663\n",
      "Episode:1887 meanR:9.7300 R:9.0 Steps:299 loss:0.0513\n",
      "Episode:1888 meanR:9.7500 R:7.0 Steps:299 loss:0.0589\n",
      "Episode:1889 meanR:9.8200 R:9.0 Steps:299 loss:0.0502\n",
      "Episode:1890 meanR:9.9100 R:15.0 Steps:299 loss:0.0636\n",
      "Episode:1891 meanR:9.9900 R:15.0 Steps:299 loss:0.0714\n",
      "Episode:1892 meanR:10.0100 R:19.0 Steps:299 loss:0.0743\n",
      "Episode:1893 meanR:9.9800 R:9.0 Steps:299 loss:0.0903\n",
      "Episode:1894 meanR:9.9400 R:6.0 Steps:299 loss:0.0547\n",
      "Episode:1895 meanR:9.9700 R:11.0 Steps:299 loss:0.0678\n",
      "Episode:1896 meanR:10.0900 R:19.0 Steps:299 loss:0.0676\n",
      "Episode:1897 meanR:10.0500 R:4.0 Steps:299 loss:0.0711\n",
      "Episode:1898 meanR:10.0400 R:9.0 Steps:299 loss:0.0573\n",
      "Episode:1899 meanR:9.9700 R:5.0 Steps:299 loss:0.0762\n",
      "Episode:1900 meanR:9.9000 R:12.0 Steps:299 loss:0.0578\n",
      "Episode:1901 meanR:9.8500 R:9.0 Steps:299 loss:0.0554\n",
      "Episode:1902 meanR:9.7800 R:7.0 Steps:299 loss:0.0587\n",
      "Episode:1903 meanR:9.7000 R:4.0 Steps:299 loss:0.0543\n",
      "Episode:1904 meanR:9.7000 R:10.0 Steps:299 loss:0.0395\n",
      "Episode:1905 meanR:9.8100 R:15.0 Steps:299 loss:0.0562\n",
      "Episode:1906 meanR:9.7600 R:13.0 Steps:299 loss:0.0700\n",
      "Episode:1907 meanR:9.7300 R:15.0 Steps:299 loss:0.0721\n",
      "Episode:1908 meanR:9.8000 R:15.0 Steps:299 loss:0.0793\n",
      "Episode:1909 meanR:9.9000 R:13.0 Steps:299 loss:0.0614\n",
      "Episode:1910 meanR:9.8700 R:9.0 Steps:299 loss:0.0704\n",
      "Episode:1911 meanR:9.8200 R:6.0 Steps:299 loss:0.0478\n",
      "Episode:1912 meanR:9.8500 R:9.0 Steps:299 loss:0.0766\n",
      "Episode:1913 meanR:9.8000 R:6.0 Steps:299 loss:0.0358\n",
      "Episode:1914 meanR:9.7100 R:6.0 Steps:299 loss:0.0333\n",
      "Episode:1915 meanR:9.6700 R:7.0 Steps:299 loss:0.0612\n",
      "Episode:1916 meanR:9.6300 R:11.0 Steps:299 loss:0.0517\n",
      "Episode:1917 meanR:9.5800 R:4.0 Steps:299 loss:0.0433\n",
      "Episode:1918 meanR:9.5900 R:12.0 Steps:299 loss:0.0492\n",
      "Episode:1919 meanR:9.5600 R:14.0 Steps:299 loss:0.0621\n",
      "Episode:1920 meanR:9.6300 R:17.0 Steps:299 loss:0.0665\n",
      "Episode:1921 meanR:9.6600 R:12.0 Steps:299 loss:0.0664\n",
      "Episode:1922 meanR:9.6900 R:13.0 Steps:299 loss:0.0507\n",
      "Episode:1923 meanR:9.7100 R:11.0 Steps:299 loss:0.0688\n",
      "Episode:1924 meanR:9.6500 R:2.0 Steps:299 loss:0.0447\n",
      "Episode:1925 meanR:9.5600 R:3.0 Steps:299 loss:0.0476\n",
      "Episode:1926 meanR:9.6200 R:12.0 Steps:299 loss:0.0416\n",
      "Episode:1927 meanR:9.7200 R:14.0 Steps:299 loss:0.0515\n",
      "Episode:1928 meanR:9.6200 R:5.0 Steps:299 loss:0.0611\n",
      "Episode:1929 meanR:9.6200 R:5.0 Steps:299 loss:0.0604\n",
      "Episode:1930 meanR:9.5300 R:6.0 Steps:299 loss:0.0363\n",
      "Episode:1931 meanR:9.5200 R:6.0 Steps:299 loss:0.0253\n",
      "Episode:1932 meanR:9.5000 R:11.0 Steps:299 loss:0.0569\n",
      "Episode:1933 meanR:9.5600 R:11.0 Steps:299 loss:0.0774\n",
      "Episode:1934 meanR:9.5700 R:5.0 Steps:299 loss:0.0764\n",
      "Episode:1935 meanR:9.5400 R:6.0 Steps:299 loss:0.0596\n",
      "Episode:1936 meanR:9.4500 R:5.0 Steps:299 loss:0.0494\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(11111111111):\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append(initial_state)\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            #batch, rnn_states = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            initial_states = memory.states\n",
    "            next_actions_logits = sess.run(model.actions_logits,\n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: initial_states[1]})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs,\n",
    "                                                                     model.initial_state: initial_states[0]})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'Steps:{}'.format(num_steps),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 2.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
