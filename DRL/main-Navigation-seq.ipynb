{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "num_steps = 0\n",
    "while True:\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "num_steps = 0\n",
    "while True: # infinite number of steps\n",
    "    num_steps += 1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.        , 0.        , 1.        , 0.        , 0.04087751,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.04246911,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.27552733,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.05028759,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.05254348,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.05888711,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.03669675,\n",
       "         0.        , 0.        ]),\n",
       "  0,\n",
       "  array([0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         3.64827104e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 4.61156890e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.66352813e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         4.72340956e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 5.99141009e-02, 2.38418579e-07,\n",
       "         7.81049442e+00]),\n",
       "  0.0,\n",
       "  0.0],\n",
       " 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 1.        , 0.        , 0.04087751,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.04246911,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.27552733,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.05028759,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.05254348,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.05888711,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.03669675,\n",
       "        0.        , 0.        ]),\n",
       " 0,\n",
       " array([0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        3.64827104e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 4.61156890e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.66352813e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        4.72340956e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 5.99141009e-02, 2.38418579e-07,\n",
       "        7.81049442e+00]),\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[1] for each in batch])\n",
    "actions = np.array([each[0] for each in batch])\n",
    "next_states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 37) (300,) (300, 37) (300,)\n",
      "float64 int64 float64 float64\n",
      "10.520530700683594 -10.738459587097168 22.25899028778076\n",
      "10.520530700683594 -10.738459587097168\n",
      "3 0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return states, actions, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "action_size = 4\n",
    "state_size = 37\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 300            # memory capacity\n",
    "batch_size = 300             # experience mini-batch size\n",
    "gamma = 0.99                 # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37) (?, 74)\n",
      "(1, ?, 74) (1, 74)\n",
      "(1, ?, 74) (1, 74)\n",
      "(?, 74)\n",
      "(?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    memory.states.append(np.zeros([1, hidden_size])) # initial_states for rnn/mem\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 74)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial_states = memory.states\n",
    "memory.states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.0 loss:0.6707\n",
      "Episode:1 meanR:0.5000 R:1.0 loss:0.0315\n",
      "Episode:2 meanR:0.0000 R:-1.0 loss:0.0442\n",
      "Episode:3 meanR:1.2500 R:5.0 loss:0.0304\n",
      "Episode:4 meanR:1.4000 R:2.0 loss:0.0518\n",
      "Episode:5 meanR:1.0000 R:-1.0 loss:0.0390\n",
      "Episode:6 meanR:0.8571 R:0.0 loss:0.0241\n",
      "Episode:7 meanR:1.0000 R:2.0 loss:0.0217\n",
      "Episode:8 meanR:1.0000 R:1.0 loss:0.0270\n",
      "Episode:9 meanR:1.0000 R:1.0 loss:0.0183\n",
      "Episode:10 meanR:0.9091 R:0.0 loss:0.0253\n",
      "Episode:11 meanR:0.7500 R:-1.0 loss:0.0221\n",
      "Episode:12 meanR:0.5385 R:-2.0 loss:0.0151\n",
      "Episode:13 meanR:0.5714 R:1.0 loss:0.0109\n",
      "Episode:14 meanR:0.5333 R:0.0 loss:0.0267\n",
      "Episode:15 meanR:0.5625 R:1.0 loss:0.0330\n",
      "Episode:16 meanR:0.5294 R:0.0 loss:0.0284\n",
      "Episode:17 meanR:0.5000 R:0.0 loss:0.0123\n",
      "Episode:18 meanR:0.4737 R:0.0 loss:0.0139\n",
      "Episode:19 meanR:0.4500 R:0.0 loss:0.0252\n",
      "Episode:20 meanR:0.5238 R:2.0 loss:0.0133\n",
      "Episode:21 meanR:0.5000 R:0.0 loss:0.0200\n",
      "Episode:22 meanR:0.4783 R:0.0 loss:0.0408\n",
      "Episode:23 meanR:0.4583 R:0.0 loss:0.0117\n",
      "Episode:24 meanR:0.4400 R:0.0 loss:0.0318\n",
      "Episode:25 meanR:0.3846 R:-1.0 loss:0.0487\n",
      "Episode:26 meanR:0.4074 R:1.0 loss:0.0439\n",
      "Episode:27 meanR:0.5357 R:4.0 loss:0.0813\n",
      "Episode:28 meanR:0.6207 R:3.0 loss:0.0533\n",
      "Episode:29 meanR:0.7000 R:3.0 loss:0.0377\n",
      "Episode:30 meanR:0.6774 R:0.0 loss:0.0285\n",
      "Episode:31 meanR:0.8438 R:6.0 loss:0.0561\n",
      "Episode:32 meanR:1.0303 R:7.0 loss:0.0424\n",
      "Episode:33 meanR:1.2647 R:9.0 loss:0.1260\n",
      "Episode:34 meanR:1.3714 R:5.0 loss:0.0489\n",
      "Episode:35 meanR:1.4167 R:3.0 loss:0.0418\n",
      "Episode:36 meanR:1.4054 R:1.0 loss:0.0291\n",
      "Episode:37 meanR:1.3421 R:-1.0 loss:0.0951\n",
      "Episode:38 meanR:1.3846 R:3.0 loss:0.0559\n",
      "Episode:39 meanR:1.5250 R:7.0 loss:0.0431\n",
      "Episode:40 meanR:1.6829 R:8.0 loss:0.0559\n",
      "Episode:41 meanR:1.9048 R:11.0 loss:0.0733\n",
      "Episode:42 meanR:1.9767 R:5.0 loss:0.0507\n",
      "Episode:43 meanR:1.9773 R:2.0 loss:0.0595\n",
      "Episode:44 meanR:2.0667 R:6.0 loss:0.0682\n",
      "Episode:45 meanR:2.1087 R:4.0 loss:0.0357\n",
      "Episode:46 meanR:2.1915 R:6.0 loss:0.0448\n",
      "Episode:47 meanR:2.2917 R:7.0 loss:0.0607\n",
      "Episode:48 meanR:2.3265 R:4.0 loss:0.0338\n",
      "Episode:49 meanR:2.5400 R:13.0 loss:0.0577\n",
      "Episode:50 meanR:2.5882 R:5.0 loss:0.0810\n",
      "Episode:51 meanR:2.6731 R:7.0 loss:0.1136\n",
      "Episode:52 meanR:2.6415 R:1.0 loss:0.0427\n",
      "Episode:53 meanR:2.7963 R:11.0 loss:0.0535\n",
      "Episode:54 meanR:2.9273 R:10.0 loss:0.0410\n",
      "Episode:55 meanR:2.8750 R:0.0 loss:0.0345\n",
      "Episode:56 meanR:3.0351 R:12.0 loss:0.0644\n",
      "Episode:57 meanR:3.1207 R:8.0 loss:0.0676\n",
      "Episode:58 meanR:3.1525 R:5.0 loss:0.0504\n",
      "Episode:59 meanR:3.2333 R:8.0 loss:0.1043\n",
      "Episode:60 meanR:3.3115 R:8.0 loss:0.0866\n",
      "Episode:61 meanR:3.3710 R:7.0 loss:0.0987\n",
      "Episode:62 meanR:3.4127 R:6.0 loss:0.0644\n",
      "Episode:63 meanR:3.4531 R:6.0 loss:0.0764\n",
      "Episode:64 meanR:3.4000 R:0.0 loss:0.0425\n",
      "Episode:65 meanR:3.5000 R:10.0 loss:0.1198\n",
      "Episode:66 meanR:3.5522 R:7.0 loss:0.0565\n",
      "Episode:67 meanR:3.6176 R:8.0 loss:0.0773\n",
      "Episode:68 meanR:3.6667 R:7.0 loss:0.0662\n",
      "Episode:69 meanR:3.6857 R:5.0 loss:0.0661\n",
      "Episode:70 meanR:3.7042 R:5.0 loss:0.0403\n",
      "Episode:71 meanR:3.6944 R:3.0 loss:0.0395\n",
      "Episode:72 meanR:3.6849 R:3.0 loss:0.0447\n",
      "Episode:73 meanR:3.7027 R:5.0 loss:0.1295\n",
      "Episode:74 meanR:3.7333 R:6.0 loss:0.0687\n",
      "Episode:75 meanR:3.8026 R:9.0 loss:0.0425\n",
      "Episode:76 meanR:3.8182 R:5.0 loss:0.0374\n",
      "Episode:77 meanR:3.8590 R:7.0 loss:0.0696\n",
      "Episode:78 meanR:3.8987 R:7.0 loss:0.0452\n",
      "Episode:79 meanR:3.8500 R:0.0 loss:0.0403\n",
      "Episode:80 meanR:3.8395 R:3.0 loss:0.0338\n",
      "Episode:81 meanR:3.8293 R:3.0 loss:0.0431\n",
      "Episode:82 meanR:3.8072 R:2.0 loss:0.0322\n",
      "Episode:83 meanR:3.7619 R:0.0 loss:0.0409\n",
      "Episode:84 meanR:3.7647 R:4.0 loss:0.0451\n",
      "Episode:85 meanR:3.7674 R:4.0 loss:0.0758\n",
      "Episode:86 meanR:3.7241 R:0.0 loss:0.0571\n",
      "Episode:87 meanR:3.7159 R:3.0 loss:0.0199\n",
      "Episode:88 meanR:3.6854 R:1.0 loss:0.0447\n",
      "Episode:89 meanR:3.6556 R:1.0 loss:0.0962\n",
      "Episode:90 meanR:3.6154 R:0.0 loss:0.0216\n",
      "Episode:91 meanR:3.6087 R:3.0 loss:0.1521\n",
      "Episode:92 meanR:3.5699 R:0.0 loss:0.0214\n",
      "Episode:93 meanR:3.5319 R:0.0 loss:0.0281\n",
      "Episode:94 meanR:3.4947 R:0.0 loss:0.0159\n",
      "Episode:95 meanR:3.4688 R:1.0 loss:0.0460\n",
      "Episode:96 meanR:3.4536 R:2.0 loss:0.0484\n",
      "Episode:97 meanR:3.4490 R:3.0 loss:0.0597\n",
      "Episode:98 meanR:3.4242 R:1.0 loss:0.0388\n",
      "Episode:99 meanR:3.4000 R:1.0 loss:0.0265\n",
      "Episode:100 meanR:3.4500 R:5.0 loss:0.0272\n",
      "Episode:101 meanR:3.5100 R:7.0 loss:0.0670\n",
      "Episode:102 meanR:3.5600 R:4.0 loss:0.0708\n",
      "Episode:103 meanR:3.5200 R:1.0 loss:0.0238\n",
      "Episode:104 meanR:3.5200 R:2.0 loss:0.0410\n",
      "Episode:105 meanR:3.5400 R:1.0 loss:0.0310\n",
      "Episode:106 meanR:3.5400 R:0.0 loss:0.0729\n",
      "Episode:107 meanR:3.5200 R:0.0 loss:0.0251\n",
      "Episode:108 meanR:3.5200 R:1.0 loss:0.0437\n",
      "Episode:109 meanR:3.5100 R:0.0 loss:0.0413\n",
      "Episode:110 meanR:3.5000 R:-1.0 loss:0.0711\n",
      "Episode:111 meanR:3.5200 R:1.0 loss:0.0824\n",
      "Episode:112 meanR:3.5400 R:0.0 loss:0.0769\n",
      "Episode:113 meanR:3.5400 R:1.0 loss:0.0371\n",
      "Episode:114 meanR:3.5500 R:1.0 loss:0.0480\n",
      "Episode:115 meanR:3.5100 R:-3.0 loss:0.0254\n",
      "Episode:116 meanR:3.5100 R:0.0 loss:0.0218\n",
      "Episode:117 meanR:3.5100 R:0.0 loss:0.0091\n",
      "Episode:118 meanR:3.5400 R:3.0 loss:0.0206\n",
      "Episode:119 meanR:3.5400 R:0.0 loss:0.0220\n",
      "Episode:120 meanR:3.5200 R:0.0 loss:0.0166\n",
      "Episode:121 meanR:3.5300 R:1.0 loss:0.0567\n",
      "Episode:122 meanR:3.5200 R:-1.0 loss:0.0742\n",
      "Episode:123 meanR:3.5200 R:0.0 loss:0.0358\n",
      "Episode:124 meanR:3.5100 R:-1.0 loss:0.0322\n",
      "Episode:125 meanR:3.5500 R:3.0 loss:0.0228\n",
      "Episode:126 meanR:3.5300 R:-1.0 loss:0.0339\n",
      "Episode:127 meanR:3.5000 R:1.0 loss:0.0285\n",
      "Episode:128 meanR:3.4700 R:0.0 loss:0.0554\n",
      "Episode:129 meanR:3.4600 R:2.0 loss:0.0577\n",
      "Episode:130 meanR:3.4800 R:2.0 loss:0.0684\n",
      "Episode:131 meanR:3.4300 R:1.0 loss:0.0643\n",
      "Episode:132 meanR:3.3900 R:3.0 loss:0.0311\n",
      "Episode:133 meanR:3.3400 R:4.0 loss:0.0435\n",
      "Episode:134 meanR:3.2900 R:0.0 loss:0.0461\n",
      "Episode:135 meanR:3.2900 R:3.0 loss:0.0572\n",
      "Episode:136 meanR:3.2700 R:-1.0 loss:0.0207\n",
      "Episode:137 meanR:3.3000 R:2.0 loss:0.0518\n",
      "Episode:138 meanR:3.2800 R:1.0 loss:0.0254\n",
      "Episode:139 meanR:3.2000 R:-1.0 loss:0.0341\n",
      "Episode:140 meanR:3.1900 R:7.0 loss:0.0449\n",
      "Episode:141 meanR:3.1200 R:4.0 loss:0.1421\n",
      "Episode:142 meanR:3.1200 R:5.0 loss:0.0884\n",
      "Episode:143 meanR:3.1400 R:4.0 loss:0.0568\n",
      "Episode:144 meanR:3.1300 R:5.0 loss:0.0717\n",
      "Episode:145 meanR:3.1700 R:8.0 loss:0.0491\n",
      "Episode:146 meanR:3.1500 R:4.0 loss:0.0501\n",
      "Episode:147 meanR:3.1000 R:2.0 loss:0.0346\n",
      "Episode:148 meanR:3.0700 R:1.0 loss:0.0262\n",
      "Episode:149 meanR:2.9700 R:3.0 loss:0.0288\n",
      "Episode:150 meanR:2.9200 R:0.0 loss:0.0397\n",
      "Episode:151 meanR:2.8700 R:2.0 loss:0.0414\n",
      "Episode:152 meanR:2.8700 R:1.0 loss:0.0346\n",
      "Episode:153 meanR:2.7600 R:0.0 loss:0.0339\n",
      "Episode:154 meanR:2.6600 R:0.0 loss:0.0367\n",
      "Episode:155 meanR:2.6600 R:0.0 loss:0.0256\n",
      "Episode:156 meanR:2.5400 R:0.0 loss:0.0344\n",
      "Episode:157 meanR:2.4500 R:-1.0 loss:0.0859\n",
      "Episode:158 meanR:2.4100 R:1.0 loss:0.0484\n",
      "Episode:159 meanR:2.3200 R:-1.0 loss:0.0314\n",
      "Episode:160 meanR:2.3000 R:6.0 loss:0.0400\n",
      "Episode:161 meanR:2.3000 R:7.0 loss:0.0397\n",
      "Episode:162 meanR:2.3200 R:8.0 loss:0.0643\n",
      "Episode:163 meanR:2.3200 R:6.0 loss:0.0667\n",
      "Episode:164 meanR:2.4300 R:11.0 loss:0.0612\n",
      "Episode:165 meanR:2.4000 R:7.0 loss:0.1124\n",
      "Episode:166 meanR:2.3600 R:3.0 loss:0.0673\n",
      "Episode:167 meanR:2.3300 R:5.0 loss:0.1314\n",
      "Episode:168 meanR:2.3200 R:6.0 loss:0.0592\n",
      "Episode:169 meanR:2.3500 R:8.0 loss:0.0657\n",
      "Episode:170 meanR:2.3800 R:8.0 loss:0.0441\n",
      "Episode:171 meanR:2.4000 R:5.0 loss:0.0579\n",
      "Episode:172 meanR:2.4300 R:6.0 loss:0.0413\n",
      "Episode:173 meanR:2.4200 R:4.0 loss:0.0685\n",
      "Episode:174 meanR:2.4400 R:8.0 loss:0.0879\n",
      "Episode:175 meanR:2.4400 R:9.0 loss:0.0574\n",
      "Episode:176 meanR:2.4100 R:2.0 loss:0.0690\n",
      "Episode:177 meanR:2.4100 R:7.0 loss:0.0379\n",
      "Episode:178 meanR:2.3900 R:5.0 loss:0.0525\n",
      "Episode:179 meanR:2.4400 R:5.0 loss:0.0454\n",
      "Episode:180 meanR:2.5200 R:11.0 loss:0.1011\n",
      "Episode:181 meanR:2.5900 R:10.0 loss:0.0621\n",
      "Episode:182 meanR:2.6300 R:6.0 loss:0.1032\n",
      "Episode:183 meanR:2.6500 R:2.0 loss:0.0842\n",
      "Episode:184 meanR:2.6200 R:1.0 loss:0.0618\n",
      "Episode:185 meanR:2.6300 R:5.0 loss:0.1258\n",
      "Episode:186 meanR:2.6600 R:3.0 loss:0.1089\n",
      "Episode:187 meanR:2.6500 R:2.0 loss:0.0838\n",
      "Episode:188 meanR:2.6600 R:2.0 loss:0.0490\n",
      "Episode:189 meanR:2.7000 R:5.0 loss:0.0523\n",
      "Episode:190 meanR:2.7300 R:3.0 loss:0.0847\n",
      "Episode:191 meanR:2.7600 R:6.0 loss:0.1009\n",
      "Episode:192 meanR:2.7600 R:0.0 loss:0.0487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:193 meanR:2.7800 R:2.0 loss:0.0581\n",
      "Episode:194 meanR:2.8600 R:8.0 loss:0.1225\n",
      "Episode:195 meanR:2.8800 R:3.0 loss:0.0765\n",
      "Episode:196 meanR:2.9300 R:7.0 loss:0.0931\n",
      "Episode:197 meanR:2.9200 R:2.0 loss:0.0874\n",
      "Episode:198 meanR:2.9400 R:3.0 loss:0.0605\n",
      "Episode:199 meanR:2.9800 R:5.0 loss:0.0381\n",
      "Episode:200 meanR:2.9900 R:6.0 loss:0.0862\n",
      "Episode:201 meanR:2.9800 R:6.0 loss:0.0796\n",
      "Episode:202 meanR:3.0400 R:10.0 loss:0.1346\n",
      "Episode:203 meanR:3.1200 R:9.0 loss:0.0862\n",
      "Episode:204 meanR:3.2300 R:13.0 loss:0.0832\n",
      "Episode:205 meanR:3.3000 R:8.0 loss:0.0829\n",
      "Episode:206 meanR:3.3800 R:8.0 loss:0.0723\n",
      "Episode:207 meanR:3.4600 R:8.0 loss:0.0811\n",
      "Episode:208 meanR:3.5300 R:8.0 loss:0.0558\n",
      "Episode:209 meanR:3.5900 R:6.0 loss:0.1203\n",
      "Episode:210 meanR:3.6700 R:7.0 loss:0.0433\n",
      "Episode:211 meanR:3.7200 R:6.0 loss:0.0582\n",
      "Episode:212 meanR:3.7600 R:4.0 loss:0.0473\n",
      "Episode:213 meanR:3.8700 R:12.0 loss:0.0469\n",
      "Episode:214 meanR:3.9300 R:7.0 loss:0.0427\n",
      "Episode:215 meanR:4.0000 R:4.0 loss:0.0457\n",
      "Episode:216 meanR:4.0700 R:7.0 loss:0.0256\n",
      "Episode:217 meanR:4.1900 R:12.0 loss:0.0456\n",
      "Episode:218 meanR:4.2700 R:11.0 loss:0.0704\n",
      "Episode:219 meanR:4.3200 R:5.0 loss:0.0534\n",
      "Episode:220 meanR:4.4000 R:8.0 loss:0.0396\n",
      "Episode:221 meanR:4.4300 R:4.0 loss:0.0332\n",
      "Episode:222 meanR:4.4900 R:5.0 loss:0.0710\n",
      "Episode:223 meanR:4.6000 R:11.0 loss:0.0325\n",
      "Episode:224 meanR:4.6600 R:5.0 loss:0.1125\n",
      "Episode:225 meanR:4.6800 R:5.0 loss:0.0778\n",
      "Episode:226 meanR:4.7600 R:7.0 loss:0.0840\n",
      "Episode:227 meanR:4.7600 R:1.0 loss:0.0526\n",
      "Episode:228 meanR:4.7900 R:3.0 loss:0.0470\n",
      "Episode:229 meanR:4.8400 R:7.0 loss:0.0343\n",
      "Episode:230 meanR:4.8400 R:2.0 loss:0.0401\n",
      "Episode:231 meanR:4.8600 R:3.0 loss:0.0607\n",
      "Episode:232 meanR:4.9600 R:13.0 loss:0.0750\n",
      "Episode:233 meanR:4.9800 R:6.0 loss:0.0436\n",
      "Episode:234 meanR:5.0600 R:8.0 loss:0.0456\n",
      "Episode:235 meanR:5.1200 R:9.0 loss:0.0776\n",
      "Episode:236 meanR:5.2400 R:11.0 loss:0.0590\n",
      "Episode:237 meanR:5.2800 R:6.0 loss:0.1528\n",
      "Episode:238 meanR:5.3000 R:3.0 loss:0.0567\n",
      "Episode:239 meanR:5.3300 R:2.0 loss:0.0952\n",
      "Episode:240 meanR:5.2900 R:3.0 loss:0.0461\n",
      "Episode:241 meanR:5.3200 R:7.0 loss:0.0388\n",
      "Episode:242 meanR:5.3100 R:4.0 loss:0.0863\n",
      "Episode:243 meanR:5.2800 R:1.0 loss:0.0448\n",
      "Episode:244 meanR:5.2700 R:4.0 loss:0.0589\n",
      "Episode:245 meanR:5.2200 R:3.0 loss:0.0855\n",
      "Episode:246 meanR:5.1900 R:1.0 loss:0.0674\n",
      "Episode:247 meanR:5.2400 R:7.0 loss:0.0638\n",
      "Episode:248 meanR:5.2600 R:3.0 loss:0.0459\n",
      "Episode:249 meanR:5.2600 R:3.0 loss:0.0870\n",
      "Episode:250 meanR:5.2900 R:3.0 loss:0.0734\n",
      "Episode:251 meanR:5.2900 R:2.0 loss:0.0780\n",
      "Episode:252 meanR:5.3300 R:5.0 loss:0.0577\n",
      "Episode:253 meanR:5.4100 R:8.0 loss:0.0527\n",
      "Episode:254 meanR:5.4400 R:3.0 loss:0.0912\n",
      "Episode:255 meanR:5.4400 R:0.0 loss:0.0227\n",
      "Episode:256 meanR:5.4800 R:4.0 loss:0.0508\n",
      "Episode:257 meanR:5.4800 R:-1.0 loss:0.0699\n",
      "Episode:258 meanR:5.5300 R:6.0 loss:0.1003\n",
      "Episode:259 meanR:5.5800 R:4.0 loss:0.0447\n",
      "Episode:260 meanR:5.6100 R:9.0 loss:0.0456\n",
      "Episode:261 meanR:5.5700 R:3.0 loss:0.0662\n",
      "Episode:262 meanR:5.5400 R:5.0 loss:0.0446\n",
      "Episode:263 meanR:5.5300 R:5.0 loss:0.0518\n",
      "Episode:264 meanR:5.5100 R:9.0 loss:0.0752\n",
      "Episode:265 meanR:5.5000 R:6.0 loss:0.0571\n",
      "Episode:266 meanR:5.5100 R:4.0 loss:0.0547\n",
      "Episode:267 meanR:5.5100 R:5.0 loss:0.0467\n",
      "Episode:268 meanR:5.5800 R:13.0 loss:0.0381\n",
      "Episode:269 meanR:5.5400 R:4.0 loss:0.0395\n",
      "Episode:270 meanR:5.4800 R:2.0 loss:0.0649\n",
      "Episode:271 meanR:5.4700 R:4.0 loss:0.0399\n",
      "Episode:272 meanR:5.4900 R:8.0 loss:0.0270\n",
      "Episode:273 meanR:5.4700 R:2.0 loss:0.0950\n",
      "Episode:274 meanR:5.4000 R:1.0 loss:0.0335\n",
      "Episode:275 meanR:5.3800 R:7.0 loss:0.1110\n",
      "Episode:276 meanR:5.3700 R:1.0 loss:0.0867\n",
      "Episode:277 meanR:5.3100 R:1.0 loss:0.0511\n",
      "Episode:278 meanR:5.2800 R:2.0 loss:0.0413\n",
      "Episode:279 meanR:5.2700 R:4.0 loss:0.0302\n",
      "Episode:280 meanR:5.1800 R:2.0 loss:0.0591\n",
      "Episode:281 meanR:5.1600 R:8.0 loss:0.0757\n",
      "Episode:282 meanR:5.1300 R:3.0 loss:0.0727\n",
      "Episode:283 meanR:5.1200 R:1.0 loss:0.1295\n",
      "Episode:284 meanR:5.1100 R:0.0 loss:0.0408\n",
      "Episode:285 meanR:5.0600 R:0.0 loss:0.0362\n",
      "Episode:286 meanR:5.0400 R:1.0 loss:0.0380\n",
      "Episode:287 meanR:5.0200 R:0.0 loss:0.0828\n",
      "Episode:288 meanR:5.0500 R:5.0 loss:0.0961\n",
      "Episode:289 meanR:4.9900 R:-1.0 loss:0.0472\n",
      "Episode:290 meanR:5.0200 R:6.0 loss:0.1201\n",
      "Episode:291 meanR:5.0000 R:4.0 loss:0.0450\n",
      "Episode:292 meanR:5.0300 R:3.0 loss:0.1078\n",
      "Episode:293 meanR:5.0100 R:0.0 loss:0.0513\n",
      "Episode:294 meanR:4.9300 R:0.0 loss:0.0409\n",
      "Episode:295 meanR:4.9400 R:4.0 loss:0.1007\n",
      "Episode:296 meanR:4.9200 R:5.0 loss:0.0503\n",
      "Episode:297 meanR:4.9300 R:3.0 loss:0.0524\n",
      "Episode:298 meanR:4.9600 R:6.0 loss:0.0832\n",
      "Episode:299 meanR:4.9300 R:2.0 loss:0.0491\n",
      "Episode:300 meanR:4.9000 R:3.0 loss:0.0420\n",
      "Episode:301 meanR:4.8700 R:3.0 loss:0.0319\n",
      "Episode:302 meanR:4.8100 R:4.0 loss:0.0355\n",
      "Episode:303 meanR:4.7400 R:2.0 loss:0.0393\n",
      "Episode:304 meanR:4.6000 R:-1.0 loss:0.0481\n",
      "Episode:305 meanR:4.5600 R:4.0 loss:0.0455\n",
      "Episode:306 meanR:4.4800 R:0.0 loss:0.0461\n",
      "Episode:307 meanR:4.4200 R:2.0 loss:0.0293\n",
      "Episode:308 meanR:4.3300 R:-1.0 loss:0.0278\n",
      "Episode:309 meanR:4.2700 R:0.0 loss:0.0132\n",
      "Episode:310 meanR:4.2100 R:1.0 loss:0.0458\n",
      "Episode:311 meanR:4.1900 R:4.0 loss:0.0267\n",
      "Episode:312 meanR:4.1600 R:1.0 loss:0.0284\n",
      "Episode:313 meanR:4.0300 R:-1.0 loss:0.0610\n",
      "Episode:314 meanR:3.9900 R:3.0 loss:0.0343\n",
      "Episode:315 meanR:3.9700 R:2.0 loss:0.0327\n",
      "Episode:316 meanR:3.9100 R:1.0 loss:0.0643\n",
      "Episode:317 meanR:3.7900 R:0.0 loss:0.0588\n",
      "Episode:318 meanR:3.6800 R:0.0 loss:0.0542\n",
      "Episode:319 meanR:3.6700 R:4.0 loss:0.0261\n",
      "Episode:320 meanR:3.6300 R:4.0 loss:0.0306\n",
      "Episode:321 meanR:3.6100 R:2.0 loss:0.0271\n",
      "Episode:322 meanR:3.6000 R:4.0 loss:0.0331\n",
      "Episode:323 meanR:3.5300 R:4.0 loss:0.0552\n",
      "Episode:324 meanR:3.4600 R:-2.0 loss:0.0504\n",
      "Episode:325 meanR:3.4500 R:4.0 loss:0.0525\n",
      "Episode:326 meanR:3.3600 R:-2.0 loss:0.0418\n",
      "Episode:327 meanR:3.4000 R:5.0 loss:0.0763\n",
      "Episode:328 meanR:3.3500 R:-2.0 loss:0.0340\n",
      "Episode:329 meanR:3.3200 R:4.0 loss:0.0474\n",
      "Episode:330 meanR:3.3500 R:5.0 loss:0.0737\n",
      "Episode:331 meanR:3.3800 R:6.0 loss:0.0487\n",
      "Episode:332 meanR:3.2800 R:3.0 loss:0.0552\n",
      "Episode:333 meanR:3.2200 R:0.0 loss:0.0363\n",
      "Episode:334 meanR:3.1700 R:3.0 loss:0.0843\n",
      "Episode:335 meanR:3.1300 R:5.0 loss:0.0415\n",
      "Episode:336 meanR:3.1200 R:10.0 loss:0.0597\n",
      "Episode:337 meanR:3.1100 R:5.0 loss:0.1067\n",
      "Episode:338 meanR:3.1400 R:6.0 loss:0.0644\n",
      "Episode:339 meanR:3.2200 R:10.0 loss:0.1235\n",
      "Episode:340 meanR:3.2200 R:3.0 loss:0.0605\n",
      "Episode:341 meanR:3.1500 R:0.0 loss:0.0401\n",
      "Episode:342 meanR:3.1600 R:5.0 loss:0.0413\n",
      "Episode:343 meanR:3.2100 R:6.0 loss:0.1535\n",
      "Episode:344 meanR:3.2500 R:8.0 loss:0.0661\n",
      "Episode:345 meanR:3.2400 R:2.0 loss:0.0430\n",
      "Episode:346 meanR:3.2300 R:0.0 loss:0.0410\n",
      "Episode:347 meanR:3.1600 R:0.0 loss:0.0704\n",
      "Episode:348 meanR:3.1400 R:1.0 loss:0.0422\n",
      "Episode:349 meanR:3.1300 R:2.0 loss:0.0444\n",
      "Episode:350 meanR:3.1200 R:2.0 loss:0.0918\n",
      "Episode:351 meanR:3.1000 R:0.0 loss:0.0355\n",
      "Episode:352 meanR:3.0500 R:0.0 loss:0.0401\n",
      "Episode:353 meanR:2.9600 R:-1.0 loss:0.0647\n",
      "Episode:354 meanR:2.9300 R:0.0 loss:0.0577\n",
      "Episode:355 meanR:2.9300 R:0.0 loss:0.0371\n",
      "Episode:356 meanR:2.9200 R:3.0 loss:0.0426\n",
      "Episode:357 meanR:2.9500 R:2.0 loss:0.0275\n",
      "Episode:358 meanR:2.9200 R:3.0 loss:0.0357\n",
      "Episode:359 meanR:2.9400 R:6.0 loss:0.0339\n",
      "Episode:360 meanR:2.9000 R:5.0 loss:0.0522\n",
      "Episode:361 meanR:2.9100 R:4.0 loss:0.0454\n",
      "Episode:362 meanR:2.8600 R:0.0 loss:0.0409\n",
      "Episode:363 meanR:2.8100 R:0.0 loss:0.0293\n",
      "Episode:364 meanR:2.7300 R:1.0 loss:0.0643\n",
      "Episode:365 meanR:2.6700 R:0.0 loss:0.0665\n",
      "Episode:366 meanR:2.6500 R:2.0 loss:0.0654\n",
      "Episode:367 meanR:2.6000 R:0.0 loss:0.0401\n",
      "Episode:368 meanR:2.4700 R:0.0 loss:0.0741\n",
      "Episode:369 meanR:2.4300 R:0.0 loss:0.0276\n",
      "Episode:370 meanR:2.3900 R:-2.0 loss:0.0355\n",
      "Episode:371 meanR:2.3500 R:0.0 loss:0.0579\n",
      "Episode:372 meanR:2.3000 R:3.0 loss:0.0245\n",
      "Episode:373 meanR:2.2700 R:-1.0 loss:0.0388\n",
      "Episode:374 meanR:2.3500 R:9.0 loss:0.0272\n",
      "Episode:375 meanR:2.3000 R:2.0 loss:0.0583\n",
      "Episode:376 meanR:2.3200 R:3.0 loss:0.0451\n",
      "Episode:377 meanR:2.2900 R:-2.0 loss:0.0810\n",
      "Episode:378 meanR:2.3100 R:4.0 loss:0.0421\n",
      "Episode:379 meanR:2.3100 R:4.0 loss:0.0271\n",
      "Episode:380 meanR:2.3200 R:3.0 loss:0.0359\n",
      "Episode:381 meanR:2.2600 R:2.0 loss:0.0826\n",
      "Episode:382 meanR:2.2700 R:4.0 loss:0.0817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:383 meanR:2.2900 R:3.0 loss:0.0610\n",
      "Episode:384 meanR:2.3600 R:7.0 loss:0.0345\n",
      "Episode:385 meanR:2.4300 R:7.0 loss:0.0481\n",
      "Episode:386 meanR:2.4500 R:3.0 loss:0.0377\n",
      "Episode:387 meanR:2.4800 R:3.0 loss:0.0471\n",
      "Episode:388 meanR:2.4500 R:2.0 loss:0.0444\n",
      "Episode:389 meanR:2.4900 R:3.0 loss:0.0358\n",
      "Episode:390 meanR:2.4600 R:3.0 loss:0.0400\n",
      "Episode:391 meanR:2.4600 R:4.0 loss:0.0757\n",
      "Episode:392 meanR:2.4300 R:0.0 loss:0.0277\n",
      "Episode:393 meanR:2.4300 R:0.0 loss:0.0239\n",
      "Episode:394 meanR:2.4400 R:1.0 loss:0.0380\n",
      "Episode:395 meanR:2.4000 R:0.0 loss:0.0379\n",
      "Episode:396 meanR:2.3600 R:1.0 loss:0.0262\n",
      "Episode:397 meanR:2.3300 R:0.0 loss:0.0465\n",
      "Episode:398 meanR:2.2500 R:-2.0 loss:0.0396\n",
      "Episode:399 meanR:2.2400 R:1.0 loss:0.0295\n",
      "Episode:400 meanR:2.2100 R:0.0 loss:0.0397\n",
      "Episode:401 meanR:2.1800 R:0.0 loss:0.0388\n",
      "Episode:402 meanR:2.1500 R:1.0 loss:0.0322\n",
      "Episode:403 meanR:2.1300 R:0.0 loss:0.0472\n",
      "Episode:404 meanR:2.1500 R:1.0 loss:0.0432\n",
      "Episode:405 meanR:2.1300 R:2.0 loss:0.0308\n",
      "Episode:406 meanR:2.1200 R:-1.0 loss:0.0384\n",
      "Episode:407 meanR:2.1000 R:0.0 loss:0.0398\n",
      "Episode:408 meanR:2.1000 R:-1.0 loss:0.0345\n",
      "Episode:409 meanR:2.0900 R:-1.0 loss:0.0172\n",
      "Episode:410 meanR:2.0600 R:-2.0 loss:0.0333\n",
      "Episode:411 meanR:2.0200 R:0.0 loss:0.0208\n",
      "Episode:412 meanR:2.0200 R:1.0 loss:0.0100\n",
      "Episode:413 meanR:2.0400 R:1.0 loss:0.0108\n",
      "Episode:414 meanR:2.0000 R:-1.0 loss:0.0203\n",
      "Episode:415 meanR:2.0300 R:5.0 loss:0.0337\n",
      "Episode:416 meanR:2.0600 R:4.0 loss:0.0385\n",
      "Episode:417 meanR:2.1000 R:4.0 loss:0.0352\n",
      "Episode:418 meanR:2.1300 R:3.0 loss:0.0417\n",
      "Episode:419 meanR:2.1500 R:6.0 loss:0.0313\n",
      "Episode:420 meanR:2.1700 R:6.0 loss:0.0286\n",
      "Episode:421 meanR:2.1800 R:3.0 loss:0.0542\n",
      "Episode:422 meanR:2.1600 R:2.0 loss:0.0479\n",
      "Episode:423 meanR:2.2400 R:12.0 loss:0.0267\n",
      "Episode:424 meanR:2.3200 R:6.0 loss:0.0738\n",
      "Episode:425 meanR:2.3400 R:6.0 loss:0.0458\n",
      "Episode:426 meanR:2.4400 R:8.0 loss:0.0411\n",
      "Episode:427 meanR:2.4900 R:10.0 loss:0.0615\n",
      "Episode:428 meanR:2.5300 R:2.0 loss:0.0881\n",
      "Episode:429 meanR:2.5300 R:4.0 loss:0.0426\n",
      "Episode:430 meanR:2.5400 R:6.0 loss:0.0793\n",
      "Episode:431 meanR:2.5500 R:7.0 loss:0.0433\n",
      "Episode:432 meanR:2.5700 R:5.0 loss:0.0715\n",
      "Episode:433 meanR:2.5900 R:2.0 loss:0.0379\n",
      "Episode:434 meanR:2.5800 R:2.0 loss:0.0136\n",
      "Episode:435 meanR:2.5400 R:1.0 loss:0.1117\n",
      "Episode:436 meanR:2.5000 R:6.0 loss:0.0804\n",
      "Episode:437 meanR:2.4600 R:1.0 loss:0.0534\n",
      "Episode:438 meanR:2.4100 R:1.0 loss:0.0753\n",
      "Episode:439 meanR:2.3700 R:6.0 loss:0.1061\n",
      "Episode:440 meanR:2.3900 R:5.0 loss:0.0676\n",
      "Episode:441 meanR:2.4200 R:3.0 loss:0.0550\n",
      "Episode:442 meanR:2.3900 R:2.0 loss:0.0774\n",
      "Episode:443 meanR:2.3500 R:2.0 loss:0.0518\n",
      "Episode:444 meanR:2.3000 R:3.0 loss:0.0467\n",
      "Episode:445 meanR:2.3600 R:8.0 loss:0.0315\n",
      "Episode:446 meanR:2.3800 R:2.0 loss:0.0324\n",
      "Episode:447 meanR:2.4000 R:2.0 loss:0.0229\n",
      "Episode:448 meanR:2.3900 R:0.0 loss:0.0479\n",
      "Episode:449 meanR:2.3900 R:2.0 loss:0.0282\n",
      "Episode:450 meanR:2.4100 R:4.0 loss:0.0540\n",
      "Episode:451 meanR:2.4700 R:6.0 loss:0.0374\n",
      "Episode:452 meanR:2.5100 R:4.0 loss:0.0498\n",
      "Episode:453 meanR:2.5900 R:7.0 loss:0.0499\n",
      "Episode:454 meanR:2.6600 R:7.0 loss:0.0335\n",
      "Episode:455 meanR:2.7300 R:7.0 loss:0.0348\n",
      "Episode:456 meanR:2.8200 R:12.0 loss:0.0850\n",
      "Episode:457 meanR:2.8700 R:7.0 loss:0.0870\n",
      "Episode:458 meanR:2.8900 R:5.0 loss:0.0736\n",
      "Episode:459 meanR:2.9400 R:11.0 loss:0.0960\n",
      "Episode:460 meanR:2.9800 R:9.0 loss:0.1091\n",
      "Episode:461 meanR:2.9400 R:0.0 loss:0.0919\n",
      "Episode:462 meanR:3.0000 R:6.0 loss:0.0923\n",
      "Episode:463 meanR:3.1100 R:11.0 loss:0.0373\n",
      "Episode:464 meanR:3.1200 R:2.0 loss:0.0914\n",
      "Episode:465 meanR:3.1800 R:6.0 loss:0.0830\n",
      "Episode:466 meanR:3.1900 R:3.0 loss:0.0451\n",
      "Episode:467 meanR:3.2400 R:5.0 loss:0.0639\n",
      "Episode:468 meanR:3.2600 R:2.0 loss:0.0691\n",
      "Episode:469 meanR:3.3500 R:9.0 loss:0.0375\n",
      "Episode:470 meanR:3.3800 R:1.0 loss:0.0301\n",
      "Episode:471 meanR:3.4500 R:7.0 loss:0.1026\n",
      "Episode:472 meanR:3.4900 R:7.0 loss:0.0438\n",
      "Episode:473 meanR:3.5800 R:8.0 loss:0.0908\n",
      "Episode:474 meanR:3.5800 R:9.0 loss:0.0461\n",
      "Episode:475 meanR:3.5900 R:3.0 loss:0.0663\n",
      "Episode:476 meanR:3.6200 R:6.0 loss:0.0596\n",
      "Episode:477 meanR:3.7700 R:13.0 loss:0.0684\n",
      "Episode:478 meanR:3.7200 R:-1.0 loss:0.0635\n",
      "Episode:479 meanR:3.7900 R:11.0 loss:0.0802\n",
      "Episode:480 meanR:3.8400 R:8.0 loss:0.1008\n",
      "Episode:481 meanR:3.8700 R:5.0 loss:0.1043\n",
      "Episode:482 meanR:3.8300 R:0.0 loss:0.0522\n",
      "Episode:483 meanR:3.8400 R:4.0 loss:0.0505\n",
      "Episode:484 meanR:3.8000 R:3.0 loss:0.0839\n",
      "Episode:485 meanR:3.7300 R:0.0 loss:0.0360\n",
      "Episode:486 meanR:3.7500 R:5.0 loss:0.1229\n",
      "Episode:487 meanR:3.7700 R:5.0 loss:0.0481\n",
      "Episode:488 meanR:3.8100 R:6.0 loss:0.0573\n",
      "Episode:489 meanR:3.8200 R:4.0 loss:0.0817\n",
      "Episode:490 meanR:3.7900 R:0.0 loss:0.0603\n",
      "Episode:491 meanR:3.7600 R:1.0 loss:0.0490\n",
      "Episode:492 meanR:3.8300 R:7.0 loss:0.0939\n",
      "Episode:493 meanR:3.8400 R:1.0 loss:0.0621\n",
      "Episode:494 meanR:3.8900 R:6.0 loss:0.0554\n",
      "Episode:495 meanR:3.9400 R:5.0 loss:0.0459\n",
      "Episode:496 meanR:4.0000 R:7.0 loss:0.0590\n",
      "Episode:497 meanR:4.0200 R:2.0 loss:0.0555\n",
      "Episode:498 meanR:4.1200 R:8.0 loss:0.0469\n",
      "Episode:499 meanR:4.1600 R:5.0 loss:0.0723\n",
      "Episode:500 meanR:4.2000 R:4.0 loss:0.0901\n",
      "Episode:501 meanR:4.2000 R:0.0 loss:0.0607\n",
      "Episode:502 meanR:4.2100 R:2.0 loss:0.0723\n",
      "Episode:503 meanR:4.2000 R:-1.0 loss:0.0396\n",
      "Episode:504 meanR:4.1900 R:0.0 loss:0.0519\n",
      "Episode:505 meanR:4.1800 R:1.0 loss:0.0357\n",
      "Episode:506 meanR:4.1800 R:-1.0 loss:0.0200\n",
      "Episode:507 meanR:4.2100 R:3.0 loss:0.0315\n",
      "Episode:508 meanR:4.2900 R:7.0 loss:0.0785\n",
      "Episode:509 meanR:4.3400 R:4.0 loss:0.1296\n",
      "Episode:510 meanR:4.3600 R:0.0 loss:0.0524\n",
      "Episode:511 meanR:4.3700 R:1.0 loss:0.0877\n",
      "Episode:512 meanR:4.4400 R:8.0 loss:0.0484\n",
      "Episode:513 meanR:4.4300 R:0.0 loss:0.0795\n",
      "Episode:514 meanR:4.4800 R:4.0 loss:0.0476\n",
      "Episode:515 meanR:4.4900 R:6.0 loss:0.0335\n",
      "Episode:516 meanR:4.4800 R:3.0 loss:0.0538\n",
      "Episode:517 meanR:4.4400 R:0.0 loss:0.0596\n",
      "Episode:518 meanR:4.4400 R:3.0 loss:0.0338\n",
      "Episode:519 meanR:4.4100 R:3.0 loss:0.0382\n",
      "Episode:520 meanR:4.3700 R:2.0 loss:0.0619\n",
      "Episode:521 meanR:4.3500 R:1.0 loss:0.0379\n",
      "Episode:522 meanR:4.3700 R:4.0 loss:0.0430\n",
      "Episode:523 meanR:4.3100 R:6.0 loss:0.0245\n",
      "Episode:524 meanR:4.2900 R:4.0 loss:0.0314\n",
      "Episode:525 meanR:4.3000 R:7.0 loss:0.0466\n",
      "Episode:526 meanR:4.2700 R:5.0 loss:0.0529\n",
      "Episode:527 meanR:4.2500 R:8.0 loss:0.0435\n",
      "Episode:528 meanR:4.2300 R:0.0 loss:0.0587\n",
      "Episode:529 meanR:4.2200 R:3.0 loss:0.0556\n",
      "Episode:530 meanR:4.1700 R:1.0 loss:0.0299\n",
      "Episode:531 meanR:4.1000 R:0.0 loss:0.0711\n",
      "Episode:532 meanR:4.0600 R:1.0 loss:0.0482\n",
      "Episode:533 meanR:4.0900 R:5.0 loss:0.0315\n",
      "Episode:534 meanR:4.1600 R:9.0 loss:0.0603\n",
      "Episode:535 meanR:4.1600 R:1.0 loss:0.0425\n",
      "Episode:536 meanR:4.1600 R:6.0 loss:0.0674\n",
      "Episode:537 meanR:4.1900 R:4.0 loss:0.0440\n",
      "Episode:538 meanR:4.1700 R:-1.0 loss:0.0850\n",
      "Episode:539 meanR:4.1400 R:3.0 loss:0.0397\n",
      "Episode:540 meanR:4.1700 R:8.0 loss:0.0530\n",
      "Episode:541 meanR:4.1700 R:3.0 loss:0.0877\n",
      "Episode:542 meanR:4.1700 R:2.0 loss:0.0587\n",
      "Episode:543 meanR:4.2100 R:6.0 loss:0.0492\n",
      "Episode:544 meanR:4.2100 R:3.0 loss:0.0461\n",
      "Episode:545 meanR:4.1700 R:4.0 loss:0.0289\n",
      "Episode:546 meanR:4.1600 R:1.0 loss:0.0493\n",
      "Episode:547 meanR:4.1700 R:3.0 loss:0.0875\n",
      "Episode:548 meanR:4.1900 R:2.0 loss:0.0403\n",
      "Episode:549 meanR:4.2000 R:3.0 loss:0.0397\n",
      "Episode:550 meanR:4.1900 R:3.0 loss:0.0405\n",
      "Episode:551 meanR:4.1900 R:6.0 loss:0.0627\n",
      "Episode:552 meanR:4.1800 R:3.0 loss:0.0836\n",
      "Episode:553 meanR:4.1400 R:3.0 loss:0.0859\n",
      "Episode:554 meanR:4.1300 R:6.0 loss:0.0396\n",
      "Episode:555 meanR:4.0900 R:3.0 loss:0.0677\n",
      "Episode:556 meanR:4.0200 R:5.0 loss:0.0524\n",
      "Episode:557 meanR:4.0400 R:9.0 loss:0.0999\n",
      "Episode:558 meanR:4.0600 R:7.0 loss:0.1015\n",
      "Episode:559 meanR:3.9800 R:3.0 loss:0.0667\n",
      "Episode:560 meanR:3.9200 R:3.0 loss:0.0686\n",
      "Episode:561 meanR:3.9500 R:3.0 loss:0.0440\n",
      "Episode:562 meanR:3.9300 R:4.0 loss:0.0760\n",
      "Episode:563 meanR:3.8100 R:-1.0 loss:0.0448\n",
      "Episode:564 meanR:3.8300 R:4.0 loss:0.0331\n",
      "Episode:565 meanR:3.8300 R:6.0 loss:0.0315\n",
      "Episode:566 meanR:3.8500 R:5.0 loss:0.0290\n",
      "Episode:567 meanR:3.8700 R:7.0 loss:0.0265\n",
      "Episode:568 meanR:3.9100 R:6.0 loss:0.0812\n",
      "Episode:569 meanR:3.8200 R:0.0 loss:0.0941\n",
      "Episode:570 meanR:3.8000 R:-1.0 loss:0.0609\n",
      "Episode:571 meanR:3.7600 R:3.0 loss:0.0584\n",
      "Episode:572 meanR:3.7500 R:6.0 loss:0.0600\n",
      "Episode:573 meanR:3.7800 R:11.0 loss:0.0725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:574 meanR:3.7400 R:5.0 loss:0.0709\n",
      "Episode:575 meanR:3.7800 R:7.0 loss:0.0604\n",
      "Episode:576 meanR:3.7600 R:4.0 loss:0.0356\n",
      "Episode:577 meanR:3.7000 R:7.0 loss:0.0388\n",
      "Episode:578 meanR:3.7300 R:2.0 loss:0.0523\n",
      "Episode:579 meanR:3.6700 R:5.0 loss:0.0449\n",
      "Episode:580 meanR:3.6300 R:4.0 loss:0.0354\n",
      "Episode:581 meanR:3.6300 R:5.0 loss:0.0296\n",
      "Episode:582 meanR:3.6600 R:3.0 loss:0.0367\n",
      "Episode:583 meanR:3.6200 R:0.0 loss:0.0219\n",
      "Episode:584 meanR:3.6400 R:5.0 loss:0.0365\n",
      "Episode:585 meanR:3.6700 R:3.0 loss:0.0427\n",
      "Episode:586 meanR:3.6900 R:7.0 loss:0.0498\n",
      "Episode:587 meanR:3.6500 R:1.0 loss:0.1001\n",
      "Episode:588 meanR:3.6200 R:3.0 loss:0.0484\n",
      "Episode:589 meanR:3.5900 R:1.0 loss:0.0510\n",
      "Episode:590 meanR:3.5900 R:0.0 loss:0.0249\n",
      "Episode:591 meanR:3.5900 R:1.0 loss:0.0416\n",
      "Episode:592 meanR:3.5300 R:1.0 loss:0.0292\n",
      "Episode:593 meanR:3.5300 R:1.0 loss:0.0295\n",
      "Episode:594 meanR:3.4900 R:2.0 loss:0.0401\n",
      "Episode:595 meanR:3.4600 R:2.0 loss:0.0575\n",
      "Episode:596 meanR:3.4400 R:5.0 loss:0.0296\n",
      "Episode:597 meanR:3.4900 R:7.0 loss:0.0342\n",
      "Episode:598 meanR:3.4500 R:4.0 loss:0.0659\n",
      "Episode:599 meanR:3.4500 R:5.0 loss:0.0499\n",
      "Episode:600 meanR:3.4600 R:5.0 loss:0.0541\n",
      "Episode:601 meanR:3.5000 R:4.0 loss:0.0506\n",
      "Episode:602 meanR:3.4800 R:0.0 loss:0.0358\n",
      "Episode:603 meanR:3.5200 R:3.0 loss:0.0532\n",
      "Episode:604 meanR:3.5800 R:6.0 loss:0.0425\n",
      "Episode:605 meanR:3.5900 R:2.0 loss:0.0632\n",
      "Episode:606 meanR:3.6400 R:4.0 loss:0.0520\n",
      "Episode:607 meanR:3.6400 R:3.0 loss:0.0661\n",
      "Episode:608 meanR:3.5900 R:2.0 loss:0.0849\n",
      "Episode:609 meanR:3.5600 R:1.0 loss:0.0650\n",
      "Episode:610 meanR:3.6100 R:5.0 loss:0.0940\n",
      "Episode:611 meanR:3.6500 R:5.0 loss:0.0843\n",
      "Episode:612 meanR:3.6100 R:4.0 loss:0.0747\n",
      "Episode:613 meanR:3.6900 R:8.0 loss:0.0621\n",
      "Episode:614 meanR:3.7500 R:10.0 loss:0.0757\n",
      "Episode:615 meanR:3.7300 R:4.0 loss:0.1818\n",
      "Episode:616 meanR:3.7500 R:5.0 loss:0.0553\n",
      "Episode:617 meanR:3.8000 R:5.0 loss:0.0726\n",
      "Episode:618 meanR:3.7600 R:-1.0 loss:0.1119\n",
      "Episode:619 meanR:3.7700 R:4.0 loss:0.0551\n",
      "Episode:620 meanR:3.7900 R:4.0 loss:0.0504\n",
      "Episode:621 meanR:3.7500 R:-3.0 loss:0.0342\n",
      "Episode:622 meanR:3.7400 R:3.0 loss:0.0329\n",
      "Episode:623 meanR:3.6900 R:1.0 loss:0.0360\n",
      "Episode:624 meanR:3.6400 R:-1.0 loss:0.0291\n",
      "Episode:625 meanR:3.6200 R:5.0 loss:0.0768\n",
      "Episode:626 meanR:3.6200 R:5.0 loss:0.0688\n",
      "Episode:627 meanR:3.5900 R:5.0 loss:0.0965\n",
      "Episode:628 meanR:3.6000 R:1.0 loss:0.0323\n",
      "Episode:629 meanR:3.6500 R:8.0 loss:0.0487\n",
      "Episode:630 meanR:3.7200 R:8.0 loss:0.0517\n",
      "Episode:631 meanR:3.7900 R:7.0 loss:0.0707\n",
      "Episode:632 meanR:3.9200 R:14.0 loss:0.0642\n",
      "Episode:633 meanR:3.9000 R:3.0 loss:0.0561\n",
      "Episode:634 meanR:3.8300 R:2.0 loss:0.0394\n",
      "Episode:635 meanR:3.8600 R:4.0 loss:0.0586\n",
      "Episode:636 meanR:3.8200 R:2.0 loss:0.0657\n",
      "Episode:637 meanR:3.7900 R:1.0 loss:0.0277\n",
      "Episode:638 meanR:3.8100 R:1.0 loss:0.0571\n",
      "Episode:639 meanR:3.7900 R:1.0 loss:0.0435\n",
      "Episode:640 meanR:3.7200 R:1.0 loss:0.0300\n",
      "Episode:641 meanR:3.7000 R:1.0 loss:0.0240\n",
      "Episode:642 meanR:3.7000 R:2.0 loss:0.0336\n",
      "Episode:643 meanR:3.6400 R:0.0 loss:0.0549\n",
      "Episode:644 meanR:3.6700 R:6.0 loss:0.0345\n",
      "Episode:645 meanR:3.6900 R:6.0 loss:0.0373\n",
      "Episode:646 meanR:3.7000 R:2.0 loss:0.0578\n",
      "Episode:647 meanR:3.7000 R:3.0 loss:0.0286\n",
      "Episode:648 meanR:3.7200 R:4.0 loss:0.0272\n",
      "Episode:649 meanR:3.7300 R:4.0 loss:0.0872\n",
      "Episode:650 meanR:3.7000 R:0.0 loss:0.0722\n",
      "Episode:651 meanR:3.6500 R:1.0 loss:0.0348\n",
      "Episode:652 meanR:3.6600 R:4.0 loss:0.0715\n",
      "Episode:653 meanR:3.6400 R:1.0 loss:0.0654\n",
      "Episode:654 meanR:3.6100 R:3.0 loss:0.0413\n",
      "Episode:655 meanR:3.5800 R:0.0 loss:0.0692\n",
      "Episode:656 meanR:3.6000 R:7.0 loss:0.0941\n",
      "Episode:657 meanR:3.5500 R:4.0 loss:0.0573\n",
      "Episode:658 meanR:3.5300 R:5.0 loss:0.0630\n",
      "Episode:659 meanR:3.5400 R:4.0 loss:0.0434\n",
      "Episode:660 meanR:3.5200 R:1.0 loss:0.0802\n",
      "Episode:661 meanR:3.5400 R:5.0 loss:0.0519\n",
      "Episode:662 meanR:3.5600 R:6.0 loss:0.0452\n",
      "Episode:663 meanR:3.5700 R:0.0 loss:0.1200\n",
      "Episode:664 meanR:3.5800 R:5.0 loss:0.0593\n",
      "Episode:665 meanR:3.5600 R:4.0 loss:0.0412\n",
      "Episode:666 meanR:3.5100 R:0.0 loss:0.0738\n",
      "Episode:667 meanR:3.4600 R:2.0 loss:0.0432\n",
      "Episode:668 meanR:3.4000 R:0.0 loss:0.0421\n",
      "Episode:669 meanR:3.4100 R:1.0 loss:0.0298\n",
      "Episode:670 meanR:3.4600 R:4.0 loss:0.0533\n",
      "Episode:671 meanR:3.4800 R:5.0 loss:0.0354\n",
      "Episode:672 meanR:3.5000 R:8.0 loss:0.0496\n",
      "Episode:673 meanR:3.4200 R:3.0 loss:0.0747\n",
      "Episode:674 meanR:3.4100 R:4.0 loss:0.0462\n",
      "Episode:675 meanR:3.3400 R:0.0 loss:0.0199\n",
      "Episode:676 meanR:3.3600 R:6.0 loss:0.0745\n",
      "Episode:677 meanR:3.3400 R:5.0 loss:0.0708\n",
      "Episode:678 meanR:3.3300 R:1.0 loss:0.0399\n",
      "Episode:679 meanR:3.2600 R:-2.0 loss:0.0331\n",
      "Episode:680 meanR:3.1800 R:-4.0 loss:0.0362\n",
      "Episode:681 meanR:3.1300 R:0.0 loss:0.0387\n",
      "Episode:682 meanR:3.1300 R:3.0 loss:0.0378\n",
      "Episode:683 meanR:3.1600 R:3.0 loss:0.0516\n",
      "Episode:684 meanR:3.2000 R:9.0 loss:0.0373\n",
      "Episode:685 meanR:3.2200 R:5.0 loss:0.0568\n",
      "Episode:686 meanR:3.1600 R:1.0 loss:0.0555\n",
      "Episode:687 meanR:3.2100 R:6.0 loss:0.1115\n",
      "Episode:688 meanR:3.1600 R:-2.0 loss:0.0574\n",
      "Episode:689 meanR:3.2000 R:5.0 loss:0.0900\n",
      "Episode:690 meanR:3.2200 R:2.0 loss:0.0490\n",
      "Episode:691 meanR:3.2500 R:4.0 loss:0.0490\n",
      "Episode:692 meanR:3.3000 R:6.0 loss:0.1169\n",
      "Episode:693 meanR:3.3100 R:2.0 loss:0.0642\n",
      "Episode:694 meanR:3.3400 R:5.0 loss:0.0477\n",
      "Episode:695 meanR:3.3700 R:5.0 loss:0.0780\n",
      "Episode:696 meanR:3.3700 R:5.0 loss:0.0537\n",
      "Episode:697 meanR:3.3800 R:8.0 loss:0.0409\n",
      "Episode:698 meanR:3.4400 R:10.0 loss:0.0546\n",
      "Episode:699 meanR:3.4600 R:7.0 loss:0.0466\n",
      "Episode:700 meanR:3.4400 R:3.0 loss:0.0802\n",
      "Episode:701 meanR:3.3900 R:-1.0 loss:0.0407\n",
      "Episode:702 meanR:3.4300 R:4.0 loss:0.0447\n",
      "Episode:703 meanR:3.4300 R:3.0 loss:0.0556\n",
      "Episode:704 meanR:3.4100 R:4.0 loss:0.0581\n",
      "Episode:705 meanR:3.4000 R:1.0 loss:0.0856\n",
      "Episode:706 meanR:3.3700 R:1.0 loss:0.0507\n",
      "Episode:707 meanR:3.3700 R:3.0 loss:0.0517\n",
      "Episode:708 meanR:3.3600 R:1.0 loss:0.0780\n",
      "Episode:709 meanR:3.3800 R:3.0 loss:0.0595\n",
      "Episode:710 meanR:3.3800 R:5.0 loss:0.0495\n",
      "Episode:711 meanR:3.3800 R:5.0 loss:0.0492\n",
      "Episode:712 meanR:3.4300 R:9.0 loss:0.0611\n",
      "Episode:713 meanR:3.4100 R:6.0 loss:0.0424\n",
      "Episode:714 meanR:3.3500 R:4.0 loss:0.0377\n",
      "Episode:715 meanR:3.3400 R:3.0 loss:0.0734\n",
      "Episode:716 meanR:3.3100 R:2.0 loss:0.0306\n",
      "Episode:717 meanR:3.3400 R:8.0 loss:0.0368\n",
      "Episode:718 meanR:3.3900 R:4.0 loss:0.0245\n",
      "Episode:719 meanR:3.3600 R:1.0 loss:0.0540\n",
      "Episode:720 meanR:3.3600 R:4.0 loss:0.0482\n",
      "Episode:721 meanR:3.4000 R:1.0 loss:0.0232\n",
      "Episode:722 meanR:3.4100 R:4.0 loss:0.0395\n",
      "Episode:723 meanR:3.4000 R:0.0 loss:0.0386\n",
      "Episode:724 meanR:3.4300 R:2.0 loss:0.0512\n",
      "Episode:725 meanR:3.3700 R:-1.0 loss:0.0667\n",
      "Episode:726 meanR:3.3500 R:3.0 loss:0.0304\n",
      "Episode:727 meanR:3.3000 R:0.0 loss:0.0625\n",
      "Episode:728 meanR:3.2900 R:0.0 loss:0.0434\n",
      "Episode:729 meanR:3.2300 R:2.0 loss:0.0296\n",
      "Episode:730 meanR:3.1900 R:4.0 loss:0.0437\n",
      "Episode:731 meanR:3.2000 R:8.0 loss:0.0354\n",
      "Episode:732 meanR:3.0700 R:1.0 loss:0.0851\n",
      "Episode:733 meanR:3.0600 R:2.0 loss:0.0297\n",
      "Episode:734 meanR:3.0900 R:5.0 loss:0.0646\n",
      "Episode:735 meanR:3.1100 R:6.0 loss:0.0571\n",
      "Episode:736 meanR:3.1300 R:4.0 loss:0.0423\n",
      "Episode:737 meanR:3.1600 R:4.0 loss:0.0965\n",
      "Episode:738 meanR:3.1700 R:2.0 loss:0.0249\n",
      "Episode:739 meanR:3.2000 R:4.0 loss:0.0548\n",
      "Episode:740 meanR:3.3100 R:12.0 loss:0.0987\n",
      "Episode:741 meanR:3.3800 R:8.0 loss:0.0869\n",
      "Episode:742 meanR:3.3700 R:1.0 loss:0.0838\n",
      "Episode:743 meanR:3.4000 R:3.0 loss:0.0548\n",
      "Episode:744 meanR:3.3700 R:3.0 loss:0.0562\n",
      "Episode:745 meanR:3.3100 R:0.0 loss:0.0575\n",
      "Episode:746 meanR:3.3100 R:2.0 loss:0.0539\n",
      "Episode:747 meanR:3.2800 R:0.0 loss:0.0446\n",
      "Episode:748 meanR:3.2700 R:3.0 loss:0.0581\n",
      "Episode:749 meanR:3.2300 R:0.0 loss:0.0621\n",
      "Episode:750 meanR:3.2300 R:0.0 loss:0.0267\n",
      "Episode:751 meanR:3.2800 R:6.0 loss:0.0381\n",
      "Episode:752 meanR:3.2400 R:0.0 loss:0.0505\n",
      "Episode:753 meanR:3.2600 R:3.0 loss:0.0979\n",
      "Episode:754 meanR:3.2800 R:5.0 loss:0.0814\n",
      "Episode:755 meanR:3.2900 R:1.0 loss:0.0631\n",
      "Episode:756 meanR:3.2400 R:2.0 loss:0.0573\n",
      "Episode:757 meanR:3.2400 R:4.0 loss:0.0507\n",
      "Episode:758 meanR:3.2000 R:1.0 loss:0.0361\n",
      "Episode:759 meanR:3.1600 R:0.0 loss:0.0912\n",
      "Episode:760 meanR:3.1600 R:1.0 loss:0.0920\n",
      "Episode:761 meanR:3.2000 R:9.0 loss:0.1198\n",
      "Episode:762 meanR:3.2200 R:8.0 loss:0.0936\n",
      "Episode:763 meanR:3.2300 R:1.0 loss:0.0644\n",
      "Episode:764 meanR:3.1900 R:1.0 loss:0.0521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:765 meanR:3.1600 R:1.0 loss:0.0616\n",
      "Episode:766 meanR:3.1800 R:2.0 loss:0.0448\n",
      "Episode:767 meanR:3.2500 R:9.0 loss:0.0782\n",
      "Episode:768 meanR:3.2500 R:0.0 loss:0.0962\n",
      "Episode:769 meanR:3.2900 R:5.0 loss:0.0579\n",
      "Episode:770 meanR:3.2500 R:0.0 loss:0.0360\n",
      "Episode:771 meanR:3.2000 R:0.0 loss:0.0194\n",
      "Episode:772 meanR:3.1100 R:-1.0 loss:0.0432\n",
      "Episode:773 meanR:3.0800 R:0.0 loss:0.0383\n",
      "Episode:774 meanR:3.0500 R:1.0 loss:0.0295\n",
      "Episode:775 meanR:3.0800 R:3.0 loss:0.0283\n",
      "Episode:776 meanR:3.0400 R:2.0 loss:0.0421\n",
      "Episode:777 meanR:3.0400 R:5.0 loss:0.0483\n",
      "Episode:778 meanR:3.0600 R:3.0 loss:0.0302\n",
      "Episode:779 meanR:3.0800 R:0.0 loss:0.0405\n",
      "Episode:780 meanR:3.1800 R:6.0 loss:0.0292\n",
      "Episode:781 meanR:3.2500 R:7.0 loss:0.0659\n",
      "Episode:782 meanR:3.2900 R:7.0 loss:0.0896\n",
      "Episode:783 meanR:3.3000 R:4.0 loss:0.0691\n",
      "Episode:784 meanR:3.2300 R:2.0 loss:0.0634\n",
      "Episode:785 meanR:3.1900 R:1.0 loss:0.0262\n",
      "Episode:786 meanR:3.2100 R:3.0 loss:0.0649\n",
      "Episode:787 meanR:3.1700 R:2.0 loss:0.0566\n",
      "Episode:788 meanR:3.1900 R:0.0 loss:0.0252\n",
      "Episode:789 meanR:3.1400 R:0.0 loss:0.0386\n",
      "Episode:790 meanR:3.1900 R:7.0 loss:0.0540\n",
      "Episode:791 meanR:3.1500 R:0.0 loss:0.0429\n",
      "Episode:792 meanR:3.1900 R:10.0 loss:0.0242\n",
      "Episode:793 meanR:3.2400 R:7.0 loss:0.0322\n",
      "Episode:794 meanR:3.2700 R:8.0 loss:0.0731\n",
      "Episode:795 meanR:3.2900 R:7.0 loss:0.0444\n",
      "Episode:796 meanR:3.2900 R:5.0 loss:0.0393\n",
      "Episode:797 meanR:3.2600 R:5.0 loss:0.0563\n",
      "Episode:798 meanR:3.1800 R:2.0 loss:0.0486\n",
      "Episode:799 meanR:3.1700 R:6.0 loss:0.0307\n",
      "Episode:800 meanR:3.1900 R:5.0 loss:0.0501\n",
      "Episode:801 meanR:3.2200 R:2.0 loss:0.0471\n",
      "Episode:802 meanR:3.2600 R:8.0 loss:0.0334\n",
      "Episode:803 meanR:3.2600 R:3.0 loss:0.0673\n",
      "Episode:804 meanR:3.2400 R:2.0 loss:0.0671\n",
      "Episode:805 meanR:3.2700 R:4.0 loss:0.0464\n",
      "Episode:806 meanR:3.2800 R:2.0 loss:0.0570\n",
      "Episode:807 meanR:3.3000 R:5.0 loss:0.0480\n",
      "Episode:808 meanR:3.3200 R:3.0 loss:0.0402\n",
      "Episode:809 meanR:3.3300 R:4.0 loss:0.0516\n",
      "Episode:810 meanR:3.3200 R:4.0 loss:0.0172\n",
      "Episode:811 meanR:3.2900 R:2.0 loss:0.0460\n",
      "Episode:812 meanR:3.1900 R:-1.0 loss:0.0419\n",
      "Episode:813 meanR:3.1700 R:4.0 loss:0.0347\n",
      "Episode:814 meanR:3.1700 R:4.0 loss:0.0453\n",
      "Episode:815 meanR:3.1500 R:1.0 loss:0.0564\n",
      "Episode:816 meanR:3.1500 R:2.0 loss:0.0287\n",
      "Episode:817 meanR:3.0800 R:1.0 loss:0.0447\n",
      "Episode:818 meanR:3.0800 R:4.0 loss:0.0412\n",
      "Episode:819 meanR:3.1400 R:7.0 loss:0.0308\n",
      "Episode:820 meanR:3.1600 R:6.0 loss:0.0469\n",
      "Episode:821 meanR:3.2500 R:10.0 loss:0.0627\n",
      "Episode:822 meanR:3.2900 R:8.0 loss:0.0524\n",
      "Episode:823 meanR:3.4000 R:11.0 loss:0.0306\n",
      "Episode:824 meanR:3.4200 R:4.0 loss:0.0614\n",
      "Episode:825 meanR:3.4700 R:4.0 loss:0.0323\n",
      "Episode:826 meanR:3.5000 R:6.0 loss:0.0304\n",
      "Episode:827 meanR:3.5500 R:5.0 loss:0.0204\n",
      "Episode:828 meanR:3.5700 R:2.0 loss:0.0974\n",
      "Episode:829 meanR:3.5400 R:-1.0 loss:0.0954\n",
      "Episode:830 meanR:3.5400 R:4.0 loss:0.0803\n",
      "Episode:831 meanR:3.4900 R:3.0 loss:0.0326\n",
      "Episode:832 meanR:3.4800 R:0.0 loss:0.0341\n",
      "Episode:833 meanR:3.5200 R:6.0 loss:0.0541\n",
      "Episode:834 meanR:3.5100 R:4.0 loss:0.0478\n",
      "Episode:835 meanR:3.5100 R:6.0 loss:0.0647\n",
      "Episode:836 meanR:3.5400 R:7.0 loss:0.0621\n",
      "Episode:837 meanR:3.5400 R:4.0 loss:0.0415\n",
      "Episode:838 meanR:3.5200 R:0.0 loss:0.0543\n",
      "Episode:839 meanR:3.5200 R:4.0 loss:0.0451\n",
      "Episode:840 meanR:3.4300 R:3.0 loss:0.0428\n",
      "Episode:841 meanR:3.3900 R:4.0 loss:0.0284\n",
      "Episode:842 meanR:3.4200 R:4.0 loss:0.0301\n",
      "Episode:843 meanR:3.4300 R:4.0 loss:0.0386\n",
      "Episode:844 meanR:3.4900 R:9.0 loss:0.0697\n",
      "Episode:845 meanR:3.6000 R:11.0 loss:0.0536\n",
      "Episode:846 meanR:3.6500 R:7.0 loss:0.0853\n",
      "Episode:847 meanR:3.6800 R:3.0 loss:0.0876\n",
      "Episode:848 meanR:3.7300 R:8.0 loss:0.0721\n",
      "Episode:849 meanR:3.7900 R:6.0 loss:0.0823\n",
      "Episode:850 meanR:3.8400 R:5.0 loss:0.0495\n",
      "Episode:851 meanR:3.8100 R:3.0 loss:0.0368\n",
      "Episode:852 meanR:3.8600 R:5.0 loss:0.0364\n",
      "Episode:853 meanR:3.8800 R:5.0 loss:0.0785\n",
      "Episode:854 meanR:3.8500 R:2.0 loss:0.0903\n",
      "Episode:855 meanR:3.8700 R:3.0 loss:0.0469\n",
      "Episode:856 meanR:3.8500 R:0.0 loss:0.0254\n",
      "Episode:857 meanR:3.8200 R:1.0 loss:0.0210\n",
      "Episode:858 meanR:3.8200 R:1.0 loss:0.0307\n",
      "Episode:859 meanR:3.8300 R:1.0 loss:0.0716\n",
      "Episode:860 meanR:3.8200 R:0.0 loss:0.0284\n",
      "Episode:861 meanR:3.7200 R:-1.0 loss:0.0398\n",
      "Episode:862 meanR:3.6300 R:-1.0 loss:0.0349\n",
      "Episode:863 meanR:3.6400 R:2.0 loss:0.0427\n",
      "Episode:864 meanR:3.6300 R:0.0 loss:0.0485\n",
      "Episode:865 meanR:3.6200 R:0.0 loss:0.0203\n",
      "Episode:866 meanR:3.6300 R:3.0 loss:0.0405\n",
      "Episode:867 meanR:3.5900 R:5.0 loss:0.0405\n",
      "Episode:868 meanR:3.6400 R:5.0 loss:0.0651\n",
      "Episode:869 meanR:3.6300 R:4.0 loss:0.0647\n",
      "Episode:870 meanR:3.6800 R:5.0 loss:0.0355\n",
      "Episode:871 meanR:3.7600 R:8.0 loss:0.0412\n",
      "Episode:872 meanR:3.8600 R:9.0 loss:0.0656\n",
      "Episode:873 meanR:3.8800 R:2.0 loss:0.0666\n",
      "Episode:874 meanR:3.9900 R:12.0 loss:0.0625\n",
      "Episode:875 meanR:3.9900 R:3.0 loss:0.0506\n",
      "Episode:876 meanR:3.9600 R:-1.0 loss:0.0350\n",
      "Episode:877 meanR:3.9600 R:5.0 loss:0.0424\n",
      "Episode:878 meanR:3.9300 R:0.0 loss:0.0440\n",
      "Episode:879 meanR:3.9500 R:2.0 loss:0.0380\n",
      "Episode:880 meanR:3.9800 R:9.0 loss:0.0431\n",
      "Episode:881 meanR:3.9400 R:3.0 loss:0.0694\n",
      "Episode:882 meanR:3.9800 R:11.0 loss:0.0954\n",
      "Episode:883 meanR:3.9600 R:2.0 loss:0.0708\n",
      "Episode:884 meanR:3.9900 R:5.0 loss:0.0719\n",
      "Episode:885 meanR:4.0600 R:8.0 loss:0.0590\n",
      "Episode:886 meanR:4.1500 R:12.0 loss:0.1003\n",
      "Episode:887 meanR:4.1900 R:6.0 loss:0.1107\n",
      "Episode:888 meanR:4.2400 R:5.0 loss:0.1518\n",
      "Episode:889 meanR:4.2800 R:4.0 loss:0.0471\n",
      "Episode:890 meanR:4.2900 R:8.0 loss:0.0585\n",
      "Episode:891 meanR:4.3800 R:9.0 loss:0.0559\n",
      "Episode:892 meanR:4.3200 R:4.0 loss:0.0846\n",
      "Episode:893 meanR:4.2800 R:3.0 loss:0.0927\n",
      "Episode:894 meanR:4.2200 R:2.0 loss:0.0633\n",
      "Episode:895 meanR:4.1900 R:4.0 loss:0.0773\n",
      "Episode:896 meanR:4.1500 R:1.0 loss:0.0523\n",
      "Episode:897 meanR:4.1600 R:6.0 loss:0.0850\n",
      "Episode:898 meanR:4.2000 R:6.0 loss:0.0826\n",
      "Episode:899 meanR:4.1900 R:5.0 loss:0.0481\n",
      "Episode:900 meanR:4.1500 R:1.0 loss:0.0470\n",
      "Episode:901 meanR:4.1800 R:5.0 loss:0.1038\n",
      "Episode:902 meanR:4.1400 R:4.0 loss:0.0821\n",
      "Episode:903 meanR:4.1300 R:2.0 loss:0.0711\n",
      "Episode:904 meanR:4.1100 R:0.0 loss:0.0505\n",
      "Episode:905 meanR:4.0900 R:2.0 loss:0.0355\n",
      "Episode:906 meanR:4.1000 R:3.0 loss:0.0451\n",
      "Episode:907 meanR:4.0700 R:2.0 loss:0.0541\n",
      "Episode:908 meanR:4.0600 R:2.0 loss:0.0749\n",
      "Episode:909 meanR:4.0500 R:3.0 loss:0.0386\n",
      "Episode:910 meanR:4.0200 R:1.0 loss:0.0296\n",
      "Episode:911 meanR:4.0400 R:4.0 loss:0.0253\n",
      "Episode:912 meanR:4.0700 R:2.0 loss:0.0321\n",
      "Episode:913 meanR:4.0700 R:4.0 loss:0.0325\n",
      "Episode:914 meanR:4.0500 R:2.0 loss:0.0435\n",
      "Episode:915 meanR:4.1100 R:7.0 loss:0.0413\n",
      "Episode:916 meanR:4.1000 R:1.0 loss:0.0618\n",
      "Episode:917 meanR:4.1500 R:6.0 loss:0.0839\n",
      "Episode:918 meanR:4.1500 R:4.0 loss:0.0442\n",
      "Episode:919 meanR:4.1100 R:3.0 loss:0.0438\n",
      "Episode:920 meanR:4.1400 R:9.0 loss:0.0517\n",
      "Episode:921 meanR:4.1000 R:6.0 loss:0.0534\n",
      "Episode:922 meanR:4.0800 R:6.0 loss:0.0454\n",
      "Episode:923 meanR:3.9800 R:1.0 loss:0.0468\n",
      "Episode:924 meanR:3.9800 R:4.0 loss:0.0688\n",
      "Episode:925 meanR:4.0000 R:6.0 loss:0.0654\n",
      "Episode:926 meanR:3.9400 R:0.0 loss:0.0393\n",
      "Episode:927 meanR:3.9500 R:6.0 loss:0.0846\n",
      "Episode:928 meanR:3.9500 R:2.0 loss:0.0818\n",
      "Episode:929 meanR:3.9600 R:0.0 loss:0.0724\n",
      "Episode:930 meanR:3.9300 R:1.0 loss:0.0758\n",
      "Episode:931 meanR:3.9100 R:1.0 loss:0.0698\n",
      "Episode:932 meanR:3.9300 R:2.0 loss:0.0541\n",
      "Episode:933 meanR:3.9300 R:6.0 loss:0.0501\n",
      "Episode:934 meanR:3.9300 R:4.0 loss:0.0420\n",
      "Episode:935 meanR:3.8700 R:0.0 loss:0.0437\n",
      "Episode:936 meanR:3.8300 R:3.0 loss:0.0408\n",
      "Episode:937 meanR:3.8500 R:6.0 loss:0.0507\n",
      "Episode:938 meanR:3.9200 R:7.0 loss:0.0698\n",
      "Episode:939 meanR:3.9800 R:10.0 loss:0.0425\n",
      "Episode:940 meanR:3.9800 R:3.0 loss:0.0604\n",
      "Episode:941 meanR:3.9600 R:2.0 loss:0.0493\n",
      "Episode:942 meanR:3.9600 R:4.0 loss:0.0290\n",
      "Episode:943 meanR:3.9600 R:4.0 loss:0.0385\n",
      "Episode:944 meanR:3.9000 R:3.0 loss:0.0302\n",
      "Episode:945 meanR:3.8400 R:5.0 loss:0.0479\n",
      "Episode:946 meanR:3.8400 R:7.0 loss:0.0255\n",
      "Episode:947 meanR:3.8100 R:0.0 loss:0.0212\n",
      "Episode:948 meanR:3.7900 R:6.0 loss:0.0586\n",
      "Episode:949 meanR:3.7600 R:3.0 loss:0.0348\n",
      "Episode:950 meanR:3.7300 R:2.0 loss:0.0520\n",
      "Episode:951 meanR:3.7500 R:5.0 loss:0.0335\n",
      "Episode:952 meanR:3.7400 R:4.0 loss:0.0514\n",
      "Episode:953 meanR:3.7400 R:5.0 loss:0.0388\n",
      "Episode:954 meanR:3.8000 R:8.0 loss:0.0544\n",
      "Episode:955 meanR:3.8300 R:6.0 loss:0.0520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:956 meanR:3.8400 R:1.0 loss:0.0326\n",
      "Episode:957 meanR:3.8800 R:5.0 loss:0.0258\n",
      "Episode:958 meanR:3.8900 R:2.0 loss:0.0465\n",
      "Episode:959 meanR:3.9300 R:5.0 loss:0.0370\n",
      "Episode:960 meanR:4.0400 R:11.0 loss:0.0870\n",
      "Episode:961 meanR:4.1500 R:10.0 loss:0.0448\n",
      "Episode:962 meanR:4.2200 R:6.0 loss:0.0521\n",
      "Episode:963 meanR:4.2700 R:7.0 loss:0.0336\n",
      "Episode:964 meanR:4.3200 R:5.0 loss:0.0375\n",
      "Episode:965 meanR:4.3300 R:1.0 loss:0.0449\n",
      "Episode:966 meanR:4.3400 R:4.0 loss:0.0318\n",
      "Episode:967 meanR:4.3500 R:6.0 loss:0.0517\n",
      "Episode:968 meanR:4.3400 R:4.0 loss:0.0234\n",
      "Episode:969 meanR:4.3000 R:0.0 loss:0.0167\n",
      "Episode:970 meanR:4.3000 R:5.0 loss:0.0226\n",
      "Episode:971 meanR:4.2100 R:-1.0 loss:0.0199\n",
      "Episode:972 meanR:4.1500 R:3.0 loss:0.0267\n",
      "Episode:973 meanR:4.1200 R:-1.0 loss:0.0279\n",
      "Episode:974 meanR:4.0200 R:2.0 loss:0.0385\n",
      "Episode:975 meanR:3.9900 R:0.0 loss:0.0600\n",
      "Episode:976 meanR:4.0200 R:2.0 loss:0.0279\n",
      "Episode:977 meanR:4.0200 R:5.0 loss:0.0253\n",
      "Episode:978 meanR:4.0500 R:3.0 loss:0.0359\n",
      "Episode:979 meanR:4.0700 R:4.0 loss:0.0536\n",
      "Episode:980 meanR:4.0000 R:2.0 loss:0.0197\n",
      "Episode:981 meanR:4.0600 R:9.0 loss:0.0406\n",
      "Episode:982 meanR:4.0100 R:6.0 loss:0.0844\n",
      "Episode:983 meanR:4.0700 R:8.0 loss:0.0451\n",
      "Episode:984 meanR:4.0600 R:4.0 loss:0.0479\n",
      "Episode:985 meanR:4.0900 R:11.0 loss:0.0655\n",
      "Episode:986 meanR:4.0600 R:9.0 loss:0.0637\n",
      "Episode:987 meanR:4.0700 R:7.0 loss:0.0501\n",
      "Episode:988 meanR:4.0500 R:3.0 loss:0.0531\n",
      "Episode:989 meanR:4.0400 R:3.0 loss:0.0646\n",
      "Episode:990 meanR:4.0500 R:9.0 loss:0.0664\n",
      "Episode:991 meanR:4.0400 R:8.0 loss:0.0761\n",
      "Episode:992 meanR:4.0200 R:2.0 loss:0.0812\n",
      "Episode:993 meanR:4.0100 R:2.0 loss:0.0396\n",
      "Episode:994 meanR:4.0700 R:8.0 loss:0.0565\n",
      "Episode:995 meanR:4.0500 R:2.0 loss:0.0423\n",
      "Episode:996 meanR:4.0700 R:3.0 loss:0.0577\n",
      "Episode:997 meanR:4.0400 R:3.0 loss:0.0464\n",
      "Episode:998 meanR:4.0700 R:9.0 loss:0.0742\n",
      "Episode:999 meanR:4.1400 R:12.0 loss:0.0469\n",
      "Episode:1000 meanR:4.1700 R:4.0 loss:0.0801\n",
      "Episode:1001 meanR:4.1400 R:2.0 loss:0.0607\n",
      "Episode:1002 meanR:4.1400 R:4.0 loss:0.0414\n",
      "Episode:1003 meanR:4.1600 R:4.0 loss:0.0477\n",
      "Episode:1004 meanR:4.2000 R:4.0 loss:0.0556\n",
      "Episode:1005 meanR:4.2000 R:2.0 loss:0.0489\n",
      "Episode:1006 meanR:4.1800 R:1.0 loss:0.0428\n",
      "Episode:1007 meanR:4.1700 R:1.0 loss:0.0638\n",
      "Episode:1008 meanR:4.1300 R:-2.0 loss:0.0335\n",
      "Episode:1009 meanR:4.1400 R:4.0 loss:0.0658\n",
      "Episode:1010 meanR:4.1400 R:1.0 loss:0.0333\n",
      "Episode:1011 meanR:4.1400 R:4.0 loss:0.0470\n",
      "Episode:1012 meanR:4.1800 R:6.0 loss:0.0507\n",
      "Episode:1013 meanR:4.1800 R:4.0 loss:0.0711\n",
      "Episode:1014 meanR:4.2000 R:4.0 loss:0.0443\n",
      "Episode:1015 meanR:4.1700 R:4.0 loss:0.0338\n",
      "Episode:1016 meanR:4.1900 R:3.0 loss:0.0588\n",
      "Episode:1017 meanR:4.1800 R:5.0 loss:0.0455\n",
      "Episode:1018 meanR:4.1700 R:3.0 loss:0.0377\n",
      "Episode:1019 meanR:4.1800 R:4.0 loss:0.0432\n",
      "Episode:1020 meanR:4.0900 R:0.0 loss:0.0318\n",
      "Episode:1021 meanR:4.0600 R:3.0 loss:0.0343\n",
      "Episode:1022 meanR:4.0100 R:1.0 loss:0.0245\n",
      "Episode:1023 meanR:4.0600 R:6.0 loss:0.0524\n",
      "Episode:1024 meanR:4.0200 R:0.0 loss:0.0558\n",
      "Episode:1025 meanR:3.9900 R:3.0 loss:0.0610\n",
      "Episode:1026 meanR:4.0200 R:3.0 loss:0.0407\n",
      "Episode:1027 meanR:4.0100 R:5.0 loss:0.0909\n",
      "Episode:1028 meanR:3.9500 R:-4.0 loss:0.0640\n",
      "Episode:1029 meanR:3.9800 R:3.0 loss:0.0564\n",
      "Episode:1030 meanR:4.0000 R:3.0 loss:0.0261\n",
      "Episode:1031 meanR:4.0200 R:3.0 loss:0.0299\n",
      "Episode:1032 meanR:3.9800 R:-2.0 loss:0.0702\n",
      "Episode:1033 meanR:3.9300 R:1.0 loss:0.0458\n",
      "Episode:1034 meanR:3.8800 R:-1.0 loss:0.0332\n",
      "Episode:1035 meanR:3.8700 R:-1.0 loss:0.0676\n",
      "Episode:1036 meanR:3.8600 R:2.0 loss:0.0407\n",
      "Episode:1037 meanR:3.7900 R:-1.0 loss:0.0502\n",
      "Episode:1038 meanR:3.7500 R:3.0 loss:0.0381\n",
      "Episode:1039 meanR:3.6800 R:3.0 loss:0.0458\n",
      "Episode:1040 meanR:3.6700 R:2.0 loss:0.0620\n",
      "Episode:1041 meanR:3.6600 R:1.0 loss:0.0532\n",
      "Episode:1042 meanR:3.6400 R:2.0 loss:0.0524\n",
      "Episode:1043 meanR:3.6500 R:5.0 loss:0.0379\n",
      "Episode:1044 meanR:3.7100 R:9.0 loss:0.0530\n",
      "Episode:1045 meanR:3.7400 R:8.0 loss:0.0568\n",
      "Episode:1046 meanR:3.6800 R:1.0 loss:0.0422\n",
      "Episode:1047 meanR:3.6900 R:1.0 loss:0.0673\n",
      "Episode:1048 meanR:3.6700 R:4.0 loss:0.0471\n",
      "Episode:1049 meanR:3.6800 R:4.0 loss:0.0671\n",
      "Episode:1050 meanR:3.6600 R:0.0 loss:0.0656\n",
      "Episode:1051 meanR:3.6300 R:2.0 loss:0.0560\n",
      "Episode:1052 meanR:3.6200 R:3.0 loss:0.0714\n",
      "Episode:1053 meanR:3.5900 R:2.0 loss:0.0684\n",
      "Episode:1054 meanR:3.5100 R:0.0 loss:0.0399\n",
      "Episode:1055 meanR:3.5000 R:5.0 loss:0.0519\n",
      "Episode:1056 meanR:3.5400 R:5.0 loss:0.0446\n",
      "Episode:1057 meanR:3.4800 R:-1.0 loss:0.0358\n",
      "Episode:1058 meanR:3.4600 R:0.0 loss:0.0448\n",
      "Episode:1059 meanR:3.4500 R:4.0 loss:0.0329\n",
      "Episode:1060 meanR:3.3500 R:1.0 loss:0.0413\n",
      "Episode:1061 meanR:3.2800 R:3.0 loss:0.0295\n",
      "Episode:1062 meanR:3.3200 R:10.0 loss:0.0721\n",
      "Episode:1063 meanR:3.3000 R:5.0 loss:0.0488\n",
      "Episode:1064 meanR:3.2800 R:3.0 loss:0.0623\n",
      "Episode:1065 meanR:3.2900 R:2.0 loss:0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception calling application: Ran out of input\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arasdar/anaconda3/envs/env3/lib/python3.6/site-packages/grpc/_server.py\", line 385, in _call_behavior\n",
      "    return behavior(argument, context), True\n",
      "  File \"/home/arasdar/anaconda3/envs/env3/lib/python3.6/site-packages/unityagents/rpc_communicator.py\", line 26, in Exchange\n",
      "    return self.child_conn.recv()\n",
      "  File \"/home/arasdar/anaconda3/envs/env3/lib/python3.6/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "EOFError: Ran out of input\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'BananaBrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4f6135c851d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m#state, reward, done, _ = env.step(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# send the action to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# get the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env3/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_brain_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BananaBrain'"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_steps in range(11111111111):\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append(initial_state)\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            #batch, rnn_states = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            initial_states = memory.states\n",
    "            next_actions_logits = sess.run(model.actions_logits,\n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: initial_states[1]})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs,\n",
    "                                                                     model.initial_state: initial_states[0]})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
