{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    #print(state)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "batch = []\n",
    "while True: # infinite number of steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    #print(state, action, reward, done)\n",
    "    batch.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1.        , 0.        , 0.        , 0.        , 0.35186428,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.37953866,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.11957462,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.43679786,\n",
       "         0.        , 1.        , 0.        , 0.        , 0.7516005 ,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.67057401,\n",
       "         0.        , 0.        , 1.        , 0.        , 0.36187497,\n",
       "         0.        , 0.        ]),\n",
       "  1,\n",
       "  array([ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.53486300e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  1.14156239e-01,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  4.53633040e-01,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          7.58845091e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  8.74741137e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  3.78134310e-01,  2.38418579e-07,\n",
       "         -7.81049633e+00]),\n",
       "  0.0,\n",
       "  0.0],\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 0.        , 0.        , 0.        , 0.35186428,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.37953866,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.11957462,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.43679786,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.7516005 ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.67057401,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.36187497,\n",
       "        0.        , 0.        ]),\n",
       " 1,\n",
       " array([ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         3.53486300e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.14156239e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  4.53633040e-01,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         7.58845091e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  8.74741137e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  3.78134310e-01,  2.38418579e-07,\n",
       "        -7.81049633e+00]),\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([each[1] for each in batch])\n",
    "actions = np.array([each[0] for each in batch])\n",
    "next_states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 37) (300,) (300, 37) (300,)\n",
      "float64 int64 float64 float64\n",
      "10.997564315795898 -10.365667343139648 22.363231658935547\n",
      "10.997564315795898 -10.365667343139648\n",
      "3 0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)), \n",
    "      (np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return states, actions, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx], [self.states[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 4\n",
    "state_size = 37\n",
    "hidden_size = 37*4             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 128            # memory capacity\n",
    "batch_size = 128             # experience mini-batch size\n",
    "gamma = 0.99                 # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37) (?, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(1, ?, 148) (1, 148)\n",
      "(?, 148)\n",
      "(?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# for _ in range(batch_size):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done is True:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "for _ in range(memory_size):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.0 loss:0.0263 exploreP:0.9707\n",
      "Episode:1 meanR:0.0000 R:0.0 loss:0.0867 exploreP:0.9423\n",
      "Episode:2 meanR:-0.3333 R:-1.0 loss:0.1435 exploreP:0.9148\n",
      "Episode:3 meanR:-0.5000 R:-1.0 loss:0.0816 exploreP:0.8881\n",
      "Episode:4 meanR:-0.4000 R:0.0 loss:0.1440 exploreP:0.8621\n",
      "Episode:5 meanR:-0.6667 R:-2.0 loss:0.1098 exploreP:0.8369\n",
      "Episode:6 meanR:-0.7143 R:-1.0 loss:0.1291 exploreP:0.8125\n",
      "Episode:7 meanR:-0.5000 R:1.0 loss:0.2234 exploreP:0.7888\n",
      "Episode:8 meanR:-0.5556 R:-1.0 loss:0.3053 exploreP:0.7657\n",
      "Episode:9 meanR:-0.7000 R:-2.0 loss:0.2095 exploreP:0.7434\n",
      "Episode:10 meanR:-0.6364 R:0.0 loss:0.1171 exploreP:0.7217\n",
      "Episode:11 meanR:-0.5000 R:1.0 loss:0.1544 exploreP:0.7007\n",
      "Episode:12 meanR:-0.4615 R:0.0 loss:0.1426 exploreP:0.6803\n",
      "Episode:13 meanR:-0.1429 R:4.0 loss:0.1855 exploreP:0.6605\n",
      "Episode:14 meanR:-0.0667 R:1.0 loss:0.1324 exploreP:0.6413\n",
      "Episode:15 meanR:0.0000 R:1.0 loss:0.0960 exploreP:0.6226\n",
      "Episode:16 meanR:0.0588 R:1.0 loss:0.0862 exploreP:0.6045\n",
      "Episode:17 meanR:0.1111 R:1.0 loss:0.0645 exploreP:0.5869\n",
      "Episode:18 meanR:0.1053 R:0.0 loss:0.0709 exploreP:0.5699\n",
      "Episode:19 meanR:0.1000 R:0.0 loss:0.0395 exploreP:0.5533\n",
      "Episode:20 meanR:0.1905 R:2.0 loss:0.0400 exploreP:0.5373\n",
      "Episode:21 meanR:0.1818 R:0.0 loss:0.0959 exploreP:0.5217\n",
      "Episode:22 meanR:0.1739 R:0.0 loss:0.0581 exploreP:0.5066\n",
      "Episode:23 meanR:0.1667 R:0.0 loss:0.0603 exploreP:0.4919\n",
      "Episode:24 meanR:0.1600 R:0.0 loss:0.0459 exploreP:0.4776\n",
      "Episode:25 meanR:0.1538 R:0.0 loss:0.1092 exploreP:0.4638\n",
      "Episode:26 meanR:0.1481 R:0.0 loss:0.1451 exploreP:0.4504\n",
      "Episode:27 meanR:0.0714 R:-2.0 loss:0.0890 exploreP:0.4374\n",
      "Episode:28 meanR:0.0345 R:-1.0 loss:0.1436 exploreP:0.4248\n",
      "Episode:29 meanR:0.0333 R:0.0 loss:0.1124 exploreP:0.4125\n",
      "Episode:30 meanR:0.0323 R:0.0 loss:0.0905 exploreP:0.4006\n",
      "Episode:31 meanR:0.0312 R:0.0 loss:0.0486 exploreP:0.3891\n",
      "Episode:32 meanR:0.0000 R:-1.0 loss:0.0744 exploreP:0.3779\n",
      "Episode:33 meanR:0.1765 R:6.0 loss:0.0856 exploreP:0.3670\n",
      "Episode:34 meanR:0.2000 R:1.0 loss:0.0895 exploreP:0.3564\n",
      "Episode:35 meanR:0.1944 R:0.0 loss:0.0694 exploreP:0.3462\n",
      "Episode:36 meanR:0.1622 R:-1.0 loss:0.0746 exploreP:0.3363\n",
      "Episode:37 meanR:0.1316 R:-1.0 loss:0.0527 exploreP:0.3266\n",
      "Episode:38 meanR:0.1282 R:0.0 loss:0.0545 exploreP:0.3173\n",
      "Episode:39 meanR:0.1500 R:1.0 loss:0.0548 exploreP:0.3082\n",
      "Episode:40 meanR:0.1463 R:0.0 loss:0.0382 exploreP:0.2994\n",
      "Episode:41 meanR:0.1429 R:0.0 loss:0.0429 exploreP:0.2908\n",
      "Episode:42 meanR:0.0930 R:-2.0 loss:0.0196 exploreP:0.2825\n",
      "Episode:43 meanR:0.1136 R:1.0 loss:0.0212 exploreP:0.2745\n",
      "Episode:44 meanR:0.1111 R:0.0 loss:0.0396 exploreP:0.2666\n",
      "Episode:45 meanR:0.1304 R:1.0 loss:0.0202 exploreP:0.2591\n",
      "Episode:46 meanR:0.1277 R:0.0 loss:0.0327 exploreP:0.2517\n",
      "Episode:47 meanR:0.1458 R:1.0 loss:0.0220 exploreP:0.2446\n",
      "Episode:48 meanR:0.1633 R:1.0 loss:0.0342 exploreP:0.2376\n",
      "Episode:49 meanR:0.1600 R:0.0 loss:0.0413 exploreP:0.2309\n",
      "Episode:50 meanR:0.1569 R:0.0 loss:0.0182 exploreP:0.2244\n",
      "Episode:51 meanR:0.1731 R:1.0 loss:0.0440 exploreP:0.2180\n",
      "Episode:52 meanR:0.1509 R:-1.0 loss:0.0907 exploreP:0.2119\n",
      "Episode:53 meanR:0.1481 R:0.0 loss:0.1122 exploreP:0.2059\n",
      "Episode:54 meanR:0.1455 R:0.0 loss:0.0952 exploreP:0.2001\n",
      "Episode:55 meanR:0.1429 R:0.0 loss:0.0449 exploreP:0.1945\n",
      "Episode:56 meanR:0.1754 R:2.0 loss:0.0478 exploreP:0.1891\n",
      "Episode:57 meanR:0.1552 R:-1.0 loss:0.1145 exploreP:0.1838\n",
      "Episode:58 meanR:0.1017 R:-3.0 loss:0.0892 exploreP:0.1786\n",
      "Episode:59 meanR:0.1500 R:3.0 loss:0.0378 exploreP:0.1736\n",
      "Episode:60 meanR:0.1475 R:0.0 loss:0.1191 exploreP:0.1688\n",
      "Episode:61 meanR:0.1290 R:-1.0 loss:0.0349 exploreP:0.1641\n",
      "Episode:62 meanR:0.1587 R:2.0 loss:0.1352 exploreP:0.1596\n",
      "Episode:63 meanR:0.2188 R:4.0 loss:0.1083 exploreP:0.1551\n",
      "Episode:64 meanR:0.2154 R:0.0 loss:0.1114 exploreP:0.1509\n",
      "Episode:65 meanR:0.2273 R:1.0 loss:0.1017 exploreP:0.1467\n",
      "Episode:66 meanR:0.2687 R:3.0 loss:0.1175 exploreP:0.1426\n",
      "Episode:67 meanR:0.2353 R:-2.0 loss:0.1114 exploreP:0.1387\n",
      "Episode:68 meanR:0.3043 R:5.0 loss:0.1574 exploreP:0.1349\n",
      "Episode:69 meanR:0.4000 R:7.0 loss:0.1635 exploreP:0.1312\n",
      "Episode:70 meanR:0.4507 R:4.0 loss:0.1959 exploreP:0.1276\n",
      "Episode:71 meanR:0.4722 R:2.0 loss:0.1857 exploreP:0.1242\n",
      "Episode:72 meanR:0.4521 R:-1.0 loss:0.0870 exploreP:0.1208\n",
      "Episode:73 meanR:0.4459 R:0.0 loss:0.0264 exploreP:0.1175\n",
      "Episode:74 meanR:0.4133 R:-2.0 loss:0.0279 exploreP:0.1143\n",
      "Episode:75 meanR:0.5000 R:7.0 loss:0.0749 exploreP:0.1113\n",
      "Episode:76 meanR:0.5195 R:2.0 loss:0.2162 exploreP:0.1083\n",
      "Episode:77 meanR:0.5385 R:2.0 loss:0.1229 exploreP:0.1054\n",
      "Episode:78 meanR:0.5570 R:2.0 loss:0.1998 exploreP:0.1025\n",
      "Episode:79 meanR:0.5375 R:-1.0 loss:0.1523 exploreP:0.0998\n",
      "Episode:80 meanR:0.5679 R:3.0 loss:0.1250 exploreP:0.0972\n",
      "Episode:81 meanR:0.5854 R:2.0 loss:0.1215 exploreP:0.0946\n",
      "Episode:82 meanR:0.5904 R:1.0 loss:0.0692 exploreP:0.0921\n",
      "Episode:83 meanR:0.5714 R:-1.0 loss:0.0914 exploreP:0.0897\n",
      "Episode:84 meanR:0.5647 R:0.0 loss:0.0280 exploreP:0.0873\n",
      "Episode:85 meanR:0.5930 R:3.0 loss:0.0656 exploreP:0.0850\n",
      "Episode:86 meanR:0.5747 R:-1.0 loss:0.0442 exploreP:0.0828\n",
      "Episode:87 meanR:0.5909 R:2.0 loss:0.0254 exploreP:0.0806\n",
      "Episode:88 meanR:0.6067 R:2.0 loss:0.0556 exploreP:0.0786\n",
      "Episode:89 meanR:0.6222 R:2.0 loss:0.0307 exploreP:0.0765\n",
      "Episode:90 meanR:0.6154 R:0.0 loss:0.0402 exploreP:0.0746\n",
      "Episode:91 meanR:0.6087 R:0.0 loss:0.0365 exploreP:0.0727\n",
      "Episode:92 meanR:0.6129 R:1.0 loss:0.0293 exploreP:0.0708\n",
      "Episode:93 meanR:0.6064 R:0.0 loss:0.0282 exploreP:0.0690\n",
      "Episode:94 meanR:0.6211 R:2.0 loss:0.0179 exploreP:0.0673\n",
      "Episode:95 meanR:0.6146 R:0.0 loss:0.0114 exploreP:0.0656\n",
      "Episode:96 meanR:0.5773 R:-3.0 loss:0.0273 exploreP:0.0639\n",
      "Episode:97 meanR:0.5918 R:2.0 loss:0.0310 exploreP:0.0623\n",
      "Episode:98 meanR:0.5859 R:0.0 loss:0.0566 exploreP:0.0608\n",
      "Episode:99 meanR:0.6200 R:4.0 loss:0.0504 exploreP:0.0593\n",
      "Episode:100 meanR:0.6200 R:0.0 loss:0.0662 exploreP:0.0578\n",
      "Episode:101 meanR:0.6200 R:0.0 loss:0.0265 exploreP:0.0564\n",
      "Episode:102 meanR:0.6600 R:3.0 loss:0.0399 exploreP:0.0550\n",
      "Episode:103 meanR:0.6600 R:-1.0 loss:0.1007 exploreP:0.0537\n",
      "Episode:104 meanR:0.6800 R:2.0 loss:0.1221 exploreP:0.0524\n",
      "Episode:105 meanR:0.7200 R:2.0 loss:0.0633 exploreP:0.0512\n",
      "Episode:106 meanR:0.7500 R:2.0 loss:0.0496 exploreP:0.0500\n",
      "Episode:107 meanR:0.7800 R:4.0 loss:0.1059 exploreP:0.0488\n",
      "Episode:108 meanR:0.7900 R:0.0 loss:0.1106 exploreP:0.0476\n",
      "Episode:109 meanR:0.8000 R:-1.0 loss:0.0647 exploreP:0.0465\n",
      "Episode:110 meanR:0.8200 R:2.0 loss:0.0452 exploreP:0.0454\n",
      "Episode:111 meanR:0.8300 R:2.0 loss:0.0625 exploreP:0.0444\n",
      "Episode:112 meanR:0.8400 R:1.0 loss:0.0321 exploreP:0.0434\n",
      "Episode:113 meanR:0.7900 R:-1.0 loss:0.0690 exploreP:0.0424\n",
      "Episode:114 meanR:0.7700 R:-1.0 loss:0.0390 exploreP:0.0414\n",
      "Episode:115 meanR:0.7600 R:0.0 loss:0.0249 exploreP:0.0405\n",
      "Episode:116 meanR:0.7800 R:3.0 loss:0.0349 exploreP:0.0396\n",
      "Episode:117 meanR:0.7900 R:2.0 loss:0.0576 exploreP:0.0387\n",
      "Episode:118 meanR:0.8200 R:3.0 loss:0.1019 exploreP:0.0379\n",
      "Episode:119 meanR:0.8600 R:4.0 loss:0.1414 exploreP:0.0371\n",
      "Episode:120 meanR:0.8800 R:4.0 loss:0.0705 exploreP:0.0363\n",
      "Episode:121 meanR:0.9200 R:4.0 loss:0.0642 exploreP:0.0355\n",
      "Episode:122 meanR:0.9200 R:0.0 loss:0.0919 exploreP:0.0347\n",
      "Episode:123 meanR:0.9300 R:1.0 loss:0.0478 exploreP:0.0340\n",
      "Episode:124 meanR:0.9200 R:-1.0 loss:0.0738 exploreP:0.0333\n",
      "Episode:125 meanR:0.9400 R:2.0 loss:0.0525 exploreP:0.0326\n",
      "Episode:126 meanR:0.9300 R:-1.0 loss:0.0678 exploreP:0.0319\n",
      "Episode:127 meanR:0.9500 R:0.0 loss:0.0588 exploreP:0.0313\n",
      "Episode:128 meanR:1.0000 R:4.0 loss:0.0328 exploreP:0.0306\n",
      "Episode:129 meanR:1.0400 R:4.0 loss:0.0549 exploreP:0.0300\n",
      "Episode:130 meanR:1.0400 R:0.0 loss:0.1387 exploreP:0.0294\n",
      "Episode:131 meanR:1.0600 R:2.0 loss:0.0426 exploreP:0.0289\n",
      "Episode:132 meanR:1.0900 R:2.0 loss:0.0427 exploreP:0.0283\n",
      "Episode:133 meanR:1.0500 R:2.0 loss:0.0458 exploreP:0.0278\n",
      "Episode:134 meanR:1.0700 R:3.0 loss:0.0486 exploreP:0.0272\n",
      "Episode:135 meanR:1.0800 R:1.0 loss:0.0306 exploreP:0.0267\n",
      "Episode:136 meanR:1.0700 R:-2.0 loss:0.0350 exploreP:0.0262\n",
      "Episode:137 meanR:1.1000 R:2.0 loss:0.0727 exploreP:0.0258\n",
      "Episode:138 meanR:1.1600 R:6.0 loss:0.0420 exploreP:0.0253\n",
      "Episode:139 meanR:1.2000 R:5.0 loss:0.0918 exploreP:0.0248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:140 meanR:1.2400 R:4.0 loss:0.1075 exploreP:0.0244\n",
      "Episode:141 meanR:1.2300 R:-1.0 loss:0.0648 exploreP:0.0240\n",
      "Episode:142 meanR:1.2600 R:1.0 loss:0.0496 exploreP:0.0236\n",
      "Episode:143 meanR:1.3400 R:9.0 loss:0.1027 exploreP:0.0232\n",
      "Episode:144 meanR:1.4500 R:11.0 loss:0.1207 exploreP:0.0228\n",
      "Episode:145 meanR:1.5000 R:6.0 loss:0.0684 exploreP:0.0224\n",
      "Episode:146 meanR:1.6200 R:12.0 loss:0.1064 exploreP:0.0220\n",
      "Episode:147 meanR:1.6800 R:7.0 loss:0.1042 exploreP:0.0217\n",
      "Episode:148 meanR:1.6900 R:2.0 loss:0.0773 exploreP:0.0213\n",
      "Episode:149 meanR:1.7200 R:3.0 loss:0.1141 exploreP:0.0210\n",
      "Episode:150 meanR:1.7600 R:4.0 loss:0.0473 exploreP:0.0207\n",
      "Episode:151 meanR:1.7800 R:3.0 loss:0.0456 exploreP:0.0204\n",
      "Episode:152 meanR:1.8500 R:6.0 loss:0.0839 exploreP:0.0201\n",
      "Episode:153 meanR:1.8600 R:1.0 loss:0.0970 exploreP:0.0198\n",
      "Episode:154 meanR:1.9300 R:7.0 loss:0.0742 exploreP:0.0195\n",
      "Episode:155 meanR:1.9500 R:2.0 loss:0.0635 exploreP:0.0192\n",
      "Episode:156 meanR:1.9700 R:4.0 loss:0.0610 exploreP:0.0189\n",
      "Episode:157 meanR:2.0100 R:3.0 loss:0.0738 exploreP:0.0187\n",
      "Episode:158 meanR:2.0600 R:2.0 loss:0.0604 exploreP:0.0184\n",
      "Episode:159 meanR:2.0500 R:2.0 loss:0.0640 exploreP:0.0181\n",
      "Episode:160 meanR:2.1000 R:5.0 loss:0.0584 exploreP:0.0179\n",
      "Episode:161 meanR:2.1100 R:0.0 loss:0.0600 exploreP:0.0177\n",
      "Episode:162 meanR:2.1100 R:2.0 loss:0.0727 exploreP:0.0174\n",
      "Episode:163 meanR:2.0900 R:2.0 loss:0.0556 exploreP:0.0172\n",
      "Episode:164 meanR:2.1300 R:4.0 loss:0.0913 exploreP:0.0170\n",
      "Episode:165 meanR:2.1500 R:3.0 loss:0.0734 exploreP:0.0168\n",
      "Episode:166 meanR:2.1400 R:2.0 loss:0.0364 exploreP:0.0166\n",
      "Episode:167 meanR:2.1600 R:0.0 loss:0.0520 exploreP:0.0164\n",
      "Episode:168 meanR:2.1200 R:1.0 loss:0.0519 exploreP:0.0162\n",
      "Episode:169 meanR:2.0400 R:-1.0 loss:0.0531 exploreP:0.0160\n",
      "Episode:170 meanR:1.9800 R:-2.0 loss:0.0225 exploreP:0.0159\n",
      "Episode:171 meanR:1.9600 R:0.0 loss:0.0136 exploreP:0.0157\n",
      "Episode:172 meanR:2.0100 R:4.0 loss:0.0426 exploreP:0.0155\n",
      "Episode:173 meanR:2.0500 R:4.0 loss:0.0843 exploreP:0.0154\n",
      "Episode:174 meanR:2.0700 R:0.0 loss:0.0991 exploreP:0.0152\n",
      "Episode:175 meanR:2.0200 R:2.0 loss:0.0694 exploreP:0.0150\n",
      "Episode:176 meanR:2.0000 R:0.0 loss:0.0455 exploreP:0.0149\n",
      "Episode:177 meanR:1.9900 R:1.0 loss:0.0652 exploreP:0.0147\n",
      "Episode:178 meanR:2.0600 R:9.0 loss:0.0538 exploreP:0.0146\n",
      "Episode:179 meanR:2.1300 R:6.0 loss:0.1207 exploreP:0.0145\n",
      "Episode:180 meanR:2.1300 R:3.0 loss:0.0392 exploreP:0.0143\n",
      "Episode:181 meanR:2.1300 R:2.0 loss:0.0222 exploreP:0.0142\n",
      "Episode:182 meanR:2.1600 R:4.0 loss:0.0385 exploreP:0.0141\n",
      "Episode:183 meanR:2.1900 R:2.0 loss:0.0491 exploreP:0.0140\n",
      "Episode:184 meanR:2.2300 R:4.0 loss:0.0449 exploreP:0.0138\n",
      "Episode:185 meanR:2.2200 R:2.0 loss:0.0790 exploreP:0.0137\n",
      "Episode:186 meanR:2.2400 R:1.0 loss:0.0558 exploreP:0.0136\n",
      "Episode:187 meanR:2.2200 R:0.0 loss:0.0274 exploreP:0.0135\n",
      "Episode:188 meanR:2.2200 R:2.0 loss:0.0581 exploreP:0.0134\n",
      "Episode:189 meanR:2.3200 R:12.0 loss:0.0992 exploreP:0.0133\n",
      "Episode:190 meanR:2.3500 R:3.0 loss:0.0961 exploreP:0.0132\n",
      "Episode:191 meanR:2.4000 R:5.0 loss:0.0681 exploreP:0.0131\n",
      "Episode:192 meanR:2.4000 R:1.0 loss:0.0866 exploreP:0.0130\n",
      "Episode:193 meanR:2.4100 R:1.0 loss:0.1462 exploreP:0.0129\n",
      "Episode:194 meanR:2.4400 R:5.0 loss:0.0873 exploreP:0.0129\n",
      "Episode:195 meanR:2.4600 R:2.0 loss:0.0689 exploreP:0.0128\n",
      "Episode:196 meanR:2.5000 R:1.0 loss:0.0939 exploreP:0.0127\n",
      "Episode:197 meanR:2.5600 R:8.0 loss:0.0440 exploreP:0.0126\n",
      "Episode:198 meanR:2.5900 R:3.0 loss:0.0622 exploreP:0.0125\n",
      "Episode:199 meanR:2.5600 R:1.0 loss:0.0567 exploreP:0.0125\n",
      "Episode:200 meanR:2.5500 R:-1.0 loss:0.0444 exploreP:0.0124\n",
      "Episode:201 meanR:2.5500 R:0.0 loss:0.0673 exploreP:0.0123\n",
      "Episode:202 meanR:2.5500 R:3.0 loss:0.0549 exploreP:0.0122\n",
      "Episode:203 meanR:2.5400 R:-2.0 loss:0.0614 exploreP:0.0122\n",
      "Episode:204 meanR:2.6000 R:8.0 loss:0.0602 exploreP:0.0121\n",
      "Episode:205 meanR:2.5800 R:0.0 loss:0.0634 exploreP:0.0120\n",
      "Episode:206 meanR:2.6300 R:7.0 loss:0.1000 exploreP:0.0120\n",
      "Episode:207 meanR:2.6100 R:2.0 loss:0.0497 exploreP:0.0119\n",
      "Episode:208 meanR:2.6200 R:1.0 loss:0.0714 exploreP:0.0119\n",
      "Episode:209 meanR:2.6900 R:6.0 loss:0.0831 exploreP:0.0118\n",
      "Episode:210 meanR:2.7100 R:4.0 loss:0.0802 exploreP:0.0118\n",
      "Episode:211 meanR:2.7800 R:9.0 loss:0.1527 exploreP:0.0117\n",
      "Episode:212 meanR:2.7700 R:0.0 loss:0.0877 exploreP:0.0117\n",
      "Episode:213 meanR:2.7900 R:1.0 loss:0.1155 exploreP:0.0116\n",
      "Episode:214 meanR:2.8400 R:4.0 loss:0.0466 exploreP:0.0116\n",
      "Episode:215 meanR:2.8800 R:4.0 loss:0.0531 exploreP:0.0115\n",
      "Episode:216 meanR:2.9000 R:5.0 loss:0.1040 exploreP:0.0115\n",
      "Episode:217 meanR:2.9200 R:4.0 loss:0.0646 exploreP:0.0114\n",
      "Episode:218 meanR:2.9000 R:1.0 loss:0.0673 exploreP:0.0114\n",
      "Episode:219 meanR:2.8900 R:3.0 loss:0.0594 exploreP:0.0113\n",
      "Episode:220 meanR:2.8800 R:3.0 loss:0.0409 exploreP:0.0113\n",
      "Episode:221 meanR:2.8900 R:5.0 loss:0.0529 exploreP:0.0113\n",
      "Episode:222 meanR:2.9100 R:2.0 loss:0.0642 exploreP:0.0112\n",
      "Episode:223 meanR:2.9500 R:5.0 loss:0.0434 exploreP:0.0112\n",
      "Episode:224 meanR:3.0600 R:10.0 loss:0.0815 exploreP:0.0112\n",
      "Episode:225 meanR:3.0800 R:4.0 loss:0.0890 exploreP:0.0111\n",
      "Episode:226 meanR:3.1100 R:2.0 loss:0.1039 exploreP:0.0111\n",
      "Episode:227 meanR:3.1700 R:6.0 loss:0.0785 exploreP:0.0111\n",
      "Episode:228 meanR:3.1500 R:2.0 loss:0.0784 exploreP:0.0110\n",
      "Episode:229 meanR:3.2100 R:10.0 loss:0.0922 exploreP:0.0110\n",
      "Episode:230 meanR:3.2600 R:5.0 loss:0.0981 exploreP:0.0110\n",
      "Episode:231 meanR:3.2700 R:3.0 loss:0.0830 exploreP:0.0109\n",
      "Episode:232 meanR:3.2700 R:2.0 loss:0.0750 exploreP:0.0109\n",
      "Episode:233 meanR:3.3100 R:6.0 loss:0.0769 exploreP:0.0109\n",
      "Episode:234 meanR:3.2800 R:0.0 loss:0.0562 exploreP:0.0109\n",
      "Episode:235 meanR:3.3100 R:4.0 loss:0.0795 exploreP:0.0108\n",
      "Episode:236 meanR:3.3500 R:2.0 loss:0.0529 exploreP:0.0108\n",
      "Episode:237 meanR:3.3300 R:0.0 loss:0.0554 exploreP:0.0108\n",
      "Episode:238 meanR:3.3100 R:4.0 loss:0.0668 exploreP:0.0108\n",
      "Episode:239 meanR:3.3400 R:8.0 loss:0.0938 exploreP:0.0107\n",
      "Episode:240 meanR:3.3200 R:2.0 loss:0.0661 exploreP:0.0107\n",
      "Episode:241 meanR:3.3800 R:5.0 loss:0.0710 exploreP:0.0107\n",
      "Episode:242 meanR:3.3700 R:0.0 loss:0.1121 exploreP:0.0107\n",
      "Episode:243 meanR:3.2800 R:0.0 loss:0.0543 exploreP:0.0107\n",
      "Episode:244 meanR:3.1700 R:0.0 loss:0.0809 exploreP:0.0106\n",
      "Episode:245 meanR:3.1300 R:2.0 loss:0.0530 exploreP:0.0106\n",
      "Episode:246 meanR:3.0500 R:4.0 loss:0.0482 exploreP:0.0106\n",
      "Episode:247 meanR:3.0000 R:2.0 loss:0.0777 exploreP:0.0106\n",
      "Episode:248 meanR:2.9800 R:0.0 loss:0.0697 exploreP:0.0106\n",
      "Episode:249 meanR:3.0000 R:5.0 loss:0.0502 exploreP:0.0105\n",
      "Episode:250 meanR:2.9700 R:1.0 loss:0.0659 exploreP:0.0105\n",
      "Episode:251 meanR:2.9400 R:0.0 loss:0.0312 exploreP:0.0105\n",
      "Episode:252 meanR:2.9100 R:3.0 loss:0.0484 exploreP:0.0105\n",
      "Episode:253 meanR:2.9100 R:1.0 loss:0.0685 exploreP:0.0105\n",
      "Episode:254 meanR:2.8600 R:2.0 loss:0.0376 exploreP:0.0105\n",
      "Episode:255 meanR:2.8700 R:3.0 loss:0.0539 exploreP:0.0105\n",
      "Episode:256 meanR:2.8300 R:0.0 loss:0.0423 exploreP:0.0104\n",
      "Episode:257 meanR:2.8300 R:3.0 loss:0.0337 exploreP:0.0104\n",
      "Episode:258 meanR:2.8400 R:3.0 loss:0.0494 exploreP:0.0104\n",
      "Episode:259 meanR:2.8500 R:3.0 loss:0.0642 exploreP:0.0104\n",
      "Episode:260 meanR:2.8400 R:4.0 loss:0.0969 exploreP:0.0104\n",
      "Episode:261 meanR:2.8800 R:4.0 loss:0.1196 exploreP:0.0104\n",
      "Episode:262 meanR:2.9300 R:7.0 loss:0.0841 exploreP:0.0104\n",
      "Episode:263 meanR:2.9600 R:5.0 loss:0.0616 exploreP:0.0104\n",
      "Episode:264 meanR:2.9700 R:5.0 loss:0.0732 exploreP:0.0103\n",
      "Episode:265 meanR:2.9600 R:2.0 loss:0.0345 exploreP:0.0103\n",
      "Episode:266 meanR:2.9700 R:3.0 loss:0.0356 exploreP:0.0103\n",
      "Episode:267 meanR:2.9800 R:1.0 loss:0.0431 exploreP:0.0103\n",
      "Episode:268 meanR:2.9900 R:2.0 loss:0.0541 exploreP:0.0103\n",
      "Episode:269 meanR:3.0000 R:0.0 loss:0.0283 exploreP:0.0103\n",
      "Episode:270 meanR:3.0400 R:2.0 loss:0.0226 exploreP:0.0103\n",
      "Episode:271 meanR:3.0600 R:2.0 loss:0.0281 exploreP:0.0103\n",
      "Episode:272 meanR:3.0800 R:6.0 loss:0.0546 exploreP:0.0103\n",
      "Episode:273 meanR:3.1300 R:9.0 loss:0.0595 exploreP:0.0103\n",
      "Episode:274 meanR:3.2100 R:8.0 loss:0.0877 exploreP:0.0103\n",
      "Episode:275 meanR:3.2200 R:3.0 loss:0.1288 exploreP:0.0103\n",
      "Episode:276 meanR:3.2300 R:1.0 loss:0.1135 exploreP:0.0102\n",
      "Episode:277 meanR:3.2400 R:2.0 loss:0.0539 exploreP:0.0102\n",
      "Episode:278 meanR:3.2200 R:7.0 loss:0.0670 exploreP:0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:279 meanR:3.1700 R:1.0 loss:0.1076 exploreP:0.0102\n",
      "Episode:280 meanR:3.2100 R:7.0 loss:0.0783 exploreP:0.0102\n",
      "Episode:281 meanR:3.2000 R:1.0 loss:0.0532 exploreP:0.0102\n",
      "Episode:282 meanR:3.1600 R:0.0 loss:0.0410 exploreP:0.0102\n",
      "Episode:283 meanR:3.1400 R:0.0 loss:0.0449 exploreP:0.0102\n",
      "Episode:284 meanR:3.0800 R:-2.0 loss:0.0515 exploreP:0.0102\n",
      "Episode:285 meanR:3.0800 R:2.0 loss:0.0527 exploreP:0.0102\n",
      "Episode:286 meanR:3.1000 R:3.0 loss:0.1355 exploreP:0.0102\n",
      "Episode:287 meanR:3.0900 R:-1.0 loss:0.1453 exploreP:0.0102\n",
      "Episode:288 meanR:3.1000 R:3.0 loss:0.0908 exploreP:0.0102\n",
      "Episode:289 meanR:2.9900 R:1.0 loss:0.0693 exploreP:0.0102\n",
      "Episode:290 meanR:3.0100 R:5.0 loss:0.0457 exploreP:0.0102\n",
      "Episode:291 meanR:2.9600 R:0.0 loss:0.0421 exploreP:0.0102\n",
      "Episode:292 meanR:2.9500 R:0.0 loss:0.0547 exploreP:0.0102\n",
      "Episode:293 meanR:2.9300 R:-1.0 loss:0.0323 exploreP:0.0101\n",
      "Episode:294 meanR:2.9100 R:3.0 loss:0.0413 exploreP:0.0101\n",
      "Episode:295 meanR:2.9100 R:2.0 loss:0.0509 exploreP:0.0101\n",
      "Episode:296 meanR:2.9100 R:1.0 loss:0.0407 exploreP:0.0101\n",
      "Episode:297 meanR:2.8800 R:5.0 loss:0.0268 exploreP:0.0101\n",
      "Episode:298 meanR:2.9100 R:6.0 loss:0.0428 exploreP:0.0101\n",
      "Episode:299 meanR:2.9300 R:3.0 loss:0.0947 exploreP:0.0101\n",
      "Episode:300 meanR:2.9700 R:3.0 loss:0.0937 exploreP:0.0101\n",
      "Episode:301 meanR:2.9800 R:1.0 loss:0.0963 exploreP:0.0101\n",
      "Episode:302 meanR:3.0000 R:5.0 loss:0.0943 exploreP:0.0101\n",
      "Episode:303 meanR:3.0400 R:2.0 loss:0.0397 exploreP:0.0101\n",
      "Episode:304 meanR:2.9700 R:1.0 loss:0.0511 exploreP:0.0101\n",
      "Episode:305 meanR:3.0100 R:4.0 loss:0.0538 exploreP:0.0101\n",
      "Episode:306 meanR:3.0200 R:8.0 loss:0.1228 exploreP:0.0101\n",
      "Episode:307 meanR:3.0300 R:3.0 loss:0.1222 exploreP:0.0101\n",
      "Episode:308 meanR:3.0600 R:4.0 loss:0.2130 exploreP:0.0101\n",
      "Episode:309 meanR:3.0200 R:2.0 loss:0.0892 exploreP:0.0101\n",
      "Episode:310 meanR:3.0200 R:4.0 loss:0.0664 exploreP:0.0101\n",
      "Episode:311 meanR:2.9800 R:5.0 loss:0.0615 exploreP:0.0101\n",
      "Episode:312 meanR:3.0000 R:2.0 loss:0.0503 exploreP:0.0101\n",
      "Episode:313 meanR:3.0200 R:3.0 loss:0.0877 exploreP:0.0101\n",
      "Episode:314 meanR:3.0300 R:5.0 loss:0.0773 exploreP:0.0101\n",
      "Episode:315 meanR:3.0200 R:3.0 loss:0.0675 exploreP:0.0101\n",
      "Episode:316 meanR:3.0800 R:11.0 loss:0.0940 exploreP:0.0101\n",
      "Episode:317 meanR:3.1000 R:6.0 loss:0.1469 exploreP:0.0101\n",
      "Episode:318 meanR:3.1500 R:6.0 loss:0.1167 exploreP:0.0101\n",
      "Episode:319 meanR:3.1600 R:4.0 loss:0.0827 exploreP:0.0101\n",
      "Episode:320 meanR:3.1600 R:3.0 loss:0.0756 exploreP:0.0101\n",
      "Episode:321 meanR:3.1500 R:4.0 loss:0.1280 exploreP:0.0101\n",
      "Episode:322 meanR:3.2100 R:8.0 loss:0.0559 exploreP:0.0101\n",
      "Episode:323 meanR:3.1800 R:2.0 loss:0.0729 exploreP:0.0101\n",
      "Episode:324 meanR:3.1500 R:7.0 loss:0.0898 exploreP:0.0101\n",
      "Episode:325 meanR:3.1800 R:7.0 loss:0.0450 exploreP:0.0101\n",
      "Episode:326 meanR:3.2100 R:5.0 loss:0.0910 exploreP:0.0101\n",
      "Episode:327 meanR:3.2200 R:7.0 loss:0.1110 exploreP:0.0101\n",
      "Episode:328 meanR:3.2100 R:1.0 loss:0.0821 exploreP:0.0101\n",
      "Episode:329 meanR:3.1300 R:2.0 loss:0.0502 exploreP:0.0100\n",
      "Episode:330 meanR:3.1000 R:2.0 loss:0.0701 exploreP:0.0100\n",
      "Episode:331 meanR:3.0800 R:1.0 loss:0.0719 exploreP:0.0100\n",
      "Episode:332 meanR:3.0700 R:1.0 loss:0.0527 exploreP:0.0100\n",
      "Episode:333 meanR:3.0300 R:2.0 loss:0.0721 exploreP:0.0100\n",
      "Episode:334 meanR:3.0200 R:-1.0 loss:0.0808 exploreP:0.0100\n",
      "Episode:335 meanR:2.9900 R:1.0 loss:0.0279 exploreP:0.0100\n",
      "Episode:336 meanR:3.0000 R:3.0 loss:0.0950 exploreP:0.0100\n",
      "Episode:337 meanR:3.0600 R:6.0 loss:0.0949 exploreP:0.0100\n",
      "Episode:338 meanR:3.0900 R:7.0 loss:0.0939 exploreP:0.0100\n",
      "Episode:339 meanR:3.0500 R:4.0 loss:0.0679 exploreP:0.0100\n",
      "Episode:340 meanR:3.0600 R:3.0 loss:0.0940 exploreP:0.0100\n",
      "Episode:341 meanR:3.0400 R:3.0 loss:0.0414 exploreP:0.0100\n",
      "Episode:342 meanR:3.0800 R:4.0 loss:0.0959 exploreP:0.0100\n",
      "Episode:343 meanR:3.1300 R:5.0 loss:0.0698 exploreP:0.0100\n",
      "Episode:344 meanR:3.1500 R:2.0 loss:0.0492 exploreP:0.0100\n",
      "Episode:345 meanR:3.1500 R:2.0 loss:0.0677 exploreP:0.0100\n",
      "Episode:346 meanR:3.1800 R:7.0 loss:0.0412 exploreP:0.0100\n",
      "Episode:347 meanR:3.2100 R:5.0 loss:0.0718 exploreP:0.0100\n",
      "Episode:348 meanR:3.2600 R:5.0 loss:0.0563 exploreP:0.0100\n",
      "Episode:349 meanR:3.2900 R:8.0 loss:0.0535 exploreP:0.0100\n",
      "Episode:350 meanR:3.3500 R:7.0 loss:0.0848 exploreP:0.0100\n",
      "Episode:351 meanR:3.4200 R:7.0 loss:0.0464 exploreP:0.0100\n",
      "Episode:352 meanR:3.4000 R:1.0 loss:0.0376 exploreP:0.0100\n",
      "Episode:353 meanR:3.4000 R:1.0 loss:0.0424 exploreP:0.0100\n",
      "Episode:354 meanR:3.3800 R:0.0 loss:0.0369 exploreP:0.0100\n",
      "Episode:355 meanR:3.3800 R:3.0 loss:0.0312 exploreP:0.0100\n",
      "Episode:356 meanR:3.4400 R:6.0 loss:0.0954 exploreP:0.0100\n",
      "Episode:357 meanR:3.4900 R:8.0 loss:0.0878 exploreP:0.0100\n",
      "Episode:358 meanR:3.4700 R:1.0 loss:0.0600 exploreP:0.0100\n",
      "Episode:359 meanR:3.4700 R:3.0 loss:0.0380 exploreP:0.0100\n",
      "Episode:360 meanR:3.5100 R:8.0 loss:0.0918 exploreP:0.0100\n",
      "Episode:361 meanR:3.5400 R:7.0 loss:0.1490 exploreP:0.0100\n",
      "Episode:362 meanR:3.5200 R:5.0 loss:0.0886 exploreP:0.0100\n",
      "Episode:363 meanR:3.4900 R:2.0 loss:0.0473 exploreP:0.0100\n",
      "Episode:364 meanR:3.4500 R:1.0 loss:0.0765 exploreP:0.0100\n",
      "Episode:365 meanR:3.4400 R:1.0 loss:0.0727 exploreP:0.0100\n",
      "Episode:366 meanR:3.4200 R:1.0 loss:0.0691 exploreP:0.0100\n",
      "Episode:367 meanR:3.4300 R:2.0 loss:0.0593 exploreP:0.0100\n",
      "Episode:368 meanR:3.4100 R:0.0 loss:0.0369 exploreP:0.0100\n",
      "Episode:369 meanR:3.4000 R:-1.0 loss:0.0391 exploreP:0.0100\n",
      "Episode:370 meanR:3.3600 R:-2.0 loss:0.0324 exploreP:0.0100\n",
      "Episode:371 meanR:3.3500 R:1.0 loss:0.0369 exploreP:0.0100\n",
      "Episode:372 meanR:3.3700 R:8.0 loss:0.0345 exploreP:0.0100\n",
      "Episode:373 meanR:3.3400 R:6.0 loss:0.0566 exploreP:0.0100\n",
      "Episode:374 meanR:3.3300 R:7.0 loss:0.0909 exploreP:0.0100\n",
      "Episode:375 meanR:3.3700 R:7.0 loss:0.1070 exploreP:0.0100\n",
      "Episode:376 meanR:3.4000 R:4.0 loss:0.0655 exploreP:0.0100\n",
      "Episode:377 meanR:3.4100 R:3.0 loss:0.0591 exploreP:0.0100\n",
      "Episode:378 meanR:3.3600 R:2.0 loss:0.0450 exploreP:0.0100\n",
      "Episode:379 meanR:3.3400 R:-1.0 loss:0.0349 exploreP:0.0100\n",
      "Episode:380 meanR:3.2700 R:0.0 loss:0.0409 exploreP:0.0100\n",
      "Episode:381 meanR:3.2500 R:-1.0 loss:0.0428 exploreP:0.0100\n",
      "Episode:382 meanR:3.2600 R:1.0 loss:0.0258 exploreP:0.0100\n",
      "Episode:383 meanR:3.2700 R:1.0 loss:0.0470 exploreP:0.0100\n",
      "Episode:384 meanR:3.3000 R:1.0 loss:0.0228 exploreP:0.0100\n",
      "Episode:385 meanR:3.3000 R:2.0 loss:0.0237 exploreP:0.0100\n",
      "Episode:386 meanR:3.2700 R:0.0 loss:0.0380 exploreP:0.0100\n",
      "Episode:387 meanR:3.2700 R:-1.0 loss:0.0156 exploreP:0.0100\n",
      "Episode:388 meanR:3.2600 R:2.0 loss:0.0331 exploreP:0.0100\n",
      "Episode:389 meanR:3.2800 R:3.0 loss:0.0254 exploreP:0.0100\n",
      "Episode:390 meanR:3.2300 R:0.0 loss:0.0381 exploreP:0.0100\n",
      "Episode:391 meanR:3.2400 R:1.0 loss:0.0256 exploreP:0.0100\n",
      "Episode:392 meanR:3.2400 R:0.0 loss:0.0749 exploreP:0.0100\n",
      "Episode:393 meanR:3.2600 R:1.0 loss:0.0601 exploreP:0.0100\n",
      "Episode:394 meanR:3.2400 R:1.0 loss:0.0343 exploreP:0.0100\n",
      "Episode:395 meanR:3.2200 R:0.0 loss:0.0313 exploreP:0.0100\n",
      "Episode:396 meanR:3.2200 R:1.0 loss:0.0168 exploreP:0.0100\n",
      "Episode:397 meanR:3.1800 R:1.0 loss:0.0256 exploreP:0.0100\n",
      "Episode:398 meanR:3.1500 R:3.0 loss:0.0196 exploreP:0.0100\n",
      "Episode:399 meanR:3.1100 R:-1.0 loss:0.0457 exploreP:0.0100\n",
      "Episode:400 meanR:3.0800 R:0.0 loss:0.0128 exploreP:0.0100\n",
      "Episode:401 meanR:3.0800 R:1.0 loss:0.0371 exploreP:0.0100\n",
      "Episode:402 meanR:3.0300 R:0.0 loss:0.0422 exploreP:0.0100\n",
      "Episode:403 meanR:3.0400 R:3.0 loss:0.0306 exploreP:0.0100\n",
      "Episode:404 meanR:3.0300 R:0.0 loss:0.0109 exploreP:0.0100\n",
      "Episode:405 meanR:3.0200 R:3.0 loss:0.0265 exploreP:0.0100\n",
      "Episode:406 meanR:2.9700 R:3.0 loss:0.0538 exploreP:0.0100\n",
      "Episode:407 meanR:3.0000 R:6.0 loss:0.0360 exploreP:0.0100\n",
      "Episode:408 meanR:3.0000 R:4.0 loss:0.0504 exploreP:0.0100\n",
      "Episode:409 meanR:3.0000 R:2.0 loss:0.0438 exploreP:0.0100\n",
      "Episode:410 meanR:3.0500 R:9.0 loss:0.0569 exploreP:0.0100\n",
      "Episode:411 meanR:3.0200 R:2.0 loss:0.0608 exploreP:0.0100\n",
      "Episode:412 meanR:3.0000 R:0.0 loss:0.0714 exploreP:0.0100\n",
      "Episode:413 meanR:3.0200 R:5.0 loss:0.0917 exploreP:0.0100\n",
      "Episode:414 meanR:3.0100 R:4.0 loss:0.0698 exploreP:0.0100\n",
      "Episode:415 meanR:3.0100 R:3.0 loss:0.0444 exploreP:0.0100\n",
      "Episode:416 meanR:2.8900 R:-1.0 loss:0.0509 exploreP:0.0100\n",
      "Episode:417 meanR:2.8800 R:5.0 loss:0.0380 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:418 meanR:2.8600 R:4.0 loss:0.0544 exploreP:0.0100\n",
      "Episode:419 meanR:2.8900 R:7.0 loss:0.0570 exploreP:0.0100\n",
      "Episode:420 meanR:2.9500 R:9.0 loss:0.0622 exploreP:0.0100\n",
      "Episode:421 meanR:2.9900 R:8.0 loss:0.0828 exploreP:0.0100\n",
      "Episode:422 meanR:2.9900 R:8.0 loss:0.0718 exploreP:0.0100\n",
      "Episode:423 meanR:3.0000 R:3.0 loss:0.0609 exploreP:0.0100\n",
      "Episode:424 meanR:2.9700 R:4.0 loss:0.0622 exploreP:0.0100\n",
      "Episode:425 meanR:2.9400 R:4.0 loss:0.0933 exploreP:0.0100\n",
      "Episode:426 meanR:2.9200 R:3.0 loss:0.0635 exploreP:0.0100\n",
      "Episode:427 meanR:2.8600 R:1.0 loss:0.0645 exploreP:0.0100\n",
      "Episode:428 meanR:2.9100 R:6.0 loss:0.0507 exploreP:0.0100\n",
      "Episode:429 meanR:2.9200 R:3.0 loss:0.0653 exploreP:0.0100\n",
      "Episode:430 meanR:2.9500 R:5.0 loss:0.0807 exploreP:0.0100\n",
      "Episode:431 meanR:3.0200 R:8.0 loss:0.0673 exploreP:0.0100\n",
      "Episode:432 meanR:3.0700 R:6.0 loss:0.1006 exploreP:0.0100\n",
      "Episode:433 meanR:3.0800 R:3.0 loss:0.0597 exploreP:0.0100\n",
      "Episode:434 meanR:3.1100 R:2.0 loss:0.0523 exploreP:0.0100\n",
      "Episode:435 meanR:3.1800 R:8.0 loss:0.0501 exploreP:0.0100\n",
      "Episode:436 meanR:3.1600 R:1.0 loss:0.0443 exploreP:0.0100\n",
      "Episode:437 meanR:3.1600 R:6.0 loss:0.0416 exploreP:0.0100\n",
      "Episode:438 meanR:3.1300 R:4.0 loss:0.0701 exploreP:0.0100\n",
      "Episode:439 meanR:3.1500 R:6.0 loss:0.0542 exploreP:0.0100\n",
      "Episode:440 meanR:3.1500 R:3.0 loss:0.0407 exploreP:0.0100\n",
      "Episode:441 meanR:3.1400 R:2.0 loss:0.1091 exploreP:0.0100\n",
      "Episode:442 meanR:3.1400 R:4.0 loss:0.1018 exploreP:0.0100\n",
      "Episode:443 meanR:3.1500 R:6.0 loss:0.0534 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "            \n",
    "            # Training\n",
    "            #batch, rnn_states = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            next_actions_logits = sess.run(model.actions_logits, \n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs,\n",
    "                                                        model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-nav-seq.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(1):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful!!!!!!!!!!!!!!!!\n",
    "# Closing the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
