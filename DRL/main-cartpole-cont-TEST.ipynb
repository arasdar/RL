{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.921946114031699 -2.9887773478448816\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44, 6), (44, 4), (44,), (44, 4), (44,), (44,), (44,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:22.0000 R:22.0000 rate:0.0440 gloss:0.7338 dlossA:0.7371 dlossQ:0.9349 exploreP:0.9978\n",
      "Episode:1 meanR:17.0000 R:12.0000 rate:0.0240 gloss:0.7302 dlossA:0.7367 dlossQ:0.9338 exploreP:0.9966\n",
      "Episode:2 meanR:15.3333 R:12.0000 rate:0.0240 gloss:0.7155 dlossA:0.7258 dlossQ:0.9416 exploreP:0.9955\n",
      "Episode:3 meanR:18.2500 R:27.0000 rate:0.0540 gloss:0.7244 dlossA:0.7297 dlossQ:0.9362 exploreP:0.9928\n",
      "Episode:4 meanR:17.8000 R:16.0000 rate:0.0320 gloss:0.7269 dlossA:0.7324 dlossQ:0.9449 exploreP:0.9912\n",
      "Episode:5 meanR:17.6667 R:17.0000 rate:0.0340 gloss:0.7361 dlossA:0.7378 dlossQ:0.9415 exploreP:0.9896\n",
      "Episode:6 meanR:17.5714 R:17.0000 rate:0.0340 gloss:0.7374 dlossA:0.7398 dlossQ:0.9395 exploreP:0.9879\n",
      "Episode:7 meanR:19.2500 R:31.0000 rate:0.0620 gloss:0.7441 dlossA:0.7372 dlossQ:0.9442 exploreP:0.9849\n",
      "Episode:8 meanR:19.2222 R:19.0000 rate:0.0380 gloss:0.7223 dlossA:0.7327 dlossQ:0.9352 exploreP:0.9830\n",
      "Episode:9 meanR:19.2000 R:19.0000 rate:0.0380 gloss:0.7236 dlossA:0.7283 dlossQ:0.9545 exploreP:0.9812\n",
      "Episode:10 meanR:18.6364 R:13.0000 rate:0.0260 gloss:0.7247 dlossA:0.7337 dlossQ:0.9432 exploreP:0.9799\n",
      "Episode:11 meanR:18.3333 R:15.0000 rate:0.0300 gloss:0.7247 dlossA:0.7372 dlossQ:0.9320 exploreP:0.9785\n",
      "Episode:12 meanR:20.1538 R:42.0000 rate:0.0840 gloss:0.7279 dlossA:0.7397 dlossQ:0.9261 exploreP:0.9744\n",
      "Episode:13 meanR:19.6429 R:13.0000 rate:0.0260 gloss:0.7347 dlossA:0.7371 dlossQ:0.9391 exploreP:0.9731\n",
      "Episode:14 meanR:19.7333 R:21.0000 rate:0.0420 gloss:0.7268 dlossA:0.7371 dlossQ:0.9355 exploreP:0.9711\n",
      "Episode:15 meanR:19.8125 R:21.0000 rate:0.0420 gloss:0.7303 dlossA:0.7323 dlossQ:0.9526 exploreP:0.9691\n",
      "Episode:16 meanR:20.2353 R:27.0000 rate:0.0540 gloss:0.7292 dlossA:0.7360 dlossQ:0.9455 exploreP:0.9665\n",
      "Episode:17 meanR:20.0000 R:16.0000 rate:0.0320 gloss:0.7177 dlossA:0.7354 dlossQ:0.9207 exploreP:0.9650\n",
      "Episode:18 meanR:19.8947 R:18.0000 rate:0.0360 gloss:0.7209 dlossA:0.7330 dlossQ:0.9394 exploreP:0.9633\n",
      "Episode:19 meanR:19.8500 R:19.0000 rate:0.0380 gloss:0.7255 dlossA:0.7300 dlossQ:0.9535 exploreP:0.9615\n",
      "Episode:20 meanR:19.7619 R:18.0000 rate:0.0360 gloss:0.7218 dlossA:0.7321 dlossQ:0.9442 exploreP:0.9598\n",
      "Episode:21 meanR:19.9091 R:23.0000 rate:0.0460 gloss:0.7168 dlossA:0.7268 dlossQ:0.9464 exploreP:0.9576\n",
      "Episode:22 meanR:21.2174 R:50.0000 rate:0.1000 gloss:0.7259 dlossA:0.7309 dlossQ:0.9496 exploreP:0.9528\n",
      "Episode:23 meanR:20.7917 R:11.0000 rate:0.0220 gloss:0.7289 dlossA:0.7404 dlossQ:0.9454 exploreP:0.9518\n",
      "Episode:24 meanR:20.9200 R:24.0000 rate:0.0480 gloss:0.7167 dlossA:0.7285 dlossQ:0.9479 exploreP:0.9496\n",
      "Episode:25 meanR:20.8462 R:19.0000 rate:0.0380 gloss:0.7229 dlossA:0.7346 dlossQ:0.9406 exploreP:0.9478\n",
      "Episode:26 meanR:24.1481 R:110.0000 rate:0.2200 gloss:0.7247 dlossA:0.7357 dlossQ:0.9409 exploreP:0.9375\n",
      "Episode:27 meanR:24.4286 R:32.0000 rate:0.0640 gloss:0.7254 dlossA:0.7356 dlossQ:0.9431 exploreP:0.9345\n",
      "Episode:28 meanR:25.4138 R:53.0000 rate:0.1060 gloss:0.7297 dlossA:0.7364 dlossQ:0.9478 exploreP:0.9297\n",
      "Episode:29 meanR:25.0000 R:13.0000 rate:0.0260 gloss:0.7209 dlossA:0.7367 dlossQ:0.9322 exploreP:0.9285\n",
      "Episode:30 meanR:24.6774 R:15.0000 rate:0.0300 gloss:0.7369 dlossA:0.7417 dlossQ:0.9407 exploreP:0.9271\n",
      "Episode:31 meanR:25.2188 R:42.0000 rate:0.0840 gloss:0.7224 dlossA:0.7345 dlossQ:0.9468 exploreP:0.9232\n",
      "Episode:32 meanR:24.9091 R:15.0000 rate:0.0300 gloss:0.7338 dlossA:0.7326 dlossQ:0.9688 exploreP:0.9219\n",
      "Episode:33 meanR:24.5000 R:11.0000 rate:0.0220 gloss:0.7192 dlossA:0.7298 dlossQ:0.9496 exploreP:0.9209\n",
      "Episode:34 meanR:24.1714 R:13.0000 rate:0.0260 gloss:0.7335 dlossA:0.7383 dlossQ:0.9474 exploreP:0.9197\n",
      "Episode:35 meanR:24.0278 R:19.0000 rate:0.0380 gloss:0.7217 dlossA:0.7298 dlossQ:0.9618 exploreP:0.9180\n",
      "Episode:36 meanR:23.8108 R:16.0000 rate:0.0320 gloss:0.7331 dlossA:0.7404 dlossQ:0.9428 exploreP:0.9165\n",
      "Episode:37 meanR:23.6053 R:16.0000 rate:0.0320 gloss:0.7333 dlossA:0.7402 dlossQ:0.9430 exploreP:0.9151\n",
      "Episode:38 meanR:23.5897 R:23.0000 rate:0.0460 gloss:0.7230 dlossA:0.7396 dlossQ:0.9320 exploreP:0.9130\n",
      "Episode:39 meanR:23.7750 R:31.0000 rate:0.0620 gloss:0.7279 dlossA:0.7360 dlossQ:0.9524 exploreP:0.9102\n",
      "Episode:40 meanR:23.5366 R:14.0000 rate:0.0280 gloss:0.7195 dlossA:0.7323 dlossQ:0.9529 exploreP:0.9089\n",
      "Episode:41 meanR:23.3333 R:15.0000 rate:0.0300 gloss:0.7333 dlossA:0.7333 dlossQ:0.9657 exploreP:0.9076\n",
      "Episode:42 meanR:23.4419 R:28.0000 rate:0.0560 gloss:0.7339 dlossA:0.7361 dlossQ:0.9578 exploreP:0.9051\n",
      "Episode:43 meanR:23.5682 R:29.0000 rate:0.0580 gloss:0.7223 dlossA:0.7362 dlossQ:0.9429 exploreP:0.9025\n",
      "Episode:44 meanR:23.5333 R:22.0000 rate:0.0440 gloss:0.7269 dlossA:0.7428 dlossQ:0.9356 exploreP:0.9005\n",
      "Episode:45 meanR:23.3261 R:14.0000 rate:0.0280 gloss:0.7155 dlossA:0.7285 dlossQ:0.9581 exploreP:0.8993\n",
      "Episode:46 meanR:23.2553 R:20.0000 rate:0.0400 gloss:0.7388 dlossA:0.7426 dlossQ:0.9536 exploreP:0.8975\n",
      "Episode:47 meanR:23.3750 R:29.0000 rate:0.0580 gloss:0.7294 dlossA:0.7395 dlossQ:0.9493 exploreP:0.8949\n",
      "Episode:48 meanR:23.5918 R:34.0000 rate:0.0680 gloss:0.7314 dlossA:0.7353 dlossQ:0.9588 exploreP:0.8919\n",
      "Episode:49 meanR:23.3600 R:12.0000 rate:0.0240 gloss:0.7480 dlossA:0.7409 dlossQ:0.9546 exploreP:0.8909\n",
      "Episode:50 meanR:23.2941 R:20.0000 rate:0.0400 gloss:0.7231 dlossA:0.7387 dlossQ:0.9426 exploreP:0.8891\n",
      "Episode:51 meanR:23.2500 R:21.0000 rate:0.0420 gloss:0.7260 dlossA:0.7379 dlossQ:0.9557 exploreP:0.8873\n",
      "Episode:52 meanR:23.9434 R:60.0000 rate:0.1200 gloss:0.7350 dlossA:0.7397 dlossQ:0.9539 exploreP:0.8820\n",
      "Episode:53 meanR:23.9815 R:26.0000 rate:0.0520 gloss:0.7295 dlossA:0.7387 dlossQ:0.9535 exploreP:0.8797\n",
      "Episode:54 meanR:23.9091 R:20.0000 rate:0.0400 gloss:0.7274 dlossA:0.7373 dlossQ:0.9613 exploreP:0.8780\n",
      "Episode:55 meanR:23.7679 R:16.0000 rate:0.0320 gloss:0.7306 dlossA:0.7389 dlossQ:0.9658 exploreP:0.8766\n",
      "Episode:56 meanR:23.6842 R:19.0000 rate:0.0380 gloss:0.7199 dlossA:0.7339 dlossQ:0.9494 exploreP:0.8750\n",
      "Episode:57 meanR:23.6034 R:19.0000 rate:0.0380 gloss:0.7286 dlossA:0.7357 dlossQ:0.9627 exploreP:0.8733\n",
      "Episode:58 meanR:23.4068 R:12.0000 rate:0.0240 gloss:0.7463 dlossA:0.7453 dlossQ:0.9592 exploreP:0.8723\n",
      "Episode:59 meanR:23.2167 R:12.0000 rate:0.0240 gloss:0.7282 dlossA:0.7383 dlossQ:0.9544 exploreP:0.8713\n",
      "Episode:60 meanR:23.2295 R:24.0000 rate:0.0480 gloss:0.7156 dlossA:0.7327 dlossQ:0.9562 exploreP:0.8692\n",
      "Episode:61 meanR:23.6452 R:49.0000 rate:0.0980 gloss:0.7273 dlossA:0.7391 dlossQ:0.9493 exploreP:0.8650\n",
      "Episode:62 meanR:23.4762 R:13.0000 rate:0.0260 gloss:0.7323 dlossA:0.7408 dlossQ:0.9525 exploreP:0.8639\n",
      "Episode:63 meanR:23.6094 R:32.0000 rate:0.0640 gloss:0.7139 dlossA:0.7342 dlossQ:0.9469 exploreP:0.8612\n",
      "Episode:64 meanR:23.8308 R:38.0000 rate:0.0760 gloss:0.7224 dlossA:0.7376 dlossQ:0.9510 exploreP:0.8579\n",
      "Episode:65 meanR:23.7273 R:17.0000 rate:0.0340 gloss:0.7458 dlossA:0.7473 dlossQ:0.9435 exploreP:0.8565\n",
      "Episode:66 meanR:23.9552 R:39.0000 rate:0.0780 gloss:0.7280 dlossA:0.7432 dlossQ:0.9491 exploreP:0.8532\n",
      "Episode:67 meanR:24.2059 R:41.0000 rate:0.0820 gloss:0.7323 dlossA:0.7383 dlossQ:0.9646 exploreP:0.8498\n",
      "Episode:68 meanR:24.0580 R:14.0000 rate:0.0280 gloss:0.7552 dlossA:0.7472 dlossQ:0.9745 exploreP:0.8486\n",
      "Episode:69 meanR:24.2857 R:40.0000 rate:0.0800 gloss:0.7348 dlossA:0.7447 dlossQ:0.9481 exploreP:0.8452\n",
      "Episode:70 meanR:24.1972 R:18.0000 rate:0.0360 gloss:0.7286 dlossA:0.7378 dlossQ:0.9589 exploreP:0.8437\n",
      "Episode:71 meanR:24.3194 R:33.0000 rate:0.0660 gloss:0.7307 dlossA:0.7392 dlossQ:0.9582 exploreP:0.8410\n",
      "Episode:72 meanR:24.6164 R:46.0000 rate:0.0920 gloss:0.7367 dlossA:0.7418 dlossQ:0.9614 exploreP:0.8372\n",
      "Episode:73 meanR:24.7568 R:35.0000 rate:0.0700 gloss:0.7237 dlossA:0.7367 dlossQ:0.9629 exploreP:0.8343\n",
      "Episode:74 meanR:24.9867 R:42.0000 rate:0.0840 gloss:0.7400 dlossA:0.7421 dlossQ:0.9592 exploreP:0.8308\n",
      "Episode:75 meanR:25.0000 R:26.0000 rate:0.0520 gloss:0.7386 dlossA:0.7386 dlossQ:0.9759 exploreP:0.8287\n",
      "Episode:76 meanR:25.1299 R:35.0000 rate:0.0700 gloss:0.7349 dlossA:0.7401 dlossQ:0.9590 exploreP:0.8258\n",
      "Episode:77 meanR:25.3205 R:40.0000 rate:0.0800 gloss:0.7167 dlossA:0.7336 dlossQ:0.9514 exploreP:0.8226\n",
      "Episode:78 meanR:25.2278 R:18.0000 rate:0.0360 gloss:0.7268 dlossA:0.7400 dlossQ:0.9505 exploreP:0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:25.7125 R:64.0000 rate:0.1280 gloss:0.7327 dlossA:0.7435 dlossQ:0.9524 exploreP:0.8159\n",
      "Episode:80 meanR:26.1852 R:64.0000 rate:0.1280 gloss:0.7247 dlossA:0.7383 dlossQ:0.9574 exploreP:0.8108\n",
      "Episode:81 meanR:26.3171 R:37.0000 rate:0.0740 gloss:0.7548 dlossA:0.7455 dlossQ:0.9566 exploreP:0.8078\n",
      "Episode:82 meanR:26.7229 R:60.0000 rate:0.1200 gloss:0.7343 dlossA:0.7418 dlossQ:0.9555 exploreP:0.8031\n",
      "Episode:83 meanR:26.7857 R:32.0000 rate:0.0640 gloss:0.7232 dlossA:0.7363 dlossQ:0.9603 exploreP:0.8005\n",
      "Episode:84 meanR:26.9176 R:38.0000 rate:0.0760 gloss:0.7382 dlossA:0.7425 dlossQ:0.9596 exploreP:0.7975\n",
      "Episode:85 meanR:26.9767 R:32.0000 rate:0.0640 gloss:0.7199 dlossA:0.7334 dlossQ:0.9598 exploreP:0.7950\n",
      "Episode:86 meanR:26.9195 R:22.0000 rate:0.0440 gloss:0.7410 dlossA:0.7434 dlossQ:0.9609 exploreP:0.7933\n",
      "Episode:87 meanR:26.8068 R:17.0000 rate:0.0340 gloss:0.7298 dlossA:0.7377 dlossQ:0.9635 exploreP:0.7920\n",
      "Episode:88 meanR:26.6966 R:17.0000 rate:0.0340 gloss:0.7350 dlossA:0.7489 dlossQ:0.9474 exploreP:0.7906\n",
      "Episode:89 meanR:26.5000 R:9.0000 rate:0.0180 gloss:0.7270 dlossA:0.7331 dlossQ:0.9686 exploreP:0.7899\n",
      "Episode:90 meanR:26.4615 R:23.0000 rate:0.0460 gloss:0.7322 dlossA:0.7395 dlossQ:0.9625 exploreP:0.7881\n",
      "Episode:91 meanR:26.5978 R:39.0000 rate:0.0780 gloss:0.7263 dlossA:0.7406 dlossQ:0.9504 exploreP:0.7851\n",
      "Episode:92 meanR:26.5806 R:25.0000 rate:0.0500 gloss:0.7254 dlossA:0.7395 dlossQ:0.9502 exploreP:0.7832\n",
      "Episode:93 meanR:26.9255 R:59.0000 rate:0.1180 gloss:0.7272 dlossA:0.7391 dlossQ:0.9553 exploreP:0.7786\n",
      "Episode:94 meanR:26.9684 R:31.0000 rate:0.0620 gloss:0.7230 dlossA:0.7385 dlossQ:0.9572 exploreP:0.7762\n",
      "Episode:95 meanR:26.8333 R:14.0000 rate:0.0280 gloss:0.7466 dlossA:0.7364 dlossQ:0.9586 exploreP:0.7752\n",
      "Episode:96 meanR:26.7113 R:15.0000 rate:0.0300 gloss:0.7340 dlossA:0.7395 dlossQ:0.9612 exploreP:0.7740\n",
      "Episode:97 meanR:26.5918 R:15.0000 rate:0.0300 gloss:0.7371 dlossA:0.7437 dlossQ:0.9573 exploreP:0.7729\n",
      "Episode:98 meanR:27.1818 R:85.0000 rate:0.1700 gloss:0.7281 dlossA:0.7402 dlossQ:0.9599 exploreP:0.7664\n",
      "Episode:99 meanR:27.4100 R:50.0000 rate:0.1000 gloss:0.7278 dlossA:0.7406 dlossQ:0.9562 exploreP:0.7627\n",
      "Episode:100 meanR:27.6400 R:45.0000 rate:0.0900 gloss:0.7259 dlossA:0.7391 dlossQ:0.9577 exploreP:0.7593\n",
      "Episode:101 meanR:28.0600 R:54.0000 rate:0.1080 gloss:0.7265 dlossA:0.7378 dlossQ:0.9643 exploreP:0.7552\n",
      "Episode:102 meanR:28.2600 R:32.0000 rate:0.0640 gloss:0.7269 dlossA:0.7398 dlossQ:0.9483 exploreP:0.7529\n",
      "Episode:103 meanR:28.1600 R:17.0000 rate:0.0340 gloss:0.7396 dlossA:0.7412 dlossQ:0.9669 exploreP:0.7516\n",
      "Episode:104 meanR:28.2300 R:23.0000 rate:0.0460 gloss:0.7388 dlossA:0.7465 dlossQ:0.9439 exploreP:0.7499\n",
      "Episode:105 meanR:28.4200 R:36.0000 rate:0.0720 gloss:0.7341 dlossA:0.7408 dlossQ:0.9611 exploreP:0.7472\n",
      "Episode:106 meanR:28.3900 R:14.0000 rate:0.0280 gloss:0.7354 dlossA:0.7486 dlossQ:0.9443 exploreP:0.7462\n",
      "Episode:107 meanR:28.6800 R:60.0000 rate:0.1200 gloss:0.7273 dlossA:0.7385 dlossQ:0.9568 exploreP:0.7418\n",
      "Episode:108 meanR:29.3800 R:89.0000 rate:0.1780 gloss:0.7356 dlossA:0.7440 dlossQ:0.9572 exploreP:0.7353\n",
      "Episode:109 meanR:29.8000 R:61.0000 rate:0.1220 gloss:0.7370 dlossA:0.7432 dlossQ:0.9654 exploreP:0.7309\n",
      "Episode:110 meanR:30.6600 R:99.0000 rate:0.1980 gloss:0.7359 dlossA:0.7445 dlossQ:0.9582 exploreP:0.7238\n",
      "Episode:111 meanR:30.7700 R:26.0000 rate:0.0520 gloss:0.7370 dlossA:0.7406 dlossQ:0.9678 exploreP:0.7219\n",
      "Episode:112 meanR:30.6100 R:26.0000 rate:0.0520 gloss:0.7389 dlossA:0.7454 dlossQ:0.9595 exploreP:0.7201\n",
      "Episode:113 meanR:30.7900 R:31.0000 rate:0.0620 gloss:0.7405 dlossA:0.7399 dlossQ:0.9600 exploreP:0.7179\n",
      "Episode:114 meanR:31.2200 R:64.0000 rate:0.1280 gloss:0.7346 dlossA:0.7398 dlossQ:0.9639 exploreP:0.7134\n",
      "Episode:115 meanR:31.1300 R:12.0000 rate:0.0240 gloss:0.7246 dlossA:0.7390 dlossQ:0.9601 exploreP:0.7125\n",
      "Episode:116 meanR:31.1500 R:29.0000 rate:0.0580 gloss:0.7338 dlossA:0.7434 dlossQ:0.9559 exploreP:0.7105\n",
      "Episode:117 meanR:31.7800 R:79.0000 rate:0.1580 gloss:0.7317 dlossA:0.7419 dlossQ:0.9564 exploreP:0.7050\n",
      "Episode:118 meanR:32.3800 R:78.0000 rate:0.1560 gloss:0.7345 dlossA:0.7421 dlossQ:0.9632 exploreP:0.6996\n",
      "Episode:119 meanR:33.1600 R:97.0000 rate:0.1940 gloss:0.7238 dlossA:0.7382 dlossQ:0.9606 exploreP:0.6929\n",
      "Episode:120 meanR:33.4400 R:46.0000 rate:0.0920 gloss:0.7411 dlossA:0.7476 dlossQ:0.9522 exploreP:0.6898\n",
      "Episode:121 meanR:33.7600 R:55.0000 rate:0.1100 gloss:0.7220 dlossA:0.7352 dlossQ:0.9624 exploreP:0.6861\n",
      "Episode:122 meanR:34.1600 R:90.0000 rate:0.1800 gloss:0.7385 dlossA:0.7432 dlossQ:0.9632 exploreP:0.6800\n",
      "Episode:123 meanR:34.6500 R:60.0000 rate:0.1200 gloss:0.7359 dlossA:0.7448 dlossQ:0.9536 exploreP:0.6760\n",
      "Episode:124 meanR:35.2100 R:80.0000 rate:0.1600 gloss:0.7345 dlossA:0.7422 dlossQ:0.9613 exploreP:0.6707\n",
      "Episode:125 meanR:35.1700 R:15.0000 rate:0.0300 gloss:0.7361 dlossA:0.7486 dlossQ:0.9466 exploreP:0.6697\n",
      "Episode:126 meanR:34.4800 R:41.0000 rate:0.0820 gloss:0.7494 dlossA:0.7465 dlossQ:0.9584 exploreP:0.6670\n",
      "Episode:127 meanR:34.7900 R:63.0000 rate:0.1260 gloss:0.7384 dlossA:0.7424 dlossQ:0.9542 exploreP:0.6629\n",
      "Episode:128 meanR:34.4700 R:21.0000 rate:0.0420 gloss:0.7156 dlossA:0.7368 dlossQ:0.9584 exploreP:0.6615\n",
      "Episode:129 meanR:34.6700 R:33.0000 rate:0.0660 gloss:0.7303 dlossA:0.7386 dlossQ:0.9638 exploreP:0.6594\n",
      "Episode:130 meanR:35.0600 R:54.0000 rate:0.1080 gloss:0.7248 dlossA:0.7374 dlossQ:0.9585 exploreP:0.6559\n",
      "Episode:131 meanR:34.9700 R:33.0000 rate:0.0660 gloss:0.7314 dlossA:0.7442 dlossQ:0.9546 exploreP:0.6537\n",
      "Episode:132 meanR:35.3000 R:48.0000 rate:0.0960 gloss:0.7342 dlossA:0.7453 dlossQ:0.9553 exploreP:0.6507\n",
      "Episode:133 meanR:35.5800 R:39.0000 rate:0.0780 gloss:0.7339 dlossA:0.7423 dlossQ:0.9557 exploreP:0.6482\n",
      "Episode:134 meanR:36.5800 R:113.0000 rate:0.2260 gloss:0.7321 dlossA:0.7432 dlossQ:0.9551 exploreP:0.6410\n",
      "Episode:135 meanR:36.9400 R:55.0000 rate:0.1100 gloss:0.7217 dlossA:0.7376 dlossQ:0.9625 exploreP:0.6375\n",
      "Episode:136 meanR:37.4600 R:68.0000 rate:0.1360 gloss:0.7273 dlossA:0.7385 dlossQ:0.9657 exploreP:0.6333\n",
      "Episode:137 meanR:37.6400 R:34.0000 rate:0.0680 gloss:0.7296 dlossA:0.7416 dlossQ:0.9594 exploreP:0.6312\n",
      "Episode:138 meanR:37.6500 R:24.0000 rate:0.0480 gloss:0.7416 dlossA:0.7452 dlossQ:0.9613 exploreP:0.6297\n",
      "Episode:139 meanR:37.6300 R:29.0000 rate:0.0580 gloss:0.7310 dlossA:0.7425 dlossQ:0.9583 exploreP:0.6279\n",
      "Episode:140 meanR:38.4200 R:93.0000 rate:0.1860 gloss:0.7325 dlossA:0.7432 dlossQ:0.9569 exploreP:0.6222\n",
      "Episode:141 meanR:39.4700 R:120.0000 rate:0.2400 gloss:0.7369 dlossA:0.7441 dlossQ:0.9565 exploreP:0.6149\n",
      "Episode:142 meanR:39.7600 R:57.0000 rate:0.1140 gloss:0.7250 dlossA:0.7385 dlossQ:0.9602 exploreP:0.6114\n",
      "Episode:143 meanR:39.6400 R:17.0000 rate:0.0340 gloss:0.7434 dlossA:0.7427 dlossQ:0.9592 exploreP:0.6104\n",
      "Episode:144 meanR:40.2200 R:80.0000 rate:0.1600 gloss:0.7281 dlossA:0.7405 dlossQ:0.9593 exploreP:0.6056\n",
      "Episode:145 meanR:40.7600 R:68.0000 rate:0.1360 gloss:0.7367 dlossA:0.7444 dlossQ:0.9609 exploreP:0.6016\n",
      "Episode:146 meanR:41.1800 R:62.0000 rate:0.1240 gloss:0.7384 dlossA:0.7440 dlossQ:0.9634 exploreP:0.5979\n",
      "Episode:147 meanR:41.1900 R:30.0000 rate:0.0600 gloss:0.7351 dlossA:0.7415 dlossQ:0.9666 exploreP:0.5962\n",
      "Episode:148 meanR:41.1700 R:32.0000 rate:0.0640 gloss:0.7326 dlossA:0.7431 dlossQ:0.9622 exploreP:0.5943\n",
      "Episode:149 meanR:41.9500 R:90.0000 rate:0.1800 gloss:0.7340 dlossA:0.7409 dlossQ:0.9610 exploreP:0.5891\n",
      "Episode:150 meanR:42.6300 R:88.0000 rate:0.1760 gloss:0.7395 dlossA:0.7445 dlossQ:0.9595 exploreP:0.5840\n",
      "Episode:151 meanR:43.7400 R:132.0000 rate:0.2640 gloss:0.7266 dlossA:0.7413 dlossQ:0.9566 exploreP:0.5765\n",
      "Episode:152 meanR:44.0300 R:89.0000 rate:0.1780 gloss:0.7336 dlossA:0.7438 dlossQ:0.9619 exploreP:0.5714\n",
      "Episode:153 meanR:44.5600 R:79.0000 rate:0.1580 gloss:0.7258 dlossA:0.7396 dlossQ:0.9605 exploreP:0.5670\n",
      "Episode:154 meanR:44.9400 R:58.0000 rate:0.1160 gloss:0.7328 dlossA:0.7415 dlossQ:0.9622 exploreP:0.5638\n",
      "Episode:155 meanR:45.1400 R:36.0000 rate:0.0720 gloss:0.7292 dlossA:0.7421 dlossQ:0.9614 exploreP:0.5618\n",
      "Episode:156 meanR:45.6100 R:66.0000 rate:0.1320 gloss:0.7400 dlossA:0.7461 dlossQ:0.9617 exploreP:0.5582\n",
      "Episode:157 meanR:46.1900 R:77.0000 rate:0.1540 gloss:0.7408 dlossA:0.7465 dlossQ:0.9590 exploreP:0.5540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:46.9800 R:91.0000 rate:0.1820 gloss:0.7439 dlossA:0.7485 dlossQ:0.9592 exploreP:0.5490\n",
      "Episode:159 meanR:47.1200 R:26.0000 rate:0.0520 gloss:0.7440 dlossA:0.7469 dlossQ:0.9637 exploreP:0.5476\n",
      "Episode:160 meanR:47.5100 R:63.0000 rate:0.1260 gloss:0.7432 dlossA:0.7466 dlossQ:0.9605 exploreP:0.5443\n",
      "Episode:161 meanR:48.0300 R:101.0000 rate:0.2020 gloss:0.7367 dlossA:0.7464 dlossQ:0.9605 exploreP:0.5389\n",
      "Episode:162 meanR:48.3700 R:47.0000 rate:0.0940 gloss:0.7379 dlossA:0.7435 dlossQ:0.9676 exploreP:0.5364\n",
      "Episode:163 meanR:49.2800 R:123.0000 rate:0.2460 gloss:0.7307 dlossA:0.7425 dlossQ:0.9654 exploreP:0.5300\n",
      "Episode:164 meanR:49.4500 R:55.0000 rate:0.1100 gloss:0.7208 dlossA:0.7363 dlossQ:0.9669 exploreP:0.5271\n",
      "Episode:165 meanR:50.9100 R:163.0000 rate:0.3260 gloss:0.7372 dlossA:0.7465 dlossQ:0.9609 exploreP:0.5188\n",
      "Episode:166 meanR:51.5900 R:107.0000 rate:0.2140 gloss:0.7365 dlossA:0.7451 dlossQ:0.9597 exploreP:0.5134\n",
      "Episode:167 meanR:54.8600 R:368.0000 rate:0.7360 gloss:0.7428 dlossA:0.7474 dlossQ:0.9640 exploreP:0.4952\n",
      "Episode:168 meanR:56.2100 R:149.0000 rate:0.2980 gloss:0.7399 dlossA:0.7456 dlossQ:0.9669 exploreP:0.4880\n",
      "Episode:169 meanR:56.8700 R:106.0000 rate:0.2120 gloss:0.7413 dlossA:0.7461 dlossQ:0.9661 exploreP:0.4830\n",
      "Episode:170 meanR:57.8300 R:114.0000 rate:0.2280 gloss:0.7392 dlossA:0.7467 dlossQ:0.9630 exploreP:0.4776\n",
      "Episode:171 meanR:57.6800 R:18.0000 rate:0.0360 gloss:0.7521 dlossA:0.7552 dlossQ:0.9668 exploreP:0.4768\n",
      "Episode:172 meanR:58.5400 R:132.0000 rate:0.2640 gloss:0.7390 dlossA:0.7470 dlossQ:0.9662 exploreP:0.4706\n",
      "Episode:173 meanR:59.7000 R:151.0000 rate:0.3020 gloss:0.7496 dlossA:0.7488 dlossQ:0.9663 exploreP:0.4637\n",
      "Episode:174 meanR:60.8200 R:154.0000 rate:0.3080 gloss:0.7510 dlossA:0.7500 dlossQ:0.9643 exploreP:0.4568\n",
      "Episode:175 meanR:61.5700 R:101.0000 rate:0.2020 gloss:0.7447 dlossA:0.7458 dlossQ:0.9643 exploreP:0.4523\n",
      "Episode:176 meanR:63.1000 R:188.0000 rate:0.3760 gloss:0.7514 dlossA:0.7509 dlossQ:0.9640 exploreP:0.4441\n",
      "Episode:177 meanR:64.9100 R:221.0000 rate:0.4420 gloss:0.7532 dlossA:0.7550 dlossQ:0.9618 exploreP:0.4346\n",
      "Episode:178 meanR:66.9400 R:221.0000 rate:0.4420 gloss:0.7558 dlossA:0.7531 dlossQ:0.9648 exploreP:0.4253\n",
      "Episode:179 meanR:67.1500 R:85.0000 rate:0.1700 gloss:0.7506 dlossA:0.7517 dlossQ:0.9643 exploreP:0.4218\n",
      "Episode:180 meanR:67.3200 R:81.0000 rate:0.1620 gloss:0.7660 dlossA:0.7584 dlossQ:0.9588 exploreP:0.4185\n",
      "Episode:181 meanR:68.5900 R:164.0000 rate:0.3280 gloss:0.7605 dlossA:0.7581 dlossQ:0.9606 exploreP:0.4118\n",
      "Episode:182 meanR:68.8100 R:82.0000 rate:0.1640 gloss:0.7589 dlossA:0.7526 dlossQ:0.9670 exploreP:0.4085\n",
      "Episode:183 meanR:70.4900 R:200.0000 rate:0.4000 gloss:0.7692 dlossA:0.7611 dlossQ:0.9622 exploreP:0.4006\n",
      "Episode:184 meanR:71.4500 R:134.0000 rate:0.2680 gloss:0.7580 dlossA:0.7579 dlossQ:0.9623 exploreP:0.3954\n",
      "Episode:185 meanR:71.7700 R:64.0000 rate:0.1280 gloss:0.7633 dlossA:0.7639 dlossQ:0.9600 exploreP:0.3930\n",
      "Episode:186 meanR:72.5400 R:99.0000 rate:0.1980 gloss:0.7612 dlossA:0.7596 dlossQ:0.9751 exploreP:0.3892\n",
      "Episode:187 meanR:76.9000 R:453.0000 rate:0.9060 gloss:0.7696 dlossA:0.7622 dlossQ:0.9623 exploreP:0.3724\n",
      "Episode:188 meanR:78.5800 R:185.0000 rate:0.3700 gloss:0.7721 dlossA:0.7661 dlossQ:0.9641 exploreP:0.3658\n",
      "Episode:189 meanR:80.3700 R:188.0000 rate:0.3760 gloss:0.7706 dlossA:0.7620 dlossQ:0.9702 exploreP:0.3592\n",
      "Episode:190 meanR:80.8300 R:69.0000 rate:0.1380 gloss:0.7655 dlossA:0.7615 dlossQ:0.9667 exploreP:0.3568\n",
      "Episode:191 meanR:81.8400 R:140.0000 rate:0.2800 gloss:0.7700 dlossA:0.7612 dlossQ:0.9646 exploreP:0.3519\n",
      "Episode:192 meanR:84.3500 R:276.0000 rate:0.5520 gloss:0.7824 dlossA:0.7691 dlossQ:0.9605 exploreP:0.3426\n",
      "Episode:193 meanR:88.7600 R:500.0000 rate:1.0000 gloss:0.7891 dlossA:0.7757 dlossQ:0.9565 exploreP:0.3264\n",
      "Episode:194 meanR:93.4500 R:500.0000 rate:1.0000 gloss:0.7911 dlossA:0.7745 dlossQ:0.9595 exploreP:0.3110\n",
      "Episode:195 meanR:98.3100 R:500.0000 rate:1.0000 gloss:0.8067 dlossA:0.7830 dlossQ:0.9524 exploreP:0.2963\n",
      "Episode:196 meanR:101.5700 R:341.0000 rate:0.6820 gloss:0.8147 dlossA:0.7889 dlossQ:0.9507 exploreP:0.2867\n",
      "Episode:197 meanR:106.4200 R:500.0000 rate:1.0000 gloss:0.8336 dlossA:0.7932 dlossQ:0.9475 exploreP:0.2732\n",
      "Episode:198 meanR:110.5700 R:500.0000 rate:1.0000 gloss:0.8611 dlossA:0.8179 dlossQ:0.9443 exploreP:0.2604\n",
      "Episode:199 meanR:113.3300 R:326.0000 rate:0.6520 gloss:0.8770 dlossA:0.8210 dlossQ:0.9448 exploreP:0.2523\n",
      "Episode:200 meanR:114.6000 R:172.0000 rate:0.3440 gloss:0.8997 dlossA:0.8370 dlossQ:0.9313 exploreP:0.2482\n",
      "Episode:201 meanR:116.8900 R:283.0000 rate:0.5660 gloss:0.8935 dlossA:0.8392 dlossQ:0.9416 exploreP:0.2416\n",
      "Episode:202 meanR:118.3900 R:182.0000 rate:0.3640 gloss:0.8983 dlossA:0.8431 dlossQ:0.9395 exploreP:0.2374\n",
      "Episode:203 meanR:123.2200 R:500.0000 rate:1.0000 gloss:0.9169 dlossA:0.8548 dlossQ:0.9401 exploreP:0.2263\n",
      "Episode:204 meanR:123.9700 R:98.0000 rate:0.1960 gloss:0.9166 dlossA:0.8455 dlossQ:0.9374 exploreP:0.2242\n",
      "Episode:205 meanR:126.7300 R:312.0000 rate:0.6240 gloss:0.9373 dlossA:0.8638 dlossQ:0.9401 exploreP:0.2176\n",
      "Episode:206 meanR:130.8700 R:428.0000 rate:0.8560 gloss:0.9533 dlossA:0.8761 dlossQ:0.9369 exploreP:0.2089\n",
      "Episode:207 meanR:135.2500 R:498.0000 rate:0.9960 gloss:0.9725 dlossA:0.8870 dlossQ:0.9370 exploreP:0.1992\n",
      "Episode:208 meanR:139.3600 R:500.0000 rate:1.0000 gloss:0.9969 dlossA:0.9039 dlossQ:0.9406 exploreP:0.1900\n",
      "Episode:209 meanR:143.7500 R:500.0000 rate:1.0000 gloss:1.0192 dlossA:0.9157 dlossQ:0.9360 exploreP:0.1812\n",
      "Episode:210 meanR:145.0000 R:224.0000 rate:0.4480 gloss:1.0407 dlossA:0.9230 dlossQ:0.9358 exploreP:0.1774\n",
      "Episode:211 meanR:149.7400 R:500.0000 rate:1.0000 gloss:1.0644 dlossA:0.9271 dlossQ:0.9312 exploreP:0.1693\n",
      "Episode:212 meanR:154.4800 R:500.0000 rate:1.0000 gloss:1.1007 dlossA:0.9502 dlossQ:0.9320 exploreP:0.1615\n",
      "Episode:213 meanR:159.1700 R:500.0000 rate:1.0000 gloss:1.1411 dlossA:0.9794 dlossQ:0.9296 exploreP:0.1541\n",
      "Episode:214 meanR:163.5300 R:500.0000 rate:1.0000 gloss:1.1757 dlossA:0.9969 dlossQ:0.9342 exploreP:0.1471\n",
      "Episode:215 meanR:168.4100 R:500.0000 rate:1.0000 gloss:1.2185 dlossA:1.0282 dlossQ:0.9332 exploreP:0.1404\n",
      "Episode:216 meanR:173.1200 R:500.0000 rate:1.0000 gloss:1.2557 dlossA:1.0567 dlossQ:0.9359 exploreP:0.1340\n",
      "Episode:217 meanR:177.3300 R:500.0000 rate:1.0000 gloss:1.2938 dlossA:1.0808 dlossQ:0.9397 exploreP:0.1280\n",
      "Episode:218 meanR:181.5500 R:500.0000 rate:1.0000 gloss:1.3345 dlossA:1.1171 dlossQ:0.9429 exploreP:0.1222\n",
      "Episode:219 meanR:185.5800 R:500.0000 rate:1.0000 gloss:1.3599 dlossA:1.1233 dlossQ:0.9282 exploreP:0.1168\n",
      "Episode:220 meanR:189.6100 R:449.0000 rate:0.8980 gloss:1.3923 dlossA:1.1610 dlossQ:0.9345 exploreP:0.1121\n",
      "Episode:221 meanR:191.4500 R:239.0000 rate:0.4780 gloss:1.4145 dlossA:1.1373 dlossQ:0.9310 exploreP:0.1097\n",
      "Episode:222 meanR:195.5500 R:500.0000 rate:1.0000 gloss:1.4542 dlossA:1.2119 dlossQ:0.9354 exploreP:0.1048\n",
      "Episode:223 meanR:198.2900 R:334.0000 rate:0.6680 gloss:1.4800 dlossA:1.2011 dlossQ:0.9280 exploreP:0.1017\n",
      "Episode:224 meanR:202.4900 R:500.0000 rate:1.0000 gloss:1.5110 dlossA:1.2198 dlossQ:0.9229 exploreP:0.0972\n",
      "Episode:225 meanR:207.3400 R:500.0000 rate:1.0000 gloss:1.5595 dlossA:1.3004 dlossQ:0.9309 exploreP:0.0930\n",
      "Episode:226 meanR:209.9800 R:305.0000 rate:0.6100 gloss:1.5894 dlossA:1.2499 dlossQ:0.9226 exploreP:0.0905\n",
      "Episode:227 meanR:214.3500 R:500.0000 rate:1.0000 gloss:1.6356 dlossA:1.3315 dlossQ:0.9292 exploreP:0.0865\n",
      "Episode:228 meanR:219.1400 R:500.0000 rate:1.0000 gloss:1.6820 dlossA:1.3614 dlossQ:0.9232 exploreP:0.0828\n",
      "Episode:229 meanR:221.1200 R:231.0000 rate:0.4620 gloss:1.7175 dlossA:1.3450 dlossQ:0.9234 exploreP:0.0812\n",
      "Episode:230 meanR:225.5800 R:500.0000 rate:1.0000 gloss:1.7436 dlossA:1.4473 dlossQ:0.9182 exploreP:0.0777\n",
      "Episode:231 meanR:228.5900 R:334.0000 rate:0.6680 gloss:1.7691 dlossA:1.4111 dlossQ:0.9185 exploreP:0.0755\n",
      "Episode:232 meanR:231.4900 R:338.0000 rate:0.6760 gloss:1.8090 dlossA:1.4257 dlossQ:0.9193 exploreP:0.0733\n",
      "Episode:233 meanR:236.1000 R:500.0000 rate:1.0000 gloss:1.8640 dlossA:1.5233 dlossQ:0.9104 exploreP:0.0702\n",
      "Episode:234 meanR:239.9700 R:500.0000 rate:1.0000 gloss:1.9081 dlossA:1.5581 dlossQ:0.9114 exploreP:0.0673\n",
      "Episode:235 meanR:244.4200 R:500.0000 rate:1.0000 gloss:1.9486 dlossA:1.5872 dlossQ:0.9094 exploreP:0.0645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:248.7400 R:500.0000 rate:1.0000 gloss:2.0100 dlossA:1.6048 dlossQ:0.9102 exploreP:0.0618\n",
      "Episode:237 meanR:253.4000 R:500.0000 rate:1.0000 gloss:2.0762 dlossA:1.6908 dlossQ:0.9109 exploreP:0.0593\n",
      "Episode:238 meanR:257.7300 R:457.0000 rate:0.9140 gloss:2.1310 dlossA:1.6895 dlossQ:0.9142 exploreP:0.0571\n",
      "Episode:239 meanR:262.4400 R:500.0000 rate:1.0000 gloss:2.2000 dlossA:1.6999 dlossQ:0.9165 exploreP:0.0548\n",
      "Episode:240 meanR:265.1500 R:364.0000 rate:0.7280 gloss:2.2641 dlossA:1.8574 dlossQ:0.9222 exploreP:0.0532\n",
      "Episode:241 meanR:267.0800 R:313.0000 rate:0.6260 gloss:2.2993 dlossA:1.9666 dlossQ:0.9229 exploreP:0.0519\n",
      "Episode:242 meanR:271.5100 R:500.0000 rate:1.0000 gloss:2.3460 dlossA:1.8890 dlossQ:0.9265 exploreP:0.0498\n",
      "Episode:243 meanR:276.3400 R:500.0000 rate:1.0000 gloss:2.4096 dlossA:1.9686 dlossQ:0.9339 exploreP:0.0479\n",
      "Episode:244 meanR:280.5400 R:500.0000 rate:1.0000 gloss:2.5023 dlossA:2.0532 dlossQ:0.9458 exploreP:0.0460\n",
      "Episode:245 meanR:284.8600 R:500.0000 rate:1.0000 gloss:2.5543 dlossA:2.0994 dlossQ:0.9482 exploreP:0.0443\n",
      "Episode:246 meanR:286.6400 R:240.0000 rate:0.4800 gloss:2.6086 dlossA:2.1656 dlossQ:0.9541 exploreP:0.0435\n",
      "Episode:247 meanR:291.3400 R:500.0000 rate:1.0000 gloss:2.6675 dlossA:2.1490 dlossQ:0.9668 exploreP:0.0418\n",
      "Episode:248 meanR:294.6600 R:364.0000 rate:0.7280 gloss:2.7362 dlossA:2.2975 dlossQ:0.9649 exploreP:0.0407\n",
      "Episode:249 meanR:298.7600 R:500.0000 rate:1.0000 gloss:2.8045 dlossA:2.2993 dlossQ:0.9758 exploreP:0.0392\n",
      "Episode:250 meanR:301.1200 R:324.0000 rate:0.6480 gloss:2.8722 dlossA:2.3598 dlossQ:0.9768 exploreP:0.0383\n",
      "Episode:251 meanR:304.8000 R:500.0000 rate:1.0000 gloss:2.9614 dlossA:2.3632 dlossQ:0.9946 exploreP:0.0369\n",
      "Episode:252 meanR:308.9100 R:500.0000 rate:1.0000 gloss:3.0438 dlossA:2.5035 dlossQ:1.0091 exploreP:0.0356\n",
      "Episode:253 meanR:313.1200 R:500.0000 rate:1.0000 gloss:3.1515 dlossA:2.6036 dlossQ:1.0214 exploreP:0.0343\n",
      "Episode:254 meanR:317.5400 R:500.0000 rate:1.0000 gloss:3.2485 dlossA:2.7989 dlossQ:1.0434 exploreP:0.0331\n",
      "Episode:255 meanR:322.1800 R:500.0000 rate:1.0000 gloss:3.3252 dlossA:2.5691 dlossQ:1.0801 exploreP:0.0320\n",
      "Episode:256 meanR:326.5200 R:500.0000 rate:1.0000 gloss:3.4802 dlossA:2.8394 dlossQ:1.0743 exploreP:0.0309\n",
      "Episode:257 meanR:330.7500 R:500.0000 rate:1.0000 gloss:3.6081 dlossA:2.9561 dlossQ:1.1314 exploreP:0.0299\n",
      "Episode:258 meanR:334.8400 R:500.0000 rate:1.0000 gloss:3.7405 dlossA:3.2018 dlossQ:1.1713 exploreP:0.0289\n",
      "Episode:259 meanR:337.8600 R:328.0000 rate:0.6560 gloss:3.7989 dlossA:3.2300 dlossQ:1.1361 exploreP:0.0283\n",
      "Episode:260 meanR:342.2300 R:500.0000 rate:1.0000 gloss:3.8842 dlossA:3.1978 dlossQ:1.1978 exploreP:0.0274\n",
      "Episode:261 meanR:345.1000 R:388.0000 rate:0.7760 gloss:4.0008 dlossA:3.2130 dlossQ:1.1862 exploreP:0.0268\n",
      "Episode:262 meanR:349.6300 R:500.0000 rate:1.0000 gloss:4.1222 dlossA:3.3393 dlossQ:1.2615 exploreP:0.0260\n",
      "Episode:263 meanR:353.4000 R:500.0000 rate:1.0000 gloss:4.2978 dlossA:3.5940 dlossQ:1.2617 exploreP:0.0252\n",
      "Episode:264 meanR:357.8500 R:500.0000 rate:1.0000 gloss:4.4373 dlossA:3.7065 dlossQ:1.2765 exploreP:0.0244\n",
      "Episode:265 meanR:361.2200 R:500.0000 rate:1.0000 gloss:4.5251 dlossA:3.6625 dlossQ:1.3134 exploreP:0.0237\n",
      "Episode:266 meanR:365.1500 R:500.0000 rate:1.0000 gloss:4.7571 dlossA:3.9704 dlossQ:1.4407 exploreP:0.0231\n",
      "Episode:267 meanR:364.8000 R:333.0000 rate:0.6660 gloss:4.8179 dlossA:3.9178 dlossQ:1.3437 exploreP:0.0226\n",
      "Episode:268 meanR:367.4600 R:415.0000 rate:0.8300 gloss:5.0346 dlossA:4.0269 dlossQ:1.4641 exploreP:0.0221\n",
      "Episode:269 meanR:371.4000 R:500.0000 rate:1.0000 gloss:5.1532 dlossA:4.1819 dlossQ:1.5108 exploreP:0.0215\n",
      "Episode:270 meanR:375.2600 R:500.0000 rate:1.0000 gloss:5.3247 dlossA:4.3868 dlossQ:1.4654 exploreP:0.0210\n",
      "Episode:271 meanR:380.0800 R:500.0000 rate:1.0000 gloss:5.4723 dlossA:4.4936 dlossQ:1.4774 exploreP:0.0204\n",
      "Episode:272 meanR:383.7600 R:500.0000 rate:1.0000 gloss:5.6783 dlossA:4.4343 dlossQ:1.5058 exploreP:0.0199\n",
      "Episode:273 meanR:387.2500 R:500.0000 rate:1.0000 gloss:5.8443 dlossA:4.7242 dlossQ:1.6272 exploreP:0.0194\n",
      "Episode:274 meanR:390.7100 R:500.0000 rate:1.0000 gloss:6.0823 dlossA:4.8413 dlossQ:1.6361 exploreP:0.0190\n",
      "Episode:275 meanR:394.7000 R:500.0000 rate:1.0000 gloss:6.3359 dlossA:5.1146 dlossQ:1.6528 exploreP:0.0185\n",
      "Episode:276 meanR:397.8200 R:500.0000 rate:1.0000 gloss:6.5709 dlossA:5.3022 dlossQ:1.8294 exploreP:0.0181\n",
      "Episode:277 meanR:400.6100 R:500.0000 rate:1.0000 gloss:6.8032 dlossA:5.6619 dlossQ:1.8962 exploreP:0.0177\n",
      "Episode:278 meanR:403.4000 R:500.0000 rate:1.0000 gloss:7.0349 dlossA:5.6184 dlossQ:2.0483 exploreP:0.0174\n",
      "Episode:279 meanR:407.5500 R:500.0000 rate:1.0000 gloss:7.2653 dlossA:5.7809 dlossQ:2.0092 exploreP:0.0170\n",
      "Episode:280 meanR:410.2100 R:347.0000 rate:0.6940 gloss:7.4871 dlossA:5.8875 dlossQ:2.0565 exploreP:0.0168\n",
      "Episode:281 meanR:413.5700 R:500.0000 rate:1.0000 gloss:7.6953 dlossA:5.8169 dlossQ:2.1122 exploreP:0.0164\n",
      "Episode:282 meanR:417.7500 R:500.0000 rate:1.0000 gloss:7.9660 dlossA:6.3162 dlossQ:2.2641 exploreP:0.0161\n",
      "Episode:283 meanR:420.7500 R:500.0000 rate:1.0000 gloss:8.1968 dlossA:6.5008 dlossQ:2.3690 exploreP:0.0158\n",
      "Episode:284 meanR:424.4100 R:500.0000 rate:1.0000 gloss:8.5274 dlossA:6.7882 dlossQ:2.6546 exploreP:0.0155\n",
      "Episode:285 meanR:428.7700 R:500.0000 rate:1.0000 gloss:8.5104 dlossA:5.9610 dlossQ:2.8014 exploreP:0.0153\n",
      "Episode:286 meanR:431.2800 R:350.0000 rate:0.7000 gloss:9.0380 dlossA:7.1720 dlossQ:2.7033 exploreP:0.0151\n",
      "Episode:287 meanR:431.7500 R:500.0000 rate:1.0000 gloss:9.0988 dlossA:7.4044 dlossQ:3.2027 exploreP:0.0148\n",
      "Episode:288 meanR:434.9000 R:500.0000 rate:1.0000 gloss:9.0770 dlossA:6.7058 dlossQ:2.8248 exploreP:0.0146\n",
      "Episode:289 meanR:438.0200 R:500.0000 rate:1.0000 gloss:9.2165 dlossA:6.5876 dlossQ:2.8368 exploreP:0.0144\n",
      "Episode:290 meanR:442.3300 R:500.0000 rate:1.0000 gloss:9.7164 dlossA:7.0126 dlossQ:3.0995 exploreP:0.0142\n",
      "Episode:291 meanR:445.9300 R:500.0000 rate:1.0000 gloss:10.0285 dlossA:7.4324 dlossQ:3.5061 exploreP:0.0140\n",
      "Episode:292 meanR:448.1700 R:500.0000 rate:1.0000 gloss:10.2283 dlossA:7.5875 dlossQ:3.2107 exploreP:0.0138\n",
      "Episode:293 meanR:448.1700 R:500.0000 rate:1.0000 gloss:10.5110 dlossA:8.0431 dlossQ:3.4329 exploreP:0.0136\n",
      "Episode:294 meanR:448.1700 R:500.0000 rate:1.0000 gloss:10.7965 dlossA:8.3961 dlossQ:3.5911 exploreP:0.0134\n",
      "Episode:295 meanR:448.1700 R:500.0000 rate:1.0000 gloss:10.9778 dlossA:7.6288 dlossQ:4.0519 exploreP:0.0132\n",
      "Episode:296 meanR:449.7600 R:500.0000 rate:1.0000 gloss:11.3610 dlossA:8.4796 dlossQ:4.1187 exploreP:0.0131\n",
      "Episode:297 meanR:449.7600 R:500.0000 rate:1.0000 gloss:11.8379 dlossA:8.9113 dlossQ:4.3719 exploreP:0.0129\n",
      "Episode:298 meanR:449.6900 R:493.0000 rate:0.9860 gloss:11.9225 dlossA:8.6132 dlossQ:4.3450 exploreP:0.0128\n",
      "Episode:299 meanR:451.4300 R:500.0000 rate:1.0000 gloss:12.2540 dlossA:8.7547 dlossQ:4.8621 exploreP:0.0127\n",
      "Episode:300 meanR:454.7100 R:500.0000 rate:1.0000 gloss:12.4302 dlossA:8.9808 dlossQ:4.8418 exploreP:0.0125\n",
      "Episode:301 meanR:456.8800 R:500.0000 rate:1.0000 gloss:12.7503 dlossA:8.8248 dlossQ:5.6583 exploreP:0.0124\n",
      "Episode:302 meanR:459.1500 R:409.0000 rate:0.8180 gloss:12.9101 dlossA:10.1097 dlossQ:5.5815 exploreP:0.0123\n",
      "Episode:303 meanR:459.1500 R:500.0000 rate:1.0000 gloss:12.9679 dlossA:9.2571 dlossQ:5.7847 exploreP:0.0122\n",
      "Episode:304 meanR:463.1700 R:500.0000 rate:1.0000 gloss:12.9198 dlossA:9.1598 dlossQ:5.5201 exploreP:0.0121\n",
      "Episode:305 meanR:465.0500 R:500.0000 rate:1.0000 gloss:13.2401 dlossA:8.7206 dlossQ:5.9699 exploreP:0.0120\n",
      "Episode:306 meanR:464.9000 R:413.0000 rate:0.8260 gloss:13.7112 dlossA:9.9272 dlossQ:6.3125 exploreP:0.0119\n",
      "Episode:307 meanR:464.9200 R:500.0000 rate:1.0000 gloss:13.8203 dlossA:8.6455 dlossQ:6.3890 exploreP:0.0118\n",
      "Episode:308 meanR:464.9200 R:500.0000 rate:1.0000 gloss:14.2341 dlossA:9.5483 dlossQ:6.5964 exploreP:0.0117\n",
      "Episode:309 meanR:464.9200 R:500.0000 rate:1.0000 gloss:14.5739 dlossA:8.9467 dlossQ:6.9835 exploreP:0.0116\n",
      "Episode:310 meanR:467.6800 R:500.0000 rate:1.0000 gloss:15.2166 dlossA:10.6271 dlossQ:7.5213 exploreP:0.0116\n",
      "Episode:311 meanR:467.6800 R:500.0000 rate:1.0000 gloss:15.0474 dlossA:9.2178 dlossQ:7.7630 exploreP:0.0115\n",
      "Episode:312 meanR:467.6800 R:500.0000 rate:1.0000 gloss:15.4136 dlossA:9.7687 dlossQ:7.4796 exploreP:0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:313 meanR:467.6800 R:500.0000 rate:1.0000 gloss:15.7621 dlossA:9.6414 dlossQ:9.2494 exploreP:0.0113\n",
      "Episode:314 meanR:467.6800 R:500.0000 rate:1.0000 gloss:15.6182 dlossA:9.4960 dlossQ:8.5264 exploreP:0.0113\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
