{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (1111,) (1111, 4) (1111,) (1111,)\n",
      "dtypes: float64 float64 int64 bool\n",
      "states: 2.2556878097289843 -3.0057814537353345\n",
      "actions: 1 0\n",
      "rewards: 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# print(rewards[-20:])\n",
    "print('shapes:', np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print('dtypes:', np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('states:', np.max(np.array(states)), np.min(np.array(states)))\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print('rewards:', np.max(np.array(rewards)), np.min(np.array(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.01\n"
     ]
    }
   ],
   "source": [
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88, 6), (88, 4), (88,), (88, 4), (88,), (88,), (88,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:24.0000 R:24.0000 rate:0.0480 gloss:0.6888 dlossA:0.7103 dlossQ:0.9459 exploreP:0.9976\n",
      "Episode:1 meanR:20.5000 R:17.0000 rate:0.0340 gloss:0.6900 dlossA:0.7110 dlossQ:0.9479 exploreP:0.9959\n",
      "Episode:2 meanR:17.6667 R:12.0000 rate:0.0240 gloss:0.6904 dlossA:0.7089 dlossQ:0.9624 exploreP:0.9948\n",
      "Episode:3 meanR:19.7500 R:26.0000 rate:0.0520 gloss:0.6893 dlossA:0.7074 dlossQ:0.9616 exploreP:0.9922\n",
      "Episode:4 meanR:19.0000 R:16.0000 rate:0.0320 gloss:0.6892 dlossA:0.7084 dlossQ:0.9682 exploreP:0.9906\n",
      "Episode:5 meanR:19.5000 R:22.0000 rate:0.0440 gloss:0.6915 dlossA:0.7131 dlossQ:0.9491 exploreP:0.9885\n",
      "Episode:6 meanR:18.1429 R:10.0000 rate:0.0200 gloss:0.6932 dlossA:0.7142 dlossQ:0.9582 exploreP:0.9875\n",
      "Episode:7 meanR:19.8750 R:32.0000 rate:0.0640 gloss:0.6922 dlossA:0.7103 dlossQ:0.9691 exploreP:0.9844\n",
      "Episode:8 meanR:23.8889 R:56.0000 rate:0.1120 gloss:0.6952 dlossA:0.7141 dlossQ:0.9646 exploreP:0.9789\n",
      "Episode:9 meanR:23.9000 R:24.0000 rate:0.0480 gloss:0.6976 dlossA:0.7152 dlossQ:0.9584 exploreP:0.9766\n",
      "Episode:10 meanR:24.5455 R:31.0000 rate:0.0620 gloss:0.6962 dlossA:0.7146 dlossQ:0.9722 exploreP:0.9736\n",
      "Episode:11 meanR:24.5833 R:25.0000 rate:0.0500 gloss:0.6991 dlossA:0.7160 dlossQ:0.9685 exploreP:0.9712\n",
      "Episode:12 meanR:25.3846 R:35.0000 rate:0.0700 gloss:0.6991 dlossA:0.7155 dlossQ:0.9818 exploreP:0.9679\n",
      "Episode:13 meanR:26.3571 R:39.0000 rate:0.0780 gloss:0.6996 dlossA:0.7159 dlossQ:0.9799 exploreP:0.9641\n",
      "Episode:14 meanR:25.9333 R:20.0000 rate:0.0400 gloss:0.7027 dlossA:0.7141 dlossQ:0.9970 exploreP:0.9622\n",
      "Episode:15 meanR:25.2500 R:15.0000 rate:0.0300 gloss:0.6985 dlossA:0.7143 dlossQ:0.9927 exploreP:0.9608\n",
      "Episode:16 meanR:25.3529 R:27.0000 rate:0.0540 gloss:0.7019 dlossA:0.7163 dlossQ:0.9903 exploreP:0.9582\n",
      "Episode:17 meanR:25.2778 R:24.0000 rate:0.0480 gloss:0.7083 dlossA:0.7196 dlossQ:0.9893 exploreP:0.9560\n",
      "Episode:18 meanR:25.0000 R:20.0000 rate:0.0400 gloss:0.7080 dlossA:0.7206 dlossQ:0.9854 exploreP:0.9541\n",
      "Episode:19 meanR:24.5000 R:15.0000 rate:0.0300 gloss:0.7029 dlossA:0.7162 dlossQ:0.9977 exploreP:0.9527\n",
      "Episode:20 meanR:24.2857 R:20.0000 rate:0.0400 gloss:0.7104 dlossA:0.7224 dlossQ:0.9922 exploreP:0.9508\n",
      "Episode:21 meanR:24.5909 R:31.0000 rate:0.0620 gloss:0.7058 dlossA:0.7185 dlossQ:0.9932 exploreP:0.9479\n",
      "Episode:22 meanR:24.7826 R:29.0000 rate:0.0580 gloss:0.7078 dlossA:0.7212 dlossQ:0.9938 exploreP:0.9451\n",
      "Episode:23 meanR:24.7083 R:23.0000 rate:0.0460 gloss:0.7113 dlossA:0.7224 dlossQ:0.9893 exploreP:0.9430\n",
      "Episode:24 meanR:25.6400 R:48.0000 rate:0.0960 gloss:0.7085 dlossA:0.7202 dlossQ:0.9999 exploreP:0.9385\n",
      "Episode:25 meanR:25.6923 R:27.0000 rate:0.0540 gloss:0.7160 dlossA:0.7218 dlossQ:1.0051 exploreP:0.9360\n",
      "Episode:26 meanR:26.3704 R:44.0000 rate:0.0880 gloss:0.7141 dlossA:0.7223 dlossQ:1.0100 exploreP:0.9320\n",
      "Episode:27 meanR:27.1429 R:48.0000 rate:0.0960 gloss:0.7193 dlossA:0.7231 dlossQ:1.0113 exploreP:0.9275\n",
      "Episode:28 meanR:26.7931 R:17.0000 rate:0.0340 gloss:0.7168 dlossA:0.7230 dlossQ:1.0218 exploreP:0.9260\n",
      "Episode:29 meanR:26.8000 R:27.0000 rate:0.0540 gloss:0.7157 dlossA:0.7250 dlossQ:1.0026 exploreP:0.9235\n",
      "Episode:30 meanR:26.3871 R:14.0000 rate:0.0280 gloss:0.7142 dlossA:0.7216 dlossQ:1.0187 exploreP:0.9222\n",
      "Episode:31 meanR:26.3125 R:24.0000 rate:0.0480 gloss:0.7220 dlossA:0.7262 dlossQ:1.0194 exploreP:0.9201\n",
      "Episode:32 meanR:26.2121 R:23.0000 rate:0.0460 gloss:0.7288 dlossA:0.7316 dlossQ:1.0119 exploreP:0.9180\n",
      "Episode:33 meanR:25.9118 R:16.0000 rate:0.0320 gloss:0.7244 dlossA:0.7290 dlossQ:1.0285 exploreP:0.9165\n",
      "Episode:34 meanR:25.6286 R:16.0000 rate:0.0320 gloss:0.7192 dlossA:0.7240 dlossQ:1.0219 exploreP:0.9151\n",
      "Episode:35 meanR:25.2500 R:12.0000 rate:0.0240 gloss:0.7174 dlossA:0.7270 dlossQ:1.0166 exploreP:0.9140\n",
      "Episode:36 meanR:25.1892 R:23.0000 rate:0.0460 gloss:0.7166 dlossA:0.7262 dlossQ:1.0088 exploreP:0.9119\n",
      "Episode:37 meanR:25.1842 R:25.0000 rate:0.0500 gloss:0.7204 dlossA:0.7268 dlossQ:1.0236 exploreP:0.9096\n",
      "Episode:38 meanR:25.0513 R:20.0000 rate:0.0400 gloss:0.7239 dlossA:0.7298 dlossQ:1.0241 exploreP:0.9079\n",
      "Episode:39 meanR:24.7250 R:12.0000 rate:0.0240 gloss:0.7231 dlossA:0.7313 dlossQ:1.0139 exploreP:0.9068\n",
      "Episode:40 meanR:24.5122 R:16.0000 rate:0.0320 gloss:0.7225 dlossA:0.7271 dlossQ:1.0287 exploreP:0.9053\n",
      "Episode:41 meanR:24.5476 R:26.0000 rate:0.0520 gloss:0.7185 dlossA:0.7267 dlossQ:1.0271 exploreP:0.9030\n",
      "Episode:42 meanR:24.7674 R:34.0000 rate:0.0680 gloss:0.7291 dlossA:0.7309 dlossQ:1.0185 exploreP:0.9000\n",
      "Episode:43 meanR:25.1364 R:41.0000 rate:0.0820 gloss:0.7280 dlossA:0.7297 dlossQ:1.0282 exploreP:0.8963\n",
      "Episode:44 meanR:25.3111 R:33.0000 rate:0.0660 gloss:0.7182 dlossA:0.7253 dlossQ:1.0356 exploreP:0.8934\n",
      "Episode:45 meanR:26.5652 R:83.0000 rate:0.1660 gloss:0.7241 dlossA:0.7296 dlossQ:1.0298 exploreP:0.8861\n",
      "Episode:46 meanR:26.4894 R:23.0000 rate:0.0460 gloss:0.7249 dlossA:0.7265 dlossQ:1.0465 exploreP:0.8841\n",
      "Episode:47 meanR:26.9375 R:48.0000 rate:0.0960 gloss:0.7250 dlossA:0.7280 dlossQ:1.0437 exploreP:0.8799\n",
      "Episode:48 meanR:26.7755 R:19.0000 rate:0.0380 gloss:0.7236 dlossA:0.7278 dlossQ:1.0360 exploreP:0.8783\n",
      "Episode:49 meanR:26.6000 R:18.0000 rate:0.0360 gloss:0.7320 dlossA:0.7344 dlossQ:1.0373 exploreP:0.8767\n",
      "Episode:50 meanR:26.3725 R:15.0000 rate:0.0300 gloss:0.7241 dlossA:0.7292 dlossQ:1.0337 exploreP:0.8754\n",
      "Episode:51 meanR:26.3462 R:25.0000 rate:0.0500 gloss:0.7228 dlossA:0.7298 dlossQ:1.0305 exploreP:0.8733\n",
      "Episode:52 meanR:26.1132 R:14.0000 rate:0.0280 gloss:0.7309 dlossA:0.7339 dlossQ:1.0381 exploreP:0.8720\n",
      "Episode:53 meanR:26.1481 R:28.0000 rate:0.0560 gloss:0.7326 dlossA:0.7351 dlossQ:1.0385 exploreP:0.8696\n",
      "Episode:54 meanR:26.0545 R:21.0000 rate:0.0420 gloss:0.7169 dlossA:0.7297 dlossQ:1.0342 exploreP:0.8678\n",
      "Episode:55 meanR:26.0536 R:26.0000 rate:0.0520 gloss:0.7266 dlossA:0.7310 dlossQ:1.0479 exploreP:0.8656\n",
      "Episode:56 meanR:26.0702 R:27.0000 rate:0.0540 gloss:0.7259 dlossA:0.7323 dlossQ:1.0326 exploreP:0.8633\n",
      "Episode:57 meanR:26.2931 R:39.0000 rate:0.0780 gloss:0.7317 dlossA:0.7327 dlossQ:1.0423 exploreP:0.8600\n",
      "Episode:58 meanR:26.8983 R:62.0000 rate:0.1240 gloss:0.7336 dlossA:0.7339 dlossQ:1.0482 exploreP:0.8547\n",
      "Episode:59 meanR:27.0500 R:36.0000 rate:0.0720 gloss:0.7272 dlossA:0.7325 dlossQ:1.0412 exploreP:0.8517\n",
      "Episode:60 meanR:27.0820 R:29.0000 rate:0.0580 gloss:0.7326 dlossA:0.7342 dlossQ:1.0456 exploreP:0.8492\n",
      "Episode:61 meanR:27.4032 R:47.0000 rate:0.0940 gloss:0.7325 dlossA:0.7356 dlossQ:1.0386 exploreP:0.8453\n",
      "Episode:62 meanR:27.2222 R:16.0000 rate:0.0320 gloss:0.7420 dlossA:0.7354 dlossQ:1.0652 exploreP:0.8440\n",
      "Episode:63 meanR:27.1562 R:23.0000 rate:0.0460 gloss:0.7401 dlossA:0.7375 dlossQ:1.0552 exploreP:0.8421\n",
      "Episode:64 meanR:27.2769 R:35.0000 rate:0.0700 gloss:0.7404 dlossA:0.7367 dlossQ:1.0632 exploreP:0.8392\n",
      "Episode:65 meanR:27.2879 R:28.0000 rate:0.0560 gloss:0.7326 dlossA:0.7345 dlossQ:1.0503 exploreP:0.8368\n",
      "Episode:66 meanR:28.2836 R:94.0000 rate:0.1880 gloss:0.7392 dlossA:0.7360 dlossQ:1.0564 exploreP:0.8291\n",
      "Episode:67 meanR:28.1765 R:21.0000 rate:0.0420 gloss:0.7412 dlossA:0.7372 dlossQ:1.0561 exploreP:0.8274\n",
      "Episode:68 meanR:27.9710 R:14.0000 rate:0.0280 gloss:0.7533 dlossA:0.7443 dlossQ:1.0829 exploreP:0.8262\n",
      "Episode:69 meanR:28.3571 R:55.0000 rate:0.1100 gloss:0.7464 dlossA:0.7402 dlossQ:1.0648 exploreP:0.8218\n",
      "Episode:70 meanR:28.5070 R:39.0000 rate:0.0780 gloss:0.7471 dlossA:0.7405 dlossQ:1.0667 exploreP:0.8186\n",
      "Episode:71 meanR:28.3750 R:19.0000 rate:0.0380 gloss:0.7359 dlossA:0.7382 dlossQ:1.0556 exploreP:0.8171\n",
      "Episode:72 meanR:28.3973 R:30.0000 rate:0.0600 gloss:0.7376 dlossA:0.7371 dlossQ:1.0620 exploreP:0.8146\n",
      "Episode:73 meanR:28.3108 R:22.0000 rate:0.0440 gloss:0.7330 dlossA:0.7394 dlossQ:1.0380 exploreP:0.8129\n",
      "Episode:74 meanR:28.0667 R:10.0000 rate:0.0200 gloss:0.7471 dlossA:0.7415 dlossQ:1.0744 exploreP:0.8121\n",
      "Episode:75 meanR:28.1184 R:32.0000 rate:0.0640 gloss:0.7467 dlossA:0.7402 dlossQ:1.0727 exploreP:0.8095\n",
      "Episode:76 meanR:28.0909 R:26.0000 rate:0.0520 gloss:0.7304 dlossA:0.7366 dlossQ:1.0572 exploreP:0.8074\n",
      "Episode:77 meanR:28.0128 R:22.0000 rate:0.0440 gloss:0.7560 dlossA:0.7429 dlossQ:1.0806 exploreP:0.8057\n",
      "Episode:78 meanR:28.1519 R:39.0000 rate:0.0780 gloss:0.7577 dlossA:0.7470 dlossQ:1.0806 exploreP:0.8026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:28.1250 R:26.0000 rate:0.0520 gloss:0.7439 dlossA:0.7415 dlossQ:1.0651 exploreP:0.8005\n",
      "Episode:80 meanR:28.5556 R:63.0000 rate:0.1260 gloss:0.7415 dlossA:0.7373 dlossQ:1.0717 exploreP:0.7956\n",
      "Episode:81 meanR:28.4878 R:23.0000 rate:0.0460 gloss:0.7294 dlossA:0.7347 dlossQ:1.0530 exploreP:0.7938\n",
      "Episode:82 meanR:28.7470 R:50.0000 rate:0.1000 gloss:0.7491 dlossA:0.7432 dlossQ:1.0732 exploreP:0.7899\n",
      "Episode:83 meanR:28.6429 R:20.0000 rate:0.0400 gloss:0.7486 dlossA:0.7417 dlossQ:1.0695 exploreP:0.7883\n",
      "Episode:84 meanR:29.0824 R:66.0000 rate:0.1320 gloss:0.7385 dlossA:0.7384 dlossQ:1.0639 exploreP:0.7832\n",
      "Episode:85 meanR:29.3023 R:48.0000 rate:0.0960 gloss:0.7448 dlossA:0.7391 dlossQ:1.0653 exploreP:0.7795\n",
      "Episode:86 meanR:29.5862 R:54.0000 rate:0.1080 gloss:0.7414 dlossA:0.7422 dlossQ:1.0578 exploreP:0.7753\n",
      "Episode:87 meanR:29.6364 R:34.0000 rate:0.0680 gloss:0.7563 dlossA:0.7399 dlossQ:1.0822 exploreP:0.7727\n",
      "Episode:88 meanR:30.0000 R:62.0000 rate:0.1240 gloss:0.7513 dlossA:0.7436 dlossQ:1.0742 exploreP:0.7680\n",
      "Episode:89 meanR:29.7889 R:11.0000 rate:0.0220 gloss:0.7596 dlossA:0.7431 dlossQ:1.0807 exploreP:0.7672\n",
      "Episode:90 meanR:29.7033 R:22.0000 rate:0.0440 gloss:0.7541 dlossA:0.7431 dlossQ:1.0781 exploreP:0.7655\n",
      "Episode:91 meanR:30.5543 R:108.0000 rate:0.2160 gloss:0.7532 dlossA:0.7444 dlossQ:1.0815 exploreP:0.7574\n",
      "Episode:92 meanR:30.7527 R:49.0000 rate:0.0980 gloss:0.7476 dlossA:0.7412 dlossQ:1.0666 exploreP:0.7537\n",
      "Episode:93 meanR:31.0106 R:55.0000 rate:0.1100 gloss:0.7471 dlossA:0.7412 dlossQ:1.0658 exploreP:0.7497\n",
      "Episode:94 meanR:30.9474 R:25.0000 rate:0.0500 gloss:0.7563 dlossA:0.7472 dlossQ:1.0791 exploreP:0.7478\n",
      "Episode:95 meanR:30.7188 R:9.0000 rate:0.0180 gloss:0.7458 dlossA:0.7425 dlossQ:1.0673 exploreP:0.7472\n",
      "Episode:96 meanR:30.9072 R:49.0000 rate:0.0980 gloss:0.7644 dlossA:0.7485 dlossQ:1.0940 exploreP:0.7436\n",
      "Episode:97 meanR:30.7755 R:18.0000 rate:0.0360 gloss:0.7376 dlossA:0.7428 dlossQ:1.0547 exploreP:0.7422\n",
      "Episode:98 meanR:30.7273 R:26.0000 rate:0.0520 gloss:0.7670 dlossA:0.7448 dlossQ:1.0791 exploreP:0.7403\n",
      "Episode:99 meanR:30.6700 R:25.0000 rate:0.0500 gloss:0.7555 dlossA:0.7447 dlossQ:1.0757 exploreP:0.7385\n",
      "Episode:100 meanR:30.5600 R:13.0000 rate:0.0260 gloss:0.7442 dlossA:0.7433 dlossQ:1.0417 exploreP:0.7376\n",
      "Episode:101 meanR:32.2200 R:183.0000 rate:0.3660 gloss:0.7532 dlossA:0.7448 dlossQ:1.0780 exploreP:0.7244\n",
      "Episode:102 meanR:32.5200 R:42.0000 rate:0.0840 gloss:0.7559 dlossA:0.7459 dlossQ:1.0858 exploreP:0.7214\n",
      "Episode:103 meanR:32.3700 R:11.0000 rate:0.0220 gloss:0.7585 dlossA:0.7441 dlossQ:1.0879 exploreP:0.7206\n",
      "Episode:104 meanR:32.4500 R:24.0000 rate:0.0480 gloss:0.7404 dlossA:0.7401 dlossQ:1.0536 exploreP:0.7189\n",
      "Episode:105 meanR:33.1400 R:91.0000 rate:0.1820 gloss:0.7520 dlossA:0.7439 dlossQ:1.0716 exploreP:0.7125\n",
      "Episode:106 meanR:33.2800 R:24.0000 rate:0.0480 gloss:0.7452 dlossA:0.7417 dlossQ:1.0730 exploreP:0.7108\n",
      "Episode:107 meanR:33.3000 R:34.0000 rate:0.0680 gloss:0.7549 dlossA:0.7462 dlossQ:1.0627 exploreP:0.7084\n",
      "Episode:108 meanR:33.0800 R:34.0000 rate:0.0680 gloss:0.7601 dlossA:0.7457 dlossQ:1.0945 exploreP:0.7060\n",
      "Episode:109 meanR:33.5000 R:66.0000 rate:0.1320 gloss:0.7651 dlossA:0.7459 dlossQ:1.0687 exploreP:0.7015\n",
      "Episode:110 meanR:33.4800 R:29.0000 rate:0.0580 gloss:0.7576 dlossA:0.7479 dlossQ:1.0765 exploreP:0.6995\n",
      "Episode:111 meanR:33.4400 R:21.0000 rate:0.0420 gloss:0.7460 dlossA:0.7426 dlossQ:1.0704 exploreP:0.6980\n",
      "Episode:112 meanR:33.4100 R:32.0000 rate:0.0640 gloss:0.7628 dlossA:0.7503 dlossQ:1.0712 exploreP:0.6958\n",
      "Episode:113 meanR:33.3800 R:36.0000 rate:0.0720 gloss:0.7594 dlossA:0.7445 dlossQ:1.0770 exploreP:0.6933\n",
      "Episode:114 meanR:34.0500 R:87.0000 rate:0.1740 gloss:0.7617 dlossA:0.7475 dlossQ:1.0811 exploreP:0.6874\n",
      "Episode:115 meanR:34.8100 R:91.0000 rate:0.1820 gloss:0.7585 dlossA:0.7468 dlossQ:1.0700 exploreP:0.6813\n",
      "Episode:116 meanR:34.9500 R:41.0000 rate:0.0820 gloss:0.7495 dlossA:0.7447 dlossQ:1.0558 exploreP:0.6785\n",
      "Episode:117 meanR:35.0800 R:37.0000 rate:0.0740 gloss:0.7608 dlossA:0.7499 dlossQ:1.0779 exploreP:0.6761\n",
      "Episode:118 meanR:35.7000 R:82.0000 rate:0.1640 gloss:0.7625 dlossA:0.7465 dlossQ:1.0797 exploreP:0.6706\n",
      "Episode:119 meanR:36.4000 R:85.0000 rate:0.1700 gloss:0.7558 dlossA:0.7452 dlossQ:1.0719 exploreP:0.6650\n",
      "Episode:120 meanR:36.3900 R:19.0000 rate:0.0380 gloss:0.7504 dlossA:0.7411 dlossQ:1.0846 exploreP:0.6638\n",
      "Episode:121 meanR:36.5800 R:50.0000 rate:0.1000 gloss:0.7652 dlossA:0.7507 dlossQ:1.0882 exploreP:0.6605\n",
      "Episode:122 meanR:36.5300 R:24.0000 rate:0.0480 gloss:0.7651 dlossA:0.7528 dlossQ:1.0881 exploreP:0.6590\n",
      "Episode:123 meanR:37.9900 R:169.0000 rate:0.3380 gloss:0.7551 dlossA:0.7455 dlossQ:1.0707 exploreP:0.6481\n",
      "Episode:124 meanR:37.8300 R:32.0000 rate:0.0640 gloss:0.7539 dlossA:0.7488 dlossQ:1.0625 exploreP:0.6461\n",
      "Episode:125 meanR:37.9400 R:38.0000 rate:0.0760 gloss:0.7424 dlossA:0.7430 dlossQ:1.0614 exploreP:0.6437\n",
      "Episode:126 meanR:38.6400 R:114.0000 rate:0.2280 gloss:0.7556 dlossA:0.7471 dlossQ:1.0654 exploreP:0.6365\n",
      "Episode:127 meanR:40.3400 R:218.0000 rate:0.4360 gloss:0.7588 dlossA:0.7483 dlossQ:1.0752 exploreP:0.6230\n",
      "Episode:128 meanR:40.4700 R:30.0000 rate:0.0600 gloss:0.7589 dlossA:0.7472 dlossQ:1.0638 exploreP:0.6211\n",
      "Episode:129 meanR:40.6700 R:47.0000 rate:0.0940 gloss:0.7617 dlossA:0.7500 dlossQ:1.0705 exploreP:0.6183\n",
      "Episode:130 meanR:40.9800 R:45.0000 rate:0.0900 gloss:0.7523 dlossA:0.7445 dlossQ:1.0753 exploreP:0.6155\n",
      "Episode:131 meanR:41.6700 R:93.0000 rate:0.1860 gloss:0.7563 dlossA:0.7474 dlossQ:1.0752 exploreP:0.6099\n",
      "Episode:132 meanR:42.1500 R:71.0000 rate:0.1420 gloss:0.7584 dlossA:0.7486 dlossQ:1.0690 exploreP:0.6057\n",
      "Episode:133 meanR:42.8100 R:82.0000 rate:0.1640 gloss:0.7616 dlossA:0.7500 dlossQ:1.0749 exploreP:0.6008\n",
      "Episode:134 meanR:44.0700 R:142.0000 rate:0.2840 gloss:0.7609 dlossA:0.7482 dlossQ:1.0751 exploreP:0.5925\n",
      "Episode:135 meanR:44.7900 R:84.0000 rate:0.1680 gloss:0.7552 dlossA:0.7483 dlossQ:1.0616 exploreP:0.5876\n",
      "Episode:136 meanR:45.0500 R:49.0000 rate:0.0980 gloss:0.7552 dlossA:0.7458 dlossQ:1.0645 exploreP:0.5848\n",
      "Episode:137 meanR:46.1200 R:132.0000 rate:0.2640 gloss:0.7600 dlossA:0.7478 dlossQ:1.0713 exploreP:0.5773\n",
      "Episode:138 meanR:46.2100 R:29.0000 rate:0.0580 gloss:0.7619 dlossA:0.7510 dlossQ:1.0660 exploreP:0.5756\n",
      "Episode:139 meanR:47.5300 R:144.0000 rate:0.2880 gloss:0.7564 dlossA:0.7483 dlossQ:1.0645 exploreP:0.5675\n",
      "Episode:140 meanR:49.3900 R:202.0000 rate:0.4040 gloss:0.7654 dlossA:0.7507 dlossQ:1.0743 exploreP:0.5564\n",
      "Episode:141 meanR:51.0000 R:187.0000 rate:0.3740 gloss:0.7654 dlossA:0.7533 dlossQ:1.0699 exploreP:0.5463\n",
      "Episode:142 meanR:53.3800 R:272.0000 rate:0.5440 gloss:0.7658 dlossA:0.7519 dlossQ:1.0669 exploreP:0.5319\n",
      "Episode:143 meanR:54.5800 R:161.0000 rate:0.3220 gloss:0.7629 dlossA:0.7532 dlossQ:1.0602 exploreP:0.5235\n",
      "Episode:144 meanR:55.9000 R:165.0000 rate:0.3300 gloss:0.7686 dlossA:0.7529 dlossQ:1.0659 exploreP:0.5151\n",
      "Episode:145 meanR:57.1500 R:208.0000 rate:0.4160 gloss:0.7738 dlossA:0.7541 dlossQ:1.0628 exploreP:0.5047\n",
      "Episode:146 meanR:58.2600 R:134.0000 rate:0.2680 gloss:0.7692 dlossA:0.7557 dlossQ:1.0578 exploreP:0.4981\n",
      "Episode:147 meanR:61.4200 R:364.0000 rate:0.7280 gloss:0.7773 dlossA:0.7573 dlossQ:1.0659 exploreP:0.4807\n",
      "Episode:148 meanR:62.1700 R:94.0000 rate:0.1880 gloss:0.7765 dlossA:0.7605 dlossQ:1.0518 exploreP:0.4763\n",
      "Episode:149 meanR:62.7100 R:72.0000 rate:0.1440 gloss:0.7785 dlossA:0.7593 dlossQ:1.0725 exploreP:0.4729\n",
      "Episode:150 meanR:63.3800 R:82.0000 rate:0.1640 gloss:0.7780 dlossA:0.7583 dlossQ:1.0595 exploreP:0.4692\n",
      "Episode:151 meanR:66.0500 R:292.0000 rate:0.5840 gloss:0.7800 dlossA:0.7599 dlossQ:1.0574 exploreP:0.4559\n",
      "Episode:152 meanR:70.4800 R:457.0000 rate:0.9140 gloss:0.7880 dlossA:0.7650 dlossQ:1.0628 exploreP:0.4360\n",
      "Episode:153 meanR:71.4200 R:122.0000 rate:0.2440 gloss:0.7840 dlossA:0.7614 dlossQ:1.0531 exploreP:0.4309\n",
      "Episode:154 meanR:72.8100 R:160.0000 rate:0.3200 gloss:0.7933 dlossA:0.7665 dlossQ:1.0558 exploreP:0.4242\n",
      "Episode:155 meanR:77.5500 R:500.0000 rate:1.0000 gloss:0.8016 dlossA:0.7709 dlossQ:1.0555 exploreP:0.4040\n",
      "Episode:156 meanR:80.3500 R:307.0000 rate:0.6140 gloss:0.8135 dlossA:0.7804 dlossQ:1.0536 exploreP:0.3921\n",
      "Episode:157 meanR:82.7200 R:276.0000 rate:0.5520 gloss:0.8190 dlossA:0.7811 dlossQ:1.0508 exploreP:0.3817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:84.6600 R:256.0000 rate:0.5120 gloss:0.8243 dlossA:0.7859 dlossQ:1.0565 exploreP:0.3723\n",
      "Episode:159 meanR:85.4100 R:111.0000 rate:0.2220 gloss:0.8285 dlossA:0.7865 dlossQ:1.0491 exploreP:0.3683\n",
      "Episode:160 meanR:88.3400 R:322.0000 rate:0.6440 gloss:0.8364 dlossA:0.7917 dlossQ:1.0540 exploreP:0.3569\n",
      "Episode:161 meanR:89.9300 R:206.0000 rate:0.4120 gloss:0.8480 dlossA:0.7966 dlossQ:1.0510 exploreP:0.3499\n",
      "Episode:162 meanR:93.6100 R:384.0000 rate:0.7680 gloss:0.8632 dlossA:0.8042 dlossQ:1.0458 exploreP:0.3370\n",
      "Episode:163 meanR:98.3800 R:500.0000 rate:1.0000 gloss:0.8713 dlossA:0.8121 dlossQ:1.0442 exploreP:0.3211\n",
      "Episode:164 meanR:99.5000 R:147.0000 rate:0.2940 gloss:0.8850 dlossA:0.8196 dlossQ:1.0440 exploreP:0.3166\n",
      "Episode:165 meanR:100.9900 R:177.0000 rate:0.3540 gloss:0.8829 dlossA:0.8177 dlossQ:1.0359 exploreP:0.3112\n",
      "Episode:166 meanR:101.9200 R:187.0000 rate:0.3740 gloss:0.9000 dlossA:0.8310 dlossQ:1.0467 exploreP:0.3056\n",
      "Episode:167 meanR:103.5900 R:188.0000 rate:0.3760 gloss:0.9010 dlossA:0.8236 dlossQ:1.0402 exploreP:0.3001\n",
      "Episode:168 meanR:105.5300 R:208.0000 rate:0.4160 gloss:0.9151 dlossA:0.8379 dlossQ:1.0415 exploreP:0.2941\n",
      "Episode:169 meanR:106.3800 R:140.0000 rate:0.2800 gloss:0.9206 dlossA:0.8412 dlossQ:1.0351 exploreP:0.2902\n",
      "Episode:170 meanR:110.0100 R:402.0000 rate:0.8040 gloss:0.9340 dlossA:0.8432 dlossQ:1.0395 exploreP:0.2791\n",
      "Episode:171 meanR:112.7200 R:290.0000 rate:0.5800 gloss:0.9592 dlossA:0.8567 dlossQ:1.0354 exploreP:0.2714\n",
      "Episode:172 meanR:117.4200 R:500.0000 rate:1.0000 gloss:0.9786 dlossA:0.8742 dlossQ:1.0311 exploreP:0.2587\n",
      "Episode:173 meanR:120.6600 R:346.0000 rate:0.6920 gloss:0.9927 dlossA:0.8774 dlossQ:1.0266 exploreP:0.2502\n",
      "Episode:174 meanR:125.5600 R:500.0000 rate:1.0000 gloss:1.0180 dlossA:0.8954 dlossQ:1.0237 exploreP:0.2385\n",
      "Episode:175 meanR:129.2900 R:405.0000 rate:0.8100 gloss:1.0472 dlossA:0.9136 dlossQ:1.0233 exploreP:0.2294\n",
      "Episode:176 meanR:131.5900 R:256.0000 rate:0.5120 gloss:1.0645 dlossA:0.9233 dlossQ:1.0221 exploreP:0.2239\n",
      "Episode:177 meanR:135.3400 R:397.0000 rate:0.7940 gloss:1.0909 dlossA:0.9363 dlossQ:1.0110 exploreP:0.2156\n",
      "Episode:178 meanR:139.4000 R:445.0000 rate:0.8900 gloss:1.1175 dlossA:0.9553 dlossQ:1.0161 exploreP:0.2066\n",
      "Episode:179 meanR:142.9600 R:382.0000 rate:0.7640 gloss:1.1486 dlossA:0.9716 dlossQ:1.0141 exploreP:0.1993\n",
      "Episode:180 meanR:147.3300 R:500.0000 rate:1.0000 gloss:1.1846 dlossA:1.0076 dlossQ:1.0088 exploreP:0.1900\n",
      "Episode:181 meanR:152.1000 R:500.0000 rate:1.0000 gloss:1.2047 dlossA:1.0053 dlossQ:1.0098 exploreP:0.1812\n",
      "Episode:182 meanR:155.8800 R:428.0000 rate:0.8560 gloss:1.2512 dlossA:1.0459 dlossQ:1.0036 exploreP:0.1741\n",
      "Episode:183 meanR:160.6800 R:500.0000 rate:1.0000 gloss:1.2756 dlossA:1.0514 dlossQ:1.0072 exploreP:0.1661\n",
      "Episode:184 meanR:164.1800 R:416.0000 rate:0.8320 gloss:1.3108 dlossA:1.0701 dlossQ:1.0046 exploreP:0.1597\n",
      "Episode:185 meanR:168.3200 R:462.0000 rate:0.9240 gloss:1.3520 dlossA:1.1089 dlossQ:1.0065 exploreP:0.1530\n",
      "Episode:186 meanR:172.7800 R:500.0000 rate:1.0000 gloss:1.3953 dlossA:1.1259 dlossQ:1.0013 exploreP:0.1460\n",
      "Episode:187 meanR:177.4400 R:500.0000 rate:1.0000 gloss:1.4449 dlossA:1.1763 dlossQ:1.0084 exploreP:0.1393\n",
      "Episode:188 meanR:181.8200 R:500.0000 rate:1.0000 gloss:1.4747 dlossA:1.2196 dlossQ:1.0029 exploreP:0.1330\n",
      "Episode:189 meanR:185.5900 R:388.0000 rate:0.7760 gloss:1.5111 dlossA:1.2042 dlossQ:1.0173 exploreP:0.1284\n",
      "Episode:190 meanR:190.3700 R:500.0000 rate:1.0000 gloss:1.5534 dlossA:1.2370 dlossQ:1.0141 exploreP:0.1226\n",
      "Episode:191 meanR:194.2900 R:500.0000 rate:1.0000 gloss:1.6061 dlossA:1.3136 dlossQ:1.0247 exploreP:0.1171\n",
      "Episode:192 meanR:198.8000 R:500.0000 rate:1.0000 gloss:1.6613 dlossA:1.3061 dlossQ:1.0289 exploreP:0.1119\n",
      "Episode:193 meanR:200.7700 R:252.0000 rate:0.5040 gloss:1.7048 dlossA:1.3417 dlossQ:1.0396 exploreP:0.1093\n",
      "Episode:194 meanR:203.8800 R:336.0000 rate:0.6720 gloss:1.7477 dlossA:1.3557 dlossQ:1.0526 exploreP:0.1061\n",
      "Episode:195 meanR:205.7800 R:199.0000 rate:0.3980 gloss:1.7977 dlossA:1.4340 dlossQ:1.0706 exploreP:0.1042\n",
      "Episode:196 meanR:210.2900 R:500.0000 rate:1.0000 gloss:1.8317 dlossA:1.4424 dlossQ:1.0714 exploreP:0.0996\n",
      "Episode:197 meanR:215.1100 R:500.0000 rate:1.0000 gloss:1.9059 dlossA:1.4926 dlossQ:1.0787 exploreP:0.0952\n",
      "Episode:198 meanR:219.8500 R:500.0000 rate:1.0000 gloss:1.9383 dlossA:1.5180 dlossQ:1.0848 exploreP:0.0910\n",
      "Episode:199 meanR:224.6000 R:500.0000 rate:1.0000 gloss:1.9932 dlossA:1.5896 dlossQ:1.0870 exploreP:0.0871\n",
      "Episode:200 meanR:228.1700 R:370.0000 rate:0.7400 gloss:2.0458 dlossA:1.5932 dlossQ:1.0962 exploreP:0.0843\n",
      "Episode:201 meanR:231.3400 R:500.0000 rate:1.0000 gloss:2.0977 dlossA:1.6739 dlossQ:1.1045 exploreP:0.0807\n",
      "Episode:202 meanR:235.9200 R:500.0000 rate:1.0000 gloss:2.1716 dlossA:1.6762 dlossQ:1.1188 exploreP:0.0772\n",
      "Episode:203 meanR:238.3000 R:249.0000 rate:0.4980 gloss:2.2438 dlossA:1.7061 dlossQ:1.1331 exploreP:0.0756\n",
      "Episode:204 meanR:243.0600 R:500.0000 rate:1.0000 gloss:2.3057 dlossA:1.8412 dlossQ:1.1552 exploreP:0.0724\n",
      "Episode:205 meanR:246.9900 R:484.0000 rate:0.9680 gloss:2.3658 dlossA:1.8290 dlossQ:1.1589 exploreP:0.0694\n",
      "Episode:206 meanR:251.7500 R:500.0000 rate:1.0000 gloss:2.4428 dlossA:1.9284 dlossQ:1.1885 exploreP:0.0665\n",
      "Episode:207 meanR:256.4100 R:500.0000 rate:1.0000 gloss:2.5310 dlossA:1.9847 dlossQ:1.2104 exploreP:0.0638\n",
      "Episode:208 meanR:261.0700 R:500.0000 rate:1.0000 gloss:2.6217 dlossA:2.0728 dlossQ:1.2368 exploreP:0.0611\n",
      "Episode:209 meanR:262.9600 R:255.0000 rate:0.5100 gloss:2.6886 dlossA:2.1512 dlossQ:1.2361 exploreP:0.0599\n",
      "Episode:210 meanR:267.6700 R:500.0000 rate:1.0000 gloss:2.7582 dlossA:2.2157 dlossQ:1.2773 exploreP:0.0574\n",
      "Episode:211 meanR:272.4600 R:500.0000 rate:1.0000 gloss:2.8347 dlossA:2.2034 dlossQ:1.2841 exploreP:0.0551\n",
      "Episode:212 meanR:277.1400 R:500.0000 rate:1.0000 gloss:2.9620 dlossA:2.3438 dlossQ:1.3457 exploreP:0.0529\n",
      "Episode:213 meanR:281.7800 R:500.0000 rate:1.0000 gloss:3.0497 dlossA:2.4211 dlossQ:1.3884 exploreP:0.0508\n",
      "Episode:214 meanR:285.9100 R:500.0000 rate:1.0000 gloss:3.1683 dlossA:2.4843 dlossQ:1.3871 exploreP:0.0488\n",
      "Episode:215 meanR:290.0000 R:500.0000 rate:1.0000 gloss:3.2885 dlossA:2.6035 dlossQ:1.4281 exploreP:0.0469\n",
      "Episode:216 meanR:292.7300 R:314.0000 rate:0.6280 gloss:3.4010 dlossA:2.6370 dlossQ:1.4605 exploreP:0.0458\n",
      "Episode:217 meanR:297.3600 R:500.0000 rate:1.0000 gloss:3.5295 dlossA:2.7847 dlossQ:1.5705 exploreP:0.0440\n",
      "Episode:218 meanR:301.5400 R:500.0000 rate:1.0000 gloss:3.6430 dlossA:2.8535 dlossQ:1.5684 exploreP:0.0424\n",
      "Episode:219 meanR:305.6900 R:500.0000 rate:1.0000 gloss:3.8554 dlossA:3.0943 dlossQ:1.7302 exploreP:0.0408\n",
      "Episode:220 meanR:308.8700 R:337.0000 rate:0.6740 gloss:3.9337 dlossA:3.2789 dlossQ:1.6762 exploreP:0.0398\n",
      "Episode:221 meanR:313.3700 R:500.0000 rate:1.0000 gloss:4.0149 dlossA:3.3536 dlossQ:1.7512 exploreP:0.0383\n",
      "Episode:222 meanR:318.1300 R:500.0000 rate:1.0000 gloss:4.1215 dlossA:3.1792 dlossQ:1.9011 exploreP:0.0370\n",
      "Episode:223 meanR:321.4400 R:500.0000 rate:1.0000 gloss:4.3650 dlossA:3.4121 dlossQ:1.9014 exploreP:0.0356\n",
      "Episode:224 meanR:326.1200 R:500.0000 rate:1.0000 gloss:4.5331 dlossA:3.5888 dlossQ:1.9979 exploreP:0.0344\n",
      "Episode:225 meanR:330.7400 R:500.0000 rate:1.0000 gloss:4.7223 dlossA:3.8529 dlossQ:2.1108 exploreP:0.0332\n",
      "Episode:226 meanR:334.6000 R:500.0000 rate:1.0000 gloss:4.8220 dlossA:3.9198 dlossQ:2.1218 exploreP:0.0321\n",
      "Episode:227 meanR:335.2100 R:279.0000 rate:0.5580 gloss:4.9688 dlossA:4.0425 dlossQ:2.2652 exploreP:0.0315\n",
      "Episode:228 meanR:339.9100 R:500.0000 rate:1.0000 gloss:5.1946 dlossA:4.1625 dlossQ:2.3928 exploreP:0.0304\n",
      "Episode:229 meanR:344.4400 R:500.0000 rate:1.0000 gloss:5.2982 dlossA:4.3639 dlossQ:2.5399 exploreP:0.0294\n",
      "Episode:230 meanR:346.3100 R:232.0000 rate:0.4640 gloss:5.4255 dlossA:4.3754 dlossQ:2.6079 exploreP:0.0290\n",
      "Episode:231 meanR:350.3800 R:500.0000 rate:1.0000 gloss:5.4970 dlossA:4.3342 dlossQ:2.5467 exploreP:0.0280\n",
      "Episode:232 meanR:352.3200 R:265.0000 rate:0.5300 gloss:5.7751 dlossA:4.4427 dlossQ:2.8395 exploreP:0.0276\n",
      "Episode:233 meanR:356.5000 R:500.0000 rate:1.0000 gloss:6.0104 dlossA:4.6070 dlossQ:3.0819 exploreP:0.0267\n",
      "Episode:234 meanR:360.0800 R:500.0000 rate:1.0000 gloss:6.2089 dlossA:5.1504 dlossQ:3.0377 exploreP:0.0259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:235 meanR:364.2400 R:500.0000 rate:1.0000 gloss:6.5525 dlossA:5.1140 dlossQ:3.5825 exploreP:0.0251\n",
      "Episode:236 meanR:368.7500 R:500.0000 rate:1.0000 gloss:6.6001 dlossA:5.0678 dlossQ:3.7415 exploreP:0.0244\n",
      "Episode:237 meanR:372.4300 R:500.0000 rate:1.0000 gloss:6.9043 dlossA:5.4634 dlossQ:3.7174 exploreP:0.0237\n",
      "Episode:238 meanR:377.1400 R:500.0000 rate:1.0000 gloss:7.1222 dlossA:5.5503 dlossQ:4.1031 exploreP:0.0230\n",
      "Episode:239 meanR:380.7000 R:500.0000 rate:1.0000 gloss:7.4029 dlossA:5.7215 dlossQ:4.2929 exploreP:0.0224\n",
      "Episode:240 meanR:383.6800 R:500.0000 rate:1.0000 gloss:7.6852 dlossA:5.9864 dlossQ:4.5288 exploreP:0.0218\n",
      "Episode:241 meanR:385.2600 R:345.0000 rate:0.6900 gloss:7.8948 dlossA:6.1228 dlossQ:4.7433 exploreP:0.0214\n",
      "Episode:242 meanR:387.5400 R:500.0000 rate:1.0000 gloss:8.0402 dlossA:6.1384 dlossQ:4.6235 exploreP:0.0208\n",
      "Episode:243 meanR:390.9300 R:500.0000 rate:1.0000 gloss:8.4995 dlossA:6.2340 dlossQ:5.4241 exploreP:0.0203\n",
      "Episode:244 meanR:394.2800 R:500.0000 rate:1.0000 gloss:8.8748 dlossA:6.4428 dlossQ:5.8611 exploreP:0.0198\n",
      "Episode:245 meanR:397.2000 R:500.0000 rate:1.0000 gloss:9.6128 dlossA:7.0053 dlossQ:7.0343 exploreP:0.0193\n",
      "Episode:246 meanR:400.8600 R:500.0000 rate:1.0000 gloss:9.9182 dlossA:7.6429 dlossQ:6.9907 exploreP:0.0189\n",
      "Episode:247 meanR:402.2200 R:500.0000 rate:1.0000 gloss:10.1350 dlossA:7.6964 dlossQ:8.2462 exploreP:0.0184\n",
      "Episode:248 meanR:406.2800 R:500.0000 rate:1.0000 gloss:10.3063 dlossA:7.7345 dlossQ:7.8849 exploreP:0.0180\n",
      "Episode:249 meanR:410.5600 R:500.0000 rate:1.0000 gloss:10.6914 dlossA:7.9054 dlossQ:9.4008 exploreP:0.0176\n",
      "Episode:250 meanR:414.7400 R:500.0000 rate:1.0000 gloss:10.6364 dlossA:7.4492 dlossQ:8.1502 exploreP:0.0173\n",
      "Episode:251 meanR:416.8200 R:500.0000 rate:1.0000 gloss:11.4578 dlossA:7.7364 dlossQ:10.3291 exploreP:0.0169\n",
      "Episode:252 meanR:417.2500 R:500.0000 rate:1.0000 gloss:11.7605 dlossA:8.6772 dlossQ:10.5128 exploreP:0.0166\n",
      "Episode:253 meanR:421.0300 R:500.0000 rate:1.0000 gloss:11.9580 dlossA:8.6257 dlossQ:10.4034 exploreP:0.0162\n",
      "Episode:254 meanR:424.4300 R:500.0000 rate:1.0000 gloss:11.9716 dlossA:8.6048 dlossQ:9.3983 exploreP:0.0159\n",
      "Episode:255 meanR:422.1600 R:273.0000 rate:0.5460 gloss:12.0547 dlossA:8.6595 dlossQ:9.4305 exploreP:0.0158\n",
      "Episode:256 meanR:424.0900 R:500.0000 rate:1.0000 gloss:12.4144 dlossA:8.5738 dlossQ:9.4613 exploreP:0.0155\n",
      "Episode:257 meanR:426.3300 R:500.0000 rate:1.0000 gloss:13.1201 dlossA:9.0000 dlossQ:10.7582 exploreP:0.0152\n",
      "Episode:258 meanR:428.7700 R:500.0000 rate:1.0000 gloss:13.6427 dlossA:10.4536 dlossQ:11.8805 exploreP:0.0150\n",
      "Episode:259 meanR:431.5300 R:387.0000 rate:0.7740 gloss:13.4783 dlossA:9.4373 dlossQ:10.9704 exploreP:0.0148\n",
      "Episode:260 meanR:433.3100 R:500.0000 rate:1.0000 gloss:14.1093 dlossA:10.3894 dlossQ:12.5940 exploreP:0.0146\n",
      "Episode:261 meanR:436.2500 R:500.0000 rate:1.0000 gloss:14.3923 dlossA:9.9297 dlossQ:14.7222 exploreP:0.0143\n",
      "Episode:262 meanR:437.4100 R:500.0000 rate:1.0000 gloss:14.4843 dlossA:9.7509 dlossQ:12.8092 exploreP:0.0141\n",
      "Episode:263 meanR:437.4100 R:500.0000 rate:1.0000 gloss:15.1591 dlossA:9.9788 dlossQ:14.0628 exploreP:0.0139\n",
      "Episode:264 meanR:440.9400 R:500.0000 rate:1.0000 gloss:15.7559 dlossA:10.4347 dlossQ:18.0489 exploreP:0.0137\n",
      "Episode:265 meanR:444.1700 R:500.0000 rate:1.0000 gloss:16.0013 dlossA:11.3002 dlossQ:17.6301 exploreP:0.0135\n",
      "Episode:266 meanR:447.3000 R:500.0000 rate:1.0000 gloss:16.4320 dlossA:11.6070 dlossQ:17.5455 exploreP:0.0134\n",
      "Episode:267 meanR:449.9800 R:456.0000 rate:0.9120 gloss:16.5037 dlossA:11.0276 dlossQ:15.5598 exploreP:0.0132\n",
      "Episode:268 meanR:452.9000 R:500.0000 rate:1.0000 gloss:17.0142 dlossA:11.6839 dlossQ:17.4356 exploreP:0.0131\n",
      "Episode:269 meanR:456.5000 R:500.0000 rate:1.0000 gloss:17.1137 dlossA:11.2124 dlossQ:16.0442 exploreP:0.0129\n",
      "Episode:270 meanR:457.4800 R:500.0000 rate:1.0000 gloss:17.5596 dlossA:11.9946 dlossQ:16.8365 exploreP:0.0128\n",
      "Episode:271 meanR:459.5800 R:500.0000 rate:1.0000 gloss:17.9123 dlossA:11.7692 dlossQ:18.6011 exploreP:0.0126\n",
      "Episode:272 meanR:459.5800 R:500.0000 rate:1.0000 gloss:18.4032 dlossA:12.1295 dlossQ:17.2988 exploreP:0.0125\n",
      "Episode:273 meanR:461.1200 R:500.0000 rate:1.0000 gloss:18.8640 dlossA:11.9993 dlossQ:19.0905 exploreP:0.0124\n",
      "Episode:274 meanR:461.1200 R:500.0000 rate:1.0000 gloss:19.4573 dlossA:12.8458 dlossQ:18.6893 exploreP:0.0123\n",
      "Episode:275 meanR:462.0700 R:500.0000 rate:1.0000 gloss:19.8942 dlossA:12.9973 dlossQ:19.1149 exploreP:0.0122\n",
      "Episode:276 meanR:464.5100 R:500.0000 rate:1.0000 gloss:20.7243 dlossA:12.9208 dlossQ:21.5373 exploreP:0.0121\n",
      "Episode:277 meanR:465.5400 R:500.0000 rate:1.0000 gloss:21.5680 dlossA:13.2088 dlossQ:22.0100 exploreP:0.0120\n",
      "Episode:278 meanR:465.6900 R:460.0000 rate:0.9200 gloss:22.6850 dlossA:14.4125 dlossQ:24.1848 exploreP:0.0119\n",
      "Episode:279 meanR:466.8700 R:500.0000 rate:1.0000 gloss:22.9045 dlossA:14.8749 dlossQ:23.0800 exploreP:0.0118\n",
      "Episode:280 meanR:466.8700 R:500.0000 rate:1.0000 gloss:23.2945 dlossA:15.1903 dlossQ:28.8068 exploreP:0.0117\n",
      "Episode:281 meanR:466.8700 R:500.0000 rate:1.0000 gloss:22.6120 dlossA:14.1918 dlossQ:21.2768 exploreP:0.0116\n",
      "Episode:282 meanR:467.5900 R:500.0000 rate:1.0000 gloss:22.4171 dlossA:13.6015 dlossQ:21.3165 exploreP:0.0115\n",
      "Episode:283 meanR:467.5900 R:500.0000 rate:1.0000 gloss:21.7646 dlossA:12.8038 dlossQ:18.9313 exploreP:0.0115\n",
      "Episode:284 meanR:468.4300 R:500.0000 rate:1.0000 gloss:22.1799 dlossA:12.4524 dlossQ:19.4897 exploreP:0.0114\n",
      "Episode:285 meanR:468.8100 R:500.0000 rate:1.0000 gloss:22.3676 dlossA:12.7357 dlossQ:19.1924 exploreP:0.0113\n",
      "Episode:286 meanR:468.8100 R:500.0000 rate:1.0000 gloss:22.7719 dlossA:12.9024 dlossQ:22.0648 exploreP:0.0113\n",
      "Episode:287 meanR:468.8100 R:500.0000 rate:1.0000 gloss:23.7750 dlossA:14.7918 dlossQ:22.1281 exploreP:0.0112\n",
      "Episode:288 meanR:468.8100 R:500.0000 rate:1.0000 gloss:23.3560 dlossA:13.5662 dlossQ:25.2812 exploreP:0.0111\n",
      "Episode:289 meanR:469.9300 R:500.0000 rate:1.0000 gloss:23.2996 dlossA:13.4805 dlossQ:25.3974 exploreP:0.0111\n",
      "Episode:290 meanR:469.9300 R:500.0000 rate:1.0000 gloss:21.9960 dlossA:12.8960 dlossQ:25.3983 exploreP:0.0110\n",
      "Episode:291 meanR:469.9300 R:500.0000 rate:1.0000 gloss:20.8946 dlossA:11.8846 dlossQ:23.1170 exploreP:0.0110\n",
      "Episode:292 meanR:469.9300 R:500.0000 rate:1.0000 gloss:19.7251 dlossA:11.0027 dlossQ:24.3961 exploreP:0.0109\n",
      "Episode:293 meanR:472.4100 R:500.0000 rate:1.0000 gloss:17.5216 dlossA:10.4505 dlossQ:24.6754 exploreP:0.0109\n",
      "Episode:294 meanR:474.0500 R:500.0000 rate:1.0000 gloss:15.9992 dlossA:9.0865 dlossQ:23.6216 exploreP:0.0108\n",
      "Episode:295 meanR:477.0600 R:500.0000 rate:1.0000 gloss:13.2053 dlossA:7.2091 dlossQ:24.5509 exploreP:0.0108\n",
      "Episode:296 meanR:477.0600 R:500.0000 rate:1.0000 gloss:10.2341 dlossA:5.4094 dlossQ:28.7866 exploreP:0.0108\n",
      "Episode:297 meanR:477.0600 R:500.0000 rate:1.0000 gloss:7.9186 dlossA:4.4449 dlossQ:25.0759 exploreP:0.0107\n",
      "Episode:298 meanR:477.0600 R:500.0000 rate:1.0000 gloss:5.3265 dlossA:3.0676 dlossQ:27.7786 exploreP:0.0107\n",
      "Episode:299 meanR:477.0600 R:500.0000 rate:1.0000 gloss:3.0106 dlossA:1.9200 dlossQ:25.9384 exploreP:0.0107\n",
      "Episode:300 meanR:478.3600 R:500.0000 rate:1.0000 gloss:0.8650 dlossA:1.2993 dlossQ:30.2959 exploreP:0.0106\n",
      "Episode:301 meanR:478.3600 R:500.0000 rate:1.0000 gloss:-0.7458 dlossA:1.1132 dlossQ:27.6891 exploreP:0.0106\n",
      "Episode:302 meanR:478.3600 R:500.0000 rate:1.0000 gloss:-1.4189 dlossA:1.1172 dlossQ:24.7769 exploreP:0.0106\n",
      "Episode:303 meanR:480.0600 R:419.0000 rate:0.8380 gloss:-1.5731 dlossA:1.0536 dlossQ:20.6827 exploreP:0.0105\n",
      "Episode:304 meanR:476.7800 R:172.0000 rate:0.3440 gloss:-0.8348 dlossA:0.9801 dlossQ:17.2082 exploreP:0.0105\n",
      "Episode:305 meanR:473.7200 R:178.0000 rate:0.3560 gloss:-0.9753 dlossA:1.0308 dlossQ:16.4814 exploreP:0.0105\n",
      "Episode:306 meanR:472.6100 R:389.0000 rate:0.7780 gloss:-0.9899 dlossA:0.9357 dlossQ:15.2269 exploreP:0.0105\n",
      "Episode:307 meanR:470.6000 R:299.0000 rate:0.5980 gloss:-0.4792 dlossA:0.8985 dlossQ:13.1417 exploreP:0.0105\n",
      "Episode:308 meanR:470.6000 R:500.0000 rate:1.0000 gloss:-0.1990 dlossA:0.8283 dlossQ:9.7723 exploreP:0.0105\n",
      "Episode:309 meanR:473.0500 R:500.0000 rate:1.0000 gloss:0.6168 dlossA:0.7278 dlossQ:5.4270 exploreP:0.0104\n",
      "Episode:310 meanR:473.0500 R:500.0000 rate:1.0000 gloss:1.3562 dlossA:0.6952 dlossQ:2.8592 exploreP:0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:311 meanR:473.0500 R:500.0000 rate:1.0000 gloss:2.3683 dlossA:0.7926 dlossQ:1.7137 exploreP:0.0104\n",
      "Episode:312 meanR:473.0500 R:500.0000 rate:1.0000 gloss:3.2104 dlossA:1.0121 dlossQ:1.7048 exploreP:0.0104\n",
      "Episode:313 meanR:473.0500 R:500.0000 rate:1.0000 gloss:4.0950 dlossA:1.4826 dlossQ:2.1137 exploreP:0.0104\n",
      "Episode:314 meanR:473.0500 R:500.0000 rate:1.0000 gloss:5.0501 dlossA:1.7677 dlossQ:2.6955 exploreP:0.0103\n",
      "Episode:315 meanR:473.0500 R:500.0000 rate:1.0000 gloss:6.3846 dlossA:2.3778 dlossQ:3.8662 exploreP:0.0103\n",
      "Episode:316 meanR:474.9100 R:500.0000 rate:1.0000 gloss:7.6884 dlossA:2.7761 dlossQ:5.4986 exploreP:0.0103\n",
      "Episode:317 meanR:474.9100 R:500.0000 rate:1.0000 gloss:9.2876 dlossA:3.7144 dlossQ:7.8351 exploreP:0.0103\n",
      "Episode:318 meanR:474.9100 R:500.0000 rate:1.0000 gloss:10.8816 dlossA:3.7770 dlossQ:11.5222 exploreP:0.0103\n",
      "Episode:319 meanR:474.9100 R:500.0000 rate:1.0000 gloss:13.0233 dlossA:4.9359 dlossQ:16.1032 exploreP:0.0103\n",
      "Episode:320 meanR:476.5400 R:500.0000 rate:1.0000 gloss:15.3656 dlossA:5.5410 dlossQ:23.0099 exploreP:0.0103\n",
      "Episode:321 meanR:476.5400 R:500.0000 rate:1.0000 gloss:16.7828 dlossA:6.4931 dlossQ:26.8418 exploreP:0.0102\n",
      "Episode:322 meanR:476.5400 R:500.0000 rate:1.0000 gloss:19.4784 dlossA:7.0257 dlossQ:39.0096 exploreP:0.0102\n",
      "Episode:323 meanR:476.5400 R:500.0000 rate:1.0000 gloss:21.2892 dlossA:7.2703 dlossQ:46.4876 exploreP:0.0102\n",
      "Episode:324 meanR:476.5400 R:500.0000 rate:1.0000 gloss:24.5097 dlossA:8.8515 dlossQ:57.5485 exploreP:0.0102\n",
      "Episode:325 meanR:476.5400 R:500.0000 rate:1.0000 gloss:26.5063 dlossA:9.7443 dlossQ:68.2975 exploreP:0.0102\n",
      "Episode:326 meanR:476.5400 R:500.0000 rate:1.0000 gloss:29.1954 dlossA:9.5799 dlossQ:80.7224 exploreP:0.0102\n",
      "Episode:327 meanR:478.7500 R:500.0000 rate:1.0000 gloss:31.4377 dlossA:11.0813 dlossQ:88.1114 exploreP:0.0102\n",
      "Episode:328 meanR:478.7500 R:500.0000 rate:1.0000 gloss:36.2337 dlossA:13.7790 dlossQ:132.6861 exploreP:0.0102\n",
      "Episode:329 meanR:478.7500 R:500.0000 rate:1.0000 gloss:36.6801 dlossA:12.1381 dlossQ:118.1594 exploreP:0.0102\n",
      "Episode:330 meanR:481.4300 R:500.0000 rate:1.0000 gloss:39.8651 dlossA:12.1306 dlossQ:139.2635 exploreP:0.0102\n",
      "Episode:331 meanR:481.4300 R:500.0000 rate:1.0000 gloss:42.0436 dlossA:13.1194 dlossQ:146.8670 exploreP:0.0101\n",
      "Episode:332 meanR:483.7800 R:500.0000 rate:1.0000 gloss:47.1889 dlossA:15.2842 dlossQ:178.7742 exploreP:0.0101\n",
      "Episode:333 meanR:483.7800 R:500.0000 rate:1.0000 gloss:49.1739 dlossA:16.4440 dlossQ:176.0665 exploreP:0.0101\n",
      "Episode:334 meanR:483.7800 R:500.0000 rate:1.0000 gloss:52.6477 dlossA:13.6376 dlossQ:209.7062 exploreP:0.0101\n",
      "Episode:335 meanR:483.7800 R:500.0000 rate:1.0000 gloss:59.0743 dlossA:19.9095 dlossQ:242.5209 exploreP:0.0101\n",
      "Episode:336 meanR:483.7800 R:500.0000 rate:1.0000 gloss:63.1808 dlossA:20.9885 dlossQ:260.4606 exploreP:0.0101\n",
      "Episode:337 meanR:483.7800 R:500.0000 rate:1.0000 gloss:64.5197 dlossA:18.8700 dlossQ:264.3833 exploreP:0.0101\n",
      "Episode:338 meanR:483.7800 R:500.0000 rate:1.0000 gloss:68.1537 dlossA:16.1216 dlossQ:302.1062 exploreP:0.0101\n",
      "Episode:339 meanR:483.7800 R:500.0000 rate:1.0000 gloss:72.4918 dlossA:18.0199 dlossQ:317.2578 exploreP:0.0101\n",
      "Episode:340 meanR:483.7800 R:500.0000 rate:1.0000 gloss:81.3894 dlossA:24.1749 dlossQ:489.6526 exploreP:0.0101\n",
      "Episode:341 meanR:485.3300 R:500.0000 rate:1.0000 gloss:79.5713 dlossA:19.9597 dlossQ:358.5812 exploreP:0.0101\n",
      "Episode:342 meanR:485.3300 R:500.0000 rate:1.0000 gloss:83.2238 dlossA:19.2633 dlossQ:407.6127 exploreP:0.0101\n",
      "Episode:343 meanR:485.3300 R:500.0000 rate:1.0000 gloss:83.9865 dlossA:18.7257 dlossQ:394.7220 exploreP:0.0101\n",
      "Episode:344 meanR:485.3300 R:500.0000 rate:1.0000 gloss:95.9179 dlossA:23.2785 dlossQ:553.8143 exploreP:0.0101\n",
      "Episode:345 meanR:485.3300 R:500.0000 rate:1.0000 gloss:96.6524 dlossA:25.8317 dlossQ:507.3092 exploreP:0.0101\n",
      "Episode:346 meanR:485.3300 R:500.0000 rate:1.0000 gloss:100.1463 dlossA:22.6735 dlossQ:661.1932 exploreP:0.0101\n",
      "Episode:347 meanR:485.3300 R:500.0000 rate:1.0000 gloss:102.0027 dlossA:23.2742 dlossQ:520.3627 exploreP:0.0101\n",
      "Episode:348 meanR:485.3300 R:500.0000 rate:1.0000 gloss:109.0850 dlossA:23.8629 dlossQ:736.9058 exploreP:0.0101\n",
      "Episode:349 meanR:485.3300 R:500.0000 rate:1.0000 gloss:109.5838 dlossA:24.8451 dlossQ:544.5945 exploreP:0.0101\n",
      "Episode:350 meanR:485.3300 R:500.0000 rate:1.0000 gloss:108.6131 dlossA:15.6967 dlossQ:527.3299 exploreP:0.0101\n",
      "Episode:351 meanR:485.3300 R:500.0000 rate:1.0000 gloss:114.8449 dlossA:18.4733 dlossQ:597.6336 exploreP:0.0101\n",
      "Episode:352 meanR:485.3300 R:500.0000 rate:1.0000 gloss:126.5643 dlossA:24.5557 dlossQ:667.7980 exploreP:0.0101\n",
      "Episode:353 meanR:485.3300 R:500.0000 rate:1.0000 gloss:126.3468 dlossA:18.7306 dlossQ:669.9673 exploreP:0.0100\n",
      "Episode:354 meanR:485.3300 R:500.0000 rate:1.0000 gloss:137.1758 dlossA:18.4403 dlossQ:821.1334 exploreP:0.0100\n",
      "Episode:355 meanR:487.6000 R:500.0000 rate:1.0000 gloss:144.1933 dlossA:19.4324 dlossQ:862.6694 exploreP:0.0100\n",
      "Episode:356 meanR:487.6000 R:500.0000 rate:1.0000 gloss:157.9212 dlossA:23.7791 dlossQ:915.5387 exploreP:0.0100\n",
      "Episode:357 meanR:487.6000 R:500.0000 rate:1.0000 gloss:159.6938 dlossA:19.1128 dlossQ:1017.8544 exploreP:0.0100\n",
      "Episode:358 meanR:487.6000 R:500.0000 rate:1.0000 gloss:178.1995 dlossA:23.1143 dlossQ:1690.8977 exploreP:0.0100\n",
      "Episode:359 meanR:488.7300 R:500.0000 rate:1.0000 gloss:170.3087 dlossA:16.8226 dlossQ:928.3571 exploreP:0.0100\n",
      "Episode:360 meanR:488.7300 R:500.0000 rate:1.0000 gloss:173.2371 dlossA:14.1428 dlossQ:1274.1951 exploreP:0.0100\n",
      "Episode:361 meanR:488.7300 R:500.0000 rate:1.0000 gloss:170.6348 dlossA:12.4918 dlossQ:984.5536 exploreP:0.0100\n",
      "Episode:362 meanR:488.7300 R:500.0000 rate:1.0000 gloss:181.8884 dlossA:13.0554 dlossQ:985.6251 exploreP:0.0100\n",
      "Episode:363 meanR:488.7300 R:500.0000 rate:1.0000 gloss:183.7876 dlossA:14.2723 dlossQ:1106.5227 exploreP:0.0100\n",
      "Episode:364 meanR:488.7300 R:500.0000 rate:1.0000 gloss:181.6679 dlossA:11.0576 dlossQ:1314.6063 exploreP:0.0100\n",
      "Episode:365 meanR:488.7300 R:500.0000 rate:1.0000 gloss:173.5807 dlossA:13.8243 dlossQ:1014.7146 exploreP:0.0100\n",
      "Episode:366 meanR:488.7300 R:500.0000 rate:1.0000 gloss:168.0380 dlossA:5.2271 dlossQ:967.8118 exploreP:0.0100\n",
      "Episode:367 meanR:489.1700 R:500.0000 rate:1.0000 gloss:172.5433 dlossA:9.9287 dlossQ:941.3515 exploreP:0.0100\n",
      "Episode:368 meanR:489.1700 R:500.0000 rate:1.0000 gloss:164.8185 dlossA:9.4602 dlossQ:865.7228 exploreP:0.0100\n",
      "Episode:369 meanR:489.1700 R:500.0000 rate:1.0000 gloss:138.8725 dlossA:4.3655 dlossQ:609.1244 exploreP:0.0100\n",
      "Episode:370 meanR:489.1700 R:500.0000 rate:1.0000 gloss:124.1464 dlossA:4.3688 dlossQ:512.9044 exploreP:0.0100\n",
      "Episode:371 meanR:489.1700 R:500.0000 rate:1.0000 gloss:95.8965 dlossA:2.3049 dlossQ:359.4132 exploreP:0.0100\n",
      "Episode:372 meanR:489.1700 R:500.0000 rate:1.0000 gloss:82.4244 dlossA:2.6787 dlossQ:534.6848 exploreP:0.0100\n",
      "Episode:373 meanR:489.1700 R:500.0000 rate:1.0000 gloss:67.7789 dlossA:1.2932 dlossQ:376.1118 exploreP:0.0100\n",
      "Episode:374 meanR:489.1700 R:500.0000 rate:1.0000 gloss:55.7608 dlossA:1.9746 dlossQ:431.0138 exploreP:0.0100\n",
      "Episode:375 meanR:489.1700 R:500.0000 rate:1.0000 gloss:41.4615 dlossA:2.4646 dlossQ:510.7706 exploreP:0.0100\n",
      "Episode:376 meanR:489.1700 R:500.0000 rate:1.0000 gloss:26.7114 dlossA:0.3977 dlossQ:664.7985 exploreP:0.0100\n",
      "Episode:377 meanR:489.1700 R:500.0000 rate:1.0000 gloss:16.4073 dlossA:0.8702 dlossQ:852.2856 exploreP:0.0100\n",
      "Episode:378 meanR:489.5700 R:500.0000 rate:1.0000 gloss:8.8351 dlossA:0.6218 dlossQ:951.0274 exploreP:0.0100\n",
      "Episode:379 meanR:489.5700 R:500.0000 rate:1.0000 gloss:4.7622 dlossA:1.0706 dlossQ:1030.9984 exploreP:0.0100\n",
      "Episode:380 meanR:489.5700 R:500.0000 rate:1.0000 gloss:3.5234 dlossA:0.9179 dlossQ:1066.1931 exploreP:0.0100\n",
      "Episode:381 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.9010 dlossA:1.1692 dlossQ:1028.1110 exploreP:0.0100\n",
      "Episode:382 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.7855 dlossA:0.9216 dlossQ:1046.8367 exploreP:0.0100\n",
      "Episode:383 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.4855 dlossA:1.0924 dlossQ:1049.4807 exploreP:0.0100\n",
      "Episode:384 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.4057 dlossA:0.9810 dlossQ:1084.6207 exploreP:0.0100\n",
      "Episode:385 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.2462 dlossA:1.0921 dlossQ:1068.4409 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:386 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.9813 dlossA:0.9467 dlossQ:1052.9614 exploreP:0.0100\n",
      "Episode:387 meanR:489.5700 R:500.0000 rate:1.0000 gloss:2.0598 dlossA:1.0800 dlossQ:1038.4426 exploreP:0.0100\n",
      "Episode:388 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.7582 dlossA:1.0107 dlossQ:1089.0941 exploreP:0.0100\n",
      "Episode:389 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.8108 dlossA:1.0575 dlossQ:1051.8445 exploreP:0.0100\n",
      "Episode:390 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.6902 dlossA:0.9980 dlossQ:1064.5778 exploreP:0.0100\n",
      "Episode:391 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.5692 dlossA:0.9708 dlossQ:1066.8135 exploreP:0.0100\n",
      "Episode:392 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.4487 dlossA:1.0435 dlossQ:1158.5687 exploreP:0.0100\n",
      "Episode:393 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.4516 dlossA:0.9903 dlossQ:1138.7810 exploreP:0.0100\n",
      "Episode:394 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.4539 dlossA:0.9616 dlossQ:1117.0834 exploreP:0.0100\n",
      "Episode:395 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.4800 dlossA:0.9986 dlossQ:1151.7115 exploreP:0.0100\n",
      "Episode:396 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.3325 dlossA:1.0469 dlossQ:1186.8986 exploreP:0.0100\n",
      "Episode:397 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.3541 dlossA:1.0317 dlossQ:1189.9771 exploreP:0.0100\n",
      "Episode:398 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.2590 dlossA:1.1112 dlossQ:1233.3046 exploreP:0.0100\n",
      "Episode:399 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.3644 dlossA:1.1021 dlossQ:1228.3948 exploreP:0.0100\n",
      "Episode:400 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.2887 dlossA:1.0544 dlossQ:1228.7762 exploreP:0.0100\n",
      "Episode:401 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.2956 dlossA:1.0755 dlossQ:1232.9098 exploreP:0.0100\n",
      "Episode:402 meanR:489.5700 R:500.0000 rate:1.0000 gloss:1.2694 dlossA:1.0675 dlossQ:1265.1652 exploreP:0.0100\n",
      "Episode:403 meanR:490.3800 R:500.0000 rate:1.0000 gloss:1.3177 dlossA:1.1448 dlossQ:1262.3407 exploreP:0.0100\n",
      "Episode:404 meanR:493.6600 R:500.0000 rate:1.0000 gloss:1.2239 dlossA:1.1107 dlossQ:1269.9514 exploreP:0.0100\n",
      "Episode:405 meanR:496.8800 R:500.0000 rate:1.0000 gloss:1.2790 dlossA:1.1394 dlossQ:1275.3495 exploreP:0.0100\n",
      "Episode:406 meanR:497.9900 R:500.0000 rate:1.0000 gloss:1.2517 dlossA:1.0978 dlossQ:1274.8888 exploreP:0.0100\n",
      "Episode:407 meanR:500.0000 R:500.0000 rate:1.0000 gloss:1.2521 dlossA:1.2532 dlossQ:1299.5292 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dloss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        #gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        #dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-a0b0af09d9f3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-a0b0af09d9f3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    plt.ylabel('G losses')`\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
