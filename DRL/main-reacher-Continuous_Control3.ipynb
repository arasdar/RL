{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/aras/unity-envs/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the train mode\n",
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "# state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# #scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# num_steps = 0\n",
    "# while True:\n",
    "#     num_steps += 1\n",
    "#     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     #print(action)\n",
    "#     action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "#     #print(action)\n",
    "#     env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "#     next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "#     reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "#     done = env_info.local_done[0]                        # see if episode finished\n",
    "#     #scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     state = next_state                               # roll over states to next time step\n",
    "#     if done is True:                                  # exit loop if episode finished\n",
    "#         #print(action.shape, reward)\n",
    "#         #print(done)\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "# num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_shape], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    training = tf.placeholder(tf.bool, [], name='training')\n",
    "    return states, actions, targetQs, rates, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        predictions = tf.nn.tanh(logits) # [-1, +1]\n",
    "\n",
    "        # return actions logits\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates, training):\n",
    "    actions_preds = generator(states=states, hidden_size=hidden_size, action_size=action_size, training=training)\n",
    "    gQs = discriminator(actions=actions_preds, hidden_size=hidden_size, states=states, training=training) # nextQs/targetQs\n",
    "    gloss = -tf.reduce_mean(gQs)\n",
    "    dQs = discriminator(actions=actions, hidden_size=hidden_size, states=states, training=training, reuse=True) # Qs\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1]) # gQs\n",
    "    dloss = tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    # rates = tf.reshape(rates, shape=[-1, 1]) # [-1,+1]\n",
    "    # dloss += tf.reduce_mean(tf.square(tf.nn.tanh(dQs) - rates)) # DQN\n",
    "    return actions_preds, gQs, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_loss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates, self.training = model_input(\n",
    "            state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_preds, self.Qs_logits, self.g_loss, self.d_loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            rates=self.rates, training=self.training) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, \n",
    "                                           d_loss=self.d_loss,\n",
    "                                           g_learning_rate=g_learning_rate, \n",
    "                                           d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 33), (1, 4), 4, 0, 4, 33)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info.vector_observations.shape, env_info.previous_vector_actions.shape, \\\n",
    "brain.vector_action_space_size, brain.number_visual_observations, \\\n",
    "brain.vector_action_space_size, brain.vector_observation_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size == one episode size is 1000/int(1e3) steps\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.01\n",
      "Progress: 0.02001\n",
      "Progress: 0.03002\n",
      "Progress: 0.04003\n",
      "Progress: 0.05004\n",
      "Progress: 0.06005\n",
      "Progress: 0.07006\n",
      "Progress: 0.08007\n",
      "Progress: 0.09008\n",
      "Progress: 0.10009\n",
      "Progress: 0.1101\n",
      "Progress: 0.12011\n",
      "Progress: 0.13012\n",
      "Progress: 0.14013\n",
      "Progress: 0.15014\n",
      "Progress: 0.16015\n",
      "Progress: 0.17016\n",
      "Progress: 0.18017\n",
      "Progress: 0.19018\n",
      "Progress: 0.20019\n",
      "Progress: 0.2102\n",
      "Progress: 0.22021\n",
      "Progress: 0.23022\n",
      "Progress: 0.24023\n",
      "Progress: 0.25024\n",
      "Progress: 0.26025\n",
      "Progress: 0.27026\n",
      "Progress: 0.28027\n",
      "Progress: 0.29028\n",
      "Progress: 0.30029\n",
      "Progress: 0.3103\n",
      "Progress: 0.32031\n",
      "Progress: 0.33032\n",
      "Progress: 0.34033\n",
      "Progress: 0.35034\n",
      "Progress: 0.36035\n",
      "Progress: 0.37036\n",
      "Progress: 0.38037\n",
      "Progress: 0.39038\n",
      "Progress: 0.40039\n",
      "Progress: 0.4104\n",
      "Progress: 0.42041\n",
      "Progress: 0.43042\n",
      "Progress: 0.44043\n",
      "Progress: 0.45044\n",
      "Progress: 0.46045\n",
      "Progress: 0.47046\n",
      "Progress: 0.48047\n",
      "Progress: 0.49048\n",
      "Progress: 0.50049\n",
      "Progress: 0.5105\n",
      "Progress: 0.52051\n",
      "Progress: 0.53052\n",
      "Progress: 0.54053\n",
      "Progress: 0.55054\n",
      "Progress: 0.56055\n",
      "Progress: 0.57056\n",
      "Progress: 0.58057\n",
      "Progress: 0.59058\n",
      "Progress: 0.60059\n",
      "Progress: 0.6106\n",
      "Progress: 0.62061\n",
      "Progress: 0.63062\n",
      "Progress: 0.64063\n",
      "Progress: 0.65064\n",
      "Progress: 0.66065\n",
      "Progress: 0.67066\n",
      "Progress: 0.68067\n",
      "Progress: 0.69068\n",
      "Progress: 0.70069\n",
      "Progress: 0.7107\n",
      "Progress: 0.72071\n",
      "Progress: 0.73072\n",
      "Progress: 0.74073\n",
      "Progress: 0.75074\n",
      "Progress: 0.76075\n",
      "Progress: 0.77076\n",
      "Progress: 0.78077\n",
      "Progress: 0.79078\n",
      "Progress: 0.80079\n",
      "Progress: 0.8108\n",
      "Progress: 0.82081\n",
      "Progress: 0.83082\n",
      "Progress: 0.84083\n",
      "Progress: 0.85084\n",
      "Progress: 0.86085\n",
      "Progress: 0.87086\n",
      "Progress: 0.88087\n",
      "Progress: 0.89088\n",
      "Progress: 0.90089\n",
      "Progress: 0.9109\n",
      "Progress: 0.92091\n",
      "Progress: 0.93092\n",
      "Progress: 0.94093\n",
      "Progress: 0.95094\n",
      "Progress: 0.96095\n",
      "Progress: 0.97096\n",
      "Progress: 0.98097\n",
      "Progress: 0.99098\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each_step in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    rate = -1 # success rate: [-1, +1]\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory updated\n",
    "    total_reward += reward # max reward 30\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        print('Progress:', each_step/memory_size)\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        rate = total_reward/30\n",
    "        rate = np.clip(rate, -1, 1) \n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.1400 R:0.1400 gloss:-13.5131 dloss:2.3775\n",
      "Episode:1 meanR:0.1100 R:0.0800 gloss:-162.8856 dloss:168.7697\n",
      "Episode:2 meanR:0.1767 R:0.3100 gloss:-182.2692 dloss:253.2398\n",
      "Episode:3 meanR:0.2850 R:0.6100 gloss:-88.9877 dloss:86.5603\n",
      "Episode:4 meanR:0.4880 R:1.3000 gloss:-42.4882 dloss:28.1309\n",
      "Episode:5 meanR:0.5017 R:0.5700 gloss:-16.4816 dloss:8.6351\n",
      "Episode:6 meanR:0.4743 R:0.3100 gloss:-37.8125 dloss:20.2637\n",
      "Episode:7 meanR:0.4287 R:0.1100 gloss:-96.9459 dloss:96.5668\n",
      "Episode:8 meanR:0.3878 R:0.0600 gloss:-84.2915 dloss:87.1654\n",
      "Episode:9 meanR:0.3880 R:0.3900 gloss:-41.7667 dloss:29.3611\n",
      "Episode:10 meanR:0.3900 R:0.4100 gloss:-17.0645 dloss:9.0286\n",
      "Episode:11 meanR:0.3700 R:0.1500 gloss:-4.7830 dloss:3.4696\n",
      "Episode:12 meanR:0.3838 R:0.5500 gloss:0.9558 dloss:1.8692\n",
      "Episode:13 meanR:0.3786 R:0.3100 gloss:-5.2411 dloss:0.6519\n",
      "Episode:14 meanR:0.4100 R:0.8500 gloss:-7.0312 dloss:0.2722\n",
      "Episode:15 meanR:0.4400 R:0.8900 gloss:-7.3477 dloss:0.2497\n",
      "Episode:16 meanR:0.4412 R:0.4600 gloss:-6.6075 dloss:0.2523\n",
      "Episode:17 meanR:0.4656 R:0.8800 gloss:-6.6226 dloss:0.1921\n",
      "Episode:18 meanR:0.4758 R:0.6600 gloss:-6.5968 dloss:0.1419\n",
      "Episode:19 meanR:0.4865 R:0.6900 gloss:-6.7577 dloss:0.1434\n",
      "Episode:20 meanR:0.4805 R:0.3600 gloss:-9.3828 dloss:0.2235\n",
      "Episode:21 meanR:0.4605 R:0.0400 gloss:-8.3086 dloss:0.1413\n",
      "Episode:22 meanR:0.4404 R:0.0000 gloss:-6.8942 dloss:0.1002\n",
      "Episode:23 meanR:0.4367 R:0.3500 gloss:-5.2734 dloss:0.0759\n",
      "Episode:24 meanR:0.4248 R:0.1400 gloss:-4.8836 dloss:0.0544\n",
      "Episode:25 meanR:0.4227 R:0.3700 gloss:-4.6309 dloss:0.0492\n",
      "Episode:26 meanR:0.4404 R:0.9000 gloss:-4.2574 dloss:0.0494\n",
      "Episode:27 meanR:0.4504 R:0.7200 gloss:-3.7335 dloss:0.0340\n",
      "Episode:28 meanR:0.4359 R:0.0300 gloss:-3.2245 dloss:0.0256\n",
      "Episode:29 meanR:0.4467 R:0.7600 gloss:-2.6084 dloss:0.0184\n",
      "Episode:30 meanR:0.4394 R:0.2200 gloss:-2.1576 dloss:0.0124\n",
      "Episode:31 meanR:0.4412 R:0.5000 gloss:-1.6944 dloss:0.0090\n",
      "Episode:32 meanR:0.4394 R:0.3800 gloss:-1.4758 dloss:0.0068\n",
      "Episode:33 meanR:0.4329 R:0.2200 gloss:-1.3234 dloss:0.0054\n",
      "Episode:34 meanR:0.4206 R:0.0000 gloss:-1.4196 dloss:0.0051\n",
      "Episode:35 meanR:0.4089 R:0.0000 gloss:-1.4922 dloss:0.0080\n",
      "Episode:36 meanR:0.3978 R:0.0000 gloss:-1.4583 dloss:0.0091\n",
      "Episode:37 meanR:0.3874 R:0.0000 gloss:-1.3198 dloss:0.0062\n",
      "Episode:38 meanR:0.3774 R:0.0000 gloss:-1.1800 dloss:0.0063\n",
      "Episode:39 meanR:0.3680 R:0.0000 gloss:-1.1450 dloss:0.0057\n",
      "Episode:40 meanR:0.3595 R:0.0200 gloss:-1.1341 dloss:0.0070\n",
      "Episode:41 meanR:0.3510 R:0.0000 gloss:-0.8306 dloss:0.0078\n",
      "Episode:42 meanR:0.3428 R:0.0000 gloss:-0.5418 dloss:0.0063\n",
      "Episode:43 meanR:0.3350 R:0.0000 gloss:-0.4697 dloss:0.0049\n",
      "Episode:44 meanR:0.3276 R:0.0000 gloss:-0.4049 dloss:0.0037\n",
      "Episode:45 meanR:0.3226 R:0.1000 gloss:-0.1639 dloss:0.0026\n",
      "Episode:46 meanR:0.3157 R:0.0000 gloss:-0.2426 dloss:0.0018\n",
      "Episode:47 meanR:0.3098 R:0.0300 gloss:-0.3591 dloss:0.0014\n",
      "Episode:48 meanR:0.3182 R:0.7200 gloss:-0.3657 dloss:0.0012\n",
      "Episode:49 meanR:0.3128 R:0.0500 gloss:-0.2601 dloss:0.0012\n",
      "Episode:50 meanR:0.3124 R:0.2900 gloss:-0.2603 dloss:0.0010\n",
      "Episode:51 meanR:0.3148 R:0.4400 gloss:-0.2297 dloss:0.0007\n",
      "Episode:52 meanR:0.3142 R:0.2800 gloss:-0.2071 dloss:0.0006\n",
      "Episode:53 meanR:0.3083 R:0.0000 gloss:-0.2042 dloss:0.0004\n",
      "Episode:54 meanR:0.3060 R:0.1800 gloss:-0.1771 dloss:0.0003\n",
      "Episode:55 meanR:0.3005 R:0.0000 gloss:-0.1583 dloss:0.0003\n",
      "Episode:56 meanR:0.2965 R:0.0700 gloss:-0.1357 dloss:0.0004\n",
      "Episode:57 meanR:0.3009 R:0.5500 gloss:-0.1794 dloss:0.0006\n",
      "Episode:58 meanR:0.2958 R:0.0000 gloss:-0.2907 dloss:0.0010\n",
      "Episode:59 meanR:0.2908 R:0.0000 gloss:-0.3192 dloss:0.0008\n",
      "Episode:60 meanR:0.2911 R:0.3100 gloss:-0.2025 dloss:0.0006\n",
      "Episode:61 meanR:0.2910 R:0.2800 gloss:-0.0994 dloss:0.0009\n",
      "Episode:62 meanR:0.2929 R:0.4100 gloss:-0.1695 dloss:0.0010\n",
      "Episode:63 meanR:0.2883 R:0.0000 gloss:-0.2279 dloss:0.0008\n",
      "Episode:64 meanR:0.2838 R:0.0000 gloss:-0.2485 dloss:0.0009\n",
      "Episode:65 meanR:0.2795 R:0.0000 gloss:-0.2237 dloss:0.0007\n",
      "Episode:66 meanR:0.2754 R:0.0000 gloss:-0.2144 dloss:0.0009\n",
      "Episode:67 meanR:0.2713 R:0.0000 gloss:-0.1800 dloss:0.0008\n",
      "Episode:68 meanR:0.2674 R:0.0000 gloss:-0.2243 dloss:0.0008\n",
      "Episode:69 meanR:0.2636 R:0.0000 gloss:-0.2030 dloss:0.0008\n",
      "Episode:70 meanR:0.2599 R:0.0000 gloss:-0.2060 dloss:0.0008\n",
      "Episode:71 meanR:0.2562 R:0.0000 gloss:-0.2365 dloss:0.0009\n",
      "Episode:72 meanR:0.2600 R:0.5300 gloss:-0.3149 dloss:0.0007\n",
      "Episode:73 meanR:0.2576 R:0.0800 gloss:-0.3691 dloss:0.0009\n",
      "Episode:74 meanR:0.2676 R:1.0100 gloss:-0.3251 dloss:0.0008\n",
      "Episode:75 meanR:0.2676 R:0.2700 gloss:-0.3129 dloss:0.0006\n",
      "Episode:76 meanR:0.2725 R:0.6400 gloss:-0.2382 dloss:0.0005\n",
      "Episode:77 meanR:0.2781 R:0.7100 gloss:-0.2141 dloss:0.0004\n",
      "Episode:78 meanR:0.2759 R:0.1100 gloss:-0.2402 dloss:0.0003\n",
      "Episode:79 meanR:0.2899 R:1.3900 gloss:-0.3309 dloss:0.0004\n",
      "Episode:80 meanR:0.2990 R:1.0300 gloss:-0.1841 dloss:0.0003\n",
      "Episode:81 meanR:0.2999 R:0.3700 gloss:-0.2520 dloss:0.0002\n",
      "Episode:82 meanR:0.2992 R:0.2400 gloss:-0.2769 dloss:0.0003\n",
      "Episode:83 meanR:0.3136 R:1.5100 gloss:-0.2627 dloss:0.0002\n",
      "Episode:84 meanR:0.3235 R:1.1600 gloss:-0.2324 dloss:0.0005\n",
      "Episode:85 meanR:0.3258 R:0.5200 gloss:-0.3102 dloss:0.0003\n",
      "Episode:86 meanR:0.3307 R:0.7500 gloss:-0.2968 dloss:0.0002\n",
      "Episode:87 meanR:0.3332 R:0.5500 gloss:-0.2530 dloss:0.0005\n",
      "Episode:88 meanR:0.3376 R:0.7300 gloss:-0.2923 dloss:0.0009\n",
      "Episode:89 meanR:0.3374 R:0.3200 gloss:-0.3477 dloss:0.0009\n",
      "Episode:90 meanR:0.3337 R:0.0000 gloss:-0.4197 dloss:0.0047\n",
      "Episode:91 meanR:0.3301 R:0.0000 gloss:-0.4720 dloss:0.0043\n",
      "Episode:92 meanR:0.3266 R:0.0000 gloss:-0.5303 dloss:0.0049\n",
      "Episode:93 meanR:0.3231 R:0.0000 gloss:-0.5607 dloss:0.0041\n",
      "Episode:94 meanR:0.3197 R:0.0000 gloss:-0.6308 dloss:0.0071\n",
      "Episode:95 meanR:0.3164 R:0.0000 gloss:-0.5745 dloss:0.0213\n",
      "Episode:96 meanR:0.3131 R:0.0000 gloss:-0.6868 dloss:0.0245\n",
      "Episode:97 meanR:0.3144 R:0.4400 gloss:-0.9162 dloss:0.0043\n",
      "Episode:98 meanR:0.3130 R:0.1800 gloss:-0.6981 dloss:0.0012\n",
      "Episode:99 meanR:0.3117 R:0.1800 gloss:-0.6090 dloss:0.0009\n",
      "Episode:100 meanR:0.3113 R:0.1000 gloss:-0.6166 dloss:0.0024\n",
      "Episode:101 meanR:0.3176 R:0.7100 gloss:-3.8997 dloss:0.4268\n",
      "Episode:102 meanR:0.3145 R:0.0000 gloss:-20.3187 dloss:2.1255\n",
      "Episode:103 meanR:0.3111 R:0.2700 gloss:-2.7653 dloss:0.9990\n",
      "Episode:104 meanR:0.3039 R:0.5800 gloss:2.6923 dloss:0.4858\n",
      "Episode:105 meanR:0.3108 R:1.2600 gloss:12.8607 dloss:0.6333\n",
      "Episode:106 meanR:0.3144 R:0.6700 gloss:38.3431 dloss:3.6519\n",
      "Episode:107 meanR:0.3231 R:0.9800 gloss:68.3737 dloss:10.1072\n",
      "Episode:108 meanR:0.3291 R:0.6600 gloss:94.6521 dloss:18.9103\n",
      "Episode:109 meanR:0.3307 R:0.5500 gloss:118.8229 dloss:25.8611\n",
      "Episode:110 meanR:0.3411 R:1.4500 gloss:138.8734 dloss:32.8613\n",
      "Episode:111 meanR:0.3586 R:1.9000 gloss:156.0670 dloss:35.5448\n",
      "Episode:112 meanR:0.3624 R:0.9300 gloss:167.4908 dloss:43.0106\n",
      "Episode:113 meanR:0.3681 R:0.8800 gloss:177.9344 dloss:40.5340\n",
      "Episode:114 meanR:0.3688 R:0.9200 gloss:186.3893 dloss:47.8122\n",
      "Episode:115 meanR:0.3635 R:0.3600 gloss:188.6002 dloss:48.6867\n",
      "Episode:116 meanR:0.3625 R:0.3600 gloss:188.0924 dloss:50.9446\n",
      "Episode:117 meanR:0.3596 R:0.5900 gloss:186.3215 dloss:51.5233\n",
      "Episode:118 meanR:0.3630 R:1.0000 gloss:187.6821 dloss:44.4857\n",
      "Episode:119 meanR:0.3709 R:1.4800 gloss:186.9382 dloss:42.1371\n",
      "Episode:120 meanR:0.3730 R:0.5700 gloss:183.3873 dloss:45.6629\n",
      "Episode:121 meanR:0.3993 R:2.6700 gloss:176.9095 dloss:41.2607\n",
      "Episode:122 meanR:0.4053 R:0.6000 gloss:174.4308 dloss:39.7873\n",
      "Episode:123 meanR:0.4100 R:0.8200 gloss:173.5059 dloss:36.1446\n",
      "Episode:124 meanR:0.4285 R:1.9900 gloss:176.2900 dloss:31.8650\n",
      "Episode:125 meanR:0.4353 R:1.0500 gloss:169.7862 dloss:37.8808\n",
      "Episode:126 meanR:0.4337 R:0.7400 gloss:163.8833 dloss:28.4969\n",
      "Episode:127 meanR:0.4319 R:0.5400 gloss:157.8510 dloss:27.2738\n",
      "Episode:128 meanR:0.4428 R:1.1200 gloss:146.8455 dloss:26.7381\n",
      "Episode:129 meanR:0.4403 R:0.5100 gloss:137.8990 dloss:22.9527\n",
      "Episode:130 meanR:0.4429 R:0.4800 gloss:128.5094 dloss:19.8924\n",
      "Episode:131 meanR:0.4422 R:0.4300 gloss:116.7230 dloss:16.3712\n",
      "Episode:132 meanR:0.4513 R:1.2900 gloss:105.2820 dloss:13.5292\n",
      "Episode:133 meanR:0.4643 R:1.5200 gloss:97.2689 dloss:8.6882\n",
      "Episode:134 meanR:0.4737 R:0.9400 gloss:91.9869 dloss:9.3091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:135 meanR:0.4814 R:0.7700 gloss:81.5567 dloss:6.9624\n",
      "Episode:136 meanR:0.4927 R:1.1300 gloss:69.4669 dloss:5.4703\n",
      "Episode:137 meanR:0.5103 R:1.7600 gloss:54.4678 dloss:3.2713\n",
      "Episode:138 meanR:0.5196 R:0.9300 gloss:36.0057 dloss:1.4869\n",
      "Episode:139 meanR:0.5251 R:0.5500 gloss:6.9729 dloss:0.5275\n",
      "Episode:140 meanR:0.5327 R:0.7800 gloss:-29.6682 dloss:2.2690\n",
      "Episode:141 meanR:0.5332 R:0.0500 gloss:-35.3471 dloss:1.9989\n",
      "Episode:142 meanR:0.5332 R:0.0000 gloss:-24.7030 dloss:0.8331\n",
      "Episode:143 meanR:0.5332 R:0.0000 gloss:-7.5000 dloss:0.2071\n",
      "Episode:144 meanR:0.5332 R:0.0000 gloss:14.5164 dloss:0.3528\n",
      "Episode:145 meanR:0.5322 R:0.0000 gloss:21.0543 dloss:0.4612\n",
      "Episode:146 meanR:0.5481 R:1.5900 gloss:-11.9796 dloss:1.1274\n",
      "Episode:147 meanR:0.5523 R:0.4500 gloss:-32.4864 dloss:2.0799\n",
      "Episode:148 meanR:0.5470 R:0.1900 gloss:-17.4096 dloss:0.4593\n",
      "Episode:149 meanR:0.5465 R:0.0000 gloss:-6.0127 dloss:0.0726\n",
      "Episode:150 meanR:0.5452 R:0.1600 gloss:-6.9828 dloss:0.0571\n",
      "Episode:151 meanR:0.5437 R:0.2900 gloss:-6.3904 dloss:0.0476\n",
      "Episode:152 meanR:0.5439 R:0.3000 gloss:-5.5515 dloss:0.1177\n",
      "Episode:153 meanR:0.5455 R:0.1600 gloss:-7.5864 dloss:0.1620\n",
      "Episode:154 meanR:0.5437 R:0.0000 gloss:-4.9541 dloss:0.0451\n",
      "Episode:155 meanR:0.5515 R:0.7800 gloss:-1.7181 dloss:0.0144\n",
      "Episode:156 meanR:0.5546 R:0.3800 gloss:0.2819 dloss:0.0096\n",
      "Episode:157 meanR:0.5491 R:0.0000 gloss:-9.7763 dloss:0.3002\n",
      "Episode:158 meanR:0.5491 R:0.0000 gloss:-23.3820 dloss:0.7518\n",
      "Episode:159 meanR:0.5505 R:0.1400 gloss:-20.2920 dloss:0.5237\n",
      "Episode:160 meanR:0.5474 R:0.0000 gloss:-7.5577 dloss:0.0848\n",
      "Episode:161 meanR:0.5446 R:0.0000 gloss:31.2829 dloss:1.1530\n",
      "Episode:162 meanR:0.5405 R:0.0000 gloss:59.0315 dloss:3.7253\n",
      "Episode:163 meanR:0.5405 R:0.0000 gloss:55.3220 dloss:3.4290\n",
      "Episode:164 meanR:0.5405 R:0.0000 gloss:44.0151 dloss:1.8109\n",
      "Episode:165 meanR:0.5405 R:0.0000 gloss:19.7101 dloss:0.4931\n",
      "Episode:166 meanR:0.5429 R:0.2400 gloss:-32.3941 dloss:2.3574\n",
      "Episode:167 meanR:0.5465 R:0.3600 gloss:-76.5874 dloss:7.3922\n",
      "Episode:168 meanR:0.5467 R:0.0200 gloss:-83.0289 dloss:7.3963\n",
      "Episode:169 meanR:0.5511 R:0.4400 gloss:-79.2783 dloss:6.8967\n",
      "Episode:170 meanR:0.5511 R:0.0000 gloss:-73.3987 dloss:5.8782\n",
      "Episode:171 meanR:0.5511 R:0.0000 gloss:-64.6245 dloss:4.6847\n",
      "Episode:172 meanR:0.5458 R:0.0000 gloss:-51.1791 dloss:2.9353\n",
      "Episode:173 meanR:0.5450 R:0.0000 gloss:-21.0468 dloss:0.6760\n",
      "Episode:174 meanR:0.5349 R:0.0000 gloss:28.9926 dloss:1.2393\n",
      "Episode:175 meanR:0.5322 R:0.0000 gloss:81.9576 dloss:7.9908\n",
      "Episode:176 meanR:0.5258 R:0.0000 gloss:94.8556 dloss:10.6691\n",
      "Episode:177 meanR:0.5198 R:0.1100 gloss:84.0821 dloss:7.2618\n",
      "Episode:178 meanR:0.5208 R:0.2100 gloss:59.9900 dloss:4.7219\n",
      "Episode:179 meanR:0.5132 R:0.6300 gloss:29.6835 dloss:1.1193\n",
      "Episode:180 meanR:0.5101 R:0.7200 gloss:-20.2012 dloss:0.8878\n",
      "Episode:181 meanR:0.5180 R:1.1600 gloss:-43.0586 dloss:3.0058\n",
      "Episode:182 meanR:0.5218 R:0.6200 gloss:-42.0828 dloss:2.0592\n",
      "Episode:183 meanR:0.5156 R:0.8900 gloss:-23.8567 dloss:0.7056\n",
      "Episode:184 meanR:0.5040 R:0.0000 gloss:-2.4741 dloss:0.0579\n",
      "Episode:185 meanR:0.4988 R:0.0000 gloss:10.0251 dloss:0.2414\n",
      "Episode:186 meanR:0.4938 R:0.2500 gloss:-0.6934 dloss:0.0110\n",
      "Episode:187 meanR:0.4943 R:0.6000 gloss:-1.3889 dloss:0.0109\n",
      "Episode:188 meanR:0.4925 R:0.5500 gloss:-2.8675 dloss:0.0393\n",
      "Episode:189 meanR:0.4914 R:0.2100 gloss:-2.1783 dloss:0.0598\n",
      "Episode:190 meanR:0.4944 R:0.3000 gloss:-3.2114 dloss:0.0233\n",
      "Episode:191 meanR:0.5003 R:0.5900 gloss:-3.5654 dloss:0.0367\n",
      "Episode:192 meanR:0.5059 R:0.5600 gloss:-3.1294 dloss:0.0298\n",
      "Episode:193 meanR:0.5118 R:0.5900 gloss:-3.1965 dloss:0.0248\n",
      "Episode:194 meanR:0.5118 R:0.0000 gloss:-2.7392 dloss:0.0187\n",
      "Episode:195 meanR:0.5163 R:0.4500 gloss:-2.3666 dloss:0.0133\n",
      "Episode:196 meanR:0.5255 R:0.9200 gloss:-2.4814 dloss:0.0176\n",
      "Episode:197 meanR:0.5250 R:0.3900 gloss:-2.0657 dloss:0.0128\n",
      "Episode:198 meanR:0.5333 R:1.0100 gloss:-2.0356 dloss:0.0121\n",
      "Episode:199 meanR:0.5349 R:0.3400 gloss:-1.9309 dloss:0.0112\n",
      "Episode:200 meanR:0.5432 R:0.9300 gloss:-2.0746 dloss:0.0112\n",
      "Episode:201 meanR:0.5387 R:0.2600 gloss:-2.0866 dloss:0.0099\n",
      "Episode:202 meanR:0.5476 R:0.8900 gloss:-1.9519 dloss:0.0092\n",
      "Episode:203 meanR:0.5524 R:0.7500 gloss:-1.8388 dloss:0.0076\n",
      "Episode:204 meanR:0.5488 R:0.2200 gloss:-1.8115 dloss:0.0071\n",
      "Episode:205 meanR:0.5439 R:0.7700 gloss:-1.7641 dloss:0.0072\n",
      "Episode:206 meanR:0.5421 R:0.4900 gloss:-1.8341 dloss:0.0066\n",
      "Episode:207 meanR:0.5404 R:0.8100 gloss:-1.7896 dloss:0.0063\n",
      "Episode:208 meanR:0.5431 R:0.9300 gloss:-1.7343 dloss:0.0065\n",
      "Episode:209 meanR:0.5386 R:0.1000 gloss:-1.6126 dloss:0.0045\n",
      "Episode:210 meanR:0.5493 R:2.5200 gloss:-1.6190 dloss:0.0044\n",
      "Episode:211 meanR:0.5371 R:0.6800 gloss:-1.5930 dloss:0.0046\n",
      "Episode:212 meanR:0.5434 R:1.5600 gloss:-1.5954 dloss:0.0043\n",
      "Episode:213 meanR:0.5468 R:1.2200 gloss:-1.6089 dloss:0.0041\n",
      "Episode:214 meanR:0.5448 R:0.7200 gloss:-2.2922 dloss:0.0102\n",
      "Episode:215 meanR:0.5480 R:0.6800 gloss:-6.9688 dloss:0.0481\n",
      "Episode:216 meanR:0.5562 R:1.1800 gloss:-6.8145 dloss:0.0540\n",
      "Episode:217 meanR:0.5702 R:1.9900 gloss:-4.1184 dloss:0.0210\n",
      "Episode:218 meanR:0.5647 R:0.4500 gloss:-2.6336 dloss:0.0086\n",
      "Episode:219 meanR:0.5574 R:0.7500 gloss:-2.1732 dloss:0.0061\n",
      "Episode:220 meanR:0.5572 R:0.5500 gloss:-1.9295 dloss:0.0048\n",
      "Episode:221 meanR:0.5367 R:0.6200 gloss:-1.5795 dloss:0.0042\n",
      "Episode:222 meanR:0.5378 R:0.7100 gloss:-1.3199 dloss:0.0029\n",
      "Episode:223 meanR:0.5377 R:0.8100 gloss:-1.1679 dloss:0.0020\n",
      "Episode:224 meanR:0.5341 R:1.6300 gloss:-1.0673 dloss:0.0016\n",
      "Episode:225 meanR:0.5288 R:0.5200 gloss:-0.8907 dloss:0.0014\n",
      "Episode:226 meanR:0.5295 R:0.8100 gloss:-0.8985 dloss:0.0010\n",
      "Episode:227 meanR:0.5252 R:0.1100 gloss:-0.7955 dloss:0.0010\n",
      "Episode:228 meanR:0.5244 R:1.0400 gloss:-0.7265 dloss:0.0009\n",
      "Episode:229 meanR:0.5290 R:0.9700 gloss:-0.7885 dloss:0.0010\n",
      "Episode:230 meanR:0.5379 R:1.3700 gloss:-0.7507 dloss:0.0008\n",
      "Episode:231 meanR:0.5466 R:1.3000 gloss:-0.7301 dloss:0.0007\n",
      "Episode:232 meanR:0.5363 R:0.2600 gloss:-0.7091 dloss:0.0008\n",
      "Episode:233 meanR:0.5275 R:0.6400 gloss:-0.6888 dloss:0.0006\n",
      "Episode:234 meanR:0.5260 R:0.7900 gloss:-0.6557 dloss:0.0006\n",
      "Episode:235 meanR:0.5339 R:1.5600 gloss:-0.6210 dloss:0.0006\n",
      "Episode:236 meanR:0.5261 R:0.3500 gloss:-0.5787 dloss:0.0008\n",
      "Episode:237 meanR:0.5171 R:0.8600 gloss:-0.5717 dloss:0.0005\n",
      "Episode:238 meanR:0.5114 R:0.3600 gloss:-0.5684 dloss:0.0005\n",
      "Episode:239 meanR:0.5176 R:1.1700 gloss:-0.5733 dloss:0.0006\n",
      "Episode:240 meanR:0.5130 R:0.3200 gloss:-0.5013 dloss:0.0006\n",
      "Episode:241 meanR:0.5206 R:0.8100 gloss:-0.5074 dloss:0.0007\n",
      "Episode:242 meanR:0.5236 R:0.3000 gloss:-10.0390 dloss:7.9243\n",
      "Episode:243 meanR:0.5304 R:0.6800 gloss:-346.1594 dloss:1023.6336\n",
      "Episode:244 meanR:0.5304 R:0.0000 gloss:-984.2233 dloss:6807.1440\n",
      "Episode:245 meanR:0.5336 R:0.3200 gloss:-1905.5034 dloss:23342.1660\n",
      "Episode:246 meanR:0.5209 R:0.3200 gloss:-3100.7566 dloss:57663.3594\n",
      "Episode:247 meanR:0.5215 R:0.5100 gloss:-4398.4692 dloss:102775.2422\n",
      "Episode:248 meanR:0.5228 R:0.3200 gloss:-5810.7139 dloss:153178.8125\n",
      "Episode:249 meanR:0.5322 R:0.9400 gloss:-7339.0361 dloss:189056.9219\n",
      "Episode:250 meanR:0.5414 R:1.0800 gloss:-8919.4873 dloss:231358.9375\n",
      "Episode:251 meanR:0.5504 R:1.1900 gloss:-10335.1211 dloss:270155.5000\n",
      "Episode:252 meanR:0.5613 R:1.3900 gloss:-11493.0312 dloss:299059.2812\n",
      "Episode:253 meanR:0.5612 R:0.1500 gloss:-12279.8633 dloss:246119.2500\n",
      "Episode:254 meanR:0.5683 R:0.7100 gloss:-12880.3877 dloss:272098.1875\n",
      "Episode:255 meanR:0.5615 R:0.1000 gloss:-12954.3389 dloss:241708.5625\n",
      "Episode:256 meanR:0.5642 R:0.6500 gloss:-12525.4482 dloss:247757.8125\n",
      "Episode:257 meanR:0.5697 R:0.5500 gloss:-11916.0410 dloss:192686.3281\n",
      "Episode:258 meanR:0.5799 R:1.0200 gloss:-11007.6348 dloss:175815.3438\n",
      "Episode:259 meanR:0.5799 R:0.1400 gloss:-9954.1279 dloss:129958.3750\n",
      "Episode:260 meanR:0.5868 R:0.6900 gloss:-8847.3945 dloss:89278.5000\n",
      "Episode:261 meanR:0.5971 R:1.0300 gloss:-7707.9141 dloss:86850.8047\n",
      "Episode:262 meanR:0.5996 R:0.2500 gloss:-6518.5645 dloss:59055.9062\n",
      "Episode:263 meanR:0.6014 R:0.1800 gloss:-5444.7163 dloss:41337.9727\n",
      "Episode:264 meanR:0.6046 R:0.3200 gloss:-4464.8101 dloss:23980.3750\n",
      "Episode:265 meanR:0.6103 R:0.5700 gloss:-3579.6665 dloss:16330.7090\n",
      "Episode:266 meanR:0.6111 R:0.3200 gloss:-2790.4758 dloss:10218.3350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:267 meanR:0.6169 R:0.9400 gloss:-2103.9961 dloss:7188.7505\n",
      "Episode:268 meanR:0.6195 R:0.2800 gloss:-1500.8718 dloss:4675.0391\n",
      "Episode:269 meanR:0.6256 R:1.0500 gloss:-1003.0054 dloss:3989.0698\n",
      "Episode:270 meanR:0.6256 R:0.0000 gloss:-1164.2573 dloss:3249.8740\n",
      "Episode:271 meanR:0.6266 R:0.1000 gloss:-1304.7975 dloss:2600.9241\n",
      "Episode:272 meanR:0.6266 R:0.0000 gloss:-1246.8591 dloss:1826.7100\n",
      "Episode:273 meanR:0.6266 R:0.0000 gloss:-1136.9845 dloss:1599.1853\n",
      "Episode:274 meanR:0.6266 R:0.0000 gloss:-1013.9969 dloss:1113.8749\n",
      "Episode:275 meanR:0.6266 R:0.0000 gloss:-878.1862 dloss:705.3230\n",
      "Episode:276 meanR:0.6266 R:0.0000 gloss:-733.9130 dloss:537.7496\n",
      "Episode:277 meanR:0.6255 R:0.0000 gloss:-599.6762 dloss:375.8143\n",
      "Episode:278 meanR:0.6234 R:0.0000 gloss:-475.5651 dloss:223.3962\n",
      "Episode:279 meanR:0.6171 R:0.0000 gloss:-363.6861 dloss:145.5717\n",
      "Episode:280 meanR:0.6099 R:0.0000 gloss:-269.6695 dloss:76.8962\n",
      "Episode:281 meanR:0.5983 R:0.0000 gloss:-189.1713 dloss:37.6037\n",
      "Episode:282 meanR:0.5921 R:0.0000 gloss:-123.1479 dloss:16.5994\n",
      "Episode:283 meanR:0.5832 R:0.0000 gloss:-69.6232 dloss:3.9393\n",
      "Episode:284 meanR:0.5832 R:0.0000 gloss:-28.5316 dloss:1.2924\n",
      "Episode:285 meanR:0.5832 R:0.0000 gloss:-28.6673 dloss:1.1479\n",
      "Episode:286 meanR:0.5838 R:0.3100 gloss:-44.0769 dloss:1.9541\n",
      "Episode:287 meanR:0.5778 R:0.0000 gloss:-40.3325 dloss:1.4721\n",
      "Episode:288 meanR:0.5723 R:0.0000 gloss:-36.7704 dloss:1.4530\n",
      "Episode:289 meanR:0.5702 R:0.0000 gloss:-34.7820 dloss:1.4485\n",
      "Episode:290 meanR:0.5672 R:0.0000 gloss:-32.6054 dloss:0.9685\n",
      "Episode:291 meanR:0.5613 R:0.0000 gloss:-27.7018 dloss:0.8930\n",
      "Episode:292 meanR:0.5557 R:0.0000 gloss:-24.6735 dloss:0.7233\n",
      "Episode:293 meanR:0.5498 R:0.0000 gloss:-20.8165 dloss:0.4714\n",
      "Episode:294 meanR:0.5508 R:0.1000 gloss:-15.6174 dloss:0.2722\n",
      "Episode:295 meanR:0.5463 R:0.0000 gloss:-10.7108 dloss:0.1591\n",
      "Episode:296 meanR:0.5371 R:0.0000 gloss:-6.5730 dloss:0.0518\n",
      "Episode:297 meanR:0.5380 R:0.4800 gloss:-15.0313 dloss:0.5946\n",
      "Episode:298 meanR:0.5313 R:0.3400 gloss:4.3648 dloss:0.1498\n",
      "Episode:299 meanR:0.5295 R:0.1600 gloss:-6.5782 dloss:0.1373\n",
      "Episode:300 meanR:0.5202 R:0.0000 gloss:0.1500 dloss:0.0046\n",
      "Episode:301 meanR:0.5176 R:0.0000 gloss:-0.9026 dloss:0.0044\n",
      "Episode:302 meanR:0.5121 R:0.3400 gloss:-0.1388 dloss:0.0281\n",
      "Episode:303 meanR:0.5100 R:0.5400 gloss:0.2768 dloss:0.1040\n",
      "Episode:304 meanR:0.5122 R:0.4400 gloss:1.0724 dloss:0.0562\n",
      "Episode:305 meanR:0.5045 R:0.0000 gloss:-2.5220 dloss:0.1969\n",
      "Episode:306 meanR:0.5020 R:0.2400 gloss:3.0452 dloss:0.1379\n",
      "Episode:307 meanR:0.4939 R:0.0000 gloss:-1.4891 dloss:0.0508\n",
      "Episode:308 meanR:0.4846 R:0.0000 gloss:-0.1320 dloss:0.0111\n",
      "Episode:309 meanR:0.4842 R:0.0600 gloss:-0.4895 dloss:0.0109\n",
      "Episode:310 meanR:0.4590 R:0.0000 gloss:-0.2487 dloss:0.0169\n",
      "Episode:311 meanR:0.4522 R:0.0000 gloss:-0.0503 dloss:0.0124\n",
      "Episode:312 meanR:0.4366 R:0.0000 gloss:-0.1170 dloss:0.0060\n",
      "Episode:313 meanR:0.4244 R:0.0000 gloss:-0.4564 dloss:0.0249\n",
      "Episode:314 meanR:0.4172 R:0.0000 gloss:0.0355 dloss:0.0020\n",
      "Episode:315 meanR:0.4104 R:0.0000 gloss:-0.0316 dloss:0.0016\n",
      "Episode:316 meanR:0.3993 R:0.0700 gloss:-3.8651 dloss:0.3151\n",
      "Episode:317 meanR:0.3813 R:0.1900 gloss:-1.2605 dloss:0.1232\n",
      "Episode:318 meanR:0.3785 R:0.1700 gloss:-0.5244 dloss:0.0230\n",
      "Episode:319 meanR:0.3710 R:0.0000 gloss:-0.3597 dloss:0.0028\n",
      "Episode:320 meanR:0.3670 R:0.1500 gloss:-0.2279 dloss:0.0005\n",
      "Episode:321 meanR:0.3608 R:0.0000 gloss:-0.0608 dloss:0.0002\n",
      "Episode:322 meanR:0.3553 R:0.1600 gloss:-0.0326 dloss:0.0001\n",
      "Episode:323 meanR:0.3528 R:0.5600 gloss:-0.0024 dloss:0.0100\n",
      "Episode:324 meanR:0.3377 R:0.1200 gloss:-0.3409 dloss:0.0208\n",
      "Episode:325 meanR:0.3342 R:0.1700 gloss:-0.3121 dloss:0.0020\n",
      "Episode:326 meanR:0.3287 R:0.2600 gloss:-0.0558 dloss:0.0004\n",
      "Episode:327 meanR:0.3276 R:0.0000 gloss:-0.0512 dloss:0.0121\n",
      "Episode:328 meanR:0.3182 R:0.1000 gloss:-0.4761 dloss:0.0125\n",
      "Episode:329 meanR:0.3098 R:0.1300 gloss:-0.0993 dloss:0.0003\n",
      "Episode:330 meanR:0.3027 R:0.6600 gloss:-0.0724 dloss:0.0008\n",
      "Episode:331 meanR:0.2977 R:0.8000 gloss:-0.1885 dloss:0.0027\n",
      "Episode:332 meanR:0.3028 R:0.7700 gloss:-0.2310 dloss:0.0010\n",
      "Episode:333 meanR:0.2983 R:0.1900 gloss:-0.2093 dloss:0.0020\n",
      "Episode:334 meanR:0.2934 R:0.3000 gloss:-0.2855 dloss:0.0022\n",
      "Episode:335 meanR:0.2845 R:0.6700 gloss:-0.1169 dloss:0.0042\n",
      "Episode:336 meanR:0.2834 R:0.2400 gloss:-0.2958 dloss:0.0025\n",
      "Episode:337 meanR:0.2782 R:0.3400 gloss:-0.2593 dloss:0.0012\n",
      "Episode:338 meanR:0.2755 R:0.0900 gloss:-0.1066 dloss:0.0036\n",
      "Episode:339 meanR:0.2671 R:0.3300 gloss:-0.2282 dloss:0.0004\n",
      "Episode:340 meanR:0.2639 R:0.0000 gloss:-0.4875 dloss:0.0033\n",
      "Episode:341 meanR:0.2558 R:0.0000 gloss:-0.4862 dloss:0.0006\n",
      "Episode:342 meanR:0.2528 R:0.0000 gloss:-0.5330 dloss:0.0013\n",
      "Episode:343 meanR:0.2460 R:0.0000 gloss:-0.5744 dloss:0.0009\n",
      "Episode:344 meanR:0.2460 R:0.0000 gloss:-0.3022 dloss:0.0003\n",
      "Episode:345 meanR:0.2428 R:0.0000 gloss:-0.6730 dloss:0.0009\n",
      "Episode:346 meanR:0.2396 R:0.0000 gloss:-0.4330 dloss:0.0005\n",
      "Episode:347 meanR:0.2354 R:0.0900 gloss:-1.0412 dloss:0.0019\n",
      "Episode:348 meanR:0.2322 R:0.0000 gloss:-0.5789 dloss:0.0008\n",
      "Episode:349 meanR:0.2228 R:0.0000 gloss:-0.9369 dloss:0.0014\n",
      "Episode:350 meanR:0.2120 R:0.0000 gloss:-0.4343 dloss:0.0006\n",
      "Episode:351 meanR:0.2011 R:0.1000 gloss:-1.1236 dloss:0.0022\n",
      "Episode:352 meanR:0.1872 R:0.0000 gloss:-0.5938 dloss:0.0009\n",
      "Episode:353 meanR:0.1857 R:0.0000 gloss:-0.5571 dloss:0.0008\n",
      "Episode:354 meanR:0.1786 R:0.0000 gloss:-0.6341 dloss:0.0013\n",
      "Episode:355 meanR:0.1776 R:0.0000 gloss:-1.0540 dloss:0.0021\n",
      "Episode:356 meanR:0.1722 R:0.1100 gloss:-0.6759 dloss:0.0008\n",
      "Episode:357 meanR:0.1667 R:0.0000 gloss:-0.7438 dloss:0.0013\n",
      "Episode:358 meanR:0.1565 R:0.0000 gloss:-0.7550 dloss:0.0013\n",
      "Episode:359 meanR:0.1551 R:0.0000 gloss:-0.9223 dloss:0.0021\n",
      "Episode:360 meanR:0.1482 R:0.0000 gloss:-0.8977 dloss:0.0016\n",
      "Episode:361 meanR:0.1379 R:0.0000 gloss:-1.2812 dloss:0.0036\n",
      "Episode:362 meanR:0.1354 R:0.0000 gloss:-0.8230 dloss:0.0015\n",
      "Episode:363 meanR:0.1345 R:0.0900 gloss:-0.5793 dloss:0.0007\n",
      "Episode:364 meanR:0.1313 R:0.0000 gloss:-0.4126 dloss:0.0005\n",
      "Episode:365 meanR:0.1256 R:0.0000 gloss:-0.4469 dloss:0.0009\n",
      "Episode:366 meanR:0.1224 R:0.0000 gloss:-0.2886 dloss:0.0020\n",
      "Episode:367 meanR:0.1130 R:0.0000 gloss:-0.6864 dloss:0.0006\n",
      "Episode:368 meanR:0.1102 R:0.0000 gloss:-0.6216 dloss:0.0011\n",
      "Episode:369 meanR:0.0997 R:0.0000 gloss:-0.4141 dloss:0.0003\n",
      "Episode:370 meanR:0.1011 R:0.1400 gloss:-0.3094 dloss:0.0006\n",
      "Episode:371 meanR:0.1067 R:0.6600 gloss:-0.2278 dloss:0.0005\n",
      "Episode:372 meanR:0.1091 R:0.2400 gloss:-0.2012 dloss:0.0015\n",
      "Episode:373 meanR:0.1091 R:0.0000 gloss:-0.2634 dloss:0.0001\n",
      "Episode:374 meanR:0.1095 R:0.0400 gloss:-0.1890 dloss:0.0001\n",
      "Episode:375 meanR:0.1095 R:0.0000 gloss:-0.1392 dloss:0.0012\n",
      "Episode:376 meanR:0.1095 R:0.0000 gloss:-0.2074 dloss:0.0003\n",
      "Episode:377 meanR:0.1135 R:0.4000 gloss:-0.1588 dloss:0.0001\n",
      "Episode:378 meanR:0.1135 R:0.0000 gloss:-0.1249 dloss:0.0001\n",
      "Episode:379 meanR:0.1135 R:0.0000 gloss:-0.1046 dloss:0.0001\n",
      "Episode:380 meanR:0.1135 R:0.0000 gloss:-0.1232 dloss:0.0001\n",
      "Episode:381 meanR:0.1160 R:0.2500 gloss:-0.0955 dloss:0.0002\n",
      "Episode:382 meanR:0.1171 R:0.1100 gloss:-0.0725 dloss:0.0000\n",
      "Episode:383 meanR:0.1201 R:0.3000 gloss:-0.0521 dloss:0.0000\n",
      "Episode:384 meanR:0.1241 R:0.4000 gloss:-0.0683 dloss:0.0000\n",
      "Episode:385 meanR:0.1258 R:0.1700 gloss:-0.0796 dloss:0.0000\n",
      "Episode:386 meanR:0.1289 R:0.6200 gloss:-0.0444 dloss:0.0000\n",
      "Episode:387 meanR:0.1308 R:0.1900 gloss:-0.0726 dloss:0.0000\n",
      "Episode:388 meanR:0.1398 R:0.9000 gloss:-0.0941 dloss:0.0000\n",
      "Episode:389 meanR:0.1435 R:0.3700 gloss:-0.1322 dloss:0.0000\n",
      "Episode:390 meanR:0.1508 R:0.7300 gloss:-0.1532 dloss:0.0001\n",
      "Episode:391 meanR:0.1643 R:1.3500 gloss:-0.1646 dloss:0.0001\n",
      "Episode:392 meanR:0.1729 R:0.8600 gloss:-0.1324 dloss:0.0001\n",
      "Episode:393 meanR:0.1911 R:1.8200 gloss:-0.1532 dloss:0.0005\n",
      "Episode:394 meanR:0.1979 R:0.7800 gloss:-0.2124 dloss:0.0001\n",
      "Episode:395 meanR:0.2118 R:1.3900 gloss:-0.2122 dloss:0.0002\n",
      "Episode:396 meanR:0.2200 R:0.8200 gloss:-0.1994 dloss:0.0002\n",
      "Episode:397 meanR:0.2255 R:1.0300 gloss:-0.2018 dloss:0.0001\n",
      "Episode:398 meanR:0.2279 R:0.5800 gloss:-0.2150 dloss:0.0001\n",
      "Episode:399 meanR:0.2388 R:1.2500 gloss:-0.2006 dloss:0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:400 meanR:0.2473 R:0.8500 gloss:-0.1546 dloss:0.0001\n",
      "Episode:401 meanR:0.2643 R:1.7000 gloss:-0.1727 dloss:0.0001\n",
      "Episode:402 meanR:0.2732 R:1.2300 gloss:-2.7117 dloss:0.0869\n",
      "Episode:403 meanR:0.2720 R:0.4200 gloss:-0.0892 dloss:0.0009\n",
      "Episode:404 meanR:0.2790 R:1.1400 gloss:-0.1621 dloss:0.0003\n",
      "Episode:405 meanR:0.2892 R:1.0200 gloss:-0.0661 dloss:0.0001\n",
      "Episode:406 meanR:0.2874 R:0.0600 gloss:-0.0664 dloss:0.0001\n",
      "Episode:407 meanR:0.2899 R:0.2500 gloss:-0.0641 dloss:0.0001\n",
      "Episode:408 meanR:0.3017 R:1.1800 gloss:-0.0883 dloss:0.0002\n",
      "Episode:409 meanR:0.3044 R:0.3300 gloss:-0.0919 dloss:0.0005\n",
      "Episode:410 meanR:0.3087 R:0.4300 gloss:-0.0979 dloss:0.0002\n",
      "Episode:411 meanR:0.3149 R:0.6200 gloss:-0.1136 dloss:0.0005\n",
      "Episode:412 meanR:0.3229 R:0.8000 gloss:-0.1520 dloss:0.0001\n",
      "Episode:413 meanR:0.3386 R:1.5700 gloss:-0.1877 dloss:0.0001\n",
      "Episode:414 meanR:0.3416 R:0.3000 gloss:-0.1818 dloss:0.0001\n",
      "Episode:415 meanR:0.3528 R:1.1200 gloss:-0.1737 dloss:0.0001\n",
      "Episode:416 meanR:0.3657 R:1.3600 gloss:-0.1707 dloss:0.0001\n",
      "Episode:417 meanR:0.3741 R:1.0300 gloss:-0.1612 dloss:0.0001\n",
      "Episode:418 meanR:0.3814 R:0.9000 gloss:-0.1558 dloss:0.0001\n",
      "Episode:419 meanR:0.3881 R:0.6700 gloss:-0.1537 dloss:0.0001\n",
      "Episode:420 meanR:0.3941 R:0.7500 gloss:-0.1464 dloss:0.0001\n",
      "Episode:421 meanR:0.3968 R:0.2700 gloss:-0.1351 dloss:0.0001\n",
      "Episode:422 meanR:0.3976 R:0.2400 gloss:-0.1324 dloss:0.0001\n",
      "Episode:423 meanR:0.4026 R:1.0600 gloss:-0.1356 dloss:0.0001\n",
      "Episode:424 meanR:0.4127 R:1.1300 gloss:-0.1210 dloss:0.0001\n",
      "Episode:425 meanR:0.4205 R:0.9500 gloss:-0.1240 dloss:0.0001\n",
      "Episode:426 meanR:0.4244 R:0.6500 gloss:-0.1210 dloss:0.0001\n",
      "Episode:427 meanR:0.4372 R:1.2800 gloss:-0.1260 dloss:0.0001\n",
      "Episode:428 meanR:0.4393 R:0.3100 gloss:-0.1273 dloss:0.0001\n",
      "Episode:429 meanR:0.4394 R:0.1400 gloss:-0.1192 dloss:0.0001\n",
      "Episode:430 meanR:0.4352 R:0.2400 gloss:-0.1146 dloss:0.0001\n",
      "Episode:431 meanR:0.4341 R:0.6900 gloss:-0.1103 dloss:0.0001\n",
      "Episode:432 meanR:0.4323 R:0.5900 gloss:-0.1142 dloss:0.0001\n",
      "Episode:433 meanR:0.4393 R:0.8900 gloss:-0.1021 dloss:0.0000\n",
      "Episode:434 meanR:0.4411 R:0.4800 gloss:-0.1047 dloss:0.0001\n",
      "Episode:435 meanR:0.4369 R:0.2500 gloss:-0.1017 dloss:0.0001\n",
      "Episode:436 meanR:0.4359 R:0.1400 gloss:-0.0999 dloss:0.0001\n",
      "Episode:437 meanR:0.4470 R:1.4500 gloss:-0.1094 dloss:0.0001\n",
      "Episode:438 meanR:0.4493 R:0.3200 gloss:-0.1285 dloss:0.0002\n",
      "Episode:439 meanR:0.4515 R:0.5500 gloss:-0.1379 dloss:0.0002\n",
      "Episode:440 meanR:0.4713 R:1.9800 gloss:-0.1373 dloss:0.0003\n",
      "Episode:441 meanR:0.4783 R:0.7000 gloss:-0.1661 dloss:0.0004\n",
      "Episode:442 meanR:0.4832 R:0.4900 gloss:-0.1478 dloss:0.0002\n",
      "Episode:443 meanR:0.4860 R:0.2800 gloss:-0.1359 dloss:0.0003\n",
      "Episode:444 meanR:0.4995 R:1.3500 gloss:-0.1404 dloss:0.0002\n",
      "Episode:445 meanR:0.5081 R:0.8600 gloss:-0.1416 dloss:0.0002\n",
      "Episode:446 meanR:0.5150 R:0.6900 gloss:-0.1361 dloss:0.0002\n",
      "Episode:447 meanR:0.5270 R:1.2900 gloss:-0.1398 dloss:0.0002\n",
      "Episode:448 meanR:0.5429 R:1.5900 gloss:-0.1293 dloss:0.0003\n",
      "Episode:449 meanR:0.5486 R:0.5700 gloss:-0.1229 dloss:0.0002\n",
      "Episode:450 meanR:0.5531 R:0.4500 gloss:-0.1328 dloss:0.0003\n",
      "Episode:451 meanR:0.5530 R:0.0900 gloss:-0.1278 dloss:0.0002\n",
      "Episode:452 meanR:0.5586 R:0.5600 gloss:-0.1187 dloss:0.0003\n",
      "Episode:453 meanR:0.5700 R:1.1400 gloss:-0.1115 dloss:0.0002\n",
      "Episode:454 meanR:0.5789 R:0.8900 gloss:-0.1294 dloss:0.0002\n",
      "Episode:455 meanR:0.5846 R:0.5700 gloss:-0.1082 dloss:0.0001\n",
      "Episode:456 meanR:0.5897 R:0.6200 gloss:-0.1147 dloss:0.0002\n",
      "Episode:457 meanR:0.5958 R:0.6100 gloss:-0.1113 dloss:0.0002\n",
      "Episode:458 meanR:0.6000 R:0.4200 gloss:-0.1147 dloss:0.0002\n",
      "Episode:459 meanR:0.6045 R:0.4500 gloss:-0.1072 dloss:0.0002\n",
      "Episode:460 meanR:0.6183 R:1.3800 gloss:-0.1048 dloss:0.0004\n",
      "Episode:461 meanR:0.6190 R:0.0700 gloss:-0.1232 dloss:0.0002\n",
      "Episode:462 meanR:0.6247 R:0.5700 gloss:-0.1204 dloss:0.0002\n",
      "Episode:463 meanR:0.6315 R:0.7700 gloss:-0.1086 dloss:0.0003\n",
      "Episode:464 meanR:0.6410 R:0.9500 gloss:-0.1129 dloss:0.0002\n",
      "Episode:465 meanR:0.6470 R:0.6000 gloss:-0.1112 dloss:0.0002\n",
      "Episode:466 meanR:0.6516 R:0.4600 gloss:-0.1136 dloss:0.0002\n",
      "Episode:467 meanR:0.6607 R:0.9100 gloss:-0.1098 dloss:0.0001\n",
      "Episode:468 meanR:0.6626 R:0.1900 gloss:-0.1016 dloss:0.0002\n",
      "Episode:469 meanR:0.6661 R:0.3500 gloss:-0.0917 dloss:0.0002\n",
      "Episode:470 meanR:0.6727 R:0.8000 gloss:-0.0960 dloss:0.0002\n",
      "Episode:471 meanR:0.6705 R:0.4400 gloss:-0.0990 dloss:0.0004\n",
      "Episode:472 meanR:0.6681 R:0.0000 gloss:-0.1128 dloss:0.0002\n",
      "Episode:473 meanR:0.6818 R:1.3700 gloss:-0.1025 dloss:0.0001\n",
      "Episode:474 meanR:0.6921 R:1.0700 gloss:-0.1045 dloss:0.0004\n",
      "Episode:475 meanR:0.6957 R:0.3600 gloss:-0.0981 dloss:0.0004\n",
      "Episode:476 meanR:0.7014 R:0.5700 gloss:-0.1167 dloss:0.0006\n",
      "Episode:477 meanR:0.7071 R:0.9700 gloss:-0.1206 dloss:0.0004\n",
      "Episode:478 meanR:0.7165 R:0.9400 gloss:-0.1166 dloss:0.0004\n",
      "Episode:479 meanR:0.7244 R:0.7900 gloss:-0.1140 dloss:0.0007\n",
      "Episode:480 meanR:0.7299 R:0.5500 gloss:-0.1190 dloss:0.0013\n",
      "Episode:481 meanR:0.7335 R:0.6100 gloss:-0.1302 dloss:0.0021\n",
      "Episode:482 meanR:0.7359 R:0.3500 gloss:-0.1383 dloss:0.0021\n",
      "Episode:483 meanR:0.7427 R:0.9800 gloss:-0.1402 dloss:0.0030\n",
      "Episode:484 meanR:0.7475 R:0.8800 gloss:-0.1527 dloss:0.0040\n",
      "Episode:485 meanR:0.7543 R:0.8500 gloss:-0.1680 dloss:0.0078\n",
      "Episode:486 meanR:0.7770 R:2.8900 gloss:-0.2062 dloss:0.0040\n",
      "Episode:487 meanR:0.7838 R:0.8700 gloss:-0.2257 dloss:0.0088\n",
      "Episode:488 meanR:0.7870 R:1.2200 gloss:-0.1245 dloss:0.0112\n",
      "Episode:489 meanR:0.7965 R:1.3200 gloss:-0.1468 dloss:0.0080\n",
      "Episode:490 meanR:0.7972 R:0.8000 gloss:-0.1701 dloss:0.0124\n",
      "Episode:491 meanR:0.8037 R:2.0000 gloss:-0.1242 dloss:0.0120\n",
      "Episode:492 meanR:0.8048 R:0.9700 gloss:-0.1975 dloss:0.0097\n",
      "Episode:493 meanR:0.7910 R:0.4400 gloss:-0.1079 dloss:0.0131\n",
      "Episode:494 meanR:0.7944 R:1.1200 gloss:-0.1512 dloss:0.0193\n",
      "Episode:495 meanR:0.7952 R:1.4700 gloss:-0.2685 dloss:0.0099\n",
      "Episode:496 meanR:0.7968 R:0.9800 gloss:-0.1670 dloss:0.0203\n",
      "Episode:497 meanR:0.7949 R:0.8400 gloss:-0.0232 dloss:0.0110\n",
      "Episode:498 meanR:0.7964 R:0.7300 gloss:0.2295 dloss:0.0307\n",
      "Episode:499 meanR:0.7899 R:0.6000 gloss:0.0254 dloss:0.0294\n",
      "Episode:500 meanR:0.7918 R:1.0400 gloss:-0.9876 dloss:0.1846\n",
      "Episode:501 meanR:0.7780 R:0.3200 gloss:-1.2388 dloss:0.2654\n",
      "Episode:502 meanR:0.7674 R:0.1700 gloss:2.1112 dloss:1.4845\n",
      "Episode:503 meanR:0.7720 R:0.8800 gloss:4.9331 dloss:4.1591\n",
      "Episode:504 meanR:0.7625 R:0.1900 gloss:9.0370 dloss:48.2985\n",
      "Episode:505 meanR:0.7527 R:0.0400 gloss:-10.5337 dloss:5.1733\n",
      "Episode:506 meanR:0.7606 R:0.8500 gloss:-20.9189 dloss:3.5490\n",
      "Episode:507 meanR:0.7610 R:0.2900 gloss:-39.0667 dloss:5.4505\n",
      "Episode:508 meanR:0.7581 R:0.8900 gloss:-15.8891 dloss:2.1391\n",
      "Episode:509 meanR:0.7599 R:0.5100 gloss:-1.8716 dloss:3.6724\n",
      "Episode:510 meanR:0.7556 R:0.0000 gloss:-9.6674 dloss:0.2559\n",
      "Episode:511 meanR:0.7494 R:0.0000 gloss:-8.2918 dloss:0.1645\n",
      "Episode:512 meanR:0.7414 R:0.0000 gloss:-6.6545 dloss:0.0920\n",
      "Episode:513 meanR:0.7288 R:0.3100 gloss:-5.7850 dloss:0.0666\n",
      "Episode:514 meanR:0.7258 R:0.0000 gloss:-8.0725 dloss:0.0796\n",
      "Episode:515 meanR:0.7146 R:0.0000 gloss:-11.7810 dloss:0.1791\n",
      "Episode:516 meanR:0.7010 R:0.0000 gloss:-11.1544 dloss:0.1655\n",
      "Episode:517 meanR:0.6918 R:0.1100 gloss:-8.1197 dloss:0.0783\n",
      "Episode:518 meanR:0.6828 R:0.0000 gloss:-7.3433 dloss:0.0573\n",
      "Episode:519 meanR:0.6761 R:0.0000 gloss:-5.6936 dloss:0.0354\n",
      "Episode:520 meanR:0.6686 R:0.0000 gloss:-4.0465 dloss:0.0184\n",
      "Episode:521 meanR:0.6659 R:0.0000 gloss:-5.1273 dloss:0.0372\n",
      "Episode:522 meanR:0.6641 R:0.0600 gloss:-4.8820 dloss:0.0270\n",
      "Episode:523 meanR:0.6535 R:0.0000 gloss:-3.7381 dloss:0.0215\n",
      "Episode:524 meanR:0.6422 R:0.0000 gloss:-6.0028 dloss:0.0468\n",
      "Episode:525 meanR:0.6398 R:0.7100 gloss:-6.8721 dloss:0.0547\n",
      "Episode:526 meanR:0.6375 R:0.4200 gloss:-6.0071 dloss:0.0355\n",
      "Episode:527 meanR:0.6297 R:0.5000 gloss:-5.1508 dloss:0.0276\n",
      "Episode:528 meanR:0.6304 R:0.3800 gloss:-3.6468 dloss:0.0164\n",
      "Episode:529 meanR:0.6290 R:0.0000 gloss:-3.0579 dloss:0.0105\n",
      "Episode:530 meanR:0.6266 R:0.0000 gloss:-2.5908 dloss:0.0109\n",
      "Episode:531 meanR:0.6197 R:0.0000 gloss:-2.5718 dloss:0.0083\n",
      "Episode:532 meanR:0.6153 R:0.1500 gloss:-2.4276 dloss:0.0091\n",
      "Episode:533 meanR:0.6064 R:0.0000 gloss:-2.0388 dloss:0.0065\n",
      "Episode:534 meanR:0.6016 R:0.0000 gloss:-2.0171 dloss:0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:535 meanR:0.5991 R:0.0000 gloss:-1.7458 dloss:0.0053\n",
      "Episode:536 meanR:0.5981 R:0.0400 gloss:-1.6910 dloss:0.0042\n",
      "Episode:537 meanR:0.5836 R:0.0000 gloss:-1.4150 dloss:0.0028\n",
      "Episode:538 meanR:0.5867 R:0.6300 gloss:-1.1862 dloss:0.0027\n",
      "Episode:539 meanR:0.5852 R:0.4000 gloss:-0.9942 dloss:0.0024\n",
      "Episode:540 meanR:0.5654 R:0.0000 gloss:-1.3599 dloss:0.0030\n",
      "Episode:541 meanR:0.5584 R:0.0000 gloss:-1.4550 dloss:0.0022\n",
      "Episode:542 meanR:0.5535 R:0.0000 gloss:-1.3181 dloss:0.0023\n",
      "Episode:543 meanR:0.5507 R:0.0000 gloss:-1.2701 dloss:0.0020\n",
      "Episode:544 meanR:0.5372 R:0.0000 gloss:-1.1549 dloss:0.0017\n",
      "Episode:545 meanR:0.5286 R:0.0000 gloss:-1.1322 dloss:0.0017\n",
      "Episode:546 meanR:0.5222 R:0.0500 gloss:-1.1063 dloss:0.0016\n",
      "Episode:547 meanR:0.5093 R:0.0000 gloss:-1.0341 dloss:0.0013\n",
      "Episode:548 meanR:0.4934 R:0.0000 gloss:-0.9887 dloss:0.0016\n",
      "Episode:549 meanR:0.4877 R:0.0000 gloss:-0.9362 dloss:0.0011\n",
      "Episode:550 meanR:0.4832 R:0.0000 gloss:-1.0472 dloss:0.0012\n",
      "Episode:551 meanR:0.4823 R:0.0000 gloss:-0.9233 dloss:0.0010\n",
      "Episode:552 meanR:0.4767 R:0.0000 gloss:-0.7998 dloss:0.0007\n",
      "Episode:553 meanR:0.4653 R:0.0000 gloss:-0.8006 dloss:0.0008\n",
      "Episode:554 meanR:0.4564 R:0.0000 gloss:-0.7826 dloss:0.0008\n",
      "Episode:555 meanR:0.4507 R:0.0000 gloss:-0.7349 dloss:0.0006\n",
      "Episode:556 meanR:0.4445 R:0.0000 gloss:-0.6994 dloss:0.0008\n",
      "Episode:557 meanR:0.4384 R:0.0000 gloss:-0.5406 dloss:0.0005\n",
      "Episode:558 meanR:0.4342 R:0.0000 gloss:-0.5843 dloss:0.0005\n",
      "Episode:559 meanR:0.4306 R:0.0900 gloss:-0.5267 dloss:0.0004\n",
      "Episode:560 meanR:0.4168 R:0.0000 gloss:-0.4730 dloss:0.0004\n",
      "Episode:561 meanR:0.4161 R:0.0000 gloss:-0.4077 dloss:0.0004\n",
      "Episode:562 meanR:0.4114 R:0.1000 gloss:-0.3662 dloss:0.0002\n",
      "Episode:563 meanR:0.4085 R:0.4800 gloss:-0.3123 dloss:0.0002\n",
      "Episode:564 meanR:0.4062 R:0.7200 gloss:-187.1624 dloss:243.0645\n",
      "Episode:565 meanR:0.4079 R:0.7700 gloss:-829.0447 dloss:2050.3596\n",
      "Episode:566 meanR:0.4141 R:1.0800 gloss:-1237.8616 dloss:3625.7756\n",
      "Episode:567 meanR:0.4177 R:1.2700 gloss:-1108.0333 dloss:2653.5938\n",
      "Episode:568 meanR:0.4261 R:1.0300 gloss:-748.7531 dloss:1118.7770\n",
      "Episode:569 meanR:0.4226 R:0.0000 gloss:-810.4620 dloss:1047.7246\n",
      "Episode:570 meanR:0.4146 R:0.0000 gloss:-817.7444 dloss:1094.6024\n",
      "Episode:571 meanR:0.4102 R:0.0000 gloss:-749.2413 dloss:804.3702\n",
      "Episode:572 meanR:0.4102 R:0.0000 gloss:-630.3953 dloss:536.3887\n",
      "Episode:573 meanR:0.3965 R:0.0000 gloss:-491.8647 dloss:303.1201\n",
      "Episode:574 meanR:0.3858 R:0.0000 gloss:-344.9457 dloss:152.6278\n",
      "Episode:575 meanR:0.3822 R:0.0000 gloss:-230.7057 dloss:65.6129\n",
      "Episode:576 meanR:0.3808 R:0.4300 gloss:-226.4780 dloss:58.6752\n",
      "Episode:577 meanR:0.3736 R:0.2500 gloss:-187.7723 dloss:36.7195\n",
      "Episode:578 meanR:0.3659 R:0.1700 gloss:-141.7423 dloss:21.8617\n",
      "Episode:579 meanR:0.3594 R:0.1400 gloss:-95.5330 dloss:9.0915\n",
      "Episode:580 meanR:0.3566 R:0.2700 gloss:-51.8538 dloss:3.1221\n",
      "Episode:581 meanR:0.3530 R:0.2500 gloss:-16.6633 dloss:0.7652\n",
      "Episode:582 meanR:0.3495 R:0.0000 gloss:-1.0878 dloss:2.8946\n",
      "Episode:583 meanR:0.3397 R:0.0000 gloss:-62.1562 dloss:8.3266\n",
      "Episode:584 meanR:0.3309 R:0.0000 gloss:-55.6366 dloss:3.6431\n",
      "Episode:585 meanR:0.3224 R:0.0000 gloss:-47.8933 dloss:2.0248\n",
      "Episode:586 meanR:0.2935 R:0.0000 gloss:-36.6819 dloss:1.4435\n",
      "Episode:587 meanR:0.2893 R:0.4500 gloss:-39.7489 dloss:1.9198\n",
      "Episode:588 meanR:0.2865 R:0.9400 gloss:-48.1757 dloss:2.4976\n",
      "Episode:589 meanR:0.2757 R:0.2400 gloss:-47.8246 dloss:2.2527\n",
      "Episode:590 meanR:0.2739 R:0.6200 gloss:-41.9249 dloss:1.9211\n",
      "Episode:591 meanR:0.2608 R:0.6900 gloss:-36.3046 dloss:1.3136\n",
      "Episode:592 meanR:0.2571 R:0.6000 gloss:-30.2631 dloss:0.7491\n",
      "Episode:593 meanR:0.2572 R:0.4500 gloss:-24.3793 dloss:0.6381\n",
      "Episode:594 meanR:0.2490 R:0.3000 gloss:-20.5243 dloss:0.3693\n",
      "Episode:595 meanR:0.2382 R:0.3900 gloss:-16.2077 dloss:0.2845\n",
      "Episode:596 meanR:0.2298 R:0.1400 gloss:-13.9655 dloss:0.2270\n",
      "Episode:597 meanR:0.2259 R:0.4500 gloss:-11.2190 dloss:0.1655\n",
      "Episode:598 meanR:0.2231 R:0.4500 gloss:-9.4731 dloss:0.0984\n",
      "Episode:599 meanR:0.2226 R:0.5500 gloss:-8.2132 dloss:0.0735\n",
      "Episode:600 meanR:0.2122 R:0.0000 gloss:-6.6424 dloss:0.0555\n",
      "Episode:601 meanR:0.2130 R:0.4000 gloss:-9.8223 dloss:0.1088\n",
      "Episode:602 meanR:0.2212 R:0.9900 gloss:-7.8330 dloss:0.0582\n",
      "Episode:603 meanR:0.2215 R:0.9100 gloss:-7.5196 dloss:0.0629\n",
      "Episode:604 meanR:0.2296 R:1.0000 gloss:-6.5002 dloss:0.0422\n",
      "Episode:605 meanR:0.2380 R:0.8800 gloss:-5.2078 dloss:0.0295\n",
      "Episode:606 meanR:0.2355 R:0.6000 gloss:-4.3824 dloss:0.0289\n",
      "Episode:607 meanR:0.2428 R:1.0200 gloss:-3.7431 dloss:0.0178\n",
      "Episode:608 meanR:0.2438 R:0.9900 gloss:-3.3567 dloss:0.0144\n",
      "Episode:609 meanR:0.2420 R:0.3300 gloss:-3.0608 dloss:0.0113\n",
      "Episode:610 meanR:0.2604 R:1.8400 gloss:-2.5713 dloss:0.0105\n",
      "Episode:611 meanR:0.2695 R:0.9100 gloss:-2.1977 dloss:0.0068\n",
      "Episode:612 meanR:0.2820 R:1.2500 gloss:-1.9087 dloss:0.0056\n",
      "Episode:613 meanR:0.2803 R:0.1400 gloss:-1.5552 dloss:0.0046\n",
      "Episode:614 meanR:0.2945 R:1.4200 gloss:-1.3471 dloss:0.0038\n",
      "Episode:615 meanR:0.2991 R:0.4600 gloss:-1.2653 dloss:0.0028\n",
      "Episode:616 meanR:0.3049 R:0.5800 gloss:-1.1958 dloss:0.0030\n",
      "Episode:617 meanR:0.3100 R:0.6200 gloss:-1.0618 dloss:0.0021\n",
      "Episode:618 meanR:0.3160 R:0.6000 gloss:-0.9921 dloss:0.0023\n",
      "Episode:619 meanR:0.3207 R:0.4700 gloss:-0.8977 dloss:0.0020\n",
      "Episode:620 meanR:0.3283 R:0.7600 gloss:-0.8063 dloss:0.0019\n",
      "Episode:621 meanR:0.3399 R:1.1600 gloss:-0.7179 dloss:0.0017\n",
      "Episode:622 meanR:0.3542 R:1.4900 gloss:-0.6277 dloss:0.0014\n",
      "Episode:623 meanR:0.3621 R:0.7900 gloss:-0.6353 dloss:0.0011\n",
      "Episode:624 meanR:0.3737 R:1.1600 gloss:-0.5574 dloss:0.0011\n",
      "Episode:625 meanR:0.3736 R:0.7000 gloss:-0.5477 dloss:0.0011\n",
      "Episode:626 meanR:0.3822 R:1.2800 gloss:-0.5695 dloss:0.0010\n",
      "Episode:627 meanR:0.3908 R:1.3600 gloss:-0.5463 dloss:0.0011\n",
      "Episode:628 meanR:0.3944 R:0.7400 gloss:-0.4786 dloss:0.0008\n",
      "Episode:629 meanR:0.4085 R:1.4100 gloss:-0.4363 dloss:0.0008\n",
      "Episode:630 meanR:0.4155 R:0.7000 gloss:-0.4268 dloss:0.0008\n",
      "Episode:631 meanR:0.4188 R:0.3300 gloss:-0.3802 dloss:0.0005\n",
      "Episode:632 meanR:0.4243 R:0.7000 gloss:-0.3090 dloss:0.0005\n",
      "Episode:633 meanR:0.4343 R:1.0000 gloss:-0.3054 dloss:0.0005\n",
      "Episode:634 meanR:0.4537 R:1.9400 gloss:-0.2995 dloss:0.0005\n",
      "Episode:635 meanR:0.4668 R:1.3100 gloss:-0.2643 dloss:0.0004\n",
      "Episode:636 meanR:0.4718 R:0.5400 gloss:-0.2660 dloss:0.0005\n",
      "Episode:637 meanR:0.4851 R:1.3300 gloss:-0.2344 dloss:0.0003\n",
      "Episode:638 meanR:0.4829 R:0.4100 gloss:-0.2133 dloss:0.0004\n",
      "Episode:639 meanR:0.4882 R:0.9300 gloss:-0.2862 dloss:0.0006\n",
      "Episode:640 meanR:0.5014 R:1.3200 gloss:-0.2424 dloss:0.0002\n",
      "Episode:641 meanR:0.5160 R:1.4600 gloss:-0.2581 dloss:0.0006\n",
      "Episode:642 meanR:0.5215 R:0.5500 gloss:-0.1967 dloss:0.0002\n",
      "Episode:643 meanR:0.5224 R:0.0900 gloss:-0.2112 dloss:0.0003\n",
      "Episode:644 meanR:0.5267 R:0.4300 gloss:-0.1975 dloss:0.0005\n",
      "Episode:645 meanR:0.5366 R:0.9900 gloss:-0.1322 dloss:0.0002\n",
      "Episode:646 meanR:0.5432 R:0.7100 gloss:-0.1995 dloss:0.0002\n",
      "Episode:647 meanR:0.5468 R:0.3600 gloss:-0.1557 dloss:0.0003\n",
      "Episode:648 meanR:0.5592 R:1.2400 gloss:-0.2596 dloss:0.0010\n",
      "Episode:649 meanR:0.5791 R:1.9900 gloss:-0.2835 dloss:0.0003\n",
      "Episode:650 meanR:0.5863 R:0.7200 gloss:-0.2261 dloss:0.0002\n",
      "Episode:651 meanR:0.5951 R:0.8800 gloss:-0.1662 dloss:0.0002\n",
      "Episode:652 meanR:0.5992 R:0.4100 gloss:-0.1544 dloss:0.0003\n",
      "Episode:653 meanR:0.6041 R:0.4900 gloss:-0.2203 dloss:0.0014\n",
      "Episode:654 meanR:0.6084 R:0.4300 gloss:-0.3292 dloss:0.0004\n",
      "Episode:655 meanR:0.6173 R:0.8900 gloss:-0.2694 dloss:0.0003\n",
      "Episode:656 meanR:0.6298 R:1.2500 gloss:-0.2117 dloss:0.0003\n",
      "Episode:657 meanR:0.6350 R:0.5200 gloss:-0.1639 dloss:0.0003\n",
      "Episode:658 meanR:0.6384 R:0.3400 gloss:-0.1580 dloss:0.0003\n",
      "Episode:659 meanR:0.6486 R:1.1100 gloss:-0.1418 dloss:0.0003\n",
      "Episode:660 meanR:0.6529 R:0.4300 gloss:-0.1705 dloss:0.0002\n",
      "Episode:661 meanR:0.6572 R:0.4300 gloss:-0.2042 dloss:0.0004\n",
      "Episode:662 meanR:0.6628 R:0.6600 gloss:-0.2203 dloss:0.0005\n",
      "Episode:663 meanR:0.6612 R:0.3200 gloss:-0.2071 dloss:0.0003\n",
      "Episode:664 meanR:0.6577 R:0.3700 gloss:-0.1643 dloss:0.0004\n",
      "Episode:665 meanR:0.6551 R:0.5100 gloss:-0.1960 dloss:0.0006\n",
      "Episode:666 meanR:0.6518 R:0.7500 gloss:-0.2084 dloss:0.0006\n",
      "Episode:667 meanR:0.6477 R:0.8600 gloss:-0.1962 dloss:0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:668 meanR:0.6374 R:0.0000 gloss:-0.1734 dloss:0.0005\n",
      "Episode:669 meanR:0.6542 R:1.6800 gloss:-0.1450 dloss:0.0014\n",
      "Episode:670 meanR:0.6586 R:0.4400 gloss:-0.2665 dloss:0.0023\n",
      "Episode:671 meanR:0.6658 R:0.7200 gloss:-0.2286 dloss:0.0016\n",
      "Episode:672 meanR:0.6685 R:0.2700 gloss:-0.2427 dloss:0.0028\n",
      "Episode:673 meanR:0.6800 R:1.1500 gloss:-0.3099 dloss:0.0035\n",
      "Episode:674 meanR:0.6827 R:0.2700 gloss:-0.4743 dloss:0.0547\n",
      "Episode:675 meanR:0.6981 R:1.5400 gloss:0.7776 dloss:0.3267\n",
      "Episode:676 meanR:0.7036 R:0.9800 gloss:-1.5041 dloss:0.3894\n",
      "Episode:677 meanR:0.7044 R:0.3300 gloss:1.0131 dloss:0.1444\n",
      "Episode:678 meanR:0.7097 R:0.7000 gloss:-0.0831 dloss:0.1257\n",
      "Episode:679 meanR:0.7151 R:0.6800 gloss:-0.7169 dloss:0.1224\n",
      "Episode:680 meanR:0.7174 R:0.5000 gloss:-0.1188 dloss:0.1801\n",
      "Episode:681 meanR:0.7221 R:0.7200 gloss:0.5955 dloss:0.2082\n",
      "Episode:682 meanR:0.7333 R:1.1200 gloss:-1.8297 dloss:0.2900\n",
      "Episode:683 meanR:0.7385 R:0.5200 gloss:3.0226 dloss:0.3916\n",
      "Episode:684 meanR:0.7417 R:0.3200 gloss:1.8977 dloss:0.6861\n",
      "Episode:685 meanR:0.7522 R:1.0500 gloss:-6.6873 dloss:1.1268\n",
      "Episode:686 meanR:0.7619 R:0.9700 gloss:2.0195 dloss:0.2109\n",
      "Episode:687 meanR:0.7736 R:1.6200 gloss:0.9435 dloss:0.0065\n",
      "Episode:688 meanR:0.7731 R:0.8900 gloss:-1.3888 dloss:0.0103\n",
      "Episode:689 meanR:0.7762 R:0.5500 gloss:-0.0235 dloss:0.0173\n",
      "Episode:690 meanR:0.7767 R:0.6700 gloss:-0.3055 dloss:0.0056\n",
      "Episode:691 meanR:0.7828 R:1.3000 gloss:0.1370 dloss:0.0143\n",
      "Episode:692 meanR:0.7948 R:1.8000 gloss:0.0307 dloss:0.0797\n",
      "Episode:693 meanR:0.8098 R:1.9500 gloss:-0.3009 dloss:0.0009\n",
      "Episode:694 meanR:0.8158 R:0.9000 gloss:-0.2913 dloss:0.0007\n",
      "Episode:695 meanR:0.8148 R:0.2900 gloss:-1.5140 dloss:0.0044\n",
      "Episode:696 meanR:0.8208 R:0.7400 gloss:-1.7774 dloss:0.0038\n",
      "Episode:697 meanR:0.8220 R:0.5700 gloss:-1.0089 dloss:0.0015\n",
      "Episode:698 meanR:0.8214 R:0.3900 gloss:-1.0524 dloss:0.0014\n",
      "Episode:699 meanR:0.8242 R:0.8300 gloss:-1.8357 dloss:0.0057\n",
      "Episode:700 meanR:0.8242 R:0.0000 gloss:-3.1294 dloss:0.0152\n",
      "Episode:701 meanR:0.8202 R:0.0000 gloss:-2.2314 dloss:0.0092\n",
      "Episode:702 meanR:0.8103 R:0.0000 gloss:-1.0299 dloss:0.0068\n",
      "Episode:703 meanR:0.8022 R:0.1000 gloss:-0.3988 dloss:0.0048\n",
      "Episode:704 meanR:0.7922 R:0.0000 gloss:-0.4062 dloss:0.0045\n",
      "Episode:705 meanR:0.7834 R:0.0000 gloss:-0.4582 dloss:0.0060\n",
      "Episode:706 meanR:0.7774 R:0.0000 gloss:-0.4521 dloss:0.0043\n",
      "Episode:707 meanR:0.7672 R:0.0000 gloss:-0.3743 dloss:0.0037\n",
      "Episode:708 meanR:0.7573 R:0.0000 gloss:-0.9348 dloss:0.0027\n",
      "Episode:709 meanR:0.7540 R:0.0000 gloss:-2.1663 dloss:0.0052\n",
      "Episode:710 meanR:0.7356 R:0.0000 gloss:-2.2810 dloss:0.0059\n",
      "Episode:711 meanR:0.7265 R:0.0000 gloss:-2.1013 dloss:0.0049\n",
      "Episode:712 meanR:0.7140 R:0.0000 gloss:-1.8444 dloss:0.0039\n",
      "Episode:713 meanR:0.7126 R:0.0000 gloss:-1.7530 dloss:0.0041\n",
      "Episode:714 meanR:0.6984 R:0.0000 gloss:-1.7490 dloss:0.0028\n",
      "Episode:715 meanR:0.6938 R:0.0000 gloss:-1.5973 dloss:0.0028\n",
      "Episode:716 meanR:0.6890 R:0.1000 gloss:-1.5317 dloss:0.0029\n",
      "Episode:717 meanR:0.6831 R:0.0300 gloss:-1.4924 dloss:0.0027\n",
      "Episode:718 meanR:0.6771 R:0.0000 gloss:-1.2593 dloss:0.0020\n",
      "Episode:719 meanR:0.6724 R:0.0000 gloss:-0.9961 dloss:0.0012\n",
      "Episode:720 meanR:0.6648 R:0.0000 gloss:-0.8591 dloss:0.0008\n",
      "Episode:721 meanR:0.6547 R:0.1500 gloss:-0.6398 dloss:0.0005\n",
      "Episode:722 meanR:0.6464 R:0.6600 gloss:-0.4659 dloss:0.0003\n",
      "Episode:723 meanR:0.6385 R:0.0000 gloss:0.6729 dloss:0.0240\n",
      "Episode:724 meanR:0.6269 R:0.0000 gloss:-0.5907 dloss:0.0008\n",
      "Episode:725 meanR:0.6199 R:0.0000 gloss:-0.4626 dloss:0.0004\n",
      "Episode:726 meanR:0.6116 R:0.4500 gloss:-0.4339 dloss:0.0281\n",
      "Episode:727 meanR:0.6013 R:0.3300 gloss:1.1044 dloss:0.0232\n",
      "Episode:728 meanR:0.6069 R:1.3000 gloss:-0.5622 dloss:0.0004\n",
      "Episode:729 meanR:0.5928 R:0.0000 gloss:-0.5717 dloss:0.0004\n",
      "Episode:730 meanR:0.5858 R:0.0000 gloss:-0.5591 dloss:0.0004\n",
      "Episode:731 meanR:0.5825 R:0.0000 gloss:-0.5157 dloss:0.0004\n",
      "Episode:732 meanR:0.5756 R:0.0100 gloss:-0.4453 dloss:0.0009\n",
      "Episode:733 meanR:0.5656 R:0.0000 gloss:-0.4476 dloss:0.0005\n",
      "Episode:734 meanR:0.5462 R:0.0000 gloss:-0.3896 dloss:0.0010\n",
      "Episode:735 meanR:0.5331 R:0.0000 gloss:-0.4079 dloss:0.0012\n",
      "Episode:736 meanR:0.5277 R:0.0000 gloss:-0.3982 dloss:0.0013\n",
      "Episode:737 meanR:0.5144 R:0.0000 gloss:-0.3840 dloss:0.0007\n",
      "Episode:738 meanR:0.5103 R:0.0000 gloss:-0.3236 dloss:0.0002\n",
      "Episode:739 meanR:0.5010 R:0.0000 gloss:-0.2768 dloss:0.0004\n",
      "Episode:740 meanR:0.4878 R:0.0000 gloss:-0.2600 dloss:0.0004\n",
      "Episode:741 meanR:0.4732 R:0.0000 gloss:-0.2374 dloss:0.0002\n",
      "Episode:742 meanR:0.4677 R:0.0000 gloss:-0.2393 dloss:0.0004\n",
      "Episode:743 meanR:0.4668 R:0.0000 gloss:-0.2301 dloss:0.0002\n",
      "Episode:744 meanR:0.4625 R:0.0000 gloss:-0.1696 dloss:0.0015\n",
      "Episode:745 meanR:0.4526 R:0.0000 gloss:-0.1585 dloss:0.0034\n",
      "Episode:746 meanR:0.4455 R:0.0000 gloss:-0.1494 dloss:0.0012\n",
      "Episode:747 meanR:0.4419 R:0.0000 gloss:0.0058 dloss:0.0090\n",
      "Episode:748 meanR:0.4295 R:0.0000 gloss:-0.1968 dloss:0.0002\n",
      "Episode:749 meanR:0.4096 R:0.0000 gloss:0.8787 dloss:0.9492\n",
      "Episode:750 meanR:0.4036 R:0.1200 gloss:1.1766 dloss:0.0321\n",
      "Episode:751 meanR:0.4003 R:0.5500 gloss:0.0657 dloss:0.0047\n",
      "Episode:752 meanR:0.3990 R:0.2800 gloss:-0.1622 dloss:0.0013\n",
      "Episode:753 meanR:0.4003 R:0.6200 gloss:-0.9545 dloss:0.0021\n",
      "Episode:754 meanR:0.4041 R:0.8100 gloss:-2.5206 dloss:0.0084\n",
      "Episode:755 meanR:0.3977 R:0.2500 gloss:-1.9988 dloss:0.0043\n",
      "Episode:756 meanR:0.3999 R:1.4700 gloss:-1.0217 dloss:0.0013\n",
      "Episode:757 meanR:0.3957 R:0.1000 gloss:-0.4171 dloss:0.0005\n",
      "Episode:758 meanR:0.3929 R:0.0600 gloss:-0.5279 dloss:0.0004\n",
      "Episode:759 meanR:0.3822 R:0.0400 gloss:-0.5080 dloss:0.0010\n",
      "Episode:760 meanR:0.3779 R:0.0000 gloss:-0.0795 dloss:0.0041\n",
      "Episode:761 meanR:0.3736 R:0.0000 gloss:-0.0866 dloss:0.0067\n",
      "Episode:762 meanR:0.3670 R:0.0000 gloss:-0.3038 dloss:0.0004\n",
      "Episode:763 meanR:0.3638 R:0.0000 gloss:-0.2780 dloss:0.0007\n",
      "Episode:764 meanR:0.3601 R:0.0000 gloss:-0.4151 dloss:0.0020\n",
      "Episode:765 meanR:0.3625 R:0.7500 gloss:-0.7148 dloss:0.0009\n",
      "Episode:766 meanR:0.3557 R:0.0700 gloss:-0.5974 dloss:0.0006\n",
      "Episode:767 meanR:0.3636 R:1.6500 gloss:-0.4886 dloss:0.0025\n",
      "Episode:768 meanR:0.3662 R:0.2600 gloss:0.8100 dloss:0.0490\n",
      "Episode:769 meanR:0.3502 R:0.0800 gloss:-0.1607 dloss:0.0019\n",
      "Episode:770 meanR:0.3458 R:0.0000 gloss:-0.3336 dloss:0.0002\n",
      "Episode:771 meanR:0.3469 R:0.8300 gloss:-0.3180 dloss:0.0003\n",
      "Episode:772 meanR:0.3568 R:1.2600 gloss:-0.3252 dloss:0.0003\n",
      "Episode:773 meanR:0.3547 R:0.9400 gloss:-0.3730 dloss:0.0003\n",
      "Episode:774 meanR:0.3655 R:1.3500 gloss:-0.3381 dloss:0.0004\n",
      "Episode:775 meanR:0.3565 R:0.6400 gloss:-0.1179 dloss:0.0055\n",
      "Episode:776 meanR:0.3519 R:0.5200 gloss:0.5162 dloss:0.0315\n",
      "Episode:777 meanR:0.3552 R:0.6600 gloss:0.5545 dloss:0.0927\n",
      "Episode:778 meanR:0.3548 R:0.6600 gloss:-1.9579 dloss:0.1579\n",
      "Episode:779 meanR:0.3589 R:1.0900 gloss:-0.2582 dloss:0.0032\n",
      "Episode:780 meanR:0.3619 R:0.8000 gloss:-0.1009 dloss:0.0003\n",
      "Episode:781 meanR:0.3595 R:0.4800 gloss:-0.1422 dloss:0.0001\n",
      "Episode:782 meanR:0.3561 R:0.7800 gloss:-0.1619 dloss:0.0001\n",
      "Episode:783 meanR:0.3559 R:0.5000 gloss:-0.1883 dloss:0.0001\n",
      "Episode:784 meanR:0.3583 R:0.5600 gloss:-0.1858 dloss:0.0013\n",
      "Episode:785 meanR:0.3574 R:0.9600 gloss:-0.0213 dloss:0.0070\n",
      "Episode:786 meanR:0.3526 R:0.4900 gloss:-0.0317 dloss:0.0137\n",
      "Episode:787 meanR:0.3418 R:0.5400 gloss:0.6963 dloss:0.0775\n",
      "Episode:788 meanR:0.3382 R:0.5300 gloss:-10.3396 dloss:13.1512\n",
      "Episode:789 meanR:0.3366 R:0.3900 gloss:-408.4077 dloss:1976.9017\n",
      "Episode:790 meanR:0.3334 R:0.3500 gloss:-1293.0642 dloss:13199.8486\n",
      "Episode:791 meanR:0.3332 R:1.2800 gloss:-2454.5701 dloss:43034.8203\n",
      "Episode:792 meanR:0.3272 R:1.2000 gloss:-3733.2019 dloss:86091.8906\n",
      "Episode:793 meanR:0.3206 R:1.2900 gloss:-5022.8506 dloss:135291.2812\n",
      "Episode:794 meanR:0.3319 R:2.0300 gloss:-6288.1470 dloss:186185.1094\n",
      "Episode:795 meanR:0.3433 R:1.4300 gloss:-7283.8350 dloss:216336.7500\n",
      "Episode:796 meanR:0.3495 R:1.3600 gloss:-7832.9580 dloss:237574.5469\n",
      "Episode:797 meanR:0.3531 R:0.9300 gloss:-7917.8760 dloss:212478.1094\n",
      "Episode:798 meanR:0.3730 R:2.3800 gloss:-7792.9590 dloss:170907.0000\n",
      "Episode:799 meanR:0.3663 R:0.1600 gloss:-7309.9160 dloss:149914.4062\n",
      "Episode:800 meanR:0.3848 R:1.8500 gloss:-6621.4556 dloss:111745.8594\n",
      "Episode:801 meanR:0.3926 R:0.7800 gloss:-5759.8291 dloss:78678.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:802 meanR:0.3985 R:0.5900 gloss:-4733.8081 dloss:54710.6250\n",
      "Episode:803 meanR:0.4145 R:1.7000 gloss:-3649.0100 dloss:31442.8184\n",
      "Episode:804 meanR:0.4195 R:0.5000 gloss:-2560.4106 dloss:19425.1289\n",
      "Episode:805 meanR:0.4263 R:0.6800 gloss:-1964.3484 dloss:18178.3379\n",
      "Episode:806 meanR:0.4295 R:0.3200 gloss:-2894.4207 dloss:21084.2949\n",
      "Episode:807 meanR:0.4318 R:0.2300 gloss:-3377.3416 dloss:21138.0098\n",
      "Episode:808 meanR:0.4330 R:0.1200 gloss:-3606.8604 dloss:22009.4121\n",
      "Episode:809 meanR:0.4413 R:0.8300 gloss:-3469.6421 dloss:18824.9434\n",
      "Episode:810 meanR:0.4460 R:0.4700 gloss:-3300.8882 dloss:16871.1504\n",
      "Episode:811 meanR:0.4489 R:0.2900 gloss:-3031.8062 dloss:14256.3936\n",
      "Episode:812 meanR:0.4520 R:0.3100 gloss:-2698.6646 dloss:9442.5732\n",
      "Episode:813 meanR:0.4560 R:0.4000 gloss:-2354.9302 dloss:6843.7852\n",
      "Episode:814 meanR:0.4575 R:0.1500 gloss:-2068.2468 dloss:5020.6245\n",
      "Episode:815 meanR:0.4585 R:0.1000 gloss:-1747.3009 dloss:3315.2212\n",
      "Episode:816 meanR:0.4585 R:0.1000 gloss:-1419.5181 dloss:2384.2212\n",
      "Episode:817 meanR:0.4633 R:0.5100 gloss:-1104.6095 dloss:1148.7600\n",
      "Episode:818 meanR:0.4697 R:0.6400 gloss:-803.1967 dloss:745.4522\n",
      "Episode:819 meanR:0.4697 R:0.0000 gloss:-515.6703 dloss:483.2649\n",
      "Episode:820 meanR:0.4761 R:0.6400 gloss:-281.8051 dloss:225.0878\n",
      "Episode:821 meanR:0.4854 R:1.0800 gloss:-101.7112 dloss:177.9437\n",
      "Episode:822 meanR:0.4823 R:0.3500 gloss:-138.8471 dloss:445.9446\n",
      "Episode:823 meanR:0.4845 R:0.2200 gloss:-506.2686 dloss:529.3839\n",
      "Episode:824 meanR:0.4967 R:1.2200 gloss:-551.7543 dloss:301.0962\n",
      "Episode:825 meanR:0.5090 R:1.2300 gloss:-490.0028 dloss:242.9158\n",
      "Episode:826 meanR:0.5045 R:0.0000 gloss:-442.5512 dloss:201.5913\n",
      "Episode:827 meanR:0.5012 R:0.0000 gloss:-375.7643 dloss:152.1986\n",
      "Episode:828 meanR:0.4882 R:0.0000 gloss:-307.4634 dloss:97.8923\n",
      "Episode:829 meanR:0.4941 R:0.5900 gloss:-248.1303 dloss:65.3112\n",
      "Episode:830 meanR:0.4981 R:0.4000 gloss:-194.9867 dloss:37.3996\n",
      "Episode:831 meanR:0.5152 R:1.7100 gloss:-140.4701 dloss:23.4573\n",
      "Episode:832 meanR:0.5393 R:2.4200 gloss:-89.1378 dloss:9.0879\n",
      "Episode:833 meanR:0.5423 R:0.3000 gloss:-51.4499 dloss:2.8616\n",
      "Episode:834 meanR:0.5423 R:0.0000 gloss:-27.1945 dloss:1.0094\n",
      "Episode:835 meanR:0.5423 R:0.0000 gloss:-13.5129 dloss:0.3673\n",
      "Episode:836 meanR:0.5423 R:0.0000 gloss:-7.2095 dloss:0.1344\n",
      "Episode:837 meanR:0.5423 R:0.0000 gloss:-3.3078 dloss:0.0644\n",
      "Episode:838 meanR:0.5423 R:0.0000 gloss:-1.5651 dloss:0.0433\n",
      "Episode:839 meanR:0.5423 R:0.0000 gloss:-0.4258 dloss:0.0328\n",
      "Episode:840 meanR:0.5423 R:0.0000 gloss:-0.0535 dloss:0.0258\n",
      "Episode:841 meanR:0.5423 R:0.0000 gloss:-0.0683 dloss:0.0219\n",
      "Episode:842 meanR:0.5423 R:0.0000 gloss:0.2243 dloss:0.0681\n",
      "Episode:843 meanR:0.5423 R:0.0000 gloss:0.0302 dloss:0.0270\n",
      "Episode:844 meanR:0.5423 R:0.0000 gloss:0.6978 dloss:0.1057\n",
      "Episode:845 meanR:0.5424 R:0.0100 gloss:0.1374 dloss:0.2404\n",
      "Episode:846 meanR:0.5424 R:0.0000 gloss:0.0147 dloss:0.0825\n",
      "Episode:847 meanR:0.5435 R:0.1100 gloss:0.1296 dloss:0.0320\n",
      "Episode:848 meanR:0.5435 R:0.0000 gloss:0.3871 dloss:0.1496\n",
      "Episode:849 meanR:0.5435 R:0.0000 gloss:0.1600 dloss:0.0556\n",
      "Episode:850 meanR:0.5423 R:0.0000 gloss:-0.1977 dloss:0.0680\n",
      "Episode:851 meanR:0.5379 R:0.1100 gloss:0.1855 dloss:0.0806\n",
      "Episode:852 meanR:0.5351 R:0.0000 gloss:-0.0110 dloss:0.0446\n",
      "Episode:853 meanR:0.5289 R:0.0000 gloss:0.2440 dloss:0.0860\n",
      "Episode:854 meanR:0.5208 R:0.0000 gloss:-0.2454 dloss:0.0466\n",
      "Episode:855 meanR:0.5183 R:0.0000 gloss:-0.0484 dloss:0.0686\n",
      "Episode:856 meanR:0.5044 R:0.0800 gloss:0.0390 dloss:0.0606\n",
      "Episode:857 meanR:0.5034 R:0.0000 gloss:-0.0418 dloss:0.0296\n",
      "Episode:858 meanR:0.5028 R:0.0000 gloss:0.0471 dloss:0.0641\n",
      "Episode:859 meanR:0.5024 R:0.0000 gloss:0.1303 dloss:0.0357\n",
      "Episode:860 meanR:0.5024 R:0.0000 gloss:-0.3673 dloss:0.0447\n",
      "Episode:861 meanR:0.5035 R:0.1100 gloss:0.0478 dloss:0.0481\n",
      "Episode:862 meanR:0.5035 R:0.0000 gloss:-0.2037 dloss:0.0416\n",
      "Episode:863 meanR:0.5039 R:0.0400 gloss:0.1611 dloss:0.0306\n",
      "Episode:864 meanR:0.5039 R:0.0000 gloss:0.1527 dloss:0.0445\n",
      "Episode:865 meanR:0.4974 R:0.1000 gloss:-0.0937 dloss:0.0392\n",
      "Episode:866 meanR:0.4967 R:0.0000 gloss:0.1498 dloss:0.0445\n",
      "Episode:867 meanR:0.4802 R:0.0000 gloss:0.0090 dloss:0.0269\n",
      "Episode:868 meanR:0.4776 R:0.0000 gloss:-0.0752 dloss:0.0359\n",
      "Episode:869 meanR:0.4768 R:0.0000 gloss:-0.1565 dloss:0.0458\n",
      "Episode:870 meanR:0.4768 R:0.0000 gloss:0.0288 dloss:0.0206\n",
      "Episode:871 meanR:0.4685 R:0.0000 gloss:0.2407 dloss:0.0486\n",
      "Episode:872 meanR:0.4559 R:0.0000 gloss:0.0146 dloss:0.0263\n",
      "Episode:873 meanR:0.4465 R:0.0000 gloss:0.0793 dloss:0.0363\n",
      "Episode:874 meanR:0.4330 R:0.0000 gloss:0.0305 dloss:0.0175\n",
      "Episode:875 meanR:0.4266 R:0.0000 gloss:-0.0776 dloss:0.0447\n",
      "Episode:876 meanR:0.4214 R:0.0000 gloss:-0.0070 dloss:0.0305\n",
      "Episode:877 meanR:0.4148 R:0.0000 gloss:0.0982 dloss:0.0402\n",
      "Episode:878 meanR:0.4082 R:0.0000 gloss:-0.0604 dloss:0.0290\n",
      "Episode:879 meanR:0.3973 R:0.0000 gloss:-0.0794 dloss:0.0343\n",
      "Episode:880 meanR:0.3893 R:0.0000 gloss:-0.1370 dloss:0.0217\n",
      "Episode:881 meanR:0.3845 R:0.0000 gloss:0.0520 dloss:0.0410\n",
      "Episode:882 meanR:0.3767 R:0.0000 gloss:0.0337 dloss:0.0328\n",
      "Episode:883 meanR:0.3728 R:0.1100 gloss:-0.0689 dloss:0.0308\n",
      "Episode:884 meanR:0.3672 R:0.0000 gloss:-0.0803 dloss:0.0493\n",
      "Episode:885 meanR:0.3576 R:0.0000 gloss:-0.0555 dloss:0.0280\n",
      "Episode:886 meanR:0.3527 R:0.0000 gloss:-0.0539 dloss:0.0496\n",
      "Episode:887 meanR:0.3473 R:0.0000 gloss:-0.1039 dloss:0.0338\n",
      "Episode:888 meanR:0.3431 R:0.1100 gloss:-0.0253 dloss:0.0416\n",
      "Episode:889 meanR:0.3392 R:0.0000 gloss:-0.1510 dloss:0.0553\n",
      "Episode:890 meanR:0.3368 R:0.1100 gloss:0.0041 dloss:0.0496\n",
      "Episode:891 meanR:0.3250 R:0.1000 gloss:-0.0042 dloss:0.0685\n",
      "Episode:892 meanR:0.3130 R:0.0000 gloss:-0.1555 dloss:0.0480\n",
      "Episode:893 meanR:0.3001 R:0.0000 gloss:-0.0617 dloss:0.0880\n",
      "Episode:894 meanR:0.2798 R:0.0000 gloss:-0.0393 dloss:0.0915\n",
      "Episode:895 meanR:0.2655 R:0.0000 gloss:0.1304 dloss:0.0719\n",
      "Episode:896 meanR:0.2519 R:0.0000 gloss:-0.0270 dloss:0.0918\n",
      "Episode:897 meanR:0.2426 R:0.0000 gloss:-0.1243 dloss:0.1282\n",
      "Episode:898 meanR:0.2188 R:0.0000 gloss:-0.0599 dloss:0.1722\n",
      "Episode:899 meanR:0.2172 R:0.0000 gloss:0.0061 dloss:0.1642\n",
      "Episode:900 meanR:0.1998 R:0.1100 gloss:-0.1110 dloss:0.1607\n",
      "Episode:901 meanR:0.1920 R:0.0000 gloss:-0.0350 dloss:0.2789\n",
      "Episode:902 meanR:0.1861 R:0.0000 gloss:0.2771 dloss:0.1948\n",
      "Episode:903 meanR:0.1691 R:0.0000 gloss:-0.2799 dloss:0.2789\n",
      "Episode:904 meanR:0.1641 R:0.0000 gloss:-0.2871 dloss:0.2535\n",
      "Episode:905 meanR:0.1573 R:0.0000 gloss:-0.0219 dloss:0.1965\n",
      "Episode:906 meanR:0.1541 R:0.0000 gloss:0.1884 dloss:0.0822\n",
      "Episode:907 meanR:0.1518 R:0.0000 gloss:-0.5287 dloss:0.1502\n",
      "Episode:908 meanR:0.1506 R:0.0000 gloss:0.0804 dloss:0.1365\n",
      "Episode:909 meanR:0.1423 R:0.0000 gloss:-0.4097 dloss:0.0417\n",
      "Episode:910 meanR:0.1376 R:0.0000 gloss:-0.2090 dloss:0.0412\n",
      "Episode:911 meanR:0.1347 R:0.0000 gloss:-0.1186 dloss:0.0516\n",
      "Episode:912 meanR:0.1325 R:0.0900 gloss:-0.1755 dloss:0.0128\n",
      "Episode:913 meanR:0.1285 R:0.0000 gloss:-0.0684 dloss:0.0187\n",
      "Episode:914 meanR:0.1270 R:0.0000 gloss:0.0073 dloss:0.0253\n",
      "Episode:915 meanR:0.1260 R:0.0000 gloss:-0.0233 dloss:0.0323\n",
      "Episode:916 meanR:0.1250 R:0.0000 gloss:-0.1879 dloss:0.0442\n",
      "Episode:917 meanR:0.1199 R:0.0000 gloss:0.1709 dloss:0.0389\n",
      "Episode:918 meanR:0.1135 R:0.0000 gloss:-0.4680 dloss:0.0480\n",
      "Episode:919 meanR:0.1135 R:0.0000 gloss:0.1413 dloss:0.1035\n",
      "Episode:920 meanR:0.1071 R:0.0000 gloss:-0.1264 dloss:0.0278\n",
      "Episode:921 meanR:0.0963 R:0.0000 gloss:-0.4943 dloss:0.0340\n",
      "Episode:922 meanR:0.0928 R:0.0000 gloss:0.0784 dloss:0.0203\n",
      "Episode:923 meanR:0.0906 R:0.0000 gloss:-0.2930 dloss:0.0472\n",
      "Episode:924 meanR:0.0784 R:0.0000 gloss:-0.1406 dloss:0.0187\n",
      "Episode:925 meanR:0.0661 R:0.0000 gloss:-0.0415 dloss:0.0005\n",
      "Episode:926 meanR:0.0661 R:0.0000 gloss:-0.0584 dloss:0.0006\n",
      "Episode:927 meanR:0.0661 R:0.0000 gloss:-0.0850 dloss:0.0056\n",
      "Episode:928 meanR:0.0661 R:0.0000 gloss:-0.0058 dloss:0.0000\n",
      "Episode:929 meanR:0.0602 R:0.0000 gloss:0.0106 dloss:0.0000\n",
      "Episode:930 meanR:0.0562 R:0.0000 gloss:-0.1585 dloss:0.0219\n",
      "Episode:931 meanR:0.0401 R:0.1000 gloss:-0.2615 dloss:0.0349\n",
      "Episode:932 meanR:0.0159 R:0.0000 gloss:0.1894 dloss:0.0565\n",
      "Episode:933 meanR:0.0129 R:0.0000 gloss:-3.2748 dloss:0.2714\n",
      "Episode:934 meanR:0.0129 R:0.0000 gloss:6.4301 dloss:0.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:935 meanR:0.0129 R:0.0000 gloss:-3.0604 dloss:0.1594\n",
      "Episode:936 meanR:0.0129 R:0.0000 gloss:1.8802 dloss:0.1216\n",
      "Episode:937 meanR:0.0129 R:0.0000 gloss:-2.4775 dloss:0.1293\n",
      "Episode:938 meanR:0.0129 R:0.0000 gloss:0.3453 dloss:0.2158\n",
      "Episode:939 meanR:0.0129 R:0.0000 gloss:1.3682 dloss:0.0496\n",
      "Episode:940 meanR:0.0129 R:0.0000 gloss:-0.2366 dloss:0.0017\n",
      "Episode:941 meanR:0.0129 R:0.0000 gloss:-0.0144 dloss:0.0006\n",
      "Episode:942 meanR:0.0129 R:0.0000 gloss:-0.0205 dloss:0.0041\n",
      "Episode:943 meanR:0.0129 R:0.0000 gloss:-0.2187 dloss:0.0119\n",
      "Episode:944 meanR:0.0129 R:0.0000 gloss:0.1065 dloss:0.0030\n",
      "Episode:945 meanR:0.0128 R:0.0000 gloss:0.0467 dloss:0.0165\n",
      "Episode:946 meanR:0.0138 R:0.1000 gloss:-0.1271 dloss:0.0048\n",
      "Episode:947 meanR:0.0127 R:0.0000 gloss:-0.0809 dloss:0.0224\n",
      "Episode:948 meanR:0.0127 R:0.0000 gloss:-0.2727 dloss:0.0131\n",
      "Episode:949 meanR:0.0127 R:0.0000 gloss:-0.1870 dloss:0.0875\n",
      "Episode:950 meanR:0.0127 R:0.0000 gloss:-0.2503 dloss:0.0264\n",
      "Episode:951 meanR:0.0116 R:0.0000 gloss:-0.0671 dloss:0.0066\n",
      "Episode:952 meanR:0.0116 R:0.0000 gloss:-0.2997 dloss:0.0440\n",
      "Episode:953 meanR:0.0116 R:0.0000 gloss:0.1683 dloss:0.0215\n",
      "Episode:954 meanR:0.0116 R:0.0000 gloss:-0.2628 dloss:0.0114\n",
      "Episode:955 meanR:0.0116 R:0.0000 gloss:-0.1096 dloss:0.0127\n",
      "Episode:956 meanR:0.0108 R:0.0000 gloss:0.2517 dloss:0.1013\n",
      "Episode:957 meanR:0.0108 R:0.0000 gloss:0.2028 dloss:0.0163\n",
      "Episode:958 meanR:0.0108 R:0.0000 gloss:-0.2636 dloss:0.0041\n",
      "Episode:959 meanR:0.0108 R:0.0000 gloss:-0.0748 dloss:0.0008\n",
      "Episode:960 meanR:0.0108 R:0.0000 gloss:-1.9676 dloss:0.0923\n",
      "Episode:961 meanR:0.0097 R:0.0000 gloss:2.3591 dloss:0.1658\n",
      "Episode:962 meanR:0.0105 R:0.0800 gloss:-1.5049 dloss:0.0416\n",
      "Episode:963 meanR:0.0101 R:0.0000 gloss:0.1621 dloss:0.0025\n",
      "Episode:964 meanR:0.0101 R:0.0000 gloss:0.2189 dloss:0.0166\n",
      "Episode:965 meanR:0.0091 R:0.0000 gloss:-0.5552 dloss:0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception calling application: Ran out of input\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aras/anaconda3/lib/python3.6/site-packages/grpc/_server.py\", line 385, in _call_behavior\n",
      "    return behavior(argument, context), True\n",
      "  File \"/home/aras/anaconda3/lib/python3.6/site-packages/unityagents/rpc_communicator.py\", line 26, in Exchange\n",
      "    return self.child_conn.recv()\n",
      "  File \"/home/aras/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "EOFError: Ran out of input\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ReacherBrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-83deab855eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#next_state, reward, done, _ = env.step(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to tne environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m         \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                         \u001b[0;31m# get reward (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_brain_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ReacherBrain'"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, gloss_list, dloss_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        num_step = 0\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_preds = sess.run(model.actions_preds, feed_dict={model.states: state.reshape([1, -1]), \n",
    "                                                                    model.training: False})\n",
    "            noise = np.random.normal(loc=0, scale=0.1, size=action_size) # randomness\n",
    "            action = action_preds + noise\n",
    "            #print(action.shape, action_logits.shape, noise.shape)\n",
    "            action = np.clip(action, -1, 1) # clipped\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            rate = -1 # success rate: -1 to +1\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "            num_step += 1 # memory updated\n",
    "            total_reward += reward # max reward 30\n",
    "            state = next_state\n",
    "            \n",
    "            if done is True:\n",
    "                # Best 100-episode average reward was +30\n",
    "                # Env is considered \"solved\" \n",
    "                #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "                rate = total_reward/30\n",
    "                rate = np.clip(rate, -1, 1)\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1:\n",
    "                        memory.buffer[-1-idx][-1] = rate\n",
    "                        \n",
    "            # Training\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states, \n",
    "                                                                   model.training: False})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # discrete DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # continuous DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dloss, _, _ = sess.run([model.g_loss, model.d_loss, model.g_opt, model.d_opt],\n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.targetQs: targetQs, \n",
    "                                                       model.rates: rates, \n",
    "                                                       model.training: False})\n",
    "            gloss_batch.append(gloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        # Best 100-episode average reward was +30\n",
    "        # Env is considered \"solved\" \n",
    "        #  when the agent obtains an average reward of at least 300 over 100 consecutive episodes.)        \n",
    "        if np.mean(episode_reward) >= +30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
