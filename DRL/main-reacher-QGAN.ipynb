{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the train mode\n",
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "# state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# #scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# num_steps = 0\n",
    "# while True:\n",
    "#     num_steps += 1\n",
    "#     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     #print(action)\n",
    "#     action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "#     #print(action)\n",
    "#     env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "#     next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "#     reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "#     done = env_info.local_done[0]                        # see if episode finished\n",
    "#     #scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     state = next_state                               # roll over states to next time step\n",
    "#     if done is True:                                  # exit loop if episode finished\n",
    "#         #print(action.shape, reward)\n",
    "#         #print(done)\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "# num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float32, [None, action_size], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Act', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Env', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        states_logits = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)\n",
    "        Qlogits = tf.layers.dense(inputs=nl2, units=1, trainable=False)\n",
    "        return states_logits, Qlogits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    ################################################ a = act(s)\n",
    "    actions_logits = Act(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    #actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    actions_labels = tf.nn.sigmoid(actions)\n",
    "    # aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "    #                                                                   labels=actions_labels))\n",
    "    aloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=actions_logits, \n",
    "                                                                   labels=actions_labels))\n",
    "    #aloss = tf.reduce_mean(tf.square(actions_logits - actions))\n",
    "    ################################################ s', r = env(s, a)\n",
    "    e_next_states_logits, eQs = Env(actions=actions, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size)\n",
    "    a_next_states_logits, aQs = Env(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    eloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=e_next_states_logits, # GQN\n",
    "                                                                   labels=next_states_labels))\n",
    "    eloss += -tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                     labels=next_states_labels)) # maximize loss\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                    labels=next_states_labels)) # minimize loss\n",
    "    eQs_logits = tf.reshape(eQs, shape=[-1])\n",
    "    aQs_logits = tf.reshape(aQs, shape=[-1])\n",
    "    #################################################### s'', Q' = ~env(s', ~a')\n",
    "    next_actions_logits = Act(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    next_states_logits, aQs2 = Env(actions=next_actions_logits, states=next_states, hidden_size=hidden_size, \n",
    "                                   action_size=action_size, state_size=state_size, reuse=True)\n",
    "    aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=eQs_logits, # GAN\n",
    "                                                                    labels=rates)) # 0-1 real\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                    labels=tf.zeros_like(rates))) # min\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                     labels=tf.ones_like(rates))) # max\n",
    "    ###################################################### Q(s,a)= r + Q'(s',a')\n",
    "    targetQs = rewards + (gamma * aQs2_logits) # DQN/Qlearning\n",
    "    eloss += tf.reduce_mean(tf.square(eQs_logits - targetQs)) # real\n",
    "#     eloss += tf.reduce_mean(tf.square(aQs_logits - rewards)) # min\n",
    "#     aloss2 += tf.reduce_mean(tf.square(aQs_logits - targetQs)) # max\n",
    "    eloss += tf.reduce_mean((aQs_logits+aQs2_logits)/2) # min\n",
    "    aloss2 += -tf.reduce_mean((aQs_logits+aQs2_logits)/2) # max\n",
    "#     #################################################### s'', Q' = ~env(s', ~a'): Repeatable 1\n",
    "#     next_actions_logits = Act(states=next_states_logits, hidden_size=hidden_size, action_size=action_size, \n",
    "#                               reuse=True)\n",
    "#     next_states_logits, aQs2 = Env(actions=next_actions_logits, states=next_states_logits, hidden_size=hidden_size, \n",
    "#                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "#     aQs_logits = aQs2_logits # pervious one\n",
    "#     aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "# #     eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "# #                                                                     labels=tf.zeros_like(rates))) # min\n",
    "# #     aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "# #                                                                      labels=tf.ones_like(rates))) # max\n",
    "#     ###################################################### Q(s,a)= r + Q'(s',a')\n",
    "#     targetQs = rewards + (gamma * aQs2_logits) # DQN/Qlearning\n",
    "#     eloss += tf.reduce_mean(tf.square(aQs_logits - rewards)) # min\n",
    "#     aloss2 += tf.reduce_mean(tf.square(aQs_logits - targetQs)) # max\n",
    "#     #################################################### s'', Q' = ~env(s', ~a'): Repeatable 2\n",
    "#     next_actions_logits = Act(states=next_states_logits, hidden_size=hidden_size, action_size=action_size, \n",
    "#                               reuse=True)\n",
    "#     next_states_logits, aQs2 = Env(actions=next_actions_logits, states=next_states_logits, hidden_size=hidden_size, \n",
    "#                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "#     aQs_logits = aQs2_logits # pervious one\n",
    "#     aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "# #     eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "# #                                                                     labels=tf.zeros_like(rates))) # min\n",
    "# #     aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "# #                                                                      labels=tf.ones_like(rates))) # max\n",
    "#     ###################################################### Q(s,a)= r + Q'(s',a')\n",
    "#     targetQs = rewards + (gamma * aQs2_logits) # DQN/Qlearning\n",
    "#     eloss += tf.reduce_mean(tf.square(aQs_logits - rewards)) # min\n",
    "#     aloss2 += tf.reduce_mean(tf.square(aQs_logits - targetQs)) # max\n",
    "    return actions_logits, aloss, eloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, e_loss, a_loss2, a_learning_rate, e_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('Act')]\n",
    "    e_vars = [var for var in t_vars if var.name.startswith('Env')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        e_opt = tf.train.AdamOptimizer(e_learning_rate).minimize(e_loss, var_list=e_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, e_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, e_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size, action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.e_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.e_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        e_loss=self.e_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        e_learning_rate=e_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "#                                size=batch_size, \n",
    "#                                replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 33), (1, 4), 4, 0, 4, 33)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info.vector_observations.shape, env_info.previous_vector_actions.shape, \\\n",
    "brain.vector_action_space_size, brain.number_visual_observations, \\\n",
    "brain.vector_action_space_size, brain.vector_observation_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "e_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(2e3)             # experience mini-batch size\n",
    "gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              a_learning_rate=a_learning_rate, e_learning_rate=e_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.01 total_reward: 0.23999999463558197 num_step: 1001\n",
      "Progress: 0.02001 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.03002 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.04003 total_reward: 0.17999999597668648 num_step: 1001\n",
      "Progress: 0.05004 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.06005 total_reward: 0.5399999879300594 num_step: 1001\n",
      "Progress: 0.07006 total_reward: 0.13999999687075615 num_step: 1001\n",
      "Progress: 0.08007 total_reward: 0.30999999307096004 num_step: 1001\n",
      "Progress: 0.09008 total_reward: 0.3499999921768904 num_step: 1001\n",
      "Progress: 0.10009 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.1101 total_reward: 0.35999999195337296 num_step: 1001\n",
      "Progress: 0.12011 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.13012 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.14013 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.15014 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.16015 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.17016 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.18017 total_reward: 0.04999999888241291 num_step: 1001\n",
      "Progress: 0.19018 total_reward: 0.12999999709427357 num_step: 1001\n",
      "Progress: 0.20019 total_reward: 0.42999999038875103 num_step: 1001\n",
      "Progress: 0.2102 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.22021 total_reward: 0.23999999463558197 num_step: 1001\n",
      "Progress: 0.23022 total_reward: 0.3299999926239252 num_step: 1001\n",
      "Progress: 0.24023 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.25024 total_reward: 0.5299999881535769 num_step: 1001\n",
      "Progress: 0.26025 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.27026 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.28027 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.29028 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.30029 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.3103 total_reward: 0.5399999879300594 num_step: 1001\n",
      "Progress: 0.32031 total_reward: 0.3199999928474426 num_step: 1001\n",
      "Progress: 0.33032 total_reward: 0.4999999888241291 num_step: 1001\n",
      "Progress: 0.34033 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.35034 total_reward: 0.3299999926239252 num_step: 1001\n",
      "Progress: 0.36035 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.37036 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.38037 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.39038 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.40039 total_reward: 0.2799999937415123 num_step: 1001\n",
      "Progress: 0.4104 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.42041 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.43042 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.44043 total_reward: 0.17999999597668648 num_step: 1001\n",
      "Progress: 0.45044 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.46045 total_reward: 0.03999999910593033 num_step: 1001\n",
      "Progress: 0.47046 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.48047 total_reward: 0.2199999950826168 num_step: 1001\n",
      "Progress: 0.49048 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.50049 total_reward: 0.29999999329447746 num_step: 1001\n",
      "Progress: 0.5105 total_reward: 0.2299999948590994 num_step: 1001\n",
      "Progress: 0.52051 total_reward: 0.669999985024333 num_step: 1001\n",
      "Progress: 0.53052 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.54053 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.55054 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.56055 total_reward: 0.1699999962002039 num_step: 1001\n",
      "Progress: 0.57056 total_reward: 0.35999999195337296 num_step: 1001\n",
      "Progress: 0.58057 total_reward: 0.20999999530613422 num_step: 1001\n",
      "Progress: 0.59058 total_reward: 0.6799999848008156 num_step: 1001\n",
      "Progress: 0.60059 total_reward: 0.1699999962002039 num_step: 1001\n",
      "Progress: 0.6106 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.62061 total_reward: 0.11999999731779099 num_step: 1001\n",
      "Progress: 0.63062 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.64063 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.65064 total_reward: 0.1599999964237213 num_step: 1001\n",
      "Progress: 0.66065 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.67066 total_reward: 0.06999999843537807 num_step: 1001\n",
      "Progress: 0.68067 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.69068 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.70069 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.7107 total_reward: 0.2199999950826168 num_step: 1001\n",
      "Progress: 0.72071 total_reward: 0.4899999890476465 num_step: 1001\n",
      "Progress: 0.73072 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.74073 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.75074 total_reward: 1.1199999749660492 num_step: 1001\n",
      "Progress: 0.76075 total_reward: 0.41999999061226845 num_step: 1001\n",
      "Progress: 0.77076 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.78077 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.79078 total_reward: 0.18999999575316906 num_step: 1001\n",
      "Progress: 0.80079 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.8108 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.82081 total_reward: 0.14999999664723873 num_step: 1001\n",
      "Progress: 0.83082 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.84083 total_reward: 0.8399999812245369 num_step: 1001\n",
      "Progress: 0.85084 total_reward: 0.3899999912828207 num_step: 1001\n",
      "Progress: 0.86085 total_reward: 0.19999999552965164 num_step: 1001\n",
      "Progress: 0.87086 total_reward: 0.2699999939650297 num_step: 1001\n",
      "Progress: 0.88087 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.89088 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.90089 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.9109 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.92091 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.93092 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.94093 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.95094 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.96095 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.97096 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.98097 total_reward: 0.0 num_step: 1001\n",
      "Progress: 0.99098 total_reward: 0.0 num_step: 1001\n"
     ]
    }
   ],
   "source": [
    "# Initializing the memory buffer\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "rate = -1 # success rate: [0, +1]\n",
    "for each_step in range(memory_size):\n",
    "    action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    action = np.clip(action, -1, 1) # [-1, +1]\n",
    "    env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "    next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "    reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "    done = env_info.local_done[0]                        # see if episode finished\n",
    "    memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory updated\n",
    "    total_reward += reward # max reward 30\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        print('Progress:', each_step/memory_size, 'total_reward:', total_reward, 'num_step:', num_step)\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        rate = np.clip(total_reward/30, 0, 1) # [0, +1]\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset\n",
    "        rate = -1 # success rate: [0, +1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.2500 R:0.2500 rate:0.0083 aloss:0.7326 eloss:-1.4825 aloss2:5.5621 exploreP:0.9057\n",
      "Episode:1 meanR:0.1250 R:0.0000 rate:0.0000 aloss:0.7086 eloss:-13.3797 aloss2:33.8662 exploreP:0.8204\n",
      "Episode:2 meanR:0.0833 R:0.0000 rate:0.0000 aloss:0.7011 eloss:-34.9512 aloss2:93.5571 exploreP:0.7432\n",
      "Episode:3 meanR:0.0625 R:0.0000 rate:0.0000 aloss:0.6996 eloss:-43.7616 aloss2:131.6449 exploreP:0.6734\n",
      "Episode:4 meanR:0.0500 R:0.0000 rate:0.0000 aloss:0.6976 eloss:-52.0119 aloss2:157.6904 exploreP:0.6102\n",
      "Episode:5 meanR:0.0417 R:0.0000 rate:0.0000 aloss:0.6980 eloss:-57.3175 aloss2:177.9844 exploreP:0.5530\n",
      "Episode:6 meanR:0.0357 R:0.0000 rate:0.0000 aloss:0.7012 eloss:-51.1244 aloss2:191.5240 exploreP:0.5013\n",
      "Episode:7 meanR:0.0512 R:0.1600 rate:0.0053 aloss:0.6996 eloss:-61.4382 aloss2:197.4263 exploreP:0.4545\n",
      "Episode:8 meanR:0.0889 R:0.3900 rate:0.0130 aloss:0.7005 eloss:-58.7482 aloss2:211.4966 exploreP:0.4121\n",
      "Episode:9 meanR:0.1070 R:0.2700 rate:0.0090 aloss:0.7018 eloss:-65.6728 aloss2:219.4412 exploreP:0.3738\n",
      "Episode:10 meanR:0.1482 R:0.5600 rate:0.0187 aloss:0.7010 eloss:-59.3672 aloss2:230.6204 exploreP:0.3392\n",
      "Episode:11 meanR:0.1358 R:0.0000 rate:0.0000 aloss:0.6998 eloss:-64.5279 aloss2:229.5581 exploreP:0.3078\n",
      "Episode:12 meanR:0.1385 R:0.1700 rate:0.0057 aloss:0.7015 eloss:-68.2230 aloss2:244.5511 exploreP:0.2795\n",
      "Episode:13 meanR:0.1393 R:0.1500 rate:0.0050 aloss:0.6995 eloss:-76.2262 aloss2:251.5123 exploreP:0.2538\n",
      "Episode:14 meanR:0.1593 R:0.4400 rate:0.0147 aloss:0.6976 eloss:-67.0931 aloss2:264.2027 exploreP:0.2306\n",
      "Episode:15 meanR:0.2075 R:0.9300 rate:0.0310 aloss:0.6978 eloss:-67.8950 aloss2:261.9399 exploreP:0.2096\n",
      "Episode:16 meanR:0.2100 R:0.2500 rate:0.0083 aloss:0.7005 eloss:-58.8566 aloss2:263.1501 exploreP:0.1905\n",
      "Episode:17 meanR:0.2167 R:0.3300 rate:0.0110 aloss:0.7006 eloss:-58.4249 aloss2:256.4129 exploreP:0.1734\n",
      "Episode:18 meanR:0.2053 R:0.0000 rate:0.0000 aloss:0.7023 eloss:-52.1939 aloss2:254.8801 exploreP:0.1578\n",
      "Episode:19 meanR:0.2315 R:0.7300 rate:0.0243 aloss:0.7037 eloss:-56.6130 aloss2:254.0853 exploreP:0.1437\n",
      "Episode:20 meanR:0.2271 R:0.1400 rate:0.0047 aloss:0.7043 eloss:-57.3639 aloss2:249.4813 exploreP:0.1310\n",
      "Episode:21 meanR:0.2423 R:0.5600 rate:0.0187 aloss:0.7053 eloss:-59.9343 aloss2:246.6206 exploreP:0.1195\n",
      "Episode:22 meanR:0.2639 R:0.7400 rate:0.0247 aloss:0.7058 eloss:-55.8291 aloss2:247.0406 exploreP:0.1090\n",
      "Episode:23 meanR:0.2621 R:0.2200 rate:0.0073 aloss:0.7042 eloss:-67.4847 aloss2:245.9183 exploreP:0.0996\n",
      "Episode:24 meanR:0.3160 R:1.6100 rate:0.0537 aloss:0.7075 eloss:-54.0864 aloss2:248.6974 exploreP:0.0911\n",
      "Episode:25 meanR:0.3419 R:0.9900 rate:0.0330 aloss:0.7052 eloss:-65.9515 aloss2:240.8825 exploreP:0.0833\n",
      "Episode:26 meanR:0.3548 R:0.6900 rate:0.0230 aloss:0.7068 eloss:-69.1224 aloss2:258.3280 exploreP:0.0764\n",
      "Episode:27 meanR:0.3632 R:0.5900 rate:0.0197 aloss:0.7037 eloss:-64.8934 aloss2:247.9953 exploreP:0.0700\n",
      "Episode:28 meanR:0.3662 R:0.4500 rate:0.0150 aloss:0.7088 eloss:-44.4906 aloss2:246.8870 exploreP:0.0643\n",
      "Episode:29 meanR:0.3613 R:0.2200 rate:0.0073 aloss:0.7052 eloss:-63.4158 aloss2:242.7712 exploreP:0.0591\n",
      "Episode:30 meanR:0.3597 R:0.3100 rate:0.0103 aloss:0.7089 eloss:-48.5094 aloss2:246.7070 exploreP:0.0545\n",
      "Episode:31 meanR:0.3622 R:0.4400 rate:0.0147 aloss:0.7060 eloss:-57.3540 aloss2:233.9058 exploreP:0.0502\n",
      "Episode:32 meanR:0.3879 R:1.2100 rate:0.0403 aloss:0.7109 eloss:-59.7820 aloss2:236.1991 exploreP:0.0464\n",
      "Episode:33 meanR:0.3944 R:0.6100 rate:0.0203 aloss:0.7050 eloss:-54.0158 aloss2:233.8724 exploreP:0.0429\n",
      "Episode:34 meanR:0.3929 R:0.3400 rate:0.0113 aloss:0.7092 eloss:-55.8254 aloss2:233.8627 exploreP:0.0398\n",
      "Episode:35 meanR:0.3819 R:0.0000 rate:0.0000 aloss:0.7085 eloss:-55.1890 aloss2:225.6868 exploreP:0.0370\n",
      "Episode:36 meanR:0.3951 R:0.8700 rate:0.0290 aloss:0.7182 eloss:-53.2378 aloss2:228.2785 exploreP:0.0344\n",
      "Episode:37 meanR:0.4032 R:0.7000 rate:0.0233 aloss:0.7167 eloss:-46.7186 aloss2:219.0526 exploreP:0.0321\n",
      "Episode:38 meanR:0.4018 R:0.3500 rate:0.0117 aloss:0.7203 eloss:-47.3607 aloss2:217.4486 exploreP:0.0300\n",
      "Episode:39 meanR:0.3980 R:0.2500 rate:0.0083 aloss:0.7225 eloss:-50.7975 aloss2:211.6500 exploreP:0.0281\n",
      "Episode:40 meanR:0.3993 R:0.4500 rate:0.0150 aloss:0.7292 eloss:-58.1475 aloss2:217.5076 exploreP:0.0263\n",
      "Episode:41 meanR:0.3969 R:0.3000 rate:0.0100 aloss:0.7357 eloss:-54.5839 aloss2:211.6528 exploreP:0.0248\n",
      "Episode:42 meanR:0.4033 R:0.6700 rate:0.0223 aloss:0.7317 eloss:-64.1577 aloss2:227.4637 exploreP:0.0234\n",
      "Episode:43 meanR:0.4057 R:0.5100 rate:0.0170 aloss:0.7383 eloss:-55.6045 aloss2:217.0091 exploreP:0.0221\n",
      "Episode:44 meanR:0.4109 R:0.6400 rate:0.0213 aloss:0.7389 eloss:-58.4015 aloss2:231.4978 exploreP:0.0209\n",
      "Episode:45 meanR:0.4185 R:0.7600 rate:0.0253 aloss:0.7634 eloss:-53.1311 aloss2:213.9054 exploreP:0.0199\n",
      "Episode:46 meanR:0.4349 R:1.1900 rate:0.0397 aloss:0.7729 eloss:-51.7655 aloss2:225.1914 exploreP:0.0190\n",
      "Episode:47 meanR:0.4402 R:0.6900 rate:0.0230 aloss:0.7710 eloss:-64.0162 aloss2:217.6286 exploreP:0.0181\n",
      "Episode:48 meanR:0.4612 R:1.4700 rate:0.0490 aloss:0.7579 eloss:-57.6582 aloss2:232.1383 exploreP:0.0173\n",
      "Episode:49 meanR:0.4608 R:0.4400 rate:0.0147 aloss:0.8060 eloss:-64.2865 aloss2:225.3902 exploreP:0.0166\n",
      "Episode:50 meanR:0.4765 R:1.2600 rate:0.0420 aloss:0.8030 eloss:-41.1080 aloss2:232.8238 exploreP:0.0160\n",
      "Episode:51 meanR:0.4850 R:0.9200 rate:0.0307 aloss:0.8084 eloss:-63.4318 aloss2:224.0723 exploreP:0.0154\n",
      "Episode:52 meanR:0.4945 R:0.9900 rate:0.0330 aloss:0.8101 eloss:-47.4541 aloss2:231.9336 exploreP:0.0149\n",
      "Episode:53 meanR:0.4972 R:0.6400 rate:0.0213 aloss:0.8290 eloss:-56.8117 aloss2:218.9947 exploreP:0.0144\n",
      "Episode:54 meanR:0.4991 R:0.6000 rate:0.0200 aloss:0.8072 eloss:-46.0676 aloss2:228.2661 exploreP:0.0140\n",
      "Episode:55 meanR:0.5002 R:0.5600 rate:0.0187 aloss:0.8534 eloss:-59.4898 aloss2:216.4261 exploreP:0.0136\n",
      "Episode:56 meanR:0.4937 R:0.1300 rate:0.0043 aloss:0.8166 eloss:-42.5180 aloss2:223.8030 exploreP:0.0133\n",
      "Episode:57 meanR:0.4860 R:0.0500 rate:0.0017 aloss:0.8567 eloss:-67.3052 aloss2:210.0807 exploreP:0.0130\n",
      "Episode:58 meanR:0.4942 R:0.9700 rate:0.0323 aloss:0.8499 eloss:-45.2445 aloss2:224.2408 exploreP:0.0127\n",
      "Episode:59 meanR:0.4997 R:0.8200 rate:0.0273 aloss:0.8612 eloss:-53.5660 aloss2:207.1374 exploreP:0.0124\n",
      "Episode:60 meanR:0.4936 R:0.1300 rate:0.0043 aloss:0.8771 eloss:-50.9333 aloss2:221.5372 exploreP:0.0122\n",
      "Episode:61 meanR:0.4863 R:0.0400 rate:0.0013 aloss:0.8561 eloss:-57.8276 aloss2:206.7820 exploreP:0.0120\n",
      "Episode:62 meanR:0.4859 R:0.4600 rate:0.0153 aloss:0.8212 eloss:-45.5624 aloss2:222.0090 exploreP:0.0118\n",
      "Episode:63 meanR:0.4847 R:0.4100 rate:0.0137 aloss:0.9197 eloss:-69.5297 aloss2:208.0983 exploreP:0.0116\n",
      "Episode:64 meanR:0.4998 R:1.4700 rate:0.0490 aloss:0.7979 eloss:-41.6316 aloss2:224.3666 exploreP:0.0115\n",
      "Episode:65 meanR:0.4961 R:0.2500 rate:0.0083 aloss:0.8196 eloss:-59.9142 aloss2:205.3728 exploreP:0.0113\n",
      "Episode:66 meanR:0.5025 R:0.9300 rate:0.0310 aloss:0.8083 eloss:-48.4238 aloss2:218.7516 exploreP:0.0112\n",
      "Episode:67 meanR:0.5035 R:0.5700 rate:0.0190 aloss:0.8694 eloss:-66.7648 aloss2:208.9614 exploreP:0.0111\n",
      "Episode:68 meanR:0.5170 R:1.4300 rate:0.0477 aloss:0.8439 eloss:-42.2268 aloss2:222.4686 exploreP:0.0110\n",
      "Episode:69 meanR:0.5203 R:0.7500 rate:0.0250 aloss:0.8504 eloss:-51.8004 aloss2:204.7851 exploreP:0.0109\n",
      "Episode:70 meanR:0.5373 R:1.7300 rate:0.0577 aloss:0.7893 eloss:-48.0545 aloss2:217.3438 exploreP:0.0108\n",
      "Episode:71 meanR:0.5347 R:0.3500 rate:0.0117 aloss:0.7940 eloss:-65.8683 aloss2:209.3535 exploreP:0.0107\n",
      "Episode:72 meanR:0.5351 R:0.5600 rate:0.0187 aloss:0.8495 eloss:-52.9792 aloss2:224.9442 exploreP:0.0107\n",
      "Episode:73 meanR:0.5351 R:0.5400 rate:0.0180 aloss:0.8071 eloss:-65.9818 aloss2:212.2904 exploreP:0.0106\n",
      "Episode:74 meanR:0.5321 R:0.3100 rate:0.0103 aloss:0.8299 eloss:-51.4751 aloss2:232.1730 exploreP:0.0105\n",
      "Episode:75 meanR:0.5346 R:0.7200 rate:0.0240 aloss:0.7971 eloss:-74.0386 aloss2:221.2825 exploreP:0.0105\n",
      "Episode:76 meanR:0.5432 R:1.2000 rate:0.0400 aloss:0.8688 eloss:-42.7039 aloss2:235.0335 exploreP:0.0104\n",
      "Episode:77 meanR:0.5456 R:0.7300 rate:0.0243 aloss:0.7938 eloss:-72.9718 aloss2:222.1488 exploreP:0.0104\n",
      "Episode:78 meanR:0.5504 R:0.9200 rate:0.0307 aloss:0.7871 eloss:-52.1198 aloss2:238.1969 exploreP:0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:0.5444 R:0.0700 rate:0.0023 aloss:0.7891 eloss:-78.3455 aloss2:226.5971 exploreP:0.0103\n",
      "Episode:80 meanR:0.5663 R:2.3200 rate:0.0773 aloss:0.8695 eloss:-30.7979 aloss2:251.2008 exploreP:0.0103\n",
      "Episode:81 meanR:0.5662 R:0.5600 rate:0.0187 aloss:0.7928 eloss:-73.4389 aloss2:226.3798 exploreP:0.0103\n",
      "Episode:82 meanR:0.5630 R:0.3000 rate:0.0100 aloss:0.7833 eloss:-49.4618 aloss2:240.5604 exploreP:0.0102\n",
      "Episode:83 meanR:0.5575 R:0.1000 rate:0.0033 aloss:0.7922 eloss:-73.7392 aloss2:229.1475 exploreP:0.0102\n",
      "Episode:84 meanR:0.5591 R:0.6900 rate:0.0230 aloss:0.8001 eloss:-41.8893 aloss2:241.6022 exploreP:0.0102\n",
      "Episode:85 meanR:0.5558 R:0.2800 rate:0.0093 aloss:0.8239 eloss:-75.7870 aloss2:226.1907 exploreP:0.0102\n",
      "Episode:86 meanR:0.5601 R:0.9300 rate:0.0310 aloss:0.8258 eloss:-42.5881 aloss2:237.8581 exploreP:0.0102\n",
      "Episode:87 meanR:0.5667 R:1.1400 rate:0.0380 aloss:0.8117 eloss:-67.0364 aloss2:225.0742 exploreP:0.0101\n",
      "Episode:88 meanR:0.5687 R:0.7400 rate:0.0247 aloss:0.8045 eloss:-49.2442 aloss2:233.6282 exploreP:0.0101\n",
      "Episode:89 meanR:0.5687 R:0.5700 rate:0.0190 aloss:0.8156 eloss:-69.9544 aloss2:227.5087 exploreP:0.0101\n",
      "Episode:90 meanR:0.5756 R:1.2000 rate:0.0400 aloss:0.8187 eloss:-47.7203 aloss2:238.4301 exploreP:0.0101\n",
      "Episode:91 meanR:0.5779 R:0.7900 rate:0.0263 aloss:0.8306 eloss:-77.0141 aloss2:230.8613 exploreP:0.0101\n",
      "Episode:92 meanR:0.5831 R:1.0600 rate:0.0353 aloss:0.8460 eloss:-44.7223 aloss2:244.9552 exploreP:0.0101\n",
      "Episode:93 meanR:0.5837 R:0.6400 rate:0.0213 aloss:0.9006 eloss:-78.8240 aloss2:235.3680 exploreP:0.0101\n",
      "Episode:94 meanR:0.5864 R:0.8400 rate:0.0280 aloss:0.8669 eloss:-37.3319 aloss2:248.7911 exploreP:0.0101\n",
      "Episode:95 meanR:0.5853 R:0.4800 rate:0.0160 aloss:0.8574 eloss:-83.3653 aloss2:236.5169 exploreP:0.0101\n",
      "Episode:96 meanR:0.5825 R:0.3100 rate:0.0103 aloss:0.8539 eloss:-48.0396 aloss2:248.9783 exploreP:0.0101\n",
      "Episode:97 meanR:0.5795 R:0.2900 rate:0.0097 aloss:0.8830 eloss:-78.4053 aloss2:240.9134 exploreP:0.0101\n",
      "Episode:98 meanR:0.5736 R:0.0000 rate:0.0000 aloss:0.9172 eloss:-46.9800 aloss2:251.5586 exploreP:0.0100\n",
      "Episode:99 meanR:0.5716 R:0.3700 rate:0.0123 aloss:0.8591 eloss:-71.3975 aloss2:242.9099 exploreP:0.0100\n",
      "Episode:100 meanR:0.5710 R:0.1900 rate:0.0063 aloss:0.8504 eloss:-44.7811 aloss2:251.4459 exploreP:0.0100\n",
      "Episode:101 meanR:0.5710 R:0.0000 rate:0.0000 aloss:0.8695 eloss:-60.7268 aloss2:240.6242 exploreP:0.0100\n",
      "Episode:102 meanR:0.5783 R:0.7300 rate:0.0243 aloss:0.8404 eloss:-50.0532 aloss2:245.6544 exploreP:0.0100\n",
      "Episode:103 meanR:0.5828 R:0.4500 rate:0.0150 aloss:0.8544 eloss:-79.4680 aloss2:241.9597 exploreP:0.0100\n",
      "Episode:104 meanR:0.5881 R:0.5300 rate:0.0177 aloss:0.8477 eloss:-51.9217 aloss2:254.2643 exploreP:0.0100\n",
      "Episode:105 meanR:0.5986 R:1.0500 rate:0.0350 aloss:0.9433 eloss:-80.3563 aloss2:249.1732 exploreP:0.0100\n",
      "Episode:106 meanR:0.5991 R:0.0500 rate:0.0017 aloss:0.8232 eloss:-46.3728 aloss2:261.1392 exploreP:0.0100\n",
      "Episode:107 meanR:0.6032 R:0.5700 rate:0.0190 aloss:0.8347 eloss:-77.9800 aloss2:251.7083 exploreP:0.0100\n",
      "Episode:108 meanR:0.6017 R:0.2400 rate:0.0080 aloss:0.8325 eloss:-38.8141 aloss2:260.4714 exploreP:0.0100\n",
      "Episode:109 meanR:0.6024 R:0.3400 rate:0.0113 aloss:0.8330 eloss:-73.1275 aloss2:250.8209 exploreP:0.0100\n",
      "Episode:110 meanR:0.6054 R:0.8600 rate:0.0287 aloss:0.8246 eloss:-35.0567 aloss2:254.4750 exploreP:0.0100\n",
      "Episode:111 meanR:0.6054 R:0.0000 rate:0.0000 aloss:0.8267 eloss:-70.9856 aloss2:245.1923 exploreP:0.0100\n",
      "Episode:112 meanR:0.6100 R:0.6300 rate:0.0210 aloss:0.8151 eloss:-27.0390 aloss2:247.6453 exploreP:0.0100\n",
      "Episode:113 meanR:0.6154 R:0.6900 rate:0.0230 aloss:0.8188 eloss:-68.0759 aloss2:237.5529 exploreP:0.0100\n",
      "Episode:114 meanR:0.6168 R:0.5800 rate:0.0193 aloss:0.9042 eloss:-53.9055 aloss2:240.2771 exploreP:0.0100\n",
      "Episode:115 meanR:0.6089 R:0.1400 rate:0.0047 aloss:0.8032 eloss:-68.3649 aloss2:239.9880 exploreP:0.0100\n",
      "Episode:116 meanR:0.6093 R:0.2900 rate:0.0097 aloss:0.7856 eloss:-46.0445 aloss2:244.3678 exploreP:0.0100\n",
      "Episode:117 meanR:0.6085 R:0.2500 rate:0.0083 aloss:0.7925 eloss:-68.2488 aloss2:238.6012 exploreP:0.0100\n",
      "Episode:118 meanR:0.6085 R:0.0000 rate:0.0000 aloss:0.7805 eloss:-45.7189 aloss2:243.6115 exploreP:0.0100\n",
      "Episode:119 meanR:0.6060 R:0.4800 rate:0.0160 aloss:0.7866 eloss:-69.6413 aloss2:238.9843 exploreP:0.0100\n",
      "Episode:120 meanR:0.6080 R:0.3400 rate:0.0113 aloss:0.7797 eloss:-49.7174 aloss2:242.0506 exploreP:0.0100\n",
      "Episode:121 meanR:0.6030 R:0.0600 rate:0.0020 aloss:0.8985 eloss:-65.7339 aloss2:238.9465 exploreP:0.0100\n",
      "Episode:122 meanR:0.6043 R:0.8700 rate:0.0290 aloss:0.7622 eloss:-54.1384 aloss2:243.2880 exploreP:0.0100\n",
      "Episode:123 meanR:0.6143 R:1.2200 rate:0.0407 aloss:0.7752 eloss:-77.8719 aloss2:241.8220 exploreP:0.0100\n",
      "Episode:124 meanR:0.6030 R:0.4800 rate:0.0160 aloss:0.7598 eloss:-44.7257 aloss2:253.8145 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, eloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        aloss_batch, eloss_batch, aloss2_batch = [], [], []\n",
    "        total_reward = 0\n",
    "        num_step = 0\n",
    "        rate = -1\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            else:\n",
    "                action = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                #action = np.argmax(action_logits)\n",
    "            action = np.clip(action, -1, 1) # [-1, +1]\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "            next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "            reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if episode finished\n",
    "            memory.buffer.append([state, action.reshape([-1]), next_state, reward, float(done), rate])\n",
    "            num_step += 1 # memory updated\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                if np.count_nonzero(dones) > 0 and len(dones) > 1 and np.max(rates) > 0:\n",
    "                    break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            eloss, _ = sess.run([model.e_loss, model.e_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print(len(dones), np.count_nonzero(dones), np.max(rates))\n",
    "            aloss_batch.append(aloss)\n",
    "            eloss_batch.append(eloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # Rating the latest played episode\n",
    "        rate = np.clip(total_reward/30, 0, 1) # [0, +1]\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "\n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'eloss:{:.4f}'.format(np.mean(eloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        eloss_list.append([ep, np.mean(eloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 30:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
