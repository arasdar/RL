{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [-0.03086462 -0.18149014 -0.04708675  0.31118283] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03449443  0.01426992 -0.0408631   0.0040298 ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03420903  0.20995336 -0.0407825  -0.30126073] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.03000996  0.01543569 -0.04680772 -0.02171357] 0 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02970125  0.21119657 -0.04724199 -0.32878972] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.02547731  0.40695811 -0.05381778 -0.63598839] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.01733815  0.60278771 -0.06653755 -0.94512228] 1 1.0 False {}\n",
      "state, action, reward, done, info: [-0.0052824   0.79873977 -0.08543999 -1.25794757] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0106924   0.99484486 -0.11059895 -1.57612099] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03058929  1.19109762 -0.14212137 -1.90115137] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN-GRU\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    # Output\n",
    "    return actions, states, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_outputs(action_size, hidden_size, states, cell, initial_state):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    return actions_logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, _ = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                  lstm_size=hidden_size, num_classes=action_size, reuse=True)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    #loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs, \n",
    "                                                                  labels=tf.nn.sigmoid(targetQs)))\n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Output of the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state = model_outputs(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_total_reward = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state:', np.array(states).shape[1], \n",
    "#       'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n",
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.03863851, -0.01005985,  0.03380489, -0.0333856 ]),\n",
       " 1,\n",
       " array([-0.03883971,  0.18456143,  0.03313718, -0.31521399]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 meanReward: 14.0000 meanLoss: 0.5954 ExploreP: 0.9986\n",
      "Episode: 1 meanReward: 18.5000 meanLoss: 0.3803 ExploreP: 0.9963\n",
      "Episode: 2 meanReward: 18.6667 meanLoss: 0.2762 ExploreP: 0.9945\n",
      "Episode: 3 meanReward: 18.7500 meanLoss: 0.2313 ExploreP: 0.9926\n",
      "Episode: 4 meanReward: 18.2000 meanLoss: 0.1183 ExploreP: 0.9910\n",
      "Episode: 5 meanReward: 17.0000 meanLoss: 0.0921 ExploreP: 0.9900\n",
      "Episode: 6 meanReward: 17.8571 meanLoss: 0.1155 ExploreP: 0.9877\n",
      "Episode: 7 meanReward: 17.8750 meanLoss: 0.0823 ExploreP: 0.9859\n",
      "Episode: 8 meanReward: 17.4444 meanLoss: 0.0989 ExploreP: 0.9846\n",
      "Episode: 9 meanReward: 17.2000 meanLoss: 0.1126 ExploreP: 0.9831\n",
      "Episode: 10 meanReward: 16.6364 meanLoss: 0.1314 ExploreP: 0.9820\n",
      "Episode: 11 meanReward: 16.5000 meanLoss: 0.1331 ExploreP: 0.9806\n",
      "Episode: 12 meanReward: 17.6154 meanLoss: 0.0990 ExploreP: 0.9776\n",
      "Episode: 13 meanReward: 18.6429 meanLoss: 0.0582 ExploreP: 0.9745\n",
      "Episode: 14 meanReward: 18.6667 meanLoss: 0.0546 ExploreP: 0.9727\n",
      "Episode: 15 meanReward: 18.5625 meanLoss: 0.0719 ExploreP: 0.9710\n",
      "Episode: 16 meanReward: 19.4118 meanLoss: 0.0502 ExploreP: 0.9679\n",
      "Episode: 17 meanReward: 19.3333 meanLoss: 0.0598 ExploreP: 0.9661\n",
      "Episode: 18 meanReward: 19.6316 meanLoss: 0.0651 ExploreP: 0.9638\n",
      "Episode: 19 meanReward: 19.2500 meanLoss: 0.0738 ExploreP: 0.9626\n",
      "Episode: 20 meanReward: 18.8095 meanLoss: 0.0947 ExploreP: 0.9617\n",
      "Episode: 21 meanReward: 18.5909 meanLoss: 0.1039 ExploreP: 0.9603\n",
      "Episode: 22 meanReward: 18.7391 meanLoss: 0.0979 ExploreP: 0.9582\n",
      "Episode: 23 meanReward: 18.7917 meanLoss: 0.0725 ExploreP: 0.9563\n",
      "Episode: 24 meanReward: 18.8000 meanLoss: 0.0756 ExploreP: 0.9545\n",
      "Episode: 25 meanReward: 20.2692 meanLoss: 0.0446 ExploreP: 0.9492\n",
      "Episode: 26 meanReward: 19.9630 meanLoss: 0.0522 ExploreP: 0.9481\n",
      "Episode: 27 meanReward: 20.3929 meanLoss: 0.0705 ExploreP: 0.9451\n",
      "Episode: 28 meanReward: 20.2069 meanLoss: 0.0551 ExploreP: 0.9437\n",
      "Episode: 29 meanReward: 20.9333 meanLoss: 0.0437 ExploreP: 0.9397\n",
      "Episode: 30 meanReward: 20.7419 meanLoss: 0.0344 ExploreP: 0.9383\n",
      "Episode: 31 meanReward: 20.4688 meanLoss: 0.0936 ExploreP: 0.9372\n",
      "Episode: 32 meanReward: 21.1562 meanLoss: 0.0665 ExploreP: 0.9339\n",
      "Episode: 33 meanReward: 21.5312 meanLoss: 0.0372 ExploreP: 0.9307\n",
      "Episode: 34 meanReward: 21.5000 meanLoss: 0.0386 ExploreP: 0.9290\n",
      "Episode: 35 meanReward: 21.4688 meanLoss: 0.0681 ExploreP: 0.9274\n",
      "Episode: 36 meanReward: 21.5000 meanLoss: 0.0790 ExploreP: 0.9258\n",
      "Episode: 37 meanReward: 21.7500 meanLoss: 0.0694 ExploreP: 0.9241\n",
      "Episode: 38 meanReward: 21.4375 meanLoss: 0.0787 ExploreP: 0.9229\n",
      "Episode: 39 meanReward: 21.2188 meanLoss: 0.0956 ExploreP: 0.9219\n",
      "Episode: 40 meanReward: 21.4375 meanLoss: 0.0864 ExploreP: 0.9200\n",
      "Episode: 41 meanReward: 21.8125 meanLoss: 0.0458 ExploreP: 0.9175\n",
      "Episode: 42 meanReward: 22.1250 meanLoss: 0.0527 ExploreP: 0.9156\n",
      "Episode: 43 meanReward: 22.6562 meanLoss: 0.0478 ExploreP: 0.9127\n",
      "Episode: 44 meanReward: 22.2812 meanLoss: 0.0570 ExploreP: 0.9110\n",
      "Episode: 45 meanReward: 21.7812 meanLoss: 0.0887 ExploreP: 0.9096\n",
      "Episode: 46 meanReward: 21.5938 meanLoss: 0.0869 ExploreP: 0.9084\n",
      "Episode: 47 meanReward: 21.7188 meanLoss: 0.0732 ExploreP: 0.9065\n",
      "Episode: 48 meanReward: 21.0625 meanLoss: 0.0848 ExploreP: 0.9054\n",
      "Episode: 49 meanReward: 21.4375 meanLoss: 0.0760 ExploreP: 0.9027\n",
      "Episode: 50 meanReward: 22.0938 meanLoss: 0.0333 ExploreP: 0.8987\n",
      "Episode: 51 meanReward: 22.2188 meanLoss: 0.0633 ExploreP: 0.8972\n",
      "Episode: 52 meanReward: 22.2188 meanLoss: 0.1139 ExploreP: 0.8963\n",
      "Episode: 53 meanReward: 22.1562 meanLoss: 0.1259 ExploreP: 0.8953\n",
      "Episode: 54 meanReward: 23.2500 meanLoss: 0.0483 ExploreP: 0.8902\n",
      "Episode: 55 meanReward: 23.5625 meanLoss: 0.0487 ExploreP: 0.8876\n",
      "Episode: 56 meanReward: 23.4062 meanLoss: 0.0496 ExploreP: 0.8864\n",
      "Episode: 57 meanReward: 22.1562 meanLoss: 0.0632 ExploreP: 0.8849\n",
      "Episode: 58 meanReward: 22.2188 meanLoss: 0.0714 ExploreP: 0.8837\n",
      "Episode: 59 meanReward: 22.4375 meanLoss: 0.0432 ExploreP: 0.8803\n",
      "Episode: 60 meanReward: 22.5938 meanLoss: 0.0256 ExploreP: 0.8785\n",
      "Episode: 61 meanReward: 21.6562 meanLoss: 0.0731 ExploreP: 0.8775\n",
      "Episode: 62 meanReward: 22.1250 meanLoss: 0.0661 ExploreP: 0.8749\n",
      "Episode: 63 meanReward: 23.0938 meanLoss: 0.0326 ExploreP: 0.8712\n",
      "Episode: 64 meanReward: 22.4062 meanLoss: 0.0657 ExploreP: 0.8700\n",
      "Episode: 65 meanReward: 22.0625 meanLoss: 0.0578 ExploreP: 0.8679\n",
      "Episode: 66 meanReward: 22.3125 meanLoss: 0.0552 ExploreP: 0.8657\n",
      "Episode: 67 meanReward: 22.1250 meanLoss: 0.0542 ExploreP: 0.8647\n",
      "Episode: 68 meanReward: 22.7812 meanLoss: 0.0521 ExploreP: 0.8614\n",
      "Episode: 69 meanReward: 24.7500 meanLoss: 0.0155 ExploreP: 0.8545\n",
      "Episode: 70 meanReward: 24.7812 meanLoss: 0.0472 ExploreP: 0.8533\n",
      "Episode: 71 meanReward: 25.0625 meanLoss: 0.0722 ExploreP: 0.8516\n",
      "Episode: 72 meanReward: 24.7188 meanLoss: 0.0844 ExploreP: 0.8508\n",
      "Episode: 73 meanReward: 24.8750 meanLoss: 0.0755 ExploreP: 0.8481\n",
      "Episode: 74 meanReward: 24.8438 meanLoss: 0.0488 ExploreP: 0.8464\n",
      "Episode: 75 meanReward: 24.2812 meanLoss: 0.0706 ExploreP: 0.8452\n",
      "Episode: 76 meanReward: 24.0938 meanLoss: 0.0811 ExploreP: 0.8441\n",
      "Episode: 77 meanReward: 23.9688 meanLoss: 0.0914 ExploreP: 0.8431\n",
      "Episode: 78 meanReward: 24.1250 meanLoss: 0.0970 ExploreP: 0.8416\n",
      "Episode: 79 meanReward: 24.6250 meanLoss: 0.0474 ExploreP: 0.8386\n",
      "Episode: 80 meanReward: 24.7812 meanLoss: 0.0305 ExploreP: 0.8372\n",
      "Episode: 81 meanReward: 25.0312 meanLoss: 0.0465 ExploreP: 0.8340\n",
      "Episode: 82 meanReward: 24.4375 meanLoss: 0.0479 ExploreP: 0.8318\n",
      "Episode: 83 meanReward: 25.0312 meanLoss: 0.0375 ExploreP: 0.8289\n",
      "Episode: 84 meanReward: 26.0938 meanLoss: 0.0314 ExploreP: 0.8253\n",
      "Episode: 85 meanReward: 26.1562 meanLoss: 0.0528 ExploreP: 0.8242\n",
      "Episode: 86 meanReward: 25.0625 meanLoss: 0.0712 ExploreP: 0.8224\n",
      "Episode: 87 meanReward: 24.5000 meanLoss: 0.0577 ExploreP: 0.8214\n",
      "Episode: 88 meanReward: 24.5000 meanLoss: 0.0842 ExploreP: 0.8203\n",
      "Episode: 89 meanReward: 24.9688 meanLoss: 0.0656 ExploreP: 0.8177\n",
      "Episode: 90 meanReward: 25.0938 meanLoss: 0.0396 ExploreP: 0.8163\n",
      "Episode: 91 meanReward: 24.3750 meanLoss: 0.0602 ExploreP: 0.8150\n",
      "Episode: 92 meanReward: 24.4062 meanLoss: 0.0651 ExploreP: 0.8133\n",
      "Episode: 93 meanReward: 24.5000 meanLoss: 0.0724 ExploreP: 0.8121\n",
      "Episode: 94 meanReward: 25.0000 meanLoss: 0.0338 ExploreP: 0.8084\n",
      "Episode: 95 meanReward: 24.4688 meanLoss: 0.0384 ExploreP: 0.8063\n",
      "Episode: 96 meanReward: 25.0000 meanLoss: 0.0312 ExploreP: 0.8039\n",
      "Episode: 97 meanReward: 26.3438 meanLoss: 0.0205 ExploreP: 0.7986\n",
      "Episode: 98 meanReward: 26.6250 meanLoss: 0.0223 ExploreP: 0.7958\n",
      "Episode: 99 meanReward: 26.8125 meanLoss: 0.0329 ExploreP: 0.7944\n",
      "Episode: 100 meanReward: 26.0000 meanLoss: 0.0768 ExploreP: 0.7934\n",
      "Episode: 101 meanReward: 23.9688 meanLoss: 0.0692 ExploreP: 0.7921\n",
      "Episode: 102 meanReward: 23.9375 meanLoss: 0.0813 ExploreP: 0.7911\n",
      "Episode: 103 meanReward: 24.0625 meanLoss: 0.0562 ExploreP: 0.7892\n",
      "Episode: 104 meanReward: 24.1562 meanLoss: 0.0501 ExploreP: 0.7882\n",
      "Episode: 105 meanReward: 25.1875 meanLoss: 0.0306 ExploreP: 0.7832\n",
      "Episode: 106 meanReward: 25.3125 meanLoss: 0.0295 ExploreP: 0.7813\n",
      "Episode: 107 meanReward: 25.5000 meanLoss: 0.0459 ExploreP: 0.7798\n",
      "Episode: 108 meanReward: 25.5312 meanLoss: 0.0865 ExploreP: 0.7787\n",
      "Episode: 109 meanReward: 25.5938 meanLoss: 0.0965 ExploreP: 0.7776\n",
      "Episode: 110 meanReward: 25.5312 meanLoss: 0.0815 ExploreP: 0.7764\n",
      "Episode: 111 meanReward: 25.2500 meanLoss: 0.0579 ExploreP: 0.7743\n",
      "Episode: 112 meanReward: 25.2500 meanLoss: 0.0440 ExploreP: 0.7730\n",
      "Episode: 113 meanReward: 24.8125 meanLoss: 0.0640 ExploreP: 0.7711\n",
      "Episode: 114 meanReward: 26.7812 meanLoss: 0.0162 ExploreP: 0.7643\n",
      "Episode: 115 meanReward: 26.1875 meanLoss: 0.0388 ExploreP: 0.7631\n",
      "Episode: 116 meanReward: 25.3750 meanLoss: 0.0744 ExploreP: 0.7618\n",
      "Episode: 117 meanReward: 25.6250 meanLoss: 0.0595 ExploreP: 0.7601\n",
      "Episode: 118 meanReward: 25.4062 meanLoss: 0.0799 ExploreP: 0.7590\n",
      "Episode: 119 meanReward: 26.0000 meanLoss: 0.0608 ExploreP: 0.7567\n",
      "Episode: 120 meanReward: 26.3438 meanLoss: 0.0416 ExploreP: 0.7548\n",
      "Episode: 121 meanReward: 26.0000 meanLoss: 0.0521 ExploreP: 0.7532\n",
      "Episode: 122 meanReward: 26.3125 meanLoss: 0.0520 ExploreP: 0.7512\n",
      "Episode: 123 meanReward: 27.2500 meanLoss: 0.0208 ExploreP: 0.7477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 124 meanReward: 27.8438 meanLoss: 0.0376 ExploreP: 0.7448\n",
      "Episode: 125 meanReward: 27.9688 meanLoss: 0.0351 ExploreP: 0.7434\n",
      "Episode: 126 meanReward: 27.2188 meanLoss: 0.0511 ExploreP: 0.7418\n",
      "Episode: 127 meanReward: 27.0000 meanLoss: 0.0620 ExploreP: 0.7404\n",
      "Episode: 128 meanReward: 26.8125 meanLoss: 0.0575 ExploreP: 0.7386\n",
      "Episode: 129 meanReward: 25.9062 meanLoss: 0.0378 ExploreP: 0.7358\n",
      "Episode: 130 meanReward: 25.7500 meanLoss: 0.0396 ExploreP: 0.7336\n",
      "Episode: 131 meanReward: 27.1875 meanLoss: 0.0185 ExploreP: 0.7290\n",
      "Episode: 132 meanReward: 27.6250 meanLoss: 0.0362 ExploreP: 0.7272\n",
      "Episode: 133 meanReward: 27.6562 meanLoss: 0.0444 ExploreP: 0.7259\n",
      "Episode: 134 meanReward: 27.7812 meanLoss: 0.0658 ExploreP: 0.7247\n",
      "Episode: 135 meanReward: 28.2812 meanLoss: 0.0420 ExploreP: 0.7218\n",
      "Episode: 136 meanReward: 29.4375 meanLoss: 0.0161 ExploreP: 0.7183\n",
      "Episode: 137 meanReward: 28.9062 meanLoss: 0.0169 ExploreP: 0.7149\n",
      "Episode: 138 meanReward: 28.5625 meanLoss: 0.0645 ExploreP: 0.7139\n",
      "Episode: 139 meanReward: 28.2500 meanLoss: 0.1167 ExploreP: 0.7132\n",
      "Episode: 140 meanReward: 28.6250 meanLoss: 0.0943 ExploreP: 0.7114\n",
      "Episode: 141 meanReward: 28.4688 meanLoss: 0.0464 ExploreP: 0.7108\n",
      "Episode: 142 meanReward: 28.7500 meanLoss: 0.0454 ExploreP: 0.7090\n",
      "Episode: 143 meanReward: 28.3125 meanLoss: 0.0493 ExploreP: 0.7081\n",
      "Episode: 144 meanReward: 28.1875 meanLoss: 0.0891 ExploreP: 0.7072\n",
      "Episode: 145 meanReward: 27.8125 meanLoss: 0.0863 ExploreP: 0.7063\n",
      "Episode: 146 meanReward: 26.5312 meanLoss: 0.0404 ExploreP: 0.7029\n",
      "Episode: 147 meanReward: 27.2188 meanLoss: 0.0312 ExploreP: 0.7003\n",
      "Episode: 148 meanReward: 27.0625 meanLoss: 0.0577 ExploreP: 0.6994\n",
      "Episode: 149 meanReward: 27.5312 meanLoss: 0.0536 ExploreP: 0.6968\n",
      "Episode: 150 meanReward: 27.6875 meanLoss: 0.0497 ExploreP: 0.6955\n",
      "Episode: 151 meanReward: 27.0938 meanLoss: 0.0909 ExploreP: 0.6946\n",
      "Episode: 152 meanReward: 26.7812 meanLoss: 0.0760 ExploreP: 0.6936\n",
      "Episode: 153 meanReward: 26.5625 meanLoss: 0.0726 ExploreP: 0.6927\n",
      "Episode: 154 meanReward: 26.8125 meanLoss: 0.0507 ExploreP: 0.6902\n",
      "Episode: 155 meanReward: 25.7812 meanLoss: 0.0447 ExploreP: 0.6893\n",
      "Episode: 156 meanReward: 25.8438 meanLoss: 0.0545 ExploreP: 0.6865\n",
      "Episode: 157 meanReward: 26.4375 meanLoss: 0.0311 ExploreP: 0.6839\n",
      "Episode: 158 meanReward: 26.4062 meanLoss: 0.0531 ExploreP: 0.6825\n",
      "Episode: 159 meanReward: 26.9375 meanLoss: 0.0479 ExploreP: 0.6801\n",
      "Episode: 160 meanReward: 26.8125 meanLoss: 0.0446 ExploreP: 0.6787\n",
      "Episode: 161 meanReward: 26.3125 meanLoss: 0.0558 ExploreP: 0.6772\n",
      "Episode: 162 meanReward: 26.3750 meanLoss: 0.0374 ExploreP: 0.6751\n",
      "Episode: 163 meanReward: 25.3438 meanLoss: 0.0375 ExploreP: 0.6730\n",
      "Episode: 164 meanReward: 25.0000 meanLoss: 0.0351 ExploreP: 0.6720\n",
      "Episode: 165 meanReward: 24.8750 meanLoss: 0.0891 ExploreP: 0.6711\n",
      "Episode: 166 meanReward: 24.7188 meanLoss: 0.0783 ExploreP: 0.6703\n",
      "Episode: 167 meanReward: 24.5938 meanLoss: 0.0466 ExploreP: 0.6679\n",
      "Episode: 168 meanReward: 23.4688 meanLoss: 0.0413 ExploreP: 0.6670\n",
      "Episode: 169 meanReward: 22.5000 meanLoss: 0.0824 ExploreP: 0.6659\n",
      "Episode: 170 meanReward: 22.6562 meanLoss: 0.0739 ExploreP: 0.6647\n",
      "Episode: 171 meanReward: 22.8438 meanLoss: 0.0678 ExploreP: 0.6637\n",
      "Episode: 172 meanReward: 24.7812 meanLoss: 0.0148 ExploreP: 0.6579\n",
      "Episode: 173 meanReward: 25.4688 meanLoss: 0.0361 ExploreP: 0.6559\n",
      "Episode: 174 meanReward: 26.5312 meanLoss: 0.0217 ExploreP: 0.6521\n",
      "Episode: 175 meanReward: 26.4688 meanLoss: 0.0390 ExploreP: 0.6514\n",
      "Episode: 176 meanReward: 26.5625 meanLoss: 0.0719 ExploreP: 0.6503\n",
      "Episode: 177 meanReward: 26.5938 meanLoss: 0.0687 ExploreP: 0.6495\n",
      "Episode: 178 meanReward: 25.3750 meanLoss: 0.0631 ExploreP: 0.6489\n",
      "Episode: 179 meanReward: 24.5000 meanLoss: 0.0812 ExploreP: 0.6482\n",
      "Episode: 180 meanReward: 25.0938 meanLoss: 0.0640 ExploreP: 0.6462\n",
      "Episode: 181 meanReward: 26.0938 meanLoss: 0.0144 ExploreP: 0.6418\n",
      "Episode: 182 meanReward: 26.8125 meanLoss: 0.0308 ExploreP: 0.6391\n",
      "Episode: 183 meanReward: 27.5625 meanLoss: 0.0421 ExploreP: 0.6368\n",
      "Episode: 184 meanReward: 30.3438 meanLoss: 0.0140 ExploreP: 0.6304\n",
      "Episode: 185 meanReward: 30.3750 meanLoss: 0.0672 ExploreP: 0.6294\n",
      "Episode: 186 meanReward: 29.9688 meanLoss: 0.0690 ExploreP: 0.6280\n",
      "Episode: 187 meanReward: 29.9375 meanLoss: 0.0630 ExploreP: 0.6273\n",
      "Episode: 188 meanReward: 31.8750 meanLoss: 0.0192 ExploreP: 0.6209\n",
      "Episode: 189 meanReward: 31.9375 meanLoss: 0.0424 ExploreP: 0.6184\n",
      "Episode: 190 meanReward: 32.0625 meanLoss: 0.0453 ExploreP: 0.6169\n",
      "Episode: 191 meanReward: 31.7188 meanLoss: 0.0497 ExploreP: 0.6154\n",
      "Episode: 192 meanReward: 33.0625 meanLoss: 0.0263 ExploreP: 0.6115\n",
      "Episode: 193 meanReward: 33.5938 meanLoss: 0.0394 ExploreP: 0.6092\n",
      "Episode: 194 meanReward: 33.2500 meanLoss: 0.0374 ExploreP: 0.6079\n",
      "Episode: 195 meanReward: 32.9062 meanLoss: 0.0587 ExploreP: 0.6068\n",
      "Episode: 196 meanReward: 33.5312 meanLoss: 0.0349 ExploreP: 0.6047\n",
      "Episode: 197 meanReward: 33.8750 meanLoss: 0.0374 ExploreP: 0.6032\n",
      "Episode: 198 meanReward: 34.5312 meanLoss: 0.0437 ExploreP: 0.6012\n",
      "Episode: 199 meanReward: 35.9375 meanLoss: 0.0150 ExploreP: 0.5965\n",
      "Episode: 200 meanReward: 37.4688 meanLoss: 0.0262 ExploreP: 0.5928\n",
      "Episode: 201 meanReward: 37.5938 meanLoss: 0.0643 ExploreP: 0.5916\n",
      "Episode: 202 meanReward: 37.5625 meanLoss: 0.0733 ExploreP: 0.5906\n",
      "Episode: 203 meanReward: 37.9375 meanLoss: 0.0608 ExploreP: 0.5889\n",
      "Episode: 204 meanReward: 36.3438 meanLoss: 0.0415 ExploreP: 0.5868\n",
      "Episode: 205 meanReward: 39.2500 meanLoss: 0.0098 ExploreP: 0.5797\n",
      "Episode: 206 meanReward: 38.5312 meanLoss: 0.0326 ExploreP: 0.5776\n",
      "Episode: 207 meanReward: 40.4688 meanLoss: 0.0109 ExploreP: 0.5735\n",
      "Episode: 208 meanReward: 42.3750 meanLoss: 0.0196 ExploreP: 0.5691\n",
      "Episode: 209 meanReward: 42.5625 meanLoss: 0.0447 ExploreP: 0.5681\n",
      "Episode: 210 meanReward: 46.5000 meanLoss: 0.0112 ExploreP: 0.5605\n",
      "Episode: 211 meanReward: 47.4688 meanLoss: 0.0420 ExploreP: 0.5583\n",
      "Episode: 212 meanReward: 46.9375 meanLoss: 0.0461 ExploreP: 0.5575\n",
      "Episode: 213 meanReward: 45.1875 meanLoss: 0.0798 ExploreP: 0.5568\n",
      "Episode: 214 meanReward: 45.1875 meanLoss: 0.0380 ExploreP: 0.5544\n",
      "Episode: 215 meanReward: 45.5000 meanLoss: 0.0219 ExploreP: 0.5519\n",
      "Episode: 216 meanReward: 43.8438 meanLoss: 0.0144 ExploreP: 0.5492\n",
      "Episode: 217 meanReward: 43.7812 meanLoss: 0.0500 ExploreP: 0.5485\n",
      "Episode: 218 meanReward: 43.4062 meanLoss: 0.1222 ExploreP: 0.5479\n",
      "Episode: 219 meanReward: 43.3438 meanLoss: 0.1153 ExploreP: 0.5473\n",
      "Episode: 220 meanReward: 40.4062 meanLoss: 0.1128 ExploreP: 0.5468\n",
      "Episode: 221 meanReward: 39.6250 meanLoss: 0.1001 ExploreP: 0.5460\n",
      "Episode: 222 meanReward: 39.2812 meanLoss: 0.0898 ExploreP: 0.5452\n",
      "Episode: 223 meanReward: 39.0000 meanLoss: 0.0822 ExploreP: 0.5444\n",
      "Episode: 224 meanReward: 37.6250 meanLoss: 0.0667 ExploreP: 0.5433\n",
      "Episode: 225 meanReward: 41.5312 meanLoss: 0.0074 ExploreP: 0.5346\n",
      "Episode: 226 meanReward: 41.6875 meanLoss: 0.0409 ExploreP: 0.5333\n",
      "Episode: 227 meanReward: 42.8750 meanLoss: 0.0270 ExploreP: 0.5302\n",
      "Episode: 228 meanReward: 43.5625 meanLoss: 0.0211 ExploreP: 0.5273\n",
      "Episode: 229 meanReward: 43.2812 meanLoss: 0.0387 ExploreP: 0.5265\n",
      "Episode: 230 meanReward: 46.1875 meanLoss: 0.0152 ExploreP: 0.5200\n",
      "Episode: 231 meanReward: 46.5938 meanLoss: 0.0138 ExploreP: 0.5152\n",
      "Episode: 232 meanReward: 47.0312 meanLoss: 0.0152 ExploreP: 0.5114\n",
      "Episode: 233 meanReward: 47.5625 meanLoss: 0.0268 ExploreP: 0.5094\n",
      "Episode: 234 meanReward: 48.2812 meanLoss: 0.0386 ExploreP: 0.5075\n",
      "Episode: 235 meanReward: 48.2500 meanLoss: 0.0376 ExploreP: 0.5061\n",
      "Episode: 236 meanReward: 49.9688 meanLoss: 0.0190 ExploreP: 0.5016\n",
      "Episode: 237 meanReward: 49.0000 meanLoss: 0.0157 ExploreP: 0.4970\n",
      "Episode: 238 meanReward: 50.5938 meanLoss: 0.0138 ExploreP: 0.4928\n",
      "Episode: 239 meanReward: 49.2500 meanLoss: 0.0411 ExploreP: 0.4913\n",
      "Episode: 240 meanReward: 47.4062 meanLoss: 0.0605 ExploreP: 0.4904\n",
      "Episode: 241 meanReward: 47.5000 meanLoss: 0.0622 ExploreP: 0.4894\n",
      "Episode: 242 meanReward: 44.7188 meanLoss: 0.0308 ExploreP: 0.4871\n",
      "Episode: 243 meanReward: 47.7188 meanLoss: 0.0089 ExploreP: 0.4806\n",
      "Episode: 244 meanReward: 47.9688 meanLoss: 0.0629 ExploreP: 0.4796\n",
      "Episode: 245 meanReward: 48.0625 meanLoss: 0.0622 ExploreP: 0.4788\n",
      "Episode: 246 meanReward: 47.1250 meanLoss: 0.0854 ExploreP: 0.4782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 247 meanReward: 46.0938 meanLoss: 0.0630 ExploreP: 0.4776\n",
      "Episode: 248 meanReward: 46.3125 meanLoss: 0.0309 ExploreP: 0.4749\n",
      "Episode: 249 meanReward: 46.3125 meanLoss: 0.0415 ExploreP: 0.4743\n",
      "Episode: 250 meanReward: 46.3125 meanLoss: 0.0959 ExploreP: 0.4738\n",
      "Episode: 251 meanReward: 46.3125 meanLoss: 0.1298 ExploreP: 0.4733\n",
      "Episode: 252 meanReward: 46.2812 meanLoss: 0.1226 ExploreP: 0.4729\n",
      "Episode: 253 meanReward: 46.3438 meanLoss: 0.0969 ExploreP: 0.4721\n",
      "Episode: 254 meanReward: 46.5000 meanLoss: 0.0799 ExploreP: 0.4712\n",
      "Episode: 255 meanReward: 46.5312 meanLoss: 0.0681 ExploreP: 0.4705\n",
      "Episode: 256 meanReward: 46.8125 meanLoss: 0.0625 ExploreP: 0.4691\n",
      "Episode: 257 meanReward: 42.4688 meanLoss: 0.0467 ExploreP: 0.4680\n",
      "Episode: 258 meanReward: 42.7188 meanLoss: 0.0440 ExploreP: 0.4664\n",
      "Episode: 259 meanReward: 41.6562 meanLoss: 0.0415 ExploreP: 0.4653\n",
      "Episode: 260 meanReward: 41.3125 meanLoss: 0.0275 ExploreP: 0.4632\n",
      "Episode: 261 meanReward: 43.3125 meanLoss: 0.0133 ExploreP: 0.4596\n",
      "Episode: 262 meanReward: 45.6875 meanLoss: 0.0070 ExploreP: 0.4506\n",
      "Episode: 263 meanReward: 48.0312 meanLoss: 0.0085 ExploreP: 0.4432\n",
      "Episode: 264 meanReward: 53.4062 meanLoss: 0.0047 ExploreP: 0.4326\n",
      "Episode: 265 meanReward: 57.0000 meanLoss: 0.0052 ExploreP: 0.4262\n",
      "Episode: 266 meanReward: 58.9375 meanLoss: 0.0137 ExploreP: 0.4220\n",
      "Episode: 267 meanReward: 58.5000 meanLoss: 0.0492 ExploreP: 0.4214\n",
      "Episode: 268 meanReward: 56.8750 meanLoss: 0.0588 ExploreP: 0.4198\n",
      "Episode: 269 meanReward: 56.2500 meanLoss: 0.0200 ExploreP: 0.4168\n",
      "Episode: 270 meanReward: 56.7188 meanLoss: 0.0130 ExploreP: 0.4127\n",
      "Episode: 271 meanReward: 59.0000 meanLoss: 0.0102 ExploreP: 0.4085\n",
      "Episode: 272 meanReward: 59.2188 meanLoss: 0.0421 ExploreP: 0.4075\n",
      "Episode: 273 meanReward: 63.0625 meanLoss: 0.0126 ExploreP: 0.4018\n",
      "Episode: 274 meanReward: 62.2500 meanLoss: 0.0556 ExploreP: 0.4010\n",
      "Episode: 275 meanReward: 58.4062 meanLoss: 0.0797 ExploreP: 0.4004\n",
      "Episode: 276 meanReward: 58.1875 meanLoss: 0.0750 ExploreP: 0.3998\n",
      "Episode: 277 meanReward: 58.0625 meanLoss: 0.0972 ExploreP: 0.3993\n",
      "Episode: 278 meanReward: 57.9688 meanLoss: 0.0873 ExploreP: 0.3989\n",
      "Episode: 279 meanReward: 58.0312 meanLoss: 0.1089 ExploreP: 0.3983\n",
      "Episode: 280 meanReward: 56.6875 meanLoss: 0.1010 ExploreP: 0.3978\n",
      "Episode: 281 meanReward: 56.8750 meanLoss: 0.0813 ExploreP: 0.3970\n",
      "Episode: 282 meanReward: 59.6875 meanLoss: 0.0221 ExploreP: 0.3931\n",
      "Episode: 283 meanReward: 62.7500 meanLoss: 0.0102 ExploreP: 0.3890\n",
      "Episode: 284 meanReward: 63.0000 meanLoss: 0.0670 ExploreP: 0.3884\n",
      "Episode: 285 meanReward: 62.7500 meanLoss: 0.0898 ExploreP: 0.3880\n",
      "Episode: 286 meanReward: 62.5000 meanLoss: 0.1314 ExploreP: 0.3876\n",
      "Episode: 287 meanReward: 62.2500 meanLoss: 0.1181 ExploreP: 0.3873\n",
      "Episode: 288 meanReward: 61.7188 meanLoss: 0.1033 ExploreP: 0.3868\n",
      "Episode: 289 meanReward: 61.4688 meanLoss: 0.1030 ExploreP: 0.3862\n",
      "Episode: 290 meanReward: 60.8125 meanLoss: 0.1049 ExploreP: 0.3857\n",
      "Episode: 291 meanReward: 60.5000 meanLoss: 0.0860 ExploreP: 0.3852\n",
      "Episode: 292 meanReward: 59.4375 meanLoss: 0.1034 ExploreP: 0.3847\n",
      "Episode: 293 meanReward: 57.5000 meanLoss: 0.0824 ExploreP: 0.3841\n",
      "Episode: 294 meanReward: 54.5312 meanLoss: 0.0161 ExploreP: 0.3801\n",
      "Episode: 295 meanReward: 49.8438 meanLoss: 0.0444 ExploreP: 0.3794\n",
      "Episode: 296 meanReward: 42.3438 meanLoss: 0.0886 ExploreP: 0.3790\n",
      "Episode: 297 meanReward: 37.8438 meanLoss: 0.0947 ExploreP: 0.3787\n",
      "Episode: 298 meanReward: 35.0000 meanLoss: 0.1448 ExploreP: 0.3783\n",
      "Episode: 299 meanReward: 34.9375 meanLoss: 0.1237 ExploreP: 0.3779\n",
      "Episode: 300 meanReward: 34.0625 meanLoss: 0.1129 ExploreP: 0.3775\n",
      "Episode: 301 meanReward: 32.2188 meanLoss: 0.1054 ExploreP: 0.3769\n",
      "Episode: 302 meanReward: 29.3125 meanLoss: 0.1082 ExploreP: 0.3766\n",
      "Episode: 303 meanReward: 26.4062 meanLoss: 0.1036 ExploreP: 0.3762\n",
      "Episode: 304 meanReward: 26.0625 meanLoss: 0.1152 ExploreP: 0.3757\n",
      "Episode: 305 meanReward: 21.9062 meanLoss: 0.0999 ExploreP: 0.3753\n",
      "Episode: 306 meanReward: 21.5625 meanLoss: 0.0958 ExploreP: 0.3749\n",
      "Episode: 307 meanReward: 21.5000 meanLoss: 0.1139 ExploreP: 0.3745\n",
      "Episode: 308 meanReward: 21.4062 meanLoss: 0.1202 ExploreP: 0.3740\n",
      "Episode: 309 meanReward: 21.4688 meanLoss: 0.1012 ExploreP: 0.3735\n",
      "Episode: 310 meanReward: 22.2188 meanLoss: 0.0675 ExploreP: 0.3722\n",
      "Episode: 311 meanReward: 22.3438 meanLoss: 0.0432 ExploreP: 0.3716\n",
      "Episode: 312 meanReward: 22.8125 meanLoss: 0.0649 ExploreP: 0.3705\n",
      "Episode: 313 meanReward: 22.5625 meanLoss: 0.0506 ExploreP: 0.3701\n",
      "Episode: 314 meanReward: 20.3125 meanLoss: 0.0551 ExploreP: 0.3690\n",
      "Episode: 315 meanReward: 17.5938 meanLoss: 0.0443 ExploreP: 0.3683\n",
      "Episode: 316 meanReward: 18.3438 meanLoss: 0.0410 ExploreP: 0.3668\n",
      "Episode: 317 meanReward: 19.2188 meanLoss: 0.0341 ExploreP: 0.3655\n",
      "Episode: 318 meanReward: 19.3438 meanLoss: 0.0434 ExploreP: 0.3650\n",
      "Episode: 319 meanReward: 20.1875 meanLoss: 0.0502 ExploreP: 0.3637\n",
      "Episode: 320 meanReward: 21.5000 meanLoss: 0.0286 ExploreP: 0.3618\n",
      "Episode: 321 meanReward: 22.0000 meanLoss: 0.0370 ExploreP: 0.3606\n",
      "Episode: 322 meanReward: 21.8438 meanLoss: 0.0405 ExploreP: 0.3603\n",
      "Episode: 323 meanReward: 21.8750 meanLoss: 0.0759 ExploreP: 0.3598\n",
      "Episode: 324 meanReward: 21.9688 meanLoss: 0.0779 ExploreP: 0.3593\n",
      "Episode: 325 meanReward: 23.5938 meanLoss: 0.0259 ExploreP: 0.3569\n",
      "Episode: 326 meanReward: 22.3750 meanLoss: 0.0240 ExploreP: 0.3545\n",
      "Episode: 327 meanReward: 22.5000 meanLoss: 0.0429 ExploreP: 0.3537\n",
      "Episode: 328 meanReward: 22.6562 meanLoss: 0.0684 ExploreP: 0.3532\n",
      "Episode: 329 meanReward: 22.6250 meanLoss: 0.0959 ExploreP: 0.3530\n",
      "Episode: 330 meanReward: 24.3438 meanLoss: 0.0390 ExploreP: 0.3507\n",
      "Episode: 331 meanReward: 25.0000 meanLoss: 0.0338 ExploreP: 0.3496\n",
      "Episode: 332 meanReward: 26.8750 meanLoss: 0.0158 ExploreP: 0.3472\n",
      "Episode: 333 meanReward: 29.2188 meanLoss: 0.0157 ExploreP: 0.3442\n",
      "Episode: 334 meanReward: 32.8125 meanLoss: 0.0110 ExploreP: 0.3401\n",
      "Episode: 335 meanReward: 35.7812 meanLoss: 0.0110 ExploreP: 0.3366\n",
      "Episode: 336 meanReward: 38.5625 meanLoss: 0.0153 ExploreP: 0.3332\n",
      "Episode: 337 meanReward: 41.3438 meanLoss: 0.0134 ExploreP: 0.3300\n",
      "Episode: 338 meanReward: 45.3750 meanLoss: 0.0070 ExploreP: 0.3256\n",
      "Episode: 339 meanReward: 49.9062 meanLoss: 0.0069 ExploreP: 0.3207\n",
      "Episode: 340 meanReward: 54.3438 meanLoss: 0.0092 ExploreP: 0.3159\n",
      "Episode: 341 meanReward: 57.3438 meanLoss: 0.0156 ExploreP: 0.3125\n",
      "Episode: 342 meanReward: 56.7500 meanLoss: 0.0411 ExploreP: 0.3121\n",
      "Episode: 343 meanReward: 56.5625 meanLoss: 0.0770 ExploreP: 0.3117\n",
      "Episode: 344 meanReward: 56.0625 meanLoss: 0.1068 ExploreP: 0.3113\n",
      "Episode: 345 meanReward: 56.2188 meanLoss: 0.0872 ExploreP: 0.3108\n",
      "Episode: 346 meanReward: 57.0938 meanLoss: 0.0353 ExploreP: 0.3091\n",
      "Episode: 347 meanReward: 56.8438 meanLoss: 0.0489 ExploreP: 0.3087\n",
      "Episode: 348 meanReward: 56.0000 meanLoss: 0.0831 ExploreP: 0.3083\n",
      "Episode: 349 meanReward: 55.1562 meanLoss: 0.1073 ExploreP: 0.3080\n",
      "Episode: 350 meanReward: 55.0312 meanLoss: 0.1085 ExploreP: 0.3076\n",
      "Episode: 351 meanReward: 54.3125 meanLoss: 0.1163 ExploreP: 0.3073\n",
      "Episode: 352 meanReward: 54.7500 meanLoss: 0.0357 ExploreP: 0.3052\n",
      "Episode: 353 meanReward: 56.4688 meanLoss: 0.0156 ExploreP: 0.3027\n",
      "Episode: 354 meanReward: 60.2188 meanLoss: 0.0096 ExploreP: 0.2989\n",
      "Episode: 355 meanReward: 63.4062 meanLoss: 0.0107 ExploreP: 0.2956\n",
      "Episode: 356 meanReward: 67.5625 meanLoss: 0.0092 ExploreP: 0.2914\n",
      "Episode: 357 meanReward: 75.0625 meanLoss: 0.0052 ExploreP: 0.2828\n",
      "Episode: 358 meanReward: 77.0625 meanLoss: 0.0105 ExploreP: 0.2792\n",
      "Episode: 359 meanReward: 83.5000 meanLoss: 0.0115 ExploreP: 0.2731\n",
      "Episode: 360 meanReward: 86.3125 meanLoss: 0.0142 ExploreP: 0.2704\n",
      "Episode: 361 meanReward: 86.9375 meanLoss: 0.0483 ExploreP: 0.2697\n",
      "Episode: 362 meanReward: 85.3438 meanLoss: 0.0419 ExploreP: 0.2693\n",
      "Episode: 363 meanReward: 84.9062 meanLoss: 0.0833 ExploreP: 0.2688\n",
      "Episode: 364 meanReward: 86.4688 meanLoss: 0.0135 ExploreP: 0.2657\n",
      "Episode: 365 meanReward: 89.8750 meanLoss: 0.0071 ExploreP: 0.2607\n",
      "Episode: 366 meanReward: 90.9688 meanLoss: 0.0074 ExploreP: 0.2567\n",
      "Episode: 367 meanReward: 91.7188 meanLoss: 0.0113 ExploreP: 0.2535\n",
      "Episode: 368 meanReward: 92.8438 meanLoss: 0.0080 ExploreP: 0.2502\n",
      "Episode: 369 meanReward: 90.3125 meanLoss: 0.0557 ExploreP: 0.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 370 meanReward: 86.5938 meanLoss: 0.0452 ExploreP: 0.2492\n",
      "Episode: 371 meanReward: 82.1562 meanLoss: 0.0739 ExploreP: 0.2488\n",
      "Episode: 372 meanReward: 77.8125 meanLoss: 0.0780 ExploreP: 0.2485\n",
      "Episode: 373 meanReward: 74.9062 meanLoss: 0.0729 ExploreP: 0.2481\n",
      "Episode: 374 meanReward: 74.8125 meanLoss: 0.0939 ExploreP: 0.2478\n",
      "Episode: 375 meanReward: 74.8438 meanLoss: 0.0968 ExploreP: 0.2474\n",
      "Episode: 376 meanReward: 74.8438 meanLoss: 0.1112 ExploreP: 0.2471\n",
      "Episode: 377 meanReward: 74.7188 meanLoss: 0.0942 ExploreP: 0.2468\n",
      "Episode: 378 meanReward: 73.3125 meanLoss: 0.0860 ExploreP: 0.2465\n",
      "Episode: 379 meanReward: 73.5938 meanLoss: 0.0822 ExploreP: 0.2460\n",
      "Episode: 380 meanReward: 73.8438 meanLoss: 0.0620 ExploreP: 0.2455\n",
      "Episode: 381 meanReward: 74.2188 meanLoss: 0.0600 ExploreP: 0.2450\n",
      "Episode: 382 meanReward: 74.6562 meanLoss: 0.0581 ExploreP: 0.2444\n",
      "Episode: 383 meanReward: 74.7188 meanLoss: 0.0590 ExploreP: 0.2440\n",
      "Episode: 384 meanReward: 76.1250 meanLoss: 0.0193 ExploreP: 0.2414\n",
      "Episode: 385 meanReward: 76.8750 meanLoss: 0.0102 ExploreP: 0.2388\n",
      "Episode: 386 meanReward: 73.2500 meanLoss: 0.0510 ExploreP: 0.2386\n",
      "Episode: 387 meanReward: 70.2188 meanLoss: 0.0892 ExploreP: 0.2381\n",
      "Episode: 388 meanReward: 65.9375 meanLoss: 0.0840 ExploreP: 0.2379\n",
      "Episode: 389 meanReward: 56.5938 meanLoss: 0.0851 ExploreP: 0.2376\n",
      "Episode: 390 meanReward: 52.8125 meanLoss: 0.1115 ExploreP: 0.2374\n",
      "Episode: 391 meanReward: 46.0000 meanLoss: 0.1183 ExploreP: 0.2371\n",
      "Episode: 392 meanReward: 43.0000 meanLoss: 0.1309 ExploreP: 0.2369\n",
      "Episode: 393 meanReward: 42.5312 meanLoss: 0.1256 ExploreP: 0.2366\n",
      "Episode: 394 meanReward: 42.4062 meanLoss: 0.1150 ExploreP: 0.2364\n",
      "Episode: 395 meanReward: 42.1562 meanLoss: 0.1193 ExploreP: 0.2362\n",
      "Episode: 396 meanReward: 38.6562 meanLoss: 0.1363 ExploreP: 0.2359\n",
      "Episode: 397 meanReward: 32.7812 meanLoss: 0.1310 ExploreP: 0.2357\n",
      "Episode: 398 meanReward: 28.1875 meanLoss: 0.1274 ExploreP: 0.2354\n",
      "Episode: 399 meanReward: 24.7500 meanLoss: 0.1038 ExploreP: 0.2350\n",
      "Episode: 400 meanReward: 23.8750 meanLoss: 0.0160 ExploreP: 0.2325\n",
      "Episode: 401 meanReward: 26.7812 meanLoss: 0.0102 ExploreP: 0.2300\n",
      "Episode: 402 meanReward: 31.6875 meanLoss: 0.0081 ExploreP: 0.2261\n",
      "Episode: 403 meanReward: 36.5000 meanLoss: 0.0104 ExploreP: 0.2225\n",
      "Episode: 404 meanReward: 39.1875 meanLoss: 0.0128 ExploreP: 0.2204\n",
      "Episode: 405 meanReward: 41.8438 meanLoss: 0.0135 ExploreP: 0.2182\n",
      "Episode: 406 meanReward: 45.5938 meanLoss: 0.0105 ExploreP: 0.2155\n",
      "Episode: 407 meanReward: 49.9688 meanLoss: 0.0068 ExploreP: 0.2124\n",
      "Episode: 408 meanReward: 52.4375 meanLoss: 0.0165 ExploreP: 0.2105\n",
      "Episode: 409 meanReward: 54.8750 meanLoss: 0.0180 ExploreP: 0.2087\n",
      "Episode: 410 meanReward: 57.8125 meanLoss: 0.0118 ExploreP: 0.2066\n",
      "Episode: 411 meanReward: 61.3750 meanLoss: 0.0086 ExploreP: 0.2039\n",
      "Episode: 412 meanReward: 65.5625 meanLoss: 0.0082 ExploreP: 0.2009\n",
      "Episode: 413 meanReward: 69.4062 meanLoss: 0.0080 ExploreP: 0.1982\n",
      "Episode: 414 meanReward: 73.2500 meanLoss: 0.0094 ExploreP: 0.1954\n",
      "Episode: 415 meanReward: 77.9688 meanLoss: 0.0082 ExploreP: 0.1924\n",
      "Episode: 416 meanReward: 80.3438 meanLoss: 0.0071 ExploreP: 0.1889\n",
      "Episode: 417 meanReward: 84.4688 meanLoss: 0.0055 ExploreP: 0.1846\n",
      "Episode: 418 meanReward: 90.6562 meanLoss: 0.0049 ExploreP: 0.1810\n",
      "Episode: 419 meanReward: 100.8438 meanLoss: 0.0030 ExploreP: 0.1752\n",
      "Episode: 420 meanReward: 101.4375 meanLoss: 0.0395 ExploreP: 0.1747\n",
      "Episode: 421 meanReward: 101.4688 meanLoss: 0.0649 ExploreP: 0.1745\n",
      "Episode: 422 meanReward: 101.4062 meanLoss: 0.0958 ExploreP: 0.1744\n",
      "Episode: 423 meanReward: 101.4375 meanLoss: 0.1124 ExploreP: 0.1742\n",
      "Episode: 424 meanReward: 101.5000 meanLoss: 0.1147 ExploreP: 0.1740\n",
      "Episode: 425 meanReward: 101.3750 meanLoss: 0.1167 ExploreP: 0.1738\n",
      "Episode: 426 meanReward: 101.3750 meanLoss: 0.1222 ExploreP: 0.1737\n",
      "Episode: 427 meanReward: 104.0000 meanLoss: 0.0347 ExploreP: 0.1721\n",
      "Episode: 428 meanReward: 107.4375 meanLoss: 0.0222 ExploreP: 0.1702\n",
      "Episode: 429 meanReward: 111.2812 meanLoss: 0.0110 ExploreP: 0.1681\n",
      "Episode: 430 meanReward: 115.5938 meanLoss: 0.0065 ExploreP: 0.1657\n",
      "Episode: 431 meanReward: 118.0000 meanLoss: 0.0114 ExploreP: 0.1642\n",
      "Episode: 432 meanReward: 115.1250 meanLoss: 0.0597 ExploreP: 0.1639\n",
      "Episode: 433 meanReward: 112.0000 meanLoss: 0.0883 ExploreP: 0.1637\n",
      "Episode: 434 meanReward: 109.8750 meanLoss: 0.0268 ExploreP: 0.1621\n",
      "Episode: 435 meanReward: 110.9062 meanLoss: 0.0054 ExploreP: 0.1590\n",
      "Episode: 436 meanReward: 112.3438 meanLoss: 0.0105 ExploreP: 0.1568\n",
      "Episode: 437 meanReward: 114.4062 meanLoss: 0.0072 ExploreP: 0.1544\n",
      "Episode: 438 meanReward: 117.3438 meanLoss: 0.0061 ExploreP: 0.1512\n",
      "Episode: 439 meanReward: 122.1562 meanLoss: 0.0041 ExploreP: 0.1469\n",
      "Episode: 440 meanReward: 134.0312 meanLoss: 0.0025 ExploreP: 0.1406\n",
      "Episode: 441 meanReward: 132.4688 meanLoss: 0.0400 ExploreP: 0.1400\n",
      "Episode: 442 meanReward: 132.8438 meanLoss: 0.0085 ExploreP: 0.1385\n",
      "Episode: 443 meanReward: 130.3125 meanLoss: 0.0257 ExploreP: 0.1378\n",
      "Episode: 444 meanReward: 128.5625 meanLoss: 0.0120 ExploreP: 0.1365\n",
      "Episode: 445 meanReward: 125.6250 meanLoss: 0.0226 ExploreP: 0.1359\n",
      "Episode: 446 meanReward: 123.4375 meanLoss: 0.0165 ExploreP: 0.1349\n",
      "Episode: 447 meanReward: 121.5625 meanLoss: 0.0138 ExploreP: 0.1336\n",
      "Episode: 448 meanReward: 120.7188 meanLoss: 0.0079 ExploreP: 0.1316\n",
      "Episode: 449 meanReward: 116.9375 meanLoss: 0.0096 ExploreP: 0.1301\n",
      "Episode: 450 meanReward: 114.9688 meanLoss: 0.0084 ExploreP: 0.1284\n",
      "Episode: 451 meanReward: 110.6875 meanLoss: 0.0046 ExploreP: 0.1259\n",
      "Episode: 452 meanReward: 116.3750 meanLoss: 0.0056 ExploreP: 0.1235\n",
      "Episode: 453 meanReward: 122.6875 meanLoss: 0.0049 ExploreP: 0.1211\n",
      "Episode: 454 meanReward: 135.3438 meanLoss: 0.0028 ExploreP: 0.1166\n",
      "Episode: 455 meanReward: 150.5938 meanLoss: 0.0028 ExploreP: 0.1114\n",
      "Episode: 456 meanReward: 151.4375 meanLoss: 0.0530 ExploreP: 0.1110\n",
      "Episode: 457 meanReward: 151.6250 meanLoss: 0.0564 ExploreP: 0.1109\n",
      "Episode: 458 meanReward: 151.6250 meanLoss: 0.1115 ExploreP: 0.1107\n",
      "Episode: 459 meanReward: 149.0625 meanLoss: 0.1137 ExploreP: 0.1106\n",
      "Episode: 460 meanReward: 145.7188 meanLoss: 0.1121 ExploreP: 0.1105\n",
      "Episode: 461 meanReward: 142.0625 meanLoss: 0.1072 ExploreP: 0.1103\n",
      "Episode: 462 meanReward: 140.1875 meanLoss: 0.0294 ExploreP: 0.1094\n",
      "Episode: 463 meanReward: 139.7500 meanLoss: 0.0214 ExploreP: 0.1086\n",
      "Episode: 464 meanReward: 140.5625 meanLoss: 0.0201 ExploreP: 0.1082\n",
      "Episode: 465 meanReward: 140.6562 meanLoss: 0.0608 ExploreP: 0.1080\n",
      "Episode: 466 meanReward: 140.7188 meanLoss: 0.0217 ExploreP: 0.1069\n",
      "Episode: 467 meanReward: 137.5000 meanLoss: 0.0202 ExploreP: 0.1060\n",
      "Episode: 468 meanReward: 135.8125 meanLoss: 0.0180 ExploreP: 0.1051\n",
      "Episode: 469 meanReward: 132.9688 meanLoss: 0.0136 ExploreP: 0.1043\n",
      "Episode: 470 meanReward: 129.5938 meanLoss: 0.0100 ExploreP: 0.1032\n",
      "Episode: 471 meanReward: 121.7812 meanLoss: 0.0239 ExploreP: 0.1027\n",
      "Episode: 472 meanReward: 108.9375 meanLoss: 0.0295 ExploreP: 0.1021\n",
      "Episode: 473 meanReward: 110.0312 meanLoss: 0.0185 ExploreP: 0.1014\n",
      "Episode: 474 meanReward: 108.9062 meanLoss: 0.0172 ExploreP: 0.1007\n",
      "Episode: 475 meanReward: 110.0625 meanLoss: 0.0156 ExploreP: 0.0999\n",
      "Episode: 476 meanReward: 110.1562 meanLoss: 0.0141 ExploreP: 0.0989\n",
      "Episode: 477 meanReward: 112.1875 meanLoss: 0.0113 ExploreP: 0.0979\n",
      "Episode: 478 meanReward: 113.4375 meanLoss: 0.0113 ExploreP: 0.0969\n",
      "Episode: 479 meanReward: 112.6562 meanLoss: 0.0164 ExploreP: 0.0962\n",
      "Episode: 480 meanReward: 110.4062 meanLoss: 0.0154 ExploreP: 0.0954\n",
      "Episode: 481 meanReward: 109.5938 meanLoss: 0.0143 ExploreP: 0.0946\n",
      "Episode: 482 meanReward: 108.4062 meanLoss: 0.0124 ExploreP: 0.0937\n",
      "Episode: 483 meanReward: 105.8750 meanLoss: 0.0107 ExploreP: 0.0926\n",
      "Episode: 484 meanReward: 102.8438 meanLoss: 0.0118 ExploreP: 0.0917\n",
      "Episode: 485 meanReward: 100.2500 meanLoss: 0.0075 ExploreP: 0.0906\n",
      "Episode: 486 meanReward: 90.5000 meanLoss: 0.0173 ExploreP: 0.0898\n",
      "Episode: 487 meanReward: 78.0000 meanLoss: 0.0108 ExploreP: 0.0890\n",
      "Episode: 488 meanReward: 80.5938 meanLoss: 0.0079 ExploreP: 0.0880\n",
      "Episode: 489 meanReward: 83.7188 meanLoss: 0.0123 ExploreP: 0.0872\n",
      "Episode: 490 meanReward: 85.6562 meanLoss: 0.0187 ExploreP: 0.0866\n",
      "Episode: 491 meanReward: 88.0000 meanLoss: 0.0197 ExploreP: 0.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 492 meanReward: 90.6250 meanLoss: 0.0156 ExploreP: 0.0852\n",
      "Episode: 493 meanReward: 93.1875 meanLoss: 0.0138 ExploreP: 0.0845\n",
      "Episode: 494 meanReward: 93.6250 meanLoss: 0.0131 ExploreP: 0.0837\n",
      "Episode: 495 meanReward: 94.4062 meanLoss: 0.0112 ExploreP: 0.0829\n",
      "Episode: 496 meanReward: 96.1875 meanLoss: 0.0127 ExploreP: 0.0822\n",
      "Episode: 497 meanReward: 98.3750 meanLoss: 0.0144 ExploreP: 0.0815\n",
      "Episode: 498 meanReward: 97.2500 meanLoss: 0.0181 ExploreP: 0.0810\n",
      "Episode: 499 meanReward: 96.9688 meanLoss: 0.0162 ExploreP: 0.0804\n",
      "Episode: 500 meanReward: 97.3438 meanLoss: 0.0121 ExploreP: 0.0796\n",
      "Episode: 501 meanReward: 97.6562 meanLoss: 0.0170 ExploreP: 0.0790\n",
      "Episode: 502 meanReward: 96.8125 meanLoss: 0.0176 ExploreP: 0.0784\n",
      "Episode: 503 meanReward: 98.7812 meanLoss: 0.0086 ExploreP: 0.0776\n",
      "Episode: 504 meanReward: 97.7812 meanLoss: 0.0528 ExploreP: 0.0774\n",
      "Episode: 505 meanReward: 95.9062 meanLoss: 0.0741 ExploreP: 0.0773\n",
      "Episode: 506 meanReward: 93.7188 meanLoss: 0.1036 ExploreP: 0.0772\n",
      "Episode: 507 meanReward: 91.1562 meanLoss: 0.0981 ExploreP: 0.0771\n",
      "Episode: 508 meanReward: 88.2812 meanLoss: 0.1244 ExploreP: 0.0771\n",
      "Episode: 509 meanReward: 84.9688 meanLoss: 0.1346 ExploreP: 0.0770\n",
      "Episode: 510 meanReward: 81.6562 meanLoss: 0.1339 ExploreP: 0.0769\n",
      "Episode: 511 meanReward: 79.5312 meanLoss: 0.1270 ExploreP: 0.0768\n",
      "Episode: 512 meanReward: 77.0000 meanLoss: 0.1243 ExploreP: 0.0768\n",
      "Episode: 513 meanReward: 74.2500 meanLoss: 0.1345 ExploreP: 0.0767\n",
      "Episode: 514 meanReward: 71.1250 meanLoss: 0.1479 ExploreP: 0.0766\n",
      "Episode: 515 meanReward: 67.4375 meanLoss: 0.1497 ExploreP: 0.0766\n",
      "Episode: 516 meanReward: 64.2188 meanLoss: 0.1455 ExploreP: 0.0765\n",
      "Episode: 517 meanReward: 60.4062 meanLoss: 0.1432 ExploreP: 0.0764\n",
      "Episode: 518 meanReward: 57.5312 meanLoss: 0.1417 ExploreP: 0.0764\n",
      "Episode: 519 meanReward: 54.7188 meanLoss: 0.1595 ExploreP: 0.0763\n",
      "Episode: 520 meanReward: 51.2812 meanLoss: 0.1512 ExploreP: 0.0762\n",
      "Episode: 521 meanReward: 47.9688 meanLoss: 0.1517 ExploreP: 0.0762\n",
      "Episode: 522 meanReward: 46.0312 meanLoss: 0.1411 ExploreP: 0.0761\n",
      "Episode: 523 meanReward: 43.6250 meanLoss: 0.1503 ExploreP: 0.0760\n",
      "Episode: 524 meanReward: 40.9062 meanLoss: 0.1483 ExploreP: 0.0760\n",
      "Episode: 525 meanReward: 38.1562 meanLoss: 0.1453 ExploreP: 0.0759\n",
      "Episode: 526 meanReward: 35.1875 meanLoss: 0.1508 ExploreP: 0.0758\n",
      "Episode: 527 meanReward: 32.1562 meanLoss: 0.1471 ExploreP: 0.0758\n",
      "Episode: 528 meanReward: 29.2812 meanLoss: 0.1305 ExploreP: 0.0757\n",
      "Episode: 529 meanReward: 26.9688 meanLoss: 0.1334 ExploreP: 0.0756\n",
      "Episode: 530 meanReward: 24.9062 meanLoss: 0.1312 ExploreP: 0.0756\n",
      "Episode: 531 meanReward: 22.3750 meanLoss: 0.1317 ExploreP: 0.0755\n",
      "Episode: 532 meanReward: 19.3750 meanLoss: 0.1362 ExploreP: 0.0754\n",
      "Episode: 533 meanReward: 16.9688 meanLoss: 0.1422 ExploreP: 0.0754\n",
      "Episode: 534 meanReward: 14.5000 meanLoss: 0.1346 ExploreP: 0.0753\n",
      "Episode: 535 meanReward: 11.0312 meanLoss: 0.1248 ExploreP: 0.0752\n",
      "Episode: 536 meanReward: 10.3750 meanLoss: 0.1240 ExploreP: 0.0752\n",
      "Episode: 537 meanReward: 10.1875 meanLoss: 0.1330 ExploreP: 0.0751\n",
      "Episode: 538 meanReward: 10.2188 meanLoss: 0.1331 ExploreP: 0.0750\n",
      "Episode: 539 meanReward: 10.2188 meanLoss: 0.1202 ExploreP: 0.0750\n",
      "Episode: 540 meanReward: 10.2188 meanLoss: 0.1181 ExploreP: 0.0749\n",
      "Episode: 541 meanReward: 10.4688 meanLoss: 0.0917 ExploreP: 0.0748\n",
      "Episode: 542 meanReward: 10.7188 meanLoss: 0.0820 ExploreP: 0.0747\n",
      "Episode: 543 meanReward: 12.1562 meanLoss: 0.0483 ExploreP: 0.0743\n",
      "Episode: 544 meanReward: 14.1250 meanLoss: 0.0276 ExploreP: 0.0738\n",
      "Episode: 545 meanReward: 14.9688 meanLoss: 0.0387 ExploreP: 0.0736\n",
      "Episode: 546 meanReward: 15.1250 meanLoss: 0.0657 ExploreP: 0.0735\n",
      "Episode: 547 meanReward: 15.0938 meanLoss: 0.1068 ExploreP: 0.0734\n",
      "Episode: 548 meanReward: 17.2500 meanLoss: 0.0463 ExploreP: 0.0729\n",
      "Episode: 549 meanReward: 19.0625 meanLoss: 0.0234 ExploreP: 0.0725\n",
      "Episode: 550 meanReward: 21.3438 meanLoss: 0.0190 ExploreP: 0.0720\n",
      "Episode: 551 meanReward: 23.8750 meanLoss: 0.0182 ExploreP: 0.0714\n",
      "Episode: 552 meanReward: 26.4688 meanLoss: 0.0162 ExploreP: 0.0709\n",
      "Episode: 553 meanReward: 29.1875 meanLoss: 0.0144 ExploreP: 0.0703\n",
      "Episode: 554 meanReward: 32.1562 meanLoss: 0.0137 ExploreP: 0.0696\n",
      "Episode: 555 meanReward: 34.5938 meanLoss: 0.0161 ExploreP: 0.0691\n",
      "Episode: 556 meanReward: 37.5000 meanLoss: 0.0137 ExploreP: 0.0685\n",
      "Episode: 557 meanReward: 40.1250 meanLoss: 0.0148 ExploreP: 0.0680\n",
      "Episode: 558 meanReward: 43.1250 meanLoss: 0.0131 ExploreP: 0.0674\n",
      "Episode: 559 meanReward: 45.7188 meanLoss: 0.0150 ExploreP: 0.0668\n",
      "Episode: 560 meanReward: 47.3750 meanLoss: 0.0169 ExploreP: 0.0665\n",
      "Episode: 561 meanReward: 50.1562 meanLoss: 0.0207 ExploreP: 0.0659\n",
      "Episode: 562 meanReward: 52.9062 meanLoss: 0.0111 ExploreP: 0.0654\n",
      "Episode: 563 meanReward: 55.9375 meanLoss: 0.0136 ExploreP: 0.0648\n",
      "Episode: 564 meanReward: 58.9688 meanLoss: 0.0131 ExploreP: 0.0642\n",
      "Episode: 565 meanReward: 61.9688 meanLoss: 0.0126 ExploreP: 0.0636\n",
      "Episode: 566 meanReward: 65.0312 meanLoss: 0.0121 ExploreP: 0.0630\n",
      "Episode: 567 meanReward: 68.0938 meanLoss: 0.0118 ExploreP: 0.0625\n",
      "Episode: 568 meanReward: 72.0312 meanLoss: 0.0066 ExploreP: 0.0618\n",
      "Episode: 569 meanReward: 74.3438 meanLoss: 0.0134 ExploreP: 0.0613\n",
      "Episode: 570 meanReward: 77.5312 meanLoss: 0.0096 ExploreP: 0.0607\n",
      "Episode: 571 meanReward: 80.0000 meanLoss: 0.0114 ExploreP: 0.0603\n",
      "Episode: 572 meanReward: 82.1250 meanLoss: 0.0241 ExploreP: 0.0599\n",
      "Episode: 573 meanReward: 85.3438 meanLoss: 0.0088 ExploreP: 0.0593\n",
      "Episode: 574 meanReward: 88.0312 meanLoss: 0.0135 ExploreP: 0.0588\n",
      "Episode: 575 meanReward: 88.5938 meanLoss: 0.0221 ExploreP: 0.0584\n",
      "Episode: 576 meanReward: 88.7812 meanLoss: 0.0175 ExploreP: 0.0580\n",
      "Episode: 577 meanReward: 90.2500 meanLoss: 0.0175 ExploreP: 0.0576\n",
      "Episode: 578 meanReward: 93.0938 meanLoss: 0.0141 ExploreP: 0.0571\n",
      "Episode: 579 meanReward: 96.9688 meanLoss: 0.0081 ExploreP: 0.0565\n",
      "Episode: 580 meanReward: 97.5938 meanLoss: 0.0098 ExploreP: 0.0560\n",
      "Episode: 581 meanReward: 98.1562 meanLoss: 0.0170 ExploreP: 0.0557\n",
      "Episode: 582 meanReward: 98.4375 meanLoss: 0.0148 ExploreP: 0.0552\n",
      "Episode: 583 meanReward: 98.2500 meanLoss: 0.0146 ExploreP: 0.0549\n",
      "Episode: 584 meanReward: 98.0312 meanLoss: 0.0150 ExploreP: 0.0545\n",
      "Episode: 585 meanReward: 98.0938 meanLoss: 0.0137 ExploreP: 0.0540\n",
      "Episode: 586 meanReward: 98.8125 meanLoss: 0.0104 ExploreP: 0.0535\n",
      "Episode: 587 meanReward: 100.4688 meanLoss: 0.0090 ExploreP: 0.0529\n",
      "Episode: 588 meanReward: 101.1562 meanLoss: 0.0099 ExploreP: 0.0523\n",
      "Episode: 589 meanReward: 103.6562 meanLoss: 0.0077 ExploreP: 0.0516\n",
      "Episode: 590 meanReward: 105.3125 meanLoss: 0.0080 ExploreP: 0.0509\n",
      "Episode: 591 meanReward: 108.7812 meanLoss: 0.0063 ExploreP: 0.0501\n",
      "Episode: 592 meanReward: 111.0312 meanLoss: 0.0090 ExploreP: 0.0496\n",
      "Episode: 593 meanReward: 114.8125 meanLoss: 0.0060 ExploreP: 0.0487\n",
      "Episode: 594 meanReward: 114.8125 meanLoss: 0.0144 ExploreP: 0.0483\n",
      "Episode: 595 meanReward: 114.5312 meanLoss: 0.0089 ExploreP: 0.0480\n",
      "Episode: 596 meanReward: 115.7500 meanLoss: 0.0097 ExploreP: 0.0474\n",
      "Episode: 597 meanReward: 117.6562 meanLoss: 0.0083 ExploreP: 0.0468\n",
      "Episode: 598 meanReward: 121.6875 meanLoss: 0.0055 ExploreP: 0.0459\n",
      "Episode: 599 meanReward: 123.7188 meanLoss: 0.0079 ExploreP: 0.0453\n",
      "Episode: 600 meanReward: 130.0938 meanLoss: 0.0032 ExploreP: 0.0441\n",
      "Episode: 601 meanReward: 139.7500 meanLoss: 0.0025 ExploreP: 0.0428\n",
      "Episode: 602 meanReward: 141.6875 meanLoss: 0.0054 ExploreP: 0.0422\n",
      "Episode: 603 meanReward: 151.4062 meanLoss: 0.0034 ExploreP: 0.0410\n",
      "Episode: 604 meanReward: 159.3750 meanLoss: 0.0042 ExploreP: 0.0400\n",
      "Episode: 605 meanReward: 161.3438 meanLoss: 0.0073 ExploreP: 0.0394\n",
      "Episode: 606 meanReward: 162.9688 meanLoss: 0.0073 ExploreP: 0.0390\n",
      "Episode: 607 meanReward: 170.0312 meanLoss: 0.0039 ExploreP: 0.0381\n",
      "Episode: 608 meanReward: 175.9688 meanLoss: 0.0041 ExploreP: 0.0373\n",
      "Episode: 609 meanReward: 189.0000 meanLoss: 0.0019 ExploreP: 0.0360\n",
      "Episode: 610 meanReward: 186.3750 meanLoss: 0.0678 ExploreP: 0.0360\n",
      "Episode: 611 meanReward: 182.5312 meanLoss: 0.0799 ExploreP: 0.0359\n",
      "Episode: 612 meanReward: 179.7188 meanLoss: 0.0894 ExploreP: 0.0359\n",
      "Episode: 613 meanReward: 177.4688 meanLoss: 0.1182 ExploreP: 0.0359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 614 meanReward: 175.0938 meanLoss: 0.0799 ExploreP: 0.0358\n",
      "Episode: 615 meanReward: 172.8438 meanLoss: 0.0839 ExploreP: 0.0358\n",
      "Episode: 616 meanReward: 170.5625 meanLoss: 0.0895 ExploreP: 0.0358\n",
      "Episode: 617 meanReward: 167.9375 meanLoss: 0.1026 ExploreP: 0.0357\n",
      "Episode: 618 meanReward: 164.3438 meanLoss: 0.1017 ExploreP: 0.0357\n",
      "Episode: 619 meanReward: 160.5312 meanLoss: 0.0877 ExploreP: 0.0356\n",
      "Episode: 620 meanReward: 157.0938 meanLoss: 0.0762 ExploreP: 0.0356\n",
      "Episode: 621 meanReward: 152.1875 meanLoss: 0.0820 ExploreP: 0.0356\n",
      "Episode: 622 meanReward: 147.7188 meanLoss: 0.0785 ExploreP: 0.0355\n",
      "Episode: 623 meanReward: 141.9375 meanLoss: 0.0796 ExploreP: 0.0355\n",
      "Episode: 624 meanReward: 138.5312 meanLoss: 0.0588 ExploreP: 0.0354\n",
      "Episode: 625 meanReward: 132.5000 meanLoss: 0.0366 ExploreP: 0.0353\n",
      "Episode: 626 meanReward: 131.9375 meanLoss: 0.0274 ExploreP: 0.0351\n",
      "Episode: 627 meanReward: 129.8750 meanLoss: 0.0325 ExploreP: 0.0351\n",
      "Episode: 628 meanReward: 125.7188 meanLoss: 0.0707 ExploreP: 0.0350\n",
      "Episode: 629 meanReward: 120.8125 meanLoss: 0.0998 ExploreP: 0.0350\n",
      "Episode: 630 meanReward: 113.6250 meanLoss: 0.1067 ExploreP: 0.0350\n",
      "Episode: 631 meanReward: 108.5000 meanLoss: 0.1364 ExploreP: 0.0350\n",
      "Episode: 632 meanReward: 98.2188 meanLoss: 0.1476 ExploreP: 0.0349\n",
      "Episode: 633 meanReward: 86.2812 meanLoss: 0.1402 ExploreP: 0.0349\n",
      "Episode: 634 meanReward: 81.0625 meanLoss: 0.1323 ExploreP: 0.0349\n",
      "Episode: 635 meanReward: 68.9062 meanLoss: 0.1276 ExploreP: 0.0349\n",
      "Episode: 636 meanReward: 58.7500 meanLoss: 0.1280 ExploreP: 0.0348\n",
      "Episode: 637 meanReward: 53.3125 meanLoss: 0.1356 ExploreP: 0.0348\n",
      "Episode: 638 meanReward: 48.6562 meanLoss: 0.1320 ExploreP: 0.0348\n",
      "Episode: 639 meanReward: 39.4688 meanLoss: 0.1478 ExploreP: 0.0348\n",
      "Episode: 640 meanReward: 31.4062 meanLoss: 0.1442 ExploreP: 0.0347\n",
      "Episode: 641 meanReward: 16.0938 meanLoss: 0.1494 ExploreP: 0.0347\n",
      "Episode: 642 meanReward: 15.7812 meanLoss: 0.1387 ExploreP: 0.0347\n",
      "Episode: 643 meanReward: 15.8125 meanLoss: 0.1286 ExploreP: 0.0347\n",
      "Episode: 644 meanReward: 15.8125 meanLoss: 0.1207 ExploreP: 0.0346\n",
      "Episode: 645 meanReward: 15.7188 meanLoss: 0.1153 ExploreP: 0.0346\n",
      "Episode: 646 meanReward: 15.5000 meanLoss: 0.1106 ExploreP: 0.0346\n",
      "Episode: 647 meanReward: 15.3750 meanLoss: 0.1131 ExploreP: 0.0346\n",
      "Episode: 648 meanReward: 15.2812 meanLoss: 0.1371 ExploreP: 0.0345\n",
      "Episode: 649 meanReward: 15.1562 meanLoss: 0.1344 ExploreP: 0.0345\n",
      "Episode: 650 meanReward: 15.0625 meanLoss: 0.1251 ExploreP: 0.0345\n",
      "Episode: 651 meanReward: 14.7500 meanLoss: 0.1174 ExploreP: 0.0345\n",
      "Episode: 652 meanReward: 14.5625 meanLoss: 0.1149 ExploreP: 0.0344\n",
      "Episode: 653 meanReward: 14.3438 meanLoss: 0.1324 ExploreP: 0.0344\n",
      "Episode: 654 meanReward: 14.1562 meanLoss: 0.1456 ExploreP: 0.0344\n",
      "Episode: 655 meanReward: 13.8125 meanLoss: 0.1389 ExploreP: 0.0344\n",
      "Episode: 656 meanReward: 13.2812 meanLoss: 0.1406 ExploreP: 0.0343\n",
      "Episode: 657 meanReward: 12.6562 meanLoss: 0.1371 ExploreP: 0.0343\n",
      "Episode: 658 meanReward: 10.5000 meanLoss: 0.1283 ExploreP: 0.0343\n",
      "Episode: 659 meanReward: 9.8438 meanLoss: 0.1244 ExploreP: 0.0343\n",
      "Episode: 660 meanReward: 9.7188 meanLoss: 0.1237 ExploreP: 0.0343\n",
      "Episode: 661 meanReward: 9.6562 meanLoss: 0.1256 ExploreP: 0.0342\n",
      "Episode: 662 meanReward: 9.7812 meanLoss: 0.1384 ExploreP: 0.0342\n",
      "Episode: 663 meanReward: 9.8750 meanLoss: 0.1096 ExploreP: 0.0342\n",
      "Episode: 664 meanReward: 9.9375 meanLoss: 0.1036 ExploreP: 0.0341\n",
      "Episode: 665 meanReward: 10.0938 meanLoss: 0.0999 ExploreP: 0.0341\n",
      "Episode: 666 meanReward: 12.1250 meanLoss: 0.0286 ExploreP: 0.0339\n",
      "Episode: 667 meanReward: 14.3438 meanLoss: 0.0167 ExploreP: 0.0337\n",
      "Episode: 668 meanReward: 17.5312 meanLoss: 0.0113 ExploreP: 0.0335\n",
      "Episode: 669 meanReward: 21.5312 meanLoss: 0.0083 ExploreP: 0.0332\n",
      "Episode: 670 meanReward: 36.8750 meanLoss: 0.0033 ExploreP: 0.0320\n",
      "Episode: 671 meanReward: 40.4688 meanLoss: 0.0147 ExploreP: 0.0318\n",
      "Episode: 672 meanReward: 44.0938 meanLoss: 0.0097 ExploreP: 0.0315\n",
      "Episode: 673 meanReward: 47.0625 meanLoss: 0.0131 ExploreP: 0.0313\n",
      "Episode: 674 meanReward: 49.9375 meanLoss: 0.0128 ExploreP: 0.0310\n",
      "Episode: 675 meanReward: 53.2188 meanLoss: 0.0107 ExploreP: 0.0308\n",
      "Episode: 676 meanReward: 56.9375 meanLoss: 0.0070 ExploreP: 0.0305\n",
      "Episode: 677 meanReward: 60.0312 meanLoss: 0.0092 ExploreP: 0.0303\n",
      "Episode: 678 meanReward: 62.6250 meanLoss: 0.0138 ExploreP: 0.0301\n",
      "Episode: 679 meanReward: 65.6250 meanLoss: 0.0091 ExploreP: 0.0299\n",
      "Episode: 680 meanReward: 66.7188 meanLoss: 0.0312 ExploreP: 0.0298\n",
      "Episode: 681 meanReward: 67.2188 meanLoss: 0.0578 ExploreP: 0.0298\n",
      "Episode: 682 meanReward: 70.2188 meanLoss: 0.0208 ExploreP: 0.0296\n",
      "Episode: 683 meanReward: 73.5625 meanLoss: 0.0101 ExploreP: 0.0293\n",
      "Episode: 684 meanReward: 76.0938 meanLoss: 0.0172 ExploreP: 0.0292\n",
      "Episode: 685 meanReward: 79.7188 meanLoss: 0.0124 ExploreP: 0.0289\n",
      "Episode: 686 meanReward: 83.6250 meanLoss: 0.0084 ExploreP: 0.0287\n",
      "Episode: 687 meanReward: 86.7500 meanLoss: 0.0117 ExploreP: 0.0285\n",
      "Episode: 688 meanReward: 91.9062 meanLoss: 0.0105 ExploreP: 0.0281\n",
      "Episode: 689 meanReward: 107.2500 meanLoss: 0.0033 ExploreP: 0.0273\n",
      "Episode: 690 meanReward: 122.5625 meanLoss: 0.0059 ExploreP: 0.0264\n",
      "Episode: 691 meanReward: 126.0938 meanLoss: 0.0138 ExploreP: 0.0262\n",
      "Episode: 692 meanReward: 129.6562 meanLoss: 0.0145 ExploreP: 0.0260\n",
      "Episode: 693 meanReward: 145.0312 meanLoss: 0.0023 ExploreP: 0.0252\n",
      "Episode: 694 meanReward: 160.2500 meanLoss: 0.0044 ExploreP: 0.0245\n",
      "Episode: 695 meanReward: 160.2812 meanLoss: 0.0649 ExploreP: 0.0245\n",
      "Episode: 696 meanReward: 160.1562 meanLoss: 0.0854 ExploreP: 0.0245\n",
      "Episode: 697 meanReward: 160.0000 meanLoss: 0.1137 ExploreP: 0.0245\n",
      "Episode: 698 meanReward: 158.0000 meanLoss: 0.1225 ExploreP: 0.0244\n",
      "Episode: 699 meanReward: 155.7500 meanLoss: 0.1235 ExploreP: 0.0244\n",
      "Episode: 700 meanReward: 152.6562 meanLoss: 0.1203 ExploreP: 0.0244\n",
      "Episode: 701 meanReward: 148.7188 meanLoss: 0.1128 ExploreP: 0.0244\n",
      "Episode: 702 meanReward: 133.4062 meanLoss: 0.1070 ExploreP: 0.0244\n",
      "Episode: 703 meanReward: 129.8438 meanLoss: 0.1187 ExploreP: 0.0244\n",
      "Episode: 704 meanReward: 126.2812 meanLoss: 0.1154 ExploreP: 0.0243\n",
      "Episode: 705 meanReward: 123.3750 meanLoss: 0.1102 ExploreP: 0.0243\n",
      "Episode: 706 meanReward: 120.4688 meanLoss: 0.1024 ExploreP: 0.0243\n",
      "Episode: 707 meanReward: 117.2188 meanLoss: 0.1065 ExploreP: 0.0243\n",
      "Episode: 708 meanReward: 113.4688 meanLoss: 0.1212 ExploreP: 0.0243\n",
      "Episode: 709 meanReward: 110.3750 meanLoss: 0.1168 ExploreP: 0.0243\n",
      "Episode: 710 meanReward: 107.8438 meanLoss: 0.1081 ExploreP: 0.0242\n",
      "Episode: 711 meanReward: 104.8750 meanLoss: 0.1057 ExploreP: 0.0242\n",
      "Episode: 712 meanReward: 103.7812 meanLoss: 0.1247 ExploreP: 0.0242\n",
      "Episode: 713 meanReward: 103.2812 meanLoss: 0.1160 ExploreP: 0.0242\n",
      "Episode: 714 meanReward: 100.2500 meanLoss: 0.1259 ExploreP: 0.0242\n",
      "Episode: 715 meanReward: 96.9375 meanLoss: 0.1259 ExploreP: 0.0242\n",
      "Episode: 716 meanReward: 94.4688 meanLoss: 0.1248 ExploreP: 0.0242\n",
      "Episode: 717 meanReward: 90.8750 meanLoss: 0.1233 ExploreP: 0.0241\n",
      "Episode: 718 meanReward: 87.0938 meanLoss: 0.1110 ExploreP: 0.0241\n",
      "Episode: 719 meanReward: 84.0938 meanLoss: 0.1003 ExploreP: 0.0241\n",
      "Episode: 720 meanReward: 79.0000 meanLoss: 0.1044 ExploreP: 0.0241\n",
      "Episode: 721 meanReward: 63.7500 meanLoss: 0.1065 ExploreP: 0.0241\n",
      "Episode: 722 meanReward: 48.5625 meanLoss: 0.1093 ExploreP: 0.0241\n",
      "Episode: 723 meanReward: 45.0938 meanLoss: 0.1036 ExploreP: 0.0240\n",
      "Episode: 724 meanReward: 41.5625 meanLoss: 0.1058 ExploreP: 0.0240\n",
      "Episode: 725 meanReward: 26.2500 meanLoss: 0.1213 ExploreP: 0.0240\n",
      "Episode: 726 meanReward: 10.9062 meanLoss: 0.1241 ExploreP: 0.0240\n",
      "Episode: 727 meanReward: 10.8125 meanLoss: 0.1298 ExploreP: 0.0240\n",
      "Episode: 728 meanReward: 10.9688 meanLoss: 0.1167 ExploreP: 0.0240\n",
      "Episode: 729 meanReward: 11.0625 meanLoss: 0.1064 ExploreP: 0.0239\n",
      "Episode: 730 meanReward: 11.1875 meanLoss: 0.0942 ExploreP: 0.0239\n",
      "Episode: 731 meanReward: 14.3125 meanLoss: 0.0198 ExploreP: 0.0238\n",
      "Episode: 732 meanReward: 14.4688 meanLoss: 0.0372 ExploreP: 0.0238\n",
      "Episode: 733 meanReward: 17.0938 meanLoss: 0.0226 ExploreP: 0.0236\n",
      "Episode: 734 meanReward: 17.1250 meanLoss: 0.0614 ExploreP: 0.0236\n",
      "Episode: 735 meanReward: 17.0938 meanLoss: 0.0878 ExploreP: 0.0236\n",
      "Episode: 736 meanReward: 17.0312 meanLoss: 0.1195 ExploreP: 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 737 meanReward: 16.9062 meanLoss: 0.1411 ExploreP: 0.0236\n",
      "Episode: 738 meanReward: 16.9375 meanLoss: 0.1344 ExploreP: 0.0236\n",
      "Episode: 739 meanReward: 16.8125 meanLoss: 0.1384 ExploreP: 0.0235\n",
      "Episode: 740 meanReward: 16.9062 meanLoss: 0.1297 ExploreP: 0.0235\n",
      "Episode: 741 meanReward: 16.9062 meanLoss: 0.1293 ExploreP: 0.0235\n",
      "Episode: 742 meanReward: 16.8438 meanLoss: 0.1261 ExploreP: 0.0235\n",
      "Episode: 743 meanReward: 19.9375 meanLoss: 0.0280 ExploreP: 0.0234\n",
      "Episode: 744 meanReward: 24.0000 meanLoss: 0.0096 ExploreP: 0.0232\n",
      "Episode: 745 meanReward: 27.1562 meanLoss: 0.0151 ExploreP: 0.0230\n",
      "Episode: 746 meanReward: 27.5312 meanLoss: 0.0411 ExploreP: 0.0230\n",
      "Episode: 747 meanReward: 27.5625 meanLoss: 0.0707 ExploreP: 0.0230\n",
      "Episode: 748 meanReward: 30.3438 meanLoss: 0.0247 ExploreP: 0.0228\n",
      "Episode: 749 meanReward: 33.8438 meanLoss: 0.0079 ExploreP: 0.0227\n",
      "Episode: 750 meanReward: 40.4375 meanLoss: 0.0076 ExploreP: 0.0224\n",
      "Episode: 751 meanReward: 55.6562 meanLoss: 0.0035 ExploreP: 0.0218\n",
      "Episode: 752 meanReward: 58.8125 meanLoss: 0.0178 ExploreP: 0.0217\n",
      "Episode: 753 meanReward: 61.8438 meanLoss: 0.0081 ExploreP: 0.0215\n",
      "Episode: 754 meanReward: 62.1562 meanLoss: 0.0463 ExploreP: 0.0215\n",
      "Episode: 755 meanReward: 64.9062 meanLoss: 0.0151 ExploreP: 0.0214\n",
      "Episode: 756 meanReward: 65.4688 meanLoss: 0.0412 ExploreP: 0.0214\n",
      "Episode: 757 meanReward: 65.7188 meanLoss: 0.0548 ExploreP: 0.0214\n",
      "Episode: 758 meanReward: 68.4688 meanLoss: 0.0275 ExploreP: 0.0212\n",
      "Episode: 759 meanReward: 71.2812 meanLoss: 0.0139 ExploreP: 0.0211\n",
      "Episode: 760 meanReward: 74.4062 meanLoss: 0.0104 ExploreP: 0.0210\n",
      "Episode: 761 meanReward: 77.9062 meanLoss: 0.0075 ExploreP: 0.0209\n",
      "Episode: 762 meanReward: 80.5312 meanLoss: 0.0196 ExploreP: 0.0208\n",
      "Episode: 763 meanReward: 80.5312 meanLoss: 0.0131 ExploreP: 0.0206\n",
      "Episode: 764 meanReward: 83.8438 meanLoss: 0.0139 ExploreP: 0.0205\n",
      "Episode: 765 meanReward: 81.4375 meanLoss: 0.0386 ExploreP: 0.0205\n",
      "Episode: 766 meanReward: 85.7188 meanLoss: 0.0114 ExploreP: 0.0203\n",
      "Episode: 767 meanReward: 91.0312 meanLoss: 0.0073 ExploreP: 0.0202\n",
      "Episode: 768 meanReward: 106.3438 meanLoss: 0.0032 ExploreP: 0.0197\n",
      "Episode: 769 meanReward: 110.5625 meanLoss: 0.0093 ExploreP: 0.0195\n",
      "Episode: 770 meanReward: 115.7812 meanLoss: 0.0118 ExploreP: 0.0194\n",
      "Episode: 771 meanReward: 120.8750 meanLoss: 0.0102 ExploreP: 0.0192\n",
      "Episode: 772 meanReward: 136.0938 meanLoss: 0.0036 ExploreP: 0.0187\n",
      "Episode: 773 meanReward: 151.4062 meanLoss: 0.0032 ExploreP: 0.0183\n",
      "Episode: 774 meanReward: 151.8750 meanLoss: 0.0551 ExploreP: 0.0183\n",
      "Episode: 775 meanReward: 151.8125 meanLoss: 0.0125 ExploreP: 0.0182\n",
      "Episode: 776 meanReward: 163.0625 meanLoss: 0.0032 ExploreP: 0.0178\n",
      "Episode: 777 meanReward: 175.2188 meanLoss: 0.0045 ExploreP: 0.0174\n",
      "Episode: 778 meanReward: 190.1562 meanLoss: 0.0040 ExploreP: 0.0171\n",
      "Episode: 779 meanReward: 199.1250 meanLoss: 0.0050 ExploreP: 0.0169\n",
      "Episode: 780 meanReward: 211.6250 meanLoss: 0.0030 ExploreP: 0.0165\n",
      "Episode: 781 meanReward: 220.9688 meanLoss: 0.0049 ExploreP: 0.0163\n",
      "Episode: 782 meanReward: 217.8438 meanLoss: 0.0095 ExploreP: 0.0162\n",
      "Episode: 783 meanReward: 205.8750 meanLoss: 0.0105 ExploreP: 0.0161\n",
      "Episode: 784 meanReward: 206.1875 meanLoss: 0.0091 ExploreP: 0.0160\n",
      "Episode: 785 meanReward: 207.1250 meanLoss: 0.0096 ExploreP: 0.0160\n",
      "Episode: 786 meanReward: 210.6875 meanLoss: 0.0085 ExploreP: 0.0159\n",
      "Episode: 787 meanReward: 211.6875 meanLoss: 0.0077 ExploreP: 0.0158\n",
      "Episode: 788 meanReward: 215.7500 meanLoss: 0.0103 ExploreP: 0.0157\n",
      "Episode: 789 meanReward: 228.2812 meanLoss: 0.0037 ExploreP: 0.0155\n",
      "Episode: 790 meanReward: 240.8750 meanLoss: 0.0022 ExploreP: 0.0152\n",
      "Episode: 791 meanReward: 240.6250 meanLoss: 0.0199 ExploreP: 0.0152\n",
      "Episode: 792 meanReward: 239.7500 meanLoss: 0.0191 ExploreP: 0.0151\n",
      "Episode: 793 meanReward: 238.7188 meanLoss: 0.0129 ExploreP: 0.0151\n",
      "Episode: 794 meanReward: 251.2500 meanLoss: 0.0028 ExploreP: 0.0148\n",
      "Episode: 795 meanReward: 248.4375 meanLoss: 0.0647 ExploreP: 0.0148\n",
      "Episode: 796 meanReward: 244.9375 meanLoss: 0.0754 ExploreP: 0.0148\n",
      "Episode: 797 meanReward: 244.6562 meanLoss: 0.0632 ExploreP: 0.0148\n",
      "Episode: 798 meanReward: 240.4062 meanLoss: 0.1012 ExploreP: 0.0148\n",
      "Episode: 799 meanReward: 235.1250 meanLoss: 0.1048 ExploreP: 0.0148\n",
      "Episode: 800 meanReward: 219.8438 meanLoss: 0.1026 ExploreP: 0.0148\n",
      "Episode: 801 meanReward: 215.6562 meanLoss: 0.1111 ExploreP: 0.0148\n",
      "Episode: 802 meanReward: 210.4688 meanLoss: 0.1145 ExploreP: 0.0148\n",
      "Episode: 803 meanReward: 205.4375 meanLoss: 0.1168 ExploreP: 0.0148\n",
      "Episode: 804 meanReward: 190.1562 meanLoss: 0.1097 ExploreP: 0.0148\n",
      "Episode: 805 meanReward: 174.8438 meanLoss: 0.1253 ExploreP: 0.0148\n",
      "Episode: 806 meanReward: 174.4375 meanLoss: 0.1101 ExploreP: 0.0148\n",
      "Episode: 807 meanReward: 171.4375 meanLoss: 0.1113 ExploreP: 0.0147\n",
      "Episode: 808 meanReward: 156.1562 meanLoss: 0.1102 ExploreP: 0.0147\n",
      "Episode: 809 meanReward: 140.9688 meanLoss: 0.1019 ExploreP: 0.0147\n",
      "Episode: 810 meanReward: 125.7188 meanLoss: 0.1000 ExploreP: 0.0147\n",
      "Episode: 811 meanReward: 116.7812 meanLoss: 0.0944 ExploreP: 0.0147\n",
      "Episode: 812 meanReward: 103.4375 meanLoss: 0.0327 ExploreP: 0.0147\n",
      "Episode: 813 meanReward: 92.8438 meanLoss: 0.0180 ExploreP: 0.0146\n",
      "Episode: 814 meanReward: 91.4688 meanLoss: 0.0174 ExploreP: 0.0146\n",
      "Episode: 815 meanReward: 90.5000 meanLoss: 0.0157 ExploreP: 0.0146\n",
      "Episode: 816 meanReward: 87.1562 meanLoss: 0.0468 ExploreP: 0.0146\n",
      "Episode: 817 meanReward: 85.4062 meanLoss: 0.0226 ExploreP: 0.0145\n",
      "Episode: 818 meanReward: 81.5312 meanLoss: 0.0450 ExploreP: 0.0145\n",
      "Episode: 819 meanReward: 80.1250 meanLoss: 0.0229 ExploreP: 0.0145\n",
      "Episode: 820 meanReward: 78.0312 meanLoss: 0.0153 ExploreP: 0.0144\n",
      "Episode: 821 meanReward: 67.9062 meanLoss: 0.0144 ExploreP: 0.0144\n",
      "Episode: 822 meanReward: 53.3125 meanLoss: 0.0404 ExploreP: 0.0144\n",
      "Episode: 823 meanReward: 53.2812 meanLoss: 0.0122 ExploreP: 0.0143\n",
      "Episode: 824 meanReward: 53.7500 meanLoss: 0.0137 ExploreP: 0.0143\n",
      "Episode: 825 meanReward: 54.0312 meanLoss: 0.0133 ExploreP: 0.0143\n",
      "Episode: 826 meanReward: 41.6562 meanLoss: 0.0131 ExploreP: 0.0142\n",
      "Episode: 827 meanReward: 44.5000 meanLoss: 0.0123 ExploreP: 0.0142\n",
      "Episode: 828 meanReward: 47.5312 meanLoss: 0.0125 ExploreP: 0.0141\n",
      "Episode: 829 meanReward: 50.5000 meanLoss: 0.0126 ExploreP: 0.0141\n",
      "Episode: 830 meanReward: 53.3438 meanLoss: 0.0133 ExploreP: 0.0140\n",
      "Episode: 831 meanReward: 56.2188 meanLoss: 0.0130 ExploreP: 0.0140\n",
      "Episode: 832 meanReward: 58.9688 meanLoss: 0.0147 ExploreP: 0.0140\n",
      "Episode: 833 meanReward: 61.9062 meanLoss: 0.0130 ExploreP: 0.0139\n",
      "Episode: 834 meanReward: 64.9062 meanLoss: 0.0123 ExploreP: 0.0139\n",
      "Episode: 835 meanReward: 67.9688 meanLoss: 0.0125 ExploreP: 0.0138\n",
      "Episode: 836 meanReward: 71.1562 meanLoss: 0.0117 ExploreP: 0.0138\n",
      "Episode: 837 meanReward: 74.1250 meanLoss: 0.0125 ExploreP: 0.0138\n",
      "Episode: 838 meanReward: 77.0938 meanLoss: 0.0129 ExploreP: 0.0137\n",
      "Episode: 839 meanReward: 80.0938 meanLoss: 0.0128 ExploreP: 0.0137\n",
      "Episode: 840 meanReward: 83.0312 meanLoss: 0.0126 ExploreP: 0.0136\n",
      "Episode: 841 meanReward: 85.9688 meanLoss: 0.0125 ExploreP: 0.0136\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver() # save the trained model\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=batch_size)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            \n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            actions_logits = sess.run(model.actions_logits, \n",
    "                                      feed_dict = {model.states: next_states, \n",
    "                                                   model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "#             actions_logits = sess.run(model.actions_logits,\n",
    "#                                       feed_dict = {model.states: states, \n",
    "#                                                    model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "#             targetQs = np.max(actions_logits, axis=1)\n",
    "            loss, _ = sess.run([model.loss, model.opt], \n",
    "                               feed_dict = {model.states: states, \n",
    "                                            model.actions: actions,\n",
    "                                            model.targetQs: targetQs,\n",
    "                                            model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'meanReward: {:.4f}'.format(np.mean(episode_reward)),\n",
    "              'meanLoss: {:.4f}'.format(np.mean(loss_batch)),\n",
    "              'ExploreP: {:.4f}'.format(explore_p))\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        if(np.mean(episode_reward) >= 500):\n",
    "            break\n",
    "    \n",
    "    saver.save(sess, 'checkpoints/model2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward:120.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                             model.initial_state: initial_state})\n",
    "        action = np.argmax(action_logits)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "print('total_reward:{}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
