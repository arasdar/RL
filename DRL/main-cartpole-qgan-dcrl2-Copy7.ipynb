{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('actor', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    actions_logits = actor(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ###############################################\n",
    "    next_states_logits = generator(actions=actions_logits, hidden_size=hidden_size, state_size=state_size)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    aloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, \n",
    "                                                                    labels=next_states_labels))\n",
    "    ####################################################\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, action_size=action_size)\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                   labels=rates)) # 0-1\n",
    "    ####################################################\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states, action_size=action_size, \n",
    "                        reuse=True)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs))) # 0-1\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.ones_like(gQs))) # 0-1\n",
    "    #####################################################\n",
    "    next_actions_logits = actor(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    gQs2 = discriminator(actions=next_actions_logits, hidden_size=hidden_size, states=next_states, \n",
    "                         action_size=action_size, reuse=True)\n",
    "    gQs2 = tf.reshape(gQs2, shape=[-1]) * dones\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                     labels=tf.ones_like(gQs2))) # 0-1\n",
    "    return actions_logits, aloss, dloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, a_loss2, d_loss, a_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('actor')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, d_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, d_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.d_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, #model input \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.d_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        d_loss=self.d_loss,\n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample(buffer, batch_size):\n",
    "#     idx = np.random.choice(np.arange(len(buffer)), size=batch_size, replace=False)\n",
    "#     return [buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              a_learning_rate=a_learning_rate, d_learning_rate=d_learning_rate, gamma=gamma)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:20.0000 R:20.0000 rate:0.0400 aloss:1.3855 dloss:1.1384 aloss2:1.6117 exploreP:0.9980\n",
      "Episode:1 meanR:21.0000 R:22.0000 rate:0.0440 aloss:1.3819 dloss:1.1182 aloss2:1.6485 exploreP:0.9959\n",
      "Episode:2 meanR:17.6667 R:11.0000 rate:0.0220 aloss:1.3834 dloss:1.1629 aloss2:1.5335 exploreP:0.9948\n",
      "Episode:3 meanR:20.7500 R:30.0000 rate:0.0600 aloss:1.3829 dloss:1.1161 aloss2:1.6132 exploreP:0.9918\n",
      "Episode:4 meanR:21.8000 R:26.0000 rate:0.0520 aloss:1.3824 dloss:1.1031 aloss2:1.6221 exploreP:0.9893\n",
      "Episode:5 meanR:21.5000 R:20.0000 rate:0.0400 aloss:1.3835 dloss:1.0866 aloss2:1.6331 exploreP:0.9873\n",
      "Episode:6 meanR:20.5714 R:15.0000 rate:0.0300 aloss:1.3838 dloss:1.0820 aloss2:1.6309 exploreP:0.9858\n",
      "Episode:7 meanR:20.2500 R:18.0000 rate:0.0360 aloss:1.3836 dloss:1.0927 aloss2:1.6088 exploreP:0.9841\n",
      "Episode:8 meanR:21.2222 R:29.0000 rate:0.0580 aloss:1.3833 dloss:1.0365 aloss2:1.7264 exploreP:0.9813\n",
      "Episode:9 meanR:20.9000 R:18.0000 rate:0.0360 aloss:1.3795 dloss:1.0079 aloss2:1.7676 exploreP:0.9795\n",
      "Episode:10 meanR:20.7273 R:19.0000 rate:0.0380 aloss:1.3811 dloss:1.0457 aloss2:1.6619 exploreP:0.9777\n",
      "Episode:11 meanR:20.3333 R:16.0000 rate:0.0320 aloss:1.3829 dloss:1.0101 aloss2:1.7271 exploreP:0.9761\n",
      "Episode:12 meanR:20.2308 R:19.0000 rate:0.0380 aloss:1.3839 dloss:0.9969 aloss2:1.7440 exploreP:0.9743\n",
      "Episode:13 meanR:19.6429 R:12.0000 rate:0.0240 aloss:1.3843 dloss:0.9901 aloss2:1.7309 exploreP:0.9731\n",
      "Episode:14 meanR:19.3333 R:15.0000 rate:0.0300 aloss:1.3839 dloss:0.9917 aloss2:1.7032 exploreP:0.9717\n",
      "Episode:15 meanR:20.0625 R:31.0000 rate:0.0620 aloss:1.3836 dloss:0.9645 aloss2:1.8018 exploreP:0.9687\n",
      "Episode:16 meanR:21.2353 R:40.0000 rate:0.0800 aloss:1.3813 dloss:0.9564 aloss2:1.7785 exploreP:0.9649\n",
      "Episode:17 meanR:22.8333 R:50.0000 rate:0.1000 aloss:1.3808 dloss:0.9382 aloss2:1.7955 exploreP:0.9601\n",
      "Episode:18 meanR:23.1053 R:28.0000 rate:0.0560 aloss:1.3814 dloss:0.9007 aloss2:1.8402 exploreP:0.9575\n",
      "Episode:19 meanR:23.3500 R:28.0000 rate:0.0560 aloss:1.3816 dloss:0.8905 aloss2:1.8463 exploreP:0.9548\n",
      "Episode:20 meanR:24.3810 R:45.0000 rate:0.0900 aloss:1.3803 dloss:0.8700 aloss2:1.9035 exploreP:0.9506\n",
      "Episode:21 meanR:24.0000 R:16.0000 rate:0.0320 aloss:1.3818 dloss:0.8753 aloss2:1.8407 exploreP:0.9491\n",
      "Episode:22 meanR:23.4783 R:12.0000 rate:0.0240 aloss:1.3783 dloss:0.8402 aloss2:1.9710 exploreP:0.9480\n",
      "Episode:23 meanR:23.0417 R:13.0000 rate:0.0260 aloss:1.3824 dloss:0.8646 aloss2:1.8788 exploreP:0.9467\n",
      "Episode:24 meanR:23.0400 R:23.0000 rate:0.0460 aloss:1.3806 dloss:0.8458 aloss2:1.9850 exploreP:0.9446\n",
      "Episode:25 meanR:22.6923 R:14.0000 rate:0.0280 aloss:1.3830 dloss:0.8667 aloss2:1.8503 exploreP:0.9433\n",
      "Episode:26 meanR:23.2593 R:38.0000 rate:0.0760 aloss:1.3789 dloss:0.8327 aloss2:2.0084 exploreP:0.9397\n",
      "Episode:27 meanR:23.2500 R:23.0000 rate:0.0460 aloss:1.3797 dloss:0.8187 aloss2:1.9800 exploreP:0.9376\n",
      "Episode:28 meanR:23.1379 R:20.0000 rate:0.0400 aloss:1.3776 dloss:0.8328 aloss2:2.0029 exploreP:0.9358\n",
      "Episode:29 meanR:23.1667 R:24.0000 rate:0.0480 aloss:1.3799 dloss:0.7703 aloss2:2.0879 exploreP:0.9335\n",
      "Episode:30 meanR:23.3548 R:29.0000 rate:0.0580 aloss:1.3795 dloss:0.7764 aloss2:2.0905 exploreP:0.9309\n",
      "Episode:31 meanR:23.0312 R:13.0000 rate:0.0260 aloss:1.3747 dloss:0.7822 aloss2:2.1126 exploreP:0.9297\n",
      "Episode:32 meanR:23.0000 R:22.0000 rate:0.0440 aloss:1.3790 dloss:0.7690 aloss2:2.0963 exploreP:0.9276\n",
      "Episode:33 meanR:22.8529 R:18.0000 rate:0.0360 aloss:1.3804 dloss:0.7650 aloss2:2.0996 exploreP:0.9260\n",
      "Episode:34 meanR:22.6000 R:14.0000 rate:0.0280 aloss:1.3785 dloss:0.7701 aloss2:2.1797 exploreP:0.9247\n",
      "Episode:35 meanR:22.4722 R:18.0000 rate:0.0360 aloss:1.3809 dloss:0.7466 aloss2:2.1173 exploreP:0.9231\n",
      "Episode:36 meanR:22.2432 R:14.0000 rate:0.0280 aloss:1.3788 dloss:0.7760 aloss2:2.1055 exploreP:0.9218\n",
      "Episode:37 meanR:22.4474 R:30.0000 rate:0.0600 aloss:1.3790 dloss:0.7576 aloss2:2.1378 exploreP:0.9191\n",
      "Episode:38 meanR:22.2051 R:13.0000 rate:0.0260 aloss:1.3814 dloss:0.7472 aloss2:2.1199 exploreP:0.9179\n",
      "Episode:39 meanR:22.3000 R:26.0000 rate:0.0520 aloss:1.3764 dloss:0.7354 aloss2:2.2668 exploreP:0.9155\n",
      "Episode:40 meanR:22.4878 R:30.0000 rate:0.0600 aloss:1.3789 dloss:0.7159 aloss2:2.2648 exploreP:0.9128\n",
      "Episode:41 meanR:22.4286 R:20.0000 rate:0.0400 aloss:1.3782 dloss:0.7018 aloss2:2.2611 exploreP:0.9110\n",
      "Episode:42 meanR:22.3721 R:20.0000 rate:0.0400 aloss:1.3742 dloss:0.7249 aloss2:2.3818 exploreP:0.9092\n",
      "Episode:43 meanR:22.8409 R:43.0000 rate:0.0860 aloss:1.3789 dloss:0.7134 aloss2:2.2616 exploreP:0.9053\n",
      "Episode:44 meanR:22.6889 R:16.0000 rate:0.0320 aloss:1.3809 dloss:0.7258 aloss2:2.2207 exploreP:0.9039\n",
      "Episode:45 meanR:22.8261 R:29.0000 rate:0.0580 aloss:1.3794 dloss:0.6753 aloss2:2.3707 exploreP:0.9013\n",
      "Episode:46 meanR:23.0000 R:31.0000 rate:0.0620 aloss:1.3804 dloss:0.6784 aloss2:2.3219 exploreP:0.8986\n",
      "Episode:47 meanR:23.7500 R:59.0000 rate:0.1180 aloss:1.3775 dloss:0.6942 aloss2:2.4635 exploreP:0.8933\n",
      "Episode:48 meanR:23.7347 R:23.0000 rate:0.0460 aloss:1.3774 dloss:0.6572 aloss2:2.5066 exploreP:0.8913\n",
      "Episode:49 meanR:23.6600 R:20.0000 rate:0.0400 aloss:1.3783 dloss:0.6585 aloss2:2.4743 exploreP:0.8895\n",
      "Episode:50 meanR:23.8235 R:32.0000 rate:0.0640 aloss:1.3798 dloss:0.6563 aloss2:2.4961 exploreP:0.8867\n",
      "Episode:51 meanR:23.7115 R:18.0000 rate:0.0360 aloss:1.3779 dloss:0.6226 aloss2:2.4579 exploreP:0.8852\n",
      "Episode:52 meanR:23.8302 R:30.0000 rate:0.0600 aloss:1.3801 dloss:0.6461 aloss2:2.6363 exploreP:0.8825\n",
      "Episode:53 meanR:23.7778 R:21.0000 rate:0.0420 aloss:1.3776 dloss:0.6680 aloss2:2.4694 exploreP:0.8807\n",
      "Episode:54 meanR:23.9818 R:35.0000 rate:0.0700 aloss:1.3797 dloss:0.6157 aloss2:2.6063 exploreP:0.8777\n",
      "Episode:55 meanR:23.8929 R:19.0000 rate:0.0380 aloss:1.3801 dloss:0.6369 aloss2:2.6013 exploreP:0.8760\n",
      "Episode:56 meanR:23.7193 R:14.0000 rate:0.0280 aloss:1.3790 dloss:0.6018 aloss2:2.6515 exploreP:0.8748\n",
      "Episode:57 meanR:23.6379 R:19.0000 rate:0.0380 aloss:1.3775 dloss:0.6242 aloss2:2.6268 exploreP:0.8732\n",
      "Episode:58 meanR:23.7458 R:30.0000 rate:0.0600 aloss:1.3782 dloss:0.6205 aloss2:2.5462 exploreP:0.8706\n",
      "Episode:59 meanR:23.8667 R:31.0000 rate:0.0620 aloss:1.3787 dloss:0.6298 aloss2:2.5117 exploreP:0.8679\n",
      "Episode:60 meanR:23.7705 R:18.0000 rate:0.0360 aloss:1.3769 dloss:0.6142 aloss2:2.6142 exploreP:0.8664\n",
      "Episode:61 meanR:23.7903 R:25.0000 rate:0.0500 aloss:1.3791 dloss:0.5843 aloss2:2.6552 exploreP:0.8642\n",
      "Episode:62 meanR:24.1111 R:44.0000 rate:0.0880 aloss:1.3821 dloss:0.6041 aloss2:2.7597 exploreP:0.8605\n",
      "Episode:63 meanR:24.5469 R:52.0000 rate:0.1040 aloss:1.3781 dloss:0.5829 aloss2:2.8277 exploreP:0.8561\n",
      "Episode:64 meanR:25.0615 R:58.0000 rate:0.1160 aloss:1.3777 dloss:0.5901 aloss2:2.6815 exploreP:0.8512\n",
      "Episode:65 meanR:24.8939 R:14.0000 rate:0.0280 aloss:1.3792 dloss:0.5766 aloss2:2.6867 exploreP:0.8500\n",
      "Episode:66 meanR:24.8806 R:24.0000 rate:0.0480 aloss:1.3749 dloss:0.5932 aloss2:3.0627 exploreP:0.8480\n",
      "Episode:67 meanR:25.0441 R:36.0000 rate:0.0720 aloss:1.3766 dloss:0.5746 aloss2:2.8076 exploreP:0.8450\n",
      "Episode:68 meanR:25.2754 R:41.0000 rate:0.0820 aloss:1.3775 dloss:0.5671 aloss2:2.9416 exploreP:0.8416\n",
      "Episode:69 meanR:25.4000 R:34.0000 rate:0.0680 aloss:1.3778 dloss:0.5660 aloss2:2.8141 exploreP:0.8387\n",
      "Episode:70 meanR:25.5070 R:33.0000 rate:0.0660 aloss:1.3792 dloss:0.5638 aloss2:2.8174 exploreP:0.8360\n",
      "Episode:71 meanR:25.5417 R:28.0000 rate:0.0560 aloss:1.3780 dloss:0.5673 aloss2:2.8593 exploreP:0.8337\n",
      "Episode:72 meanR:25.5890 R:29.0000 rate:0.0580 aloss:1.3776 dloss:0.5590 aloss2:2.9477 exploreP:0.8313\n",
      "Episode:73 meanR:25.6892 R:33.0000 rate:0.0660 aloss:1.3752 dloss:0.5479 aloss2:2.9683 exploreP:0.8286\n",
      "Episode:74 meanR:25.6400 R:22.0000 rate:0.0440 aloss:1.3792 dloss:0.5735 aloss2:2.9461 exploreP:0.8268\n",
      "Episode:75 meanR:25.5789 R:21.0000 rate:0.0420 aloss:1.3789 dloss:0.5370 aloss2:2.8989 exploreP:0.8251\n",
      "Episode:76 meanR:25.4416 R:15.0000 rate:0.0300 aloss:1.3743 dloss:0.5487 aloss2:2.9706 exploreP:0.8239\n",
      "Episode:77 meanR:25.2949 R:14.0000 rate:0.0280 aloss:1.3800 dloss:0.4891 aloss2:3.1820 exploreP:0.8227\n",
      "Episode:78 meanR:25.8861 R:72.0000 rate:0.1440 aloss:1.3772 dloss:0.5378 aloss2:2.9808 exploreP:0.8169\n",
      "Episode:79 meanR:25.8625 R:24.0000 rate:0.0480 aloss:1.3800 dloss:0.5224 aloss2:3.1906 exploreP:0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:80 meanR:25.8148 R:22.0000 rate:0.0440 aloss:1.3767 dloss:0.5011 aloss2:3.1224 exploreP:0.8132\n",
      "Episode:81 meanR:25.6463 R:12.0000 rate:0.0240 aloss:1.3782 dloss:0.5130 aloss2:2.9848 exploreP:0.8122\n",
      "Episode:82 meanR:25.5904 R:21.0000 rate:0.0420 aloss:1.3749 dloss:0.5211 aloss2:3.1156 exploreP:0.8106\n",
      "Episode:83 meanR:25.4881 R:17.0000 rate:0.0340 aloss:1.3773 dloss:0.5274 aloss2:2.9776 exploreP:0.8092\n",
      "Episode:84 meanR:25.8000 R:52.0000 rate:0.1040 aloss:1.3796 dloss:0.5261 aloss2:3.1484 exploreP:0.8050\n",
      "Episode:85 meanR:25.8605 R:31.0000 rate:0.0620 aloss:1.3786 dloss:0.5344 aloss2:3.0953 exploreP:0.8026\n",
      "Episode:86 meanR:25.7356 R:15.0000 rate:0.0300 aloss:1.3783 dloss:0.5382 aloss2:3.1425 exploreP:0.8014\n",
      "Episode:87 meanR:25.6364 R:17.0000 rate:0.0340 aloss:1.3835 dloss:0.5312 aloss2:3.2769 exploreP:0.8001\n",
      "Episode:88 meanR:25.6966 R:31.0000 rate:0.0620 aloss:1.3777 dloss:0.5189 aloss2:3.3114 exploreP:0.7976\n",
      "Episode:89 meanR:25.8333 R:38.0000 rate:0.0760 aloss:1.3832 dloss:0.4987 aloss2:3.2381 exploreP:0.7946\n",
      "Episode:90 meanR:25.9011 R:32.0000 rate:0.0640 aloss:1.3766 dloss:0.5039 aloss2:3.2229 exploreP:0.7921\n",
      "Episode:91 meanR:25.8587 R:22.0000 rate:0.0440 aloss:1.3749 dloss:0.5160 aloss2:3.2163 exploreP:0.7904\n",
      "Episode:92 meanR:26.1398 R:52.0000 rate:0.1040 aloss:1.3778 dloss:0.5002 aloss2:3.3208 exploreP:0.7864\n",
      "Episode:93 meanR:26.1596 R:28.0000 rate:0.0560 aloss:1.3740 dloss:0.5254 aloss2:3.1947 exploreP:0.7842\n",
      "Episode:94 meanR:26.3263 R:42.0000 rate:0.0840 aloss:1.3784 dloss:0.5068 aloss2:3.2764 exploreP:0.7809\n",
      "Episode:95 meanR:26.4375 R:37.0000 rate:0.0740 aloss:1.3753 dloss:0.4933 aloss2:3.2016 exploreP:0.7781\n",
      "Episode:96 meanR:26.5361 R:36.0000 rate:0.0720 aloss:1.3734 dloss:0.5042 aloss2:3.2724 exploreP:0.7753\n",
      "Episode:97 meanR:26.4490 R:18.0000 rate:0.0360 aloss:1.3740 dloss:0.4717 aloss2:3.2202 exploreP:0.7740\n",
      "Episode:98 meanR:26.3636 R:18.0000 rate:0.0360 aloss:1.3722 dloss:0.5082 aloss2:3.3033 exploreP:0.7726\n",
      "Episode:99 meanR:26.5300 R:43.0000 rate:0.0860 aloss:1.3751 dloss:0.5022 aloss2:3.2808 exploreP:0.7693\n",
      "Episode:100 meanR:26.4800 R:15.0000 rate:0.0300 aloss:1.3744 dloss:0.4784 aloss2:3.2962 exploreP:0.7682\n",
      "Episode:101 meanR:26.5200 R:26.0000 rate:0.0520 aloss:1.3690 dloss:0.5021 aloss2:3.3148 exploreP:0.7662\n",
      "Episode:102 meanR:26.5600 R:15.0000 rate:0.0300 aloss:1.3728 dloss:0.4726 aloss2:3.3363 exploreP:0.7651\n",
      "Episode:103 meanR:26.4900 R:23.0000 rate:0.0460 aloss:1.3726 dloss:0.4967 aloss2:3.3122 exploreP:0.7633\n",
      "Episode:104 meanR:26.3900 R:16.0000 rate:0.0320 aloss:1.3734 dloss:0.4918 aloss2:3.1847 exploreP:0.7621\n",
      "Episode:105 meanR:27.2800 R:109.0000 rate:0.2180 aloss:1.3737 dloss:0.5007 aloss2:3.3419 exploreP:0.7540\n",
      "Episode:106 meanR:27.2500 R:12.0000 rate:0.0240 aloss:1.3702 dloss:0.5450 aloss2:3.4676 exploreP:0.7531\n",
      "Episode:107 meanR:27.3400 R:27.0000 rate:0.0540 aloss:1.3705 dloss:0.4961 aloss2:3.3759 exploreP:0.7511\n",
      "Episode:108 meanR:27.2100 R:16.0000 rate:0.0320 aloss:1.3686 dloss:0.4903 aloss2:3.2532 exploreP:0.7499\n",
      "Episode:109 meanR:28.2300 R:120.0000 rate:0.2400 aloss:1.3746 dloss:0.4974 aloss2:3.3996 exploreP:0.7411\n",
      "Episode:110 meanR:29.2600 R:122.0000 rate:0.2440 aloss:1.3713 dloss:0.5007 aloss2:3.3554 exploreP:0.7322\n",
      "Episode:111 meanR:29.2100 R:11.0000 rate:0.0220 aloss:1.3805 dloss:0.4276 aloss2:3.3834 exploreP:0.7314\n",
      "Episode:112 meanR:29.7600 R:74.0000 rate:0.1480 aloss:1.3761 dloss:0.4836 aloss2:3.3474 exploreP:0.7261\n",
      "Episode:113 meanR:30.1900 R:55.0000 rate:0.1100 aloss:1.3734 dloss:0.4883 aloss2:3.4203 exploreP:0.7222\n",
      "Episode:114 meanR:30.5800 R:54.0000 rate:0.1080 aloss:1.3716 dloss:0.4742 aloss2:3.3669 exploreP:0.7183\n",
      "Episode:115 meanR:30.4300 R:16.0000 rate:0.0320 aloss:1.3753 dloss:0.4676 aloss2:3.3838 exploreP:0.7172\n",
      "Episode:116 meanR:30.2500 R:22.0000 rate:0.0440 aloss:1.3770 dloss:0.4663 aloss2:3.4118 exploreP:0.7156\n",
      "Episode:117 meanR:29.8700 R:12.0000 rate:0.0240 aloss:1.3740 dloss:0.4791 aloss2:3.3601 exploreP:0.7148\n",
      "Episode:118 meanR:29.7200 R:13.0000 rate:0.0260 aloss:1.3711 dloss:0.5124 aloss2:3.4051 exploreP:0.7139\n",
      "Episode:119 meanR:29.7900 R:35.0000 rate:0.0700 aloss:1.3720 dloss:0.4919 aloss2:3.3866 exploreP:0.7114\n",
      "Episode:120 meanR:29.4500 R:11.0000 rate:0.0220 aloss:1.3745 dloss:0.4923 aloss2:3.3741 exploreP:0.7106\n",
      "Episode:121 meanR:29.6500 R:36.0000 rate:0.0720 aloss:1.3755 dloss:0.4784 aloss2:3.4437 exploreP:0.7081\n",
      "Episode:122 meanR:29.9600 R:43.0000 rate:0.0860 aloss:1.3713 dloss:0.4865 aloss2:3.3792 exploreP:0.7051\n",
      "Episode:123 meanR:30.4300 R:60.0000 rate:0.1200 aloss:1.3784 dloss:0.4726 aloss2:3.4254 exploreP:0.7010\n",
      "Episode:124 meanR:30.6500 R:45.0000 rate:0.0900 aloss:1.3731 dloss:0.4794 aloss2:3.3933 exploreP:0.6979\n",
      "Episode:125 meanR:31.8400 R:133.0000 rate:0.2660 aloss:1.3736 dloss:0.4801 aloss2:3.4449 exploreP:0.6888\n",
      "Episode:126 meanR:32.3500 R:89.0000 rate:0.1780 aloss:1.3714 dloss:0.4901 aloss2:3.4219 exploreP:0.6828\n",
      "Episode:127 meanR:32.2800 R:16.0000 rate:0.0320 aloss:1.3740 dloss:0.4536 aloss2:3.3949 exploreP:0.6817\n",
      "Episode:128 meanR:32.3800 R:30.0000 rate:0.0600 aloss:1.3763 dloss:0.5016 aloss2:3.4399 exploreP:0.6797\n",
      "Episode:129 meanR:32.3100 R:17.0000 rate:0.0340 aloss:1.3713 dloss:0.4703 aloss2:3.4381 exploreP:0.6785\n",
      "Episode:130 meanR:33.3200 R:130.0000 rate:0.2600 aloss:1.3748 dloss:0.4731 aloss2:3.4530 exploreP:0.6699\n",
      "Episode:131 meanR:33.3200 R:13.0000 rate:0.0260 aloss:1.3790 dloss:0.4690 aloss2:3.5347 exploreP:0.6691\n",
      "Episode:132 meanR:34.7500 R:165.0000 rate:0.3300 aloss:1.3740 dloss:0.4888 aloss2:3.4827 exploreP:0.6583\n",
      "Episode:133 meanR:35.2900 R:72.0000 rate:0.1440 aloss:1.3714 dloss:0.4786 aloss2:3.4888 exploreP:0.6536\n",
      "Episode:134 meanR:36.8300 R:168.0000 rate:0.3360 aloss:1.3706 dloss:0.4841 aloss2:3.4618 exploreP:0.6429\n",
      "Episode:135 meanR:36.9700 R:32.0000 rate:0.0640 aloss:1.3742 dloss:0.4867 aloss2:3.4649 exploreP:0.6409\n",
      "Episode:136 meanR:37.1300 R:30.0000 rate:0.0600 aloss:1.3685 dloss:0.5198 aloss2:3.4051 exploreP:0.6390\n",
      "Episode:137 meanR:37.1900 R:36.0000 rate:0.0720 aloss:1.3696 dloss:0.4641 aloss2:3.4244 exploreP:0.6367\n",
      "Episode:138 meanR:37.9300 R:87.0000 rate:0.1740 aloss:1.3788 dloss:0.4941 aloss2:3.4507 exploreP:0.6313\n",
      "Episode:139 meanR:37.9200 R:25.0000 rate:0.0500 aloss:1.3844 dloss:0.4688 aloss2:3.5596 exploreP:0.6297\n",
      "Episode:140 meanR:38.5800 R:96.0000 rate:0.1920 aloss:1.3683 dloss:0.4662 aloss2:3.4559 exploreP:0.6238\n",
      "Episode:141 meanR:38.8800 R:50.0000 rate:0.1000 aloss:1.3713 dloss:0.4769 aloss2:3.4843 exploreP:0.6208\n",
      "Episode:142 meanR:39.7600 R:108.0000 rate:0.2160 aloss:1.3769 dloss:0.4835 aloss2:3.4893 exploreP:0.6142\n",
      "Episode:143 meanR:39.6800 R:35.0000 rate:0.0700 aloss:1.3794 dloss:0.4691 aloss2:3.4748 exploreP:0.6121\n",
      "Episode:144 meanR:40.4400 R:92.0000 rate:0.1840 aloss:1.3746 dloss:0.4771 aloss2:3.4727 exploreP:0.6066\n",
      "Episode:145 meanR:40.8000 R:65.0000 rate:0.1300 aloss:1.3689 dloss:0.4807 aloss2:3.4740 exploreP:0.6027\n",
      "Episode:146 meanR:40.7700 R:28.0000 rate:0.0560 aloss:1.3733 dloss:0.4635 aloss2:3.4791 exploreP:0.6011\n",
      "Episode:147 meanR:40.5100 R:33.0000 rate:0.0660 aloss:1.3783 dloss:0.4818 aloss2:3.4672 exploreP:0.5991\n",
      "Episode:148 meanR:41.2400 R:96.0000 rate:0.1920 aloss:1.3775 dloss:0.4883 aloss2:3.4746 exploreP:0.5935\n",
      "Episode:149 meanR:42.0700 R:103.0000 rate:0.2060 aloss:1.3675 dloss:0.4890 aloss2:3.4626 exploreP:0.5875\n",
      "Episode:150 meanR:41.8800 R:13.0000 rate:0.0260 aloss:1.3685 dloss:0.4637 aloss2:3.4320 exploreP:0.5867\n",
      "Episode:151 meanR:41.8300 R:13.0000 rate:0.0260 aloss:1.3684 dloss:0.4719 aloss2:3.4950 exploreP:0.5860\n",
      "Episode:152 meanR:42.8000 R:127.0000 rate:0.2540 aloss:1.3745 dloss:0.4825 aloss2:3.4662 exploreP:0.5787\n",
      "Episode:153 meanR:43.1800 R:59.0000 rate:0.1180 aloss:1.3730 dloss:0.4776 aloss2:3.4818 exploreP:0.5754\n",
      "Episode:154 meanR:43.3400 R:51.0000 rate:0.1020 aloss:1.3645 dloss:0.4543 aloss2:3.4984 exploreP:0.5725\n",
      "Episode:155 meanR:43.4200 R:27.0000 rate:0.0540 aloss:1.3788 dloss:0.4718 aloss2:3.4919 exploreP:0.5710\n",
      "Episode:156 meanR:44.7100 R:143.0000 rate:0.2860 aloss:1.3871 dloss:0.4669 aloss2:3.5188 exploreP:0.5630\n",
      "Episode:157 meanR:47.2400 R:272.0000 rate:0.5440 aloss:1.3782 dloss:0.4767 aloss2:3.4874 exploreP:0.5482\n",
      "Episode:158 meanR:47.6000 R:66.0000 rate:0.1320 aloss:1.3774 dloss:0.4650 aloss2:3.4891 exploreP:0.5446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:159 meanR:48.5500 R:126.0000 rate:0.2520 aloss:1.3937 dloss:0.4903 aloss2:3.5030 exploreP:0.5380\n",
      "Episode:160 meanR:50.3600 R:199.0000 rate:0.3980 aloss:1.3868 dloss:0.4751 aloss2:3.4958 exploreP:0.5275\n",
      "Episode:161 meanR:50.6800 R:57.0000 rate:0.1140 aloss:1.3805 dloss:0.4759 aloss2:3.4877 exploreP:0.5246\n",
      "Episode:162 meanR:51.8900 R:165.0000 rate:0.3300 aloss:1.3918 dloss:0.4863 aloss2:3.4700 exploreP:0.5162\n",
      "Episode:163 meanR:55.4600 R:409.0000 rate:0.8180 aloss:1.3961 dloss:0.4913 aloss2:3.4542 exploreP:0.4959\n",
      "Episode:164 meanR:55.4100 R:53.0000 rate:0.1060 aloss:1.3970 dloss:0.4696 aloss2:3.4443 exploreP:0.4933\n",
      "Episode:165 meanR:55.7900 R:52.0000 rate:0.1040 aloss:1.4188 dloss:0.4991 aloss2:3.4538 exploreP:0.4908\n",
      "Episode:166 meanR:56.8200 R:127.0000 rate:0.2540 aloss:1.4177 dloss:0.4951 aloss2:3.4371 exploreP:0.4848\n",
      "Episode:167 meanR:57.0700 R:61.0000 rate:0.1220 aloss:1.3951 dloss:0.5055 aloss2:3.4391 exploreP:0.4819\n",
      "Episode:168 meanR:57.4000 R:74.0000 rate:0.1480 aloss:1.4047 dloss:0.5323 aloss2:3.4124 exploreP:0.4784\n",
      "Episode:169 meanR:58.3500 R:129.0000 rate:0.2580 aloss:1.4075 dloss:0.4941 aloss2:3.3926 exploreP:0.4724\n",
      "Episode:170 meanR:62.1100 R:409.0000 rate:0.8180 aloss:1.4123 dloss:0.5186 aloss2:3.3899 exploreP:0.4539\n",
      "Episode:171 meanR:65.1900 R:336.0000 rate:0.6720 aloss:1.4312 dloss:0.5292 aloss2:3.3936 exploreP:0.4392\n",
      "Episode:172 meanR:66.2800 R:138.0000 rate:0.2760 aloss:1.4190 dloss:0.5402 aloss2:3.3038 exploreP:0.4333\n",
      "Episode:173 meanR:66.8700 R:92.0000 rate:0.1840 aloss:1.4312 dloss:0.5571 aloss2:3.2980 exploreP:0.4294\n",
      "Episode:174 meanR:67.7800 R:113.0000 rate:0.2260 aloss:1.4341 dloss:0.5029 aloss2:3.3146 exploreP:0.4247\n",
      "Episode:175 meanR:68.5400 R:97.0000 rate:0.1940 aloss:1.4356 dloss:0.4840 aloss2:3.3637 exploreP:0.4207\n",
      "Episode:176 meanR:68.5200 R:13.0000 rate:0.0260 aloss:1.4128 dloss:0.5039 aloss2:3.3841 exploreP:0.4202\n",
      "Episode:177 meanR:69.5000 R:112.0000 rate:0.2240 aloss:1.4109 dloss:0.5488 aloss2:3.3680 exploreP:0.4156\n",
      "Episode:178 meanR:69.7000 R:92.0000 rate:0.1840 aloss:1.4161 dloss:0.5360 aloss2:3.3305 exploreP:0.4119\n",
      "Episode:179 meanR:70.6900 R:123.0000 rate:0.2460 aloss:1.4168 dloss:0.6082 aloss2:3.3142 exploreP:0.4070\n",
      "Episode:180 meanR:72.3900 R:192.0000 rate:0.3840 aloss:1.4523 dloss:0.5412 aloss2:3.2733 exploreP:0.3994\n",
      "Episode:181 meanR:73.9900 R:172.0000 rate:0.3440 aloss:1.4110 dloss:0.5467 aloss2:3.2893 exploreP:0.3928\n",
      "Episode:182 meanR:74.7100 R:93.0000 rate:0.1860 aloss:1.4251 dloss:0.5273 aloss2:3.2944 exploreP:0.3893\n",
      "Episode:183 meanR:76.6300 R:209.0000 rate:0.4180 aloss:1.4294 dloss:0.5237 aloss2:3.3232 exploreP:0.3814\n",
      "Episode:184 meanR:77.3800 R:127.0000 rate:0.2540 aloss:1.4224 dloss:0.5337 aloss2:3.3116 exploreP:0.3767\n",
      "Episode:185 meanR:79.6600 R:259.0000 rate:0.5180 aloss:1.4364 dloss:0.5597 aloss2:3.3114 exploreP:0.3673\n",
      "Episode:186 meanR:80.0100 R:50.0000 rate:0.1000 aloss:1.4676 dloss:0.5308 aloss2:3.2712 exploreP:0.3656\n",
      "Episode:187 meanR:84.8400 R:500.0000 rate:1.0000 aloss:1.4277 dloss:0.5597 aloss2:3.2601 exploreP:0.3482\n",
      "Episode:188 meanR:88.7900 R:426.0000 rate:0.8520 aloss:1.4321 dloss:0.5515 aloss2:3.2943 exploreP:0.3341\n",
      "Episode:189 meanR:91.2100 R:280.0000 rate:0.5600 aloss:1.4321 dloss:0.5647 aloss2:3.2539 exploreP:0.3252\n",
      "Episode:190 meanR:95.8900 R:500.0000 rate:1.0000 aloss:1.4231 dloss:0.5794 aloss2:3.2338 exploreP:0.3098\n",
      "Episode:191 meanR:98.6500 R:298.0000 rate:0.5960 aloss:1.4159 dloss:0.6016 aloss2:3.2075 exploreP:0.3010\n",
      "Episode:192 meanR:102.3700 R:424.0000 rate:0.8480 aloss:1.4293 dloss:0.5712 aloss2:3.2220 exploreP:0.2889\n",
      "Episode:193 meanR:106.7000 R:461.0000 rate:0.9220 aloss:1.4172 dloss:0.6131 aloss2:3.1965 exploreP:0.2763\n",
      "Episode:194 meanR:109.4400 R:316.0000 rate:0.6320 aloss:1.4262 dloss:0.6159 aloss2:3.1256 exploreP:0.2681\n",
      "Episode:195 meanR:112.2200 R:315.0000 rate:0.6300 aloss:1.4163 dloss:0.6124 aloss2:3.1645 exploreP:0.2601\n",
      "Episode:196 meanR:115.4500 R:359.0000 rate:0.7180 aloss:1.4194 dloss:0.5996 aloss2:3.1559 exploreP:0.2512\n",
      "Episode:197 meanR:117.9500 R:268.0000 rate:0.5360 aloss:1.4275 dloss:0.6244 aloss2:3.1385 exploreP:0.2449\n",
      "Episode:198 meanR:120.8100 R:304.0000 rate:0.6080 aloss:1.4088 dloss:0.6565 aloss2:3.1132 exploreP:0.2378\n",
      "Episode:199 meanR:123.3400 R:296.0000 rate:0.5920 aloss:1.4278 dloss:0.6684 aloss2:3.0685 exploreP:0.2312\n",
      "Episode:200 meanR:125.8800 R:269.0000 rate:0.5380 aloss:1.4277 dloss:0.6150 aloss2:3.0608 exploreP:0.2253\n",
      "Episode:201 meanR:128.4900 R:287.0000 rate:0.5740 aloss:1.4197 dloss:0.6431 aloss2:3.1168 exploreP:0.2192\n",
      "Episode:202 meanR:130.9700 R:263.0000 rate:0.5260 aloss:1.4179 dloss:0.6290 aloss2:3.0953 exploreP:0.2138\n",
      "Episode:203 meanR:133.2900 R:255.0000 rate:0.5100 aloss:1.4255 dloss:0.6650 aloss2:3.0608 exploreP:0.2087\n",
      "Episode:204 meanR:135.4800 R:235.0000 rate:0.4700 aloss:1.4229 dloss:0.6171 aloss2:3.0702 exploreP:0.2040\n",
      "Episode:205 meanR:137.0300 R:264.0000 rate:0.5280 aloss:1.4162 dloss:0.6258 aloss2:3.0989 exploreP:0.1990\n",
      "Episode:206 meanR:139.3400 R:243.0000 rate:0.4860 aloss:1.4117 dloss:0.6293 aloss2:3.1083 exploreP:0.1945\n",
      "Episode:207 meanR:141.5600 R:249.0000 rate:0.4980 aloss:1.4167 dloss:0.6742 aloss2:3.0578 exploreP:0.1899\n",
      "Episode:208 meanR:143.7900 R:239.0000 rate:0.4780 aloss:1.4315 dloss:0.6492 aloss2:3.0571 exploreP:0.1857\n",
      "Episode:209 meanR:144.7100 R:212.0000 rate:0.4240 aloss:1.4289 dloss:0.6279 aloss2:3.0865 exploreP:0.1820\n",
      "Episode:210 meanR:145.7800 R:229.0000 rate:0.4580 aloss:1.4239 dloss:0.6192 aloss2:3.0960 exploreP:0.1781\n",
      "Episode:211 meanR:147.6000 R:193.0000 rate:0.3860 aloss:1.4226 dloss:0.6298 aloss2:3.0907 exploreP:0.1749\n",
      "Episode:212 meanR:149.1600 R:230.0000 rate:0.4600 aloss:1.4280 dloss:0.7166 aloss2:3.0089 exploreP:0.1711\n",
      "Episode:213 meanR:151.3600 R:275.0000 rate:0.5500 aloss:1.4383 dloss:0.6763 aloss2:2.9977 exploreP:0.1668\n",
      "Episode:214 meanR:152.5300 R:171.0000 rate:0.3420 aloss:1.4292 dloss:0.6436 aloss2:3.0118 exploreP:0.1641\n",
      "Episode:215 meanR:154.7200 R:235.0000 rate:0.4700 aloss:1.4241 dloss:0.6873 aloss2:3.0088 exploreP:0.1605\n",
      "Episode:216 meanR:156.3400 R:184.0000 rate:0.3680 aloss:1.4398 dloss:0.6755 aloss2:2.9876 exploreP:0.1578\n",
      "Episode:217 meanR:158.6900 R:247.0000 rate:0.4940 aloss:1.4388 dloss:0.6665 aloss2:3.0090 exploreP:0.1542\n",
      "Episode:218 meanR:160.2000 R:164.0000 rate:0.3280 aloss:1.4337 dloss:0.6550 aloss2:3.0002 exploreP:0.1518\n",
      "Episode:219 meanR:161.5800 R:173.0000 rate:0.3460 aloss:1.4435 dloss:0.6339 aloss2:3.0436 exploreP:0.1494\n",
      "Episode:220 meanR:163.2900 R:182.0000 rate:0.3640 aloss:1.4425 dloss:0.6761 aloss2:3.0335 exploreP:0.1469\n",
      "Episode:221 meanR:164.8800 R:195.0000 rate:0.3900 aloss:1.4400 dloss:0.6651 aloss2:2.9916 exploreP:0.1442\n",
      "Episode:222 meanR:166.0300 R:158.0000 rate:0.3160 aloss:1.4525 dloss:0.6531 aloss2:3.0181 exploreP:0.1421\n",
      "Episode:223 meanR:167.2800 R:185.0000 rate:0.3700 aloss:1.4626 dloss:0.6277 aloss2:3.0541 exploreP:0.1397\n",
      "Episode:224 meanR:168.7900 R:196.0000 rate:0.3920 aloss:1.4458 dloss:0.6184 aloss2:3.0691 exploreP:0.1372\n",
      "Episode:225 meanR:169.2700 R:181.0000 rate:0.3620 aloss:1.4336 dloss:0.6324 aloss2:3.0715 exploreP:0.1349\n",
      "Episode:226 meanR:170.4900 R:211.0000 rate:0.4220 aloss:1.4538 dloss:0.6687 aloss2:3.0609 exploreP:0.1323\n",
      "Episode:227 meanR:172.7100 R:238.0000 rate:0.4760 aloss:1.4453 dloss:0.7019 aloss2:2.9920 exploreP:0.1294\n",
      "Episode:228 meanR:174.6300 R:222.0000 rate:0.4440 aloss:1.4557 dloss:0.6698 aloss2:3.0016 exploreP:0.1268\n",
      "Episode:229 meanR:175.8800 R:142.0000 rate:0.2840 aloss:1.4333 dloss:0.7117 aloss2:2.9478 exploreP:0.1252\n",
      "Episode:230 meanR:177.3400 R:276.0000 rate:0.5520 aloss:1.4505 dloss:0.7007 aloss2:2.9428 exploreP:0.1220\n",
      "Episode:231 meanR:179.9300 R:272.0000 rate:0.5440 aloss:1.4465 dloss:0.6413 aloss2:2.9723 exploreP:0.1190\n",
      "Episode:232 meanR:179.8300 R:155.0000 rate:0.3100 aloss:1.4563 dloss:0.6763 aloss2:3.0027 exploreP:0.1173\n",
      "Episode:233 meanR:180.6000 R:149.0000 rate:0.2980 aloss:1.4539 dloss:0.6991 aloss2:2.9999 exploreP:0.1158\n",
      "Episode:234 meanR:182.3600 R:344.0000 rate:0.6880 aloss:1.4579 dloss:0.6408 aloss2:3.0447 exploreP:0.1122\n",
      "Episode:235 meanR:183.5800 R:154.0000 rate:0.3080 aloss:1.4658 dloss:0.6697 aloss2:3.0402 exploreP:0.1106\n",
      "Episode:236 meanR:185.2500 R:197.0000 rate:0.3940 aloss:1.4484 dloss:0.6854 aloss2:2.9856 exploreP:0.1087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:237 meanR:186.6700 R:178.0000 rate:0.3560 aloss:1.4486 dloss:0.7126 aloss2:2.9591 exploreP:0.1069\n",
      "Episode:238 meanR:187.1400 R:134.0000 rate:0.2680 aloss:1.4779 dloss:0.6741 aloss2:2.9446 exploreP:0.1056\n",
      "Episode:239 meanR:191.8900 R:500.0000 rate:1.0000 aloss:1.4492 dloss:0.7117 aloss2:2.9256 exploreP:0.1010\n",
      "Episode:240 meanR:192.8800 R:195.0000 rate:0.3900 aloss:1.4517 dloss:0.6834 aloss2:2.9407 exploreP:0.0992\n",
      "Episode:241 meanR:193.5600 R:118.0000 rate:0.2360 aloss:1.4602 dloss:0.6618 aloss2:2.9892 exploreP:0.0982\n",
      "Episode:242 meanR:197.2100 R:473.0000 rate:0.9460 aloss:1.4432 dloss:0.7186 aloss2:2.9409 exploreP:0.0941\n",
      "Episode:243 meanR:198.9500 R:209.0000 rate:0.4180 aloss:1.4551 dloss:0.7361 aloss2:2.8856 exploreP:0.0923\n",
      "Episode:244 meanR:199.4500 R:142.0000 rate:0.2840 aloss:1.4303 dloss:0.7309 aloss2:2.8650 exploreP:0.0912\n",
      "Episode:245 meanR:200.4300 R:163.0000 rate:0.3260 aloss:1.4428 dloss:0.6786 aloss2:2.9074 exploreP:0.0899\n",
      "Episode:246 meanR:202.2500 R:210.0000 rate:0.4200 aloss:1.4361 dloss:0.7212 aloss2:2.9028 exploreP:0.0882\n",
      "Episode:247 meanR:203.6100 R:169.0000 rate:0.3380 aloss:1.4333 dloss:0.6618 aloss2:2.9377 exploreP:0.0869\n",
      "Episode:248 meanR:204.8600 R:221.0000 rate:0.4420 aloss:1.4351 dloss:0.7330 aloss2:2.9173 exploreP:0.0852\n",
      "Episode:249 meanR:205.7000 R:187.0000 rate:0.3740 aloss:1.4397 dloss:0.7312 aloss2:2.8896 exploreP:0.0838\n",
      "Episode:250 meanR:207.3300 R:176.0000 rate:0.3520 aloss:1.4345 dloss:0.7185 aloss2:2.8878 exploreP:0.0825\n",
      "Episode:251 meanR:208.5900 R:139.0000 rate:0.2780 aloss:1.4346 dloss:0.7542 aloss2:2.8550 exploreP:0.0815\n",
      "Episode:252 meanR:209.0500 R:173.0000 rate:0.3460 aloss:1.4326 dloss:0.7466 aloss2:2.9041 exploreP:0.0803\n",
      "Episode:253 meanR:209.8600 R:140.0000 rate:0.2800 aloss:1.4353 dloss:0.7251 aloss2:2.8767 exploreP:0.0793\n",
      "Episode:254 meanR:211.8500 R:250.0000 rate:0.5000 aloss:1.4210 dloss:0.7608 aloss2:2.8201 exploreP:0.0776\n",
      "Episode:255 meanR:212.9800 R:140.0000 rate:0.2800 aloss:1.4397 dloss:0.7100 aloss2:2.8604 exploreP:0.0767\n",
      "Episode:256 meanR:214.8000 R:325.0000 rate:0.6500 aloss:1.4359 dloss:0.7242 aloss2:2.8732 exploreP:0.0745\n",
      "Episode:257 meanR:213.9600 R:188.0000 rate:0.3760 aloss:1.4261 dloss:0.7396 aloss2:2.8703 exploreP:0.0733\n",
      "Episode:258 meanR:215.9900 R:269.0000 rate:0.5380 aloss:1.4167 dloss:0.8052 aloss2:2.7879 exploreP:0.0717\n",
      "Episode:259 meanR:217.4900 R:276.0000 rate:0.5520 aloss:1.4144 dloss:0.8065 aloss2:2.7244 exploreP:0.0700\n",
      "Episode:260 meanR:217.5800 R:208.0000 rate:0.4160 aloss:1.4245 dloss:0.6997 aloss2:2.8405 exploreP:0.0688\n",
      "Episode:261 meanR:218.6700 R:166.0000 rate:0.3320 aloss:1.4345 dloss:0.6968 aloss2:2.8635 exploreP:0.0678\n",
      "Episode:262 meanR:218.8400 R:182.0000 rate:0.3640 aloss:1.4260 dloss:0.6964 aloss2:2.8865 exploreP:0.0667\n",
      "Episode:263 meanR:217.7300 R:298.0000 rate:0.5960 aloss:1.4300 dloss:0.7037 aloss2:2.9146 exploreP:0.0651\n",
      "Episode:264 meanR:221.3600 R:416.0000 rate:0.8320 aloss:1.4160 dloss:0.7739 aloss2:2.8582 exploreP:0.0628\n",
      "Episode:265 meanR:222.3800 R:154.0000 rate:0.3080 aloss:1.4041 dloss:0.7646 aloss2:2.8039 exploreP:0.0620\n",
      "Episode:266 meanR:224.9000 R:379.0000 rate:0.7580 aloss:1.4072 dloss:0.7786 aloss2:2.7930 exploreP:0.0601\n",
      "Episode:267 meanR:226.9400 R:265.0000 rate:0.5300 aloss:1.4097 dloss:0.8047 aloss2:2.7142 exploreP:0.0588\n",
      "Episode:268 meanR:228.7700 R:257.0000 rate:0.5140 aloss:1.4263 dloss:0.7695 aloss2:2.7681 exploreP:0.0575\n",
      "Episode:269 meanR:228.7500 R:127.0000 rate:0.2540 aloss:1.4228 dloss:0.6762 aloss2:2.8156 exploreP:0.0569\n",
      "Episode:270 meanR:226.0100 R:135.0000 rate:0.2700 aloss:1.4163 dloss:0.7543 aloss2:2.8320 exploreP:0.0563\n",
      "Episode:271 meanR:223.6400 R:99.0000 rate:0.1980 aloss:1.4048 dloss:0.8627 aloss2:2.7643 exploreP:0.0559\n",
      "Episode:272 meanR:223.6600 R:140.0000 rate:0.2800 aloss:1.4043 dloss:0.8065 aloss2:2.7456 exploreP:0.0552\n",
      "Episode:273 meanR:226.6700 R:393.0000 rate:0.7860 aloss:1.4165 dloss:0.7826 aloss2:2.7477 exploreP:0.0535\n",
      "Episode:274 meanR:226.7400 R:120.0000 rate:0.2400 aloss:1.3978 dloss:0.9241 aloss2:2.6991 exploreP:0.0530\n",
      "Episode:275 meanR:226.8900 R:112.0000 rate:0.2240 aloss:1.3890 dloss:0.8489 aloss2:2.6800 exploreP:0.0525\n",
      "Episode:276 meanR:229.1000 R:234.0000 rate:0.4680 aloss:1.4148 dloss:0.7887 aloss2:2.6999 exploreP:0.0515\n",
      "Episode:277 meanR:230.1400 R:216.0000 rate:0.4320 aloss:1.4137 dloss:0.8324 aloss2:2.6292 exploreP:0.0506\n",
      "Episode:278 meanR:230.5200 R:130.0000 rate:0.2600 aloss:1.3969 dloss:0.8138 aloss2:2.6946 exploreP:0.0501\n",
      "Episode:279 meanR:232.9700 R:368.0000 rate:0.7360 aloss:1.4152 dloss:0.7927 aloss2:2.7055 exploreP:0.0486\n",
      "Episode:280 meanR:233.3200 R:227.0000 rate:0.4540 aloss:1.4093 dloss:0.7708 aloss2:2.7512 exploreP:0.0478\n",
      "Episode:281 meanR:232.8700 R:127.0000 rate:0.2540 aloss:1.3811 dloss:0.9186 aloss2:2.7065 exploreP:0.0473\n",
      "Episode:282 meanR:235.2800 R:334.0000 rate:0.6680 aloss:1.4035 dloss:0.8154 aloss2:2.6466 exploreP:0.0461\n",
      "Episode:283 meanR:235.8500 R:266.0000 rate:0.5320 aloss:1.4108 dloss:0.8141 aloss2:2.6926 exploreP:0.0451\n",
      "Episode:284 meanR:235.7900 R:121.0000 rate:0.2420 aloss:1.3754 dloss:0.8814 aloss2:2.6115 exploreP:0.0447\n",
      "Episode:285 meanR:235.8900 R:269.0000 rate:0.5380 aloss:1.3976 dloss:0.8334 aloss2:2.6547 exploreP:0.0438\n",
      "Episode:286 meanR:238.2800 R:289.0000 rate:0.5780 aloss:1.4025 dloss:0.7935 aloss2:2.6647 exploreP:0.0428\n",
      "Episode:287 meanR:234.4700 R:119.0000 rate:0.2380 aloss:1.3835 dloss:0.8770 aloss2:2.6240 exploreP:0.0424\n",
      "Episode:288 meanR:234.1900 R:398.0000 rate:0.7960 aloss:1.4010 dloss:0.8454 aloss2:2.6282 exploreP:0.0412\n",
      "Episode:289 meanR:233.3200 R:193.0000 rate:0.3860 aloss:1.3647 dloss:0.9029 aloss2:2.6419 exploreP:0.0406\n",
      "Episode:290 meanR:230.8000 R:248.0000 rate:0.4960 aloss:1.4054 dloss:0.8082 aloss2:2.6190 exploreP:0.0398\n",
      "Episode:291 meanR:230.1200 R:230.0000 rate:0.4600 aloss:1.3817 dloss:0.8528 aloss2:2.6422 exploreP:0.0391\n",
      "Episode:292 meanR:226.9600 R:108.0000 rate:0.2160 aloss:1.3890 dloss:0.8642 aloss2:2.6489 exploreP:0.0388\n",
      "Episode:293 meanR:223.8600 R:151.0000 rate:0.3020 aloss:1.3972 dloss:0.8548 aloss2:2.6320 exploreP:0.0384\n",
      "Episode:294 meanR:221.8800 R:118.0000 rate:0.2360 aloss:1.3889 dloss:0.8127 aloss2:2.6386 exploreP:0.0381\n",
      "Episode:295 meanR:221.4800 R:275.0000 rate:0.5500 aloss:1.3881 dloss:0.8445 aloss2:2.6521 exploreP:0.0373\n",
      "Episode:296 meanR:222.8900 R:500.0000 rate:1.0000 aloss:1.3879 dloss:0.8542 aloss2:2.5997 exploreP:0.0360\n",
      "Episode:297 meanR:221.3100 R:110.0000 rate:0.2200 aloss:1.3881 dloss:0.8757 aloss2:2.5865 exploreP:0.0357\n",
      "Episode:298 meanR:220.0400 R:177.0000 rate:0.3540 aloss:1.3903 dloss:0.8157 aloss2:2.6447 exploreP:0.0352\n",
      "Episode:299 meanR:220.3600 R:328.0000 rate:0.6560 aloss:1.3753 dloss:0.8807 aloss2:2.5622 exploreP:0.0344\n",
      "Episode:300 meanR:219.1800 R:151.0000 rate:0.3020 aloss:1.3619 dloss:0.9625 aloss2:2.5419 exploreP:0.0341\n",
      "Episode:301 meanR:218.4800 R:217.0000 rate:0.4340 aloss:1.3827 dloss:0.9082 aloss2:2.5227 exploreP:0.0335\n",
      "Episode:302 meanR:217.4500 R:160.0000 rate:0.3200 aloss:1.3761 dloss:0.9178 aloss2:2.5259 exploreP:0.0332\n",
      "Episode:303 meanR:215.9100 R:101.0000 rate:0.2020 aloss:1.3925 dloss:0.8593 aloss2:2.5309 exploreP:0.0329\n",
      "Episode:304 meanR:214.8700 R:131.0000 rate:0.2620 aloss:1.3673 dloss:0.8996 aloss2:2.5311 exploreP:0.0326\n",
      "Episode:305 meanR:214.8800 R:265.0000 rate:0.5300 aloss:1.3796 dloss:0.8560 aloss2:2.5383 exploreP:0.0320\n",
      "Episode:306 meanR:213.7700 R:132.0000 rate:0.2640 aloss:1.3872 dloss:0.8513 aloss2:2.6031 exploreP:0.0318\n",
      "Episode:307 meanR:213.7200 R:244.0000 rate:0.4880 aloss:1.3898 dloss:0.8313 aloss2:2.6123 exploreP:0.0312\n",
      "Episode:308 meanR:215.4600 R:413.0000 rate:0.8260 aloss:1.3699 dloss:0.8681 aloss2:2.5842 exploreP:0.0304\n",
      "Episode:309 meanR:215.2300 R:189.0000 rate:0.3780 aloss:1.3743 dloss:0.8778 aloss2:2.5588 exploreP:0.0300\n",
      "Episode:310 meanR:215.5500 R:261.0000 rate:0.5220 aloss:1.3520 dloss:0.9797 aloss2:2.5321 exploreP:0.0295\n",
      "Episode:311 meanR:215.5200 R:190.0000 rate:0.3800 aloss:1.3592 dloss:0.9183 aloss2:2.4855 exploreP:0.0291\n",
      "Episode:312 meanR:217.7800 R:456.0000 rate:0.9120 aloss:1.3630 dloss:0.9080 aloss2:2.4861 exploreP:0.0283\n",
      "Episode:313 meanR:217.9200 R:289.0000 rate:0.5780 aloss:1.3773 dloss:0.8731 aloss2:2.4961 exploreP:0.0277\n",
      "Episode:314 meanR:217.5000 R:129.0000 rate:0.2580 aloss:1.3717 dloss:0.9457 aloss2:2.5807 exploreP:0.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:315 meanR:216.5300 R:138.0000 rate:0.2760 aloss:1.3663 dloss:0.9242 aloss2:2.4985 exploreP:0.0273\n",
      "Episode:316 meanR:216.0300 R:134.0000 rate:0.2680 aloss:1.3454 dloss:0.9317 aloss2:2.5072 exploreP:0.0270\n",
      "Episode:317 meanR:214.7800 R:122.0000 rate:0.2440 aloss:1.3689 dloss:0.8720 aloss2:2.5106 exploreP:0.0268\n",
      "Episode:318 meanR:218.1400 R:500.0000 rate:1.0000 aloss:1.3673 dloss:0.8918 aloss2:2.4999 exploreP:0.0260\n",
      "Episode:319 meanR:220.3800 R:397.0000 rate:0.7940 aloss:1.3580 dloss:0.9336 aloss2:2.5148 exploreP:0.0254\n",
      "Episode:320 meanR:223.5600 R:500.0000 rate:1.0000 aloss:1.3611 dloss:0.9231 aloss2:2.4675 exploreP:0.0246\n",
      "Episode:321 meanR:224.1300 R:252.0000 rate:0.5040 aloss:1.3654 dloss:0.9232 aloss2:2.4523 exploreP:0.0243\n",
      "Episode:322 meanR:225.1900 R:264.0000 rate:0.5280 aloss:1.3548 dloss:0.9319 aloss2:2.4777 exploreP:0.0239\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "# aloss_list, dloss_list, aloss2_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0 # each episode\n",
    "        aloss_batch, dloss_batch, aloss2_batch = [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rate = -1\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            states = states[rates >= np.max(rates)]\n",
    "            actions = actions[rates >= np.max(rates)]\n",
    "            next_states = next_states[rates >= np.max(rates)]\n",
    "            rewards = rewards[rates >= np.max(rates)]\n",
    "            dones = dones[rates >= np.max(rates)]\n",
    "            rates = rates[rates >= np.max(rates)]\n",
    "            aloss, dloss, _, _ = sess.run([model.a_loss, model.d_loss, model.a_opt, model.d_opt],\n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.next_states: next_states,\n",
    "                                                       model.rewards: rewards,\n",
    "                                                       model.dones: dones,\n",
    "                                                       model.rates: rates})\n",
    "            aloss2, _ = sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            aloss_batch.append(aloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        # gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        # dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 481.0\n",
      "total_reward: 481.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
