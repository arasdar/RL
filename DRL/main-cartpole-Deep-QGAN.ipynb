{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Act', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Env', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        states_logits = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)\n",
    "        Qlogits = tf.layers.dense(inputs=nl2, units=1, trainable=False)\n",
    "        return states_logits, Qlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, dones, rates):\n",
    "    ################################################ a = act(s)\n",
    "    actions_logits = Act(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    ################################################ s', r = env(s, a)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    next_states_logits, Qlogits = Env(actions=actions_labels, states=states, hidden_size=hidden_size, \n",
    "                                      action_size=action_size, state_size=state_size)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    eloss_ = tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, labels=next_states_labels)\n",
    "    eloss = tf.reduce_mean(tf.reduce_sum(eloss_, axis=1))\n",
    "    #################################################### s'', Q' = ~env(s', ~a')\n",
    "    next_actions_logits = Act(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    next_states_logits, nextQlogits = Env(actions=next_actions_logits, states=next_states, hidden_size=hidden_size, \n",
    "                                          action_size=action_size, state_size=state_size, reuse=True)\n",
    "    #################################################### GAN\n",
    "    Qlogits = tf.reshape(Qlogits, shape=[-1])\n",
    "    nextQlogits = tf.reshape(nextQlogits, shape=[-1]) * (1-dones)\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qlogits, # GAN\n",
    "                                                                    labels=rates)) # 0-1 real\n",
    "    eloss += tf.reduce_mean(nextQlogits) # minimize nextQ\n",
    "    aloss = -tf.reduce_mean(nextQlogits) # maximize nextQ\n",
    "    ###################################################### Qlearning\n",
    "    ###################################################### Q(s,a)= r + Q'(s',a')\n",
    "    rewards = tf.reduce_mean(tf.square(Qlogits - (gamma * nextQlogits)))\n",
    "    ##################################################### GAN + DDPG\n",
    "    eloss += tf.reduce_mean(rewards) # minimize entropy=error\n",
    "    aloss += -tf.reduce_mean(rewards) # maximize entropy=rewards\n",
    "    return actions_logits, aloss, eloss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, e_loss, a_learning_rate, e_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('Act')]\n",
    "    e_vars = [var for var in t_vars if var.name.startswith('Env')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        e_opt = tf.train.AdamOptimizer(e_learning_rate).minimize(e_loss, var_list=e_vars)\n",
    "    return a_opt, e_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, e_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.e_loss, self.rewards_ = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.e_opt = model_opt(a_loss=self.a_loss, e_loss=self.e_loss,\n",
    "                                           a_learning_rate=a_learning_rate,\n",
    "                                           e_learning_rate=e_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "e_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size\n",
    "gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              a_learning_rate=a_learning_rate, e_learning_rate=e_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        #print(done)\n",
    "        rate = total_reward/500\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        state = env.reset()\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibacth(memory):\n",
    "    # Training with the maxrated minibatch\n",
    "    batch = memory.buffer\n",
    "    #for idx in range(memory_size// batch_size):\n",
    "    while True:\n",
    "        idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "        states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "        states = states[rates >= np.max(rates)]\n",
    "        actions = actions[rates >= np.max(rates)]\n",
    "        next_states = next_states[rates >= np.max(rates)]\n",
    "        rewards = rewards[rates >= np.max(rates)]\n",
    "        dones = dones[rates >= np.max(rates)]\n",
    "        rates = rates[rates >= np.max(rates)]\n",
    "        if np.count_nonzero(dones) > 0 and len(dones) > 1 and np.max(rates) > 0:\n",
    "            break\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:17.0000 R:17.0000 rate:0.0340 aloss:-0.3665 eloss:4.1107 exploreP:0.9983\n",
      "Episode:1 meanR:21.5000 R:26.0000 rate:0.0520 aloss:-0.2882 eloss:3.9948 exploreP:0.9958\n",
      "Episode:2 meanR:19.3333 R:15.0000 rate:0.0300 aloss:-0.4877 eloss:4.2906 exploreP:0.9943\n",
      "Episode:3 meanR:20.7500 R:25.0000 rate:0.0500 aloss:-0.2650 eloss:3.9523 exploreP:0.9918\n",
      "Episode:4 meanR:20.2000 R:18.0000 rate:0.0360 aloss:-0.2235 eloss:3.8969 exploreP:0.9901\n",
      "Episode:5 meanR:20.8333 R:24.0000 rate:0.0480 aloss:-0.2887 eloss:4.0132 exploreP:0.9877\n",
      "Episode:6 meanR:20.5714 R:19.0000 rate:0.0380 aloss:-0.2152 eloss:3.8745 exploreP:0.9858\n",
      "Episode:7 meanR:20.5000 R:20.0000 rate:0.0400 aloss:-0.1842 eloss:3.8401 exploreP:0.9839\n",
      "Episode:8 meanR:23.8889 R:51.0000 rate:0.1020 aloss:-0.2077 eloss:3.8755 exploreP:0.9789\n",
      "Episode:9 meanR:23.4000 R:19.0000 rate:0.0380 aloss:-0.1881 eloss:3.8501 exploreP:0.9771\n",
      "Episode:10 meanR:24.2727 R:33.0000 rate:0.0660 aloss:-0.1627 eloss:3.7988 exploreP:0.9739\n",
      "Episode:11 meanR:23.1667 R:11.0000 rate:0.0220 aloss:-0.2417 eloss:3.9496 exploreP:0.9729\n",
      "Episode:12 meanR:23.6923 R:30.0000 rate:0.0600 aloss:-0.1564 eloss:3.7890 exploreP:0.9700\n",
      "Episode:13 meanR:23.4286 R:20.0000 rate:0.0400 aloss:-0.1677 eloss:3.8009 exploreP:0.9681\n",
      "Episode:14 meanR:22.6667 R:12.0000 rate:0.0240 aloss:-0.1216 eloss:3.7367 exploreP:0.9669\n",
      "Episode:15 meanR:22.0625 R:13.0000 rate:0.0260 aloss:-0.1599 eloss:3.8041 exploreP:0.9657\n",
      "Episode:16 meanR:22.0588 R:22.0000 rate:0.0440 aloss:-0.1389 eloss:3.7451 exploreP:0.9636\n",
      "Episode:17 meanR:22.0000 R:21.0000 rate:0.0420 aloss:-0.1320 eloss:3.7177 exploreP:0.9616\n",
      "Episode:18 meanR:21.4737 R:12.0000 rate:0.0240 aloss:-0.1394 eloss:3.7626 exploreP:0.9604\n",
      "Episode:19 meanR:21.8500 R:29.0000 rate:0.0580 aloss:-0.0971 eloss:3.6806 exploreP:0.9577\n",
      "Episode:20 meanR:21.5714 R:16.0000 rate:0.0320 aloss:-0.1004 eloss:3.6841 exploreP:0.9562\n",
      "Episode:21 meanR:21.3636 R:17.0000 rate:0.0340 aloss:-0.0785 eloss:3.6299 exploreP:0.9545\n",
      "Episode:22 meanR:21.0870 R:15.0000 rate:0.0300 aloss:-0.0925 eloss:3.6744 exploreP:0.9531\n",
      "Episode:23 meanR:20.9167 R:17.0000 rate:0.0340 aloss:-0.1102 eloss:3.7159 exploreP:0.9515\n",
      "Episode:24 meanR:20.8000 R:18.0000 rate:0.0360 aloss:-0.0949 eloss:3.6541 exploreP:0.9498\n",
      "Episode:25 meanR:21.3462 R:35.0000 rate:0.0700 aloss:-0.0524 eloss:3.6176 exploreP:0.9466\n",
      "Episode:26 meanR:21.5185 R:26.0000 rate:0.0520 aloss:-0.0615 eloss:3.5978 exploreP:0.9441\n",
      "Episode:27 meanR:21.2143 R:13.0000 rate:0.0260 aloss:-0.0521 eloss:3.6139 exploreP:0.9429\n",
      "Episode:28 meanR:21.4138 R:27.0000 rate:0.0540 aloss:-0.0505 eloss:3.5963 exploreP:0.9404\n",
      "Episode:29 meanR:21.6667 R:29.0000 rate:0.0580 aloss:-0.0305 eloss:3.5513 exploreP:0.9377\n",
      "Episode:30 meanR:21.9032 R:29.0000 rate:0.0580 aloss:-0.0096 eloss:3.5288 exploreP:0.9350\n",
      "Episode:31 meanR:22.1875 R:31.0000 rate:0.0620 aloss:0.0104 eloss:3.5144 exploreP:0.9321\n",
      "Episode:32 meanR:22.0000 R:16.0000 rate:0.0320 aloss:0.0456 eloss:3.4429 exploreP:0.9307\n",
      "Episode:33 meanR:21.7059 R:12.0000 rate:0.0240 aloss:0.0369 eloss:3.4682 exploreP:0.9296\n",
      "Episode:34 meanR:22.1143 R:36.0000 rate:0.0720 aloss:0.0442 eloss:3.4611 exploreP:0.9263\n",
      "Episode:35 meanR:21.8056 R:11.0000 rate:0.0220 aloss:0.0553 eloss:3.4639 exploreP:0.9253\n",
      "Episode:36 meanR:21.5946 R:14.0000 rate:0.0280 aloss:0.0766 eloss:3.4139 exploreP:0.9240\n",
      "Episode:37 meanR:21.3684 R:13.0000 rate:0.0260 aloss:0.0802 eloss:3.4540 exploreP:0.9228\n",
      "Episode:38 meanR:21.1795 R:14.0000 rate:0.0280 aloss:0.0933 eloss:3.3927 exploreP:0.9215\n",
      "Episode:39 meanR:21.9250 R:51.0000 rate:0.1020 aloss:0.0956 eloss:3.4045 exploreP:0.9169\n",
      "Episode:40 meanR:21.8049 R:17.0000 rate:0.0340 aloss:0.1404 eloss:3.3495 exploreP:0.9153\n",
      "Episode:41 meanR:21.6905 R:17.0000 rate:0.0340 aloss:0.1451 eloss:3.3347 exploreP:0.9138\n",
      "Episode:42 meanR:22.0930 R:39.0000 rate:0.0780 aloss:0.1767 eloss:3.3050 exploreP:0.9103\n",
      "Episode:43 meanR:21.8636 R:12.0000 rate:0.0240 aloss:0.1905 eloss:3.3073 exploreP:0.9092\n",
      "Episode:44 meanR:21.8000 R:19.0000 rate:0.0380 aloss:0.2045 eloss:3.2657 exploreP:0.9075\n",
      "Episode:45 meanR:21.8043 R:22.0000 rate:0.0440 aloss:0.2120 eloss:3.2658 exploreP:0.9055\n",
      "Episode:46 meanR:21.7872 R:21.0000 rate:0.0420 aloss:0.2418 eloss:3.2539 exploreP:0.9036\n",
      "Episode:47 meanR:21.5000 R:8.0000 rate:0.0160 aloss:0.2244 eloss:3.2473 exploreP:0.9029\n",
      "Episode:48 meanR:21.6735 R:30.0000 rate:0.0600 aloss:0.2617 eloss:3.1793 exploreP:0.9003\n",
      "Episode:49 meanR:21.6200 R:19.0000 rate:0.0380 aloss:0.2792 eloss:3.2312 exploreP:0.8986\n",
      "Episode:50 meanR:21.6863 R:25.0000 rate:0.0500 aloss:0.2949 eloss:3.1504 exploreP:0.8963\n",
      "Episode:51 meanR:21.9423 R:35.0000 rate:0.0700 aloss:0.3126 eloss:3.1321 exploreP:0.8932\n",
      "Episode:52 meanR:21.8491 R:17.0000 rate:0.0340 aloss:0.3287 eloss:3.1196 exploreP:0.8917\n",
      "Episode:53 meanR:22.1667 R:39.0000 rate:0.0780 aloss:0.3550 eloss:3.0883 exploreP:0.8883\n",
      "Episode:54 meanR:21.9818 R:12.0000 rate:0.0240 aloss:0.3554 eloss:3.1153 exploreP:0.8873\n",
      "Episode:55 meanR:21.9821 R:22.0000 rate:0.0440 aloss:0.3756 eloss:3.0458 exploreP:0.8853\n",
      "Episode:56 meanR:22.0877 R:28.0000 rate:0.0560 aloss:0.3836 eloss:3.0647 exploreP:0.8829\n",
      "Episode:57 meanR:22.3966 R:40.0000 rate:0.0800 aloss:0.4114 eloss:3.0118 exploreP:0.8794\n",
      "Episode:58 meanR:22.2034 R:11.0000 rate:0.0220 aloss:0.4072 eloss:3.0570 exploreP:0.8784\n",
      "Episode:59 meanR:22.0500 R:13.0000 rate:0.0260 aloss:0.4254 eloss:2.9953 exploreP:0.8773\n",
      "Episode:60 meanR:22.0492 R:22.0000 rate:0.0440 aloss:0.4472 eloss:2.9882 exploreP:0.8754\n",
      "Episode:61 meanR:21.9032 R:13.0000 rate:0.0260 aloss:0.4610 eloss:2.9862 exploreP:0.8743\n",
      "Episode:62 meanR:21.8889 R:21.0000 rate:0.0420 aloss:0.4755 eloss:2.9670 exploreP:0.8725\n",
      "Episode:63 meanR:21.6875 R:9.0000 rate:0.0180 aloss:0.4828 eloss:2.9741 exploreP:0.8717\n",
      "Episode:64 meanR:21.6923 R:22.0000 rate:0.0440 aloss:0.4832 eloss:2.9784 exploreP:0.8698\n",
      "Episode:65 meanR:21.7273 R:24.0000 rate:0.0480 aloss:0.4970 eloss:2.9343 exploreP:0.8677\n",
      "Episode:66 meanR:21.5821 R:12.0000 rate:0.0240 aloss:0.5290 eloss:2.8826 exploreP:0.8667\n",
      "Episode:67 meanR:21.4853 R:15.0000 rate:0.0300 aloss:0.5418 eloss:2.8787 exploreP:0.8654\n",
      "Episode:68 meanR:21.4493 R:19.0000 rate:0.0380 aloss:0.5565 eloss:2.8567 exploreP:0.8638\n",
      "Episode:69 meanR:21.3571 R:15.0000 rate:0.0300 aloss:0.5743 eloss:2.8506 exploreP:0.8625\n",
      "Episode:70 meanR:21.3662 R:22.0000 rate:0.0440 aloss:0.5900 eloss:2.8341 exploreP:0.8607\n",
      "Episode:71 meanR:21.6944 R:45.0000 rate:0.0900 aloss:0.6229 eloss:2.8025 exploreP:0.8568\n",
      "Episode:72 meanR:21.6712 R:20.0000 rate:0.0400 aloss:0.6579 eloss:2.7624 exploreP:0.8551\n",
      "Episode:73 meanR:21.5811 R:15.0000 rate:0.0300 aloss:0.6410 eloss:2.7774 exploreP:0.8539\n",
      "Episode:74 meanR:21.6000 R:23.0000 rate:0.0460 aloss:0.6741 eloss:2.7577 exploreP:0.8519\n",
      "Episode:75 meanR:21.6579 R:26.0000 rate:0.0520 aloss:0.7095 eloss:2.7106 exploreP:0.8498\n",
      "Episode:76 meanR:21.5584 R:14.0000 rate:0.0280 aloss:0.7155 eloss:2.7062 exploreP:0.8486\n",
      "Episode:77 meanR:21.3974 R:9.0000 rate:0.0180 aloss:0.7064 eloss:2.7351 exploreP:0.8478\n",
      "Episode:78 meanR:21.2911 R:13.0000 rate:0.0260 aloss:0.7404 eloss:2.6737 exploreP:0.8467\n",
      "Episode:79 meanR:21.2375 R:17.0000 rate:0.0340 aloss:0.7176 eloss:2.6820 exploreP:0.8453\n",
      "Episode:80 meanR:21.1481 R:14.0000 rate:0.0280 aloss:0.7403 eloss:2.7042 exploreP:0.8441\n",
      "Episode:81 meanR:21.1829 R:24.0000 rate:0.0480 aloss:0.7657 eloss:2.6403 exploreP:0.8421\n",
      "Episode:82 meanR:21.1566 R:19.0000 rate:0.0380 aloss:0.7706 eloss:2.6990 exploreP:0.8406\n",
      "Episode:83 meanR:21.1667 R:22.0000 rate:0.0440 aloss:0.7742 eloss:2.6550 exploreP:0.8387\n",
      "Episode:84 meanR:21.0471 R:11.0000 rate:0.0220 aloss:0.7861 eloss:2.6704 exploreP:0.8378\n",
      "Episode:85 meanR:20.9535 R:13.0000 rate:0.0260 aloss:0.7805 eloss:2.6491 exploreP:0.8368\n",
      "Episode:86 meanR:20.8276 R:10.0000 rate:0.0200 aloss:0.7702 eloss:2.6376 exploreP:0.8359\n",
      "Episode:87 meanR:20.7273 R:12.0000 rate:0.0240 aloss:0.8108 eloss:2.6238 exploreP:0.8349\n",
      "Episode:88 meanR:20.9888 R:44.0000 rate:0.0880 aloss:0.8063 eloss:2.6294 exploreP:0.8313\n",
      "Episode:89 meanR:20.9333 R:16.0000 rate:0.0320 aloss:0.8375 eloss:2.5721 exploreP:0.8300\n",
      "Episode:90 meanR:20.8242 R:11.0000 rate:0.0220 aloss:0.8204 eloss:2.6371 exploreP:0.8291\n",
      "Episode:91 meanR:20.7391 R:13.0000 rate:0.0260 aloss:0.8281 eloss:2.5928 exploreP:0.8280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:92 meanR:20.6452 R:12.0000 rate:0.0240 aloss:0.8284 eloss:2.6117 exploreP:0.8271\n",
      "Episode:93 meanR:20.8191 R:37.0000 rate:0.0740 aloss:0.8173 eloss:2.6251 exploreP:0.8240\n",
      "Episode:94 meanR:20.7368 R:13.0000 rate:0.0260 aloss:0.8357 eloss:2.5810 exploreP:0.8230\n",
      "Episode:95 meanR:20.6875 R:16.0000 rate:0.0320 aloss:0.8522 eloss:2.5574 exploreP:0.8217\n",
      "Episode:96 meanR:20.6082 R:13.0000 rate:0.0260 aloss:0.8222 eloss:2.6608 exploreP:0.8206\n",
      "Episode:97 meanR:20.6020 R:20.0000 rate:0.0400 aloss:0.8343 eloss:2.5980 exploreP:0.8190\n",
      "Episode:98 meanR:20.6970 R:30.0000 rate:0.0600 aloss:0.8182 eloss:2.6448 exploreP:0.8166\n",
      "Episode:99 meanR:20.6500 R:16.0000 rate:0.0320 aloss:0.8303 eloss:2.6050 exploreP:0.8153\n",
      "Episode:100 meanR:20.6500 R:17.0000 rate:0.0340 aloss:0.8014 eloss:2.6812 exploreP:0.8139\n",
      "Episode:101 meanR:20.5400 R:15.0000 rate:0.0300 aloss:0.8428 eloss:2.5687 exploreP:0.8127\n",
      "Episode:102 meanR:20.5200 R:13.0000 rate:0.0260 aloss:0.8252 eloss:2.6390 exploreP:0.8117\n",
      "Episode:103 meanR:20.3800 R:11.0000 rate:0.0220 aloss:0.8363 eloss:2.5299 exploreP:0.8108\n",
      "Episode:104 meanR:20.3500 R:15.0000 rate:0.0300 aloss:0.8210 eloss:2.6288 exploreP:0.8096\n",
      "Episode:105 meanR:20.2700 R:16.0000 rate:0.0320 aloss:0.8407 eloss:2.6004 exploreP:0.8083\n",
      "Episode:106 meanR:20.2500 R:17.0000 rate:0.0340 aloss:0.8170 eloss:2.6598 exploreP:0.8070\n",
      "Episode:107 meanR:20.1600 R:11.0000 rate:0.0220 aloss:0.8233 eloss:2.6148 exploreP:0.8061\n",
      "Episode:108 meanR:19.7700 R:12.0000 rate:0.0240 aloss:0.8136 eloss:2.6609 exploreP:0.8051\n",
      "Episode:109 meanR:19.6800 R:10.0000 rate:0.0200 aloss:0.8308 eloss:2.5978 exploreP:0.8043\n",
      "Episode:110 meanR:19.5300 R:18.0000 rate:0.0360 aloss:0.7360 eloss:2.6771 exploreP:0.8029\n",
      "Episode:111 meanR:19.6200 R:20.0000 rate:0.0400 aloss:0.7987 eloss:2.6792 exploreP:0.8013\n",
      "Episode:112 meanR:19.4200 R:10.0000 rate:0.0200 aloss:0.7612 eloss:2.7193 exploreP:0.8005\n",
      "Episode:113 meanR:19.3700 R:15.0000 rate:0.0300 aloss:0.7804 eloss:2.6162 exploreP:0.7993\n",
      "Episode:114 meanR:19.5000 R:25.0000 rate:0.0500 aloss:0.7463 eloss:2.7102 exploreP:0.7974\n",
      "Episode:115 meanR:19.4700 R:10.0000 rate:0.0200 aloss:0.7624 eloss:2.6999 exploreP:0.7966\n",
      "Episode:116 meanR:19.3500 R:10.0000 rate:0.0200 aloss:0.6785 eloss:2.8621 exploreP:0.7958\n",
      "Episode:117 meanR:19.3100 R:17.0000 rate:0.0340 aloss:0.7332 eloss:2.6930 exploreP:0.7945\n",
      "Episode:118 meanR:19.3100 R:12.0000 rate:0.0240 aloss:0.7086 eloss:2.7887 exploreP:0.7935\n",
      "Episode:119 meanR:19.1300 R:11.0000 rate:0.0220 aloss:0.7497 eloss:2.7095 exploreP:0.7927\n",
      "Episode:120 meanR:19.1000 R:13.0000 rate:0.0260 aloss:0.7303 eloss:2.7378 exploreP:0.7916\n",
      "Episode:121 meanR:19.0500 R:12.0000 rate:0.0240 aloss:0.7111 eloss:2.7624 exploreP:0.7907\n",
      "Episode:122 meanR:19.0000 R:10.0000 rate:0.0200 aloss:0.7294 eloss:2.6669 exploreP:0.7899\n",
      "Episode:123 meanR:19.0400 R:21.0000 rate:0.0420 aloss:0.7145 eloss:2.7403 exploreP:0.7883\n",
      "Episode:124 meanR:19.1100 R:25.0000 rate:0.0500 aloss:0.7427 eloss:2.7109 exploreP:0.7864\n",
      "Episode:125 meanR:18.8900 R:13.0000 rate:0.0260 aloss:0.6659 eloss:2.7425 exploreP:0.7853\n",
      "Episode:126 meanR:18.8300 R:20.0000 rate:0.0400 aloss:0.7186 eloss:2.7478 exploreP:0.7838\n",
      "Episode:127 meanR:18.8200 R:12.0000 rate:0.0240 aloss:0.7251 eloss:2.7487 exploreP:0.7829\n",
      "Episode:128 meanR:18.7200 R:17.0000 rate:0.0340 aloss:0.7052 eloss:2.7137 exploreP:0.7816\n",
      "Episode:129 meanR:18.6000 R:17.0000 rate:0.0340 aloss:0.7120 eloss:2.7105 exploreP:0.7802\n",
      "Episode:130 meanR:18.7400 R:43.0000 rate:0.0860 aloss:0.7169 eloss:2.7021 exploreP:0.7769\n",
      "Episode:131 meanR:18.5500 R:12.0000 rate:0.0240 aloss:0.7064 eloss:2.6848 exploreP:0.7760\n",
      "Episode:132 meanR:18.5200 R:13.0000 rate:0.0260 aloss:0.7162 eloss:2.7231 exploreP:0.7750\n",
      "Episode:133 meanR:18.5400 R:14.0000 rate:0.0280 aloss:0.6541 eloss:2.8436 exploreP:0.7740\n",
      "Episode:134 meanR:18.3400 R:16.0000 rate:0.0320 aloss:0.6950 eloss:2.7372 exploreP:0.7727\n",
      "Episode:135 meanR:18.3400 R:11.0000 rate:0.0220 aloss:0.7089 eloss:2.7770 exploreP:0.7719\n",
      "Episode:136 meanR:18.3600 R:16.0000 rate:0.0320 aloss:0.6815 eloss:2.7422 exploreP:0.7707\n",
      "Episode:137 meanR:18.3700 R:14.0000 rate:0.0280 aloss:0.7001 eloss:2.7395 exploreP:0.7696\n",
      "Episode:138 meanR:18.3500 R:12.0000 rate:0.0240 aloss:0.6875 eloss:2.7654 exploreP:0.7687\n",
      "Episode:139 meanR:17.9700 R:13.0000 rate:0.0260 aloss:0.6962 eloss:2.7805 exploreP:0.7677\n",
      "Episode:140 meanR:17.9400 R:14.0000 rate:0.0280 aloss:0.6942 eloss:2.7260 exploreP:0.7667\n",
      "Episode:141 meanR:17.9100 R:14.0000 rate:0.0280 aloss:0.7002 eloss:2.7590 exploreP:0.7656\n",
      "Episode:142 meanR:17.6900 R:17.0000 rate:0.0340 aloss:0.6469 eloss:2.8079 exploreP:0.7643\n",
      "Episode:143 meanR:17.6800 R:11.0000 rate:0.0220 aloss:0.7016 eloss:2.7550 exploreP:0.7635\n",
      "Episode:144 meanR:17.6000 R:11.0000 rate:0.0220 aloss:0.6989 eloss:2.7576 exploreP:0.7627\n",
      "Episode:145 meanR:17.4800 R:10.0000 rate:0.0200 aloss:0.6873 eloss:2.7576 exploreP:0.7619\n",
      "Episode:146 meanR:17.4000 R:13.0000 rate:0.0260 aloss:0.7057 eloss:2.6837 exploreP:0.7609\n",
      "Episode:147 meanR:17.4500 R:13.0000 rate:0.0260 aloss:0.7084 eloss:2.7233 exploreP:0.7599\n",
      "Episode:148 meanR:17.2900 R:14.0000 rate:0.0280 aloss:0.7175 eloss:2.7176 exploreP:0.7589\n",
      "Episode:149 meanR:17.3500 R:25.0000 rate:0.0500 aloss:0.7179 eloss:2.7435 exploreP:0.7570\n",
      "Episode:150 meanR:17.2700 R:17.0000 rate:0.0340 aloss:0.7191 eloss:2.7224 exploreP:0.7558\n",
      "Episode:151 meanR:17.1200 R:20.0000 rate:0.0400 aloss:0.7373 eloss:2.6969 exploreP:0.7543\n",
      "Episode:152 meanR:17.1400 R:19.0000 rate:0.0380 aloss:0.7084 eloss:2.7354 exploreP:0.7529\n",
      "Episode:153 meanR:16.9100 R:16.0000 rate:0.0320 aloss:0.7485 eloss:2.7133 exploreP:0.7517\n",
      "Episode:154 meanR:16.9300 R:14.0000 rate:0.0280 aloss:0.7416 eloss:2.6670 exploreP:0.7506\n",
      "Episode:155 meanR:16.8900 R:18.0000 rate:0.0360 aloss:0.7483 eloss:2.6585 exploreP:0.7493\n",
      "Episode:156 meanR:16.7600 R:15.0000 rate:0.0300 aloss:0.7161 eloss:2.7045 exploreP:0.7482\n",
      "Episode:157 meanR:16.5500 R:19.0000 rate:0.0380 aloss:0.7408 eloss:2.7130 exploreP:0.7468\n",
      "Episode:158 meanR:16.6700 R:23.0000 rate:0.0460 aloss:0.7865 eloss:2.6589 exploreP:0.7451\n",
      "Episode:159 meanR:16.7000 R:16.0000 rate:0.0320 aloss:0.7703 eloss:2.6636 exploreP:0.7439\n",
      "Episode:160 meanR:16.7200 R:24.0000 rate:0.0480 aloss:0.8050 eloss:2.5978 exploreP:0.7422\n",
      "Episode:161 meanR:16.8100 R:22.0000 rate:0.0440 aloss:0.8215 eloss:2.5346 exploreP:0.7406\n",
      "Episode:162 meanR:16.7400 R:14.0000 rate:0.0280 aloss:0.7857 eloss:2.6539 exploreP:0.7395\n",
      "Episode:163 meanR:16.8500 R:20.0000 rate:0.0400 aloss:0.8174 eloss:2.6033 exploreP:0.7381\n",
      "Episode:164 meanR:16.7700 R:14.0000 rate:0.0280 aloss:0.8521 eloss:2.5581 exploreP:0.7371\n",
      "Episode:165 meanR:16.6800 R:15.0000 rate:0.0300 aloss:0.8518 eloss:2.6030 exploreP:0.7360\n",
      "Episode:166 meanR:16.8600 R:30.0000 rate:0.0600 aloss:0.8543 eloss:2.5521 exploreP:0.7338\n",
      "Episode:167 meanR:16.8400 R:13.0000 rate:0.0260 aloss:0.8671 eloss:2.5029 exploreP:0.7329\n",
      "Episode:168 meanR:16.7500 R:10.0000 rate:0.0200 aloss:0.8969 eloss:2.4733 exploreP:0.7321\n",
      "Episode:169 meanR:16.6900 R:9.0000 rate:0.0180 aloss:0.8810 eloss:2.5758 exploreP:0.7315\n",
      "Episode:170 meanR:16.7000 R:23.0000 rate:0.0460 aloss:0.8995 eloss:2.4862 exploreP:0.7298\n",
      "Episode:171 meanR:16.3800 R:13.0000 rate:0.0260 aloss:0.8647 eloss:2.5298 exploreP:0.7289\n",
      "Episode:172 meanR:16.3400 R:16.0000 rate:0.0320 aloss:0.9089 eloss:2.5067 exploreP:0.7277\n",
      "Episode:173 meanR:16.4000 R:21.0000 rate:0.0420 aloss:0.9247 eloss:2.5108 exploreP:0.7262\n",
      "Episode:174 meanR:16.3900 R:22.0000 rate:0.0440 aloss:0.9309 eloss:2.4746 exploreP:0.7247\n",
      "Episode:175 meanR:16.2500 R:12.0000 rate:0.0240 aloss:0.9528 eloss:2.4446 exploreP:0.7238\n",
      "Episode:176 meanR:16.3800 R:27.0000 rate:0.0540 aloss:0.9557 eloss:2.4825 exploreP:0.7219\n",
      "Episode:177 meanR:16.4200 R:13.0000 rate:0.0260 aloss:0.9740 eloss:2.4146 exploreP:0.7210\n",
      "Episode:178 meanR:16.4300 R:14.0000 rate:0.0280 aloss:1.0038 eloss:2.3875 exploreP:0.7200\n",
      "Episode:179 meanR:16.4400 R:18.0000 rate:0.0360 aloss:1.0033 eloss:2.3910 exploreP:0.7187\n",
      "Episode:180 meanR:16.4900 R:19.0000 rate:0.0380 aloss:0.9548 eloss:2.4391 exploreP:0.7173\n",
      "Episode:181 meanR:16.3800 R:13.0000 rate:0.0260 aloss:1.0215 eloss:2.3914 exploreP:0.7164\n",
      "Episode:182 meanR:16.3300 R:14.0000 rate:0.0280 aloss:1.0213 eloss:2.3855 exploreP:0.7154\n",
      "Episode:183 meanR:16.2200 R:11.0000 rate:0.0220 aloss:1.0461 eloss:2.3101 exploreP:0.7147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:184 meanR:16.2400 R:13.0000 rate:0.0260 aloss:1.0450 eloss:2.3896 exploreP:0.7137\n",
      "Episode:185 meanR:16.2900 R:18.0000 rate:0.0360 aloss:1.0556 eloss:2.3953 exploreP:0.7125\n",
      "Episode:186 meanR:16.3600 R:17.0000 rate:0.0340 aloss:1.0732 eloss:2.3173 exploreP:0.7113\n",
      "Episode:187 meanR:16.4000 R:16.0000 rate:0.0320 aloss:1.0680 eloss:2.3458 exploreP:0.7102\n",
      "Episode:188 meanR:16.1000 R:14.0000 rate:0.0280 aloss:1.0838 eloss:2.2711 exploreP:0.7092\n",
      "Episode:189 meanR:16.0500 R:11.0000 rate:0.0220 aloss:1.0998 eloss:2.3463 exploreP:0.7084\n",
      "Episode:190 meanR:16.1400 R:20.0000 rate:0.0400 aloss:1.1044 eloss:2.3035 exploreP:0.7070\n",
      "Episode:191 meanR:16.1200 R:11.0000 rate:0.0220 aloss:1.1143 eloss:2.2669 exploreP:0.7062\n",
      "Episode:192 meanR:16.1600 R:16.0000 rate:0.0320 aloss:1.1079 eloss:2.2882 exploreP:0.7051\n",
      "Episode:193 meanR:15.9200 R:13.0000 rate:0.0260 aloss:1.1376 eloss:2.2209 exploreP:0.7042\n",
      "Episode:194 meanR:15.9800 R:19.0000 rate:0.0380 aloss:1.1419 eloss:2.2566 exploreP:0.7029\n",
      "Episode:195 meanR:16.0200 R:20.0000 rate:0.0400 aloss:1.1442 eloss:2.2336 exploreP:0.7015\n",
      "Episode:196 meanR:16.0300 R:14.0000 rate:0.0280 aloss:1.1597 eloss:2.2483 exploreP:0.7006\n",
      "Episode:197 meanR:15.9900 R:16.0000 rate:0.0320 aloss:1.1619 eloss:2.2129 exploreP:0.6995\n",
      "Episode:198 meanR:15.9900 R:30.0000 rate:0.0600 aloss:1.1867 eloss:2.2002 exploreP:0.6974\n",
      "Episode:199 meanR:16.0200 R:19.0000 rate:0.0380 aloss:1.1981 eloss:2.1975 exploreP:0.6961\n",
      "Episode:200 meanR:15.9600 R:11.0000 rate:0.0220 aloss:1.1722 eloss:2.2240 exploreP:0.6953\n",
      "Episode:201 meanR:15.9100 R:10.0000 rate:0.0200 aloss:1.2089 eloss:2.1955 exploreP:0.6946\n",
      "Episode:202 meanR:15.9400 R:16.0000 rate:0.0320 aloss:1.2082 eloss:2.1774 exploreP:0.6936\n",
      "Episode:203 meanR:15.9900 R:16.0000 rate:0.0320 aloss:1.2143 eloss:2.1419 exploreP:0.6925\n",
      "Episode:204 meanR:15.9400 R:10.0000 rate:0.0200 aloss:1.2275 eloss:2.1763 exploreP:0.6918\n",
      "Episode:205 meanR:15.9200 R:14.0000 rate:0.0280 aloss:1.2348 eloss:2.2039 exploreP:0.6908\n",
      "Episode:206 meanR:15.9100 R:16.0000 rate:0.0320 aloss:1.2420 eloss:2.1405 exploreP:0.6897\n",
      "Episode:207 meanR:15.9500 R:15.0000 rate:0.0300 aloss:1.2430 eloss:2.1465 exploreP:0.6887\n",
      "Episode:208 meanR:15.9400 R:11.0000 rate:0.0220 aloss:1.2595 eloss:2.1153 exploreP:0.6880\n",
      "Episode:209 meanR:16.1000 R:26.0000 rate:0.0520 aloss:1.2602 eloss:2.1192 exploreP:0.6862\n",
      "Episode:210 meanR:16.1400 R:22.0000 rate:0.0440 aloss:1.2732 eloss:2.0993 exploreP:0.6847\n",
      "Episode:211 meanR:16.1000 R:16.0000 rate:0.0320 aloss:1.2687 eloss:2.1308 exploreP:0.6836\n",
      "Episode:212 meanR:16.1500 R:15.0000 rate:0.0300 aloss:1.2742 eloss:2.0683 exploreP:0.6826\n",
      "Episode:213 meanR:16.1400 R:14.0000 rate:0.0280 aloss:1.2915 eloss:2.1104 exploreP:0.6817\n",
      "Episode:214 meanR:16.0700 R:18.0000 rate:0.0360 aloss:1.2964 eloss:2.0958 exploreP:0.6805\n",
      "Episode:215 meanR:16.0800 R:11.0000 rate:0.0220 aloss:1.3034 eloss:2.1112 exploreP:0.6798\n",
      "Episode:216 meanR:16.0900 R:11.0000 rate:0.0220 aloss:1.3083 eloss:2.0837 exploreP:0.6790\n",
      "Episode:217 meanR:16.0700 R:15.0000 rate:0.0300 aloss:1.3062 eloss:2.1117 exploreP:0.6780\n",
      "Episode:218 meanR:16.0600 R:11.0000 rate:0.0220 aloss:1.3162 eloss:2.0951 exploreP:0.6773\n",
      "Episode:219 meanR:16.1100 R:16.0000 rate:0.0320 aloss:1.3149 eloss:2.0575 exploreP:0.6762\n",
      "Episode:220 meanR:16.0700 R:9.0000 rate:0.0180 aloss:1.3303 eloss:2.1386 exploreP:0.6756\n",
      "Episode:221 meanR:16.1500 R:20.0000 rate:0.0400 aloss:1.3267 eloss:2.0841 exploreP:0.6743\n",
      "Episode:222 meanR:16.1500 R:10.0000 rate:0.0200 aloss:1.3346 eloss:2.1132 exploreP:0.6736\n",
      "Episode:223 meanR:16.1300 R:19.0000 rate:0.0380 aloss:1.3371 eloss:2.0817 exploreP:0.6724\n",
      "Episode:224 meanR:16.0600 R:18.0000 rate:0.0360 aloss:1.3370 eloss:2.0573 exploreP:0.6712\n",
      "Episode:225 meanR:16.0900 R:16.0000 rate:0.0320 aloss:1.3435 eloss:1.9663 exploreP:0.6701\n",
      "Episode:226 meanR:16.0300 R:14.0000 rate:0.0280 aloss:1.3583 eloss:2.0714 exploreP:0.6692\n",
      "Episode:227 meanR:16.0300 R:12.0000 rate:0.0240 aloss:1.3101 eloss:1.9853 exploreP:0.6684\n",
      "Episode:228 meanR:16.0000 R:14.0000 rate:0.0280 aloss:1.3363 eloss:2.1217 exploreP:0.6675\n",
      "Episode:229 meanR:15.9300 R:10.0000 rate:0.0200 aloss:1.3574 eloss:2.0348 exploreP:0.6668\n",
      "Episode:230 meanR:15.6000 R:10.0000 rate:0.0200 aloss:1.3537 eloss:2.0305 exploreP:0.6662\n",
      "Episode:231 meanR:15.6100 R:13.0000 rate:0.0260 aloss:1.3785 eloss:1.9799 exploreP:0.6653\n",
      "Episode:232 meanR:15.5900 R:11.0000 rate:0.0220 aloss:1.3403 eloss:1.9928 exploreP:0.6646\n",
      "Episode:233 meanR:15.5900 R:14.0000 rate:0.0280 aloss:1.3752 eloss:1.9939 exploreP:0.6637\n",
      "Episode:234 meanR:15.5400 R:11.0000 rate:0.0220 aloss:1.3725 eloss:2.0263 exploreP:0.6630\n",
      "Episode:235 meanR:15.5700 R:14.0000 rate:0.0280 aloss:1.3645 eloss:2.0142 exploreP:0.6620\n",
      "Episode:236 meanR:15.6000 R:19.0000 rate:0.0380 aloss:1.3898 eloss:2.0194 exploreP:0.6608\n",
      "Episode:237 meanR:15.5700 R:11.0000 rate:0.0220 aloss:1.3431 eloss:2.0027 exploreP:0.6601\n",
      "Episode:238 meanR:15.5400 R:9.0000 rate:0.0180 aloss:1.3922 eloss:2.0485 exploreP:0.6595\n",
      "Episode:239 meanR:15.5500 R:14.0000 rate:0.0280 aloss:1.3948 eloss:2.0352 exploreP:0.6586\n",
      "Episode:240 meanR:15.5500 R:14.0000 rate:0.0280 aloss:1.3986 eloss:1.9967 exploreP:0.6577\n",
      "Episode:241 meanR:15.5300 R:12.0000 rate:0.0240 aloss:1.3902 eloss:1.9888 exploreP:0.6569\n",
      "Episode:242 meanR:15.4600 R:10.0000 rate:0.0200 aloss:1.3941 eloss:1.9885 exploreP:0.6563\n",
      "Episode:243 meanR:15.4600 R:11.0000 rate:0.0220 aloss:1.3961 eloss:2.0360 exploreP:0.6556\n",
      "Episode:244 meanR:15.5100 R:16.0000 rate:0.0320 aloss:1.4040 eloss:1.9780 exploreP:0.6545\n",
      "Episode:245 meanR:15.5200 R:11.0000 rate:0.0220 aloss:1.3992 eloss:1.9944 exploreP:0.6538\n",
      "Episode:246 meanR:15.6400 R:25.0000 rate:0.0500 aloss:1.3265 eloss:2.0630 exploreP:0.6522\n",
      "Episode:247 meanR:15.6200 R:11.0000 rate:0.0220 aloss:1.2821 eloss:2.1821 exploreP:0.6515\n",
      "Episode:248 meanR:15.5700 R:9.0000 rate:0.0180 aloss:1.3998 eloss:1.9457 exploreP:0.6509\n",
      "Episode:249 meanR:15.5500 R:23.0000 rate:0.0460 aloss:1.3500 eloss:2.0693 exploreP:0.6494\n",
      "Episode:250 meanR:15.5100 R:13.0000 rate:0.0260 aloss:1.4314 eloss:1.9395 exploreP:0.6486\n",
      "Episode:251 meanR:15.4200 R:11.0000 rate:0.0220 aloss:1.4297 eloss:1.9371 exploreP:0.6479\n",
      "Episode:252 meanR:15.4500 R:22.0000 rate:0.0440 aloss:1.4228 eloss:1.9345 exploreP:0.6465\n",
      "Episode:253 meanR:15.4100 R:12.0000 rate:0.0240 aloss:1.4316 eloss:1.9798 exploreP:0.6457\n",
      "Episode:254 meanR:15.4000 R:13.0000 rate:0.0260 aloss:1.4324 eloss:1.9676 exploreP:0.6449\n",
      "Episode:255 meanR:15.3300 R:11.0000 rate:0.0220 aloss:1.4248 eloss:1.9599 exploreP:0.6442\n",
      "Episode:256 meanR:15.3200 R:14.0000 rate:0.0280 aloss:1.4312 eloss:1.9485 exploreP:0.6433\n",
      "Episode:257 meanR:15.2800 R:15.0000 rate:0.0300 aloss:1.4353 eloss:1.9452 exploreP:0.6424\n",
      "Episode:258 meanR:15.1500 R:10.0000 rate:0.0200 aloss:1.4276 eloss:1.9226 exploreP:0.6418\n",
      "Episode:259 meanR:15.0800 R:9.0000 rate:0.0180 aloss:1.4137 eloss:1.9526 exploreP:0.6412\n",
      "Episode:260 meanR:15.0500 R:21.0000 rate:0.0420 aloss:1.4378 eloss:1.9452 exploreP:0.6399\n",
      "Episode:261 meanR:15.0100 R:18.0000 rate:0.0360 aloss:1.4354 eloss:1.9405 exploreP:0.6387\n",
      "Episode:262 meanR:14.9600 R:9.0000 rate:0.0180 aloss:1.4254 eloss:2.0156 exploreP:0.6382\n",
      "Episode:263 meanR:14.8500 R:9.0000 rate:0.0180 aloss:1.4691 eloss:1.8949 exploreP:0.6376\n",
      "Episode:264 meanR:14.8200 R:11.0000 rate:0.0220 aloss:1.4386 eloss:1.9457 exploreP:0.6369\n",
      "Episode:265 meanR:14.7700 R:10.0000 rate:0.0200 aloss:1.4595 eloss:1.9554 exploreP:0.6363\n",
      "Episode:266 meanR:14.5900 R:12.0000 rate:0.0240 aloss:1.4474 eloss:1.9255 exploreP:0.6355\n",
      "Episode:267 meanR:14.5800 R:12.0000 rate:0.0240 aloss:1.4404 eloss:1.9473 exploreP:0.6348\n",
      "Episode:268 meanR:14.6500 R:17.0000 rate:0.0340 aloss:1.3921 eloss:2.0077 exploreP:0.6337\n",
      "Episode:269 meanR:14.6800 R:12.0000 rate:0.0240 aloss:1.4531 eloss:1.9761 exploreP:0.6330\n",
      "Episode:270 meanR:14.5900 R:14.0000 rate:0.0280 aloss:1.4566 eloss:1.9288 exploreP:0.6321\n",
      "Episode:271 meanR:14.6100 R:15.0000 rate:0.0300 aloss:1.3932 eloss:2.0226 exploreP:0.6312\n",
      "Episode:272 meanR:14.7000 R:25.0000 rate:0.0500 aloss:1.4679 eloss:1.8757 exploreP:0.6296\n",
      "Episode:273 meanR:14.6100 R:12.0000 rate:0.0240 aloss:1.4562 eloss:1.9501 exploreP:0.6289\n",
      "Episode:274 meanR:14.5100 R:12.0000 rate:0.0240 aloss:1.4849 eloss:1.9313 exploreP:0.6281\n",
      "Episode:275 meanR:14.5000 R:11.0000 rate:0.0220 aloss:1.4730 eloss:1.9375 exploreP:0.6275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:276 meanR:14.4000 R:17.0000 rate:0.0340 aloss:1.4700 eloss:1.8911 exploreP:0.6264\n",
      "Episode:277 meanR:14.4400 R:17.0000 rate:0.0340 aloss:1.4845 eloss:1.9031 exploreP:0.6254\n",
      "Episode:278 meanR:14.4000 R:10.0000 rate:0.0200 aloss:1.4781 eloss:1.8971 exploreP:0.6247\n",
      "Episode:279 meanR:14.3200 R:10.0000 rate:0.0200 aloss:1.4890 eloss:1.9267 exploreP:0.6241\n",
      "Episode:280 meanR:14.3300 R:20.0000 rate:0.0400 aloss:1.4880 eloss:1.8617 exploreP:0.6229\n",
      "Episode:281 meanR:14.3300 R:13.0000 rate:0.0260 aloss:1.4430 eloss:1.9170 exploreP:0.6221\n",
      "Episode:282 meanR:14.3700 R:18.0000 rate:0.0360 aloss:1.4915 eloss:1.9414 exploreP:0.6210\n",
      "Episode:283 meanR:14.5500 R:29.0000 rate:0.0580 aloss:1.4848 eloss:1.9404 exploreP:0.6192\n",
      "Episode:284 meanR:14.5800 R:16.0000 rate:0.0320 aloss:1.4428 eloss:1.9968 exploreP:0.6183\n",
      "Episode:285 meanR:14.5500 R:15.0000 rate:0.0300 aloss:1.4847 eloss:1.9173 exploreP:0.6173\n",
      "Episode:286 meanR:14.5000 R:12.0000 rate:0.0240 aloss:1.3810 eloss:1.9203 exploreP:0.6166\n",
      "Episode:287 meanR:14.4500 R:11.0000 rate:0.0220 aloss:1.5187 eloss:1.9082 exploreP:0.6160\n",
      "Episode:288 meanR:14.4500 R:14.0000 rate:0.0280 aloss:1.5097 eloss:1.8956 exploreP:0.6151\n",
      "Episode:289 meanR:14.4300 R:9.0000 rate:0.0180 aloss:1.5366 eloss:1.8995 exploreP:0.6146\n",
      "Episode:290 meanR:14.4000 R:17.0000 rate:0.0340 aloss:1.5134 eloss:1.9019 exploreP:0.6135\n",
      "Episode:291 meanR:14.3900 R:10.0000 rate:0.0200 aloss:1.5171 eloss:1.8046 exploreP:0.6129\n",
      "Episode:292 meanR:14.3700 R:14.0000 rate:0.0280 aloss:1.5167 eloss:1.9096 exploreP:0.6121\n",
      "Episode:293 meanR:14.3400 R:10.0000 rate:0.0200 aloss:1.5085 eloss:1.8806 exploreP:0.6115\n",
      "Episode:294 meanR:14.2600 R:11.0000 rate:0.0220 aloss:1.5233 eloss:1.8784 exploreP:0.6108\n",
      "Episode:295 meanR:14.3300 R:27.0000 rate:0.0540 aloss:1.5300 eloss:1.9084 exploreP:0.6092\n",
      "Episode:296 meanR:14.3000 R:11.0000 rate:0.0220 aloss:1.5361 eloss:1.7919 exploreP:0.6085\n",
      "Episode:297 meanR:14.2900 R:15.0000 rate:0.0300 aloss:1.5422 eloss:1.8283 exploreP:0.6076\n",
      "Episode:298 meanR:14.1000 R:11.0000 rate:0.0220 aloss:1.5657 eloss:1.7824 exploreP:0.6070\n",
      "Episode:299 meanR:14.0100 R:10.0000 rate:0.0200 aloss:1.4752 eloss:1.8561 exploreP:0.6064\n",
      "Episode:300 meanR:14.0400 R:14.0000 rate:0.0280 aloss:1.5462 eloss:1.7829 exploreP:0.6056\n",
      "Episode:301 meanR:14.1200 R:18.0000 rate:0.0360 aloss:1.5423 eloss:1.7989 exploreP:0.6045\n",
      "Episode:302 meanR:14.1300 R:17.0000 rate:0.0340 aloss:1.5550 eloss:1.8150 exploreP:0.6035\n",
      "Episode:303 meanR:14.0700 R:10.0000 rate:0.0200 aloss:1.5505 eloss:1.8155 exploreP:0.6029\n",
      "Episode:304 meanR:14.0800 R:11.0000 rate:0.0220 aloss:1.5630 eloss:1.8199 exploreP:0.6022\n",
      "Episode:305 meanR:14.0600 R:12.0000 rate:0.0240 aloss:1.6017 eloss:1.7419 exploreP:0.6015\n",
      "Episode:306 meanR:14.0500 R:15.0000 rate:0.0300 aloss:1.4470 eloss:1.9379 exploreP:0.6006\n",
      "Episode:307 meanR:14.0400 R:14.0000 rate:0.0280 aloss:1.6052 eloss:1.7973 exploreP:0.5998\n",
      "Episode:308 meanR:14.0500 R:12.0000 rate:0.0240 aloss:1.5810 eloss:1.7798 exploreP:0.5991\n",
      "Episode:309 meanR:13.9300 R:14.0000 rate:0.0280 aloss:1.6099 eloss:1.7427 exploreP:0.5983\n",
      "Episode:310 meanR:13.8600 R:15.0000 rate:0.0300 aloss:1.6301 eloss:1.7250 exploreP:0.5974\n",
      "Episode:311 meanR:13.8300 R:13.0000 rate:0.0260 aloss:1.6352 eloss:1.8132 exploreP:0.5966\n",
      "Episode:312 meanR:13.7700 R:9.0000 rate:0.0180 aloss:1.6532 eloss:1.7607 exploreP:0.5961\n",
      "Episode:313 meanR:13.7600 R:13.0000 rate:0.0260 aloss:1.6500 eloss:1.7457 exploreP:0.5953\n",
      "Episode:314 meanR:13.6800 R:10.0000 rate:0.0200 aloss:1.6522 eloss:1.7346 exploreP:0.5948\n",
      "Episode:315 meanR:13.7000 R:13.0000 rate:0.0260 aloss:1.6593 eloss:1.7539 exploreP:0.5940\n",
      "Episode:316 meanR:13.6800 R:9.0000 rate:0.0180 aloss:1.6568 eloss:1.8211 exploreP:0.5935\n",
      "Episode:317 meanR:13.6800 R:15.0000 rate:0.0300 aloss:1.6782 eloss:1.7140 exploreP:0.5926\n",
      "Episode:318 meanR:13.6800 R:11.0000 rate:0.0220 aloss:1.6806 eloss:1.7375 exploreP:0.5920\n",
      "Episode:319 meanR:13.6500 R:13.0000 rate:0.0260 aloss:1.6850 eloss:1.7485 exploreP:0.5912\n",
      "Episode:320 meanR:13.7200 R:16.0000 rate:0.0320 aloss:1.6226 eloss:1.7531 exploreP:0.5903\n",
      "Episode:321 meanR:13.6600 R:14.0000 rate:0.0280 aloss:1.7129 eloss:1.7221 exploreP:0.5895\n",
      "Episode:322 meanR:13.7000 R:14.0000 rate:0.0280 aloss:1.6490 eloss:1.8119 exploreP:0.5887\n",
      "Episode:323 meanR:13.6300 R:12.0000 rate:0.0240 aloss:1.6841 eloss:1.7298 exploreP:0.5880\n",
      "Episode:324 meanR:13.6000 R:15.0000 rate:0.0300 aloss:1.7358 eloss:1.6754 exploreP:0.5871\n",
      "Episode:325 meanR:13.5300 R:9.0000 rate:0.0180 aloss:1.7543 eloss:1.6891 exploreP:0.5866\n",
      "Episode:326 meanR:13.5000 R:11.0000 rate:0.0220 aloss:1.7408 eloss:1.6566 exploreP:0.5859\n",
      "Episode:327 meanR:13.4900 R:11.0000 rate:0.0220 aloss:1.7568 eloss:1.6722 exploreP:0.5853\n",
      "Episode:328 meanR:13.4400 R:9.0000 rate:0.0180 aloss:1.7684 eloss:1.6078 exploreP:0.5848\n",
      "Episode:329 meanR:13.4700 R:13.0000 rate:0.0260 aloss:1.7784 eloss:1.6657 exploreP:0.5840\n",
      "Episode:330 meanR:13.4600 R:9.0000 rate:0.0180 aloss:1.7862 eloss:1.6950 exploreP:0.5835\n",
      "Episode:331 meanR:13.4600 R:13.0000 rate:0.0260 aloss:1.7895 eloss:1.5913 exploreP:0.5828\n",
      "Episode:332 meanR:13.4600 R:11.0000 rate:0.0220 aloss:1.8012 eloss:1.6164 exploreP:0.5822\n",
      "Episode:333 meanR:13.4500 R:13.0000 rate:0.0260 aloss:1.7991 eloss:1.5953 exploreP:0.5814\n",
      "Episode:334 meanR:13.4500 R:11.0000 rate:0.0220 aloss:1.7893 eloss:1.6266 exploreP:0.5808\n",
      "Episode:335 meanR:13.4400 R:13.0000 rate:0.0260 aloss:1.8209 eloss:1.6102 exploreP:0.5800\n",
      "Episode:336 meanR:13.3400 R:9.0000 rate:0.0180 aloss:1.8257 eloss:1.5942 exploreP:0.5795\n",
      "Episode:337 meanR:13.3800 R:15.0000 rate:0.0300 aloss:1.8213 eloss:1.5312 exploreP:0.5787\n",
      "Episode:338 meanR:13.3900 R:10.0000 rate:0.0200 aloss:1.7997 eloss:1.6254 exploreP:0.5781\n",
      "Episode:339 meanR:13.3800 R:13.0000 rate:0.0260 aloss:1.8432 eloss:1.5754 exploreP:0.5774\n",
      "Episode:340 meanR:13.4300 R:19.0000 rate:0.0380 aloss:1.8580 eloss:1.5520 exploreP:0.5763\n",
      "Episode:341 meanR:13.4700 R:16.0000 rate:0.0320 aloss:1.8637 eloss:1.5709 exploreP:0.5754\n",
      "Episode:342 meanR:13.5700 R:20.0000 rate:0.0400 aloss:1.8599 eloss:1.5693 exploreP:0.5743\n",
      "Episode:343 meanR:13.5900 R:13.0000 rate:0.0260 aloss:1.8612 eloss:1.5502 exploreP:0.5735\n",
      "Episode:344 meanR:13.6200 R:19.0000 rate:0.0380 aloss:1.8884 eloss:1.5717 exploreP:0.5725\n",
      "Episode:345 meanR:13.6300 R:12.0000 rate:0.0240 aloss:1.8845 eloss:1.5772 exploreP:0.5718\n",
      "Episode:346 meanR:13.4800 R:10.0000 rate:0.0200 aloss:1.9124 eloss:1.5112 exploreP:0.5712\n",
      "Episode:347 meanR:13.5800 R:21.0000 rate:0.0420 aloss:1.9037 eloss:1.5468 exploreP:0.5700\n",
      "Episode:348 meanR:13.6200 R:13.0000 rate:0.0260 aloss:1.9098 eloss:1.5253 exploreP:0.5693\n",
      "Episode:349 meanR:13.5400 R:15.0000 rate:0.0300 aloss:1.9064 eloss:1.5701 exploreP:0.5685\n",
      "Episode:350 meanR:13.5300 R:12.0000 rate:0.0240 aloss:1.9260 eloss:1.5399 exploreP:0.5678\n",
      "Episode:351 meanR:13.5000 R:8.0000 rate:0.0160 aloss:1.8912 eloss:1.5688 exploreP:0.5674\n",
      "Episode:352 meanR:13.3900 R:11.0000 rate:0.0220 aloss:1.9367 eloss:1.4917 exploreP:0.5667\n",
      "Episode:353 meanR:13.4100 R:14.0000 rate:0.0280 aloss:1.9349 eloss:1.4736 exploreP:0.5660\n",
      "Episode:354 meanR:13.4200 R:14.0000 rate:0.0280 aloss:1.9412 eloss:1.4747 exploreP:0.5652\n",
      "Episode:355 meanR:13.4100 R:10.0000 rate:0.0200 aloss:1.9395 eloss:1.4716 exploreP:0.5646\n",
      "Episode:356 meanR:13.4500 R:18.0000 rate:0.0360 aloss:1.9410 eloss:1.5043 exploreP:0.5636\n",
      "Episode:357 meanR:13.4000 R:10.0000 rate:0.0200 aloss:1.9405 eloss:1.5599 exploreP:0.5631\n",
      "Episode:358 meanR:13.4400 R:14.0000 rate:0.0280 aloss:1.9698 eloss:1.4965 exploreP:0.5623\n",
      "Episode:359 meanR:13.4500 R:10.0000 rate:0.0200 aloss:1.8712 eloss:1.5563 exploreP:0.5618\n",
      "Episode:360 meanR:13.4700 R:23.0000 rate:0.0460 aloss:1.9673 eloss:1.4968 exploreP:0.5605\n",
      "Episode:361 meanR:13.3900 R:10.0000 rate:0.0200 aloss:1.9840 eloss:1.4247 exploreP:0.5599\n",
      "Episode:362 meanR:13.4800 R:18.0000 rate:0.0360 aloss:1.9016 eloss:1.5332 exploreP:0.5589\n",
      "Episode:363 meanR:13.5300 R:14.0000 rate:0.0280 aloss:1.9221 eloss:1.5946 exploreP:0.5582\n",
      "Episode:364 meanR:13.5900 R:17.0000 rate:0.0340 aloss:1.9715 eloss:1.4750 exploreP:0.5572\n",
      "Episode:365 meanR:13.5900 R:10.0000 rate:0.0200 aloss:1.9917 eloss:1.4224 exploreP:0.5567\n",
      "Episode:366 meanR:13.5700 R:10.0000 rate:0.0200 aloss:1.9756 eloss:1.5227 exploreP:0.5562\n",
      "Episode:367 meanR:13.6400 R:19.0000 rate:0.0380 aloss:1.9874 eloss:1.4891 exploreP:0.5551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:368 meanR:13.5600 R:9.0000 rate:0.0180 aloss:2.0017 eloss:1.4190 exploreP:0.5546\n",
      "Episode:369 meanR:13.5800 R:14.0000 rate:0.0280 aloss:1.9655 eloss:1.4364 exploreP:0.5539\n",
      "Episode:370 meanR:13.5400 R:10.0000 rate:0.0200 aloss:1.8257 eloss:1.6330 exploreP:0.5533\n",
      "Episode:371 meanR:13.4800 R:9.0000 rate:0.0180 aloss:1.9817 eloss:1.5298 exploreP:0.5528\n",
      "Episode:372 meanR:13.3300 R:10.0000 rate:0.0200 aloss:2.0117 eloss:1.4596 exploreP:0.5523\n",
      "Episode:373 meanR:13.3700 R:16.0000 rate:0.0320 aloss:2.0193 eloss:1.4775 exploreP:0.5514\n",
      "Episode:374 meanR:13.4600 R:21.0000 rate:0.0420 aloss:2.0186 eloss:1.5114 exploreP:0.5503\n",
      "Episode:375 meanR:13.4700 R:12.0000 rate:0.0240 aloss:1.9829 eloss:1.4012 exploreP:0.5496\n",
      "Episode:376 meanR:13.4800 R:18.0000 rate:0.0360 aloss:1.9735 eloss:1.5122 exploreP:0.5487\n",
      "Episode:377 meanR:13.4300 R:12.0000 rate:0.0240 aloss:2.0219 eloss:1.4493 exploreP:0.5480\n",
      "Episode:378 meanR:13.4400 R:11.0000 rate:0.0220 aloss:2.0139 eloss:1.4800 exploreP:0.5474\n",
      "Episode:379 meanR:13.4500 R:11.0000 rate:0.0220 aloss:1.9702 eloss:1.3771 exploreP:0.5468\n",
      "Episode:380 meanR:13.3700 R:12.0000 rate:0.0240 aloss:1.9753 eloss:1.4713 exploreP:0.5462\n",
      "Episode:381 meanR:13.3800 R:14.0000 rate:0.0280 aloss:1.9612 eloss:1.5068 exploreP:0.5454\n",
      "Episode:382 meanR:13.3600 R:16.0000 rate:0.0320 aloss:1.9999 eloss:1.4430 exploreP:0.5446\n",
      "Episode:383 meanR:13.1900 R:12.0000 rate:0.0240 aloss:2.0054 eloss:1.4384 exploreP:0.5440\n",
      "Episode:384 meanR:13.1200 R:9.0000 rate:0.0180 aloss:2.0502 eloss:1.4892 exploreP:0.5435\n",
      "Episode:385 meanR:13.0800 R:11.0000 rate:0.0220 aloss:2.0289 eloss:1.3885 exploreP:0.5429\n",
      "Episode:386 meanR:13.0700 R:11.0000 rate:0.0220 aloss:2.0155 eloss:1.4871 exploreP:0.5423\n",
      "Episode:387 meanR:13.1300 R:17.0000 rate:0.0340 aloss:2.0426 eloss:1.4248 exploreP:0.5414\n",
      "Episode:388 meanR:13.0900 R:10.0000 rate:0.0200 aloss:2.0457 eloss:1.4390 exploreP:0.5409\n",
      "Episode:389 meanR:13.1400 R:14.0000 rate:0.0280 aloss:2.0616 eloss:1.3933 exploreP:0.5401\n",
      "Episode:390 meanR:13.1100 R:14.0000 rate:0.0280 aloss:2.0477 eloss:1.4706 exploreP:0.5394\n",
      "Episode:391 meanR:13.1000 R:9.0000 rate:0.0180 aloss:2.0254 eloss:1.3933 exploreP:0.5389\n",
      "Episode:392 meanR:13.0400 R:8.0000 rate:0.0160 aloss:2.0255 eloss:1.4724 exploreP:0.5385\n",
      "Episode:393 meanR:13.0800 R:14.0000 rate:0.0280 aloss:2.0411 eloss:1.4210 exploreP:0.5377\n",
      "Episode:394 meanR:13.1000 R:13.0000 rate:0.0260 aloss:2.0646 eloss:1.4322 exploreP:0.5371\n",
      "Episode:395 meanR:12.9400 R:11.0000 rate:0.0220 aloss:1.7392 eloss:1.7127 exploreP:0.5365\n",
      "Episode:396 meanR:12.9500 R:12.0000 rate:0.0240 aloss:2.0512 eloss:1.4220 exploreP:0.5358\n",
      "Episode:397 meanR:12.9900 R:19.0000 rate:0.0380 aloss:2.0513 eloss:1.4507 exploreP:0.5348\n",
      "Episode:398 meanR:13.0000 R:12.0000 rate:0.0240 aloss:2.0709 eloss:1.4165 exploreP:0.5342\n",
      "Episode:399 meanR:13.0200 R:12.0000 rate:0.0240 aloss:1.7526 eloss:1.7530 exploreP:0.5336\n",
      "Episode:400 meanR:12.9900 R:11.0000 rate:0.0220 aloss:2.0840 eloss:1.3832 exploreP:0.5330\n",
      "Episode:401 meanR:12.9000 R:9.0000 rate:0.0180 aloss:2.0747 eloss:1.3485 exploreP:0.5325\n",
      "Episode:402 meanR:12.8300 R:10.0000 rate:0.0200 aloss:2.0785 eloss:1.3043 exploreP:0.5320\n",
      "Episode:403 meanR:12.8500 R:12.0000 rate:0.0240 aloss:2.0826 eloss:1.4224 exploreP:0.5314\n",
      "Episode:404 meanR:12.8600 R:12.0000 rate:0.0240 aloss:2.0768 eloss:1.3925 exploreP:0.5308\n",
      "Episode:405 meanR:12.8900 R:15.0000 rate:0.0300 aloss:2.0806 eloss:1.4393 exploreP:0.5300\n",
      "Episode:406 meanR:12.8800 R:14.0000 rate:0.0280 aloss:2.0946 eloss:1.3290 exploreP:0.5293\n",
      "Episode:407 meanR:12.8400 R:10.0000 rate:0.0200 aloss:2.0835 eloss:1.3289 exploreP:0.5287\n",
      "Episode:408 meanR:12.8000 R:8.0000 rate:0.0160 aloss:2.1107 eloss:1.3782 exploreP:0.5283\n",
      "Episode:409 meanR:12.7700 R:11.0000 rate:0.0220 aloss:2.0269 eloss:1.4550 exploreP:0.5278\n",
      "Episode:410 meanR:12.7700 R:15.0000 rate:0.0300 aloss:2.1156 eloss:1.3665 exploreP:0.5270\n",
      "Episode:411 meanR:12.7500 R:11.0000 rate:0.0220 aloss:2.1084 eloss:1.2863 exploreP:0.5264\n",
      "Episode:412 meanR:12.7700 R:11.0000 rate:0.0220 aloss:2.1086 eloss:1.3922 exploreP:0.5258\n",
      "Episode:413 meanR:12.8300 R:19.0000 rate:0.0380 aloss:2.1267 eloss:1.3467 exploreP:0.5249\n",
      "Episode:414 meanR:12.8800 R:15.0000 rate:0.0300 aloss:2.1263 eloss:1.3230 exploreP:0.5241\n",
      "Episode:415 meanR:12.8500 R:10.0000 rate:0.0200 aloss:2.1362 eloss:1.3447 exploreP:0.5236\n",
      "Episode:416 meanR:12.8700 R:11.0000 rate:0.0220 aloss:2.1390 eloss:1.3293 exploreP:0.5230\n",
      "Episode:417 meanR:12.8100 R:9.0000 rate:0.0180 aloss:2.1602 eloss:1.3082 exploreP:0.5226\n",
      "Episode:418 meanR:12.7900 R:9.0000 rate:0.0180 aloss:2.1241 eloss:1.2816 exploreP:0.5221\n",
      "Episode:419 meanR:12.7600 R:10.0000 rate:0.0200 aloss:2.1507 eloss:1.3020 exploreP:0.5216\n",
      "Episode:420 meanR:12.7200 R:12.0000 rate:0.0240 aloss:2.1373 eloss:1.3206 exploreP:0.5210\n",
      "Episode:421 meanR:12.6700 R:9.0000 rate:0.0180 aloss:2.1527 eloss:1.3422 exploreP:0.5205\n",
      "Episode:422 meanR:12.7900 R:26.0000 rate:0.0520 aloss:2.0036 eloss:1.4514 exploreP:0.5192\n",
      "Episode:423 meanR:12.7700 R:10.0000 rate:0.0200 aloss:2.1725 eloss:1.2627 exploreP:0.5187\n",
      "Episode:424 meanR:12.7200 R:10.0000 rate:0.0200 aloss:2.1716 eloss:1.2758 exploreP:0.5182\n",
      "Episode:425 meanR:12.7400 R:11.0000 rate:0.0220 aloss:1.7900 eloss:1.6694 exploreP:0.5176\n",
      "Episode:426 meanR:12.8400 R:21.0000 rate:0.0420 aloss:2.1836 eloss:1.2606 exploreP:0.5165\n",
      "Episode:427 meanR:12.8200 R:9.0000 rate:0.0180 aloss:2.1965 eloss:1.2814 exploreP:0.5161\n",
      "Episode:428 meanR:12.8600 R:13.0000 rate:0.0260 aloss:2.2004 eloss:1.2637 exploreP:0.5154\n",
      "Episode:429 meanR:12.8800 R:15.0000 rate:0.0300 aloss:2.1996 eloss:1.2737 exploreP:0.5147\n",
      "Episode:430 meanR:12.9300 R:14.0000 rate:0.0280 aloss:2.2024 eloss:1.2646 exploreP:0.5140\n",
      "Episode:431 meanR:12.9000 R:10.0000 rate:0.0200 aloss:2.2224 eloss:1.2700 exploreP:0.5135\n",
      "Episode:432 meanR:12.9100 R:12.0000 rate:0.0240 aloss:2.1862 eloss:1.2653 exploreP:0.5129\n",
      "Episode:433 meanR:12.8700 R:9.0000 rate:0.0180 aloss:2.2183 eloss:1.2758 exploreP:0.5124\n",
      "Episode:434 meanR:12.9100 R:15.0000 rate:0.0300 aloss:2.2241 eloss:1.2724 exploreP:0.5117\n",
      "Episode:435 meanR:12.8700 R:9.0000 rate:0.0180 aloss:2.2374 eloss:1.2313 exploreP:0.5112\n",
      "Episode:436 meanR:12.9400 R:16.0000 rate:0.0320 aloss:2.2398 eloss:1.2610 exploreP:0.5104\n",
      "Episode:437 meanR:12.8700 R:8.0000 rate:0.0160 aloss:2.2559 eloss:1.2130 exploreP:0.5100\n",
      "Episode:438 meanR:12.9100 R:14.0000 rate:0.0280 aloss:2.2127 eloss:1.2943 exploreP:0.5093\n",
      "Episode:439 meanR:12.8900 R:11.0000 rate:0.0220 aloss:2.2274 eloss:1.1614 exploreP:0.5087\n",
      "Episode:440 meanR:12.8100 R:11.0000 rate:0.0220 aloss:2.0921 eloss:1.3511 exploreP:0.5082\n",
      "Episode:441 meanR:12.8000 R:15.0000 rate:0.0300 aloss:2.1943 eloss:1.2219 exploreP:0.5075\n",
      "Episode:442 meanR:12.7600 R:16.0000 rate:0.0320 aloss:2.2478 eloss:1.2147 exploreP:0.5067\n",
      "Episode:443 meanR:12.7400 R:11.0000 rate:0.0220 aloss:2.2473 eloss:1.1521 exploreP:0.5061\n",
      "Episode:444 meanR:12.7000 R:15.0000 rate:0.0300 aloss:2.2813 eloss:1.1546 exploreP:0.5054\n",
      "Episode:445 meanR:12.7000 R:12.0000 rate:0.0240 aloss:2.2885 eloss:1.1941 exploreP:0.5048\n",
      "Episode:446 meanR:12.6900 R:9.0000 rate:0.0180 aloss:2.2832 eloss:1.1724 exploreP:0.5043\n",
      "Episode:447 meanR:12.5800 R:10.0000 rate:0.0200 aloss:2.2822 eloss:1.1527 exploreP:0.5038\n",
      "Episode:448 meanR:12.5400 R:9.0000 rate:0.0180 aloss:2.2828 eloss:1.1330 exploreP:0.5034\n",
      "Episode:449 meanR:12.5500 R:16.0000 rate:0.0320 aloss:2.3162 eloss:1.1083 exploreP:0.5026\n",
      "Episode:450 meanR:12.6000 R:17.0000 rate:0.0340 aloss:2.3029 eloss:1.1921 exploreP:0.5018\n",
      "Episode:451 meanR:12.6800 R:16.0000 rate:0.0320 aloss:2.2042 eloss:1.3031 exploreP:0.5010\n",
      "Episode:452 meanR:12.6600 R:9.0000 rate:0.0180 aloss:2.3113 eloss:1.1621 exploreP:0.5005\n",
      "Episode:453 meanR:12.6500 R:13.0000 rate:0.0260 aloss:2.3102 eloss:1.1400 exploreP:0.4999\n",
      "Episode:454 meanR:12.6400 R:13.0000 rate:0.0260 aloss:2.3296 eloss:1.1539 exploreP:0.4993\n",
      "Episode:455 meanR:12.6800 R:14.0000 rate:0.0280 aloss:2.3035 eloss:1.1667 exploreP:0.4986\n",
      "Episode:456 meanR:12.6000 R:10.0000 rate:0.0200 aloss:2.2883 eloss:1.1543 exploreP:0.4981\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, eloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        aloss_batch, eloss_batch, aloss2_batch = [], [], []\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        rate = -1\n",
    "\n",
    "        # Training steps/batches\n",
    "        for num_step in range(11111):\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Online policy/on-policy/online training\n",
    "            states, actions, next_states, rewards, dones, rates = minibacth(memory=memory)\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states,\n",
    "                         model.rewards: rewards, model.dones: dones, model.rates: rates}\n",
    "            eloss, _ = sess.run([model.e_loss, model.e_opt], feed_dict)\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt], feed_dict)\n",
    "            eloss_batch.append(eloss)\n",
    "            aloss_batch.append(aloss)\n",
    "            # End of episode/result/success rate available\n",
    "            if done is True:\n",
    "                # Rating the latest played episode\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                break\n",
    "\n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'eloss:{:.4f}'.format(np.mean(eloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        eloss_list.append([ep, np.mean(eloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of 500 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Act losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(eloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Env losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
