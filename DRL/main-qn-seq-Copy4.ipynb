{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.02782733 -0.19801476  0.04266009  0.30762757] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02386703 -0.00352583  0.04881264  0.02869784] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02379652  0.19086337  0.0493866  -0.24819343] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02761378 -0.00492783  0.04442273  0.05964904] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02751523  0.18952997  0.04561571 -0.21869391] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03130583  0.38397118  0.04124183 -0.49664596] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03898525  0.57848806  0.03130891 -0.77605152] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.05055501  0.77316572  0.01578788 -1.05872159] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06601833  0.968075   -0.00538655 -1.34640762] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.08537983  1.16326426 -0.0323147  -1.6407709 ] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    labelQs = tf.placeholder(tf.float32, [None], name='labelQs')\n",
    "        \n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return actions, states, targetQs, labelQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs, labelQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    lossQtgt = tf.reduce_mean(tf.square(Qs - targetQs)) # next state, next action and nextQs\n",
    "    lossQlbl = tf.reduce_mean(tf.square(Qs - labelQs)) # current state, action, and currentQs\n",
    "    lossQtgt_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs, \n",
    "                                                                           labels=tf.nn.sigmoid(targetQs)))\n",
    "    lossQlbl_sigm = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs,\n",
    "                                                                           labels=tf.nn.sigmoid(labelQs)))\n",
    "    loss = lossQtgt + lossQlbl + lossQtgt_sigm + lossQlbl_sigm\n",
    "    return actions_logits, final_state, loss, lossQtgt, lossQlbl, lossQtgt_sigm, lossQlbl_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs, self.labelQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss, self.lossQtgt, self.lossQlbl, self.lossQtgt_sigm, self.lossQlbl_sigm = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, labelQs=self.labelQs, \n",
    "            cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_total_reward = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state:', np.array(states).shape[1], \n",
    "#       'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 32                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.0297722 , -0.04377287,  0.04515347, -0.03944067]),\n",
       " 0,\n",
       " array([-0.03064765, -0.23951226,  0.04436466,  0.26713977]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 meanReward: 10.0000 meanLoss: 2.3103 meanLossQlbl: 0.0153 meanLossQlbl_sigm: 0.6746 meanLossQtgt: 0.9949 meanLossQtgt_sigm: 0.6255\n",
      "Episode: 1 meanReward: 9.5000 meanLoss: 2.5527 meanLossQlbl: 0.4651 meanLossQlbl_sigm: 0.5507 meanLossQtgt: 1.0530 meanLossQtgt_sigm: 0.4838\n",
      "Episode: 2 meanReward: 14.0000 meanLoss: 5.0278 meanLossQlbl: 2.4131 meanLossQlbl_sigm: 0.4239 meanLossQtgt: 1.7648 meanLossQtgt_sigm: 0.4260\n",
      "Episode: 3 meanReward: 12.7500 meanLoss: 4.0847 meanLossQlbl: 1.7088 meanLossQlbl_sigm: 0.2778 meanLossQtgt: 1.8149 meanLossQtgt_sigm: 0.2832\n",
      "Episode: 4 meanReward: 14.0000 meanLoss: 4.6253 meanLossQlbl: 1.6886 meanLossQlbl_sigm: 0.3908 meanLossQtgt: 2.1612 meanLossQtgt_sigm: 0.3846\n",
      "Episode: 5 meanReward: 13.3333 meanLoss: 5.4610 meanLossQlbl: 2.1427 meanLossQlbl_sigm: 0.4434 meanLossQtgt: 2.4018 meanLossQtgt_sigm: 0.4731\n",
      "Episode: 6 meanReward: 17.0000 meanLoss: 4.9509 meanLossQlbl: 1.5487 meanLossQlbl_sigm: 0.2573 meanLossQtgt: 2.8808 meanLossQtgt_sigm: 0.2641\n",
      "Episode: 7 meanReward: 16.1250 meanLoss: 7.8355 meanLossQlbl: 0.8652 meanLossQlbl_sigm: 0.0113 meanLossQtgt: 6.8367 meanLossQtgt_sigm: 0.1222\n",
      "Episode: 8 meanReward: 15.3333 meanLoss: 12.5044 meanLossQlbl: 0.4032 meanLossQlbl_sigm: 0.0011 meanLossQtgt: 11.8595 meanLossQtgt_sigm: 0.2406\n",
      "Episode: 9 meanReward: 14.8000 meanLoss: 17.7818 meanLossQlbl: 0.1849 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.2328 meanLossQtgt_sigm: 0.3641\n",
      "Episode: 10 meanReward: 14.3636 meanLoss: 16.3511 meanLossQlbl: 0.0168 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.9666 meanLossQtgt_sigm: 0.3677\n",
      "Episode: 11 meanReward: 13.9167 meanLoss: 14.1787 meanLossQlbl: 0.0853 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.7516 meanLossQtgt_sigm: 0.3418\n",
      "Episode: 12 meanReward: 14.0769 meanLoss: 12.8172 meanLossQlbl: 0.4969 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.0302 meanLossQtgt_sigm: 0.2900\n",
      "Episode: 13 meanReward: 14.0000 meanLoss: 12.8703 meanLossQlbl: 0.0884 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.5016 meanLossQtgt_sigm: 0.2803\n",
      "Episode: 14 meanReward: 13.8667 meanLoss: 11.9832 meanLossQlbl: 0.0042 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.7232 meanLossQtgt_sigm: 0.2558\n",
      "Episode: 15 meanReward: 15.0625 meanLoss: 9.4376 meanLossQlbl: 0.1327 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.1135 meanLossQtgt_sigm: 0.1914\n",
      "Episode: 16 meanReward: 15.2353 meanLoss: 7.5568 meanLossQlbl: 0.0551 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.3693 meanLossQtgt_sigm: 0.1325\n",
      "Episode: 17 meanReward: 15.8333 meanLoss: 11.6746 meanLossQlbl: 0.0956 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.3759 meanLossQtgt_sigm: 0.2031\n",
      "Episode: 18 meanReward: 21.2105 meanLoss: 3.4131 meanLossQlbl: 0.0546 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 3.3137 meanLossQtgt_sigm: 0.0447\n",
      "Episode: 19 meanReward: 21.2500 meanLoss: 21.2402 meanLossQlbl: 0.2055 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.8099 meanLossQtgt_sigm: 0.2248\n",
      "Episode: 20 meanReward: 21.4286 meanLoss: 27.7040 meanLossQlbl: 0.3565 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.0498 meanLossQtgt_sigm: 0.2977\n",
      "Episode: 21 meanReward: 21.3636 meanLoss: 27.4402 meanLossQlbl: 0.7160 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.4376 meanLossQtgt_sigm: 0.2867\n",
      "Episode: 22 meanReward: 24.7826 meanLoss: 8.1944 meanLossQlbl: 0.2744 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.8338 meanLossQtgt_sigm: 0.0862\n",
      "Episode: 23 meanReward: 24.2917 meanLoss: 35.8542 meanLossQlbl: 0.0864 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 35.4679 meanLossQtgt_sigm: 0.2999\n",
      "Episode: 24 meanReward: 23.9600 meanLoss: 49.5287 meanLossQlbl: 1.6892 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 47.3740 meanLossQtgt_sigm: 0.4655\n",
      "Episode: 25 meanReward: 28.3846 meanLoss: 7.4797 meanLossQlbl: 0.2578 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.1491 meanLossQtgt_sigm: 0.0728\n",
      "Episode: 26 meanReward: 29.4815 meanLoss: 155.0566 meanLossQlbl: 77.2807 meanLossQlbl_sigm: 0.3547 meanLossQtgt: 77.0041 meanLossQtgt_sigm: 0.4171\n",
      "Episode: 27 meanReward: 30.1786 meanLoss: 312.2537 meanLossQlbl: 159.3016 meanLossQlbl_sigm: 0.5194 meanLossQtgt: 151.8773 meanLossQtgt_sigm: 0.5555\n",
      "Episode: 28 meanReward: 31.3103 meanLoss: 149.0692 meanLossQlbl: 79.1013 meanLossQlbl_sigm: 0.3939 meanLossQtgt: 69.1672 meanLossQtgt_sigm: 0.4067\n",
      "Episode: 29 meanReward: 31.3667 meanLoss: 354.9331 meanLossQlbl: 188.4335 meanLossQlbl_sigm: 0.6172 meanLossQtgt: 165.2292 meanLossQtgt_sigm: 0.6531\n",
      "Episode: 30 meanReward: 32.0000 meanLoss: 205.1032 meanLossQlbl: 108.8614 meanLossQlbl_sigm: 0.6292 meanLossQtgt: 94.9683 meanLossQtgt_sigm: 0.6443\n",
      "Episode: 31 meanReward: 32.7812 meanLoss: 386.9685 meanLossQlbl: 206.1592 meanLossQlbl_sigm: 0.6869 meanLossQtgt: 179.4338 meanLossQtgt_sigm: 0.6885\n",
      "Episode: 32 meanReward: 34.2500 meanLoss: 576.8875 meanLossQlbl: 307.7949 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 267.7063 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 33 meanReward: 36.1250 meanLoss: 574.4919 meanLossQlbl: 307.2584 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 265.8472 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 34 meanReward: 37.2500 meanLoss: 536.4017 meanLossQlbl: 287.3186 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 247.6969 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 35 meanReward: 38.5938 meanLoss: 523.4202 meanLossQlbl: 279.8384 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 242.1955 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 36 meanReward: 39.7188 meanLoss: 580.2056 meanLossQlbl: 309.5599 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 269.2594 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 37 meanReward: 41.1875 meanLoss: 587.7781 meanLossQlbl: 314.1956 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 272.1961 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 38 meanReward: 41.7188 meanLoss: 527.0193 meanLossQlbl: 282.8803 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 242.7527 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 39 meanReward: 42.5625 meanLoss: 460.0213 meanLossQlbl: 247.1327 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 211.5023 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 40 meanReward: 43.4375 meanLoss: 295.7777 meanLossQlbl: 156.5619 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 137.8295 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 41 meanReward: 44.3438 meanLoss: 300.8414 meanLossQlbl: 159.1966 meanLossQlbl_sigm: 0.6930 meanLossQtgt: 140.2577 meanLossQtgt_sigm: 0.6941\n",
      "Episode: 42 meanReward: 45.7188 meanLoss: 290.4327 meanLossQlbl: 155.6337 meanLossQlbl_sigm: 0.6929 meanLossQtgt: 133.4134 meanLossQtgt_sigm: 0.6927\n",
      "Episode: 43 meanReward: 46.8750 meanLoss: 338.8088 meanLossQlbl: 181.0413 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 156.3812 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 44 meanReward: 48.0000 meanLoss: 325.3978 meanLossQlbl: 173.3046 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 150.7070 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 45 meanReward: 49.1875 meanLoss: 409.7245 meanLossQlbl: 221.8041 meanLossQlbl_sigm: 0.6929 meanLossQtgt: 186.5332 meanLossQtgt_sigm: 0.6943\n",
      "Episode: 46 meanReward: 50.2188 meanLoss: 411.6870 meanLossQlbl: 220.0852 meanLossQlbl_sigm: 0.6796 meanLossQtgt: 190.2339 meanLossQtgt_sigm: 0.6883\n",
      "Episode: 47 meanReward: 50.6250 meanLoss: 590.4302 meanLossQlbl: 316.0619 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 272.9819 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 48 meanReward: 52.2500 meanLoss: 559.5798 meanLossQlbl: 299.6664 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 258.5270 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 49 meanReward: 53.7812 meanLoss: 650.4262 meanLossQlbl: 346.8364 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 302.2036 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 50 meanReward: 52.0000 meanLoss: 741.4402 meanLossQlbl: 395.1353 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 344.9187 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 51 meanReward: 53.1250 meanLoss: 756.5868 meanLossQlbl: 403.4249 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 351.7756 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 52 meanReward: 54.2812 meanLoss: 731.5789 meanLossQlbl: 389.9395 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 340.2531 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 53 meanReward: 55.4375 meanLoss: 770.4407 meanLossQlbl: 410.6671 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 358.3872 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 54 meanReward: 55.0000 meanLoss: 624.9839 meanLossQlbl: 334.3820 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 289.2156 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 55 meanReward: 56.9375 meanLoss: 574.0433 meanLossQlbl: 307.5802 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 265.0768 meanLossQtgt_sigm: 0.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 56 meanReward: 58.0938 meanLoss: 632.5571 meanLossQlbl: 337.8204 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 293.3504 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 57 meanReward: 55.9062 meanLoss: 469.8569 meanLossQlbl: 253.3288 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 215.1418 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 58 meanReward: 55.8438 meanLoss: 484.6460 meanLossQlbl: 259.8382 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 223.4214 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 59 meanReward: 56.1562 meanLoss: 384.9443 meanLossQlbl: 208.2625 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 175.2955 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 60 meanReward: 55.6875 meanLoss: 278.4822 meanLossQlbl: 149.5078 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 127.5881 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 61 meanReward: 56.1250 meanLoss: 402.5965 meanLossQlbl: 215.0907 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 186.1194 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 62 meanReward: 56.2812 meanLoss: 504.1919 meanLossQlbl: 270.3630 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 232.4426 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 63 meanReward: 56.5000 meanLoss: 448.3781 meanLossQlbl: 241.7424 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 205.2494 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 64 meanReward: 56.4375 meanLoss: 469.5883 meanLossQlbl: 251.2525 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 216.9495 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 65 meanReward: 56.0938 meanLoss: 515.9482 meanLossQlbl: 277.2711 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 237.2908 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 66 meanReward: 55.6562 meanLoss: 281.6799 meanLossQlbl: 150.8155 meanLossQlbl_sigm: 0.6890 meanLossQtgt: 129.4786 meanLossQtgt_sigm: 0.6967\n",
      "Episode: 67 meanReward: 55.7500 meanLoss: 468.9600 meanLossQlbl: 250.0221 meanLossQlbl_sigm: 0.6810 meanLossQtgt: 217.5616 meanLossQtgt_sigm: 0.6952\n",
      "Episode: 68 meanReward: 57.0312 meanLoss: 481.5567 meanLossQlbl: 258.8650 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 221.3054 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 69 meanReward: 57.1562 meanLoss: 670.9063 meanLossQlbl: 357.8251 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 311.6947 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 70 meanReward: 59.6562 meanLoss: 597.5172 meanLossQlbl: 319.5400 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 276.5909 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 71 meanReward: 61.2188 meanLoss: 679.2295 meanLossQlbl: 362.3063 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 315.5369 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 72 meanReward: 63.1875 meanLoss: 633.7983 meanLossQlbl: 338.8361 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 293.5758 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 73 meanReward: 64.5312 meanLoss: 682.3218 meanLossQlbl: 363.7897 meanLossQlbl_sigm: 0.6931 meanLossQtgt: 317.1457 meanLossQtgt_sigm: 0.6931\n",
      "Episode: 74 meanReward: 65.9375 meanLoss: 395.0945 meanLossQlbl: 211.9314 meanLossQlbl_sigm: 0.4414 meanLossQtgt: 182.2768 meanLossQtgt_sigm: 0.4450\n",
      "Episode: 75 meanReward: 65.4062 meanLoss: 135.6930 meanLossQlbl: 53.2283 meanLossQlbl_sigm: 0.2266 meanLossQtgt: 81.7718 meanLossQtgt_sigm: 0.4662\n",
      "Episode: 76 meanReward: 65.0312 meanLoss: 260.8513 meanLossQlbl: 128.7982 meanLossQlbl_sigm: 0.3948 meanLossQtgt: 131.2013 meanLossQtgt_sigm: 0.4571\n",
      "Episode: 77 meanReward: 64.0312 meanLoss: 62.1350 meanLossQlbl: 0.7975 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 60.9590 meanLossQtgt_sigm: 0.3785\n",
      "Episode: 78 meanReward: 63.5625 meanLoss: 58.8586 meanLossQlbl: 0.5073 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 57.9125 meanLossQtgt_sigm: 0.4388\n",
      "Episode: 79 meanReward: 62.9688 meanLoss: 45.5265 meanLossQlbl: 0.2376 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 44.9550 meanLossQtgt_sigm: 0.3339\n",
      "Episode: 80 meanReward: 61.9375 meanLoss: 35.2691 meanLossQlbl: 1.0250 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.9712 meanLossQtgt_sigm: 0.2729\n",
      "Episode: 81 meanReward: 62.5312 meanLoss: 18.9744 meanLossQlbl: 0.5392 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.3199 meanLossQtgt_sigm: 0.1152\n",
      "Episode: 82 meanReward: 61.1562 meanLoss: 80.9186 meanLossQlbl: 0.2658 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 80.2083 meanLossQtgt_sigm: 0.4446\n",
      "Episode: 83 meanReward: 60.9375 meanLoss: 56.8095 meanLossQlbl: 0.9597 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 55.5110 meanLossQtgt_sigm: 0.3387\n",
      "Episode: 84 meanReward: 59.8750 meanLoss: 73.7802 meanLossQlbl: 0.1947 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 73.1660 meanLossQtgt_sigm: 0.4195\n",
      "Episode: 85 meanReward: 61.6875 meanLoss: 18.3637 meanLossQlbl: 0.5383 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.7155 meanLossQtgt_sigm: 0.1099\n",
      "Episode: 86 meanReward: 62.3750 meanLoss: 23.6498 meanLossQlbl: 0.1560 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 23.3697 meanLossQtgt_sigm: 0.1241\n",
      "Episode: 87 meanReward: 62.9688 meanLoss: 27.4697 meanLossQlbl: 0.1223 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.2028 meanLossQtgt_sigm: 0.1447\n",
      "Episode: 88 meanReward: 67.3438 meanLoss: 20.5197 meanLossQlbl: 5.7627 meanLossQlbl_sigm: 0.0163 meanLossQtgt: 14.6691 meanLossQtgt_sigm: 0.0716\n",
      "Episode: 89 meanReward: 66.0000 meanLoss: 468.5540 meanLossQlbl: 248.8844 meanLossQlbl_sigm: 0.4522 meanLossQtgt: 218.7032 meanLossQtgt_sigm: 0.5142\n",
      "Episode: 90 meanReward: 65.0312 meanLoss: 116.0417 meanLossQlbl: 4.1836 meanLossQlbl_sigm: 0.0181 meanLossQtgt: 111.2718 meanLossQtgt_sigm: 0.5683\n",
      "Episode: 91 meanReward: 64.0000 meanLoss: 99.8543 meanLossQlbl: 0.0762 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 99.2384 meanLossQtgt_sigm: 0.5396\n",
      "Episode: 92 meanReward: 68.2812 meanLoss: 13.5787 meanLossQlbl: 0.2550 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.2494 meanLossQtgt_sigm: 0.0743\n",
      "Episode: 93 meanReward: 69.3750 meanLoss: 15.3691 meanLossQlbl: 0.4050 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.8776 meanLossQtgt_sigm: 0.0865\n",
      "Episode: 94 meanReward: 69.9062 meanLoss: 39.5575 meanLossQlbl: 1.3631 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 38.0016 meanLossQtgt_sigm: 0.1928\n",
      "Episode: 95 meanReward: 69.6875 meanLoss: 58.5308 meanLossQlbl: 0.5100 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 57.7547 meanLossQtgt_sigm: 0.2661\n",
      "Episode: 96 meanReward: 68.7500 meanLoss: 110.8830 meanLossQlbl: 1.7150 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 108.6599 meanLossQtgt_sigm: 0.5081\n",
      "Episode: 97 meanReward: 69.7812 meanLoss: 37.9872 meanLossQlbl: 0.5105 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.2932 meanLossQtgt_sigm: 0.1835\n",
      "Episode: 98 meanReward: 72.0312 meanLoss: 32.1925 meanLossQlbl: 0.1095 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 31.9420 meanLossQtgt_sigm: 0.1411\n",
      "Episode: 99 meanReward: 71.2500 meanLoss: 103.7607 meanLossQlbl: 0.4523 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 102.8139 meanLossQtgt_sigm: 0.4945\n",
      "Episode: 100 meanReward: 77.4062 meanLoss: 11.2420 meanLossQlbl: 0.2588 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.9311 meanLossQtgt_sigm: 0.0520\n",
      "Episode: 101 meanReward: 76.0625 meanLoss: 149.7786 meanLossQlbl: 0.1534 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 149.0255 meanLossQtgt_sigm: 0.5998\n",
      "Episode: 102 meanReward: 75.7812 meanLoss: 22.6186 meanLossQlbl: 1.9395 meanLossQlbl_sigm: 0.0112 meanLossQtgt: 20.5753 meanLossQtgt_sigm: 0.0926\n",
      "Episode: 103 meanReward: 73.4688 meanLoss: 89.9493 meanLossQlbl: 0.1199 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 89.3608 meanLossQtgt_sigm: 0.4687\n",
      "Episode: 104 meanReward: 71.3750 meanLoss: 123.4170 meanLossQlbl: 0.2915 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 122.4782 meanLossQtgt_sigm: 0.6473\n",
      "Episode: 105 meanReward: 69.4062 meanLoss: 69.4008 meanLossQlbl: 0.2961 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 68.6973 meanLossQtgt_sigm: 0.4074\n",
      "Episode: 106 meanReward: 68.3750 meanLoss: 33.6975 meanLossQlbl: 0.3029 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.1691 meanLossQtgt_sigm: 0.2255\n",
      "Episode: 107 meanReward: 83.0938 meanLoss: 4.4622 meanLossQlbl: 0.0261 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.4123 meanLossQtgt_sigm: 0.0238\n",
      "Episode: 108 meanReward: 97.4688 meanLoss: 10.9542 meanLossQlbl: 0.0700 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.8449 meanLossQtgt_sigm: 0.0393\n",
      "Episode: 109 meanReward: 97.5938 meanLoss: 201.9109 meanLossQlbl: 1.0993 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 200.1203 meanLossQtgt_sigm: 0.6913\n",
      "Episode: 110 meanReward: 107.6562 meanLoss: 15.2956 meanLossQlbl: 0.3212 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.9159 meanLossQtgt_sigm: 0.0585\n",
      "Episode: 111 meanReward: 110.3750 meanLoss: 40.5605 meanLossQlbl: 0.6455 meanLossQlbl_sigm: 0.0006 meanLossQtgt: 39.7631 meanLossQtgt_sigm: 0.1513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 112 meanReward: 113.3750 meanLoss: 22.5578 meanLossQlbl: 0.1112 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 22.3382 meanLossQtgt_sigm: 0.1083\n",
      "Episode: 113 meanReward: 117.2188 meanLoss: 15.3576 meanLossQlbl: 5.9367 meanLossQlbl_sigm: 0.0247 meanLossQtgt: 9.3544 meanLossQtgt_sigm: 0.0418\n",
      "Episode: 114 meanReward: 126.3750 meanLoss: 12.0738 meanLossQlbl: 0.3848 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.6508 meanLossQtgt_sigm: 0.0382\n",
      "Episode: 115 meanReward: 140.4062 meanLoss: 10.4779 meanLossQlbl: 0.0379 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.4017 meanLossQtgt_sigm: 0.0383\n",
      "Episode: 116 meanReward: 140.1562 meanLoss: 262.0642 meanLossQlbl: 0.8934 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 260.3792 meanLossQtgt_sigm: 0.7917\n",
      "Episode: 117 meanReward: 137.8438 meanLoss: 156.3128 meanLossQlbl: 3.8418 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 152.0909 meanLossQtgt_sigm: 0.3801\n",
      "Episode: 118 meanReward: 138.2500 meanLoss: 38.1679 meanLossQlbl: 0.4697 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.5537 meanLossQtgt_sigm: 0.1445\n",
      "Episode: 119 meanReward: 139.0625 meanLoss: 18.1890 meanLossQlbl: 0.1903 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.8959 meanLossQtgt_sigm: 0.1029\n",
      "Episode: 120 meanReward: 134.5625 meanLoss: 45.5570 meanLossQlbl: 0.5337 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 44.7669 meanLossQtgt_sigm: 0.2564\n",
      "Episode: 121 meanReward: 135.6875 meanLoss: 30.9102 meanLossQlbl: 0.0214 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 30.6990 meanLossQtgt_sigm: 0.1898\n",
      "Episode: 122 meanReward: 135.8750 meanLoss: 52.6461 meanLossQlbl: 0.0019 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 52.2901 meanLossQtgt_sigm: 0.3541\n",
      "Episode: 123 meanReward: 137.9688 meanLoss: 14.5615 meanLossQlbl: 0.4674 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.9960 meanLossQtgt_sigm: 0.0981\n",
      "Episode: 124 meanReward: 134.9688 meanLoss: 17.1497 meanLossQlbl: 0.1635 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.8727 meanLossQtgt_sigm: 0.1135\n",
      "Episode: 125 meanReward: 141.0938 meanLoss: 4.9783 meanLossQlbl: 0.0206 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.9240 meanLossQtgt_sigm: 0.0337\n",
      "Episode: 126 meanReward: 141.5625 meanLoss: 17.0737 meanLossQlbl: 0.3811 meanLossQlbl_sigm: 0.0039 meanLossQtgt: 16.5840 meanLossQtgt_sigm: 0.1047\n",
      "Episode: 127 meanReward: 145.1875 meanLoss: 19.8806 meanLossQlbl: 7.8338 meanLossQlbl_sigm: 0.0276 meanLossQtgt: 11.9711 meanLossQtgt_sigm: 0.0480\n",
      "Episode: 128 meanReward: 154.2188 meanLoss: 5.6155 meanLossQlbl: 0.1095 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.4733 meanLossQtgt_sigm: 0.0327\n",
      "Episode: 129 meanReward: 152.6875 meanLoss: 30.4854 meanLossQlbl: 0.1029 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 30.1609 meanLossQtgt_sigm: 0.2217\n",
      "Episode: 130 meanReward: 164.6562 meanLoss: 13.8074 meanLossQlbl: 0.2419 meanLossQlbl_sigm: 0.0005 meanLossQtgt: 13.5215 meanLossQtgt_sigm: 0.0434\n",
      "Episode: 131 meanReward: 164.2188 meanLoss: 281.4816 meanLossQlbl: 1.0634 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 279.5933 meanLossQtgt_sigm: 0.8250\n",
      "Episode: 132 meanReward: 155.3438 meanLoss: 393.8289 meanLossQlbl: 6.7482 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 385.7453 meanLossQtgt_sigm: 1.3354\n",
      "Episode: 133 meanReward: 167.4375 meanLoss: 15.1209 meanLossQlbl: 0.2974 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.7689 meanLossQtgt_sigm: 0.0546\n",
      "Episode: 134 meanReward: 179.0938 meanLoss: 9.6506 meanLossQlbl: 0.2718 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.3430 meanLossQtgt_sigm: 0.0358\n",
      "Episode: 135 meanReward: 194.3125 meanLoss: 12.1729 meanLossQlbl: 0.2921 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.8401 meanLossQtgt_sigm: 0.0407\n",
      "Episode: 136 meanReward: 198.1562 meanLoss: 52.4150 meanLossQlbl: 0.2642 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 51.9957 meanLossQtgt_sigm: 0.1551\n",
      "Episode: 137 meanReward: 213.1875 meanLoss: 15.1795 meanLossQlbl: 0.2524 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.8813 meanLossQtgt_sigm: 0.0459\n",
      "Episode: 138 meanReward: 226.7500 meanLoss: 16.8571 meanLossQlbl: 0.0485 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.7595 meanLossQtgt_sigm: 0.0491\n",
      "Episode: 139 meanReward: 213.6562 meanLoss: 101.9628 meanLossQlbl: 0.2846 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 101.3754 meanLossQtgt_sigm: 0.3028\n",
      "Episode: 140 meanReward: 213.6562 meanLoss: 13.3756 meanLossQlbl: 0.3025 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.0298 meanLossQtgt_sigm: 0.0433\n",
      "Episode: 141 meanReward: 228.5625 meanLoss: 15.3113 meanLossQlbl: 0.0836 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.1811 meanLossQtgt_sigm: 0.0465\n",
      "Episode: 142 meanReward: 233.1875 meanLoss: 10.7193 meanLossQlbl: 0.3937 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.2877 meanLossQtgt_sigm: 0.0379\n",
      "Episode: 143 meanReward: 245.2500 meanLoss: 16.7201 meanLossQlbl: 0.1166 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.5543 meanLossQtgt_sigm: 0.0492\n",
      "Episode: 144 meanReward: 242.7812 meanLoss: 156.6877 meanLossQlbl: 0.4563 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 155.7760 meanLossQtgt_sigm: 0.4554\n",
      "Episode: 145 meanReward: 251.6250 meanLoss: 10.9861 meanLossQlbl: 0.0767 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.8703 meanLossQtgt_sigm: 0.0391\n",
      "Episode: 146 meanReward: 257.5625 meanLoss: 16.0623 meanLossQlbl: 0.0427 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.9714 meanLossQtgt_sigm: 0.0481\n",
      "Episode: 147 meanReward: 242.5000 meanLoss: 261.3553 meanLossQlbl: 1.1747 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 259.3885 meanLossQtgt_sigm: 0.7921\n",
      "Episode: 148 meanReward: 257.5000 meanLoss: 17.7149 meanLossQlbl: 0.6413 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0194 meanLossQtgt_sigm: 0.0542\n",
      "Episode: 149 meanReward: 271.8438 meanLoss: 16.2780 meanLossQlbl: 0.1194 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.1104 meanLossQtgt_sigm: 0.0482\n",
      "Episode: 150 meanReward: 283.6875 meanLoss: 16.5271 meanLossQlbl: 0.1249 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.3534 meanLossQtgt_sigm: 0.0488\n",
      "Episode: 151 meanReward: 295.5625 meanLoss: 15.9985 meanLossQlbl: 0.5076 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.4437 meanLossQtgt_sigm: 0.0472\n",
      "Episode: 152 meanReward: 309.6562 meanLoss: 17.0203 meanLossQlbl: 0.0596 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.9112 meanLossQtgt_sigm: 0.0495\n",
      "Episode: 153 meanReward: 309.7188 meanLoss: 127.1391 meanLossQlbl: 0.7026 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 126.0568 meanLossQtgt_sigm: 0.3797\n",
      "Episode: 154 meanReward: 309.1250 meanLoss: 270.9096 meanLossQlbl: 0.8638 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 269.2266 meanLossQtgt_sigm: 0.8192\n",
      "Episode: 155 meanReward: 321.8438 meanLoss: 17.6895 meanLossQlbl: 0.9729 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.6605 meanLossQtgt_sigm: 0.0561\n",
      "Episode: 156 meanReward: 321.3750 meanLoss: 107.9505 meanLossQlbl: 0.2648 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 107.3624 meanLossQtgt_sigm: 0.3233\n",
      "Episode: 157 meanReward: 328.3125 meanLoss: 11.4094 meanLossQlbl: 0.0574 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.3122 meanLossQtgt_sigm: 0.0399\n",
      "Episode: 158 meanReward: 326.0938 meanLoss: 267.9464 meanLossQlbl: 0.3244 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 266.8159 meanLossQtgt_sigm: 0.8061\n",
      "Episode: 159 meanReward: 322.0625 meanLoss: 176.8591 meanLossQlbl: 11.8246 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 164.5957 meanLossQtgt_sigm: 0.4388\n",
      "Episode: 160 meanReward: 324.0000 meanLoss: 10.8932 meanLossQlbl: 0.8524 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.9993 meanLossQtgt_sigm: 0.0414\n",
      "Episode: 161 meanReward: 329.3125 meanLoss: 18.3351 meanLossQlbl: 0.5915 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.6745 meanLossQtgt_sigm: 0.0690\n",
      "Episode: 162 meanReward: 328.8750 meanLoss: 7.0104 meanLossQlbl: 0.1686 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.8123 meanLossQtgt_sigm: 0.0295\n",
      "Episode: 163 meanReward: 336.9062 meanLoss: 12.2564 meanLossQlbl: 0.8552 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.3509 meanLossQtgt_sigm: 0.0503\n",
      "Episode: 164 meanReward: 342.0938 meanLoss: 29.1775 meanLossQlbl: 0.3497 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 28.7289 meanLossQtgt_sigm: 0.0988\n",
      "Episode: 165 meanReward: 334.4062 meanLoss: 15.1526 meanLossQlbl: 0.8544 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.2280 meanLossQtgt_sigm: 0.0702\n",
      "Episode: 166 meanReward: 321.0000 meanLoss: 49.2121 meanLossQlbl: 0.8859 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 48.1065 meanLossQtgt_sigm: 0.2198\n",
      "Episode: 167 meanReward: 310.0312 meanLoss: 11.4136 meanLossQlbl: 0.4147 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.9383 meanLossQtgt_sigm: 0.0606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 168 meanReward: 320.7812 meanLoss: 4.6868 meanLossQlbl: 0.1019 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.5612 meanLossQtgt_sigm: 0.0237\n",
      "Episode: 169 meanReward: 320.7812 meanLoss: 10.5964 meanLossQlbl: 0.0274 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.5310 meanLossQtgt_sigm: 0.0380\n",
      "Episode: 170 meanReward: 310.9062 meanLoss: 43.0273 meanLossQlbl: 0.4497 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 42.4510 meanLossQtgt_sigm: 0.1266\n",
      "Episode: 171 meanReward: 309.3438 meanLoss: 148.3476 meanLossQlbl: 1.8674 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 145.9056 meanLossQtgt_sigm: 0.5746\n",
      "Episode: 172 meanReward: 309.3438 meanLoss: 5.7978 meanLossQlbl: 0.1870 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.5868 meanLossQtgt_sigm: 0.0240\n",
      "Episode: 173 meanReward: 299.7188 meanLoss: 25.0550 meanLossQlbl: 1.1195 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 23.8454 meanLossQtgt_sigm: 0.0901\n",
      "Episode: 174 meanReward: 288.4688 meanLoss: 14.2404 meanLossQlbl: 0.8483 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.3237 meanLossQtgt_sigm: 0.0684\n",
      "Episode: 175 meanReward: 288.4688 meanLoss: 3.9215 meanLossQlbl: 0.0136 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 3.8850 meanLossQtgt_sigm: 0.0229\n",
      "Episode: 176 meanReward: 289.2188 meanLoss: 107.8023 meanLossQlbl: 1.1017 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 106.3873 meanLossQtgt_sigm: 0.3133\n",
      "Episode: 177 meanReward: 274.5938 meanLoss: 40.3075 meanLossQlbl: 0.5201 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 39.5070 meanLossQtgt_sigm: 0.2804\n",
      "Episode: 178 meanReward: 274.5938 meanLoss: 2.2049 meanLossQlbl: 0.0721 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 2.1206 meanLossQtgt_sigm: 0.0122\n",
      "Episode: 179 meanReward: 280.5312 meanLoss: 34.1439 meanLossQlbl: 0.5439 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.4984 meanLossQtgt_sigm: 0.1016\n",
      "Episode: 180 meanReward: 270.3125 meanLoss: 13.4930 meanLossQlbl: 0.3058 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.1157 meanLossQtgt_sigm: 0.0714\n",
      "Episode: 181 meanReward: 258.2188 meanLoss: 13.4819 meanLossQlbl: 1.0521 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.3449 meanLossQtgt_sigm: 0.0849\n",
      "Episode: 182 meanReward: 246.0312 meanLoss: 12.7813 meanLossQlbl: 0.3520 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.3455 meanLossQtgt_sigm: 0.0839\n",
      "Episode: 183 meanReward: 234.4062 meanLoss: 6.1892 meanLossQlbl: 0.2405 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 5.8984 meanLossQtgt_sigm: 0.0502\n",
      "Episode: 184 meanReward: 222.7500 meanLoss: 4.5219 meanLossQlbl: 0.4275 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.0593 meanLossQtgt_sigm: 0.0351\n",
      "Episode: 185 meanReward: 224.8750 meanLoss: 6.3611 meanLossQlbl: 0.3918 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.9183 meanLossQtgt_sigm: 0.0509\n",
      "Episode: 186 meanReward: 229.0312 meanLoss: 4.3556 meanLossQlbl: 0.2285 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.0846 meanLossQtgt_sigm: 0.0425\n",
      "Episode: 187 meanReward: 226.9688 meanLoss: 1.4797 meanLossQlbl: 0.1211 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 1.3451 meanLossQtgt_sigm: 0.0135\n",
      "Episode: 188 meanReward: 231.8750 meanLoss: 3.3547 meanLossQlbl: 0.1393 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 3.1874 meanLossQtgt_sigm: 0.0280\n",
      "Episode: 189 meanReward: 222.8125 meanLoss: 1.4835 meanLossQlbl: 0.2157 meanLossQlbl_sigm: 0.0070 meanLossQtgt: 1.2490 meanLossQtgt_sigm: 0.0117\n",
      "Episode: 190 meanReward: 229.6875 meanLoss: 7.3937 meanLossQlbl: 0.1843 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.1635 meanLossQtgt_sigm: 0.0459\n",
      "Episode: 191 meanReward: 235.6875 meanLoss: 6.5043 meanLossQlbl: 0.3289 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.1344 meanLossQtgt_sigm: 0.0410\n",
      "Episode: 192 meanReward: 226.1562 meanLoss: 19.4146 meanLossQlbl: 1.0767 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.2185 meanLossQtgt_sigm: 0.1194\n",
      "Episode: 193 meanReward: 223.3125 meanLoss: 13.9432 meanLossQlbl: 1.2503 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.6183 meanLossQtgt_sigm: 0.0746\n",
      "Episode: 194 meanReward: 213.8750 meanLoss: 4.2652 meanLossQlbl: 0.3489 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 3.8909 meanLossQtgt_sigm: 0.0254\n",
      "Episode: 195 meanReward: 220.9688 meanLoss: 2.1018 meanLossQlbl: 0.0284 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 2.0587 meanLossQtgt_sigm: 0.0147\n",
      "Episode: 196 meanReward: 224.5000 meanLoss: 32.9332 meanLossQlbl: 0.5011 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 32.3437 meanLossQtgt_sigm: 0.0884\n",
      "Episode: 197 meanReward: 235.1562 meanLoss: 4.7957 meanLossQlbl: 0.3705 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.4040 meanLossQtgt_sigm: 0.0212\n",
      "Episode: 198 meanReward: 238.0000 meanLoss: 50.8621 meanLossQlbl: 1.3531 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 49.3710 meanLossQtgt_sigm: 0.1380\n",
      "Episode: 199 meanReward: 233.6562 meanLoss: 39.0738 meanLossQlbl: 0.9584 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.8726 meanLossQtgt_sigm: 0.2428\n",
      "Episode: 200 meanReward: 218.3438 meanLoss: 112.7152 meanLossQlbl: 3.7723 meanLossQlbl_sigm: 0.0002 meanLossQtgt: 108.5864 meanLossQtgt_sigm: 0.3563\n",
      "Episode: 201 meanReward: 205.0000 meanLoss: 53.8133 meanLossQlbl: 5.8449 meanLossQlbl_sigm: 0.0692 meanLossQtgt: 47.7639 meanLossQtgt_sigm: 0.1353\n",
      "Episode: 202 meanReward: 202.1250 meanLoss: 7.1198 meanLossQlbl: 0.5048 meanLossQlbl_sigm: 0.0025 meanLossQtgt: 6.5852 meanLossQtgt_sigm: 0.0274\n",
      "Episode: 203 meanReward: 205.1562 meanLoss: 4.8143 meanLossQlbl: 0.2312 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.5557 meanLossQtgt_sigm: 0.0274\n",
      "Episode: 204 meanReward: 199.7500 meanLoss: 3.3438 meanLossQlbl: 0.1104 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 3.2145 meanLossQtgt_sigm: 0.0189\n",
      "Episode: 205 meanReward: 200.0938 meanLoss: 10.5601 meanLossQlbl: 0.2794 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.2357 meanLossQtgt_sigm: 0.0450\n",
      "Episode: 206 meanReward: 209.2812 meanLoss: 1.9962 meanLossQlbl: 0.0793 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 1.9032 meanLossQtgt_sigm: 0.0137\n",
      "Episode: 207 meanReward: 205.4062 meanLoss: 1.1700 meanLossQlbl: 0.0288 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 1.1345 meanLossQtgt_sigm: 0.0067\n",
      "Episode: 208 meanReward: 207.0938 meanLoss: 3.3349 meanLossQlbl: 0.1006 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 3.2132 meanLossQtgt_sigm: 0.0211\n",
      "Episode: 209 meanReward: 209.0000 meanLoss: 6.8432 meanLossQlbl: 0.2027 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.5857 meanLossQtgt_sigm: 0.0549\n",
      "Episode: 210 meanReward: 196.9062 meanLoss: 3.3982 meanLossQlbl: 0.1056 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 3.2698 meanLossQtgt_sigm: 0.0227\n",
      "Episode: 211 meanReward: 194.1562 meanLoss: 1.9825 meanLossQlbl: 0.0414 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 1.9220 meanLossQtgt_sigm: 0.0191\n",
      "Episode: 212 meanReward: 193.2188 meanLoss: 5.2492 meanLossQlbl: 0.0581 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.1439 meanLossQtgt_sigm: 0.0471\n",
      "Episode: 213 meanReward: 192.1250 meanLoss: 15.2164 meanLossQlbl: 0.4047 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.7068 meanLossQtgt_sigm: 0.1049\n",
      "Episode: 214 meanReward: 192.9688 meanLoss: 4.4217 meanLossQlbl: 0.2821 meanLossQlbl_sigm: 0.0038 meanLossQtgt: 4.1135 meanLossQtgt_sigm: 0.0222\n",
      "Episode: 215 meanReward: 194.4062 meanLoss: 5.4739 meanLossQlbl: 0.0396 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.3901 meanLossQtgt_sigm: 0.0442\n",
      "Episode: 216 meanReward: 196.0000 meanLoss: 6.8701 meanLossQlbl: 0.0533 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.7681 meanLossQtgt_sigm: 0.0487\n",
      "Episode: 217 meanReward: 200.6250 meanLoss: 5.0128 meanLossQlbl: 0.0488 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.9301 meanLossQtgt_sigm: 0.0339\n",
      "Episode: 218 meanReward: 196.4375 meanLoss: 113.5549 meanLossQlbl: 1.8796 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 111.1629 meanLossQtgt_sigm: 0.5124\n",
      "Episode: 219 meanReward: 198.5000 meanLoss: 10.9295 meanLossQlbl: 0.6087 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.2740 meanLossQtgt_sigm: 0.0468\n",
      "Episode: 220 meanReward: 196.6875 meanLoss: 19.9908 meanLossQlbl: 0.1789 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.7270 meanLossQtgt_sigm: 0.0848\n",
      "Episode: 221 meanReward: 194.9062 meanLoss: 13.4080 meanLossQlbl: 0.1574 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.1741 meanLossQtgt_sigm: 0.0764\n",
      "Episode: 222 meanReward: 203.1250 meanLoss: 4.4931 meanLossQlbl: 0.1495 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.3201 meanLossQtgt_sigm: 0.0235\n",
      "Episode: 223 meanReward: 211.3750 meanLoss: 17.6381 meanLossQlbl: 0.5238 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0706 meanLossQtgt_sigm: 0.0438\n",
      "Episode: 224 meanReward: 224.7812 meanLoss: 23.0282 meanLossQlbl: 0.0515 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 22.9194 meanLossQtgt_sigm: 0.0573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 225 meanReward: 221.4375 meanLoss: 404.2200 meanLossQlbl: 0.9776 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 402.2487 meanLossQtgt_sigm: 0.9938\n",
      "Episode: 226 meanReward: 216.0312 meanLoss: 567.8749 meanLossQlbl: 3.5865 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 562.6924 meanLossQtgt_sigm: 1.5960\n",
      "Episode: 227 meanReward: 216.0312 meanLoss: 10.4405 meanLossQlbl: 0.3865 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.0186 meanLossQtgt_sigm: 0.0355\n",
      "Episode: 228 meanReward: 215.0000 meanLoss: 35.7084 meanLossQlbl: 0.4942 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 35.1160 meanLossQtgt_sigm: 0.0982\n",
      "Episode: 229 meanReward: 215.0000 meanLoss: 17.0261 meanLossQlbl: 0.1525 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.8247 meanLossQtgt_sigm: 0.0489\n",
      "Episode: 230 meanReward: 225.5625 meanLoss: 18.7203 meanLossQlbl: 0.1992 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.4707 meanLossQtgt_sigm: 0.0504\n",
      "Episode: 231 meanReward: 240.8750 meanLoss: 17.2588 meanLossQlbl: 0.1810 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0283 meanLossQtgt_sigm: 0.0494\n",
      "Episode: 232 meanReward: 241.2500 meanLoss: 270.7761 meanLossQlbl: 0.8332 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 269.1399 meanLossQtgt_sigm: 0.8031\n",
      "Episode: 233 meanReward: 239.2812 meanLoss: 451.2922 meanLossQlbl: 2.0914 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 447.8149 meanLossQtgt_sigm: 1.3858\n",
      "Episode: 234 meanReward: 252.0312 meanLoss: 11.4246 meanLossQlbl: 0.2689 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.1147 meanLossQtgt_sigm: 0.0410\n",
      "Episode: 235 meanReward: 263.6562 meanLoss: 16.1985 meanLossQlbl: 0.0590 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.0917 meanLossQtgt_sigm: 0.0479\n",
      "Episode: 236 meanReward: 269.0625 meanLoss: 19.5476 meanLossQlbl: 0.1426 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.3523 meanLossQtgt_sigm: 0.0527\n",
      "Episode: 237 meanReward: 278.3438 meanLoss: 13.3725 meanLossQlbl: 0.1152 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.2153 meanLossQtgt_sigm: 0.0419\n",
      "Episode: 238 meanReward: 267.2812 meanLoss: 115.1569 meanLossQlbl: 0.5402 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 114.2960 meanLossQtgt_sigm: 0.3207\n",
      "Episode: 239 meanReward: 271.1562 meanLoss: 15.7070 meanLossQlbl: 0.1480 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.5117 meanLossQtgt_sigm: 0.0472\n",
      "Episode: 240 meanReward: 282.6562 meanLoss: 19.3559 meanLossQlbl: 0.0672 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.2358 meanLossQtgt_sigm: 0.0528\n",
      "Episode: 241 meanReward: 280.6875 meanLoss: 277.2960 meanLossQlbl: 0.7921 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 275.6969 meanLossQtgt_sigm: 0.8070\n",
      "Episode: 242 meanReward: 289.1250 meanLoss: 16.0540 meanLossQlbl: 0.1956 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.8042 meanLossQtgt_sigm: 0.0542\n",
      "Episode: 243 meanReward: 285.6562 meanLoss: 184.6537 meanLossQlbl: 0.4180 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 183.5591 meanLossQtgt_sigm: 0.6766\n",
      "Episode: 244 meanReward: 281.5000 meanLoss: 245.8654 meanLossQlbl: 5.0925 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 239.7896 meanLossQtgt_sigm: 0.9833\n",
      "Episode: 245 meanReward: 288.7188 meanLoss: 19.7262 meanLossQlbl: 0.8315 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.8406 meanLossQtgt_sigm: 0.0541\n",
      "Episode: 246 meanReward: 288.5625 meanLoss: 10.2519 meanLossQlbl: 0.1342 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.0502 meanLossQtgt_sigm: 0.0675\n",
      "Episode: 247 meanReward: 287.5000 meanLoss: 9.0146 meanLossQlbl: 0.1342 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.8269 meanLossQtgt_sigm: 0.0534\n",
      "Episode: 248 meanReward: 290.1250 meanLoss: 5.2518 meanLossQlbl: 0.1103 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.1116 meanLossQtgt_sigm: 0.0299\n",
      "Episode: 249 meanReward: 287.5312 meanLoss: 9.7649 meanLossQlbl: 0.2430 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.4735 meanLossQtgt_sigm: 0.0484\n",
      "Episode: 250 meanReward: 293.3750 meanLoss: 15.6486 meanLossQlbl: 0.3581 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.2235 meanLossQtgt_sigm: 0.0669\n",
      "Episode: 251 meanReward: 293.3750 meanLoss: 4.3908 meanLossQlbl: 0.0134 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.3542 meanLossQtgt_sigm: 0.0231\n",
      "Episode: 252 meanReward: 303.5938 meanLoss: 17.0471 meanLossQlbl: 0.0565 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.9423 meanLossQtgt_sigm: 0.0483\n",
      "Episode: 253 meanReward: 302.2188 meanLoss: 67.1828 meanLossQlbl: 0.5286 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 66.4452 meanLossQtgt_sigm: 0.2090\n",
      "Episode: 254 meanReward: 290.6562 meanLoss: 28.2550 meanLossQlbl: 1.0172 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.1296 meanLossQtgt_sigm: 0.1081\n",
      "Episode: 255 meanReward: 275.5312 meanLoss: 93.4760 meanLossQlbl: 1.3106 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 91.7423 meanLossQtgt_sigm: 0.4231\n",
      "Episode: 256 meanReward: 263.0312 meanLoss: 53.4629 meanLossQlbl: 1.3784 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 51.8647 meanLossQtgt_sigm: 0.2199\n",
      "Episode: 257 meanReward: 268.5312 meanLoss: 8.9248 meanLossQlbl: 0.0660 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.8049 meanLossQtgt_sigm: 0.0539\n",
      "Episode: 258 meanReward: 276.8438 meanLoss: 6.3920 meanLossQlbl: 0.3377 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.0199 meanLossQtgt_sigm: 0.0344\n",
      "Episode: 259 meanReward: 264.8438 meanLoss: 34.0411 meanLossQlbl: 0.3806 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.5194 meanLossQtgt_sigm: 0.1411\n",
      "Episode: 260 meanReward: 266.9375 meanLoss: 10.0743 meanLossQlbl: 0.2383 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.7905 meanLossQtgt_sigm: 0.0455\n",
      "Episode: 261 meanReward: 254.0312 meanLoss: 34.6997 meanLossQlbl: 0.3732 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 34.1596 meanLossQtgt_sigm: 0.1669\n",
      "Episode: 262 meanReward: 241.8125 meanLoss: 38.9105 meanLossQlbl: 0.4876 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 38.2743 meanLossQtgt_sigm: 0.1486\n",
      "Episode: 263 meanReward: 226.5938 meanLoss: 37.9565 meanLossQlbl: 0.1833 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.5562 meanLossQtgt_sigm: 0.2170\n",
      "Episode: 264 meanReward: 241.5312 meanLoss: 7.7361 meanLossQlbl: 0.3999 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.3102 meanLossQtgt_sigm: 0.0260\n",
      "Episode: 265 meanReward: 256.8438 meanLoss: 20.2241 meanLossQlbl: 0.2305 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.9409 meanLossQtgt_sigm: 0.0527\n",
      "Episode: 266 meanReward: 241.9062 meanLoss: 325.3535 meanLossQlbl: 1.3401 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 323.1365 meanLossQtgt_sigm: 0.8768\n",
      "Episode: 267 meanReward: 226.5938 meanLoss: 349.8302 meanLossQlbl: 4.9814 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 343.7578 meanLossQtgt_sigm: 1.0910\n",
      "Episode: 268 meanReward: 215.2812 meanLoss: 24.6553 meanLossQlbl: 1.7382 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 22.8320 meanLossQtgt_sigm: 0.0851\n",
      "Episode: 269 meanReward: 203.1875 meanLoss: 9.1012 meanLossQlbl: 0.6325 meanLossQlbl_sigm: 0.0191 meanLossQtgt: 8.4126 meanLossQtgt_sigm: 0.0370\n",
      "Episode: 270 meanReward: 202.9375 meanLoss: 9.3243 meanLossQlbl: 1.8268 meanLossQlbl_sigm: 0.0468 meanLossQtgt: 7.3808 meanLossQtgt_sigm: 0.0700\n",
      "Episode: 271 meanReward: 201.4375 meanLoss: 4.9727 meanLossQlbl: 0.2673 meanLossQlbl_sigm: 0.0044 meanLossQtgt: 4.6888 meanLossQtgt_sigm: 0.0121\n",
      "Episode: 272 meanReward: 191.9375 meanLoss: 19.8792 meanLossQlbl: 1.0739 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.7485 meanLossQtgt_sigm: 0.0568\n",
      "Episode: 273 meanReward: 197.2188 meanLoss: 12.7639 meanLossQlbl: 0.2197 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.4967 meanLossQtgt_sigm: 0.0474\n",
      "Episode: 274 meanReward: 192.7188 meanLoss: 11.7797 meanLossQlbl: 0.5068 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.2179 meanLossQtgt_sigm: 0.0550\n",
      "Episode: 275 meanReward: 208.0625 meanLoss: 2.1312 meanLossQlbl: 0.0268 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 2.0899 meanLossQtgt_sigm: 0.0144\n",
      "Episode: 276 meanReward: 223.3750 meanLoss: 20.7162 meanLossQlbl: 0.0584 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.6037 meanLossQtgt_sigm: 0.0541\n",
      "Episode: 277 meanReward: 229.3438 meanLoss: 18.9514 meanLossQlbl: 0.0784 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.8206 meanLossQtgt_sigm: 0.0524\n",
      "Episode: 278 meanReward: 240.8438 meanLoss: 18.0865 meanLossQlbl: 0.1725 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.8631 meanLossQtgt_sigm: 0.0508\n",
      "Episode: 279 meanReward: 252.0938 meanLoss: 5.7867 meanLossQlbl: 0.0536 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.7071 meanLossQtgt_sigm: 0.0259\n",
      "Episode: 280 meanReward: 259.5312 meanLoss: 17.8323 meanLossQlbl: 0.0532 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.7282 meanLossQtgt_sigm: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 281 meanReward: 254.3125 meanLoss: 316.7501 meanLossQlbl: 5.1973 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 310.7095 meanLossQtgt_sigm: 0.8434\n",
      "Episode: 282 meanReward: 248.4062 meanLoss: 327.5249 meanLossQlbl: 4.7648 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 321.8145 meanLossQtgt_sigm: 0.9456\n",
      "Episode: 283 meanReward: 233.1875 meanLoss: 360.9575 meanLossQlbl: 5.7629 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 354.0004 meanLossQtgt_sigm: 1.1943\n",
      "Episode: 284 meanReward: 226.0625 meanLoss: 13.4693 meanLossQlbl: 2.0005 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.4204 meanLossQtgt_sigm: 0.0483\n",
      "Episode: 285 meanReward: 228.2500 meanLoss: 39.3827 meanLossQlbl: 0.3657 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 38.8938 meanLossQtgt_sigm: 0.1231\n",
      "Episode: 286 meanReward: 224.5000 meanLoss: 180.2837 meanLossQlbl: 0.9088 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 178.7184 meanLossQtgt_sigm: 0.6565\n",
      "Episode: 287 meanReward: 224.2812 meanLoss: 317.4598 meanLossQlbl: 0.3521 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 315.9306 meanLossQtgt_sigm: 1.1771\n",
      "Episode: 288 meanReward: 229.1250 meanLoss: 23.5020 meanLossQlbl: 1.4496 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 21.9842 meanLossQtgt_sigm: 0.0682\n",
      "Episode: 289 meanReward: 238.8125 meanLoss: 9.9760 meanLossQlbl: 0.1256 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.8155 meanLossQtgt_sigm: 0.0348\n",
      "Episode: 290 meanReward: 245.7812 meanLoss: 15.0347 meanLossQlbl: 0.0142 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.9743 meanLossQtgt_sigm: 0.0463\n",
      "Episode: 291 meanReward: 257.7812 meanLoss: 14.4590 meanLossQlbl: 0.0204 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.3936 meanLossQtgt_sigm: 0.0450\n",
      "Episode: 292 meanReward: 254.7188 meanLoss: 36.0225 meanLossQlbl: 0.4693 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 35.4527 meanLossQtgt_sigm: 0.1005\n",
      "Episode: 293 meanReward: 257.6875 meanLoss: 14.1114 meanLossQlbl: 0.0550 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.9848 meanLossQtgt_sigm: 0.0716\n",
      "Episode: 294 meanReward: 259.0938 meanLoss: 16.5628 meanLossQlbl: 0.1414 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.3370 meanLossQtgt_sigm: 0.0844\n",
      "Episode: 295 meanReward: 274.3125 meanLoss: 4.5865 meanLossQlbl: 0.1332 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.4317 meanLossQtgt_sigm: 0.0215\n",
      "Episode: 296 meanReward: 263.2500 meanLoss: 43.3157 meanLossQlbl: 1.0472 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 42.1342 meanLossQtgt_sigm: 0.1344\n",
      "Episode: 297 meanReward: 252.9062 meanLoss: 11.7071 meanLossQlbl: 0.1514 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.4898 meanLossQtgt_sigm: 0.0659\n",
      "Episode: 298 meanReward: 253.4688 meanLoss: 88.4306 meanLossQlbl: 4.2766 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 83.8985 meanLossQtgt_sigm: 0.2555\n",
      "Episode: 299 meanReward: 256.2188 meanLoss: 60.0436 meanLossQlbl: 1.6107 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 58.2514 meanLossQtgt_sigm: 0.1815\n",
      "Episode: 300 meanReward: 267.5312 meanLoss: 13.7566 meanLossQlbl: 0.1208 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.5961 meanLossQtgt_sigm: 0.0397\n",
      "Episode: 301 meanReward: 264.8125 meanLoss: 309.1403 meanLossQlbl: 0.3145 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 307.9721 meanLossQtgt_sigm: 0.8537\n",
      "Episode: 302 meanReward: 262.9375 meanLoss: 356.5135 meanLossQlbl: 2.0904 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 353.3983 meanLossQtgt_sigm: 1.0248\n",
      "Episode: 303 meanReward: 264.4375 meanLoss: 16.7448 meanLossQlbl: 0.6819 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.0096 meanLossQtgt_sigm: 0.0533\n",
      "Episode: 304 meanReward: 273.9375 meanLoss: 15.6149 meanLossQlbl: 0.0898 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.4794 meanLossQtgt_sigm: 0.0458\n",
      "Episode: 305 meanReward: 283.3438 meanLoss: 19.3389 meanLossQlbl: 0.0772 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.2088 meanLossQtgt_sigm: 0.0529\n",
      "Episode: 306 meanReward: 291.5000 meanLoss: 19.1364 meanLossQlbl: 0.1463 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.9379 meanLossQtgt_sigm: 0.0522\n",
      "Episode: 307 meanReward: 277.3750 meanLoss: 184.4073 meanLossQlbl: 0.4936 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 183.3867 meanLossQtgt_sigm: 0.5271\n",
      "Episode: 308 meanReward: 263.7188 meanLoss: 99.1570 meanLossQlbl: 0.1778 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 98.6449 meanLossQtgt_sigm: 0.3343\n",
      "Episode: 309 meanReward: 248.4375 meanLoss: 227.7099 meanLossQlbl: 0.8097 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 226.1649 meanLossQtgt_sigm: 0.7352\n",
      "Episode: 310 meanReward: 233.1875 meanLoss: 199.5447 meanLossQlbl: 2.5379 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 196.2457 meanLossQtgt_sigm: 0.7611\n",
      "Episode: 311 meanReward: 222.0312 meanLoss: 20.7853 meanLossQlbl: 1.4120 meanLossQlbl_sigm: 0.0249 meanLossQtgt: 19.2878 meanLossQtgt_sigm: 0.0607\n",
      "Episode: 312 meanReward: 209.3438 meanLoss: 11.0113 meanLossQlbl: 0.2571 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.6805 meanLossQtgt_sigm: 0.0737\n",
      "Episode: 313 meanReward: 211.4688 meanLoss: 4.3201 meanLossQlbl: 0.0150 meanLossQlbl_sigm: 0.0031 meanLossQtgt: 4.2900 meanLossQtgt_sigm: 0.0120\n",
      "Episode: 314 meanReward: 213.7812 meanLoss: 3.8374 meanLossQlbl: 0.0764 meanLossQlbl_sigm: 0.0024 meanLossQtgt: 3.7304 meanLossQtgt_sigm: 0.0282\n",
      "Episode: 315 meanReward: 216.1250 meanLoss: 2.2256 meanLossQlbl: 0.2457 meanLossQlbl_sigm: 0.0305 meanLossQtgt: 1.8898 meanLossQtgt_sigm: 0.0596\n",
      "Episode: 316 meanReward: 208.2500 meanLoss: 5.1451 meanLossQlbl: 0.0225 meanLossQlbl_sigm: 0.0257 meanLossQtgt: 5.0300 meanLossQtgt_sigm: 0.0668\n",
      "Episode: 317 meanReward: 205.5312 meanLoss: 16.0575 meanLossQlbl: 0.3799 meanLossQlbl_sigm: 0.0029 meanLossQtgt: 15.5811 meanLossQtgt_sigm: 0.0936\n",
      "Episode: 318 meanReward: 208.5312 meanLoss: 4.5267 meanLossQlbl: 0.1699 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.3239 meanLossQtgt_sigm: 0.0328\n",
      "Episode: 319 meanReward: 223.8750 meanLoss: 2.0764 meanLossQlbl: 0.0549 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 2.0130 meanLossQtgt_sigm: 0.0085\n",
      "Episode: 320 meanReward: 231.5312 meanLoss: 15.3798 meanLossQlbl: 0.1354 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.1989 meanLossQtgt_sigm: 0.0456\n",
      "Episode: 321 meanReward: 216.5312 meanLoss: 237.5447 meanLossQlbl: 2.5326 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 234.2860 meanLossQtgt_sigm: 0.7261\n",
      "Episode: 322 meanReward: 205.0312 meanLoss: 44.4635 meanLossQlbl: 0.8403 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 43.4582 meanLossQtgt_sigm: 0.1650\n",
      "Episode: 323 meanReward: 190.7188 meanLoss: 187.4717 meanLossQlbl: 0.3566 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 186.5468 meanLossQtgt_sigm: 0.5683\n",
      "Episode: 324 meanReward: 187.3125 meanLoss: 23.7387 meanLossQlbl: 0.2773 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 23.3484 meanLossQtgt_sigm: 0.1130\n",
      "Episode: 325 meanReward: 183.9062 meanLoss: 96.5574 meanLossQlbl: 0.1347 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 96.1147 meanLossQtgt_sigm: 0.3080\n",
      "Episode: 326 meanReward: 182.2500 meanLoss: 32.8241 meanLossQlbl: 1.4500 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 31.2411 meanLossQtgt_sigm: 0.1331\n",
      "Episode: 327 meanReward: 168.7500 meanLoss: 63.5843 meanLossQlbl: 2.2086 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 61.1277 meanLossQtgt_sigm: 0.2480\n",
      "Episode: 328 meanReward: 165.7812 meanLoss: 94.8051 meanLossQlbl: 1.6654 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 92.8002 meanLossQtgt_sigm: 0.3395\n",
      "Episode: 329 meanReward: 164.5625 meanLoss: 37.0327 meanLossQlbl: 0.6798 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 36.2201 meanLossQtgt_sigm: 0.1328\n",
      "Episode: 330 meanReward: 165.1875 meanLoss: 51.1658 meanLossQlbl: 0.7213 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 50.2148 meanLossQtgt_sigm: 0.2297\n",
      "Episode: 331 meanReward: 172.0938 meanLoss: 18.7935 meanLossQlbl: 0.1845 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.5472 meanLossQtgt_sigm: 0.0619\n",
      "Episode: 332 meanReward: 156.9375 meanLoss: 174.0670 meanLossQlbl: 0.1618 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 173.2767 meanLossQtgt_sigm: 0.6285\n",
      "Episode: 333 meanReward: 157.5938 meanLoss: 67.6262 meanLossQlbl: 2.5922 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 64.6974 meanLossQtgt_sigm: 0.3366\n",
      "Episode: 334 meanReward: 157.6250 meanLoss: 193.6718 meanLossQlbl: 0.3807 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 192.6043 meanLossQtgt_sigm: 0.6868\n",
      "Episode: 335 meanReward: 142.4688 meanLoss: 335.7061 meanLossQlbl: 0.0864 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 334.4025 meanLossQtgt_sigm: 1.2171\n",
      "Episode: 336 meanReward: 127.5312 meanLoss: 221.2299 meanLossQlbl: 0.1143 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 220.1629 meanLossQtgt_sigm: 0.9528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 337 meanReward: 112.3750 meanLoss: 138.5301 meanLossQlbl: 0.0385 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 137.8032 meanLossQtgt_sigm: 0.6885\n",
      "Episode: 338 meanReward: 102.6250 meanLoss: 12.7394 meanLossQlbl: 0.7641 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.9361 meanLossQtgt_sigm: 0.0392\n",
      "Episode: 339 meanReward: 107.9062 meanLoss: 10.6703 meanLossQlbl: 0.3005 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.3161 meanLossQtgt_sigm: 0.0538\n",
      "Episode: 340 meanReward: 112.0625 meanLoss: 19.7199 meanLossQlbl: 0.9436 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.7103 meanLossQtgt_sigm: 0.0660\n",
      "Episode: 341 meanReward: 113.8438 meanLoss: 61.5299 meanLossQlbl: 0.6273 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 60.6541 meanLossQtgt_sigm: 0.2485\n",
      "Episode: 342 meanReward: 126.0312 meanLoss: 12.1106 meanLossQlbl: 0.4989 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.5815 meanLossQtgt_sigm: 0.0302\n",
      "Episode: 343 meanReward: 122.3125 meanLoss: 131.6662 meanLossQlbl: 2.2036 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 128.9348 meanLossQtgt_sigm: 0.5277\n",
      "Episode: 344 meanReward: 119.8750 meanLoss: 216.8828 meanLossQlbl: 1.4873 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 214.5614 meanLossQtgt_sigm: 0.8341\n",
      "Episode: 345 meanReward: 117.4688 meanLoss: 277.2075 meanLossQlbl: 0.1683 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 275.9933 meanLossQtgt_sigm: 1.0459\n",
      "Episode: 346 meanReward: 115.3125 meanLoss: 178.0362 meanLossQlbl: 0.5081 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 176.8241 meanLossQtgt_sigm: 0.7040\n",
      "Episode: 347 meanReward: 118.3750 meanLoss: 16.2136 meanLossQlbl: 0.8891 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.2643 meanLossQtgt_sigm: 0.0602\n",
      "Episode: 348 meanReward: 127.3125 meanLoss: 12.9990 meanLossQlbl: 0.6425 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.3250 meanLossQtgt_sigm: 0.0315\n",
      "Episode: 349 meanReward: 125.0000 meanLoss: 191.9985 meanLossQlbl: 0.3054 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 191.0238 meanLossQtgt_sigm: 0.6694\n",
      "Episode: 350 meanReward: 130.8750 meanLoss: 25.2366 meanLossQlbl: 0.3680 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 24.7859 meanLossQtgt_sigm: 0.0827\n",
      "Episode: 351 meanReward: 118.9688 meanLoss: 55.5611 meanLossQlbl: 0.1490 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 55.2296 meanLossQtgt_sigm: 0.1825\n",
      "Episode: 352 meanReward: 109.2188 meanLoss: 24.3026 meanLossQlbl: 0.1676 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 24.0405 meanLossQtgt_sigm: 0.0944\n",
      "Episode: 353 meanReward: 111.2188 meanLoss: 51.4390 meanLossQlbl: 0.0787 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 51.1528 meanLossQtgt_sigm: 0.2075\n",
      "Episode: 354 meanReward: 110.8750 meanLoss: 46.5218 meanLossQlbl: 0.5082 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 45.8559 meanLossQtgt_sigm: 0.1577\n",
      "Episode: 355 meanReward: 121.7812 meanLoss: 8.6622 meanLossQlbl: 0.1392 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.4857 meanLossQtgt_sigm: 0.0373\n",
      "Episode: 356 meanReward: 119.4062 meanLoss: 56.3307 meanLossQlbl: 0.6788 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 55.3856 meanLossQtgt_sigm: 0.2663\n",
      "Episode: 357 meanReward: 122.8125 meanLoss: 22.1054 meanLossQlbl: 0.3667 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 21.6524 meanLossQtgt_sigm: 0.0863\n",
      "Episode: 358 meanReward: 134.6250 meanLoss: 7.1024 meanLossQlbl: 0.0983 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.9737 meanLossQtgt_sigm: 0.0303\n",
      "Episode: 359 meanReward: 132.8750 meanLoss: 140.7827 meanLossQlbl: 1.7651 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 138.4915 meanLossQtgt_sigm: 0.5262\n",
      "Episode: 360 meanReward: 131.6875 meanLoss: 162.9631 meanLossQlbl: 0.9412 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 161.2634 meanLossQtgt_sigm: 0.7585\n",
      "Episode: 361 meanReward: 132.0625 meanLoss: 27.5020 meanLossQlbl: 1.2364 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.1568 meanLossQtgt_sigm: 0.1088\n",
      "Episode: 362 meanReward: 133.5312 meanLoss: 30.5024 meanLossQlbl: 0.4866 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 29.8829 meanLossQtgt_sigm: 0.1329\n",
      "Episode: 363 meanReward: 128.5625 meanLoss: 20.4568 meanLossQlbl: 0.3116 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.0517 meanLossQtgt_sigm: 0.0936\n",
      "Episode: 364 meanReward: 132.6562 meanLoss: 27.2432 meanLossQlbl: 0.1258 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.0034 meanLossQtgt_sigm: 0.1140\n",
      "Episode: 365 meanReward: 136.4062 meanLoss: 23.5592 meanLossQlbl: 0.2172 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 23.2450 meanLossQtgt_sigm: 0.0970\n",
      "Episode: 366 meanReward: 139.4062 meanLoss: 39.3082 meanLossQlbl: 0.1576 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 38.9909 meanLossQtgt_sigm: 0.1597\n",
      "Episode: 367 meanReward: 143.6875 meanLoss: 22.0533 meanLossQlbl: 0.1883 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 21.7727 meanLossQtgt_sigm: 0.0923\n",
      "Episode: 368 meanReward: 147.2500 meanLoss: 20.3177 meanLossQlbl: 0.0835 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.1392 meanLossQtgt_sigm: 0.0950\n",
      "Episode: 369 meanReward: 154.2812 meanLoss: 14.3058 meanLossQlbl: 0.2542 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.9911 meanLossQtgt_sigm: 0.0605\n",
      "Episode: 370 meanReward: 151.3125 meanLoss: 42.4198 meanLossQlbl: 0.3644 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 41.8784 meanLossQtgt_sigm: 0.1771\n",
      "Episode: 371 meanReward: 148.7500 meanLoss: 23.8473 meanLossQlbl: 1.6251 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 22.1487 meanLossQtgt_sigm: 0.0735\n",
      "Episode: 372 meanReward: 145.5625 meanLoss: 42.5705 meanLossQlbl: 0.9172 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 41.4831 meanLossQtgt_sigm: 0.1702\n",
      "Episode: 373 meanReward: 143.8125 meanLoss: 108.8188 meanLossQlbl: 7.4321 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 100.9178 meanLossQtgt_sigm: 0.4689\n",
      "Episode: 374 meanReward: 131.6562 meanLoss: 260.5129 meanLossQlbl: 4.8507 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 254.5982 meanLossQtgt_sigm: 1.0640\n",
      "Episode: 375 meanReward: 136.0000 meanLoss: 35.1572 meanLossQlbl: 1.3215 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.7097 meanLossQtgt_sigm: 0.1260\n",
      "Episode: 376 meanReward: 139.9062 meanLoss: 32.8428 meanLossQlbl: 0.2444 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 32.4722 meanLossQtgt_sigm: 0.1262\n",
      "Episode: 377 meanReward: 143.3125 meanLoss: 14.3921 meanLossQlbl: 0.4281 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.8849 meanLossQtgt_sigm: 0.0791\n",
      "Episode: 378 meanReward: 147.5312 meanLoss: 9.5406 meanLossQlbl: 0.5692 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.9227 meanLossQtgt_sigm: 0.0487\n",
      "Episode: 379 meanReward: 146.7188 meanLoss: 12.3207 meanLossQlbl: 0.3316 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.9244 meanLossQtgt_sigm: 0.0647\n",
      "Episode: 380 meanReward: 139.2500 meanLoss: 39.2198 meanLossQlbl: 1.8785 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.1708 meanLossQtgt_sigm: 0.1705\n",
      "Episode: 381 meanReward: 142.0000 meanLoss: 44.4560 meanLossQlbl: 0.7719 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 43.5225 meanLossQtgt_sigm: 0.1616\n",
      "Episode: 382 meanReward: 136.5938 meanLoss: 28.1490 meanLossQlbl: 1.0513 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.9812 meanLossQtgt_sigm: 0.1165\n",
      "Episode: 383 meanReward: 148.5000 meanLoss: 6.2156 meanLossQlbl: 0.3624 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.8422 meanLossQtgt_sigm: 0.0110\n",
      "Episode: 384 meanReward: 158.2500 meanLoss: 5.9958 meanLossQlbl: 0.2325 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 5.7408 meanLossQtgt_sigm: 0.0225\n",
      "Episode: 385 meanReward: 161.4062 meanLoss: 37.8286 meanLossQlbl: 0.4124 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 37.2991 meanLossQtgt_sigm: 0.1170\n",
      "Episode: 386 meanReward: 161.8125 meanLoss: 15.6751 meanLossQlbl: 0.4967 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.1012 meanLossQtgt_sigm: 0.0773\n",
      "Episode: 387 meanReward: 165.2188 meanLoss: 4.2606 meanLossQlbl: 0.1846 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.0650 meanLossQtgt_sigm: 0.0110\n",
      "Episode: 388 meanReward: 164.6250 meanLoss: 336.2865 meanLossQlbl: 0.2467 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 335.1415 meanLossQtgt_sigm: 0.8983\n",
      "Episode: 389 meanReward: 159.3438 meanLoss: 497.3986 meanLossQlbl: 1.1592 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 494.7708 meanLossQtgt_sigm: 1.4685\n",
      "Episode: 390 meanReward: 160.0000 meanLoss: 11.3729 meanLossQlbl: 0.5275 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.8034 meanLossQtgt_sigm: 0.0420\n",
      "Episode: 391 meanReward: 175.2500 meanLoss: 22.0009 meanLossQlbl: 0.3008 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 21.6471 meanLossQtgt_sigm: 0.0530\n",
      "Episode: 392 meanReward: 190.4688 meanLoss: 18.4499 meanLossQlbl: 0.0309 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.3674 meanLossQtgt_sigm: 0.0516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 393 meanReward: 201.6562 meanLoss: 18.5665 meanLossQlbl: 0.3003 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.2155 meanLossQtgt_sigm: 0.0506\n",
      "Episode: 394 meanReward: 198.8438 meanLoss: 302.5993 meanLossQlbl: 0.3387 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 301.4079 meanLossQtgt_sigm: 0.8527\n",
      "Episode: 395 meanReward: 194.1875 meanLoss: 489.9209 meanLossQlbl: 0.1253 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 488.2701 meanLossQtgt_sigm: 1.5256\n",
      "Episode: 396 meanReward: 190.0000 meanLoss: 444.9995 meanLossQlbl: 1.9461 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 441.5470 meanLossQtgt_sigm: 1.5064\n",
      "Episode: 397 meanReward: 186.7500 meanLoss: 86.4650 meanLossQlbl: 2.5459 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 83.6084 meanLossQtgt_sigm: 0.3107\n",
      "Episode: 398 meanReward: 188.2500 meanLoss: 33.3212 meanLossQlbl: 0.5963 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 32.6188 meanLossQtgt_sigm: 0.1061\n",
      "Episode: 399 meanReward: 199.1250 meanLoss: 10.4269 meanLossQlbl: 0.1072 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.2817 meanLossQtgt_sigm: 0.0379\n",
      "Episode: 400 meanReward: 207.3750 meanLoss: 23.0250 meanLossQlbl: 0.0341 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 22.9272 meanLossQtgt_sigm: 0.0637\n",
      "Episode: 401 meanReward: 203.8750 meanLoss: 28.8644 meanLossQlbl: 0.4126 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 28.3486 meanLossQtgt_sigm: 0.1033\n",
      "Episode: 402 meanReward: 201.6250 meanLoss: 153.0574 meanLossQlbl: 0.3136 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 152.1408 meanLossQtgt_sigm: 0.6030\n",
      "Episode: 403 meanReward: 203.0625 meanLoss: 28.5025 meanLossQlbl: 1.0267 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.3878 meanLossQtgt_sigm: 0.0881\n",
      "Episode: 404 meanReward: 207.4062 meanLoss: 26.4282 meanLossQlbl: 0.1494 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.1935 meanLossQtgt_sigm: 0.0853\n",
      "Episode: 405 meanReward: 222.6562 meanLoss: 7.5436 meanLossQlbl: 0.0993 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.4157 meanLossQtgt_sigm: 0.0286\n",
      "Episode: 406 meanReward: 237.8750 meanLoss: 17.1894 meanLossQlbl: 0.0637 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0760 meanLossQtgt_sigm: 0.0498\n",
      "Episode: 407 meanReward: 248.4062 meanLoss: 17.7500 meanLossQlbl: 0.2737 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.4269 meanLossQtgt_sigm: 0.0494\n",
      "Episode: 408 meanReward: 259.6250 meanLoss: 17.5535 meanLossQlbl: 0.1409 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.3626 meanLossQtgt_sigm: 0.0501\n",
      "Episode: 409 meanReward: 258.0938 meanLoss: 109.1344 meanLossQlbl: 1.1559 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 107.6700 meanLossQtgt_sigm: 0.3086\n",
      "Episode: 410 meanReward: 258.4688 meanLoss: 9.3024 meanLossQlbl: 0.5906 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 8.6791 meanLossQtgt_sigm: 0.0325\n",
      "Episode: 411 meanReward: 254.3750 meanLoss: 84.2534 meanLossQlbl: 0.5825 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 83.2490 meanLossQtgt_sigm: 0.4219\n",
      "Episode: 412 meanReward: 259.2500 meanLoss: 20.5244 meanLossQlbl: 1.0714 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.3801 meanLossQtgt_sigm: 0.0729\n",
      "Episode: 413 meanReward: 270.2500 meanLoss: 2.3385 meanLossQlbl: 0.2521 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 2.0785 meanLossQtgt_sigm: 0.0079\n",
      "Episode: 414 meanReward: 267.3125 meanLoss: 224.0261 meanLossQlbl: 0.6724 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 222.6380 meanLossQtgt_sigm: 0.7157\n",
      "Episode: 415 meanReward: 252.7500 meanLoss: 124.9701 meanLossQlbl: 14.0257 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 110.5194 meanLossQtgt_sigm: 0.4249\n",
      "Episode: 416 meanReward: 252.7500 meanLoss: 15.7358 meanLossQlbl: 0.2830 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.4094 meanLossQtgt_sigm: 0.0434\n",
      "Episode: 417 meanReward: 251.3438 meanLoss: 58.3497 meanLossQlbl: 0.8004 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 57.3857 meanLossQtgt_sigm: 0.1636\n",
      "Episode: 418 meanReward: 254.0625 meanLoss: 15.1299 meanLossQlbl: 0.6567 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.4116 meanLossQtgt_sigm: 0.0617\n",
      "Episode: 419 meanReward: 254.0625 meanLoss: 12.7108 meanLossQlbl: 0.0241 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.6444 meanLossQtgt_sigm: 0.0423\n",
      "Episode: 420 meanReward: 269.0625 meanLoss: 18.8437 meanLossQlbl: 0.0500 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.7417 meanLossQtgt_sigm: 0.0519\n",
      "Episode: 421 meanReward: 272.0938 meanLoss: 67.1929 meanLossQlbl: 0.7166 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 66.2685 meanLossQtgt_sigm: 0.2078\n",
      "Episode: 422 meanReward: 261.0938 meanLoss: 32.8617 meanLossQlbl: 0.4608 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 32.2770 meanLossQtgt_sigm: 0.1238\n",
      "Episode: 423 meanReward: 246.7500 meanLoss: 114.2985 meanLossQlbl: 0.3256 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 113.5258 meanLossQtgt_sigm: 0.4471\n",
      "Episode: 424 meanReward: 234.7500 meanLoss: 40.9302 meanLossQlbl: 0.9914 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 39.7955 meanLossQtgt_sigm: 0.1432\n",
      "Episode: 425 meanReward: 234.7500 meanLoss: 7.2590 meanLossQlbl: 0.2208 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.0069 meanLossQtgt_sigm: 0.0314\n",
      "Episode: 426 meanReward: 249.8438 meanLoss: 12.5806 meanLossQlbl: 0.2486 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 12.2904 meanLossQtgt_sigm: 0.0417\n",
      "Episode: 427 meanReward: 265.1250 meanLoss: 13.9452 meanLossQlbl: 0.1718 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.7288 meanLossQtgt_sigm: 0.0447\n",
      "Episode: 428 meanReward: 280.3750 meanLoss: 16.7218 meanLossQlbl: 0.0421 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.6307 meanLossQtgt_sigm: 0.0490\n",
      "Episode: 429 meanReward: 285.9062 meanLoss: 28.5198 meanLossQlbl: 0.1982 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 28.2290 meanLossQtgt_sigm: 0.0926\n",
      "Episode: 430 meanReward: 282.1250 meanLoss: 132.7385 meanLossQlbl: 3.1795 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 129.1056 meanLossQtgt_sigm: 0.4535\n",
      "Episode: 431 meanReward: 267.0625 meanLoss: 231.3643 meanLossQlbl: 2.7255 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 227.9359 meanLossQtgt_sigm: 0.7030\n",
      "Episode: 432 meanReward: 270.1875 meanLoss: 11.2896 meanLossQlbl: 0.3646 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.8896 meanLossQtgt_sigm: 0.0354\n",
      "Episode: 433 meanReward: 269.9375 meanLoss: 58.7843 meanLossQlbl: 0.5086 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 58.0939 meanLossQtgt_sigm: 0.1818\n",
      "Episode: 434 meanReward: 269.8438 meanLoss: 105.1444 meanLossQlbl: 9.7759 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 94.9243 meanLossQtgt_sigm: 0.4442\n",
      "Episode: 435 meanReward: 272.6250 meanLoss: 21.6284 meanLossQlbl: 0.8432 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.7090 meanLossQtgt_sigm: 0.0762\n",
      "Episode: 436 meanReward: 280.9688 meanLoss: 8.7072 meanLossQlbl: 0.0935 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.5809 meanLossQtgt_sigm: 0.0328\n",
      "Episode: 437 meanReward: 269.2500 meanLoss: 80.8075 meanLossQlbl: 0.6877 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 79.9089 meanLossQtgt_sigm: 0.2109\n",
      "Episode: 438 meanReward: 255.5625 meanLoss: 85.3806 meanLossQlbl: 0.4451 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 84.6205 meanLossQtgt_sigm: 0.3150\n",
      "Episode: 439 meanReward: 247.2188 meanLoss: 21.4720 meanLossQlbl: 0.9572 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.4447 meanLossQtgt_sigm: 0.0701\n",
      "Episode: 440 meanReward: 247.2188 meanLoss: 9.9772 meanLossQlbl: 0.0399 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.8998 meanLossQtgt_sigm: 0.0375\n",
      "Episode: 441 meanReward: 249.7188 meanLoss: 55.3315 meanLossQlbl: 0.7809 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 54.3951 meanLossQtgt_sigm: 0.1555\n",
      "Episode: 442 meanReward: 260.3125 meanLoss: 8.7091 meanLossQlbl: 0.3436 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 8.3314 meanLossQtgt_sigm: 0.0341\n",
      "Episode: 443 meanReward: 275.0312 meanLoss: 19.4833 meanLossQlbl: 0.1045 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 19.3257 meanLossQtgt_sigm: 0.0530\n",
      "Episode: 444 meanReward: 283.6875 meanLoss: 18.6880 meanLossQlbl: 0.1032 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.5329 meanLossQtgt_sigm: 0.0519\n",
      "Episode: 445 meanReward: 285.0000 meanLoss: 17.8573 meanLossQlbl: 0.0584 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.7480 meanLossQtgt_sigm: 0.0509\n",
      "Episode: 446 meanReward: 299.7812 meanLoss: 14.4062 meanLossQlbl: 0.2861 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.0752 meanLossQtgt_sigm: 0.0449\n",
      "Episode: 447 meanReward: 314.3438 meanLoss: 13.2290 meanLossQlbl: 0.1400 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.0464 meanLossQtgt_sigm: 0.0425\n",
      "Episode: 448 meanReward: 314.3438 meanLoss: 18.4435 meanLossQlbl: 0.0704 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.3216 meanLossQtgt_sigm: 0.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 449 meanReward: 325.5938 meanLoss: 17.3615 meanLossQlbl: 0.2721 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0400 meanLossQtgt_sigm: 0.0494\n",
      "Episode: 450 meanReward: 321.3438 meanLoss: 106.7128 meanLossQlbl: 1.8193 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 104.5992 meanLossQtgt_sigm: 0.2943\n",
      "Episode: 451 meanReward: 311.4062 meanLoss: 28.4163 meanLossQlbl: 0.3455 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 27.9661 meanLossQtgt_sigm: 0.1047\n",
      "Episode: 452 meanReward: 296.2188 meanLoss: 290.3668 meanLossQlbl: 1.9414 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 287.5859 meanLossQtgt_sigm: 0.8394\n",
      "Episode: 453 meanReward: 293.1250 meanLoss: 543.4620 meanLossQlbl: 5.7164 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 536.1633 meanLossQtgt_sigm: 1.5823\n",
      "Episode: 454 meanReward: 288.8438 meanLoss: 478.0087 meanLossQlbl: 1.5494 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 474.9526 meanLossQtgt_sigm: 1.5067\n",
      "Episode: 455 meanReward: 289.1875 meanLoss: 83.3444 meanLossQlbl: 4.9762 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 78.1065 meanLossQtgt_sigm: 0.2618\n",
      "Episode: 456 meanReward: 288.5312 meanLoss: 72.5291 meanLossQlbl: 0.8688 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 71.4394 meanLossQtgt_sigm: 0.2209\n",
      "Episode: 457 meanReward: 273.2812 meanLoss: 40.9762 meanLossQlbl: 1.8416 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 39.0037 meanLossQtgt_sigm: 0.1309\n",
      "Episode: 458 meanReward: 258.1250 meanLoss: 159.1723 meanLossQlbl: 1.2634 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 157.2767 meanLossQtgt_sigm: 0.6321\n",
      "Episode: 459 meanReward: 250.2812 meanLoss: 18.3864 meanLossQlbl: 0.7087 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.6082 meanLossQtgt_sigm: 0.0696\n",
      "Episode: 460 meanReward: 250.2812 meanLoss: 5.3649 meanLossQlbl: 0.3937 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.9462 meanLossQtgt_sigm: 0.0251\n",
      "Episode: 461 meanReward: 251.9062 meanLoss: 20.7075 meanLossQlbl: 0.1675 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 20.4734 meanLossQtgt_sigm: 0.0666\n",
      "Episode: 462 meanReward: 266.4062 meanLoss: 11.9272 meanLossQlbl: 0.0677 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 11.8186 meanLossQtgt_sigm: 0.0409\n",
      "Episode: 463 meanReward: 272.7500 meanLoss: 31.9928 meanLossQlbl: 0.3273 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 31.5656 meanLossQtgt_sigm: 0.0999\n",
      "Episode: 464 meanReward: 262.4062 meanLoss: 15.9263 meanLossQlbl: 0.2048 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.6453 meanLossQtgt_sigm: 0.0762\n",
      "Episode: 465 meanReward: 267.4688 meanLoss: 7.6762 meanLossQlbl: 0.2596 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.3837 meanLossQtgt_sigm: 0.0328\n",
      "Episode: 466 meanReward: 273.8438 meanLoss: 9.8924 meanLossQlbl: 0.6587 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.1850 meanLossQtgt_sigm: 0.0487\n",
      "Episode: 467 meanReward: 281.0312 meanLoss: 4.5315 meanLossQlbl: 0.3683 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 4.1411 meanLossQtgt_sigm: 0.0221\n",
      "Episode: 468 meanReward: 265.9375 meanLoss: 328.7096 meanLossQlbl: 0.7769 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 327.0432 meanLossQtgt_sigm: 0.8895\n",
      "Episode: 469 meanReward: 262.4375 meanLoss: 533.9689 meanLossQlbl: 2.4940 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 529.9599 meanLossQtgt_sigm: 1.5150\n",
      "Episode: 470 meanReward: 276.1250 meanLoss: 15.7232 meanLossQlbl: 0.6342 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 15.0349 meanLossQtgt_sigm: 0.0541\n",
      "Episode: 471 meanReward: 271.8125 meanLoss: 79.1673 meanLossQlbl: 0.7648 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 78.1626 meanLossQtgt_sigm: 0.2399\n",
      "Episode: 472 meanReward: 256.9688 meanLoss: 15.6909 meanLossQlbl: 0.7934 meanLossQlbl_sigm: 0.0002 meanLossQtgt: 14.7981 meanLossQtgt_sigm: 0.0993\n",
      "Episode: 473 meanReward: 255.0938 meanLoss: 27.1727 meanLossQlbl: 0.6377 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.4223 meanLossQtgt_sigm: 0.1126\n",
      "Episode: 474 meanReward: 253.2812 meanLoss: 1.4610 meanLossQlbl: 0.0430 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 1.4104 meanLossQtgt_sigm: 0.0076\n",
      "Episode: 475 meanReward: 241.1875 meanLoss: 61.7999 meanLossQlbl: 3.9213 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 57.7012 meanLossQtgt_sigm: 0.1773\n",
      "Episode: 476 meanReward: 230.0625 meanLoss: 5.8616 meanLossQlbl: 0.2243 meanLossQlbl_sigm: 0.0009 meanLossQtgt: 5.6176 meanLossQtgt_sigm: 0.0187\n",
      "Episode: 477 meanReward: 218.4062 meanLoss: 31.6446 meanLossQlbl: 0.3744 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 31.1458 meanLossQtgt_sigm: 0.1244\n",
      "Episode: 478 meanReward: 208.7500 meanLoss: 20.1188 meanLossQlbl: 4.0271 meanLossQlbl_sigm: 0.1119 meanLossQtgt: 15.8050 meanLossQtgt_sigm: 0.1749\n",
      "Episode: 479 meanReward: 208.7500 meanLoss: 3.3352 meanLossQlbl: 1.0093 meanLossQlbl_sigm: 0.0215 meanLossQtgt: 2.2822 meanLossQtgt_sigm: 0.0222\n",
      "Episode: 480 meanReward: 194.2188 meanLoss: 36.0537 meanLossQlbl: 1.4098 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 34.4352 meanLossQtgt_sigm: 0.2087\n",
      "Episode: 481 meanReward: 181.9375 meanLoss: 78.5432 meanLossQlbl: 1.6411 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 76.6922 meanLossQtgt_sigm: 0.2098\n",
      "Episode: 482 meanReward: 182.4688 meanLoss: 49.8892 meanLossQlbl: 1.3783 meanLossQlbl_sigm: 0.0021 meanLossQtgt: 48.3395 meanLossQtgt_sigm: 0.1693\n",
      "Episode: 483 meanReward: 180.3750 meanLoss: 8.3864 meanLossQlbl: 0.9619 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 7.3954 meanLossQtgt_sigm: 0.0290\n",
      "Episode: 484 meanReward: 184.2500 meanLoss: 27.0154 meanLossQlbl: 0.6702 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.2322 meanLossQtgt_sigm: 0.1130\n",
      "Episode: 485 meanReward: 188.7188 meanLoss: 26.5363 meanLossQlbl: 0.2971 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 26.1317 meanLossQtgt_sigm: 0.1075\n",
      "Episode: 486 meanReward: 191.3438 meanLoss: 33.9533 meanLossQlbl: 0.6964 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 33.1105 meanLossQtgt_sigm: 0.1463\n",
      "Episode: 487 meanReward: 194.2188 meanLoss: 6.5766 meanLossQlbl: 0.2652 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.2718 meanLossQtgt_sigm: 0.0396\n",
      "Episode: 488 meanReward: 206.8750 meanLoss: 6.6876 meanLossQlbl: 0.0783 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 6.5800 meanLossQtgt_sigm: 0.0293\n",
      "Episode: 489 meanReward: 221.1875 meanLoss: 25.4638 meanLossQlbl: 0.3416 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 25.0650 meanLossQtgt_sigm: 0.0572\n",
      "Episode: 490 meanReward: 230.6562 meanLoss: 36.0830 meanLossQlbl: 0.2127 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 35.7825 meanLossQtgt_sigm: 0.0877\n",
      "Episode: 491 meanReward: 225.6562 meanLoss: 103.5128 meanLossQlbl: 0.4436 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 102.7869 meanLossQtgt_sigm: 0.2823\n",
      "Episode: 492 meanReward: 211.9062 meanLoss: 146.1951 meanLossQlbl: 2.0573 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 143.7414 meanLossQtgt_sigm: 0.3964\n",
      "Episode: 493 meanReward: 218.4062 meanLoss: 10.2307 meanLossQlbl: 0.0650 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 10.1289 meanLossQtgt_sigm: 0.0368\n",
      "Episode: 494 meanReward: 204.4375 meanLoss: 180.5108 meanLossQlbl: 1.1643 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 178.8647 meanLossQtgt_sigm: 0.4819\n",
      "Episode: 495 meanReward: 201.2812 meanLoss: 20.0856 meanLossQlbl: 2.5283 meanLossQlbl_sigm: 0.0039 meanLossQtgt: 17.5291 meanLossQtgt_sigm: 0.0243\n",
      "Episode: 496 meanReward: 199.5625 meanLoss: 18.1516 meanLossQlbl: 0.7478 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.3271 meanLossQtgt_sigm: 0.0768\n",
      "Episode: 497 meanReward: 194.1875 meanLoss: 15.1002 meanLossQlbl: 0.3391 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.6754 meanLossQtgt_sigm: 0.0858\n",
      "Episode: 498 meanReward: 190.6875 meanLoss: 13.7526 meanLossQlbl: 0.1304 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 13.5376 meanLossQtgt_sigm: 0.0847\n",
      "Episode: 499 meanReward: 181.4688 meanLoss: 9.4941 meanLossQlbl: 0.2121 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 9.2336 meanLossQtgt_sigm: 0.0484\n",
      "Episode: 500 meanReward: 186.0000 meanLoss: 28.4788 meanLossQlbl: 0.3169 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 28.0562 meanLossQtgt_sigm: 0.1057\n",
      "Episode: 501 meanReward: 193.4062 meanLoss: 17.1392 meanLossQlbl: 0.3413 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 16.7323 meanLossQtgt_sigm: 0.0656\n",
      "Episode: 502 meanReward: 192.0000 meanLoss: 17.3186 meanLossQlbl: 0.1721 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.0960 meanLossQtgt_sigm: 0.0505\n",
      "Episode: 503 meanReward: 190.8438 meanLoss: 153.5650 meanLossQlbl: 0.3628 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 152.7703 meanLossQtgt_sigm: 0.4319\n",
      "Episode: 504 meanReward: 205.6875 meanLoss: 18.0528 meanLossQlbl: 0.1430 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 17.8600 meanLossQtgt_sigm: 0.0497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 505 meanReward: 211.1562 meanLoss: 36.0566 meanLossQlbl: 0.1683 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 35.7919 meanLossQtgt_sigm: 0.0964\n",
      "Episode: 506 meanReward: 212.9688 meanLoss: 14.7717 meanLossQlbl: 0.1638 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 14.5630 meanLossQtgt_sigm: 0.0449\n",
      "Episode: 507 meanReward: 214.4375 meanLoss: 448.3215 meanLossQlbl: 224.9740 meanLossQlbl_sigm: 0.3496 meanLossQtgt: 222.5566 meanLossQtgt_sigm: 0.4413\n",
      "Episode: 508 meanReward: 217.1875 meanLoss: 72.1639 meanLossQlbl: 27.9076 meanLossQlbl_sigm: 0.1029 meanLossQtgt: 44.0022 meanLossQtgt_sigm: 0.1513\n",
      "Episode: 509 meanReward: 213.6875 meanLoss: 116.7195 meanLossQlbl: 2.9293 meanLossQlbl_sigm: 0.0072 meanLossQtgt: 113.5731 meanLossQtgt_sigm: 0.2099\n",
      "Episode: 510 meanReward: 208.0000 meanLoss: 108.0712 meanLossQlbl: 1.3680 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 106.2301 meanLossQtgt_sigm: 0.4730\n",
      "Episode: 511 meanReward: 192.7500 meanLoss: 156.8881 meanLossQlbl: 2.9252 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 153.2759 meanLossQtgt_sigm: 0.6870\n",
      "Episode: 512 meanReward: 192.1562 meanLoss: 85.9296 meanLossQlbl: 4.2439 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 81.3124 meanLossQtgt_sigm: 0.3734\n",
      "Episode: 513 meanReward: 189.5625 meanLoss: 29.3326 meanLossQlbl: 0.8923 meanLossQlbl_sigm: 0.0005 meanLossQtgt: 28.2805 meanLossQtgt_sigm: 0.1593\n",
      "Episode: 514 meanReward: 187.8438 meanLoss: 13.5488 meanLossQlbl: 0.6506 meanLossQlbl_sigm: 0.0001 meanLossQtgt: 12.8235 meanLossQtgt_sigm: 0.0746\n",
      "Episode: 515 meanReward: 185.5938 meanLoss: 3.4082 meanLossQlbl: 0.0408 meanLossQlbl_sigm: 0.0049 meanLossQtgt: 3.3357 meanLossQtgt_sigm: 0.0268\n",
      "Episode: 516 meanReward: 182.9688 meanLoss: 4.2408 meanLossQlbl: 0.1783 meanLossQlbl_sigm: 0.0176 meanLossQtgt: 3.9995 meanLossQtgt_sigm: 0.0454\n",
      "Episode: 517 meanReward: 180.1250 meanLoss: 4.5147 meanLossQlbl: 0.6050 meanLossQlbl_sigm: 0.0601 meanLossQtgt: 3.7737 meanLossQtgt_sigm: 0.0759\n",
      "Episode: 518 meanReward: 180.5000 meanLoss: 11.1485 meanLossQlbl: 2.1360 meanLossQlbl_sigm: 0.0964 meanLossQtgt: 8.8065 meanLossQtgt_sigm: 0.1095\n",
      "Episode: 519 meanReward: 180.3438 meanLoss: 84.2662 meanLossQlbl: 44.0960 meanLossQlbl_sigm: 0.3456 meanLossQtgt: 39.4736 meanLossQtgt_sigm: 0.3510\n",
      "Episode: 520 meanReward: 180.3438 meanLoss: 11.8020 meanLossQlbl: 4.5052 meanLossQlbl_sigm: 0.0215 meanLossQtgt: 7.2521 meanLossQtgt_sigm: 0.0232\n",
      "Episode: 521 meanReward: 181.2812 meanLoss: 18.4130 meanLossQlbl: 0.0243 meanLossQlbl_sigm: 0.0000 meanLossQtgt: 18.3382 meanLossQtgt_sigm: 0.0505\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver() # save the trained model\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    episode_loss = deque(maxlen=batch_size)\n",
    "    episode_reward = deque(maxlen=batch_size)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        lossQlbl_batch, lossQlbl_sigm_batch, lossQtgt_batch, lossQtgt_sigm_batch = [], [], [], []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Testing\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            actions_logits = sess.run(model.actions_logits, \n",
    "                                      feed_dict = {model.states: states, \n",
    "                                                   model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            labelQs = np.max(actions_logits, axis=1) # explore\n",
    "            next_actions_logits = sess.run(model.actions_logits, \n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones) # exploit\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            loss, _, lossQlbl, lossQlbl_sigm, lossQtgt, lossQtgt_sigm = sess.run([model.loss, model.opt, \n",
    "                                                                                  model.lossQlbl, \n",
    "                                                                                  model.lossQlbl_sigm, \n",
    "                                                                                  model.lossQtgt, \n",
    "                                                                                  model.lossQtgt_sigm], \n",
    "                                            feed_dict = {model.states: states, \n",
    "                                                         model.actions: actions,\n",
    "                                                         model.targetQs: targetQs,\n",
    "                                                         model.labelQs: labelQs,\n",
    "                                                         model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            lossQlbl_batch.append(lossQlbl)\n",
    "            lossQlbl_sigm_batch.append(lossQlbl_sigm)\n",
    "            lossQtgt_batch.append(lossQtgt)\n",
    "            lossQtgt_sigm_batch.append(lossQtgt_sigm)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'meanReward: {:.4f}'.format(np.mean(episode_reward)),\n",
    "              'meanLoss: {:.4f}'.format(np.mean(loss_batch)),\n",
    "              'meanLossQlbl: {:.4f}'.format(np.mean(lossQlbl_batch)),\n",
    "              'meanLossQlbl_sigm: {:.4f}'.format(np.mean(lossQlbl_sigm_batch)),\n",
    "              'meanLossQtgt: {:.4f}'.format(np.mean(lossQtgt_batch)),\n",
    "              'meanLossQtgt_sigm: {:.4f}'.format(np.mean(lossQtgt_sigm_batch)))\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        if(np.mean(episode_reward) >= 500):\n",
    "            break\n",
    "    \n",
    "    saver.save(sess, 'checkpoints/model5.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward:120.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                             model.initial_state: initial_state})\n",
    "        action = np.argmax(action_logits)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "print('total_reward:{}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
