{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info: [ 0.02803708 -0.22961454 -0.02932536  0.24674302] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02344479 -0.0340863  -0.0243905  -0.05504365] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02276307  0.16137673 -0.02549138 -0.35532109] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0259906  -0.03337368 -0.0325978  -0.07078404] 0 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02532313  0.16220008 -0.03401348 -0.37357088] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.02856713  0.35778828 -0.0414849  -0.67678152] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.03572289  0.55346134 -0.05502053 -0.98223165] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.04679212  0.74927568 -0.07466516 -1.29167656] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.06177763  0.94526325 -0.10049869 -1.60677   ] 1 1.0 False {}\n",
      "state, action, reward, done, info: [ 0.0806829   1.14141963 -0.13263409 -1.92901684] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    #labelQs = tf.placeholder(tf.float32, [None], name='labelQs')\n",
    "        \n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return actions, states, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_outputs(action_size, hidden_size, states, cell, initial_state):\n",
    "#     actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "#                                             lstm_size=hidden_size, num_classes=action_size)\n",
    "#     return actions_logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "#     loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs_logits, \n",
    "#                                                                   labels=tf.nn.sigmoid(targetQs[1:])))\n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "#         # Output of the Model: calculating the loss and forwad pass\n",
    "#         self.actions_logits, self.final_state = model_outputs(\n",
    "#             action_size=action_size, hidden_size=hidden_size, \n",
    "#             states=self.states, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_total_reward = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state:', np.array(states).shape[1], \n",
    "#       'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32                # number of samples in the memory/ experience as mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.03167448, 0.01401935, 0.02102474, 0.04001393]),\n",
       " 0,\n",
       " array([ 0.03195487, -0.18139769,  0.02182502,  0.33925552]),\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 meanReward: 19.0000 meanLoss: 1.8555 ExploreP: 0.9981\n",
      "Episode: 1 meanReward: 14.5000 meanLoss: 7.7982 ExploreP: 0.9971\n",
      "Episode: 2 meanReward: 13.0000 meanLoss: 11.7375 ExploreP: 0.9961\n",
      "Episode: 3 meanReward: 18.7500 meanLoss: 7.2698 ExploreP: 0.9926\n",
      "Episode: 4 meanReward: 19.4000 meanLoss: 5.4157 ExploreP: 0.9904\n",
      "Episode: 5 meanReward: 18.5000 meanLoss: 9.0220 ExploreP: 0.9891\n",
      "Episode: 6 meanReward: 18.2857 meanLoss: 11.4750 ExploreP: 0.9874\n",
      "Episode: 7 meanReward: 19.2500 meanLoss: 9.4785 ExploreP: 0.9849\n",
      "Episode: 8 meanReward: 18.5556 meanLoss: 9.8010 ExploreP: 0.9836\n",
      "Episode: 9 meanReward: 18.9000 meanLoss: 12.5939 ExploreP: 0.9815\n",
      "Episode: 10 meanReward: 20.2727 meanLoss: 9.7284 ExploreP: 0.9782\n",
      "Episode: 11 meanReward: 19.5833 meanLoss: 9.8080 ExploreP: 0.9770\n",
      "Episode: 12 meanReward: 20.3846 meanLoss: 7.2012 ExploreP: 0.9741\n",
      "Episode: 13 meanReward: 21.0000 meanLoss: 11.8704 ExploreP: 0.9713\n",
      "Episode: 14 meanReward: 21.2667 meanLoss: 13.9603 ExploreP: 0.9689\n",
      "Episode: 15 meanReward: 20.8750 meanLoss: 17.2782 ExploreP: 0.9675\n",
      "Episode: 16 meanReward: 21.3529 meanLoss: 10.7348 ExploreP: 0.9647\n",
      "Episode: 17 meanReward: 21.6667 meanLoss: 7.5907 ExploreP: 0.9621\n",
      "Episode: 18 meanReward: 21.3684 meanLoss: 14.2639 ExploreP: 0.9606\n",
      "Episode: 19 meanReward: 21.5000 meanLoss: 7.5093 ExploreP: 0.9583\n",
      "Episode: 20 meanReward: 21.1905 meanLoss: 20.1576 ExploreP: 0.9569\n",
      "Episode: 21 meanReward: 21.1364 meanLoss: 34.1169 ExploreP: 0.9550\n",
      "Episode: 22 meanReward: 21.1304 meanLoss: 19.5405 ExploreP: 0.9530\n",
      "Episode: 23 meanReward: 20.8750 meanLoss: 11.8032 ExploreP: 0.9516\n",
      "Episode: 24 meanReward: 21.5600 meanLoss: 8.6690 ExploreP: 0.9481\n",
      "Episode: 25 meanReward: 22.7308 meanLoss: 9.2424 ExploreP: 0.9432\n",
      "Episode: 26 meanReward: 22.8148 meanLoss: 13.7920 ExploreP: 0.9409\n",
      "Episode: 27 meanReward: 23.2857 meanLoss: 13.5488 ExploreP: 0.9375\n",
      "Episode: 28 meanReward: 23.2759 meanLoss: 10.3741 ExploreP: 0.9354\n",
      "Episode: 29 meanReward: 22.8667 meanLoss: 16.8156 ExploreP: 0.9344\n",
      "Episode: 30 meanReward: 23.4194 meanLoss: 8.2917 ExploreP: 0.9307\n",
      "Episode: 31 meanReward: 23.0625 meanLoss: 5.3542 ExploreP: 0.9296\n",
      "Episode: 32 meanReward: 23.8125 meanLoss: 5.2536 ExploreP: 0.9256\n",
      "Episode: 33 meanReward: 23.9688 meanLoss: 9.8608 ExploreP: 0.9243\n",
      "Episode: 34 meanReward: 24.5938 meanLoss: 11.7941 ExploreP: 0.9215\n",
      "Episode: 35 meanReward: 24.1250 meanLoss: 44.3304 ExploreP: 0.9196\n",
      "Episode: 36 meanReward: 24.0938 meanLoss: 32.3644 ExploreP: 0.9177\n",
      "Episode: 37 meanReward: 23.9688 meanLoss: 41.0123 ExploreP: 0.9168\n",
      "Episode: 38 meanReward: 24.0938 meanLoss: 14.3635 ExploreP: 0.9149\n",
      "Episode: 39 meanReward: 23.6250 meanLoss: 4.5226 ExploreP: 0.9139\n",
      "Episode: 40 meanReward: 24.1250 meanLoss: 5.1634 ExploreP: 0.9113\n",
      "Episode: 41 meanReward: 23.9688 meanLoss: 7.7743 ExploreP: 0.9097\n",
      "Episode: 42 meanReward: 23.3438 meanLoss: 10.6569 ExploreP: 0.9085\n",
      "Episode: 43 meanReward: 24.0312 meanLoss: 7.9648 ExploreP: 0.9054\n",
      "Episode: 44 meanReward: 23.6250 meanLoss: 9.9642 ExploreP: 0.9039\n",
      "Episode: 45 meanReward: 23.4062 meanLoss: 11.3934 ExploreP: 0.9019\n",
      "Episode: 46 meanReward: 22.9062 meanLoss: 15.8594 ExploreP: 0.9011\n",
      "Episode: 47 meanReward: 22.8438 meanLoss: 15.9910 ExploreP: 0.9000\n",
      "Episode: 48 meanReward: 22.8750 meanLoss: 19.0231 ExploreP: 0.8973\n",
      "Episode: 49 meanReward: 23.0312 meanLoss: 25.8656 ExploreP: 0.8945\n",
      "Episode: 50 meanReward: 23.9062 meanLoss: 23.2721 ExploreP: 0.8906\n",
      "Episode: 51 meanReward: 23.9688 meanLoss: 7.8333 ExploreP: 0.8883\n",
      "Episode: 52 meanReward: 25.1562 meanLoss: 14.0566 ExploreP: 0.8837\n",
      "Episode: 53 meanReward: 24.9375 meanLoss: 6.6516 ExploreP: 0.8825\n",
      "Episode: 54 meanReward: 24.9688 meanLoss: 19.6241 ExploreP: 0.8806\n",
      "Episode: 55 meanReward: 25.3125 meanLoss: 48.1997 ExploreP: 0.8784\n",
      "Episode: 56 meanReward: 25.1562 meanLoss: 16.5508 ExploreP: 0.8755\n",
      "Episode: 57 meanReward: 24.7812 meanLoss: 8.0744 ExploreP: 0.8720\n",
      "Episode: 58 meanReward: 25.4375 meanLoss: 4.4441 ExploreP: 0.8681\n",
      "Episode: 59 meanReward: 25.3438 meanLoss: 7.1127 ExploreP: 0.8653\n",
      "Episode: 60 meanReward: 25.1875 meanLoss: 84.1994 ExploreP: 0.8637\n",
      "Episode: 61 meanReward: 25.2812 meanLoss: 91.0816 ExploreP: 0.8625\n",
      "Episode: 62 meanReward: 24.4688 meanLoss: 16.0143 ExploreP: 0.8613\n",
      "Episode: 63 meanReward: 26.5000 meanLoss: 4.3057 ExploreP: 0.8548\n",
      "Episode: 64 meanReward: 25.6250 meanLoss: 9.0866 ExploreP: 0.8535\n",
      "Episode: 65 meanReward: 25.5938 meanLoss: 21.5287 ExploreP: 0.8524\n",
      "Episode: 66 meanReward: 25.7500 meanLoss: 16.8652 ExploreP: 0.8494\n",
      "Episode: 67 meanReward: 26.3750 meanLoss: 7.7234 ExploreP: 0.8460\n",
      "Episode: 68 meanReward: 26.3125 meanLoss: 29.1072 ExploreP: 0.8444\n",
      "Episode: 69 meanReward: 27.0312 meanLoss: 15.5638 ExploreP: 0.8416\n",
      "Episode: 70 meanReward: 27.6250 meanLoss: 48.2838 ExploreP: 0.8383\n",
      "Episode: 71 meanReward: 27.6875 meanLoss: 10.7596 ExploreP: 0.8372\n",
      "Episode: 72 meanReward: 27.6562 meanLoss: 29.5319 ExploreP: 0.8349\n",
      "Episode: 73 meanReward: 27.4375 meanLoss: 25.9692 ExploreP: 0.8341\n",
      "Episode: 74 meanReward: 27.8750 meanLoss: 28.5861 ExploreP: 0.8318\n",
      "Episode: 75 meanReward: 28.7812 meanLoss: 15.7488 ExploreP: 0.8266\n",
      "Episode: 76 meanReward: 29.3438 meanLoss: 10.1423 ExploreP: 0.8238\n",
      "Episode: 77 meanReward: 29.1562 meanLoss: 22.4987 ExploreP: 0.8225\n",
      "Episode: 78 meanReward: 29.2188 meanLoss: 31.2294 ExploreP: 0.8216\n",
      "Episode: 79 meanReward: 29.5938 meanLoss: 26.0192 ExploreP: 0.8196\n",
      "Episode: 80 meanReward: 29.3125 meanLoss: 28.3115 ExploreP: 0.8179\n",
      "Episode: 81 meanReward: 29.2188 meanLoss: 29.2813 ExploreP: 0.8155\n",
      "Episode: 82 meanReward: 29.8750 meanLoss: 17.8204 ExploreP: 0.8103\n",
      "Episode: 83 meanReward: 30.1562 meanLoss: 16.6692 ExploreP: 0.8075\n",
      "Episode: 84 meanReward: 29.1562 meanLoss: 16.9930 ExploreP: 0.8058\n",
      "Episode: 85 meanReward: 31.3125 meanLoss: 8.5387 ExploreP: 0.7993\n",
      "Episode: 86 meanReward: 32.6250 meanLoss: 32.7064 ExploreP: 0.7943\n",
      "Episode: 87 meanReward: 34.6562 meanLoss: 7.2745 ExploreP: 0.7872\n",
      "Episode: 88 meanReward: 34.7812 meanLoss: 12.6698 ExploreP: 0.7843\n",
      "Episode: 89 meanReward: 34.9062 meanLoss: 10.6481 ExploreP: 0.7809\n",
      "Episode: 90 meanReward: 33.9062 meanLoss: 15.5486 ExploreP: 0.7799\n",
      "Episode: 91 meanReward: 33.6250 meanLoss: 17.2844 ExploreP: 0.7780\n",
      "Episode: 92 meanReward: 33.7188 meanLoss: 22.8149 ExploreP: 0.7764\n",
      "Episode: 93 meanReward: 33.8750 meanLoss: 19.9833 ExploreP: 0.7749\n",
      "Episode: 94 meanReward: 34.3438 meanLoss: 12.3170 ExploreP: 0.7727\n",
      "Episode: 95 meanReward: 32.4688 meanLoss: 21.6375 ExploreP: 0.7714\n",
      "Episode: 96 meanReward: 32.9062 meanLoss: 17.0448 ExploreP: 0.7692\n",
      "Episode: 97 meanReward: 33.1875 meanLoss: 18.8216 ExploreP: 0.7675\n",
      "Episode: 98 meanReward: 32.8750 meanLoss: 15.7559 ExploreP: 0.7656\n",
      "Episode: 99 meanReward: 34.9375 meanLoss: 3.9299 ExploreP: 0.7576\n",
      "Episode: 100 meanReward: 35.3125 meanLoss: 33.1861 ExploreP: 0.7552\n",
      "Episode: 101 meanReward: 37.3438 meanLoss: 9.5613 ExploreP: 0.7480\n",
      "Episode: 102 meanReward: 37.3750 meanLoss: 16.7651 ExploreP: 0.7450\n",
      "Episode: 103 meanReward: 37.6875 meanLoss: 26.4745 ExploreP: 0.7433\n",
      "Episode: 104 meanReward: 37.8750 meanLoss: 34.5555 ExploreP: 0.7408\n",
      "Episode: 105 meanReward: 38.8125 meanLoss: 20.9825 ExploreP: 0.7379\n",
      "Episode: 106 meanReward: 41.6562 meanLoss: 11.7607 ExploreP: 0.7292\n",
      "Episode: 107 meanReward: 40.4688 meanLoss: 28.5881 ExploreP: 0.7275\n",
      "Episode: 108 meanReward: 40.0625 meanLoss: 29.6452 ExploreP: 0.7259\n",
      "Episode: 109 meanReward: 40.1875 meanLoss: 35.2352 ExploreP: 0.7244\n",
      "Episode: 110 meanReward: 40.4062 meanLoss: 50.3685 ExploreP: 0.7232\n",
      "Episode: 111 meanReward: 40.5312 meanLoss: 26.5036 ExploreP: 0.7211\n",
      "Episode: 112 meanReward: 40.7812 meanLoss: 58.5253 ExploreP: 0.7190\n",
      "Episode: 113 meanReward: 40.6250 meanLoss: 66.7309 ExploreP: 0.7173\n",
      "Episode: 114 meanReward: 39.8438 meanLoss: 55.8513 ExploreP: 0.7145\n",
      "Episode: 115 meanReward: 39.2812 meanLoss: 46.0295 ExploreP: 0.7133\n",
      "Episode: 116 meanReward: 39.2500 meanLoss: 66.2344 ExploreP: 0.7119\n",
      "Episode: 117 meanReward: 37.6562 meanLoss: 56.3316 ExploreP: 0.7097\n",
      "Episode: 118 meanReward: 36.2188 meanLoss: 36.1714 ExploreP: 0.7085\n",
      "Episode: 119 meanReward: 34.4688 meanLoss: 23.7393 ExploreP: 0.7060\n",
      "Episode: 120 meanReward: 34.1875 meanLoss: 12.8253 ExploreP: 0.7041\n",
      "Episode: 121 meanReward: 36.1562 meanLoss: 10.7669 ExploreP: 0.6967\n",
      "Episode: 122 meanReward: 36.3438 meanLoss: 35.2643 ExploreP: 0.6953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 123 meanReward: 36.2188 meanLoss: 44.1763 ExploreP: 0.6940\n",
      "Episode: 124 meanReward: 36.6562 meanLoss: 30.0719 ExploreP: 0.6916\n",
      "Episode: 125 meanReward: 37.3750 meanLoss: 29.7347 ExploreP: 0.6887\n",
      "Episode: 126 meanReward: 37.5938 meanLoss: 8.4834 ExploreP: 0.6863\n",
      "Episode: 127 meanReward: 37.7812 meanLoss: 20.2507 ExploreP: 0.6847\n",
      "Episode: 128 meanReward: 37.4375 meanLoss: 32.5105 ExploreP: 0.6835\n",
      "Episode: 129 meanReward: 38.3125 meanLoss: 18.2685 ExploreP: 0.6801\n",
      "Episode: 130 meanReward: 37.8750 meanLoss: 22.4417 ExploreP: 0.6793\n",
      "Episode: 131 meanReward: 35.0938 meanLoss: 42.0542 ExploreP: 0.6781\n",
      "Episode: 132 meanReward: 36.5312 meanLoss: 16.3450 ExploreP: 0.6730\n",
      "Episode: 133 meanReward: 33.9375 meanLoss: 20.9813 ExploreP: 0.6720\n",
      "Episode: 134 meanReward: 33.0000 meanLoss: 19.8883 ExploreP: 0.6713\n",
      "Episode: 135 meanReward: 38.3125 meanLoss: 7.9742 ExploreP: 0.6587\n",
      "Episode: 136 meanReward: 37.6562 meanLoss: 87.7673 ExploreP: 0.6578\n",
      "Episode: 137 meanReward: 37.0625 meanLoss: 77.3160 ExploreP: 0.6565\n",
      "Episode: 138 meanReward: 35.0312 meanLoss: 27.5345 ExploreP: 0.6530\n",
      "Episode: 139 meanReward: 34.8125 meanLoss: 24.6662 ExploreP: 0.6518\n",
      "Episode: 140 meanReward: 35.0625 meanLoss: 47.9932 ExploreP: 0.6499\n",
      "Episode: 141 meanReward: 35.0000 meanLoss: 123.9969 ExploreP: 0.6487\n",
      "Episode: 142 meanReward: 37.0938 meanLoss: 29.4527 ExploreP: 0.6433\n",
      "Episode: 143 meanReward: 37.9375 meanLoss: 97.2083 ExploreP: 0.6398\n",
      "Episode: 144 meanReward: 37.5938 meanLoss: 31.4633 ExploreP: 0.6387\n",
      "Episode: 145 meanReward: 39.5938 meanLoss: 22.3080 ExploreP: 0.6332\n",
      "Episode: 146 meanReward: 39.5938 meanLoss: 324.8968 ExploreP: 0.6307\n",
      "Episode: 147 meanReward: 39.7188 meanLoss: 149.1318 ExploreP: 0.6294\n",
      "Episode: 148 meanReward: 39.8750 meanLoss: 81.6485 ExploreP: 0.6278\n",
      "Episode: 149 meanReward: 41.3125 meanLoss: 18.4204 ExploreP: 0.6231\n",
      "Episode: 150 meanReward: 41.8125 meanLoss: 22.2456 ExploreP: 0.6210\n",
      "Episode: 151 meanReward: 41.1875 meanLoss: 55.9441 ExploreP: 0.6201\n",
      "Episode: 152 meanReward: 40.6562 meanLoss: 101.8513 ExploreP: 0.6194\n",
      "Episode: 153 meanReward: 39.2188 meanLoss: 50.8967 ExploreP: 0.6157\n",
      "Episode: 154 meanReward: 39.8125 meanLoss: 52.0389 ExploreP: 0.6134\n",
      "Episode: 155 meanReward: 39.7188 meanLoss: 229.2308 ExploreP: 0.6123\n",
      "Episode: 156 meanReward: 40.9062 meanLoss: 66.0127 ExploreP: 0.6079\n",
      "Episode: 157 meanReward: 40.0625 meanLoss: 52.5215 ExploreP: 0.6071\n",
      "Episode: 158 meanReward: 40.5312 meanLoss: 13.9424 ExploreP: 0.6040\n",
      "Episode: 159 meanReward: 41.4688 meanLoss: 43.4590 ExploreP: 0.6009\n",
      "Episode: 160 meanReward: 41.2188 meanLoss: 279.4912 ExploreP: 0.6003\n",
      "Episode: 161 meanReward: 40.6562 meanLoss: 90.0123 ExploreP: 0.5983\n",
      "Episode: 162 meanReward: 42.3438 meanLoss: 10.2938 ExploreP: 0.5945\n",
      "Episode: 163 meanReward: 42.6250 meanLoss: 19.2132 ExploreP: 0.5930\n",
      "Episode: 164 meanReward: 40.7812 meanLoss: 21.2807 ExploreP: 0.5919\n",
      "Episode: 165 meanReward: 40.6875 meanLoss: 31.6780 ExploreP: 0.5912\n",
      "Episode: 166 meanReward: 41.9062 meanLoss: 47.3128 ExploreP: 0.5883\n",
      "Episode: 167 meanReward: 36.9062 meanLoss: 26.9938 ExploreP: 0.5864\n",
      "Episode: 168 meanReward: 37.0625 meanLoss: 44.8406 ExploreP: 0.5854\n",
      "Episode: 169 meanReward: 39.9688 meanLoss: 18.4718 ExploreP: 0.5788\n",
      "Episode: 170 meanReward: 40.8750 meanLoss: 157.6893 ExploreP: 0.5741\n",
      "Episode: 171 meanReward: 41.8438 meanLoss: 26.1591 ExploreP: 0.5714\n",
      "Episode: 172 meanReward: 41.4062 meanLoss: 54.1715 ExploreP: 0.5705\n",
      "Episode: 173 meanReward: 43.9688 meanLoss: 24.3391 ExploreP: 0.5649\n",
      "Episode: 174 meanReward: 42.1562 meanLoss: 223.8487 ExploreP: 0.5634\n",
      "Episode: 175 meanReward: 41.2188 meanLoss: 89.2424 ExploreP: 0.5620\n",
      "Episode: 176 meanReward: 41.1875 meanLoss: 74.9088 ExploreP: 0.5610\n",
      "Episode: 177 meanReward: 40.0625 meanLoss: 31.0688 ExploreP: 0.5582\n",
      "Episode: 178 meanReward: 39.5625 meanLoss: 80.2781 ExploreP: 0.5569\n",
      "Episode: 179 meanReward: 41.2188 meanLoss: 37.0139 ExploreP: 0.5528\n",
      "Episode: 180 meanReward: 42.3750 meanLoss: 37.5419 ExploreP: 0.5495\n",
      "Episode: 181 meanReward: 41.3438 meanLoss: 33.7767 ExploreP: 0.5471\n",
      "Episode: 182 meanReward: 40.8750 meanLoss: 34.5708 ExploreP: 0.5461\n",
      "Episode: 183 meanReward: 40.8750 meanLoss: 264.5787 ExploreP: 0.5453\n",
      "Episode: 184 meanReward: 41.0938 meanLoss: 180.5320 ExploreP: 0.5443\n",
      "Episode: 185 meanReward: 40.2188 meanLoss: 82.1422 ExploreP: 0.5426\n",
      "Episode: 186 meanReward: 39.3125 meanLoss: 99.8285 ExploreP: 0.5420\n",
      "Episode: 187 meanReward: 40.3750 meanLoss: 62.9519 ExploreP: 0.5393\n",
      "Episode: 188 meanReward: 38.7812 meanLoss: 359.4457 ExploreP: 0.5382\n",
      "Episode: 189 meanReward: 38.9688 meanLoss: 169.8988 ExploreP: 0.5371\n",
      "Episode: 190 meanReward: 39.5312 meanLoss: 22.6613 ExploreP: 0.5334\n",
      "Episode: 191 meanReward: 38.6562 meanLoss: 12.4911 ExploreP: 0.5321\n",
      "Episode: 192 meanReward: 42.5938 meanLoss: 20.9632 ExploreP: 0.5251\n",
      "Episode: 193 meanReward: 45.3438 meanLoss: 45.7999 ExploreP: 0.5189\n",
      "Episode: 194 meanReward: 44.3125 meanLoss: 122.9969 ExploreP: 0.5173\n",
      "Episode: 195 meanReward: 44.5625 meanLoss: 28.3675 ExploreP: 0.5155\n",
      "Episode: 196 meanReward: 44.5312 meanLoss: 90.9787 ExploreP: 0.5146\n",
      "Episode: 197 meanReward: 45.1562 meanLoss: 185.1976 ExploreP: 0.5130\n",
      "Episode: 198 meanReward: 44.6875 meanLoss: 42.7183 ExploreP: 0.5112\n",
      "Episode: 199 meanReward: 45.9375 meanLoss: 31.7150 ExploreP: 0.5076\n",
      "Episode: 200 meanReward: 47.0000 meanLoss: 199.6785 ExploreP: 0.5050\n",
      "Episode: 201 meanReward: 46.4688 meanLoss: 44.6983 ExploreP: 0.5002\n",
      "Episode: 202 meanReward: 44.6250 meanLoss: 473.2101 ExploreP: 0.4991\n",
      "Episode: 203 meanReward: 43.6562 meanLoss: 532.9238 ExploreP: 0.4982\n",
      "Episode: 204 meanReward: 43.8125 meanLoss: 534.5204 ExploreP: 0.4972\n",
      "Episode: 205 meanReward: 41.0938 meanLoss: 403.4536 ExploreP: 0.4965\n",
      "Episode: 206 meanReward: 49.5625 meanLoss: 21.5175 ExploreP: 0.4822\n",
      "Episode: 207 meanReward: 49.0625 meanLoss: 406.5055 ExploreP: 0.4818\n",
      "Episode: 208 meanReward: 52.6562 meanLoss: 113.8967 ExploreP: 0.4756\n",
      "Episode: 209 meanReward: 52.4375 meanLoss: 29.0369 ExploreP: 0.4735\n",
      "Episode: 210 meanReward: 54.7812 meanLoss: 85.4350 ExploreP: 0.4689\n",
      "Episode: 211 meanReward: 56.2812 meanLoss: 84.7867 ExploreP: 0.4634\n",
      "Episode: 212 meanReward: 55.2500 meanLoss: 311.1227 ExploreP: 0.4621\n",
      "Episode: 213 meanReward: 59.5312 meanLoss: 26.7991 ExploreP: 0.4539\n",
      "Episode: 214 meanReward: 63.0625 meanLoss: 45.8141 ExploreP: 0.4481\n",
      "Episode: 215 meanReward: 63.4375 meanLoss: 42.8749 ExploreP: 0.4469\n",
      "Episode: 216 meanReward: 67.9375 meanLoss: 20.9639 ExploreP: 0.4399\n",
      "Episode: 217 meanReward: 67.7500 meanLoss: 64.5866 ExploreP: 0.4388\n",
      "Episode: 218 meanReward: 68.1875 meanLoss: 133.8896 ExploreP: 0.4377\n",
      "Episode: 219 meanReward: 72.6875 meanLoss: 8.9177 ExploreP: 0.4295\n",
      "Episode: 220 meanReward: 73.3750 meanLoss: 185.9654 ExploreP: 0.4276\n",
      "Episode: 221 meanReward: 78.4062 meanLoss: 157.8504 ExploreP: 0.4201\n",
      "Episode: 222 meanReward: 80.5000 meanLoss: 413.2466 ExploreP: 0.4146\n",
      "Episode: 223 meanReward: 81.1875 meanLoss: 161.3173 ExploreP: 0.4127\n",
      "Episode: 224 meanReward: 77.6562 meanLoss: 22.7874 ExploreP: 0.4117\n",
      "Episode: 225 meanReward: 76.3125 meanLoss: 141.4816 ExploreP: 0.4086\n",
      "Episode: 226 meanReward: 80.6250 meanLoss: 403.2274 ExploreP: 0.4019\n",
      "Episode: 227 meanReward: 83.0312 meanLoss: 442.4873 ExploreP: 0.3975\n",
      "Episode: 228 meanReward: 84.1250 meanLoss: 558.5098 ExploreP: 0.3955\n",
      "Episode: 229 meanReward: 83.4688 meanLoss: 241.4137 ExploreP: 0.3951\n",
      "Episode: 230 meanReward: 82.7188 meanLoss: 399.7965 ExploreP: 0.3947\n",
      "Episode: 231 meanReward: 80.7500 meanLoss: 592.5994 ExploreP: 0.3943\n",
      "Episode: 232 meanReward: 79.4688 meanLoss: 668.4283 ExploreP: 0.3939\n",
      "Episode: 233 meanReward: 76.7500 meanLoss: 669.3016 ExploreP: 0.3935\n",
      "Episode: 234 meanReward: 76.4688 meanLoss: 610.3356 ExploreP: 0.3929\n",
      "Episode: 235 meanReward: 76.1875 meanLoss: 725.2821 ExploreP: 0.3926\n",
      "Episode: 236 meanReward: 75.8125 meanLoss: 900.1379 ExploreP: 0.3922\n",
      "Episode: 237 meanReward: 75.9375 meanLoss: 1091.9515 ExploreP: 0.3916\n",
      "Episode: 238 meanReward: 67.0000 meanLoss: 1143.0785 ExploreP: 0.3911\n",
      "Episode: 239 meanReward: 67.2500 meanLoss: 1140.4746 ExploreP: 0.3904\n",
      "Episode: 240 meanReward: 63.5625 meanLoss: 1060.9053 ExploreP: 0.3899\n",
      "Episode: 241 meanReward: 62.5625 meanLoss: 1185.6405 ExploreP: 0.3894\n",
      "Episode: 242 meanReward: 59.7500 meanLoss: 1375.1522 ExploreP: 0.3891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 243 meanReward: 56.4375 meanLoss: 1528.9994 ExploreP: 0.3885\n",
      "Episode: 244 meanReward: 55.8438 meanLoss: 1521.2595 ExploreP: 0.3881\n",
      "Episode: 245 meanReward: 50.5000 meanLoss: 1475.2307 ExploreP: 0.3877\n",
      "Episode: 246 meanReward: 46.8125 meanLoss: 1482.5260 ExploreP: 0.3872\n",
      "Episode: 247 meanReward: 46.2812 meanLoss: 1465.4651 ExploreP: 0.3868\n",
      "Episode: 248 meanReward: 41.5625 meanLoss: 1482.1561 ExploreP: 0.3864\n",
      "Episode: 249 meanReward: 41.0938 meanLoss: 1405.7157 ExploreP: 0.3859\n",
      "Episode: 250 meanReward: 40.6875 meanLoss: 1368.0604 ExploreP: 0.3855\n",
      "Episode: 251 meanReward: 34.8750 meanLoss: 1355.4873 ExploreP: 0.3852\n",
      "Episode: 252 meanReward: 33.8125 meanLoss: 1407.3969 ExploreP: 0.3848\n",
      "Episode: 253 meanReward: 28.5625 meanLoss: 1408.3229 ExploreP: 0.3843\n",
      "Episode: 254 meanReward: 24.6562 meanLoss: 1398.8888 ExploreP: 0.3839\n",
      "Episode: 255 meanReward: 23.7500 meanLoss: 1356.2793 ExploreP: 0.3832\n",
      "Episode: 256 meanReward: 23.3438 meanLoss: 1307.8210 ExploreP: 0.3828\n",
      "Episode: 257 meanReward: 21.2188 meanLoss: 1274.4108 ExploreP: 0.3825\n",
      "Episode: 258 meanReward: 16.9688 meanLoss: 1188.6632 ExploreP: 0.3812\n",
      "Episode: 259 meanReward: 13.7500 meanLoss: 1222.9546 ExploreP: 0.3809\n",
      "Episode: 260 meanReward: 12.6250 meanLoss: 1387.0533 ExploreP: 0.3803\n",
      "Episode: 261 meanReward: 12.8438 meanLoss: 1461.5612 ExploreP: 0.3796\n",
      "Episode: 262 meanReward: 12.9062 meanLoss: 1466.0706 ExploreP: 0.3791\n",
      "Episode: 263 meanReward: 13.0000 meanLoss: 1535.0747 ExploreP: 0.3786\n",
      "Episode: 264 meanReward: 13.0625 meanLoss: 1598.8394 ExploreP: 0.3782\n",
      "Episode: 265 meanReward: 13.0625 meanLoss: 1605.2039 ExploreP: 0.3778\n",
      "Episode: 266 meanReward: 13.0938 meanLoss: 1530.8011 ExploreP: 0.3772\n",
      "Episode: 267 meanReward: 13.4375 meanLoss: 1458.8604 ExploreP: 0.3765\n",
      "Episode: 268 meanReward: 13.4688 meanLoss: 1497.2677 ExploreP: 0.3761\n",
      "Episode: 269 meanReward: 13.5938 meanLoss: 1436.6862 ExploreP: 0.3753\n",
      "Episode: 270 meanReward: 13.5625 meanLoss: 1411.5819 ExploreP: 0.3749\n",
      "Episode: 271 meanReward: 13.2812 meanLoss: 1471.9661 ExploreP: 0.3746\n",
      "Episode: 272 meanReward: 13.2188 meanLoss: 1360.0757 ExploreP: 0.3742\n",
      "Episode: 273 meanReward: 13.1875 meanLoss: 1296.0533 ExploreP: 0.3737\n",
      "Episode: 274 meanReward: 13.2812 meanLoss: 1243.5516 ExploreP: 0.3733\n",
      "Episode: 275 meanReward: 13.2188 meanLoss: 1264.9219 ExploreP: 0.3728\n",
      "Episode: 276 meanReward: 13.5312 meanLoss: 1260.9745 ExploreP: 0.3721\n",
      "Episode: 277 meanReward: 13.7812 meanLoss: 1266.0935 ExploreP: 0.3714\n",
      "Episode: 278 meanReward: 13.8125 meanLoss: 1129.3724 ExploreP: 0.3709\n",
      "Episode: 279 meanReward: 13.7812 meanLoss: 1242.2594 ExploreP: 0.3705\n",
      "Episode: 280 meanReward: 13.8125 meanLoss: 1568.9854 ExploreP: 0.3701\n",
      "Episode: 281 meanReward: 13.8125 meanLoss: 2036.2733 ExploreP: 0.3697\n",
      "Episode: 282 meanReward: 13.9688 meanLoss: 2067.0327 ExploreP: 0.3691\n",
      "Episode: 283 meanReward: 14.0312 meanLoss: 2062.8435 ExploreP: 0.3687\n",
      "Episode: 284 meanReward: 14.0312 meanLoss: 2088.6685 ExploreP: 0.3683\n",
      "Episode: 285 meanReward: 14.0000 meanLoss: 2166.4834 ExploreP: 0.3679\n",
      "Episode: 286 meanReward: 14.4688 meanLoss: 2058.5315 ExploreP: 0.3670\n",
      "Episode: 287 meanReward: 15.2500 meanLoss: 2018.9709 ExploreP: 0.3654\n",
      "Episode: 288 meanReward: 15.2500 meanLoss: 2080.9438 ExploreP: 0.3651\n",
      "Episode: 289 meanReward: 17.8125 meanLoss: 669.9951 ExploreP: 0.3618\n",
      "Episode: 290 meanReward: 17.1875 meanLoss: 681.7584 ExploreP: 0.3613\n",
      "Episode: 291 meanReward: 17.2812 meanLoss: 539.8002 ExploreP: 0.3609\n",
      "Episode: 292 meanReward: 17.0938 meanLoss: 380.3273 ExploreP: 0.3606\n",
      "Episode: 293 meanReward: 17.4375 meanLoss: 245.4908 ExploreP: 0.3595\n",
      "Episode: 294 meanReward: 20.5938 meanLoss: 52.8645 ExploreP: 0.3556\n",
      "Episode: 295 meanReward: 24.0625 meanLoss: 73.3614 ExploreP: 0.3513\n",
      "Episode: 296 meanReward: 28.0938 meanLoss: 62.1990 ExploreP: 0.3465\n",
      "Episode: 297 meanReward: 35.6250 meanLoss: 21.4638 ExploreP: 0.3382\n",
      "Episode: 298 meanReward: 45.0938 meanLoss: 31.0913 ExploreP: 0.3279\n",
      "Episode: 299 meanReward: 60.0938 meanLoss: 10.6315 ExploreP: 0.3124\n",
      "Episode: 300 meanReward: 74.1250 meanLoss: 12.9174 ExploreP: 0.2988\n",
      "Episode: 301 meanReward: 76.5625 meanLoss: 107.3897 ExploreP: 0.2959\n",
      "Episode: 302 meanReward: 76.5625 meanLoss: 241.1614 ExploreP: 0.2956\n",
      "Episode: 303 meanReward: 76.6875 meanLoss: 305.7688 ExploreP: 0.2953\n",
      "Episode: 304 meanReward: 76.6562 meanLoss: 215.1841 ExploreP: 0.2949\n",
      "Episode: 305 meanReward: 76.8125 meanLoss: 244.0030 ExploreP: 0.2945\n",
      "Episode: 306 meanReward: 80.3438 meanLoss: 43.7634 ExploreP: 0.2909\n",
      "Episode: 307 meanReward: 83.7188 meanLoss: 12.6287 ExploreP: 0.2875\n",
      "Episode: 308 meanReward: 83.8438 meanLoss: 57.1735 ExploreP: 0.2869\n",
      "Episode: 309 meanReward: 83.7812 meanLoss: 269.4912 ExploreP: 0.2864\n",
      "Episode: 310 meanReward: 83.8125 meanLoss: 431.5717 ExploreP: 0.2860\n",
      "Episode: 311 meanReward: 83.9375 meanLoss: 254.8009 ExploreP: 0.2856\n",
      "Episode: 312 meanReward: 84.4062 meanLoss: 89.5068 ExploreP: 0.2849\n",
      "Episode: 313 meanReward: 91.3125 meanLoss: 8.1387 ExploreP: 0.2785\n",
      "Episode: 314 meanReward: 99.0000 meanLoss: 17.8290 ExploreP: 0.2716\n",
      "Episode: 315 meanReward: 104.6250 meanLoss: 19.1386 ExploreP: 0.2666\n",
      "Episode: 316 meanReward: 112.7188 meanLoss: 14.7313 ExploreP: 0.2598\n",
      "Episode: 317 meanReward: 126.1562 meanLoss: 10.4161 ExploreP: 0.2490\n",
      "Episode: 318 meanReward: 129.0000 meanLoss: 69.0881 ExploreP: 0.2462\n",
      "Episode: 319 meanReward: 131.0625 meanLoss: 102.3576 ExploreP: 0.2437\n",
      "Episode: 320 meanReward: 137.6562 meanLoss: 37.5534 ExploreP: 0.2386\n",
      "Episode: 321 meanReward: 135.5000 meanLoss: 327.7982 ExploreP: 0.2380\n",
      "Episode: 322 meanReward: 143.2500 meanLoss: 29.2804 ExploreP: 0.2321\n",
      "Episode: 323 meanReward: 145.6562 meanLoss: 42.0132 ExploreP: 0.2302\n",
      "Episode: 324 meanReward: 147.0312 meanLoss: 112.2785 ExploreP: 0.2290\n",
      "Episode: 325 meanReward: 146.9062 meanLoss: 60.6619 ExploreP: 0.2284\n",
      "Episode: 326 meanReward: 144.9062 meanLoss: 159.3951 ExploreP: 0.2273\n",
      "Episode: 327 meanReward: 148.1875 meanLoss: 8.2257 ExploreP: 0.2224\n",
      "Episode: 328 meanReward: 147.1562 meanLoss: 26.3424 ExploreP: 0.2201\n",
      "Episode: 329 meanReward: 145.8438 meanLoss: 9.1684 ExploreP: 0.2158\n",
      "Episode: 330 meanReward: 144.5312 meanLoss: 14.2928 ExploreP: 0.2102\n",
      "Episode: 331 meanReward: 140.8750 meanLoss: 11.0937 ExploreP: 0.2026\n",
      "Episode: 332 meanReward: 131.0000 meanLoss: 39.7819 ExploreP: 0.1999\n",
      "Episode: 333 meanReward: 132.8125 meanLoss: 30.3356 ExploreP: 0.1969\n",
      "Episode: 334 meanReward: 135.3125 meanLoss: 84.7542 ExploreP: 0.1952\n",
      "Episode: 335 meanReward: 141.8125 meanLoss: 10.2800 ExploreP: 0.1912\n",
      "Episode: 336 meanReward: 157.0938 meanLoss: 4.3698 ExploreP: 0.1824\n",
      "Episode: 337 meanReward: 166.7188 meanLoss: 29.9253 ExploreP: 0.1769\n",
      "Episode: 338 meanReward: 170.2812 meanLoss: 32.9688 ExploreP: 0.1729\n",
      "Episode: 339 meanReward: 172.1562 meanLoss: 28.4836 ExploreP: 0.1700\n",
      "Episode: 340 meanReward: 176.2188 meanLoss: 37.0113 ExploreP: 0.1675\n",
      "Episode: 341 meanReward: 181.7812 meanLoss: 32.9142 ExploreP: 0.1645\n",
      "Episode: 342 meanReward: 186.5625 meanLoss: 37.3855 ExploreP: 0.1619\n",
      "Episode: 343 meanReward: 201.7812 meanLoss: 5.6533 ExploreP: 0.1545\n",
      "Episode: 344 meanReward: 204.8750 meanLoss: 70.1346 ExploreP: 0.1527\n",
      "Episode: 345 meanReward: 199.0625 meanLoss: 80.5710 ExploreP: 0.1520\n",
      "Episode: 346 meanReward: 192.0000 meanLoss: 200.5753 ExploreP: 0.1515\n",
      "Episode: 347 meanReward: 186.4375 meanLoss: 220.9424 ExploreP: 0.1513\n",
      "Episode: 348 meanReward: 178.3438 meanLoss: 386.3871 ExploreP: 0.1512\n",
      "Episode: 349 meanReward: 164.7812 meanLoss: 331.7462 ExploreP: 0.1511\n",
      "Episode: 350 meanReward: 161.4062 meanLoss: 159.5136 ExploreP: 0.1509\n",
      "Episode: 351 meanReward: 159.3750 meanLoss: 56.0848 ExploreP: 0.1503\n",
      "Episode: 352 meanReward: 155.2188 meanLoss: 62.6010 ExploreP: 0.1491\n",
      "Episode: 353 meanReward: 160.0000 meanLoss: 10.2273 ExploreP: 0.1467\n",
      "Episode: 354 meanReward: 158.0000 meanLoss: 11.3761 ExploreP: 0.1440\n",
      "Episode: 355 meanReward: 170.8438 meanLoss: 6.4191 ExploreP: 0.1374\n",
      "Episode: 356 meanReward: 182.3750 meanLoss: 18.2637 ExploreP: 0.1322\n",
      "Episode: 357 meanReward: 186.8438 meanLoss: 45.0056 ExploreP: 0.1301\n",
      "Episode: 358 meanReward: 200.9062 meanLoss: 12.1237 ExploreP: 0.1243\n",
      "Episode: 359 meanReward: 196.9062 meanLoss: 75.1324 ExploreP: 0.1231\n",
      "Episode: 360 meanReward: 197.5000 meanLoss: 13.5213 ExploreP: 0.1217\n",
      "Episode: 361 meanReward: 201.9062 meanLoss: 8.2755 ExploreP: 0.1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 362 meanReward: 208.8750 meanLoss: 1.4039 ExploreP: 0.1126\n",
      "Episode: 363 meanReward: 212.5312 meanLoss: 17.3541 ExploreP: 0.1076\n",
      "Episode: 364 meanReward: 208.4688 meanLoss: 276.4479 ExploreP: 0.1075\n",
      "Episode: 365 meanReward: 204.0312 meanLoss: 444.3580 ExploreP: 0.1073\n",
      "Episode: 366 meanReward: 209.9375 meanLoss: 17.6224 ExploreP: 0.1046\n",
      "Episode: 367 meanReward: 218.6562 meanLoss: 13.1574 ExploreP: 0.1000\n",
      "Episode: 368 meanReward: 203.6250 meanLoss: 247.1835 ExploreP: 0.0998\n",
      "Episode: 369 meanReward: 206.2188 meanLoss: 10.8975 ExploreP: 0.0962\n",
      "Episode: 370 meanReward: 204.6875 meanLoss: 32.8970 ExploreP: 0.0946\n",
      "Episode: 371 meanReward: 214.6250 meanLoss: 3.2934 ExploreP: 0.0905\n",
      "Episode: 372 meanReward: 215.5312 meanLoss: 45.7641 ExploreP: 0.0890\n",
      "Episode: 373 meanReward: 214.5312 meanLoss: 46.8563 ExploreP: 0.0878\n",
      "Episode: 374 meanReward: 215.5312 meanLoss: 50.6885 ExploreP: 0.0862\n",
      "Episode: 375 meanReward: 207.0625 meanLoss: 154.1705 ExploreP: 0.0845\n",
      "Episode: 376 meanReward: 216.1250 meanLoss: 38.5206 ExploreP: 0.0815\n",
      "Episode: 377 meanReward: 230.2812 meanLoss: 7.5582 ExploreP: 0.0780\n",
      "Episode: 378 meanReward: 232.8438 meanLoss: 34.6008 ExploreP: 0.0772\n",
      "Episode: 379 meanReward: 236.4688 meanLoss: 38.3715 ExploreP: 0.0763\n",
      "Episode: 380 meanReward: 240.4375 meanLoss: 43.5880 ExploreP: 0.0754\n",
      "Episode: 381 meanReward: 244.3750 meanLoss: 41.6684 ExploreP: 0.0745\n",
      "Episode: 382 meanReward: 248.8750 meanLoss: 24.3868 ExploreP: 0.0736\n",
      "Episode: 383 meanReward: 252.9062 meanLoss: 24.7703 ExploreP: 0.0725\n",
      "Episode: 384 meanReward: 265.7812 meanLoss: 8.8788 ExploreP: 0.0694\n",
      "Episode: 385 meanReward: 265.1250 meanLoss: 30.4703 ExploreP: 0.0685\n",
      "Episode: 386 meanReward: 264.0625 meanLoss: 154.0327 ExploreP: 0.0676\n",
      "Episode: 387 meanReward: 248.6875 meanLoss: 605.3443 ExploreP: 0.0675\n",
      "Episode: 388 meanReward: 235.7500 meanLoss: 528.5632 ExploreP: 0.0675\n",
      "Episode: 389 meanReward: 246.1250 meanLoss: 14.8334 ExploreP: 0.0647\n",
      "Episode: 390 meanReward: 246.1250 meanLoss: 6.8940 ExploreP: 0.0620\n",
      "Episode: 391 meanReward: 247.4688 meanLoss: 53.9737 ExploreP: 0.0612\n",
      "Episode: 392 meanReward: 259.0938 meanLoss: 9.8475 ExploreP: 0.0587\n",
      "Episode: 393 meanReward: 263.7812 meanLoss: 15.8907 ExploreP: 0.0564\n",
      "Episode: 394 meanReward: 263.7812 meanLoss: 10.9371 ExploreP: 0.0541\n",
      "Episode: 395 meanReward: 250.6562 meanLoss: 73.4271 ExploreP: 0.0538\n",
      "Episode: 396 meanReward: 250.8125 meanLoss: 276.5094 ExploreP: 0.0537\n",
      "Episode: 397 meanReward: 250.6562 meanLoss: 499.7895 ExploreP: 0.0536\n",
      "Episode: 398 meanReward: 242.2500 meanLoss: 357.0692 ExploreP: 0.0536\n",
      "Episode: 399 meanReward: 228.4062 meanLoss: 90.1695 ExploreP: 0.0533\n",
      "Episode: 400 meanReward: 232.7188 meanLoss: 53.5715 ExploreP: 0.0527\n",
      "Episode: 401 meanReward: 235.5938 meanLoss: 121.5213 ExploreP: 0.0506\n",
      "Episode: 402 meanReward: 245.2812 meanLoss: 10.9681 ExploreP: 0.0486\n",
      "Episode: 403 meanReward: 245.2812 meanLoss: 15.9103 ExploreP: 0.0467\n",
      "Episode: 404 meanReward: 253.5625 meanLoss: 20.0879 ExploreP: 0.0451\n",
      "Episode: 405 meanReward: 258.6875 meanLoss: 20.1986 ExploreP: 0.0440\n",
      "Episode: 406 meanReward: 261.0938 meanLoss: 20.2932 ExploreP: 0.0431\n",
      "Episode: 407 meanReward: 269.5625 meanLoss: 12.0116 ExploreP: 0.0414\n",
      "Episode: 408 meanReward: 272.1875 meanLoss: 16.1983 ExploreP: 0.0399\n",
      "Episode: 409 meanReward: 272.1875 meanLoss: 16.9799 ExploreP: 0.0384\n",
      "Episode: 410 meanReward: 284.1250 meanLoss: 17.0718 ExploreP: 0.0371\n",
      "Episode: 411 meanReward: 293.9688 meanLoss: 19.4839 ExploreP: 0.0359\n",
      "Episode: 412 meanReward: 301.7188 meanLoss: 17.1234 ExploreP: 0.0349\n",
      "Episode: 413 meanReward: 304.9688 meanLoss: 22.6743 ExploreP: 0.0343\n",
      "Episode: 414 meanReward: 315.8125 meanLoss: 9.4161 ExploreP: 0.0331\n",
      "Episode: 415 meanReward: 326.0312 meanLoss: 17.2388 ExploreP: 0.0320\n",
      "Episode: 416 meanReward: 315.5625 meanLoss: 53.2858 ExploreP: 0.0316\n",
      "Episode: 417 meanReward: 315.7188 meanLoss: 374.1765 ExploreP: 0.0313\n",
      "Episode: 418 meanReward: 310.8750 meanLoss: 1481.9462 ExploreP: 0.0313\n",
      "Episode: 419 meanReward: 317.5938 meanLoss: 204.4522 ExploreP: 0.0308\n",
      "Episode: 420 meanReward: 332.9375 meanLoss: 40.3155 ExploreP: 0.0298\n",
      "Episode: 421 meanReward: 332.9375 meanLoss: 60.5595 ExploreP: 0.0288\n",
      "Episode: 422 meanReward: 325.4688 meanLoss: 31.5864 ExploreP: 0.0283\n",
      "Episode: 423 meanReward: 328.6562 meanLoss: 22.0077 ExploreP: 0.0279\n",
      "Episode: 424 meanReward: 323.5000 meanLoss: 12.6136 ExploreP: 0.0273\n",
      "Episode: 425 meanReward: 311.8750 meanLoss: 23.2501 ExploreP: 0.0271\n",
      "Episode: 426 meanReward: 299.3125 meanLoss: 24.2325 ExploreP: 0.0269\n",
      "Episode: 427 meanReward: 302.3125 meanLoss: 12.5150 ExploreP: 0.0266\n",
      "Episode: 428 meanReward: 305.0000 meanLoss: 31.1562 ExploreP: 0.0265\n",
      "Episode: 429 meanReward: 308.1875 meanLoss: 20.7573 ExploreP: 0.0263\n",
      "Episode: 430 meanReward: 308.6875 meanLoss: 58.7401 ExploreP: 0.0262\n",
      "Episode: 431 meanReward: 307.8438 meanLoss: 139.3977 ExploreP: 0.0262\n",
      "Episode: 432 meanReward: 306.6875 meanLoss: 27.8924 ExploreP: 0.0260\n",
      "Episode: 433 meanReward: 306.6875 meanLoss: 2.5685 ExploreP: 0.0252\n",
      "Episode: 434 meanReward: 295.8125 meanLoss: 45.3081 ExploreP: 0.0250\n",
      "Episode: 435 meanReward: 288.9688 meanLoss: 7.4368 ExploreP: 0.0246\n",
      "Episode: 436 meanReward: 279.5312 meanLoss: 22.6370 ExploreP: 0.0244\n",
      "Episode: 437 meanReward: 275.5312 meanLoss: 15.7968 ExploreP: 0.0241\n",
      "Episode: 438 meanReward: 278.8125 meanLoss: 7.4553 ExploreP: 0.0235\n",
      "Episode: 439 meanReward: 278.8125 meanLoss: 6.0862 ExploreP: 0.0229\n",
      "Episode: 440 meanReward: 269.5938 meanLoss: 38.8262 ExploreP: 0.0226\n",
      "Episode: 441 meanReward: 264.7812 meanLoss: 8.5031 ExploreP: 0.0222\n",
      "Episode: 442 meanReward: 264.7812 meanLoss: 8.4163 ExploreP: 0.0216\n",
      "Episode: 443 meanReward: 266.5312 meanLoss: 10.8136 ExploreP: 0.0210\n",
      "Episode: 444 meanReward: 270.1250 meanLoss: 16.6615 ExploreP: 0.0205\n",
      "Episode: 445 meanReward: 278.2812 meanLoss: 9.8022 ExploreP: 0.0200\n",
      "Episode: 446 meanReward: 278.2812 meanLoss: 17.3639 ExploreP: 0.0195\n",
      "Episode: 447 meanReward: 278.2812 meanLoss: 12.7799 ExploreP: 0.0190\n",
      "Episode: 448 meanReward: 288.7500 meanLoss: 16.8009 ExploreP: 0.0186\n",
      "Episode: 449 meanReward: 299.3750 meanLoss: 15.3576 ExploreP: 0.0182\n",
      "Episode: 450 meanReward: 314.7188 meanLoss: 16.3945 ExploreP: 0.0178\n",
      "Episode: 451 meanReward: 323.3750 meanLoss: 22.6752 ExploreP: 0.0174\n",
      "Episode: 452 meanReward: 323.3750 meanLoss: 16.0989 ExploreP: 0.0170\n",
      "Episode: 453 meanReward: 323.3750 meanLoss: 16.1081 ExploreP: 0.0167\n",
      "Episode: 454 meanReward: 330.8438 meanLoss: 16.5173 ExploreP: 0.0164\n",
      "Episode: 455 meanReward: 325.3750 meanLoss: 114.0165 ExploreP: 0.0163\n",
      "Episode: 456 meanReward: 315.9062 meanLoss: 181.4962 ExploreP: 0.0163\n",
      "Episode: 457 meanReward: 327.5312 meanLoss: 11.3699 ExploreP: 0.0160\n",
      "Episode: 458 meanReward: 324.8750 meanLoss: 263.6846 ExploreP: 0.0160\n",
      "Episode: 459 meanReward: 319.8125 meanLoss: 471.8708 ExploreP: 0.0160\n",
      "Episode: 460 meanReward: 316.9062 meanLoss: 400.4555 ExploreP: 0.0160\n",
      "Episode: 461 meanReward: 313.7812 meanLoss: 158.9798 ExploreP: 0.0160\n",
      "Episode: 462 meanReward: 314.0625 meanLoss: 43.7909 ExploreP: 0.0159\n",
      "Episode: 463 meanReward: 328.7500 meanLoss: 3.2609 ExploreP: 0.0157\n",
      "Episode: 464 meanReward: 340.6250 meanLoss: 12.7842 ExploreP: 0.0154\n",
      "Episode: 465 meanReward: 340.6250 meanLoss: 13.3955 ExploreP: 0.0151\n",
      "Episode: 466 meanReward: 351.5000 meanLoss: 12.1086 ExploreP: 0.0149\n",
      "Episode: 467 meanReward: 358.3438 meanLoss: 12.7993 ExploreP: 0.0146\n",
      "Episode: 468 meanReward: 369.4062 meanLoss: 16.5551 ExploreP: 0.0144\n",
      "Episode: 469 meanReward: 366.5938 meanLoss: 66.5997 ExploreP: 0.0144\n",
      "Episode: 470 meanReward: 361.4375 meanLoss: 30.4885 ExploreP: 0.0143\n",
      "Episode: 471 meanReward: 348.6875 meanLoss: 59.2955 ExploreP: 0.0142\n",
      "Episode: 472 meanReward: 344.7188 meanLoss: 20.1039 ExploreP: 0.0142\n",
      "Episode: 473 meanReward: 336.6875 meanLoss: 18.8458 ExploreP: 0.0142\n",
      "Episode: 474 meanReward: 321.9375 meanLoss: 16.3571 ExploreP: 0.0141\n",
      "Episode: 475 meanReward: 309.4375 meanLoss: 44.2834 ExploreP: 0.0141\n",
      "Episode: 476 meanReward: 295.5625 meanLoss: 15.1199 ExploreP: 0.0141\n",
      "Episode: 477 meanReward: 282.3750 meanLoss: 6.2581 ExploreP: 0.0140\n",
      "Episode: 478 meanReward: 269.5625 meanLoss: 6.4608 ExploreP: 0.0140\n",
      "Episode: 479 meanReward: 255.6562 meanLoss: 5.9709 ExploreP: 0.0140\n",
      "Episode: 480 meanReward: 240.8750 meanLoss: 19.1113 ExploreP: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 481 meanReward: 226.7500 meanLoss: 25.0663 ExploreP: 0.0140\n",
      "Episode: 482 meanReward: 213.2812 meanLoss: 42.5829 ExploreP: 0.0139\n",
      "Episode: 483 meanReward: 200.4688 meanLoss: 11.6991 ExploreP: 0.0139\n",
      "Episode: 484 meanReward: 189.5938 meanLoss: 5.0247 ExploreP: 0.0138\n",
      "Episode: 485 meanReward: 189.5938 meanLoss: 1.3832 ExploreP: 0.0136\n",
      "Episode: 486 meanReward: 179.3438 meanLoss: 29.6784 ExploreP: 0.0136\n",
      "Episode: 487 meanReward: 192.7500 meanLoss: 1.1867 ExploreP: 0.0134\n",
      "Episode: 488 meanReward: 207.3750 meanLoss: 2.1987 ExploreP: 0.0132\n",
      "Episode: 489 meanReward: 207.3750 meanLoss: 15.2709 ExploreP: 0.0131\n",
      "Episode: 490 meanReward: 211.4375 meanLoss: 14.4314 ExploreP: 0.0130\n",
      "Episode: 491 meanReward: 215.0938 meanLoss: 43.2845 ExploreP: 0.0130\n",
      "Episode: 492 meanReward: 220.2188 meanLoss: 20.5609 ExploreP: 0.0130\n",
      "Episode: 493 meanReward: 223.7500 meanLoss: 12.6692 ExploreP: 0.0129\n",
      "Episode: 494 meanReward: 238.2500 meanLoss: 3.6229 ExploreP: 0.0128\n",
      "Episode: 495 meanReward: 225.5312 meanLoss: 50.8353 ExploreP: 0.0127\n",
      "Episode: 496 meanReward: 210.4062 meanLoss: 63.1453 ExploreP: 0.0127\n",
      "Episode: 497 meanReward: 195.5000 meanLoss: 461.4634 ExploreP: 0.0127\n",
      "Episode: 498 meanReward: 180.2812 meanLoss: 305.8294 ExploreP: 0.0127\n",
      "Episode: 499 meanReward: 165.0312 meanLoss: 70.7050 ExploreP: 0.0127\n",
      "Episode: 500 meanReward: 149.7500 meanLoss: 32.0603 ExploreP: 0.0127\n",
      "Episode: 501 meanReward: 146.7812 meanLoss: 20.7343 ExploreP: 0.0127\n",
      "Episode: 502 meanReward: 146.5312 meanLoss: 37.0489 ExploreP: 0.0127\n",
      "Episode: 503 meanReward: 158.0938 meanLoss: 7.9965 ExploreP: 0.0125\n",
      "Episode: 504 meanReward: 171.2812 meanLoss: 8.0038 ExploreP: 0.0124\n",
      "Episode: 505 meanReward: 171.2188 meanLoss: 69.3821 ExploreP: 0.0124\n",
      "Episode: 506 meanReward: 185.9688 meanLoss: 3.8292 ExploreP: 0.0123\n",
      "Episode: 507 meanReward: 183.9375 meanLoss: 147.1010 ExploreP: 0.0123\n",
      "Episode: 508 meanReward: 189.7812 meanLoss: 10.0182 ExploreP: 0.0122\n",
      "Episode: 509 meanReward: 191.7812 meanLoss: 15.9256 ExploreP: 0.0122\n",
      "Episode: 510 meanReward: 194.7500 meanLoss: 12.0406 ExploreP: 0.0121\n",
      "Episode: 511 meanReward: 197.5000 meanLoss: 11.8072 ExploreP: 0.0121\n",
      "Episode: 512 meanReward: 200.4688 meanLoss: 7.8668 ExploreP: 0.0121\n",
      "Episode: 513 meanReward: 214.5938 meanLoss: 3.2677 ExploreP: 0.0120\n",
      "Episode: 514 meanReward: 228.0625 meanLoss: 18.6064 ExploreP: 0.0119\n",
      "Episode: 515 meanReward: 228.7812 meanLoss: 68.4923 ExploreP: 0.0119\n",
      "Episode: 516 meanReward: 228.8438 meanLoss: 67.5478 ExploreP: 0.0118\n",
      "Episode: 517 meanReward: 228.8438 meanLoss: 4.1461 ExploreP: 0.0118\n",
      "Episode: 518 meanReward: 227.0312 meanLoss: 67.3768 ExploreP: 0.0117\n",
      "Episode: 519 meanReward: 215.5000 meanLoss: 16.9841 ExploreP: 0.0117\n",
      "Episode: 520 meanReward: 203.6562 meanLoss: 15.0677 ExploreP: 0.0117\n",
      "Episode: 521 meanReward: 191.9375 meanLoss: 8.8983 ExploreP: 0.0117\n",
      "Episode: 522 meanReward: 187.8125 meanLoss: 42.6619 ExploreP: 0.0117\n",
      "Episode: 523 meanReward: 184.0000 meanLoss: 186.3269 ExploreP: 0.0117\n",
      "Episode: 524 meanReward: 178.8438 meanLoss: 223.0066 ExploreP: 0.0117\n",
      "Episode: 525 meanReward: 175.2500 meanLoss: 774.3193 ExploreP: 0.0117\n",
      "Episode: 526 meanReward: 159.9062 meanLoss: 1236.3748 ExploreP: 0.0117\n",
      "Episode: 527 meanReward: 157.2812 meanLoss: 1521.7935 ExploreP: 0.0117\n",
      "Episode: 528 meanReward: 157.0312 meanLoss: 1699.0471 ExploreP: 0.0117\n",
      "Episode: 529 meanReward: 156.6250 meanLoss: 1812.1047 ExploreP: 0.0117\n",
      "Episode: 530 meanReward: 156.5312 meanLoss: 1922.4397 ExploreP: 0.0117\n",
      "Episode: 531 meanReward: 156.4062 meanLoss: 1983.9736 ExploreP: 0.0117\n",
      "Episode: 532 meanReward: 156.3750 meanLoss: 2026.9769 ExploreP: 0.0117\n",
      "Episode: 533 meanReward: 156.2188 meanLoss: 2058.0845 ExploreP: 0.0117\n",
      "Episode: 534 meanReward: 149.9688 meanLoss: 2076.5750 ExploreP: 0.0116\n",
      "Episode: 535 meanReward: 135.8750 meanLoss: 2132.4041 ExploreP: 0.0116\n",
      "Episode: 536 meanReward: 120.5625 meanLoss: 2175.8164 ExploreP: 0.0116\n",
      "Episode: 537 meanReward: 118.1875 meanLoss: 2212.7783 ExploreP: 0.0116\n",
      "Episode: 538 meanReward: 102.9062 meanLoss: 2233.0322 ExploreP: 0.0116\n",
      "Episode: 539 meanReward: 102.2188 meanLoss: 2259.6267 ExploreP: 0.0116\n",
      "Episode: 540 meanReward: 95.0625 meanLoss: 2287.1243 ExploreP: 0.0116\n",
      "Episode: 541 meanReward: 90.9688 meanLoss: 2327.5576 ExploreP: 0.0116\n",
      "Episode: 542 meanReward: 85.5000 meanLoss: 2349.1892 ExploreP: 0.0116\n",
      "Episode: 543 meanReward: 81.3750 meanLoss: 2363.1772 ExploreP: 0.0116\n",
      "Episode: 544 meanReward: 77.9375 meanLoss: 2343.3284 ExploreP: 0.0116\n",
      "Episode: 545 meanReward: 62.6562 meanLoss: 2347.9634 ExploreP: 0.0116\n",
      "Episode: 546 meanReward: 47.4062 meanLoss: 2374.5994 ExploreP: 0.0116\n",
      "Episode: 547 meanReward: 44.2188 meanLoss: 2422.6538 ExploreP: 0.0116\n",
      "Episode: 548 meanReward: 39.7500 meanLoss: 2449.6575 ExploreP: 0.0116\n",
      "Episode: 549 meanReward: 24.5312 meanLoss: 2439.1672 ExploreP: 0.0116\n",
      "Episode: 550 meanReward: 21.3438 meanLoss: 2429.9563 ExploreP: 0.0116\n",
      "Episode: 551 meanReward: 17.5938 meanLoss: 2449.6240 ExploreP: 0.0116\n",
      "Episode: 552 meanReward: 14.1562 meanLoss: 2456.7773 ExploreP: 0.0116\n",
      "Episode: 553 meanReward: 10.6562 meanLoss: 2425.4353 ExploreP: 0.0116\n",
      "Episode: 554 meanReward: 10.6562 meanLoss: 2416.7646 ExploreP: 0.0116\n",
      "Episode: 555 meanReward: 10.8125 meanLoss: 2422.4185 ExploreP: 0.0116\n",
      "Episode: 556 meanReward: 10.8438 meanLoss: 2433.1406 ExploreP: 0.0116\n",
      "Episode: 557 meanReward: 10.9062 meanLoss: 2449.7021 ExploreP: 0.0116\n",
      "Episode: 558 meanReward: 11.0312 meanLoss: 2427.5557 ExploreP: 0.0116\n",
      "Episode: 559 meanReward: 11.1562 meanLoss: 2398.1133 ExploreP: 0.0116\n",
      "Episode: 560 meanReward: 11.3438 meanLoss: 2410.4421 ExploreP: 0.0116\n",
      "Episode: 561 meanReward: 11.4062 meanLoss: 2446.6140 ExploreP: 0.0116\n",
      "Episode: 562 meanReward: 11.5000 meanLoss: 2467.5718 ExploreP: 0.0116\n",
      "Episode: 563 meanReward: 11.6875 meanLoss: 2459.2512 ExploreP: 0.0116\n",
      "Episode: 564 meanReward: 11.8125 meanLoss: 2431.6101 ExploreP: 0.0116\n",
      "Episode: 565 meanReward: 12.0000 meanLoss: 2432.0349 ExploreP: 0.0116\n",
      "Episode: 566 meanReward: 12.0625 meanLoss: 2460.2349 ExploreP: 0.0116\n",
      "Episode: 567 meanReward: 12.1562 meanLoss: 2451.1289 ExploreP: 0.0116\n",
      "Episode: 568 meanReward: 12.3438 meanLoss: 2419.0835 ExploreP: 0.0116\n",
      "Episode: 569 meanReward: 12.5000 meanLoss: 2437.9102 ExploreP: 0.0116\n",
      "Episode: 570 meanReward: 12.5938 meanLoss: 2475.8391 ExploreP: 0.0116\n",
      "Episode: 571 meanReward: 12.6250 meanLoss: 2459.8247 ExploreP: 0.0116\n",
      "Episode: 572 meanReward: 12.6250 meanLoss: 2426.9280 ExploreP: 0.0116\n",
      "Episode: 573 meanReward: 12.7500 meanLoss: 2466.3135 ExploreP: 0.0116\n",
      "Episode: 574 meanReward: 12.8750 meanLoss: 2506.7273 ExploreP: 0.0116\n",
      "Episode: 575 meanReward: 12.9375 meanLoss: 2503.9634 ExploreP: 0.0116\n",
      "Episode: 576 meanReward: 13.0938 meanLoss: 2477.3833 ExploreP: 0.0116\n",
      "Episode: 577 meanReward: 13.2812 meanLoss: 2508.1621 ExploreP: 0.0116\n",
      "Episode: 578 meanReward: 13.4375 meanLoss: 2533.6699 ExploreP: 0.0116\n",
      "Episode: 579 meanReward: 13.5625 meanLoss: 2546.7424 ExploreP: 0.0116\n",
      "Episode: 580 meanReward: 13.6875 meanLoss: 2585.5369 ExploreP: 0.0116\n",
      "Episode: 581 meanReward: 13.8125 meanLoss: 2582.6011 ExploreP: 0.0116\n",
      "Episode: 582 meanReward: 13.9375 meanLoss: 2547.9324 ExploreP: 0.0115\n",
      "Episode: 583 meanReward: 14.0938 meanLoss: 2551.8240 ExploreP: 0.0115\n",
      "Episode: 584 meanReward: 14.1562 meanLoss: 2593.7422 ExploreP: 0.0115\n",
      "Episode: 585 meanReward: 14.8438 meanLoss: 2429.4375 ExploreP: 0.0115\n",
      "Episode: 586 meanReward: 14.8125 meanLoss: 1161.7418 ExploreP: 0.0115\n",
      "Episode: 587 meanReward: 14.6562 meanLoss: 472.4554 ExploreP: 0.0115\n",
      "Episode: 588 meanReward: 14.7188 meanLoss: 423.3875 ExploreP: 0.0115\n",
      "Episode: 589 meanReward: 14.6875 meanLoss: 827.5623 ExploreP: 0.0115\n",
      "Episode: 590 meanReward: 14.6562 meanLoss: 1182.8253 ExploreP: 0.0115\n",
      "Episode: 591 meanReward: 14.5312 meanLoss: 1346.1038 ExploreP: 0.0115\n",
      "Episode: 592 meanReward: 14.4688 meanLoss: 1383.9214 ExploreP: 0.0115\n",
      "Episode: 593 meanReward: 14.4688 meanLoss: 1439.8037 ExploreP: 0.0115\n",
      "Episode: 594 meanReward: 14.4375 meanLoss: 1458.3558 ExploreP: 0.0115\n",
      "Episode: 595 meanReward: 14.4062 meanLoss: 1489.1173 ExploreP: 0.0115\n",
      "Episode: 596 meanReward: 14.3750 meanLoss: 1511.4839 ExploreP: 0.0115\n",
      "Episode: 597 meanReward: 14.2812 meanLoss: 1542.8257 ExploreP: 0.0115\n",
      "Episode: 598 meanReward: 14.3438 meanLoss: 1542.7067 ExploreP: 0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 599 meanReward: 14.3438 meanLoss: 1498.0432 ExploreP: 0.0115\n",
      "Episode: 600 meanReward: 14.2812 meanLoss: 1467.5280 ExploreP: 0.0115\n",
      "Episode: 601 meanReward: 14.1875 meanLoss: 1463.2268 ExploreP: 0.0115\n",
      "Episode: 602 meanReward: 14.1875 meanLoss: 1510.8684 ExploreP: 0.0115\n",
      "Episode: 603 meanReward: 14.1250 meanLoss: 1567.5067 ExploreP: 0.0115\n",
      "Episode: 604 meanReward: 14.1562 meanLoss: 1570.2809 ExploreP: 0.0115\n",
      "Episode: 605 meanReward: 14.0938 meanLoss: 1518.1061 ExploreP: 0.0115\n",
      "Episode: 606 meanReward: 14.0938 meanLoss: 1526.1163 ExploreP: 0.0115\n",
      "Episode: 607 meanReward: 14.1250 meanLoss: 1531.3870 ExploreP: 0.0115\n",
      "Episode: 608 meanReward: 14.0625 meanLoss: 1528.2032 ExploreP: 0.0115\n",
      "Episode: 609 meanReward: 13.8750 meanLoss: 1561.2769 ExploreP: 0.0115\n",
      "Episode: 610 meanReward: 13.7812 meanLoss: 1590.7172 ExploreP: 0.0115\n",
      "Episode: 611 meanReward: 13.7812 meanLoss: 1553.5946 ExploreP: 0.0115\n",
      "Episode: 612 meanReward: 13.7500 meanLoss: 1570.7556 ExploreP: 0.0115\n",
      "Episode: 613 meanReward: 13.5938 meanLoss: 1630.6182 ExploreP: 0.0115\n",
      "Episode: 614 meanReward: 13.5625 meanLoss: 1626.8445 ExploreP: 0.0115\n",
      "Episode: 615 meanReward: 13.4688 meanLoss: 1588.4966 ExploreP: 0.0115\n",
      "Episode: 616 meanReward: 13.4375 meanLoss: 1593.2388 ExploreP: 0.0115\n",
      "Episode: 617 meanReward: 12.8438 meanLoss: 1570.9368 ExploreP: 0.0115\n",
      "Episode: 618 meanReward: 12.9375 meanLoss: 1627.3391 ExploreP: 0.0115\n",
      "Episode: 619 meanReward: 13.0938 meanLoss: 1725.7825 ExploreP: 0.0115\n",
      "Episode: 620 meanReward: 13.2812 meanLoss: 1652.6285 ExploreP: 0.0115\n",
      "Episode: 621 meanReward: 13.4062 meanLoss: 1692.4187 ExploreP: 0.0115\n",
      "Episode: 622 meanReward: 13.5312 meanLoss: 1723.8125 ExploreP: 0.0115\n",
      "Episode: 623 meanReward: 13.8750 meanLoss: 1721.4358 ExploreP: 0.0115\n",
      "Episode: 624 meanReward: 14.0000 meanLoss: 1809.5167 ExploreP: 0.0115\n",
      "Episode: 625 meanReward: 14.1875 meanLoss: 1832.7048 ExploreP: 0.0115\n",
      "Episode: 626 meanReward: 14.3125 meanLoss: 1824.3390 ExploreP: 0.0115\n",
      "Episode: 627 meanReward: 14.4062 meanLoss: 1822.7554 ExploreP: 0.0115\n",
      "Episode: 628 meanReward: 14.6250 meanLoss: 1796.5154 ExploreP: 0.0115\n",
      "Episode: 629 meanReward: 14.7812 meanLoss: 1845.6986 ExploreP: 0.0114\n",
      "Episode: 630 meanReward: 15.0312 meanLoss: 1862.7001 ExploreP: 0.0114\n",
      "Episode: 631 meanReward: 15.0938 meanLoss: 1930.0651 ExploreP: 0.0114\n",
      "Episode: 632 meanReward: 15.2500 meanLoss: 1907.1497 ExploreP: 0.0114\n",
      "Episode: 633 meanReward: 15.3438 meanLoss: 1948.3856 ExploreP: 0.0114\n",
      "Episode: 634 meanReward: 15.5000 meanLoss: 2002.5851 ExploreP: 0.0114\n",
      "Episode: 635 meanReward: 15.5625 meanLoss: 2053.6672 ExploreP: 0.0114\n",
      "Episode: 636 meanReward: 15.5625 meanLoss: 2060.4602 ExploreP: 0.0114\n",
      "Episode: 637 meanReward: 15.6250 meanLoss: 1965.5598 ExploreP: 0.0114\n",
      "Episode: 638 meanReward: 15.7812 meanLoss: 1985.8824 ExploreP: 0.0114\n",
      "Episode: 639 meanReward: 16.7812 meanLoss: 1358.3562 ExploreP: 0.0114\n",
      "Episode: 640 meanReward: 25.4062 meanLoss: 19.0733 ExploreP: 0.0114\n",
      "Episode: 641 meanReward: 30.4688 meanLoss: 8.0827 ExploreP: 0.0114\n",
      "Episode: 642 meanReward: 45.6562 meanLoss: 2.5721 ExploreP: 0.0113\n",
      "Episode: 643 meanReward: 47.7500 meanLoss: 113.6227 ExploreP: 0.0113\n",
      "Episode: 644 meanReward: 48.4062 meanLoss: 54.4541 ExploreP: 0.0113\n",
      "Episode: 645 meanReward: 53.2500 meanLoss: 269.4841 ExploreP: 0.0113\n",
      "Episode: 646 meanReward: 64.8125 meanLoss: 6.2663 ExploreP: 0.0112\n",
      "Episode: 647 meanReward: 69.3125 meanLoss: 21.0122 ExploreP: 0.0112\n",
      "Episode: 648 meanReward: 76.7500 meanLoss: 4.9638 ExploreP: 0.0112\n",
      "Episode: 649 meanReward: 84.4688 meanLoss: 3.9036 ExploreP: 0.0111\n",
      "Episode: 650 meanReward: 94.2500 meanLoss: 1.7194 ExploreP: 0.0111\n",
      "Episode: 651 meanReward: 104.0625 meanLoss: 2.8477 ExploreP: 0.0111\n",
      "Episode: 652 meanReward: 114.3125 meanLoss: 4.2178 ExploreP: 0.0110\n",
      "Episode: 653 meanReward: 123.9375 meanLoss: 3.8951 ExploreP: 0.0110\n",
      "Episode: 654 meanReward: 135.6875 meanLoss: 2.3116 ExploreP: 0.0109\n",
      "Episode: 655 meanReward: 150.6875 meanLoss: 1.3502 ExploreP: 0.0109\n",
      "Episode: 656 meanReward: 159.6875 meanLoss: 10.6509 ExploreP: 0.0109\n",
      "Episode: 657 meanReward: 159.4062 meanLoss: 117.9130 ExploreP: 0.0109\n",
      "Episode: 658 meanReward: 159.1875 meanLoss: 491.6247 ExploreP: 0.0109\n",
      "Episode: 659 meanReward: 159.0000 meanLoss: 920.3625 ExploreP: 0.0109\n",
      "Episode: 660 meanReward: 158.6875 meanLoss: 1166.8724 ExploreP: 0.0109\n",
      "Episode: 661 meanReward: 158.5312 meanLoss: 1135.3168 ExploreP: 0.0109\n",
      "Episode: 662 meanReward: 158.1562 meanLoss: 1015.5047 ExploreP: 0.0109\n",
      "Episode: 663 meanReward: 157.9688 meanLoss: 1159.1624 ExploreP: 0.0109\n",
      "Episode: 664 meanReward: 157.6875 meanLoss: 1150.1733 ExploreP: 0.0109\n",
      "Episode: 665 meanReward: 157.5625 meanLoss: 1176.7792 ExploreP: 0.0109\n",
      "Episode: 666 meanReward: 157.2812 meanLoss: 1283.8375 ExploreP: 0.0109\n",
      "Episode: 667 meanReward: 157.1875 meanLoss: 1371.3119 ExploreP: 0.0109\n",
      "Episode: 668 meanReward: 157.0312 meanLoss: 1381.5011 ExploreP: 0.0109\n",
      "Episode: 669 meanReward: 156.8750 meanLoss: 1159.7937 ExploreP: 0.0109\n",
      "Episode: 670 meanReward: 156.6250 meanLoss: 1200.0840 ExploreP: 0.0109\n",
      "Episode: 671 meanReward: 155.5000 meanLoss: 1126.4368 ExploreP: 0.0109\n",
      "Episode: 672 meanReward: 146.7500 meanLoss: 1276.1603 ExploreP: 0.0109\n",
      "Episode: 673 meanReward: 141.6250 meanLoss: 1325.3881 ExploreP: 0.0109\n",
      "Episode: 674 meanReward: 126.3125 meanLoss: 1156.5557 ExploreP: 0.0109\n",
      "Episode: 675 meanReward: 124.1250 meanLoss: 1281.2355 ExploreP: 0.0109\n",
      "Episode: 676 meanReward: 123.3438 meanLoss: 1292.1454 ExploreP: 0.0109\n",
      "Episode: 677 meanReward: 118.5312 meanLoss: 1375.1431 ExploreP: 0.0109\n",
      "Episode: 678 meanReward: 106.7812 meanLoss: 1307.7639 ExploreP: 0.0109\n",
      "Episode: 679 meanReward: 102.2812 meanLoss: 1279.9857 ExploreP: 0.0109\n",
      "Episode: 680 meanReward: 94.8750 meanLoss: 1416.3169 ExploreP: 0.0109\n",
      "Episode: 681 meanReward: 86.9375 meanLoss: 1494.3265 ExploreP: 0.0109\n",
      "Episode: 682 meanReward: 77.1250 meanLoss: 1426.0380 ExploreP: 0.0109\n",
      "Episode: 683 meanReward: 67.1875 meanLoss: 1331.4926 ExploreP: 0.0109\n",
      "Episode: 684 meanReward: 56.7812 meanLoss: 1354.9122 ExploreP: 0.0109\n",
      "Episode: 685 meanReward: 47.0000 meanLoss: 1269.3289 ExploreP: 0.0108\n",
      "Episode: 686 meanReward: 35.1250 meanLoss: 1289.3439 ExploreP: 0.0108\n",
      "Episode: 687 meanReward: 19.7812 meanLoss: 1185.0889 ExploreP: 0.0108\n",
      "Episode: 688 meanReward: 10.6562 meanLoss: 1174.6569 ExploreP: 0.0108\n",
      "Episode: 689 meanReward: 10.6875 meanLoss: 1181.5256 ExploreP: 0.0108\n",
      "Episode: 690 meanReward: 10.7188 meanLoss: 1113.6793 ExploreP: 0.0108\n",
      "Episode: 691 meanReward: 10.6875 meanLoss: 1081.2554 ExploreP: 0.0108\n",
      "Episode: 692 meanReward: 10.6875 meanLoss: 996.6639 ExploreP: 0.0108\n",
      "Episode: 693 meanReward: 10.6562 meanLoss: 1147.2823 ExploreP: 0.0108\n",
      "Episode: 694 meanReward: 10.7188 meanLoss: 1143.0758 ExploreP: 0.0108\n",
      "Episode: 695 meanReward: 10.7812 meanLoss: 1220.7899 ExploreP: 0.0108\n",
      "Episode: 696 meanReward: 10.7812 meanLoss: 1267.4305 ExploreP: 0.0108\n",
      "Episode: 697 meanReward: 10.8125 meanLoss: 1331.7361 ExploreP: 0.0108\n",
      "Episode: 698 meanReward: 10.8125 meanLoss: 1272.4427 ExploreP: 0.0108\n",
      "Episode: 699 meanReward: 10.8438 meanLoss: 1260.7303 ExploreP: 0.0108\n",
      "Episode: 700 meanReward: 10.8438 meanLoss: 1149.1569 ExploreP: 0.0108\n",
      "Episode: 701 meanReward: 10.8750 meanLoss: 1168.5986 ExploreP: 0.0108\n",
      "Episode: 702 meanReward: 10.8125 meanLoss: 1116.5304 ExploreP: 0.0108\n",
      "Episode: 703 meanReward: 10.8125 meanLoss: 1093.9309 ExploreP: 0.0108\n",
      "Episode: 704 meanReward: 10.7500 meanLoss: 1245.4532 ExploreP: 0.0108\n",
      "Episode: 705 meanReward: 10.8750 meanLoss: 1178.1185 ExploreP: 0.0108\n",
      "Episode: 706 meanReward: 10.8750 meanLoss: 1256.4189 ExploreP: 0.0108\n",
      "Episode: 707 meanReward: 10.8750 meanLoss: 1270.0441 ExploreP: 0.0108\n",
      "Episode: 708 meanReward: 10.9062 meanLoss: 1234.0178 ExploreP: 0.0108\n",
      "Episode: 709 meanReward: 10.9375 meanLoss: 1324.5330 ExploreP: 0.0108\n",
      "Episode: 710 meanReward: 10.9375 meanLoss: 1283.3730 ExploreP: 0.0108\n",
      "Episode: 711 meanReward: 10.8438 meanLoss: 1284.0900 ExploreP: 0.0108\n",
      "Episode: 712 meanReward: 10.7500 meanLoss: 1216.3214 ExploreP: 0.0108\n",
      "Episode: 713 meanReward: 10.7500 meanLoss: 927.1711 ExploreP: 0.0108\n",
      "Episode: 714 meanReward: 10.7188 meanLoss: 912.0208 ExploreP: 0.0108\n",
      "Episode: 715 meanReward: 10.7188 meanLoss: 793.6478 ExploreP: 0.0108\n",
      "Episode: 716 meanReward: 10.5938 meanLoss: 1064.8311 ExploreP: 0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 717 meanReward: 10.5938 meanLoss: 1222.9071 ExploreP: 0.0108\n",
      "Episode: 718 meanReward: 10.5938 meanLoss: 1315.0004 ExploreP: 0.0108\n",
      "Episode: 719 meanReward: 10.6250 meanLoss: 1295.7953 ExploreP: 0.0108\n",
      "Episode: 720 meanReward: 10.6250 meanLoss: 1252.3107 ExploreP: 0.0108\n",
      "Episode: 721 meanReward: 10.5938 meanLoss: 1228.5435 ExploreP: 0.0108\n",
      "Episode: 722 meanReward: 10.5938 meanLoss: 1172.8031 ExploreP: 0.0108\n",
      "Episode: 723 meanReward: 10.6562 meanLoss: 1262.5013 ExploreP: 0.0108\n",
      "Episode: 724 meanReward: 10.6562 meanLoss: 1070.5933 ExploreP: 0.0108\n",
      "Episode: 725 meanReward: 10.6562 meanLoss: 1073.2683 ExploreP: 0.0108\n",
      "Episode: 726 meanReward: 10.5938 meanLoss: 967.4330 ExploreP: 0.0108\n",
      "Episode: 727 meanReward: 10.5312 meanLoss: 1123.0015 ExploreP: 0.0108\n",
      "Episode: 728 meanReward: 10.5000 meanLoss: 1002.4890 ExploreP: 0.0108\n",
      "Episode: 729 meanReward: 10.4375 meanLoss: 1072.6544 ExploreP: 0.0108\n",
      "Episode: 730 meanReward: 10.3750 meanLoss: 1013.4668 ExploreP: 0.0108\n",
      "Episode: 731 meanReward: 10.3438 meanLoss: 1140.6984 ExploreP: 0.0108\n",
      "Episode: 732 meanReward: 10.3750 meanLoss: 1494.5262 ExploreP: 0.0108\n",
      "Episode: 733 meanReward: 10.3438 meanLoss: 1489.8344 ExploreP: 0.0108\n",
      "Episode: 734 meanReward: 10.4062 meanLoss: 1500.0571 ExploreP: 0.0108\n",
      "Episode: 735 meanReward: 10.3750 meanLoss: 1288.5110 ExploreP: 0.0108\n",
      "Episode: 736 meanReward: 10.3750 meanLoss: 1153.1533 ExploreP: 0.0108\n",
      "Episode: 737 meanReward: 10.2500 meanLoss: 1224.9010 ExploreP: 0.0108\n",
      "Episode: 738 meanReward: 10.3438 meanLoss: 1053.4640 ExploreP: 0.0108\n",
      "Episode: 739 meanReward: 10.2500 meanLoss: 1105.9918 ExploreP: 0.0108\n",
      "Episode: 740 meanReward: 10.2500 meanLoss: 1106.4943 ExploreP: 0.0108\n",
      "Episode: 741 meanReward: 10.0938 meanLoss: 1095.4121 ExploreP: 0.0108\n",
      "Episode: 742 meanReward: 10.2500 meanLoss: 1218.6263 ExploreP: 0.0108\n",
      "Episode: 743 meanReward: 10.4375 meanLoss: 1738.8307 ExploreP: 0.0108\n",
      "Episode: 744 meanReward: 10.4688 meanLoss: 2223.9829 ExploreP: 0.0108\n",
      "Episode: 745 meanReward: 10.5000 meanLoss: 2002.2312 ExploreP: 0.0108\n",
      "Episode: 746 meanReward: 10.4688 meanLoss: 1606.3655 ExploreP: 0.0108\n",
      "Episode: 747 meanReward: 10.4062 meanLoss: 1266.9570 ExploreP: 0.0108\n",
      "Episode: 748 meanReward: 10.5312 meanLoss: 1041.6156 ExploreP: 0.0108\n",
      "Episode: 749 meanReward: 10.5625 meanLoss: 1009.8358 ExploreP: 0.0108\n",
      "Episode: 750 meanReward: 10.5625 meanLoss: 1100.7343 ExploreP: 0.0108\n",
      "Episode: 751 meanReward: 10.5000 meanLoss: 1012.2678 ExploreP: 0.0108\n",
      "Episode: 752 meanReward: 10.5312 meanLoss: 1023.5031 ExploreP: 0.0108\n",
      "Episode: 753 meanReward: 10.6250 meanLoss: 1100.0878 ExploreP: 0.0108\n",
      "Episode: 754 meanReward: 10.5938 meanLoss: 1104.0685 ExploreP: 0.0108\n",
      "Episode: 755 meanReward: 10.5625 meanLoss: 1063.2957 ExploreP: 0.0108\n",
      "Episode: 756 meanReward: 16.7188 meanLoss: 111.5382 ExploreP: 0.0108\n",
      "Episode: 757 meanReward: 32.0312 meanLoss: 4.4294 ExploreP: 0.0107\n",
      "Episode: 758 meanReward: 47.3438 meanLoss: 15.6577 ExploreP: 0.0107\n",
      "Episode: 759 meanReward: 47.5625 meanLoss: 292.5078 ExploreP: 0.0107\n",
      "Episode: 760 meanReward: 53.2188 meanLoss: 40.2794 ExploreP: 0.0107\n",
      "Episode: 761 meanReward: 68.5000 meanLoss: 5.2131 ExploreP: 0.0107\n",
      "Episode: 762 meanReward: 83.8750 meanLoss: 14.4827 ExploreP: 0.0106\n",
      "Episode: 763 meanReward: 99.1562 meanLoss: 20.0726 ExploreP: 0.0106\n",
      "Episode: 764 meanReward: 114.4375 meanLoss: 13.3294 ExploreP: 0.0106\n",
      "Episode: 765 meanReward: 129.4688 meanLoss: 19.4325 ExploreP: 0.0105\n",
      "Episode: 766 meanReward: 136.5000 meanLoss: 24.2694 ExploreP: 0.0105\n",
      "Episode: 767 meanReward: 151.8438 meanLoss: 6.7691 ExploreP: 0.0105\n",
      "Episode: 768 meanReward: 167.1875 meanLoss: 13.2829 ExploreP: 0.0105\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver() # save the trained model\n",
    "rewards_list, loss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_loss = deque(maxlen=batch_size)\n",
    "    episode_reward = deque(maxlen=batch_size)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            \n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            actions_logits = sess.run(model.actions_logits, \n",
    "                                      feed_dict = {model.states: next_states, \n",
    "                                                   model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (0.99 * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], \n",
    "                               feed_dict = {model.states: states, \n",
    "                                            model.actions: actions,\n",
    "                                            model.targetQs: targetQs,\n",
    "                                            model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'meanReward: {:.4f}'.format(np.mean(episode_reward)),\n",
    "              'meanLoss: {:.4f}'.format(np.mean(loss_batch)),\n",
    "              'ExploreP: {:.4f}'.format(explore_p))\n",
    "        rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        if(np.mean(episode_reward) >= 500):\n",
    "            break\n",
    "    \n",
    "    saver.save(sess, 'checkpoints/model3.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl83HWd+PHXe+6ZZDK50zRt2gLlLEKhAoIoomLFA3BRQUFcUbzY1V38reju/taLdWW9VlERhRUVUZdDURBlFX6AQGmBAoVSerdp0tzHTI45378/5iBtZ5JJmsn5fj4eeWTmO9/vzGcy7ec9n+v9EVXFGGOMGY9jpgtgjDFmbrCAYYwxpigWMIwxxhTFAoYxxpiiWMAwxhhTFAsYxhhjimIBwxhjTFEsYBhjjCmKBQxjjDFFcc10AaZSbW2tLl++fKaLYYwxc8ZTTz3Vpap1xZw7rwLG8uXL2bBhw0wXwxhj5gwR2V3sudYlZYwxpigWMIwxxhTFAoYxxpiiWMAwxhhTFAsYxhhjimIBwxhjTFEsYBhjjCmKBQxjzIIUj8fp7e0lkUjMdFHmjHm1cM8YY/IJh8OICGVlZYgIQ0NDtLa2kkwm6ezsJBQKUVVVhcfjmemizmoWMIwx85qq0tbWhqri8XgoKyujr68Pt9tNY2MjkUiE/v5++vr68Hq9VFRUEAqFcDqdM130WccChjFmXksmk6gqwWAw1w1VXl5OY2MjDoeDsrIyampqCIfDhMNhOjs7CYfDNDc3IyIzXfxZxQKGMWZey45RVFRUUF5eTiwWw+12HxAMXC4XVVVVVFVVEQ6HaW1tpaenh5qaGgBSqRRDQ0O5Lq2FygKGMWZeywYMlytd3Y03ThEMBgkGg3R3d1NeXo7D4WDfvn1Eo1E8Hg91dXWUl5eXvNyzkc2SMsbMa/F4HAC32130NQ0NDTidTlpbW9m9ezeJRIL6+noA9u3bR1tbW0nKOttZwDDGzGuJRAIRmdAgttPppKGhgVgshsvlorm5maqqKpYvX85z7TFufGATD23pIJ5MlbDks491SRlj5rV4PD6h1kVWeXk5y5Ytw+Px4HA4iCVSXHfvi9zxxFZqnSP8/MV1hMp8vO+0Zq4+9yh87vk/q8oChjFmXkskErnxi4ny+XwAdEeifOSnG3h6Tx9Xnr6cC1b62BsP8LtN3dzw4DZ+91wr1114Iq9dWTuVRZ91rEvKGDOvTbaFkbs+meLjtz3NC60D3PC+1fzT+SfgdTs5+8hqbrz8VH7xkdNxiHDZzeu4+dGdU1jy2ccChjFm3lLVw2phAHzpdy/y5M4evvY3r+Ltr1qcCz7Z2VdnHlnLHz51Nucd38BX79vMxr19U1L22ahkAUNEfCLypIg8KyIviMgXM8dXiMg6EdkqIr8Skbxz3ETkcyKyTUS2iMhbSlVOY8z8la3UJ9vC+OWTe/jZE7v5yNkruHB1EwAOhwOn05mbfQXgczv5z4tPoqHCx9/d/jQDI/FCTzmnlbKFEQXOVdWTgJOBtSJyBvA14FuquhLoBa48+EIROR64BDgBWAt8X0Tm/4iSMWZKHbwGYyIe2drJv/52E2evrOWza4894DGXy3VI0sJQwM13Ll1Na98In7vreVR18gWfpUoWMDQtkrnrzvwocC5wR+b4rcCFeS6/APilqkZVdSewDTitVGU1xsxPk1mDAbB+Vw8f+ekGjqwr54ZLT8HlPLCqdLlcB7Qwsk5dVsU15x3Nvc+1cctfd0263LNVSccwRMQpIhuBDuABYDvQp6rZ0NwCNOW5tAnYO+p+ofOMMaagybQwnm/p50P/vZ7FIT8/u/J0QoFDg43b7S6YFv1jrzuSt5zQwHX3vsiDWzomV/BZqqQBQ1WTqnoysIR0C+G4fKflOZYvWUve9p2IXCUiG0RkQ2dn5+QLa4yZd+LxOE6nE4ejuKqufzjOlbeup8Lv5ucfPp26oDfveS6Xi2QySSp16MI9h0P41ntP5thFFfzdL55hy/7wYb2H2WRaZkmpah/wEHAGUCki2XC/BGjNc0kLsHTU/ULnoao3qeoaVV1TV1c3dYU2xsx5E50h9Z9/fImuSJQbLzuVxZX+gucdPFPqYAGPix9fsQa/x8mHfrKe9oGRiRV8lirlLKk6EanM3PYDbwI2Aw8CF2dOuwL4bZ7L7wEuERGviKwAVgJPlqqsxpjxzcVB3ImswXhqdy+3rdvDB89cwYlLQmOemw1C+cYxshZX+rn5ijX0DsX4wM1P0jcUK77gs1QpWxiNwIMi8hywHnhAVX8PfBb4RxHZBtQANwOIyDtF5EsAqvoC8GvgReB+4JOqmixhWY0xY1BVduzYwf79+2e6KBNSbAsjnkzx+buep7HCxzXnHT3u+eO1MLJetaSSH31gDTu7Bvnbn6xnMDq3t4MtWWoQVX0OWJ3n+A7yzHhS1XtItyyy968DritV+YwxxYvFYiQSCfr7+/H5fFRWVs50kfJSVZLJJC6Xi1Qqlbs9nh89soMt7WF+9IE1lHnHPz/7nMXsB37WUbV859LVfOK2p7jy1vX86ANrCPomv/J8JtlKb2PMuIaHhwHw+/10dHTk7heSSCRmpAurp6eHHTt25AIcjD+ldlfXIP/1v1tZe8Ii3nx8Q1GvIyIFp9bms3bVIr7xnpNYv6uXS256gs5wtKjrZhsLGMaYcQ0PD+NyuWhqasLtdtPa2lrw27Wqsnv3bvbs2ZN3FlGpqCp9fX2oKp2dnbnKfKwWhqry+bufx+Ny8MULTpjQ6+VbvDeWi1Yv4ccfWMOOzkEuvvEx9nQPTej1ZgMLGMaYcQ0PD+P3+3E6nSxevJhEIkFfX/6cSZFIhEQiwcjICG1tbdPW0hgcHCSRSBAIBIhEIvT39wNjtzD+56kWHtvezbVvPZaGCt+EXs/tdhfdwsh6w7H1/OIjp9M/HOcDt6yjZ3BuDYRbwDDGjCmRSBCPx/H709NMvV4v5eXl9Pf35w0G/f39uN1uGhoaiEQidHRMz+K1/v7+XCvI4/EQDqfXPxRqYXSGo1x372ZevbyKS1/dPOHXm2gLI2t1cxU3X/Fq2vpH+PCt6xmJz535PBYwjDFjGj1+kVVZWUkikSASiRxwbiKRYHBwkIqKCiorK6murqavry/3bb9Usq8bCoVwOBxk12S5XC5E8q0Dhq/d/xJDsQRffdeJOBz5zxmL2+3ODaxP1KnLqvj2e0/mmb19/MOvNpJKzY0pyxYwjDFjGh4exuFw4PW+suo5EAjgdrsP6ZbKBoaKigoAamtrCQQCdHZ2TurbeLGyrZ3s65aXlxMMBnMbIB3smT293PFUC1e+9giOqg9O6jUnMlPqYKlUirWrFvHP5x/HHzbt55a/zo19NGzHPWPMmIaHh/H5fAd8UxcRKisr6ezsJBqN5oLJwMAAgUCA4YTw5O4uNrX2s6W1l+RAOwnHbmKeivT+2iI4HCCZLECnrajm/ac3H5LkrxiqSn9/P4FAAI/nld0SGhsb87YuUinlC/e8QH3Qy9XnHjXh18savXhvdDAdTzQaZe/evVRWVnLla1fwxI5uvv6nLZx3/CKaawKTLs90sIBhjCkolUoRjUaprq4+5LFQKERXVxd9fX00NDTw9PZ2/rBuB091KM+0P0d2eGNxyEedWwhoP4MuiImblCrZXphYIsW9z7dx+5N7uO6iVZy67NDXGsvw8DDxeJza2gO3Ry3UFXXH0y0829LPt957EuVFrLkopNjFe6Mlk0laW1tJJpP09/dTU1PDly9cxZu/+TDX3vUct3349ILlng0sYBizAPT399Pb20soFMr18yeTSWKx2CGth9FGRkZQ1QPGL7KcTicVFRU8vbWF3921gef29uF0CA1LlvHpNy5jzfIqTlhcQWXAQyqVYvfu3QAsW7bsgGSAqsofX2jni797gb/5weNc8+ajufrco4quOIeGhhARysrKxj23OxLl+vu3cEpzJReefHgJsJ1OJyJS9EwpVWX//v3E43Gqqqro7e1laGiIxlAZ1771WP7lN5v49Ya9vHeCA/D9/f1Eo1Hq6upKHmwsYBizAAwMDBCLxejo6KC7uxuXy0U0ml48FggEWLJkSd7KJjvgXWgs4I/bInzz3p1U+Fy8+8xjePcZR7K4tuqQ8xwOBw0NDezdu5fOzk4aGl5ZICcirF21iLNX1vIvv9nENx54mUg0wbVvPbaoCnB4eBiv14vTOfYea8OxJFfeuoHwSJwvXfDqw65cs4v3im1h9Pb2EolEqK+vp7KykoGBAQYGBigrK+N9pzVzz7OtfOXezbx2ZR1NYyQ+PFg4HCYej1NfXz/Zt1I0G/Q2ZgGIRqOEQiGam5sJBAK4XC5qa2upq6tjaGgob46oVCpFOBwuWBnv7RniP/64jZNWNvOHa9/Op96+Jm+wyAoEArlZUwfPrgIo87r4xrtP4vIzlvHDh3fwr7/dNO7sIVXNrREZSyKZ4u9uf5rnWvr4zqWrWdU0dnLBYnk8nlzgHfP1Ewm6u7sJBoNUVVUhIgSDQSKRCKlUCodDuP5vXkUqpXzq9mdIJItb8KiqjIyMjPv+p4oFDGPmuVgsRjKZxOfz4ff7Wbx4MUuWLKGmpobq6mpqa2sZGBigq6srd00qlaKlpYVYLHbI2ACkK6p//s0mHAJffdeJRY8F1NbW4vP5cl0zB3M4hC9dcAIfff0R/PyJPVx9+9NjrlMYHh5GVQkECg8Wqyr/+tsX+N/NHXzxglW85YRFRZW1GD6fj1gsNu6K9p6eHlT1gL9lRUVFLigDLK8t47qLTmTD7l6+8+etRb1+9rMd6/1PJQsYxsxz2W/AhWby1NTUEAqF6O7uZufOnXR3d7Nv3z5GRkZobGykvLz8kGvufmYfD7/cyT+tPXZC3SciQmNjI6pKS0sLnZ2duW/Zo8+5du2x/MvbjuO+5/dz+c3rCqYGz7dG5GDff2g7tz+5h0+ccySXn7Gs6LIWw+/3577lF5JdFV9RUXHALC6/34/H42FgYCB37MLVTVx86hK+++A2Htvele/pDlDM+59KFjCMmedGRkYQkTGnfjY0NNDQ0IDL5aKrq4uhoSEWLVpEMHjoGoWewRhf/v2LnNJcyWWTqIA9Hg+LFy/G6XTS29vLvn37aG9vP+AcEeHDZx/Bdy9dzbN7+3nLtx/m2juf4zfP7KMr8koX0NDQED6fr+D4xW837uM//7iFC05ezP95yzETLut4smM7YwWM7u5uIB2YDxYMBhkaGjqgtfXFd57AitoyPv3LjeMmKczm+JronuWTZYPexsxzIyMjeL3eMQd5s+sqsiu4E4lEwYHuG/6yjf7hOF9916twTmKFNEBZWRllZWWoKq2trQWz377jpMU0hnz88OEd3Pd8G79cvxenQzjrqFouOKmR5e4Bli56ZadNVaVvKM7uniFeaO3ni/e8yBlHVHP9xa8qyQwip9OJx+MpWP54PE5/fz+hUChvpR4MBunu7mZoaIhQKD2uUuZ18f33n8KF3/srf3/7M/z8w6cX/DsPDQ1NW+sCLGAYM69lu0uyK6CL4XK5CuZf2tszxM+f2M27T13KMYsmt0J6NBHB7/cTiURIJpN5WwprllezZnk1yZTy+KYdPLq1nd+9HObz/9NGvSOCN1TLEY01dEai7OgcpH/4lW/rxzQE+eFla/C6xp5BdTh8Ph9DQ4dmnlXVXMspX+sC0q0tETlk4PzYRRV85cIT+cz/PMu3HniZz+RpHcXjcRKJhAUMY8zUiMfjpFKpgq2FifrWAy8jAp9+88opeT54ZWwlGo2OOXjrEGjwp7hoVQ3vP62MHb1Rnt3WwubBAFs6Iiyq8PH2VzWyvKaM5bVlLKsJsKK2DPckVo9PhN/vZ2Bg4JDtYLu6uhgcHGTRokUFA3C2qzDfTKuLT13Chl093PDgNmrKPbx7zdIDJhcMDw/T2jfMb7e10BrexbcvOWS/uilXsoAhIkuBnwKLgBRwk6r+l4j8CsiGy0qgT1VPznP9LiAMJIGEqq4pVVmNma+yfetTETBebB3g7o37+OjrjqQxNHXfakePA4wVMEZGRkilUgSDQcLhMI0+Yfmpy/n7ZVM7kD1Ro8ufDRjhcJienh4qKytzXU2FeL3evNOMAb7wzhPY3hnhi797kevv38J5JzRQ7nXRNxxn//529nf10qYVnL2ynlgihcdV2uBYyhZGArhGVZ8WkSDwlIg8oKrvzZ4gIt8Axkpj+QZVHX+qgDEmr+yA9+jZOZN1/R9fosLn5uOvP3IKSvYKp9OJ2+0ec+AYyHX7NDQ05Hb+m67ppGPxer04HA6Gh4cJBoPEYjH279+P3+8vajGd1+ulv78/7/7jPreTX3/0NTy9p487ntrL/Zv24xAhFHCz3JPiTWcdxd+87lXUB6emBTmeUu7p3Qa0ZW6HRWQz0AS8CCDpEaj3AOeWqgzGLHQjIyNjpv4o1nMtfTy0pZPPrj2WUGDqZ+T4fL5xF8CNnhFVVVWFz+ebUNK/Usl2K2XTqLS2tiIiLF68uKi/++guuXxdVyLCqcuqOHVZFV98e3r1u4iwfft2amtrqZmmYAHTNIYhIsuB1cC6UYfPBtpVtdAKFQX+JCIK/FBVbyppIY2ZZ1Q1t8L7cP3w/+0g6HNx2RkT32ioGF6vl3A4XHDgO5VKMTw8TFXVKyvJp3Owdzx+v5/e3l46OjqIRqM0NTWNuTXsaKMDRqF8WLFYLLdm5eDXnU4lDxgiUg7cCXxaVQdGPXQpcPsYl56lqq0iUg88ICIvqerDeZ7/KuAqgObm0vxjNmYuyq5APtzxi11dg/xhUxsfff2RBH2lme+fLWOhge/st/fZ0AWVj8/ny+0pXlVVlXexYyHZLrnRLazBwcHcCvBUKkUkEkFEqK2txeVy5dZtzKuAISJu0sHiNlW9a9RxF/Au4NRC16pqa+Z3h4jcDZwGHBIwMi2PmwDWrFkzN7atMmYajLfCu1g3PbIDl9PB3561fApKld94A9/ZjLSzqVUx2ujta7O7/U3EwTOlOjo6SCQSudZWRUVFLljMpFLOkhLgZmCzqn7zoIffBLykqi0Fri0DHJmxjzLgPOBLpSqrMfNRduvQw6lkOsNR7niqhb85ZUlJB1bzfcseLTt+MTot+mzicrlobGzE7/dParzI6/UyODiYWzcTi8VobGyc0PqZ6VDKv/5ZwOXAuSKyMfNzfuaxSzioO0pEFovIfZm7DcCjIvIs8CRwr6reX8KyGjPvJBKJ9O5246T9HstPHttJPJniI2evmMKS5ZcdOIb0GoN9+/YxODhIKpUad8rtbFBRUTHpFB1erzc35tTf34/D4ZhQt9Z0KeUsqUeBvKFWVT+Y51grcH7m9g7gpFKVzZiFIN80zYnoGYxx62O7eeuqRRxRV/rKy+fzEYlECIfD7N+/H1UlEonkKtPZHjAOR7bbcGRkhHA4TDAYnJWtqdlXImPMlCg046hYP3x4O4OxBP/wpqOnsFSFZccxWltbcbvdrFixgrq6OuLxOA6HY8pWq89GbrcbEaG7u5tUKjUlM9tKwVKDGDNPJRKJSXeRdIRHuPWxXVx4chMrGw4/Z1QxsgkSs3t2OJ1OqqurCYVCJJPJWfmNe6qMXsvh8Xhm7eC+BQxj5qnDSUz3/Qe3E08qn3rj1OWMGo/L5WLFihW4XK4DBo6dTudhtZTmimzAmG0D3aPN35BtzDTIzpEfb8e16aaqk+6Sau0b5hfr9vDuU5ewvDb/QrJSyXbNLER+vx+HwzFru6PAWhjGTMrw8DC9vb1EIhFUlfLycpqamma6WDmHM6X2+vtfAuDqc4+a0jKZsVVUVFBeXj6rW1MWMIyZIFVl3759AFRWViIi9PT00NfXR2Vl5QyXLi2RSAATDxiPbO3kNxtb+fs3rmRJ1fydlTQbHe4U6OlgAcOYCRoeHiaZTNLU1JSbKx+NRuno6MDv98+KhHjZFsZEKqCReJJ/vnsTR9SW8YlzpjYjrZkfbAzDmAkKh8M4HI4D1gUsWrQIp9NJa2vrrBjPmEwL4zt/3sqeniG+ctEqfO7Z/U3XzAwLGMZMQHYxWVlZ2QHTPLOpIeLxOG1tbajObFqzbMAotoXx0v4Bbnp4BxefuoQzj6wtZdHMHGYBw5gJGBkZIZFI5E3bEAgEqK+vJxKJ0NHRMQOle0V2hlQxaxcSyRSfveM5Qn43nz//uGkonZmrbAzDmAkIh8OISME8P5WVlcTjcXp6enC73VRXV09zCdNGZzodz3//dRfPtvTz3UtXU112+DvzmfnLAoYxE5CvO+pgtbW1xONxOjs7cbvdBIPTs1J6tGLzSO3qGuTrf9rCm49v4O2vapyGkpm5zAKGMUUaGRkhHo9TU1Mz5nkiQmNjI4lEgra2NlwuV8EV16rKlvYwj27tYltHhO2dEeqCXv7xzUdzVP3kA00ymRx3tlYypXz2zufwuBx85cJVC3bBnCmeBQxjipTd9ayYtNMiQlNTE3v27GHfvn00Nzfj8bzS3TMUS3DLI9v57bP72dqR3nazuszDkXVlPPJyF398oZ33vnop17z5aGrKJz5NN5FIFNzuM+ubD2xh3c4e/vPiV9FQMX8T+5mpYwHDmCKFw2H8fn/RYwNOp5MlS5awe/fuXNBwOp1s2R/mH3/+VwZ6ulnS1MQHLjiB805YlKu0uyNRvvuXbfz8id08tq2L2z5yBk2VxeeESqVSpFKpMbuk/vB8G997cDuXnraUd69ZWvRzm4XNZkkZU4RoNEosFpvweITb7aapqSk33fb2dbu5+IYHYXiAz5x3NNe/80guf83yA77h15R7+cI7T+BXH30N3YMx3nPj4+zsGiz6NcdbtLdlf5hr/udZVjdX8oV3njCh92MWtpIFDBFZKiIPishmEXlBRD6VOf4FEdmXZxe+g69fKyJbRGSbiFxbqnIaU4xIJN1tNJld0Px+P2Whar59//N87Tfrec0iJ19510mctWoFQ0NDBbclPXVZFbd/5AyG40ne88Pig8ZYi/bCI3E+9vOnKPO6uPGyU/G6bIGeKV4pWxgJ4BpVPQ44A/ikiByfeexbqnpy5ue+gy8UESfwPeCtwPHApaOuNWbaZbujJpPM74XWfi7/2SYe2hHh8tU1XHPeSk5YuYKqqiocDge9vb25c/v6+ujv78/dX9UU4ldXnUEypXz41vUMjMTHfb1CAUNV+dxdz7O7e5AbLl1t4xZmwkoWMFS1TVWfztwOA5uBYtN5ngZsU9UdqhoDfglcUJqSGjO2WCxGNBqdcHdUKqX8+JEdXPS9xxiMJfjuh87h/a87jiVNTXg8HpxOJxUVFQwMDJBIJOju7qa9vZ3Ozs4DVoqvbAjy/fefwu7uIT79y40kU2OvIi/UJXXbuj38/rk2rjnvGE4/YuyZXsbkMy1jGCKyHFgNrMsculpEnhORW0SkKs8lTcDeUfdbKD7YGDOlJtMd1REe4Yr/fpKv3LuZ1x9Tx31/fzavOaqORYsWHTB7qaqqKpf9tqurC4/HQzKZJBaLHfB8ZxxRw7+943j+8lIHX//TljFfO5FIHJL5dNO+fr70+xd5/dF1fPz1lljQTE7JA4aIlAN3Ap9W1QHgB8CRwMlAG/CNfJflOZb3a5WIXCUiG0RkQ2dn5xSV2phXRCIRfD5f0dudbusIc9H3HmP9rh7+/aITuenyUwtOjfV4PJSVlTEyMkIwGGTJkiUADA0NHXLuZWcs49LTmvnBQ9v52v0vkSrQ0siu8s6uq9jfP8KHb91AdcDDN99zEg6HrbcwkzNuwBCRd4lIMHP7WhH5tYicXMyTi4ibdLC4TVXvAlDVdlVNqmoK+BHp7qeDtQCj5/otAVrzvYaq3qSqa1R1TV1dXTHFMqZoiUSC4eHholsXT+3u4eIbHyeaSHHHx87kfac3j7sgrr6+ntraWhobG3G73bjd7rwBQ0T48gUn8L7T00HjU7/aSDSRPOS80TvthUfi/O1P1hOJJrjlg6+e1JoOY7KKaWF8QVXDInIm8A7gV8CN410k6f8lNwObVfWbo46Pzj9wEbApz+XrgZUiskJEPMAlwD1FlNWYKRUOhwGKGr94ZGsn7/vROqoCHu76+Jmsaipuq02Px0NNTU0usAQCAYaHh/NmvHU5HVx34So+u/ZYfvdsK+/54RNs2td/wDnZtCDxZIpP/uIZXm4P8733n8Lxi2fvXtFmbigmYGS/wrwd+L6q3gkU8zXlLOBy4NyDptBeLyLPi8hzwBuAfwAQkcUich+AqiaAq4E/kh4s/7WqvjCRN2bMVBgYGMDn8x2wSjufdTu6+chPN3BEXTl3fOw1NNdMfre6QCCQdxwjS0T4+DlH8v33n0JLzxDvuOFRPnfX83SER4B0C2MgmuSyH6/j4Zc7+feLVvH6o631bQ5fMXME20Tke8BaYE3mG/+4gUZVHyX/WMQh02gz57cC54+6f1+hc42ZDrFYjJGREcbr6nxmTy8f+sl6mir9/OzK0w672yebd2poaGjMfFDnn9jIWUfV8p0/b+XWx3Zxx1N7WbuqkdUVw/z8mS5ao26+9d6TuGj1ksMqjzFZxQSM95CuyL+rqr0ishiwhXRm3iumO2pn1yAf/O/11JR7ue3DZ1A7BWMEo8cxqqryTSJ8Rcjv5l/ffjyXnbGMnz2+m3ue2sHGWD+BUBV3X3kWxzVaN5SZOgUDhoiM/pd2/6hjEeCvJS6XMTMuu1iv0OyoSDTBVT/dgEPgtg+fzqLQ1C2ECwQCRCIRVLWoLLJNFW6uPCXEhUccwY7uKGevPoaa4OS7xYzJZ6wWxgukp7IKsBgIZ26XA/uA5pKXzpgZEo1GiUajNDQ05H1cVfnMr59lR9cgP/vQaSytntrK2e/309/fTywWGz9NeTLJ3r17SaVSNC1q4MTjqy1VuSmJgmMRqrpUVZuB3wEXqWqlqoaAC0nPlDJm3hoYGEBECnZHfe/Bbdz/wn4+99ZjOfOoqd8DOxBIB6DBwfHzR3V3d5NIJFi6dOkBs62MmWrFzJI6TVVzU1pV9XekZzcZMy+pKuFwmEAgkDfj6+Pbu/nmAy9zwcmLufK1K0pSBrfbjd/vp6uri4GBgYLnxWIx+vr6CIVC+HyWG8qUVjEBoyezYG+JiDSJyGec5BxYAAAgAElEQVSB3nGvMmaOikQixONxKioOHTDujkT51C+fYXltGf9+0Ykl/Tbf1NSE3++nra3tgASFo3V0dOBwOKitnfpWjjEHKyZgvI/0qus/ZH6WApeWslDGzJRUKkVnZyder/eQ7qhUSrnmf56lbzjODZeeQpm3tPuPZTdgKi8vp6OjI5fTKisSiTA4OEhNTc2ksugaM1Fj/ivLpBn/jKp+cprKY8yM6u3tJR6Ps3Tp0kNaDz96ZAcPbenkyxecMG2rpkWExYsXs3PnTnp7ew9IUZJNVlhZWTktZTFmzBaGqibJn+vJmKKpasFVyzMhGo3S19d3yPF4PE5PTw/BYDA36Jz12PYuvnb/S5x/4iIuO2PZdBUVSAeNUCjE0NBQ7u84ODhINBqlutpmRJnpU0w79mkRuQv4HyA3ZWP0QLgxY+nq6qKnpwe/3091dTUej4dwOEw4HEZVcblcuN1uampqis4IOxmqSk9PD93d3bn1DaHQK/mesvtQHLyyu7VvmL/7xTMcUVfO9RefNCMVdCgUoru7m76+Purr6+nt7cXlcuUdZzGmVIoJGA2kA8XorVQVSwZoihCNRunt7SUQCBCPx9m3b1/usewOdolEgoGBARwOB/X19VNehlQqxeDgIL29vQwPDxMMBkkkEnR2dlJeXo7T6aS/v59wOExtbe0BQSuaSPLx254mmkhx42WnUl7icYtCXC4XwWCQ/v5+gsEgg4OD1NXVWevCTKtx//Wr6uXTUZC5JpVK0dLSQn19vU1nHKW9vR23251LadHe3o7D4WDx4sU4HA4ikQiJRIJgMHjAQG1raysDAwNTWgkmk0k6OzsJh8OkUilcLheNjY1UVFQQjUbZvXs3nZ2dhEIh2tvbKSsro7q6Ond972CMj9/2FM/u7ePGy07hqPqJ7+c9lSorKxkYGKC1tRWHw3FA68iY6TBuwBARL/BB4AQgVzOq6lWlK9bsl90nIRwOW8DIiEQiubGBoaEh/H4/w8PDNDY25tYzFFoIFwqFCIfDRCKRCW+FWqgs7e3tJJNJKioqqKiowO/354KR1+ulurqa7u5uIpFILphkH9/WEeHDt66ntW+Eb77nJNauahzr5aaF3+/H6/Xmxi7yrRExppSKaV//FNhBOr35daSn2S74VOPZfZOj0egMl2T26O7uxu12U11dTWdnJ4ODgwQCgaL62QOBAC6Xi/auHnb1p0ikUqQUAh4ny2vK8HuKrxw7Ozvp6enB6/WyZMmSgqk1qqurGRgYIJlM0tTUhNPpJJlSfrFuN9ffvwWv28HtV53Oqcuq814/E6qrq2lvbx83KaExpVBMwDhaVd8rIm9T1ZtF5Kek96lY0FKpFAAjIyMzXJLZYXBwkJGRERYtWkQoFCIQCNDT00NNTc2417YPjPCDh7bz7La9dHd3sy9ZQfKgCXyLQz6WVgdYXOlnUcjHm49v4JTmQyvNcDhMT08PoVCIhoaGMbu3HA4HS5cuJZVK4fV62bCrh//72xd4sW2AM4+s4fqLX8WSqtmVwK+iooJgMGhjF2ZGFBMw4pnffSJyHNAOTO+8wlkoGzCyG92Mt8HOfJdtXWRbEx6Ph0WLFo15TSKZ4tbHd/OtB14mlkhxxvIKXtPk5ehliwlVVSFAeCTBzq5BdnRG2Nc3zPpdPezvTweYk5dW8sEzl3PikhBLqvw4NEV7ezs+n2/cYJHldrtpHxjhP36zkbuf2UdjyMf33ncK55+4aNZWyrO1XGb+KyZg3CwiVcC/kW5ZBID/W9JSzQHZgAHpVsZCDBjJZBJVZXh4mOHh4aIraVXloZc7+Y/7XmJLe5hzjqnji+88gWU1ZezZs4dkMsmKFYVnSw1GE9z5dAv//dddfPpXGwEQUY6viLO0wsPiJUtZvGcnZV4XAY+TMo+LgNeJz+0kkVRG4kkGRuLs7h5iV9cg9z7fRiKpfPINR/KJc44q+QpuY+aqYmZJ/TBz80EmkNJcRJaSHv9YBKSAm1T1v0TkP0nvDR4DtgN/q6qHrKISkV2kU6ongYSqrin2tadDNmCIyIIcx+jr66O9vT133+VyFTVrZ+PePq6//yUe297NspoAN152Km854ZVAEwqF2L9/PyMjIwUnE5R5XXzgNcu57PRlbGzpY2fnIDtbO2jb3872iIuHNuxjJJ7Ke+3Basu9vOGYev5p7TEsqykr6hpjFqpiZkm9DDwOPAI8rKovF/ncCeAaVX1aRILAUyLyAPAA8DlVTYjI14DPAZ8t8BxvUNWuIl9vWmUDhtfrXZDjGAMDA3g8ntxKY5/PN2brYt2Obm54cBuPbO2iKuDmC+84nvedvgyP68CxivLyckSESCQy7uwzh0M4pbmK1Usr2VEZxXNyXWZMQgmPJBiKJxiMJhiKJRmKJRmOJ/E4HfjcDsq8LpZWBaw1YcwEFPO/5WTgDOBs4AYRORJ4WlXfPdZFqtoGtGVuh0VkM9Ckqn8addoTwMWTKvkMS6VSOBwOfD4fAwMDeXdGa21txeVylWQx2kzKTimura0ds1Whqjy+vZtv/3krT+7sobbcw7VvPZbLzlhWcAGc0+nE7/cTiUQKZmDN/u2zsms7spsdORxCKOAmROlWjRuzEBUTMKKku4YGgWGgCyicoD8PEVkOrAbWHfTQhyi8GZMCfxIRBX6oqjcVeO6rgKsAmpunbxPAZDKZCxh9fX3E4/FDxjGGhoZIpVJUV1fPq2yi2aypoxPhHeyp3T189b6X2LC7l4YKL//2juO59LRmfO7xp8dms7MePJlgZGSE3t5ewuEwlZWVuUDc29uLx+OhrMy6lIwppWJqsX7S6y6+DXxEVTsm8gIiUg7cCXxaVQdGHf9n0t1WtxW49CxVbRWReuABEXlJVR8++KRMILkJYM2aNTqRsh2O0S0MOHTgO5lM5tZqDAwMHLCCeK6LRCJ4PJ686xv29gzxH/e/xL3PtVEf9PLlC07g3WuWFhUoskan887+3fbv309/f3/ub97b24vP58PtdjM8PEx9fb3NHjKmxIoJGFcArwU+AVwhIn8lPZbx/8a7UETcpIPFbap616jjV5BeCPhGVc1byatqa+Z3h4jcTTpr7iEBY6ZkA4bH48HhcDAyMnLAArVsVlGHw0FfXx9VVVWzpkJTVXp7e3G73ZSVlR3QvZNPPB7H5XIhIiSTSYaGhg5ZOJZMKbc8upOv/2kLDhE+/aaVXPW6Iwh4Jt6ycrvdeL3eXMAIh8P09/dTVVVFTU0NDoeDlpYW2tvb8Xq9libDmGlSzCypO4E7ReQo4G3APwL/Aoy5M72ka8ebgc2q+s1Rx9eSHuR+vaoOFbi2DHBkxj7KgPOALxX3lqZHKpXC6XQiInkHvuPx9PKV6upqurq6GBwcHLMLZzoNDg7S2dkJpANaWVkZwWAwb/AYGhqipaWFQCBAU1MTg4ODqOoB72Vz2wCfv/t5ntnTx5uOa+ArF65iUejw0qWUl5fT09NDLBajo6MDn893QJ6pxsZGdu/ezfDwMFVVVeMGPWPM4StmltSvgFOAPaRnSn2I9Kyp8ZwFXA48LyIbM8c+D3yHdLB5IPOf/wlV/ZiILAZ+rKrnk86Qe3fmcRfwC1W9fyJvrNSyyeyA3DjG6IHvWCyGiFBVVUVfXx99fX2zJmBkU2M3Njbm0oyHw2FEhPLycurr63G5XMTjcVpbW3E6nQwODtLW1pZLR+71enlkayc/emQnD7/cSWXAzX9dcjLvPGnxlLSkysvL6e7upqWlhUQiQVNT0wHP63K5aGpqoqury9JkGDNNiukv+DawXlUTE3liVX0UyFdz3Ffg/FYyKdRVdQdw0kReb7qNnqnj8/lymwRl+/Wz3TgOh4PKykq6urpmxYrwaDTK0NAQtbW1BAIBAoEA9fX1jIyM5JIH7tq1i4aGBnp6elBVmpubGRwcpKMjPXw1Il4uv+VJ/rqtm7qgl8+cdzTvP30ZVWVT996y4xPxeJyqqqq8U2x9Ph9LliyZstc0xoytmICxEfiMiCxT1Y9nuqZWquofSly2We3ggAHpge9swBgdHLKb3wwMDBScKjpdent7EZEDtvUUEfx+P36/n1AoRFtbG62trQA0NTXh8XjweDzE4wnufmILN27oJYabL11wAu999VK8rtJkTa2oqGBgYKCofFTGmNIrJmDcAjxPeh0GQCvp3fcsYGQChtvtzg18Zwdf4/F4LpC4XC58Ph9DQ3mHbKZNMplkYGCAioqKgqmxPR4Pzc3N9PT04HK5KC8vR1V5cEsH19+/hZf393PWynq++q4TS56Yr7a2lpqamlkzWcCYha6YgLFSVS8VkXcDqOqQLPD/walUClXNBYzsSudsipDslNrRO7dls7cmk8kJ7WOQbbVMxZ88O84yXp+/iOS+1T+7t4/r7t3Mk7t6WFYT4FuXnDJl4xTFWOD/1IyZVYoJGDER8ZFeSIeIrCCdB2rByqYFGT0zx+v10t/fj6rmZkiNHq8oKyuju7uboaGh3AZBqVSKRCJRcFzjwU0tfP3uxzj/1KP4xFtXF115joyM0NHRgdPpxOVykUqlGBoaIpFIUFZWVnB/iCxVZXvnIN97cBt3P7OP2nIPX75wFZe8eilup81GMmahKiZgfAm4H1giIrcCrweuLGmpZrl8ASO7mCwWi+XWYIxuYfh8PhwOxwEBo6Ojg4GBAY444ohDVoJvbQ/z+Ts2UK4pfvnoZp7bP8T1l55OyD9+uouuri6i0Shutzs33TcQCOD3+w/ZzU5V2dwW5uX2MNs6ImxuG+CZvX30DMbwuBx84pwj+fg5RxL0WZoNYxa6MQNGpuvpWeDdwJmkZz39n4mu9p5vsgFjdNfS6IHvRCI9oWx0wBARAoEAg4ODQDofUzYHVW9vL3V1dblzuyJR/vYn6wk4lesuOImn9vRz+xM7eMe3h7n6Tcdy0SlNBb/pR6NRBgcHc/3/Y9nXN8w/3/08D21Jr8lwOoQVtWW88dh6VjdXcc4xdSyu9E/0z2OMmafGDBiqqiLye1U9FfjtNJVp1sum/BjdwsgOfEejUZLJZG5K7WhlZWVEIhFisRj9/f1A+pt/X19fbo9mVeVjP3uKrkiU777tCJbWVbL6uKNYUe3l1ida+Kc7n+W7D27lopObqC7zEAq4OfPIWhoq0gGrt7c3N5W3kFgixe1P7uH6+18ipfD584/lnGPqWV5Tdkj2WGOMySqmS+pJETlFVZ8ueWnmiHxdUgev+M43LpFNjhcOh+nr6yMYDFJdXc2uXbvo6+ujpqaGx7d3s2F3L19553E0V8Xx+/243W7OOvEollf72Tnk5sbH9/Odv2zLPW/A4+Tqc4/iijOWMjAwQCgUOmRgPZFMMRhNctczLfzo4R209o9w9spa/v2iE1laPbu2ITXGzE7FBIzXAh8Rke2kM9YK6cbHKSUt2SyWL2BAuluqv78/t2L6YG63G4/HQ3d3d262ktfrpby8nN7eXqqqqvjZE7upCrh5y7FV9HR24Penu4TKy8vxer0c73fw20+eRTyZYmA4Tlv/CN/581auv38Lv1u3hVMXuVm8ZBnBHcM829LP07t72d0zRDL1SsquVy+v4rqLTuScY+psFpIxpmjFBIwLS16KOWasgNHb2wvkb2HAK11QZWVluXGP6upq9uzZw7aWDv70Yjsffu0KNBHPJTcEcmlG2tvbGRoaIhAIUFPupabcy00fWMNDL7Vz6x/X8dddUXZuTrc+aso8rG6uYu2qRfjdTrxuB6c0V7Fm+fzJnGuMmT7FJB/cPh0FmUsKBYzR01VHD3iPVl5enhuzyPL7/QQCAX65bgspTfG+05sZjnQdsotdRUUFXV1d9Pb2Eggc2I10Yp2La9ceQ3NzM0mHm4HhOI2hsXfBM8aYibARzknIrvI+uDLOpjrP3s6nrKyMI4888pAKPxiq5OGX2nnDiiDN1QGi0egh+ZMcDgdVVVW5gfOsRCJBT08PwWAQv99PudfF4kq/BQtjzJSygDEJB28RmpUd+IbCLQwg7+57T+yJ0DGU4vyV5QwPD6OqufGL0SorKxERurq6yG4lkk0SONN5qowx89v82Td0GhUKGJAeoxjr8UJuW7eHQLCSExrLcntV5MvQ6nQ6qa6upru7m2QySW1tLX19fYRCoRnPhGuMmd8KBgwR6SWTDuTgh0jPklqwI6fZ/bzzKWbB3MHaB0Z4dFsXV7/hKPy+9NRct9tdcB/w2tpa3G43HR0d7NmzB4fDYRldjTElN1YLw/o3ChivBTHRsYN7NraiChetbqLam6KtrS1vd9RooVAIn89He3s7wWCwYHAxxpipUrDWU9Xk6B8gRHonvOzPmERkqYg8KCKbReQFEflU5ni1iDwgIlszv/OmThWRKzLnbM3sAT5rTKbLaSy/2biPk5ZWckRdOcFgkIqKigP2By/E6/XS3NxsO84ZY6bFuLWeiLxNRF4GWoB1md9/KeK5E8A1qnoccAbwSRE5HrgW+LOqrgT+nLl/8GtWA/8GnA6cBvxbocAyE6YyYLzcHuaF1gEuOnkxkG6dNDY25laFG2PMbFFMrXcd6f25t6jqUuAtwEPjXaSqbdl0IqoaBjYDTcAFwK2Z024l/8LAtwAPqGqPqvYCDwBriyjrtEilUhPa02Isv3lmH06H8PaTFk/J8xljTKkUEzASqtoJOEREVPUBYEJpQURkObCadAulQVXbIB1UgPo8lzQBe0fdb8kcmxWmqoWRSim/3djK61bWUls+9h4Vxhgz04qp9fpFpAx4FPipiHwDSBX7AiJSDtwJfFpVB4q9LM+xfDO2EJGrRGSDiGzITkctpYN32zsc63f1sK9vmAtXz5pYaIwxBRVT610IjACfJt0VtQ94ezFPLiJu0sHiNlW9K3O4XUQaM483Avn21mgBlo66v4T0XuKHUNWbVHWNqq4ZvadEqRRKCzIZdz+zj4DHyZuPH3cOgTHGzLhiar3PZWZKxVX1ZlX9JvCP412U2XzpZmBz5pqse4DsrKcryL/Pxh+B80SkKjPYfV7m2IybqoAxEk9y7/NtrF21iIDHpsQaY2a/Ymq9fIPNbyviurOAy4FzRWRj5ud84D+AN4vIVuDNmfuIyBoR+TGAqvYAXwbWZ36+lDk246YqYPx5cwfhkQTvWr1kKopljDElN9ZK748CHwOOFpHRmycFgQ3jPbGqPkr+sQiAN+Y5fwPw4VH3bwFuGe91pttUBYy7n2mhocLLa460FdrGmLlhrL6QX5NeJ/FVDlwrEV7Ie3rn2897orojUR7a0smHXrsCp8Myyhpj5oaCASOz/qEXeLeIrCK98x7AI+QfqF4Q8u3nPVG/f66NREq5yGZHGWPmkGJWen+SdGujOfPzaxH5RKkLNltNRZfUXc/s49hFQY5rHD/9hzHGzBbFTM/5KHCaqkYAROTfgceA75eyYLPV4QaMHZ0Rnt3bx+fPP3Yqi2WMMSVXTK0nQHzU/TiFB7PnvVQqhYhMeje7P2zaD8A7LBWIMWaOGWuWlEtVE8DPgCdE5M7MQxfxSi6oBedw80j97+Z2TmwK0RgaO325McbMNmO1MJ4EUNXrgauAIWAY+Jiqfn0ayjYrHU4eqY7wCBv39tnKbmPMnDTWGEauz0VVswvoFrzDCRh/2dyBKrzpOAsYxpi5Z6yAUSciBVOAHJTuY8EYa3vW8fzv5naaKv0c1xic4lIZY0zpjRUwnEA5C3iAO59UKoXb7Z7wdcOxJI9s7eKSVy+d9IC5McbMpLECRpuqfmnaSjJHTLZL6pGtnUQTKd58/KISlMoYY0pvrJrPvgbnMdmA8b+b2wl6XZy2oroEpTLGmNIbq+Y7JEGgmdy02mRK+fPmDl5/TB0e19TsBW6MMdOtYO01W9KJzyaT3W2vOxKlqcrPeSdYd5QxZu6ynXsmYLJpQeorfNxz9WtRzbvLrDHGzAnWPzIBh5tHymZHGWPmspK1METkFtJ7f3eo6qrMsV8Bx2ROqQT6VPXkPNfuAsJAEkio6ppSlXMisqnNDyc1iDHGzFWl7JL6CXAD8NPsAVV9b/a2iHwD6B/j+jeoalfJSjcJU7XbnjHGzEUlCxiq+rCILM/3mKT7Zt4DnFuq1y8FCxjGmIVspmq+s4F2Vd1a4HEF/iQiT4nIVdNYrjFNxfasxhgzV83ULKlLgdvHePwsVW0VkXrgARF5SVUfzndiJqBcBdDc3Dz1JR1lKrZnNcaYuWraaz4RcQHvAn5V6BxVbc387gDuBk4b49ybVHWNqq6pq6ub6uIeILt5kgUMY8xCNBM135uAl1S1Jd+DIlImIsHsbeA8YNM0lq+gw0ltbowxc13Jaj8RuR14HDhGRFpE5MrMQ5dwUHeUiCwWkfsydxuAR0XkWdKbON2rqveXqpwTcTipzY0xZq4r5SypSwsc/2CeY63A+ZnbO4CTSlWuw2EtDGPMQma13wRYwDDGLGRW+03AZDLVGmPMfGEBYwJsDMMYs5BZ7TcB1iVljFnIrPabAOuSMsYsZBYwijTZzZOMMWa+sNqvSJZ40Biz0FntVyQLGMaYhc5qvyLZ5knGmIXOAkaRrIVhjFnorPYrkgUMY8xCZ7VfkSxgGGMWOqv9imRjGMaYhc4CRpGyLYz0duTGGLPwWMAoUnaVtwUMY8xCZQGjSJZ40Biz0FkNWCRLPGiMWehKuUXrLSLSISKbRh37gojsE5GNmZ/zC1y7VkS2iMg2Ebm2VGWcCAsYxpiFrpQ14E+AtXmOf0tVT8783HfwgyLiBL4HvBU4HrhURI4vYTmLYplqjTELXckChqo+DPRM4tLTgG2qukNVY8AvgQumtHCTYGMYxpiFbiZqwKtF5LlMl1VVnsebgL2j7rdkjuUlIleJyAYR2dDZ2TnVZc2xLiljzEI33TXgD4AjgZOBNuAbec7JN29VCz2hqt6kqmtUdU1dXd3UlDIPCxjGmIVuWmtAVW1X1aSqpoAfke5+OlgLsHTU/SVA63SUr5Ds5kk2hmGMWcimNWCISOOouxcBm/Kcth5YKSIrRMQDXALcMx3lK8TySBljDLhK9cQicjtwDlArIi3AvwHniMjJpLuYdgEfzZy7GPixqp6vqgkRuRr4I+AEblHVF0pVzmJYwDDGmBIGDFW9NM/hmwuc2wqcP+r+fcAhU25nSjbxoAUMY8xCZjVgEbItDBvDMMYsZBYwimBdUsYYYwGjKBYwjDHGAsYhssFhNBvDMMYYCxgHiMVibNu2jUgkcsBxa2EYY4wFjANEo1FUld7e3gOOZ1d52+ZJxpiFzALGKPF4HIChoSFisVjuuGWqNcYYCxgHiMfjuZZEX18fAIlEgkgkgsfjmeHSGWPMzLKAMUo8Hsfj8VBeXs7AwACpVIr29nZSqRT19fUzXTxjjJlRFjBGicVieDweKisrSSaTtLW1EYlEqK2ttRaGMWbBs4CRoaokEgncbjeBQACPx0MkEsHn81FVlW/bDmOMWVgsYGTE43FUFbfbDUBVVRUOh4NFixbZ7ChjjKGEyQfnmuwMqWzAqKyspKKiwtZeGGNMhtWGGdmAMXqswoKFMca8wmrEjHg8jojYegtjjCnAAkZGdoaUjVcYY0x+JQsYInKLiHSIyKZRx/5TRF4SkedE5G4RqSxw7S4ReV5ENorIhlKVcbR4PJ4bvzDGGHOoUrYwfgKsPejYA8AqVX0V8DLwuTGuf4Oqnqyqa0pUvgNYwDDGmLGVLGCo6sNAz0HH/qSqiczdJ4AlpXr9iUgkEqRSKQsYxhgzhpkcw/gQ8IcCjynwJxF5SkSuKnVB8s2QMsYYc6AZWYchIv8MJIDbCpxylqq2ikg98ICIvJRpseR7rquAqwCam5snVZ6D12AYY4w51LS3METkCuDtwPtVVfOdo6qtmd8dwN3AaYWeT1VvUtU1qrqmrq5uUmXKpjK3gGGMMYVNa8AQkbXAZ4F3qupQgXPKRCSYvQ2cB2zKd+5UyQ5425RaY4wprJTTam8HHgeOEZEWEbkSuAEIku5m2igiN2bOXSwi92UubQAeFZFngSeBe1X1/lKVE2yGlDHGFKNkYxiqemmewzcXOLcVOD9zewdwUqnKlU88HqesrGw6X9IYY+acBb/SW1UJBAIEAoGZLooxxsxqCz5brYjQ2Ng408UwxphZb8G3MIwxxhTHAoYxxpiiWMAwxhhTFAsYxhhjimIBwxhjTFEsYBhjjCmKBQxjjDFFsYBhjDGmKFIgYeycJCKdwO5JXl4LdE1hcWYTe29z13x+f/beZodlqlpUqu95FTAOh4hsmK7tYKebvbe5az6/P3tvc491SRljjCmKBQxjjDFFsYDxiptmugAlZO9t7prP78/e2xxjYxjGGGOKYi0MY4wxRVnwAUNE1orIFhHZJiLXznR5DpeILBWRB0Vks4i8ICKfyhyvFpEHRGRr5nfVTJd1skTEKSLPiMjvM/dXiMi6zHv7lYh4ZrqMkyEilSJyh4i8lPn8XjPPPrd/yPyb3CQit4uIb65+diJyi4h0iMimUcfyflaS9p1MHfOciJwycyU/PAs6YIiIE/ge8FbgeOBSETl+Zkt12BLANap6HHAG8MnMe7oW+LOqrgT+nLk/V30K2Dzq/teAb2XeWy9w5YyU6vD9F3C/qh5LepvizcyTz01EmoC/B9ao6irACVzC3P3sfgKsPehYoc/qrcDKzM9VwA+mqYxTbkEHDOA0YJuq7lDVGPBL4IIZLtNhUdU2VX06cztMutJpIv2+bs2cditw4cyU8PCIyBLgbcCPM/cFOBe4I3PKnHxvIlIBvI7MvveqGlPVPubJ55bhAvwi4gICQBtz9LNT1YeBnoMOF/qsLgB+qmlPAJUiMie3+VzoAaMJ2Dvqfkvm2LwgIsuB1cA6oEFV2yAdVID6mSvZYfk28E9AKnO/BuhT1UTm/lz9DI8AOoH/znS3/VhEypgnn5uq7gO+DuwhHSj6gaeYH59dVqHPat7UMws9YGFuhXsAAAQFSURBVEieY/Ni2piIlAN3Ap9W1YGZLs9UEJG3Ax2q+tTow3lOnYufoQs4BfiBqq4GBpmj3U/5ZPrzLwBWAIuBMtJdNQebi5/deObLv9EFHzBagKWj7i8BWmeoLFNGRNykg8VtqnpX5nB7thmc+d0xU+U7DGcB7xSRXaS7D88l3eKozHRzwNz9DFuAFlVdl7l/B+kAMh8+N4A3ATtVtVNV48BdwJnMj88uq9BnNW/qmYUeMNYDKzMzNTykB+HumeEyHZZMn/7NwGZV/eaoh+4BrsjcvgL47XSX7XCp6udUdYmqLif9Wf1FVd8PPAhcnDltrr63/cBeETkmc+iNwIvMg88tYw9whogEMv9Gs+9vzn92oxT6rO4BPpCZLXUG0J/tuvr/7d09aBRBGMbx/2NECQSUKHYqiI0IISLYmCJg5QcWfhDEIAQsAoJVCg1KksLCSgubNIKiRKyCIEhAxQ/wExMipJRYCgFFRBEJr8VM8Ah3cZOcHpd7fnDc3eze7S7L3bszO/NOvWn4gXuSDpCuUpuA6xFxqca7tCySOoBnwHv+tPP3k+5j3AW2kH68xyNi/k27uiGpE+iLiEOStpFqHK3AONAdET9ruX9LIamddDN/DfAB6CFd1K2I8yZpCOgi9eQbB06T2vLr7txJGgE6SVlpPwEDwChlzlUOkNdIvaq+Az0R8bYW+71cDR8wzMysmEZvkjIzs4IcMMzMrBAHDDMzK8QBw8zMCnHAMDOzQhwwzCqQNCtpouSx4MhrSb2STlVhu9OSNi73e8yqzd1qzSqQ9C0iWmqw3WlSVteZ/71ts4W4hmG2SLkGcFnS6/zYnssHJfXl12clTeX5D+7kslZJo7nspaS2XL5B0lhOOjhMSe4hSd15GxOShnNKfrOacMAwq6x5XpNUV8myrxGxhzSC92qZz54DdkVEG9Cby4aA8VzWD9zM5QPA85x08B5ppDCSdpBGRu+NiHZgFjhZ3UM0K27131cxa1g/8h91OSMlz1fKLJ8EbksaJaWMAOgAjgJExKNcs1hHmgfjSC6/L+lzXn8fsBt4k7JL0Ez9Jh+0FcABw2xposLrOQdJgeAwcFHSThZOc13uOwTciIjzy9lRs2pxk5TZ0nSVPL8oXSBpFbA5Ih6TJntaD7QAT8lNSjl54kyeq6S0fD8wN2/3Q+CYpE15Waukrf/wmMwW5BqGWWXNkiZK3j+IiLmutWslvSJddJ2Y97km4FZubhJpzuovkgZJM+pNkrKWzqXCHgJGJL0DnpAynRIRU5IuAGM5CP0CzgAfq32gZkW4W63ZIrnbqzUqN0mZmVkhrmGYmVkhrmGYmVkhDhhmZlaIA4aZmRXigGFmZoU4YJiZWSEOGGZmVshvzTUZCHUSZsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward:120.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                             model.initial_state: initial_state})\n",
    "        action = np.argmax(action_logits)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "print('total_reward:{}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
