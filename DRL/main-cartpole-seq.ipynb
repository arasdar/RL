{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, next_state, reward, done, info: [ 0.00848692 -0.01232851 -0.03539592  0.04655268] 0 [ 0.00824035 -0.2069255  -0.03446486  0.3278611 ] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.00824035 -0.2069255  -0.03446486  0.3278611 ] 1 [ 0.00410184 -0.01133027 -0.02790764  0.02451182] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.00410184 -0.01133027 -0.02790764  0.02451182] 1 [ 0.00387524  0.18418056 -0.02741741 -0.276844  ] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.00387524  0.18418056 -0.02741741 -0.276844  ] 0 [ 0.00755885 -0.01053973 -0.03295429  0.00706695] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.00755885 -0.01053973 -0.03295429  0.00706695] 1 [ 0.00734806  0.18503896 -0.03281295 -0.2958286 ] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.00734806  0.18503896 -0.03281295 -0.2958286 ] 1 [ 0.01104883  0.38061295 -0.03872952 -0.59867696] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.01104883  0.38061295 -0.03872952 -0.59867696] 1 [ 0.01866109  0.57625479 -0.05070306 -0.90330327] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.01866109  0.57625479 -0.05070306 -0.90330327] 1 [ 0.03018619  0.77202551 -0.06876912 -1.21148229] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.03018619  0.77202551 -0.06876912 -1.21148229] 1 [ 0.0456267   0.96796451 -0.09299877 -1.52489828] 1.0 False {}\n",
      "state, action, next_state, reward, done, info: [ 0.0456267   0.96796451 -0.09299877 -1.52489828] 1 [ 0.06498599  1.1640781  -0.12349673 -1.84509972] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action) # take a random action\n",
    "    print('state, action, next_state, reward, done, info:', state, action, next_state, reward, done, info)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, lstm_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return states, actions, targetQs, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(states, initial_state, cell, lstm_size, num_classes, reuse=False): \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = tf.layers.dense(inputs=states, units=lstm_size)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, lstm_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state.shape)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state.shape)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, lstm_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=num_classes)\n",
    "        print(logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cell, initial_state, actions, targetQs):\n",
    "    actions_logits, final_state = generator(states=states, cell=cell, initial_state=initial_state, \n",
    "                                            lstm_size=hidden_size, num_classes=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # # Optimize\n",
    "    # with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    # #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "    grads = tf.gradients(loss, g_vars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, cell, self.initial_state = model_input(\n",
    "            state_size=state_size, lstm_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cell=cell, initial_state=self.initial_state)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.states = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx], [self.states[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state:', np.array(states).shape[1], \n",
    "#       'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 2\n",
    "state_size = 4\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity - 1000 DQN\n",
    "batch_size = 128               # experience mini-batch size - 20 DQN\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(1, ?, 64) (1, 64)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:12.0000 R:12.0000 loss:0.8702 exploreP:0.9988\n",
      "Episode:1 meanR:32.0000 R:52.0000 loss:0.9405 exploreP:0.9937\n",
      "Episode:2 meanR:45.6667 R:73.0000 loss:1.3626 exploreP:0.9865\n",
      "Episode:3 meanR:42.7500 R:34.0000 loss:1.7622 exploreP:0.9832\n",
      "Episode:4 meanR:40.2000 R:30.0000 loss:1.7501 exploreP:0.9803\n",
      "Episode:5 meanR:40.0000 R:39.0000 loss:1.7121 exploreP:0.9765\n",
      "Episode:6 meanR:38.1429 R:27.0000 loss:1.8985 exploreP:0.9739\n",
      "Episode:7 meanR:37.3750 R:32.0000 loss:2.1310 exploreP:0.9708\n",
      "Episode:8 meanR:36.5556 R:30.0000 loss:2.4070 exploreP:0.9680\n",
      "Episode:9 meanR:35.2000 R:23.0000 loss:2.6545 exploreP:0.9658\n",
      "Episode:10 meanR:34.3636 R:26.0000 loss:2.9866 exploreP:0.9633\n",
      "Episode:11 meanR:33.0000 R:18.0000 loss:3.2750 exploreP:0.9616\n",
      "Episode:12 meanR:31.8462 R:18.0000 loss:3.5348 exploreP:0.9599\n",
      "Episode:13 meanR:30.9286 R:19.0000 loss:3.9222 exploreP:0.9580\n",
      "Episode:14 meanR:30.1333 R:19.0000 loss:4.1273 exploreP:0.9562\n",
      "Episode:15 meanR:29.3750 R:18.0000 loss:4.3936 exploreP:0.9545\n",
      "Episode:16 meanR:28.5294 R:15.0000 loss:4.8395 exploreP:0.9531\n",
      "Episode:17 meanR:27.8333 R:16.0000 loss:5.2882 exploreP:0.9516\n",
      "Episode:18 meanR:27.1579 R:15.0000 loss:5.5950 exploreP:0.9502\n",
      "Episode:19 meanR:26.5000 R:14.0000 loss:5.8755 exploreP:0.9489\n",
      "Episode:20 meanR:25.9524 R:15.0000 loss:6.0196 exploreP:0.9475\n",
      "Episode:21 meanR:25.4091 R:14.0000 loss:6.2211 exploreP:0.9462\n",
      "Episode:22 meanR:24.9130 R:14.0000 loss:6.4526 exploreP:0.9449\n",
      "Episode:23 meanR:24.4583 R:14.0000 loss:6.6291 exploreP:0.9436\n",
      "Episode:24 meanR:24.0000 R:13.0000 loss:6.5832 exploreP:0.9423\n",
      "Episode:25 meanR:23.6154 R:14.0000 loss:6.3271 exploreP:0.9410\n",
      "Episode:26 meanR:23.2963 R:15.0000 loss:6.1062 exploreP:0.9396\n",
      "Episode:27 meanR:23.0357 R:16.0000 loss:5.5260 exploreP:0.9382\n",
      "Episode:28 meanR:22.8966 R:19.0000 loss:16.0552 exploreP:0.9364\n",
      "Episode:29 meanR:23.2000 R:32.0000 loss:34.8518 exploreP:0.9334\n",
      "Episode:30 meanR:23.7419 R:40.0000 loss:15.8681 exploreP:0.9298\n",
      "Episode:31 meanR:23.9375 R:30.0000 loss:14.6386 exploreP:0.9270\n",
      "Episode:32 meanR:24.1515 R:31.0000 loss:8.4746 exploreP:0.9242\n",
      "Episode:33 meanR:24.5294 R:37.0000 loss:2.9875 exploreP:0.9208\n",
      "Episode:34 meanR:24.6000 R:27.0000 loss:3.2211 exploreP:0.9183\n",
      "Episode:35 meanR:24.9167 R:36.0000 loss:3.3322 exploreP:0.9151\n",
      "Episode:36 meanR:25.1622 R:34.0000 loss:3.3413 exploreP:0.9120\n",
      "Episode:37 meanR:25.2895 R:30.0000 loss:3.5353 exploreP:0.9093\n",
      "Episode:38 meanR:25.3846 R:29.0000 loss:3.6319 exploreP:0.9067\n",
      "Episode:39 meanR:25.3750 R:25.0000 loss:3.7262 exploreP:0.9044\n",
      "Episode:40 meanR:25.2927 R:22.0000 loss:4.0646 exploreP:0.9025\n",
      "Episode:41 meanR:25.2857 R:25.0000 loss:4.3827 exploreP:0.9003\n",
      "Episode:42 meanR:25.4419 R:32.0000 loss:4.4432 exploreP:0.8974\n",
      "Episode:43 meanR:25.5455 R:30.0000 loss:4.5112 exploreP:0.8947\n",
      "Episode:44 meanR:26.0000 R:46.0000 loss:4.2995 exploreP:0.8907\n",
      "Episode:45 meanR:26.2609 R:38.0000 loss:3.7943 exploreP:0.8873\n",
      "Episode:46 meanR:26.7234 R:48.0000 loss:3.6372 exploreP:0.8831\n",
      "Episode:47 meanR:27.0625 R:43.0000 loss:3.5189 exploreP:0.8794\n",
      "Episode:48 meanR:27.3265 R:40.0000 loss:3.6017 exploreP:0.8759\n",
      "Episode:49 meanR:27.9000 R:56.0000 loss:3.6380 exploreP:0.8711\n",
      "Episode:50 meanR:27.7843 R:22.0000 loss:4.0608 exploreP:0.8692\n",
      "Episode:51 meanR:27.5769 R:17.0000 loss:4.8568 exploreP:0.8677\n",
      "Episode:52 meanR:27.3208 R:14.0000 loss:5.3723 exploreP:0.8665\n",
      "Episode:53 meanR:27.0926 R:15.0000 loss:6.5176 exploreP:0.8653\n",
      "Episode:54 meanR:26.8909 R:16.0000 loss:6.9513 exploreP:0.8639\n",
      "Episode:55 meanR:26.6786 R:15.0000 loss:7.7800 exploreP:0.8626\n",
      "Episode:56 meanR:26.4561 R:14.0000 loss:8.9273 exploreP:0.8614\n",
      "Episode:57 meanR:26.2414 R:14.0000 loss:10.0552 exploreP:0.8602\n",
      "Episode:58 meanR:26.0508 R:15.0000 loss:10.1507 exploreP:0.8590\n",
      "Episode:59 meanR:25.9667 R:21.0000 loss:10.4013 exploreP:0.8572\n",
      "Episode:60 meanR:26.1639 R:38.0000 loss:9.1845 exploreP:0.8540\n",
      "Episode:61 meanR:26.4032 R:41.0000 loss:7.3532 exploreP:0.8505\n",
      "Episode:62 meanR:26.4603 R:30.0000 loss:5.9563 exploreP:0.8480\n",
      "Episode:63 meanR:26.3906 R:22.0000 loss:5.5741 exploreP:0.8461\n",
      "Episode:64 meanR:26.2462 R:17.0000 loss:5.8438 exploreP:0.8447\n",
      "Episode:65 meanR:26.0455 R:13.0000 loss:7.1337 exploreP:0.8436\n",
      "Episode:66 meanR:25.8955 R:16.0000 loss:7.6084 exploreP:0.8423\n",
      "Episode:67 meanR:25.7353 R:15.0000 loss:8.4210 exploreP:0.8411\n",
      "Episode:68 meanR:25.5652 R:14.0000 loss:9.6470 exploreP:0.8399\n",
      "Episode:69 meanR:25.4714 R:19.0000 loss:9.6283 exploreP:0.8383\n",
      "Episode:70 meanR:25.4507 R:24.0000 loss:9.9960 exploreP:0.8363\n",
      "Episode:71 meanR:25.6250 R:38.0000 loss:9.2862 exploreP:0.8332\n",
      "Episode:72 meanR:25.8904 R:45.0000 loss:7.2926 exploreP:0.8295\n",
      "Episode:73 meanR:25.9865 R:33.0000 loss:5.7013 exploreP:0.8268\n",
      "Episode:74 meanR:25.8667 R:17.0000 loss:5.9426 exploreP:0.8254\n",
      "Episode:75 meanR:25.7105 R:14.0000 loss:6.4497 exploreP:0.8243\n",
      "Episode:76 meanR:25.5584 R:14.0000 loss:7.8841 exploreP:0.8231\n",
      "Episode:77 meanR:25.3974 R:13.0000 loss:8.4605 exploreP:0.8221\n",
      "Episode:78 meanR:25.2405 R:13.0000 loss:9.2246 exploreP:0.8210\n",
      "Episode:79 meanR:25.1250 R:16.0000 loss:10.4628 exploreP:0.8197\n",
      "Episode:80 meanR:25.0247 R:17.0000 loss:10.9619 exploreP:0.8184\n",
      "Episode:81 meanR:24.9268 R:17.0000 loss:11.5132 exploreP:0.8170\n",
      "Episode:82 meanR:24.8434 R:18.0000 loss:11.9384 exploreP:0.8155\n",
      "Episode:83 meanR:24.7381 R:16.0000 loss:11.8390 exploreP:0.8142\n",
      "Episode:84 meanR:24.6000 R:13.0000 loss:11.7318 exploreP:0.8132\n",
      "Episode:85 meanR:24.4651 R:13.0000 loss:11.8426 exploreP:0.8122\n",
      "Episode:86 meanR:24.3448 R:14.0000 loss:11.8017 exploreP:0.8110\n",
      "Episode:87 meanR:24.2386 R:15.0000 loss:11.6965 exploreP:0.8098\n",
      "Episode:88 meanR:24.1124 R:13.0000 loss:11.8850 exploreP:0.8088\n",
      "Episode:89 meanR:23.9778 R:12.0000 loss:12.3040 exploreP:0.8078\n",
      "Episode:90 meanR:23.8462 R:12.0000 loss:12.5316 exploreP:0.8069\n",
      "Episode:91 meanR:23.7391 R:14.0000 loss:12.6343 exploreP:0.8058\n",
      "Episode:92 meanR:23.6452 R:15.0000 loss:12.7894 exploreP:0.8046\n",
      "Episode:93 meanR:23.5532 R:15.0000 loss:12.7658 exploreP:0.8034\n",
      "Episode:94 meanR:23.4737 R:16.0000 loss:12.4107 exploreP:0.8021\n",
      "Episode:95 meanR:23.3958 R:16.0000 loss:11.9242 exploreP:0.8008\n",
      "Episode:96 meanR:23.2990 R:14.0000 loss:11.8689 exploreP:0.7997\n",
      "Episode:97 meanR:23.2041 R:14.0000 loss:11.9233 exploreP:0.7986\n",
      "Episode:98 meanR:23.1313 R:16.0000 loss:11.6401 exploreP:0.7974\n",
      "Episode:99 meanR:23.0500 R:15.0000 loss:11.2995 exploreP:0.7962\n",
      "Episode:100 meanR:23.0700 R:14.0000 loss:11.2434 exploreP:0.7951\n",
      "Episode:101 meanR:22.7000 R:15.0000 loss:11.2297 exploreP:0.7939\n",
      "Episode:102 meanR:22.0900 R:12.0000 loss:11.4130 exploreP:0.7930\n",
      "Episode:103 meanR:21.8700 R:12.0000 loss:11.6825 exploreP:0.7920\n",
      "Episode:104 meanR:21.6700 R:10.0000 loss:11.8701 exploreP:0.7913\n",
      "Episode:105 meanR:21.4100 R:13.0000 loss:12.3080 exploreP:0.7902\n",
      "Episode:106 meanR:21.2400 R:10.0000 loss:12.5147 exploreP:0.7895\n",
      "Episode:107 meanR:21.0200 R:10.0000 loss:12.7240 exploreP:0.7887\n",
      "Episode:108 meanR:20.8400 R:12.0000 loss:12.7141 exploreP:0.7877\n",
      "Episode:109 meanR:20.7300 R:12.0000 loss:12.9768 exploreP:0.7868\n",
      "Episode:110 meanR:20.5700 R:10.0000 loss:13.2518 exploreP:0.7860\n",
      "Episode:111 meanR:20.5200 R:13.0000 loss:13.2018 exploreP:0.7850\n",
      "Episode:112 meanR:20.4500 R:11.0000 loss:13.2130 exploreP:0.7842\n",
      "Episode:113 meanR:20.3900 R:13.0000 loss:13.2020 exploreP:0.7832\n",
      "Episode:114 meanR:20.3200 R:12.0000 loss:12.9260 exploreP:0.7822\n",
      "Episode:115 meanR:20.2400 R:10.0000 loss:12.6930 exploreP:0.7815\n",
      "Episode:116 meanR:20.2000 R:11.0000 loss:12.2276 exploreP:0.7806\n",
      "Episode:117 meanR:20.1700 R:13.0000 loss:11.7995 exploreP:0.7796\n",
      "Episode:118 meanR:20.1500 R:13.0000 loss:10.8860 exploreP:0.7786\n",
      "Episode:119 meanR:20.1300 R:12.0000 loss:10.2831 exploreP:0.7777\n",
      "Episode:120 meanR:20.0800 R:10.0000 loss:10.1074 exploreP:0.7769\n",
      "Episode:121 meanR:20.0400 R:10.0000 loss:10.0798 exploreP:0.7762\n",
      "Episode:122 meanR:20.0200 R:12.0000 loss:9.9605 exploreP:0.7753\n",
      "Episode:123 meanR:19.9900 R:11.0000 loss:9.7807 exploreP:0.7744\n",
      "Episode:124 meanR:19.9800 R:12.0000 loss:9.4673 exploreP:0.7735\n",
      "Episode:125 meanR:19.9600 R:12.0000 loss:8.9243 exploreP:0.7726\n",
      "Episode:126 meanR:19.9100 R:10.0000 loss:8.2909 exploreP:0.7718\n",
      "Episode:127 meanR:19.8700 R:12.0000 loss:7.7241 exploreP:0.7709\n",
      "Episode:128 meanR:19.8100 R:13.0000 loss:7.4252 exploreP:0.7699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:129 meanR:19.6600 R:17.0000 loss:7.0923 exploreP:0.7686\n",
      "Episode:130 meanR:19.4000 R:14.0000 loss:6.9766 exploreP:0.7676\n",
      "Episode:131 meanR:19.2600 R:16.0000 loss:7.1571 exploreP:0.7664\n",
      "Episode:132 meanR:19.1300 R:18.0000 loss:7.1388 exploreP:0.7650\n",
      "Episode:133 meanR:18.9800 R:22.0000 loss:6.8797 exploreP:0.7633\n",
      "Episode:134 meanR:18.9000 R:19.0000 loss:6.4964 exploreP:0.7619\n",
      "Episode:135 meanR:18.7900 R:25.0000 loss:6.1994 exploreP:0.7600\n",
      "Episode:136 meanR:18.6800 R:23.0000 loss:6.5970 exploreP:0.7583\n",
      "Episode:137 meanR:18.6000 R:22.0000 loss:6.8233 exploreP:0.7567\n",
      "Episode:138 meanR:18.5200 R:21.0000 loss:7.2645 exploreP:0.7551\n",
      "Episode:139 meanR:18.5100 R:24.0000 loss:7.6009 exploreP:0.7533\n",
      "Episode:140 meanR:18.4800 R:19.0000 loss:7.9517 exploreP:0.7519\n",
      "Episode:141 meanR:18.4300 R:20.0000 loss:8.3710 exploreP:0.7504\n",
      "Episode:142 meanR:18.3900 R:28.0000 loss:7.8867 exploreP:0.7483\n",
      "Episode:143 meanR:18.3900 R:30.0000 loss:7.1859 exploreP:0.7461\n",
      "Episode:144 meanR:18.3400 R:41.0000 loss:7.0020 exploreP:0.7431\n",
      "Episode:145 meanR:18.3100 R:35.0000 loss:6.6469 exploreP:0.7406\n",
      "Episode:146 meanR:18.0500 R:22.0000 loss:6.9576 exploreP:0.7389\n",
      "Episode:147 meanR:17.8600 R:24.0000 loss:7.2708 exploreP:0.7372\n",
      "Episode:148 meanR:17.6600 R:20.0000 loss:7.9522 exploreP:0.7357\n",
      "Episode:149 meanR:17.2500 R:15.0000 loss:9.0983 exploreP:0.7347\n",
      "Episode:150 meanR:17.1900 R:16.0000 loss:10.1028 exploreP:0.7335\n",
      "Episode:151 meanR:17.2600 R:24.0000 loss:9.0820 exploreP:0.7318\n",
      "Episode:152 meanR:17.3000 R:18.0000 loss:8.8466 exploreP:0.7305\n",
      "Episode:153 meanR:17.4000 R:25.0000 loss:8.2692 exploreP:0.7287\n",
      "Episode:154 meanR:17.4300 R:19.0000 loss:8.1762 exploreP:0.7273\n",
      "Episode:155 meanR:17.5400 R:26.0000 loss:7.7294 exploreP:0.7254\n",
      "Episode:156 meanR:17.6100 R:21.0000 loss:7.6885 exploreP:0.7239\n",
      "Episode:157 meanR:17.7000 R:23.0000 loss:7.1824 exploreP:0.7223\n",
      "Episode:158 meanR:17.8500 R:30.0000 loss:7.2138 exploreP:0.7202\n",
      "Episode:159 meanR:17.9600 R:32.0000 loss:6.5356 exploreP:0.7179\n",
      "Episode:160 meanR:17.8900 R:31.0000 loss:6.3534 exploreP:0.7157\n",
      "Episode:161 meanR:17.7600 R:28.0000 loss:6.6665 exploreP:0.7137\n",
      "Episode:162 meanR:17.8000 R:34.0000 loss:6.9095 exploreP:0.7113\n",
      "Episode:163 meanR:17.9700 R:39.0000 loss:5.7043 exploreP:0.7086\n",
      "Episode:164 meanR:18.1700 R:37.0000 loss:5.9787 exploreP:0.7060\n",
      "Episode:165 meanR:18.3500 R:31.0000 loss:6.2673 exploreP:0.7039\n",
      "Episode:166 meanR:18.6400 R:45.0000 loss:7.1600 exploreP:0.7008\n",
      "Episode:167 meanR:19.0600 R:57.0000 loss:7.1622 exploreP:0.6968\n",
      "Episode:168 meanR:19.1300 R:21.0000 loss:7.5321 exploreP:0.6954\n",
      "Episode:169 meanR:19.1600 R:22.0000 loss:8.3338 exploreP:0.6939\n",
      "Episode:170 meanR:19.0700 R:15.0000 loss:10.3514 exploreP:0.6929\n",
      "Episode:171 meanR:18.8800 R:19.0000 loss:12.0470 exploreP:0.6916\n",
      "Episode:172 meanR:18.5900 R:16.0000 loss:12.9185 exploreP:0.6905\n",
      "Episode:173 meanR:18.4100 R:15.0000 loss:15.3201 exploreP:0.6895\n",
      "Episode:174 meanR:18.4000 R:16.0000 loss:17.6508 exploreP:0.6884\n",
      "Episode:175 meanR:18.4100 R:15.0000 loss:18.2623 exploreP:0.6874\n",
      "Episode:176 meanR:18.4200 R:15.0000 loss:18.7758 exploreP:0.6863\n",
      "Episode:177 meanR:18.4400 R:15.0000 loss:18.5295 exploreP:0.6853\n",
      "Episode:178 meanR:18.5500 R:24.0000 loss:32.0831 exploreP:0.6837\n",
      "Episode:179 meanR:18.6700 R:28.0000 loss:33.3211 exploreP:0.6818\n",
      "Episode:180 meanR:18.8800 R:38.0000 loss:34.7715 exploreP:0.6793\n",
      "Episode:181 meanR:19.0600 R:35.0000 loss:35.8935 exploreP:0.6769\n",
      "Episode:182 meanR:19.1900 R:31.0000 loss:29.9067 exploreP:0.6749\n",
      "Episode:183 meanR:19.4300 R:40.0000 loss:8.6062 exploreP:0.6722\n",
      "Episode:184 meanR:19.6000 R:30.0000 loss:8.9420 exploreP:0.6702\n",
      "Episode:185 meanR:19.8800 R:41.0000 loss:8.8595 exploreP:0.6675\n",
      "Episode:186 meanR:20.1500 R:41.0000 loss:8.5352 exploreP:0.6648\n",
      "Episode:187 meanR:20.4000 R:40.0000 loss:8.8139 exploreP:0.6622\n",
      "Episode:188 meanR:20.7100 R:44.0000 loss:8.5234 exploreP:0.6594\n",
      "Episode:189 meanR:21.2100 R:62.0000 loss:8.1166 exploreP:0.6554\n",
      "Episode:190 meanR:21.4800 R:39.0000 loss:7.9356 exploreP:0.6528\n",
      "Episode:191 meanR:21.6500 R:31.0000 loss:8.9454 exploreP:0.6509\n",
      "Episode:192 meanR:21.8500 R:35.0000 loss:9.2898 exploreP:0.6486\n",
      "Episode:193 meanR:22.0800 R:38.0000 loss:10.9268 exploreP:0.6462\n",
      "Episode:194 meanR:22.2800 R:36.0000 loss:8.3375 exploreP:0.6439\n",
      "Episode:195 meanR:22.5900 R:47.0000 loss:8.2680 exploreP:0.6409\n",
      "Episode:196 meanR:22.7700 R:32.0000 loss:8.4828 exploreP:0.6389\n",
      "Episode:197 meanR:23.1300 R:50.0000 loss:6.7485 exploreP:0.6358\n",
      "Episode:198 meanR:23.4000 R:43.0000 loss:6.8465 exploreP:0.6331\n",
      "Episode:199 meanR:23.7400 R:49.0000 loss:7.0004 exploreP:0.6301\n",
      "Episode:200 meanR:24.2600 R:66.0000 loss:7.0803 exploreP:0.6260\n",
      "Episode:201 meanR:24.3300 R:22.0000 loss:8.6691 exploreP:0.6246\n",
      "Episode:202 meanR:24.4800 R:27.0000 loss:10.4303 exploreP:0.6230\n",
      "Episode:203 meanR:24.5000 R:14.0000 loss:13.2483 exploreP:0.6221\n",
      "Episode:204 meanR:24.8500 R:45.0000 loss:9.4410 exploreP:0.6194\n",
      "Episode:205 meanR:25.3400 R:62.0000 loss:9.2355 exploreP:0.6156\n",
      "Episode:206 meanR:25.4500 R:21.0000 loss:9.6937 exploreP:0.6143\n",
      "Episode:207 meanR:25.6500 R:30.0000 loss:9.2649 exploreP:0.6125\n",
      "Episode:208 meanR:25.9700 R:44.0000 loss:7.9322 exploreP:0.6099\n",
      "Episode:209 meanR:26.0500 R:20.0000 loss:8.9457 exploreP:0.6087\n",
      "Episode:210 meanR:26.2900 R:34.0000 loss:9.4534 exploreP:0.6066\n",
      "Episode:211 meanR:26.9400 R:78.0000 loss:7.3824 exploreP:0.6020\n",
      "Episode:212 meanR:27.2300 R:40.0000 loss:7.2090 exploreP:0.5996\n",
      "Episode:213 meanR:27.5200 R:42.0000 loss:7.5572 exploreP:0.5972\n",
      "Episode:214 meanR:27.6200 R:22.0000 loss:10.5940 exploreP:0.5959\n",
      "Episode:215 meanR:28.3800 R:86.0000 loss:7.9792 exploreP:0.5909\n",
      "Episode:216 meanR:28.9500 R:68.0000 loss:6.3166 exploreP:0.5869\n",
      "Episode:217 meanR:29.2300 R:41.0000 loss:7.7896 exploreP:0.5846\n",
      "Episode:218 meanR:29.4400 R:34.0000 loss:10.3630 exploreP:0.5826\n",
      "Episode:219 meanR:29.6600 R:34.0000 loss:12.5320 exploreP:0.5807\n",
      "Episode:220 meanR:29.8000 R:24.0000 loss:15.9094 exploreP:0.5793\n",
      "Episode:221 meanR:29.9600 R:26.0000 loss:17.2058 exploreP:0.5778\n",
      "Episode:222 meanR:30.1000 R:26.0000 loss:18.6289 exploreP:0.5763\n",
      "Episode:223 meanR:30.2600 R:27.0000 loss:19.4776 exploreP:0.5748\n",
      "Episode:224 meanR:30.4000 R:26.0000 loss:20.2985 exploreP:0.5734\n",
      "Episode:225 meanR:30.5200 R:24.0000 loss:19.8842 exploreP:0.5720\n",
      "Episode:226 meanR:30.6500 R:23.0000 loss:19.2821 exploreP:0.5707\n",
      "Episode:227 meanR:30.7700 R:24.0000 loss:18.6110 exploreP:0.5694\n",
      "Episode:228 meanR:31.0400 R:40.0000 loss:17.2108 exploreP:0.5671\n",
      "Episode:229 meanR:31.2100 R:34.0000 loss:16.0784 exploreP:0.5652\n",
      "Episode:230 meanR:31.4000 R:33.0000 loss:15.8269 exploreP:0.5634\n",
      "Episode:231 meanR:31.5800 R:34.0000 loss:14.2798 exploreP:0.5615\n",
      "Episode:232 meanR:31.6800 R:28.0000 loss:15.9023 exploreP:0.5600\n",
      "Episode:233 meanR:31.8300 R:37.0000 loss:15.5630 exploreP:0.5580\n",
      "Episode:234 meanR:31.9200 R:28.0000 loss:15.6777 exploreP:0.5564\n",
      "Episode:235 meanR:32.0300 R:36.0000 loss:15.5635 exploreP:0.5545\n",
      "Episode:236 meanR:32.1700 R:37.0000 loss:14.2646 exploreP:0.5525\n",
      "Episode:237 meanR:32.2400 R:29.0000 loss:15.1569 exploreP:0.5509\n",
      "Episode:238 meanR:32.3400 R:31.0000 loss:14.7433 exploreP:0.5492\n",
      "Episode:239 meanR:32.4700 R:37.0000 loss:14.5773 exploreP:0.5472\n",
      "Episode:240 meanR:32.6000 R:32.0000 loss:15.0848 exploreP:0.5455\n",
      "Episode:241 meanR:32.7000 R:30.0000 loss:14.8550 exploreP:0.5439\n",
      "Episode:242 meanR:32.7000 R:28.0000 loss:14.9619 exploreP:0.5424\n",
      "Episode:243 meanR:32.7800 R:38.0000 loss:15.0062 exploreP:0.5404\n",
      "Episode:244 meanR:32.7400 R:37.0000 loss:14.5249 exploreP:0.5384\n",
      "Episode:245 meanR:33.0500 R:66.0000 loss:12.7853 exploreP:0.5350\n",
      "Episode:246 meanR:33.5300 R:70.0000 loss:10.4631 exploreP:0.5313\n",
      "Episode:247 meanR:33.7900 R:50.0000 loss:10.0747 exploreP:0.5287\n",
      "Episode:248 meanR:34.0600 R:47.0000 loss:10.9963 exploreP:0.5263\n",
      "Episode:249 meanR:34.2700 R:36.0000 loss:14.5546 exploreP:0.5244\n",
      "Episode:250 meanR:35.0100 R:90.0000 loss:12.8156 exploreP:0.5198\n",
      "Episode:251 meanR:34.9800 R:21.0000 loss:10.3542 exploreP:0.5187\n",
      "Episode:252 meanR:34.9400 R:14.0000 loss:13.9614 exploreP:0.5180\n",
      "Episode:253 meanR:35.6500 R:96.0000 loss:10.0424 exploreP:0.5132\n",
      "Episode:254 meanR:36.2400 R:78.0000 loss:8.0418 exploreP:0.5092\n",
      "Episode:255 meanR:36.2600 R:28.0000 loss:11.8367 exploreP:0.5079\n",
      "Episode:256 meanR:36.2700 R:22.0000 loss:17.2900 exploreP:0.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:257 meanR:36.2400 R:20.0000 loss:18.1862 exploreP:0.5058\n",
      "Episode:258 meanR:36.3700 R:43.0000 loss:21.7344 exploreP:0.5036\n",
      "Episode:259 meanR:37.1200 R:107.0000 loss:39.5012 exploreP:0.4984\n",
      "Episode:260 meanR:38.0200 R:121.0000 loss:7.4086 exploreP:0.4925\n",
      "Episode:261 meanR:39.3800 R:164.0000 loss:5.2127 exploreP:0.4847\n",
      "Episode:262 meanR:41.1000 R:206.0000 loss:4.4917 exploreP:0.4750\n",
      "Episode:263 meanR:41.7500 R:104.0000 loss:6.6695 exploreP:0.4702\n",
      "Episode:264 meanR:42.2900 R:91.0000 loss:5.8990 exploreP:0.4660\n",
      "Episode:265 meanR:42.5000 R:52.0000 loss:4.2510 exploreP:0.4636\n",
      "Episode:266 meanR:42.3300 R:28.0000 loss:9.8185 exploreP:0.4624\n",
      "Episode:267 meanR:41.9400 R:18.0000 loss:16.9045 exploreP:0.4616\n",
      "Episode:268 meanR:41.8500 R:12.0000 loss:23.1944 exploreP:0.4610\n",
      "Episode:269 meanR:42.7200 R:109.0000 loss:14.8802 exploreP:0.4561\n",
      "Episode:270 meanR:43.4400 R:87.0000 loss:4.4321 exploreP:0.4523\n",
      "Episode:271 meanR:44.5500 R:130.0000 loss:3.4473 exploreP:0.4466\n",
      "Episode:272 meanR:45.1900 R:80.0000 loss:2.9617 exploreP:0.4431\n",
      "Episode:273 meanR:45.7600 R:72.0000 loss:4.7917 exploreP:0.4400\n",
      "Episode:274 meanR:46.2000 R:60.0000 loss:5.6948 exploreP:0.4374\n",
      "Episode:275 meanR:46.6600 R:61.0000 loss:5.8684 exploreP:0.4348\n",
      "Episode:276 meanR:47.9200 R:141.0000 loss:4.3886 exploreP:0.4288\n",
      "Episode:277 meanR:48.5000 R:73.0000 loss:9.2374 exploreP:0.4258\n",
      "Episode:278 meanR:49.7200 R:146.0000 loss:6.0487 exploreP:0.4198\n",
      "Episode:279 meanR:50.1300 R:69.0000 loss:3.0944 exploreP:0.4170\n",
      "Episode:280 meanR:50.9100 R:116.0000 loss:4.4002 exploreP:0.4123\n",
      "Episode:281 meanR:51.3100 R:75.0000 loss:10.8573 exploreP:0.4093\n",
      "Episode:282 meanR:51.5800 R:58.0000 loss:12.2366 exploreP:0.4069\n",
      "Episode:283 meanR:52.3600 R:118.0000 loss:10.9548 exploreP:0.4023\n",
      "Episode:284 meanR:52.5800 R:52.0000 loss:12.6066 exploreP:0.4003\n",
      "Episode:285 meanR:52.2600 R:9.0000 loss:14.5069 exploreP:0.3999\n",
      "Episode:286 meanR:51.9500 R:10.0000 loss:18.2550 exploreP:0.3995\n",
      "Episode:287 meanR:53.3700 R:182.0000 loss:9.6669 exploreP:0.3925\n",
      "Episode:288 meanR:55.0100 R:208.0000 loss:6.0180 exploreP:0.3846\n",
      "Episode:289 meanR:55.6600 R:127.0000 loss:9.0733 exploreP:0.3799\n",
      "Episode:290 meanR:55.5500 R:28.0000 loss:12.7910 exploreP:0.3789\n",
      "Episode:291 meanR:55.5000 R:26.0000 loss:23.9057 exploreP:0.3779\n",
      "Episode:292 meanR:55.3800 R:23.0000 loss:33.7206 exploreP:0.3771\n",
      "Episode:293 meanR:57.9400 R:294.0000 loss:6.0786 exploreP:0.3664\n",
      "Episode:294 meanR:59.4300 R:185.0000 loss:1.3758 exploreP:0.3599\n",
      "Episode:295 meanR:60.8700 R:191.0000 loss:1.0059 exploreP:0.3533\n",
      "Episode:296 meanR:60.8200 R:27.0000 loss:15.2665 exploreP:0.3523\n",
      "Episode:297 meanR:62.5300 R:221.0000 loss:10.7281 exploreP:0.3449\n",
      "Episode:298 meanR:63.6700 R:157.0000 loss:8.4483 exploreP:0.3396\n",
      "Episode:299 meanR:64.3000 R:112.0000 loss:3.6220 exploreP:0.3360\n",
      "Episode:300 meanR:64.0000 R:36.0000 loss:13.1410 exploreP:0.3348\n",
      "Episode:301 meanR:65.4200 R:164.0000 loss:14.3327 exploreP:0.3295\n",
      "Episode:302 meanR:65.4600 R:31.0000 loss:12.1719 exploreP:0.3285\n",
      "Episode:303 meanR:65.6000 R:28.0000 loss:18.5799 exploreP:0.3276\n",
      "Episode:304 meanR:66.2800 R:113.0000 loss:20.2385 exploreP:0.3241\n",
      "Episode:305 meanR:66.4800 R:82.0000 loss:11.4171 exploreP:0.3215\n",
      "Episode:306 meanR:67.3700 R:110.0000 loss:14.4182 exploreP:0.3181\n",
      "Episode:307 meanR:68.0200 R:95.0000 loss:15.2874 exploreP:0.3152\n",
      "Episode:308 meanR:69.3600 R:178.0000 loss:3.3077 exploreP:0.3098\n",
      "Episode:309 meanR:70.5700 R:141.0000 loss:17.2819 exploreP:0.3056\n",
      "Episode:310 meanR:72.8700 R:264.0000 loss:63.0926 exploreP:0.2979\n",
      "Episode:311 meanR:74.7300 R:264.0000 loss:112.2196 exploreP:0.2904\n",
      "Episode:312 meanR:76.1200 R:179.0000 loss:190.5271 exploreP:0.2854\n",
      "Episode:313 meanR:77.6200 R:192.0000 loss:234.4814 exploreP:0.2802\n",
      "Episode:314 meanR:82.4000 R:500.0000 loss:172.3251 exploreP:0.2670\n",
      "Episode:315 meanR:83.1900 R:165.0000 loss:4.7612 exploreP:0.2628\n",
      "Episode:316 meanR:83.4100 R:90.0000 loss:1.6349 exploreP:0.2605\n",
      "Episode:317 meanR:83.5600 R:56.0000 loss:1.2201 exploreP:0.2591\n",
      "Episode:318 meanR:83.6200 R:40.0000 loss:21.2062 exploreP:0.2581\n",
      "Episode:319 meanR:84.4000 R:112.0000 loss:24.2588 exploreP:0.2554\n",
      "Episode:320 meanR:85.1700 R:101.0000 loss:3.6203 exploreP:0.2529\n",
      "Episode:321 meanR:86.9700 R:206.0000 loss:0.7076 exploreP:0.2480\n",
      "Episode:322 meanR:88.5800 R:187.0000 loss:0.8403 exploreP:0.2436\n",
      "Episode:323 meanR:92.8400 R:453.0000 loss:0.7972 exploreP:0.2332\n",
      "Episode:324 meanR:93.9000 R:132.0000 loss:0.6286 exploreP:0.2303\n",
      "Episode:325 meanR:94.3600 R:70.0000 loss:12.0055 exploreP:0.2287\n",
      "Episode:326 meanR:97.5700 R:344.0000 loss:2.1335 exploreP:0.2213\n",
      "Episode:327 meanR:97.5600 R:23.0000 loss:12.8131 exploreP:0.2209\n",
      "Episode:328 meanR:97.4300 R:27.0000 loss:25.0914 exploreP:0.2203\n",
      "Episode:329 meanR:99.9300 R:284.0000 loss:49.5288 exploreP:0.2144\n",
      "Episode:330 meanR:100.8700 R:127.0000 loss:100.4337 exploreP:0.2118\n",
      "Episode:331 meanR:105.2200 R:469.0000 loss:1.6819 exploreP:0.2026\n",
      "Episode:332 meanR:106.0600 R:112.0000 loss:18.8759 exploreP:0.2004\n",
      "Episode:333 meanR:108.2700 R:258.0000 loss:1.9106 exploreP:0.1956\n",
      "Episode:334 meanR:109.3200 R:133.0000 loss:2.4421 exploreP:0.1931\n",
      "Episode:335 meanR:110.4900 R:153.0000 loss:24.8997 exploreP:0.1904\n",
      "Episode:336 meanR:111.1400 R:102.0000 loss:3.3471 exploreP:0.1885\n",
      "Episode:337 meanR:112.8900 R:204.0000 loss:1.9026 exploreP:0.1849\n",
      "Episode:338 meanR:115.1000 R:252.0000 loss:3.5941 exploreP:0.1806\n",
      "Episode:339 meanR:116.7600 R:203.0000 loss:4.5350 exploreP:0.1771\n",
      "Episode:340 meanR:121.4400 R:500.0000 loss:1.3837 exploreP:0.1690\n",
      "Episode:341 meanR:126.1400 R:500.0000 loss:0.6351 exploreP:0.1612\n",
      "Episode:342 meanR:127.0600 R:120.0000 loss:41.3794 exploreP:0.1594\n",
      "Episode:343 meanR:129.1000 R:242.0000 loss:2.5271 exploreP:0.1559\n",
      "Episode:344 meanR:133.7300 R:500.0000 loss:0.6431 exploreP:0.1487\n",
      "Episode:345 meanR:135.3500 R:228.0000 loss:25.8203 exploreP:0.1456\n",
      "Episode:346 meanR:138.0200 R:337.0000 loss:2.3428 exploreP:0.1411\n",
      "Episode:347 meanR:137.7600 R:24.0000 loss:44.6773 exploreP:0.1408\n",
      "Episode:348 meanR:138.5300 R:124.0000 loss:42.0601 exploreP:0.1392\n",
      "Episode:349 meanR:138.3900 R:22.0000 loss:53.6056 exploreP:0.1389\n",
      "Episode:350 meanR:139.4900 R:200.0000 loss:21.7678 exploreP:0.1364\n",
      "Episode:351 meanR:141.6800 R:240.0000 loss:1.0298 exploreP:0.1334\n",
      "Episode:352 meanR:144.9200 R:338.0000 loss:0.6766 exploreP:0.1293\n",
      "Episode:353 meanR:148.9600 R:500.0000 loss:0.7909 exploreP:0.1234\n",
      "Episode:354 meanR:149.6900 R:151.0000 loss:42.9005 exploreP:0.1217\n",
      "Episode:355 meanR:150.3500 R:94.0000 loss:5.8615 exploreP:0.1207\n",
      "Episode:356 meanR:155.1300 R:500.0000 loss:1.5907 exploreP:0.1153\n",
      "Episode:357 meanR:159.7000 R:477.0000 loss:3.1982 exploreP:0.1104\n",
      "Episode:358 meanR:164.2700 R:500.0000 loss:0.8810 exploreP:0.1055\n",
      "Episode:359 meanR:164.3800 R:118.0000 loss:54.0463 exploreP:0.1044\n",
      "Episode:360 meanR:168.1700 R:500.0000 loss:2.0291 exploreP:0.0998\n",
      "Episode:361 meanR:171.5300 R:500.0000 loss:13.5517 exploreP:0.0954\n",
      "Episode:362 meanR:174.4700 R:500.0000 loss:14.2171 exploreP:0.0912\n",
      "Episode:363 meanR:174.3600 R:93.0000 loss:55.5742 exploreP:0.0905\n",
      "Episode:364 meanR:176.5400 R:309.0000 loss:10.2529 exploreP:0.0880\n",
      "Episode:365 meanR:181.0200 R:500.0000 loss:2.8419 exploreP:0.0842\n",
      "Episode:366 meanR:185.7400 R:500.0000 loss:14.5984 exploreP:0.0806\n",
      "Episode:367 meanR:187.1900 R:163.0000 loss:45.2832 exploreP:0.0795\n",
      "Episode:368 meanR:192.0700 R:500.0000 loss:1.7826 exploreP:0.0761\n",
      "Episode:369 meanR:195.9800 R:500.0000 loss:2.5465 exploreP:0.0729\n",
      "Episode:370 meanR:200.1100 R:500.0000 loss:15.4317 exploreP:0.0698\n",
      "Episode:371 meanR:203.8100 R:500.0000 loss:15.3791 exploreP:0.0669\n",
      "Episode:372 meanR:208.0100 R:500.0000 loss:14.8099 exploreP:0.0641\n",
      "Episode:373 meanR:208.8500 R:156.0000 loss:49.3629 exploreP:0.0633\n",
      "Episode:374 meanR:213.2500 R:500.0000 loss:3.0943 exploreP:0.0607\n",
      "Episode:375 meanR:212.9400 R:30.0000 loss:63.8958 exploreP:0.0605\n",
      "Episode:376 meanR:211.7400 R:21.0000 loss:123.8176 exploreP:0.0604\n",
      "Episode:377 meanR:213.7400 R:273.0000 loss:44.8560 exploreP:0.0590\n",
      "Episode:378 meanR:217.2800 R:500.0000 loss:0.8383 exploreP:0.0567\n",
      "Episode:379 meanR:221.5900 R:500.0000 loss:5.3125 exploreP:0.0544\n",
      "Episode:380 meanR:225.4300 R:500.0000 loss:15.8165 exploreP:0.0522\n",
      "Episode:381 meanR:227.4800 R:280.0000 loss:28.0765 exploreP:0.0511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:382 meanR:231.9000 R:500.0000 loss:5.1639 exploreP:0.0490\n",
      "Episode:383 meanR:234.6800 R:396.0000 loss:19.9895 exploreP:0.0475\n",
      "Episode:384 meanR:239.1600 R:500.0000 loss:6.4433 exploreP:0.0457\n",
      "Episode:385 meanR:244.0700 R:500.0000 loss:16.0802 exploreP:0.0440\n",
      "Episode:386 meanR:246.6000 R:263.0000 loss:30.0700 exploreP:0.0431\n",
      "Episode:387 meanR:245.0700 R:29.0000 loss:63.2757 exploreP:0.0430\n",
      "Episode:388 meanR:244.1200 R:113.0000 loss:72.0559 exploreP:0.0426\n",
      "Episode:389 meanR:247.8500 R:500.0000 loss:8.8695 exploreP:0.0410\n",
      "Episode:390 meanR:252.5700 R:500.0000 loss:15.3101 exploreP:0.0395\n",
      "Episode:391 meanR:255.9900 R:368.0000 loss:23.0226 exploreP:0.0384\n",
      "Episode:392 meanR:260.7600 R:500.0000 loss:7.4707 exploreP:0.0371\n",
      "Episode:393 meanR:262.8200 R:500.0000 loss:15.5268 exploreP:0.0357\n",
      "Episode:394 meanR:262.5500 R:158.0000 loss:47.0453 exploreP:0.0353\n",
      "Episode:395 meanR:265.6400 R:500.0000 loss:4.0197 exploreP:0.0341\n",
      "Episode:396 meanR:266.4600 R:109.0000 loss:61.5701 exploreP:0.0338\n",
      "Episode:397 meanR:264.5400 R:29.0000 loss:52.1844 exploreP:0.0338\n",
      "Episode:398 meanR:267.9700 R:500.0000 loss:13.7701 exploreP:0.0326\n",
      "Episode:399 meanR:271.4100 R:456.0000 loss:15.5682 exploreP:0.0316\n",
      "Episode:400 meanR:272.2900 R:124.0000 loss:9.9203 exploreP:0.0313\n",
      "Episode:401 meanR:272.5500 R:190.0000 loss:4.9188 exploreP:0.0309\n",
      "Episode:402 meanR:273.6200 R:138.0000 loss:11.8843 exploreP:0.0306\n",
      "Episode:403 meanR:276.5400 R:320.0000 loss:3.3424 exploreP:0.0300\n",
      "Episode:404 meanR:280.4100 R:500.0000 loss:5.3073 exploreP:0.0290\n",
      "Episode:405 meanR:284.5900 R:500.0000 loss:15.5935 exploreP:0.0281\n",
      "Episode:406 meanR:288.4900 R:500.0000 loss:15.5390 exploreP:0.0272\n",
      "Episode:407 meanR:292.5400 R:500.0000 loss:16.2365 exploreP:0.0264\n",
      "Episode:408 meanR:295.7600 R:500.0000 loss:15.1935 exploreP:0.0256\n",
      "Episode:409 meanR:299.3500 R:500.0000 loss:14.9264 exploreP:0.0248\n",
      "Episode:410 meanR:298.0000 R:129.0000 loss:59.4281 exploreP:0.0246\n",
      "Episode:411 meanR:300.3600 R:500.0000 loss:0.8769 exploreP:0.0239\n",
      "Episode:412 meanR:303.5700 R:500.0000 loss:15.5753 exploreP:0.0232\n",
      "Episode:413 meanR:306.6500 R:500.0000 loss:15.4167 exploreP:0.0226\n",
      "Episode:414 meanR:306.6500 R:500.0000 loss:15.0768 exploreP:0.0220\n",
      "Episode:415 meanR:307.7800 R:278.0000 loss:29.7155 exploreP:0.0216\n",
      "Episode:416 meanR:311.8800 R:500.0000 loss:5.0465 exploreP:0.0211\n",
      "Episode:417 meanR:312.7600 R:144.0000 loss:56.6707 exploreP:0.0209\n",
      "Episode:418 meanR:317.3600 R:500.0000 loss:0.6765 exploreP:0.0204\n",
      "Episode:419 meanR:321.2400 R:500.0000 loss:16.1368 exploreP:0.0199\n",
      "Episode:420 meanR:325.2300 R:500.0000 loss:9.4396 exploreP:0.0194\n",
      "Episode:421 meanR:324.8100 R:164.0000 loss:4.9022 exploreP:0.0192\n",
      "Episode:422 meanR:327.9400 R:500.0000 loss:0.9653 exploreP:0.0188\n",
      "Episode:423 meanR:328.4100 R:500.0000 loss:14.5593 exploreP:0.0184\n",
      "Episode:424 meanR:332.0900 R:500.0000 loss:16.5796 exploreP:0.0180\n",
      "Episode:425 meanR:333.2500 R:186.0000 loss:43.5610 exploreP:0.0178\n",
      "Episode:426 meanR:334.8100 R:500.0000 loss:1.1933 exploreP:0.0174\n",
      "Episode:427 meanR:339.5800 R:500.0000 loss:16.3915 exploreP:0.0171\n",
      "Episode:428 meanR:344.3100 R:500.0000 loss:8.0710 exploreP:0.0167\n",
      "Episode:429 meanR:346.4700 R:500.0000 loss:12.4438 exploreP:0.0164\n",
      "Episode:430 meanR:350.2000 R:500.0000 loss:16.6366 exploreP:0.0161\n",
      "Episode:431 meanR:350.5100 R:500.0000 loss:15.7423 exploreP:0.0158\n",
      "Episode:432 meanR:354.3900 R:500.0000 loss:9.2867 exploreP:0.0155\n",
      "Episode:433 meanR:356.7800 R:497.0000 loss:9.5478 exploreP:0.0152\n",
      "Episode:434 meanR:360.4500 R:500.0000 loss:0.8499 exploreP:0.0150\n",
      "Episode:435 meanR:363.9200 R:500.0000 loss:9.3082 exploreP:0.0147\n",
      "Episode:436 meanR:367.9000 R:500.0000 loss:10.0006 exploreP:0.0145\n",
      "Episode:437 meanR:370.8600 R:500.0000 loss:1.0487 exploreP:0.0143\n",
      "Episode:438 meanR:373.3400 R:500.0000 loss:18.5015 exploreP:0.0141\n",
      "Episode:439 meanR:376.3100 R:500.0000 loss:17.9978 exploreP:0.0139\n",
      "Episode:440 meanR:373.7100 R:240.0000 loss:27.8784 exploreP:0.0138\n",
      "Episode:441 meanR:373.7100 R:500.0000 loss:0.9084 exploreP:0.0136\n",
      "Episode:442 meanR:377.5100 R:500.0000 loss:7.2711 exploreP:0.0134\n",
      "Episode:443 meanR:380.0900 R:500.0000 loss:14.6007 exploreP:0.0133\n",
      "Episode:444 meanR:380.0900 R:500.0000 loss:10.6582 exploreP:0.0131\n",
      "Episode:445 meanR:382.8100 R:500.0000 loss:16.0611 exploreP:0.0129\n",
      "Episode:446 meanR:384.4400 R:500.0000 loss:14.2520 exploreP:0.0128\n",
      "Episode:447 meanR:388.0500 R:385.0000 loss:21.4933 exploreP:0.0127\n",
      "Episode:448 meanR:391.1500 R:434.0000 loss:1.7410 exploreP:0.0126\n",
      "Episode:449 meanR:395.9300 R:500.0000 loss:0.6033 exploreP:0.0125\n",
      "Episode:450 meanR:398.9300 R:500.0000 loss:14.7502 exploreP:0.0123\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                  feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                               model.initial_state: initial_state})\n",
    "            # Explore (Env) or Exploit (Model): NO\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            # if explore_p > np.random.rand():\n",
    "            #     action = env.action_space.sample()\n",
    "            # else:\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            memory.states.append([initial_state, final_state])\n",
    "            total_reward += reward\n",
    "            initial_state = final_state\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            #batch, rnn_states = memory.sample(batch_size)\n",
    "            batch = memory.buffer\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rnn_states = memory.states\n",
    "            initial_states = np.array([each[0] for each in rnn_states])\n",
    "            final_states = np.array([each[1] for each in rnn_states])\n",
    "            next_actions_logits = sess.run(model.actions_logits, \n",
    "                                           feed_dict = {model.states: next_states, \n",
    "                                                        model.initial_state: final_states[0].reshape([1, -1])})\n",
    "            nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                                     model.actions: actions,\n",
    "                                                                     model.targetQs: targetQs,\n",
    "                                                        model.initial_state: initial_states[0].reshape([1, -1])})\n",
    "            loss_batch.append(loss)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward:120.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-seq.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "    \n",
    "    # Episode/epoch\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                    feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                                 model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # At the end of each episode\n",
    "        print('total_reward:{}'.format(total_reward))\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
