{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score and steps: 0.0 and 299\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "for steps in range(1111111):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     #print(state)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# batch = []\n",
    "# while True: # infinite number of steps\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     #print(state, action, reward, done)\n",
    "#     batch.append([action, state, reward, done])\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Act', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Env', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        states_logits = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)\n",
    "        Qlogits = tf.layers.dense(inputs=nl2, units=1, trainable=False)\n",
    "        return states_logits, Qlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    ################################################ a = act(s)\n",
    "    actions_logits = Act(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ################################################ s', r = env(s, a)\n",
    "    ################################################ s', Q = env(s, a)\n",
    "    ################################################ ~s', ~Q = env(s, ~a)\n",
    "    e_next_states_logits, eQs = Env(actions=actions_labels, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size)\n",
    "    a_next_states_logits, aQs = Env(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    eloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=e_next_states_logits, \n",
    "                                                                   labels=next_states_labels))\n",
    "    eloss += -tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                     labels=next_states_labels)) # maximize loss\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                    labels=next_states_labels)) # minimize loss\n",
    "    #eQs_logits = tf.reshape(eQs, shape=[-1])\n",
    "    eQs_logits = tf.nn.tanh(tf.reshape(eQs, shape=[-1]))\n",
    "    aQs_logits = tf.reshape(aQs, shape=[-1])\n",
    "    #################################################### s'', Q' = ~env(s', ~a')\n",
    "    next_actions_logits = Act(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    _, aQs2 = Env(actions=next_actions_logits, states=next_states, hidden_size=hidden_size, \n",
    "                  action_size=action_size, state_size=state_size, reuse=True)\n",
    "    aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "    #     eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=eQs_logits, # GAN\n",
    "    #                                                                     labels=rates)) # 0-1 real\n",
    "    eloss += tf.reduce_mean(tf.square(eQs_logits-rates)) # [-1, +1]\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                    labels=tf.zeros_like(rates))) # min\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                     labels=tf.ones_like(rates))) # max\n",
    "    #     ###################################################### Q(s,a)= r + Q'(s',a') # max\n",
    "    #     ###################################################### ~Q(s,~a)= r # min\n",
    "    #     ###################################################### ~Q(s,~a)= r + Q'(s',a') # max\n",
    "    #     targetQs = rewards + (gamma * aQs2_logits)\n",
    "    #     eloss += tf.reduce_mean(tf.square(eQs_logits - targetQs)) # real\n",
    "    #     eloss += tf.reduce_mean((aQs_logits+aQs2_logits)/2) # min\n",
    "    #     aloss2 += -tf.reduce_mean((aQs_logits+aQs2_logits)/2) # max\n",
    "    return actions_logits, aloss, eloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, e_loss, a_loss2, a_learning_rate, e_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('Act')]\n",
    "    e_vars = [var for var in t_vars if var.name.startswith('Env')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        e_opt = tf.train.AdamOptimizer(e_learning_rate).minimize(e_loss, var_list=e_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, e_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, e_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.e_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.e_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        e_loss=self.e_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        e_learning_rate=e_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 'continuous', 4, 'discrete')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_observation_space_type, \\\n",
    "brain.vector_action_space_size, brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "e_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(5e3)             # experience mini-batch size: 300*10 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              a_learning_rate=a_learning_rate, e_learning_rate=e_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.00299 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.00599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.00899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.01199 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.01499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.01799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.02099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.02399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.02699 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.02999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.03299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.03599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.03899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.04199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.04499 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.04799 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.05099 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.05399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.05699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.05999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.06299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.06599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.06899 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.07199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.07499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.07799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.08099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.08399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.08699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.08999 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.09299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.09599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.09899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.10199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.10499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.10799 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.11099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.11399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.11699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.11999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.12299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.12599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.12899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.13199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.13499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.13799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.14099 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.14399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.14699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.14999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.15299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.15599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.15899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.16199 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.16499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.16799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.17099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.17399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.17699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.17999 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.18299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.18599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.18899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.19199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.19499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.19799 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.20099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.20399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.20699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.20999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.21299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.21599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.21899 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.22199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.22499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.22799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.23099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.23399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.23699 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.23999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.24299 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.24599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.24899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.25199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.25499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.25799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.26099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.26399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.26699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.26999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.27299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.27599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.27899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.28199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.28499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.28799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.29099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.29399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.29699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.29999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.30299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.30599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.30899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.31199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.31499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.31799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.32099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.32399 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.32699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.32999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.33299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.33599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.33899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.34199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.34499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.34799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.35099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.35399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.35699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.35999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.36299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.36599 rate: 0.07692307692307693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.36899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.37199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.37499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.37799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.38099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.38399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.38699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.38999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.39299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.39599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.39899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.40199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.40499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.40799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.41099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.41399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.41699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.41999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.42299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.42599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.42899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.43199 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.43499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.43799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.45299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.45599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.45899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.46199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.46499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.46799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.48299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.48599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.48899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.49199 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.49499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.49799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.50099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.50399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.50699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.50999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.51299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.51599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.51899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.52199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.52499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.52799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.53099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.53399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.53699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.53999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.54299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.54599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.54899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.55199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.55499 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.55799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.56099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.56399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.56699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.56999 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.57299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.57599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.57899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.58199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.58499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.58799 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.59099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.59399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.59699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.59999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.60299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.60599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.60899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.61199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.61499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.61799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.62099 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.62399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.62699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.62999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.63299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.63599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.63899 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.64199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.64499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.64799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.65099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.65399 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.65699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.65999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.66299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.66599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.66899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.67199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.67499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.67799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.69299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.69599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.69899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.70199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.70499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.70799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.71099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.71399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.71699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.71999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.72299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.72599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.72899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.73199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.73499 rate: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.73799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.74099 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.74399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.74699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.74999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.75299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.75599 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.75899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.76199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.76499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.76799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.77099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.77399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.77699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.77999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.78299 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.78599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.78899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.79199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.79499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.79799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.80099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.80399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.80699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.80999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.81299 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.81599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.81899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.82199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.82499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.82799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.83099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.83399 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.83699 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.83999 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.84299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.84599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.84899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.85199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.85499 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.85799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.86099 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.86399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.86699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.86999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.87299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.87599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.87899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.88199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.88499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.88799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.89099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.89399 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.89699 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.89999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.90299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.90599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.90899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.91199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.91499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.91799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.92099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92699 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.92999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.94199 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.94499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.94799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.95099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.95399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.95699 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.95999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.96299 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.96599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.96899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.97199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.97499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.97799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.98099 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.98399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.98699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.98999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99899 rate: 0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "# state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        # state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        total_reward = 0 # reset\n",
    "        print('number of steps per episode:', num_step, \n",
    "              '%gone:', each/memory_size, 'rate:', rate)\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4109 eloss:0.7335 aloss2:1.4176 exploreP:0.9707\n",
      "Episode:1 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4017 eloss:0.7055 aloss2:1.4410 exploreP:0.9423\n",
      "Episode:2 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4339 eloss:0.7029 aloss2:1.4461 exploreP:0.9148\n",
      "Episode:3 meanR:0.5000 R:2.0000 rate:0.1538 aloss:1.4493 eloss:0.7015 aloss2:1.4515 exploreP:0.8881\n",
      "Episode:4 meanR:0.8000 R:2.0000 rate:0.1538 aloss:1.4587 eloss:0.7007 aloss2:1.4507 exploreP:0.8621\n",
      "Episode:5 meanR:0.8333 R:1.0000 rate:0.0769 aloss:1.4407 eloss:0.6968 aloss2:1.4539 exploreP:0.8369\n",
      "Episode:6 meanR:1.0000 R:2.0000 rate:0.1538 aloss:1.4665 eloss:0.6960 aloss2:1.4523 exploreP:0.8125\n",
      "Episode:7 meanR:0.8750 R:0.0000 rate:0.0000 aloss:1.4591 eloss:0.6941 aloss2:1.4595 exploreP:0.7888\n",
      "Episode:8 meanR:1.0000 R:2.0000 rate:0.1538 aloss:1.4434 eloss:0.6817 aloss2:1.4672 exploreP:0.7657\n",
      "Episode:9 meanR:0.8000 R:-1.0000 rate:-0.0769 aloss:1.4402 eloss:0.6812 aloss2:1.4687 exploreP:0.7434\n",
      "Episode:10 meanR:0.6364 R:-1.0000 rate:-0.0769 aloss:1.4377 eloss:0.6801 aloss2:1.4689 exploreP:0.7217\n",
      "Episode:11 meanR:0.4167 R:-2.0000 rate:-0.1538 aloss:1.4292 eloss:0.6714 aloss2:1.4751 exploreP:0.7007\n",
      "Episode:12 meanR:0.2308 R:-2.0000 rate:-0.1538 aloss:1.4274 eloss:0.6742 aloss2:1.4721 exploreP:0.6803\n",
      "Episode:13 meanR:0.2857 R:1.0000 rate:0.0769 aloss:1.4254 eloss:0.6683 aloss2:1.4742 exploreP:0.6605\n",
      "Episode:14 meanR:0.3333 R:1.0000 rate:0.0769 aloss:1.4319 eloss:0.6663 aloss2:1.4773 exploreP:0.6413\n",
      "Episode:15 meanR:0.3750 R:1.0000 rate:0.0769 aloss:1.4415 eloss:0.6624 aloss2:1.4815 exploreP:0.6226\n",
      "Episode:16 meanR:0.4706 R:2.0000 rate:0.1538 aloss:1.4411 eloss:0.6609 aloss2:1.4836 exploreP:0.6045\n",
      "Episode:17 meanR:0.4444 R:0.0000 rate:0.0000 aloss:1.4468 eloss:0.6581 aloss2:1.4870 exploreP:0.5869\n",
      "Episode:18 meanR:0.4737 R:1.0000 rate:0.0769 aloss:1.4507 eloss:0.6528 aloss2:1.4958 exploreP:0.5699\n",
      "Episode:19 meanR:0.5000 R:1.0000 rate:0.0769 aloss:1.4490 eloss:0.6542 aloss2:1.4975 exploreP:0.5533\n",
      "Episode:20 meanR:0.4286 R:-1.0000 rate:-0.0769 aloss:1.4491 eloss:0.6532 aloss2:1.4975 exploreP:0.5373\n",
      "Episode:21 meanR:0.5000 R:2.0000 rate:0.1538 aloss:1.4515 eloss:0.6556 aloss2:1.4990 exploreP:0.5217\n",
      "Episode:22 meanR:0.5652 R:2.0000 rate:0.1538 aloss:1.4518 eloss:0.6524 aloss2:1.5024 exploreP:0.5066\n",
      "Episode:23 meanR:0.5000 R:-1.0000 rate:-0.0769 aloss:1.4485 eloss:0.6453 aloss2:1.5124 exploreP:0.4919\n",
      "Episode:24 meanR:0.4000 R:-2.0000 rate:-0.1538 aloss:1.4425 eloss:0.6478 aloss2:1.5134 exploreP:0.4776\n",
      "Episode:25 meanR:0.3077 R:-2.0000 rate:-0.1538 aloss:1.4492 eloss:0.6415 aloss2:1.5206 exploreP:0.4638\n",
      "Episode:26 meanR:0.3333 R:1.0000 rate:0.0769 aloss:1.4443 eloss:0.6429 aloss2:1.5238 exploreP:0.4504\n",
      "Episode:27 meanR:0.3571 R:1.0000 rate:0.0769 aloss:1.4466 eloss:0.6392 aloss2:1.5303 exploreP:0.4374\n",
      "Episode:28 meanR:0.2759 R:-2.0000 rate:-0.1538 aloss:1.4464 eloss:0.6379 aloss2:1.5334 exploreP:0.4248\n",
      "Episode:29 meanR:0.2333 R:-1.0000 rate:-0.0769 aloss:1.4486 eloss:0.6373 aloss2:1.5365 exploreP:0.4125\n",
      "Episode:30 meanR:0.2903 R:2.0000 rate:0.1538 aloss:1.4471 eloss:0.6364 aloss2:1.5432 exploreP:0.4006\n",
      "Episode:31 meanR:0.2500 R:-1.0000 rate:-0.0769 aloss:1.4435 eloss:0.6292 aloss2:1.5564 exploreP:0.3891\n",
      "Episode:32 meanR:0.2121 R:-1.0000 rate:-0.0769 aloss:1.4430 eloss:0.6211 aloss2:1.5751 exploreP:0.3779\n",
      "Episode:33 meanR:0.2647 R:2.0000 rate:0.1538 aloss:1.4439 eloss:0.6105 aloss2:1.5949 exploreP:0.3670\n",
      "Episode:34 meanR:0.2571 R:0.0000 rate:0.0000 aloss:1.4424 eloss:0.6108 aloss2:1.6022 exploreP:0.3564\n",
      "Episode:35 meanR:0.2778 R:1.0000 rate:0.0769 aloss:1.4375 eloss:0.6085 aloss2:1.6105 exploreP:0.3462\n",
      "Episode:36 meanR:0.2703 R:0.0000 rate:0.0000 aloss:1.4403 eloss:0.6010 aloss2:1.6247 exploreP:0.3363\n",
      "Episode:37 meanR:0.2632 R:0.0000 rate:0.0000 aloss:1.4354 eloss:0.6074 aloss2:1.6142 exploreP:0.3266\n",
      "Episode:38 meanR:0.2564 R:0.0000 rate:0.0000 aloss:1.4318 eloss:0.6156 aloss2:1.5838 exploreP:0.3173\n",
      "Episode:39 meanR:0.2250 R:-1.0000 rate:-0.0769 aloss:1.4423 eloss:0.6195 aloss2:1.5729 exploreP:0.3082\n",
      "Episode:40 meanR:0.1463 R:-3.0000 rate:-0.2308 aloss:1.4417 eloss:0.6221 aloss2:1.5720 exploreP:0.2994\n",
      "Episode:41 meanR:0.0952 R:-2.0000 rate:-0.1538 aloss:1.4425 eloss:0.6217 aloss2:1.5764 exploreP:0.2908\n",
      "Episode:42 meanR:0.0698 R:-1.0000 rate:-0.0769 aloss:1.4405 eloss:0.6161 aloss2:1.5911 exploreP:0.2825\n",
      "Episode:43 meanR:0.0909 R:1.0000 rate:0.0769 aloss:1.4427 eloss:0.6150 aloss2:1.6010 exploreP:0.2745\n",
      "Episode:44 meanR:0.0667 R:-1.0000 rate:-0.0769 aloss:1.4466 eloss:0.6118 aloss2:1.6127 exploreP:0.2666\n",
      "Episode:45 meanR:-0.0217 R:-4.0000 rate:-0.3077 aloss:1.4344 eloss:0.6040 aloss2:1.6324 exploreP:0.2591\n",
      "Episode:46 meanR:-0.0213 R:0.0000 rate:0.0000 aloss:1.4368 eloss:0.6045 aloss2:1.6416 exploreP:0.2517\n",
      "Episode:47 meanR:-0.0625 R:-2.0000 rate:-0.1538 aloss:1.4429 eloss:0.5944 aloss2:1.6632 exploreP:0.2446\n",
      "Episode:48 meanR:-0.0612 R:0.0000 rate:0.0000 aloss:1.4377 eloss:0.5868 aloss2:1.6886 exploreP:0.2376\n",
      "Episode:49 meanR:-0.0800 R:-1.0000 rate:-0.0769 aloss:1.4323 eloss:0.5816 aloss2:1.7034 exploreP:0.2309\n",
      "Episode:50 meanR:-0.0784 R:0.0000 rate:0.0000 aloss:1.4193 eloss:0.5777 aloss2:1.7118 exploreP:0.2244\n",
      "Episode:51 meanR:-0.1538 R:-4.0000 rate:-0.3077 aloss:1.4169 eloss:0.5747 aloss2:1.7298 exploreP:0.2180\n",
      "Episode:52 meanR:-0.2453 R:-5.0000 rate:-0.3846 aloss:1.4163 eloss:0.5696 aloss2:1.7431 exploreP:0.2119\n",
      "Episode:53 meanR:-0.2222 R:1.0000 rate:0.0769 aloss:1.4142 eloss:0.5671 aloss2:1.7557 exploreP:0.2059\n",
      "Episode:54 meanR:-0.2545 R:-2.0000 rate:-0.1538 aloss:1.4240 eloss:0.5599 aloss2:1.7726 exploreP:0.2001\n",
      "Episode:55 meanR:-0.2679 R:-1.0000 rate:-0.0769 aloss:1.4148 eloss:0.5600 aloss2:1.7396 exploreP:0.1945\n",
      "Episode:56 meanR:-0.2632 R:0.0000 rate:0.0000 aloss:1.4114 eloss:0.5649 aloss2:1.7241 exploreP:0.1891\n",
      "Episode:57 meanR:-0.2931 R:-2.0000 rate:-0.1538 aloss:1.4200 eloss:0.5696 aloss2:1.7214 exploreP:0.1838\n",
      "Episode:58 meanR:-0.3220 R:-2.0000 rate:-0.1538 aloss:1.4258 eloss:0.5688 aloss2:1.7295 exploreP:0.1786\n",
      "Episode:59 meanR:-0.3500 R:-2.0000 rate:-0.1538 aloss:1.4169 eloss:0.5645 aloss2:1.7390 exploreP:0.1736\n",
      "Episode:60 meanR:-0.3279 R:1.0000 rate:0.0769 aloss:1.4200 eloss:0.5633 aloss2:1.7474 exploreP:0.1688\n",
      "Episode:61 meanR:-0.3387 R:-1.0000 rate:-0.0769 aloss:1.4140 eloss:0.5599 aloss2:1.7579 exploreP:0.1641\n",
      "Episode:62 meanR:-0.3651 R:-2.0000 rate:-0.1538 aloss:1.4230 eloss:0.5608 aloss2:1.7581 exploreP:0.1596\n",
      "Episode:63 meanR:-0.3594 R:0.0000 rate:0.0000 aloss:1.4003 eloss:0.5605 aloss2:1.7629 exploreP:0.1551\n",
      "Episode:64 meanR:-0.4000 R:-3.0000 rate:-0.2308 aloss:1.4075 eloss:0.5542 aloss2:1.7702 exploreP:0.1509\n",
      "Episode:65 meanR:-0.3939 R:0.0000 rate:0.0000 aloss:1.4071 eloss:0.5509 aloss2:1.7792 exploreP:0.1467\n",
      "Episode:66 meanR:-0.3881 R:0.0000 rate:0.0000 aloss:1.4099 eloss:0.5422 aloss2:1.7959 exploreP:0.1426\n",
      "Episode:67 meanR:-0.3529 R:2.0000 rate:0.1538 aloss:1.4027 eloss:0.5391 aloss2:1.8032 exploreP:0.1387\n",
      "Episode:68 meanR:-0.3333 R:1.0000 rate:0.0769 aloss:1.3982 eloss:0.5394 aloss2:1.8038 exploreP:0.1349\n",
      "Episode:69 meanR:-0.3429 R:-1.0000 rate:-0.0769 aloss:1.3883 eloss:0.5373 aloss2:1.8099 exploreP:0.1312\n",
      "Episode:70 meanR:-0.3662 R:-2.0000 rate:-0.1538 aloss:1.4105 eloss:0.5253 aloss2:1.8308 exploreP:0.1276\n",
      "Episode:71 meanR:-0.3750 R:-1.0000 rate:-0.0769 aloss:1.4110 eloss:0.5211 aloss2:1.8456 exploreP:0.1242\n",
      "Episode:72 meanR:-0.3699 R:0.0000 rate:0.0000 aloss:1.3872 eloss:0.5228 aloss2:1.8514 exploreP:0.1208\n",
      "Episode:73 meanR:-0.3378 R:2.0000 rate:0.1538 aloss:1.3830 eloss:0.5172 aloss2:1.8687 exploreP:0.1175\n",
      "Episode:74 meanR:-0.3067 R:2.0000 rate:0.1538 aloss:1.3956 eloss:0.5084 aloss2:1.8880 exploreP:0.1143\n",
      "Episode:75 meanR:-0.3026 R:0.0000 rate:0.0000 aloss:1.4034 eloss:0.5041 aloss2:1.9092 exploreP:0.1113\n",
      "Episode:76 meanR:-0.2987 R:0.0000 rate:0.0000 aloss:1.4114 eloss:0.4974 aloss2:1.9252 exploreP:0.1083\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, eloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        aloss_batch, eloss_batch, aloss2_batch = [], [], []\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        num_step = 0 # each episode\n",
    "        rate = -1\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                if np.count_nonzero(dones) > 0 and len(dones) > 1 and np.max(rates) > 0:\n",
    "                    break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            eloss, _ = sess.run([model.e_loss, model.e_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print(len(dones), np.count_nonzero(dones), np.max(rates))\n",
    "            aloss_batch.append(aloss)\n",
    "            eloss_batch.append(eloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # Rating the memory\n",
    "        #if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1) # [-1, +1]\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                    \n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'eloss:{:.4f}'.format(np.mean(eloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        eloss_list.append([ep, np.mean(eloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss2_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(11):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
