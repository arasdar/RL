{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/home/arasdar/VisualBanana_Linux/Banana.x86\")\n",
    "# env = UnityEnvironment(file_name=\"/home/arasdar/unity-envs/Banana_Linux/Banana.x86_64\")\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "# print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "# print(state.shape, len(env_info.vector_observations), env_info.vector_observations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n",
      "Score and steps: 0.0 and 299\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "for steps in range(1111111):\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        print(state.shape)\n",
    "        break\n",
    "    \n",
    "print(\"Score and steps: {} and {}\".format(score, steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     #print(state)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# batch = []\n",
    "# while True: # infinite number of steps\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     #print(state, action, reward, done)\n",
    "#     batch.append([action, state, reward, done])\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Act', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Env(states, actions, state_size, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('Env', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        states_logits = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)\n",
    "        Qlogits = tf.layers.dense(inputs=nl2, units=1, trainable=False)\n",
    "        return states_logits, Qlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    ################################################ a = act(s)\n",
    "    actions_logits = Act(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ################################################ s', r = env(s, a)\n",
    "    e_next_states_logits, eQs = Env(actions=actions_labels, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size)\n",
    "    a_next_states_logits, aQs = Env(actions=actions_logits, states=states, hidden_size=hidden_size, \n",
    "                                    action_size=action_size, state_size=state_size, reuse=True)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    eloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=e_next_states_logits, # GQN\n",
    "                                                                   labels=next_states_labels))\n",
    "    eloss += -tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                     labels=next_states_labels)) # maximize loss\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a_next_states_logits, \n",
    "                                                                    labels=next_states_labels)) # minimize loss\n",
    "    eQs_logits = tf.reshape(eQs, shape=[-1])\n",
    "    aQs_logits = tf.reshape(aQs, shape=[-1])\n",
    "    #################################################### s'', Q' = ~env(s', ~a')\n",
    "    next_actions_logits = Act(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    _, aQs2 = Env(actions=next_actions_logits, states=next_states, hidden_size=hidden_size, \n",
    "                  action_size=action_size, state_size=state_size, reuse=True)\n",
    "    aQs2_logits = tf.reshape(aQs2, shape=[-1]) * (1-dones)\n",
    "    rates_labels = (rates+1)/2 # -1->0, 0->0.5, 1->1: tanh->sigmoid\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=eQs_logits, # GAN\n",
    "                                                                    labels=rates_labels)) # 0-1 real\n",
    "    eloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                    labels=tf.zeros_like(rates))) # min\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=(aQs_logits+aQs2_logits)/2, # GAN\n",
    "                                                                     labels=tf.ones_like(rates))) # max\n",
    "    ###################################################### Q(s,a)= r + Q'(s',a')\n",
    "    targetQs = rewards + (gamma * aQs2_logits) # DQN/Qlearning\n",
    "    eloss += tf.reduce_mean(tf.square(eQs_logits - targetQs)) # real\n",
    "    eloss += tf.reduce_mean(tf.square(aQs_logits - rewards)) # min\n",
    "    aloss2 += tf.reduce_mean(tf.square(aQs_logits - targetQs)) # max\n",
    "    eloss += tf.reduce_mean((aQs_logits+aQs2_logits)/2) # min\n",
    "    aloss2 += -tf.reduce_mean((aQs_logits+aQs2_logits)/2) # max\n",
    "    return actions_logits, aloss, eloss, aloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, e_loss, a_loss2, a_learning_rate, e_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('Act')]\n",
    "    e_vars = [var for var in t_vars if var.name.startswith('Env')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        e_opt = tf.train.AdamOptimizer(e_learning_rate).minimize(e_loss, var_list=e_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, e_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, e_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.e_loss, self.a_loss2 = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.e_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        e_loss=self.e_loss,\n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        e_learning_rate=e_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch\n",
    "#     def sample(self, batch_size):\n",
    "#         idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "#         return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 'continuous', 4, 'discrete')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_size, brain.vector_observation_space_type, \\\n",
    "brain.vector_action_space_size, brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "hidden_size = 37*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "e_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 300*10 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              a_learning_rate=a_learning_rate, e_learning_rate=e_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.00299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.00599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.00899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.01199 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.01499 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.01799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.02099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.02399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.02699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.02999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.03299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.03599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.03899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.04199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.04499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.04799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.05099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.05399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.05699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.05999 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.06299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.06599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.06899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.07199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.07499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.07799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.08099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.08399 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.08699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.08999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.09299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.09599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.09899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.10199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.10499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.10799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.11099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.11399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.11699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.11999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.12299 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.12599 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.12899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.13199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.13499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.13799 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.14099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.14399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.14699 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.14999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.15299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.15599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.15899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.16199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.16499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.16799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.17099 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.17399 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.17699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.17999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.18299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.18599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.18899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.19199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.19499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.19799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.20099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.20399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.20699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.20999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.21299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.21599 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.21899 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.22199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.22499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.22799 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.23099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.23399 rate: -0.3076923076923077\n",
      "number of steps per episode: 300 %gone: 0.23699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.23999 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.24299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.24599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.24899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.25199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.25499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.25799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.26099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.26399 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.26699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.26999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.27299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.27599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.27899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.28199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.28499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.28799 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.29099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.29399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.29699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.29999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.30299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.30599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.30899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.31199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.31499 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.31799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.32099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.32399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.32699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.32999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.33299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.33599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.33899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.34199 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.34499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.34799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.35099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.35399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.35699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.35999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.36299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.36599 rate: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.36899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.37199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.37499 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.37799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.38099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.38399 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.38699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.38999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.39299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.39599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.39899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.40199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.40499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.40799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.41099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.41399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.41699 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.41999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.42299 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.42599 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.42899 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.43199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.43499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.43799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44099 rate: -0.3076923076923077\n",
      "number of steps per episode: 300 %gone: 0.44399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.44999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.45299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.45599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.45899 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.46199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.46499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.46799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.47099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.47699 rate: -0.3076923076923077\n",
      "number of steps per episode: 300 %gone: 0.47999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.48299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.48599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.48899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.49199 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.49499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.49799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.50099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.50399 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.50699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.50999 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.51299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.51599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.51899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.52199 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.52499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.52799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.53099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.53399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.53699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.53999 rate: 0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.54299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.54599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.54899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.55199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.55499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.55799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.56099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.56399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.56699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.56999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.57299 rate: -0.23076923076923078\n",
      "number of steps per episode: 300 %gone: 0.57599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.57899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.58199 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.58499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.58799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.59099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.59399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.59699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.59999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.60299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.60599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.60899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.61199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.61499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.61799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.62099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.62399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.62699 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.62999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.63299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.63599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.63899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.64199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.64499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.64799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.65099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.65399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.65699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.65999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.66299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.66599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.66899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.67199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.67499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.67799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.68099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.68999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.69299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.69599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.69899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.70199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.70499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.70799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.71099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.71399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.71699 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.71999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.72299 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.72599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.72899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.73199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.73499 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.73799 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.74099 rate: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of steps per episode: 300 %gone: 0.74399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.74699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.74999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.75299 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.75599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.75899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.76199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.76499 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.76799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.77099 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.77399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.77699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.77999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.78299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.78599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.78899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.79199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.79499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.79799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.80099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.80399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.80699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.80999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.81299 rate: -0.3076923076923077\n",
      "number of steps per episode: 300 %gone: 0.81599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.81899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.82199 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.82499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.82799 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.83099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.83399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.83699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.83999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.84299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.84599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.84899 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.85199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.85499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.85799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.86099 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.86399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.86699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.86999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.87299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.87599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.87899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.88199 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.88499 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.88799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.89099 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.89399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.89699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.89999 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.90299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.90599 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.90899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.91199 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.91499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.91799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92399 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.92999 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93299 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93599 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.93899 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.94199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.94499 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.94799 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.95099 rate: 0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.95399 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.95699 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.95999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.96299 rate: -0.15384615384615385\n",
      "number of steps per episode: 300 %gone: 0.96599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.96899 rate: 0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.97199 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.97499 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.97799 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.98099 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.98399 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.98699 rate: -0.07692307692307693\n",
      "number of steps per episode: 300 %gone: 0.98999 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99299 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99599 rate: 0.0\n",
      "number of steps per episode: 300 %gone: 0.99899 rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# state = env.reset()\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]   # get the state\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for each in range(memory_size):\n",
    "    # action = env.action_space.sample()\n",
    "    # next_state, reward, done, _ = env.step(action)\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1)\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        # state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the state\n",
    "        total_reward = 0 # reset\n",
    "        print('number of steps per episode:', num_step, \n",
    "              '%gone:', each/memory_size, 'rate:', rate)\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:1.0000 R:1.0000 rate:0.0769 aloss:1.4319 eloss:1.2352 aloss2:1.7882 exploreP:0.9707\n",
      "Episode:1 meanR:0.0000 R:-1.0000 rate:-0.0769 aloss:1.4144 eloss:1.0427 aloss2:2.1762 exploreP:0.9423\n",
      "Episode:2 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.4253 eloss:1.0041 aloss2:2.2476 exploreP:0.9148\n",
      "Episode:3 meanR:0.2500 R:1.0000 rate:0.0769 aloss:1.3987 eloss:0.9904 aloss2:2.2847 exploreP:0.8881\n",
      "Episode:4 meanR:0.2000 R:0.0000 rate:0.0000 aloss:1.4008 eloss:0.9888 aloss2:2.2936 exploreP:0.8621\n",
      "Episode:5 meanR:0.3333 R:1.0000 rate:0.0769 aloss:1.4118 eloss:0.9850 aloss2:2.2956 exploreP:0.8369\n",
      "Episode:6 meanR:0.2857 R:0.0000 rate:0.0000 aloss:1.4169 eloss:0.9847 aloss2:2.3011 exploreP:0.8125\n",
      "Episode:7 meanR:0.5000 R:2.0000 rate:0.1538 aloss:1.4297 eloss:0.9865 aloss2:2.3042 exploreP:0.7888\n",
      "Episode:8 meanR:0.5556 R:1.0000 rate:0.0769 aloss:1.4372 eloss:0.9857 aloss2:2.3077 exploreP:0.7657\n",
      "Episode:9 meanR:0.5000 R:0.0000 rate:0.0000 aloss:1.4528 eloss:0.9861 aloss2:2.3126 exploreP:0.7434\n",
      "Episode:10 meanR:0.4545 R:0.0000 rate:0.0000 aloss:1.4582 eloss:0.9899 aloss2:2.3167 exploreP:0.7217\n",
      "Episode:11 meanR:0.5000 R:1.0000 rate:0.0769 aloss:1.4570 eloss:0.9848 aloss2:2.3097 exploreP:0.7007\n",
      "Episode:12 meanR:0.4615 R:0.0000 rate:0.0000 aloss:1.4663 eloss:0.9837 aloss2:2.3173 exploreP:0.6803\n",
      "Episode:13 meanR:0.5000 R:1.0000 rate:0.0769 aloss:1.4563 eloss:0.9818 aloss2:2.3150 exploreP:0.6605\n",
      "Episode:14 meanR:0.6000 R:2.0000 rate:0.1538 aloss:1.4504 eloss:0.9825 aloss2:2.3138 exploreP:0.6413\n",
      "Episode:15 meanR:0.9375 R:6.0000 rate:0.4615 aloss:1.4589 eloss:0.9828 aloss2:2.3148 exploreP:0.6226\n",
      "Episode:16 meanR:0.8235 R:-1.0000 rate:-0.0769 aloss:1.4609 eloss:0.9839 aloss2:2.3153 exploreP:0.6045\n",
      "Episode:17 meanR:0.8333 R:1.0000 rate:0.0769 aloss:1.4506 eloss:0.9829 aloss2:2.3148 exploreP:0.5869\n",
      "Episode:18 meanR:0.7895 R:0.0000 rate:0.0000 aloss:1.4419 eloss:0.9831 aloss2:2.3127 exploreP:0.5699\n",
      "Episode:19 meanR:0.7500 R:0.0000 rate:0.0000 aloss:1.4476 eloss:0.9828 aloss2:2.3136 exploreP:0.5533\n",
      "Episode:20 meanR:0.8095 R:2.0000 rate:0.1538 aloss:1.4403 eloss:0.9830 aloss2:2.3134 exploreP:0.5373\n",
      "Episode:21 meanR:0.7727 R:0.0000 rate:0.0000 aloss:1.4375 eloss:0.9815 aloss2:2.3129 exploreP:0.5217\n",
      "Episode:22 meanR:0.7826 R:1.0000 rate:0.0769 aloss:1.4440 eloss:0.9822 aloss2:2.3157 exploreP:0.5066\n",
      "Episode:23 meanR:0.7917 R:1.0000 rate:0.0769 aloss:1.4406 eloss:0.9798 aloss2:2.3145 exploreP:0.4919\n",
      "Episode:24 meanR:0.7600 R:0.0000 rate:0.0000 aloss:1.4402 eloss:0.9815 aloss2:2.3116 exploreP:0.4776\n",
      "Episode:25 meanR:0.8077 R:2.0000 rate:0.1538 aloss:1.4382 eloss:0.9793 aloss2:2.3130 exploreP:0.4638\n",
      "Episode:26 meanR:0.8148 R:1.0000 rate:0.0769 aloss:1.4381 eloss:0.9795 aloss2:2.3150 exploreP:0.4504\n",
      "Episode:27 meanR:0.8214 R:1.0000 rate:0.0769 aloss:1.4452 eloss:0.9799 aloss2:2.3136 exploreP:0.4374\n",
      "Episode:28 meanR:0.8276 R:1.0000 rate:0.0769 aloss:1.4396 eloss:0.9805 aloss2:2.3140 exploreP:0.4248\n",
      "Episode:29 meanR:0.7667 R:-1.0000 rate:-0.0769 aloss:1.4371 eloss:0.9772 aloss2:2.3173 exploreP:0.4125\n",
      "Episode:30 meanR:0.7419 R:0.0000 rate:0.0000 aloss:1.4391 eloss:0.9780 aloss2:2.3181 exploreP:0.4006\n",
      "Episode:31 meanR:0.7812 R:2.0000 rate:0.1538 aloss:1.4399 eloss:0.9780 aloss2:2.3174 exploreP:0.3891\n",
      "Episode:32 meanR:0.7576 R:0.0000 rate:0.0000 aloss:1.4417 eloss:0.9785 aloss2:2.3186 exploreP:0.3779\n",
      "Episode:33 meanR:0.7353 R:0.0000 rate:0.0000 aloss:1.4363 eloss:0.9744 aloss2:2.3222 exploreP:0.3670\n",
      "Episode:34 meanR:0.7143 R:0.0000 rate:0.0000 aloss:1.4493 eloss:0.9751 aloss2:2.3241 exploreP:0.3564\n",
      "Episode:35 meanR:0.7222 R:1.0000 rate:0.0769 aloss:1.4380 eloss:0.9750 aloss2:2.3246 exploreP:0.3462\n",
      "Episode:36 meanR:0.7568 R:2.0000 rate:0.1538 aloss:1.4313 eloss:0.9771 aloss2:2.3227 exploreP:0.3363\n",
      "Episode:37 meanR:0.7368 R:0.0000 rate:0.0000 aloss:1.4365 eloss:0.9802 aloss2:2.3284 exploreP:0.3266\n",
      "Episode:38 meanR:0.6923 R:-1.0000 rate:-0.0769 aloss:1.4372 eloss:0.9780 aloss2:2.3270 exploreP:0.3173\n",
      "Episode:39 meanR:0.7250 R:2.0000 rate:0.1538 aloss:1.4299 eloss:0.9724 aloss2:2.3395 exploreP:0.3082\n",
      "Episode:40 meanR:0.7317 R:1.0000 rate:0.0769 aloss:1.4304 eloss:0.9762 aloss2:2.3401 exploreP:0.2994\n",
      "Episode:41 meanR:0.7143 R:0.0000 rate:0.0000 aloss:1.4338 eloss:0.9780 aloss2:2.3432 exploreP:0.2908\n",
      "Episode:42 meanR:0.7209 R:1.0000 rate:0.0769 aloss:1.4207 eloss:0.9742 aloss2:2.3500 exploreP:0.2825\n",
      "Episode:43 meanR:0.7045 R:0.0000 rate:0.0000 aloss:1.4247 eloss:0.9717 aloss2:2.3566 exploreP:0.2745\n",
      "Episode:44 meanR:0.7111 R:1.0000 rate:0.0769 aloss:1.4286 eloss:0.9732 aloss2:2.3563 exploreP:0.2666\n",
      "Episode:45 meanR:0.6739 R:-1.0000 rate:-0.0769 aloss:1.4286 eloss:0.9708 aloss2:2.3657 exploreP:0.2591\n",
      "Episode:46 meanR:0.6596 R:0.0000 rate:0.0000 aloss:1.4240 eloss:0.9713 aloss2:2.3736 exploreP:0.2517\n",
      "Episode:47 meanR:0.6042 R:-2.0000 rate:-0.1538 aloss:1.4116 eloss:0.9691 aloss2:2.3869 exploreP:0.2446\n",
      "Episode:48 meanR:0.5918 R:0.0000 rate:0.0000 aloss:1.4170 eloss:0.9734 aloss2:2.3910 exploreP:0.2376\n",
      "Episode:49 meanR:0.5800 R:0.0000 rate:0.0000 aloss:1.4204 eloss:0.9752 aloss2:2.3941 exploreP:0.2309\n",
      "Episode:50 meanR:0.5686 R:0.0000 rate:0.0000 aloss:1.4250 eloss:0.9767 aloss2:2.3935 exploreP:0.2244\n",
      "Episode:51 meanR:0.5192 R:-2.0000 rate:-0.1538 aloss:1.4246 eloss:0.9739 aloss2:2.3933 exploreP:0.2180\n",
      "Episode:52 meanR:0.4906 R:-1.0000 rate:-0.0769 aloss:1.4254 eloss:0.9722 aloss2:2.4036 exploreP:0.2119\n",
      "Episode:53 meanR:0.4815 R:0.0000 rate:0.0000 aloss:1.4237 eloss:0.9728 aloss2:2.4080 exploreP:0.2059\n",
      "Episode:54 meanR:0.5091 R:2.0000 rate:0.1538 aloss:1.4297 eloss:0.9705 aloss2:2.4140 exploreP:0.2001\n",
      "Episode:55 meanR:0.5357 R:2.0000 rate:0.1538 aloss:1.4157 eloss:0.9707 aloss2:2.4232 exploreP:0.1945\n",
      "Episode:56 meanR:0.5263 R:0.0000 rate:0.0000 aloss:1.4207 eloss:0.9717 aloss2:2.4258 exploreP:0.1891\n",
      "Episode:57 meanR:0.4310 R:-5.0000 rate:-0.3846 aloss:1.4266 eloss:0.9677 aloss2:2.4391 exploreP:0.1838\n",
      "Episode:58 meanR:0.4407 R:1.0000 rate:0.0769 aloss:1.4088 eloss:0.9762 aloss2:2.4422 exploreP:0.1786\n",
      "Episode:59 meanR:0.4333 R:0.0000 rate:0.0000 aloss:1.4192 eloss:0.9711 aloss2:2.4504 exploreP:0.1736\n",
      "Episode:60 meanR:0.4262 R:0.0000 rate:0.0000 aloss:1.4305 eloss:0.9652 aloss2:2.4661 exploreP:0.1688\n",
      "Episode:61 meanR:0.3871 R:-2.0000 rate:-0.1538 aloss:1.4215 eloss:0.9685 aloss2:2.4749 exploreP:0.1641\n",
      "Episode:62 meanR:0.3651 R:-1.0000 rate:-0.0769 aloss:1.4195 eloss:0.9707 aloss2:2.4858 exploreP:0.1596\n",
      "Episode:63 meanR:0.3594 R:0.0000 rate:0.0000 aloss:1.4091 eloss:0.9625 aloss2:2.5077 exploreP:0.1551\n",
      "Episode:64 meanR:0.3538 R:0.0000 rate:0.0000 aloss:1.4171 eloss:0.9611 aloss2:2.5239 exploreP:0.1509\n",
      "Episode:65 meanR:0.3939 R:3.0000 rate:0.2308 aloss:1.4135 eloss:0.9637 aloss2:2.5368 exploreP:0.1467\n",
      "Episode:66 meanR:0.4179 R:2.0000 rate:0.1538 aloss:1.4140 eloss:0.9627 aloss2:2.5520 exploreP:0.1426\n",
      "Episode:67 meanR:0.4118 R:0.0000 rate:0.0000 aloss:1.4105 eloss:0.9596 aloss2:2.5716 exploreP:0.1387\n",
      "Episode:68 meanR:0.3913 R:-1.0000 rate:-0.0769 aloss:1.4061 eloss:0.9595 aloss2:2.5858 exploreP:0.1349\n",
      "Episode:69 meanR:0.3857 R:0.0000 rate:0.0000 aloss:1.3948 eloss:0.9649 aloss2:2.6057 exploreP:0.1312\n",
      "Episode:70 meanR:0.4225 R:3.0000 rate:0.2308 aloss:1.4049 eloss:0.9567 aloss2:2.6222 exploreP:0.1276\n",
      "Episode:71 meanR:0.4028 R:-1.0000 rate:-0.0769 aloss:1.3943 eloss:0.9574 aloss2:2.6381 exploreP:0.1242\n",
      "Episode:72 meanR:0.3562 R:-3.0000 rate:-0.2308 aloss:1.3870 eloss:0.9586 aloss2:2.6551 exploreP:0.1208\n",
      "Episode:73 meanR:0.3514 R:0.0000 rate:0.0000 aloss:1.3766 eloss:0.9605 aloss2:2.6693 exploreP:0.1175\n",
      "Episode:74 meanR:0.3600 R:1.0000 rate:0.0769 aloss:1.3932 eloss:0.9579 aloss2:2.6821 exploreP:0.1143\n",
      "Episode:75 meanR:0.3684 R:1.0000 rate:0.0769 aloss:1.3892 eloss:0.9551 aloss2:2.6976 exploreP:0.1113\n",
      "Episode:76 meanR:0.3636 R:0.0000 rate:0.0000 aloss:1.3847 eloss:0.9540 aloss2:2.7211 exploreP:0.1083\n",
      "Episode:77 meanR:0.3846 R:2.0000 rate:0.1538 aloss:1.3714 eloss:0.9545 aloss2:2.7385 exploreP:0.1054\n",
      "Episode:78 meanR:0.3671 R:-1.0000 rate:-0.0769 aloss:1.3950 eloss:0.9537 aloss2:2.7551 exploreP:0.1025\n",
      "Episode:79 meanR:0.3500 R:-1.0000 rate:-0.0769 aloss:1.3832 eloss:0.9562 aloss2:2.7770 exploreP:0.0998\n",
      "Episode:80 meanR:0.3210 R:-2.0000 rate:-0.1538 aloss:1.3888 eloss:0.9531 aloss2:2.8038 exploreP:0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:81 meanR:0.3171 R:0.0000 rate:0.0000 aloss:1.3779 eloss:0.9542 aloss2:2.8217 exploreP:0.0946\n",
      "Episode:82 meanR:0.3253 R:1.0000 rate:0.0769 aloss:1.3722 eloss:0.9560 aloss2:2.8430 exploreP:0.0921\n",
      "Episode:83 meanR:0.3333 R:1.0000 rate:0.0769 aloss:1.3715 eloss:0.9570 aloss2:2.8694 exploreP:0.0897\n",
      "Episode:84 meanR:0.3294 R:0.0000 rate:0.0000 aloss:1.3718 eloss:0.9528 aloss2:2.8910 exploreP:0.0873\n",
      "Episode:85 meanR:0.3140 R:-1.0000 rate:-0.0769 aloss:1.3695 eloss:0.9596 aloss2:2.9148 exploreP:0.0850\n",
      "Episode:86 meanR:0.2874 R:-2.0000 rate:-0.1538 aloss:1.3608 eloss:0.9523 aloss2:2.9463 exploreP:0.0828\n",
      "Episode:87 meanR:0.2841 R:0.0000 rate:0.0000 aloss:1.3765 eloss:0.9535 aloss2:2.9805 exploreP:0.0806\n",
      "Episode:88 meanR:0.2697 R:-1.0000 rate:-0.0769 aloss:1.3687 eloss:0.9479 aloss2:3.0130 exploreP:0.0786\n",
      "Episode:89 meanR:0.2667 R:0.0000 rate:0.0000 aloss:1.3702 eloss:0.9503 aloss2:3.0389 exploreP:0.0765\n",
      "Episode:90 meanR:0.2747 R:1.0000 rate:0.0769 aloss:1.3512 eloss:0.9450 aloss2:3.0682 exploreP:0.0746\n",
      "Episode:91 meanR:0.2609 R:-1.0000 rate:-0.0769 aloss:1.3618 eloss:0.9432 aloss2:3.1027 exploreP:0.0727\n",
      "Episode:92 meanR:0.2581 R:0.0000 rate:0.0000 aloss:1.3659 eloss:0.9471 aloss2:3.1421 exploreP:0.0708\n",
      "Episode:93 meanR:0.2660 R:1.0000 rate:0.0769 aloss:1.3740 eloss:0.9477 aloss2:3.1779 exploreP:0.0690\n",
      "Episode:94 meanR:0.2421 R:-2.0000 rate:-0.1538 aloss:1.3837 eloss:0.9512 aloss2:3.2273 exploreP:0.0673\n",
      "Episode:95 meanR:0.2500 R:1.0000 rate:0.0769 aloss:1.3497 eloss:0.9462 aloss2:3.2825 exploreP:0.0656\n",
      "Episode:96 meanR:0.2474 R:0.0000 rate:0.0000 aloss:1.3592 eloss:0.9429 aloss2:3.3411 exploreP:0.0639\n",
      "Episode:97 meanR:0.2551 R:1.0000 rate:0.0769 aloss:1.3645 eloss:0.9443 aloss2:3.3915 exploreP:0.0623\n",
      "Episode:98 meanR:0.2626 R:1.0000 rate:0.0769 aloss:1.3522 eloss:0.9456 aloss2:3.4266 exploreP:0.0608\n",
      "Episode:99 meanR:0.2700 R:1.0000 rate:0.0769 aloss:1.3284 eloss:0.9468 aloss2:3.4611 exploreP:0.0593\n",
      "Episode:100 meanR:0.2600 R:0.0000 rate:0.0000 aloss:1.3301 eloss:0.9439 aloss2:3.4951 exploreP:0.0578\n",
      "Episode:101 meanR:0.2800 R:1.0000 rate:0.0769 aloss:1.3365 eloss:0.9382 aloss2:3.5368 exploreP:0.0564\n",
      "Episode:102 meanR:0.2900 R:1.0000 rate:0.0769 aloss:1.3377 eloss:0.9414 aloss2:3.5749 exploreP:0.0550\n",
      "Episode:103 meanR:0.2900 R:1.0000 rate:0.0769 aloss:1.3488 eloss:0.9353 aloss2:3.6063 exploreP:0.0537\n",
      "Episode:104 meanR:0.2600 R:-3.0000 rate:-0.2308 aloss:1.3534 eloss:0.9404 aloss2:3.6687 exploreP:0.0524\n",
      "Episode:105 meanR:0.2600 R:1.0000 rate:0.0769 aloss:1.3331 eloss:0.9394 aloss2:3.6991 exploreP:0.0512\n",
      "Episode:106 meanR:0.2600 R:0.0000 rate:0.0000 aloss:1.3239 eloss:0.9373 aloss2:3.7510 exploreP:0.0500\n",
      "Episode:107 meanR:0.2500 R:1.0000 rate:0.0769 aloss:1.3327 eloss:0.9386 aloss2:3.7980 exploreP:0.0488\n",
      "Episode:108 meanR:0.2400 R:0.0000 rate:0.0000 aloss:1.3145 eloss:0.9343 aloss2:3.8261 exploreP:0.0476\n",
      "Episode:109 meanR:0.2400 R:0.0000 rate:0.0000 aloss:1.3077 eloss:0.9392 aloss2:3.8546 exploreP:0.0465\n",
      "Episode:110 meanR:0.2400 R:0.0000 rate:0.0000 aloss:1.2954 eloss:0.9421 aloss2:3.8641 exploreP:0.0454\n",
      "Episode:111 meanR:0.2300 R:0.0000 rate:0.0000 aloss:1.3219 eloss:0.9369 aloss2:3.9509 exploreP:0.0444\n",
      "Episode:112 meanR:0.2300 R:0.0000 rate:0.0000 aloss:1.3397 eloss:0.9453 aloss2:3.9875 exploreP:0.0434\n",
      "Episode:113 meanR:0.2200 R:0.0000 rate:0.0000 aloss:1.3345 eloss:0.9541 aloss2:4.0753 exploreP:0.0424\n",
      "Episode:114 meanR:0.1900 R:-1.0000 rate:-0.0769 aloss:1.3246 eloss:0.9548 aloss2:4.1542 exploreP:0.0414\n",
      "Episode:115 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:1.2952 eloss:0.9543 aloss2:4.2288 exploreP:0.0405\n",
      "Episode:116 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.3155 eloss:0.9454 aloss2:4.3214 exploreP:0.0396\n",
      "Episode:117 meanR:0.1100 R:-1.0000 rate:-0.0769 aloss:1.3205 eloss:0.9444 aloss2:4.4183 exploreP:0.0387\n",
      "Episode:118 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.3300 eloss:0.9383 aloss2:4.4955 exploreP:0.0379\n",
      "Episode:119 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.3073 eloss:0.9439 aloss2:4.6039 exploreP:0.0371\n",
      "Episode:120 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.3231 eloss:0.9386 aloss2:4.6582 exploreP:0.0363\n",
      "Episode:121 meanR:0.1000 R:1.0000 rate:0.0769 aloss:1.3154 eloss:0.9301 aloss2:4.7243 exploreP:0.0355\n",
      "Episode:122 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.3015 eloss:0.9382 aloss2:4.8002 exploreP:0.0347\n",
      "Episode:123 meanR:0.0700 R:-1.0000 rate:-0.0769 aloss:1.2776 eloss:0.9358 aloss2:4.8148 exploreP:0.0340\n",
      "Episode:124 meanR:0.0600 R:-1.0000 rate:-0.0769 aloss:1.3161 eloss:0.9364 aloss2:4.9048 exploreP:0.0333\n",
      "Episode:125 meanR:0.0600 R:2.0000 rate:0.1538 aloss:1.2961 eloss:0.9337 aloss2:4.9600 exploreP:0.0326\n",
      "Episode:126 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.3045 eloss:0.9327 aloss2:4.9708 exploreP:0.0319\n",
      "Episode:127 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2972 eloss:0.9320 aloss2:4.9627 exploreP:0.0313\n",
      "Episode:128 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2999 eloss:0.9277 aloss2:4.9532 exploreP:0.0306\n",
      "Episode:129 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2955 eloss:0.9302 aloss2:4.9663 exploreP:0.0300\n",
      "Episode:130 meanR:0.0600 R:2.0000 rate:0.1538 aloss:1.2887 eloss:0.9291 aloss2:4.9549 exploreP:0.0294\n",
      "Episode:131 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.2865 eloss:0.9304 aloss2:4.9694 exploreP:0.0289\n",
      "Episode:132 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:1.2538 eloss:0.9362 aloss2:4.9736 exploreP:0.0283\n",
      "Episode:133 meanR:0.0300 R:1.0000 rate:0.0769 aloss:1.2957 eloss:0.9304 aloss2:4.9847 exploreP:0.0278\n",
      "Episode:134 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2669 eloss:0.9343 aloss2:5.0129 exploreP:0.0272\n",
      "Episode:135 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:1.2793 eloss:0.9362 aloss2:4.9856 exploreP:0.0267\n",
      "Episode:136 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.2730 eloss:0.9314 aloss2:5.0766 exploreP:0.0262\n",
      "Episode:137 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.2399 eloss:0.9344 aloss2:5.0917 exploreP:0.0258\n",
      "Episode:138 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2570 eloss:0.9261 aloss2:5.1122 exploreP:0.0253\n",
      "Episode:139 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.2377 eloss:0.9368 aloss2:5.1438 exploreP:0.0248\n",
      "Episode:140 meanR:-0.0300 R:-1.0000 rate:-0.0769 aloss:1.2729 eloss:0.9274 aloss2:5.2498 exploreP:0.0244\n",
      "Episode:141 meanR:-0.0400 R:-1.0000 rate:-0.0769 aloss:1.2737 eloss:0.9228 aloss2:5.2928 exploreP:0.0240\n",
      "Episode:142 meanR:-0.0400 R:1.0000 rate:0.0769 aloss:1.2583 eloss:0.9258 aloss2:5.3043 exploreP:0.0236\n",
      "Episode:143 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2729 eloss:0.9221 aloss2:5.3608 exploreP:0.0232\n",
      "Episode:144 meanR:-0.0500 R:0.0000 rate:0.0000 aloss:1.2629 eloss:0.9251 aloss2:5.3804 exploreP:0.0228\n",
      "Episode:145 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2129 eloss:0.9395 aloss2:5.3707 exploreP:0.0224\n",
      "Episode:146 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.2201 eloss:0.9505 aloss2:5.3720 exploreP:0.0220\n",
      "Episode:147 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2710 eloss:0.9530 aloss2:5.4531 exploreP:0.0217\n",
      "Episode:148 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2568 eloss:0.9439 aloss2:5.5165 exploreP:0.0213\n",
      "Episode:149 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2531 eloss:0.9416 aloss2:5.6297 exploreP:0.0210\n",
      "Episode:150 meanR:-0.0200 R:0.0000 rate:0.0000 aloss:1.2608 eloss:0.9388 aloss2:5.6821 exploreP:0.0207\n",
      "Episode:151 meanR:0.0200 R:2.0000 rate:0.1538 aloss:1.2584 eloss:0.9318 aloss2:5.6556 exploreP:0.0204\n",
      "Episode:152 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.2540 eloss:0.9366 aloss2:5.6675 exploreP:0.0201\n",
      "Episode:153 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.2448 eloss:0.9374 aloss2:5.5305 exploreP:0.0198\n",
      "Episode:154 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2426 eloss:0.9362 aloss2:5.4988 exploreP:0.0195\n",
      "Episode:155 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.2320 eloss:0.9463 aloss2:5.4028 exploreP:0.0192\n",
      "Episode:156 meanR:-0.0200 R:-2.0000 rate:-0.1538 aloss:1.1949 eloss:0.9469 aloss2:5.4187 exploreP:0.0189\n",
      "Episode:157 meanR:0.0200 R:-1.0000 rate:-0.0769 aloss:1.2062 eloss:0.9476 aloss2:5.4444 exploreP:0.0187\n",
      "Episode:158 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2167 eloss:0.9455 aloss2:5.4779 exploreP:0.0184\n",
      "Episode:159 meanR:0.0200 R:1.0000 rate:0.0769 aloss:1.2037 eloss:0.9465 aloss2:5.5125 exploreP:0.0181\n",
      "Episode:160 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.2314 eloss:0.9462 aloss2:5.5800 exploreP:0.0179\n",
      "Episode:161 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.2274 eloss:0.9466 aloss2:5.5356 exploreP:0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:162 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.1794 eloss:0.9447 aloss2:5.5930 exploreP:0.0174\n",
      "Episode:163 meanR:0.0400 R:-1.0000 rate:-0.0769 aloss:1.2074 eloss:0.9431 aloss2:5.6472 exploreP:0.0172\n",
      "Episode:164 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.1863 eloss:0.9468 aloss2:5.6505 exploreP:0.0170\n",
      "Episode:165 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1937 eloss:0.9429 aloss2:5.6733 exploreP:0.0168\n",
      "Episode:166 meanR:-0.0100 R:1.0000 rate:0.0769 aloss:1.1946 eloss:0.9382 aloss2:5.7424 exploreP:0.0166\n",
      "Episode:167 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.1986 eloss:0.9459 aloss2:5.7121 exploreP:0.0164\n",
      "Episode:168 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1371 eloss:0.9427 aloss2:5.7342 exploreP:0.0162\n",
      "Episode:169 meanR:-0.0100 R:-1.0000 rate:-0.0769 aloss:1.2154 eloss:0.9340 aloss2:5.7716 exploreP:0.0160\n",
      "Episode:170 meanR:-0.0200 R:2.0000 rate:0.1538 aloss:1.2008 eloss:0.9364 aloss2:5.7533 exploreP:0.0159\n",
      "Episode:171 meanR:-0.0100 R:0.0000 rate:0.0000 aloss:1.1977 eloss:0.9397 aloss2:5.8393 exploreP:0.0157\n",
      "Episode:172 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.1657 eloss:0.9382 aloss2:5.8186 exploreP:0.0155\n",
      "Episode:173 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.1793 eloss:0.9293 aloss2:5.8262 exploreP:0.0154\n",
      "Episode:174 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.2274 eloss:0.9374 aloss2:5.8906 exploreP:0.0152\n",
      "Episode:175 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1473 eloss:0.9422 aloss2:5.8739 exploreP:0.0150\n",
      "Episode:176 meanR:-0.0200 R:-2.0000 rate:-0.1538 aloss:1.2014 eloss:0.9296 aloss2:5.9266 exploreP:0.0149\n",
      "Episode:177 meanR:-0.0400 R:0.0000 rate:0.0000 aloss:1.1999 eloss:0.9350 aloss2:5.9937 exploreP:0.0147\n",
      "Episode:178 meanR:-0.0200 R:1.0000 rate:0.0769 aloss:1.1771 eloss:0.9381 aloss2:5.9809 exploreP:0.0146\n",
      "Episode:179 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.1522 eloss:0.9302 aloss2:5.9918 exploreP:0.0145\n",
      "Episode:180 meanR:0.0100 R:-1.0000 rate:-0.0769 aloss:1.1787 eloss:0.9288 aloss2:6.0568 exploreP:0.0143\n",
      "Episode:181 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1902 eloss:0.9283 aloss2:6.0780 exploreP:0.0142\n",
      "Episode:182 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1472 eloss:0.9337 aloss2:6.0690 exploreP:0.0141\n",
      "Episode:183 meanR:0.0000 R:1.0000 rate:0.0769 aloss:1.1626 eloss:0.9253 aloss2:6.0709 exploreP:0.0140\n",
      "Episode:184 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1811 eloss:0.9296 aloss2:6.0731 exploreP:0.0138\n",
      "Episode:185 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1864 eloss:0.9269 aloss2:6.1097 exploreP:0.0137\n",
      "Episode:186 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.1539 eloss:0.9208 aloss2:6.1630 exploreP:0.0136\n",
      "Episode:187 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.1854 eloss:0.9248 aloss2:6.1977 exploreP:0.0135\n",
      "Episode:188 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.1280 eloss:0.9258 aloss2:6.1074 exploreP:0.0134\n",
      "Episode:189 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.1517 eloss:0.9175 aloss2:6.1814 exploreP:0.0133\n",
      "Episode:190 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.1684 eloss:0.9195 aloss2:6.1252 exploreP:0.0132\n",
      "Episode:191 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.1848 eloss:0.9151 aloss2:6.2082 exploreP:0.0131\n",
      "Episode:192 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.1122 eloss:0.9248 aloss2:6.1543 exploreP:0.0130\n",
      "Episode:193 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.1489 eloss:0.9179 aloss2:6.1632 exploreP:0.0129\n",
      "Episode:194 meanR:0.0700 R:1.0000 rate:0.0769 aloss:1.1188 eloss:0.9250 aloss2:6.2147 exploreP:0.0129\n",
      "Episode:195 meanR:0.0500 R:-1.0000 rate:-0.0769 aloss:1.1212 eloss:0.9222 aloss2:6.1566 exploreP:0.0128\n",
      "Episode:196 meanR:0.0500 R:0.0000 rate:0.0000 aloss:1.0981 eloss:0.9145 aloss2:6.1505 exploreP:0.0127\n",
      "Episode:197 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.1090 eloss:0.9100 aloss2:6.1773 exploreP:0.0126\n",
      "Episode:198 meanR:0.0200 R:0.0000 rate:0.0000 aloss:1.1281 eloss:0.9162 aloss2:6.1578 exploreP:0.0125\n",
      "Episode:199 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1305 eloss:0.9114 aloss2:6.1733 exploreP:0.0125\n",
      "Episode:200 meanR:0.0100 R:0.0000 rate:0.0000 aloss:1.1860 eloss:0.9183 aloss2:6.2210 exploreP:0.0124\n",
      "Episode:201 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.1497 eloss:0.9201 aloss2:6.2817 exploreP:0.0123\n",
      "Episode:202 meanR:0.0100 R:2.0000 rate:0.1538 aloss:1.1084 eloss:0.9222 aloss2:6.2318 exploreP:0.0122\n",
      "Episode:203 meanR:0.0000 R:0.0000 rate:0.0000 aloss:1.0951 eloss:0.9116 aloss2:6.2330 exploreP:0.0122\n",
      "Episode:204 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.1411 eloss:0.9141 aloss2:6.2660 exploreP:0.0121\n",
      "Episode:205 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.0801 eloss:0.9113 aloss2:6.1567 exploreP:0.0120\n",
      "Episode:206 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.1033 eloss:0.9056 aloss2:6.2136 exploreP:0.0120\n",
      "Episode:207 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.1016 eloss:0.9091 aloss2:6.1930 exploreP:0.0119\n",
      "Episode:208 meanR:0.0400 R:1.0000 rate:0.0769 aloss:1.0869 eloss:0.9084 aloss2:6.1638 exploreP:0.0119\n",
      "Episode:209 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.0534 eloss:0.9037 aloss2:6.0998 exploreP:0.0118\n",
      "Episode:210 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.1159 eloss:0.9041 aloss2:6.1389 exploreP:0.0118\n",
      "Episode:211 meanR:0.0400 R:0.0000 rate:0.0000 aloss:1.0989 eloss:0.9101 aloss2:6.1370 exploreP:0.0117\n",
      "Episode:212 meanR:0.0300 R:-1.0000 rate:-0.0769 aloss:1.0335 eloss:0.9077 aloss2:6.1479 exploreP:0.0117\n",
      "Episode:213 meanR:0.0300 R:0.0000 rate:0.0000 aloss:1.0865 eloss:0.9059 aloss2:6.1373 exploreP:0.0116\n",
      "Episode:214 meanR:0.0600 R:2.0000 rate:0.1538 aloss:1.0947 eloss:0.9111 aloss2:6.1502 exploreP:0.0116\n",
      "Episode:215 meanR:0.0900 R:2.0000 rate:0.1538 aloss:1.0661 eloss:0.9128 aloss2:6.1150 exploreP:0.0115\n",
      "Episode:216 meanR:0.1200 R:3.0000 rate:0.2308 aloss:1.1093 eloss:0.9121 aloss2:6.1172 exploreP:0.0115\n",
      "Episode:217 meanR:0.1400 R:1.0000 rate:0.0769 aloss:1.1201 eloss:0.9114 aloss2:6.1683 exploreP:0.0114\n",
      "Episode:218 meanR:0.1500 R:1.0000 rate:0.0769 aloss:1.0805 eloss:0.9123 aloss2:6.1583 exploreP:0.0114\n",
      "Episode:219 meanR:0.1500 R:0.0000 rate:0.0000 aloss:1.0681 eloss:0.9087 aloss2:6.1651 exploreP:0.0113\n",
      "Episode:220 meanR:0.1500 R:0.0000 rate:0.0000 aloss:1.0769 eloss:0.9060 aloss2:6.1646 exploreP:0.0113\n",
      "Episode:221 meanR:0.1400 R:0.0000 rate:0.0000 aloss:1.0851 eloss:0.9076 aloss2:6.2192 exploreP:0.0113\n",
      "Episode:222 meanR:0.1400 R:0.0000 rate:0.0000 aloss:1.0379 eloss:0.9110 aloss2:6.1492 exploreP:0.0112\n",
      "Episode:223 meanR:0.1600 R:1.0000 rate:0.0769 aloss:1.0281 eloss:0.9063 aloss2:6.2155 exploreP:0.0112\n",
      "Episode:224 meanR:0.1500 R:-2.0000 rate:-0.1538 aloss:1.0952 eloss:0.9153 aloss2:6.2561 exploreP:0.0112\n",
      "Episode:225 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.0117 eloss:0.9095 aloss2:6.1791 exploreP:0.0111\n",
      "Episode:226 meanR:0.1400 R:1.0000 rate:0.0769 aloss:1.0441 eloss:0.8996 aloss2:6.1908 exploreP:0.0111\n",
      "Episode:227 meanR:0.1300 R:-1.0000 rate:-0.0769 aloss:1.0182 eloss:0.8966 aloss2:6.2219 exploreP:0.0111\n",
      "Episode:228 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.0711 eloss:0.9105 aloss2:6.1949 exploreP:0.0110\n",
      "Episode:229 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.0238 eloss:0.8991 aloss2:6.2034 exploreP:0.0110\n",
      "Episode:230 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.0889 eloss:0.9012 aloss2:6.2348 exploreP:0.0110\n",
      "Episode:231 meanR:0.1200 R:0.0000 rate:0.0000 aloss:1.0216 eloss:0.9019 aloss2:6.2684 exploreP:0.0109\n",
      "Episode:232 meanR:0.1400 R:1.0000 rate:0.0769 aloss:1.0439 eloss:0.9042 aloss2:6.2140 exploreP:0.0109\n",
      "Episode:233 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:1.0265 eloss:0.8923 aloss2:6.1962 exploreP:0.0109\n",
      "Episode:234 meanR:0.1300 R:1.0000 rate:0.0769 aloss:1.0051 eloss:0.8888 aloss2:6.2811 exploreP:0.0109\n",
      "Episode:235 meanR:0.1400 R:0.0000 rate:0.0000 aloss:0.9847 eloss:0.9054 aloss2:6.2252 exploreP:0.0108\n",
      "Episode:236 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.0385 eloss:0.8988 aloss2:6.2010 exploreP:0.0108\n",
      "Episode:237 meanR:0.1300 R:0.0000 rate:0.0000 aloss:1.0642 eloss:0.9061 aloss2:6.3043 exploreP:0.0108\n",
      "Episode:238 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9559 eloss:0.8906 aloss2:6.2590 exploreP:0.0108\n",
      "Episode:239 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:0.9987 eloss:0.8942 aloss2:6.2467 exploreP:0.0107\n",
      "Episode:240 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:1.0130 eloss:0.8942 aloss2:6.2259 exploreP:0.0107\n",
      "Episode:241 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9963 eloss:0.8932 aloss2:6.2102 exploreP:0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:242 meanR:0.1100 R:-1.0000 rate:-0.0769 aloss:0.9663 eloss:0.8860 aloss2:6.2180 exploreP:0.0107\n",
      "Episode:243 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.0052 eloss:0.8842 aloss2:6.1127 exploreP:0.0107\n",
      "Episode:244 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.0314 eloss:0.9018 aloss2:6.1785 exploreP:0.0106\n",
      "Episode:245 meanR:0.1100 R:0.0000 rate:0.0000 aloss:0.9600 eloss:0.8848 aloss2:6.1768 exploreP:0.0106\n",
      "Episode:246 meanR:0.1100 R:0.0000 rate:0.0000 aloss:1.0219 eloss:0.8964 aloss2:6.1526 exploreP:0.0106\n",
      "Episode:247 meanR:0.1100 R:0.0000 rate:0.0000 aloss:0.9810 eloss:0.8866 aloss2:6.1640 exploreP:0.0106\n",
      "Episode:248 meanR:0.1100 R:0.0000 rate:0.0000 aloss:0.9975 eloss:0.8972 aloss2:6.1678 exploreP:0.0106\n",
      "Episode:249 meanR:0.1000 R:-1.0000 rate:-0.0769 aloss:0.9446 eloss:0.8739 aloss2:6.1189 exploreP:0.0105\n",
      "Episode:250 meanR:0.1100 R:1.0000 rate:0.0769 aloss:1.0047 eloss:0.8901 aloss2:6.1127 exploreP:0.0105\n",
      "Episode:251 meanR:0.0900 R:0.0000 rate:0.0000 aloss:1.0167 eloss:0.8906 aloss2:6.1834 exploreP:0.0105\n",
      "Episode:252 meanR:0.0900 R:0.0000 rate:0.0000 aloss:0.9431 eloss:0.8772 aloss2:6.1366 exploreP:0.0105\n",
      "Episode:253 meanR:0.0800 R:0.0000 rate:0.0000 aloss:0.9915 eloss:0.8863 aloss2:6.1484 exploreP:0.0105\n",
      "Episode:254 meanR:0.0900 R:1.0000 rate:0.0769 aloss:0.9743 eloss:0.8932 aloss2:6.0769 exploreP:0.0105\n",
      "Episode:255 meanR:0.0900 R:0.0000 rate:0.0000 aloss:0.9340 eloss:0.8795 aloss2:6.0985 exploreP:0.0105\n",
      "Episode:256 meanR:0.1100 R:0.0000 rate:0.0000 aloss:0.9493 eloss:0.8750 aloss2:6.0749 exploreP:0.0104\n",
      "Episode:257 meanR:0.1500 R:3.0000 rate:0.2308 aloss:0.9417 eloss:0.8750 aloss2:6.0921 exploreP:0.0104\n",
      "Episode:258 meanR:0.1500 R:0.0000 rate:0.0000 aloss:0.9997 eloss:0.8758 aloss2:6.0373 exploreP:0.0104\n",
      "Episode:259 meanR:0.1400 R:0.0000 rate:0.0000 aloss:0.9418 eloss:0.8592 aloss2:5.9701 exploreP:0.0104\n",
      "Episode:260 meanR:0.1300 R:-1.0000 rate:-0.0769 aloss:0.9685 eloss:0.8806 aloss2:5.9453 exploreP:0.0104\n",
      "Episode:261 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:0.9714 eloss:0.8735 aloss2:5.9976 exploreP:0.0104\n",
      "Episode:262 meanR:0.1200 R:0.0000 rate:0.0000 aloss:0.9541 eloss:0.8701 aloss2:5.8957 exploreP:0.0104\n",
      "Episode:263 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9666 eloss:0.8733 aloss2:5.8656 exploreP:0.0104\n",
      "Episode:264 meanR:0.1400 R:0.0000 rate:0.0000 aloss:0.9803 eloss:0.8764 aloss2:5.9258 exploreP:0.0103\n",
      "Episode:265 meanR:0.1400 R:0.0000 rate:0.0000 aloss:0.9537 eloss:0.8772 aloss2:5.8665 exploreP:0.0103\n",
      "Episode:266 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9979 eloss:0.8834 aloss2:5.7796 exploreP:0.0103\n",
      "Episode:267 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9329 eloss:0.8523 aloss2:5.8476 exploreP:0.0103\n",
      "Episode:268 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9376 eloss:0.8657 aloss2:5.8369 exploreP:0.0103\n",
      "Episode:269 meanR:0.1500 R:1.0000 rate:0.0769 aloss:0.9587 eloss:0.8560 aloss2:5.8189 exploreP:0.0103\n",
      "Episode:270 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9759 eloss:0.8537 aloss2:5.7475 exploreP:0.0103\n",
      "Episode:271 meanR:0.1300 R:0.0000 rate:0.0000 aloss:0.9634 eloss:0.8519 aloss2:5.7802 exploreP:0.0103\n",
      "Episode:272 meanR:0.1200 R:-1.0000 rate:-0.0769 aloss:0.9592 eloss:0.8536 aloss2:5.7502 exploreP:0.0103\n",
      "Episode:273 meanR:0.1200 R:0.0000 rate:0.0000 aloss:0.9808 eloss:0.8471 aloss2:5.7324 exploreP:0.0103\n",
      "Episode:274 meanR:0.1000 R:-2.0000 rate:-0.1538 aloss:0.9724 eloss:0.8418 aloss2:5.7576 exploreP:0.0103\n",
      "Episode:275 meanR:0.1000 R:0.0000 rate:0.0000 aloss:0.9427 eloss:0.8291 aloss2:5.6973 exploreP:0.0103\n",
      "Episode:276 meanR:0.1200 R:0.0000 rate:0.0000 aloss:0.9772 eloss:0.8368 aloss2:5.6611 exploreP:0.0102\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "aloss_list, eloss_list, aloss2_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        aloss_batch, eloss_batch, aloss2_batch = [], [], []\n",
    "        total_reward = 0\n",
    "        #state = env.reset() # each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "        num_step = 0 # each episode\n",
    "        rate = -1\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                action = np.random.randint(action_size)        # select an action\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "                states = states[rates >= np.max(rates)]\n",
    "                actions = actions[rates >= np.max(rates)]\n",
    "                next_states = next_states[rates >= np.max(rates)]\n",
    "                rewards = rewards[rates >= np.max(rates)]\n",
    "                dones = dones[rates >= np.max(rates)]\n",
    "                rates = rates[rates >= np.max(rates)]\n",
    "                if np.count_nonzero(dones) > 0 and len(dones) > 1 and np.max(rates) > 0:\n",
    "                    break\n",
    "            aloss, _ = sess.run([model.a_loss, model.a_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            eloss, _ = sess.run([model.e_loss, model.e_opt],\n",
    "                                  feed_dict = {model.states: states, \n",
    "                                               model.actions: actions,\n",
    "                                               model.next_states: next_states,\n",
    "                                               model.rewards: rewards,\n",
    "                                               model.dones: dones,\n",
    "                                               model.rates: rates})\n",
    "            aloss2, _= sess.run([model.a_loss2, model.a_opt2], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            # print(len(dones), np.count_nonzero(dones), np.max(rates))\n",
    "            aloss_batch.append(aloss)\n",
    "            eloss_batch.append(eloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # Rating the memory\n",
    "        #if done is True:\n",
    "        rate = np.clip(total_reward/13, a_min=-1, a_max=+1) # [-1, +1]\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                    \n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'eloss:{:.4f}'.format(np.mean(eloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        aloss_list.append([ep, np.mean(aloss_batch)])\n",
    "        eloss_list.append([ep, np.mean(eloss_batch)])\n",
    "        aloss2_list.append([ep, np.mean(aloss2_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= +13:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(aloss2_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('A losses 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-nav.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 14.00\n"
     ]
    }
   ],
   "source": [
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Testing episodes/epochs\n",
    "    for _ in range(11):\n",
    "        total_reward = 0\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]   # get the current state\n",
    "\n",
    "        # Testing steps/batches\n",
    "        while True:\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        print('total_reward: {:.2f}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Be careful!!!!!!!!!!!!!!!!\n",
    "# # Closing the env\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
