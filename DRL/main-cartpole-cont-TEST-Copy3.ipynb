{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 6), (190, 4), (190,), (190, 4), (190,), (190,), (190,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:23.0000 R:23.0000 rate:0.0460 gloss:0.7312 dlossA:0.7009 dlossQ:0.9339 exploreP:0.9977\n",
      "Episode:1 meanR:16.5000 R:10.0000 rate:0.0200 gloss:0.7280 dlossA:0.7003 dlossQ:0.9225 exploreP:0.9967\n",
      "Episode:2 meanR:14.6667 R:11.0000 rate:0.0220 gloss:0.7206 dlossA:0.7004 dlossQ:0.9124 exploreP:0.9957\n",
      "Episode:3 meanR:18.7500 R:31.0000 rate:0.0620 gloss:0.7264 dlossA:0.7010 dlossQ:0.9184 exploreP:0.9926\n",
      "Episode:4 meanR:17.6000 R:13.0000 rate:0.0260 gloss:0.7180 dlossA:0.7011 dlossQ:0.9362 exploreP:0.9913\n",
      "Episode:5 meanR:17.0000 R:14.0000 rate:0.0280 gloss:0.7142 dlossA:0.7042 dlossQ:0.9335 exploreP:0.9900\n",
      "Episode:6 meanR:16.2857 R:12.0000 rate:0.0240 gloss:0.7179 dlossA:0.7029 dlossQ:0.9371 exploreP:0.9888\n",
      "Episode:7 meanR:17.0000 R:22.0000 rate:0.0440 gloss:0.7089 dlossA:0.7020 dlossQ:0.9222 exploreP:0.9866\n",
      "Episode:8 meanR:17.6667 R:23.0000 rate:0.0460 gloss:0.7120 dlossA:0.7038 dlossQ:0.9303 exploreP:0.9844\n",
      "Episode:9 meanR:17.7000 R:18.0000 rate:0.0360 gloss:0.7109 dlossA:0.7057 dlossQ:0.9345 exploreP:0.9826\n",
      "Episode:10 meanR:17.1818 R:12.0000 rate:0.0240 gloss:0.7087 dlossA:0.7058 dlossQ:0.9333 exploreP:0.9815\n",
      "Episode:11 meanR:16.6667 R:11.0000 rate:0.0220 gloss:0.7077 dlossA:0.7048 dlossQ:0.9319 exploreP:0.9804\n",
      "Episode:12 meanR:20.5385 R:67.0000 rate:0.1340 gloss:0.7041 dlossA:0.7021 dlossQ:0.9452 exploreP:0.9739\n",
      "Episode:13 meanR:19.9286 R:12.0000 rate:0.0240 gloss:0.6907 dlossA:0.7039 dlossQ:0.9276 exploreP:0.9728\n",
      "Episode:14 meanR:20.4667 R:28.0000 rate:0.0560 gloss:0.7053 dlossA:0.7057 dlossQ:0.9439 exploreP:0.9701\n",
      "Episode:15 meanR:20.3750 R:19.0000 rate:0.0380 gloss:0.7003 dlossA:0.7043 dlossQ:0.9461 exploreP:0.9682\n",
      "Episode:16 meanR:20.4706 R:22.0000 rate:0.0440 gloss:0.6991 dlossA:0.7069 dlossQ:0.9425 exploreP:0.9661\n",
      "Episode:17 meanR:20.1667 R:15.0000 rate:0.0300 gloss:0.6944 dlossA:0.7061 dlossQ:0.9446 exploreP:0.9647\n",
      "Episode:18 meanR:19.8421 R:14.0000 rate:0.0280 gloss:0.6948 dlossA:0.7066 dlossQ:0.9475 exploreP:0.9634\n",
      "Episode:19 meanR:20.4000 R:31.0000 rate:0.0620 gloss:0.6940 dlossA:0.7001 dlossQ:0.9778 exploreP:0.9604\n",
      "Episode:20 meanR:20.8095 R:29.0000 rate:0.0580 gloss:0.6945 dlossA:0.7066 dlossQ:0.9421 exploreP:0.9577\n",
      "Episode:21 meanR:20.8636 R:22.0000 rate:0.0440 gloss:0.6928 dlossA:0.7027 dlossQ:0.9549 exploreP:0.9556\n",
      "Episode:22 meanR:20.5652 R:14.0000 rate:0.0280 gloss:0.6861 dlossA:0.7077 dlossQ:0.9399 exploreP:0.9543\n",
      "Episode:23 meanR:20.2500 R:13.0000 rate:0.0260 gloss:0.6873 dlossA:0.7067 dlossQ:0.9547 exploreP:0.9530\n",
      "Episode:24 meanR:20.6800 R:31.0000 rate:0.0620 gloss:0.6885 dlossA:0.7068 dlossQ:0.9460 exploreP:0.9501\n",
      "Episode:25 meanR:21.3077 R:37.0000 rate:0.0740 gloss:0.6908 dlossA:0.7055 dlossQ:0.9571 exploreP:0.9466\n",
      "Episode:26 meanR:20.9630 R:12.0000 rate:0.0240 gloss:0.6892 dlossA:0.7045 dlossQ:0.9569 exploreP:0.9455\n",
      "Episode:27 meanR:20.8571 R:18.0000 rate:0.0360 gloss:0.6858 dlossA:0.7045 dlossQ:0.9564 exploreP:0.9438\n",
      "Episode:28 meanR:20.8966 R:22.0000 rate:0.0440 gloss:0.6839 dlossA:0.7057 dlossQ:0.9452 exploreP:0.9418\n",
      "Episode:29 meanR:20.7667 R:17.0000 rate:0.0340 gloss:0.6892 dlossA:0.7048 dlossQ:0.9544 exploreP:0.9402\n",
      "Episode:30 meanR:20.4839 R:12.0000 rate:0.0240 gloss:0.6816 dlossA:0.7029 dlossQ:0.9627 exploreP:0.9391\n",
      "Episode:31 meanR:21.2188 R:44.0000 rate:0.0880 gloss:0.6787 dlossA:0.7036 dlossQ:0.9542 exploreP:0.9350\n",
      "Episode:32 meanR:20.9394 R:12.0000 rate:0.0240 gloss:0.6731 dlossA:0.6999 dlossQ:0.9886 exploreP:0.9339\n",
      "Episode:33 meanR:20.6471 R:11.0000 rate:0.0220 gloss:0.6856 dlossA:0.7082 dlossQ:0.9485 exploreP:0.9329\n",
      "Episode:34 meanR:21.2857 R:43.0000 rate:0.0860 gloss:0.6863 dlossA:0.7058 dlossQ:0.9633 exploreP:0.9289\n",
      "Episode:35 meanR:21.1111 R:15.0000 rate:0.0300 gloss:0.6782 dlossA:0.7057 dlossQ:0.9478 exploreP:0.9275\n",
      "Episode:36 meanR:20.8649 R:12.0000 rate:0.0240 gloss:0.6808 dlossA:0.7042 dlossQ:0.9617 exploreP:0.9264\n",
      "Episode:37 meanR:21.5263 R:46.0000 rate:0.0920 gloss:0.6808 dlossA:0.7073 dlossQ:0.9478 exploreP:0.9222\n",
      "Episode:38 meanR:21.4615 R:19.0000 rate:0.0380 gloss:0.6909 dlossA:0.7080 dlossQ:0.9663 exploreP:0.9205\n",
      "Episode:39 meanR:21.5500 R:25.0000 rate:0.0500 gloss:0.6813 dlossA:0.7047 dlossQ:0.9550 exploreP:0.9182\n",
      "Episode:40 meanR:21.2927 R:11.0000 rate:0.0220 gloss:0.6791 dlossA:0.7057 dlossQ:0.9489 exploreP:0.9172\n",
      "Episode:41 meanR:21.2857 R:21.0000 rate:0.0420 gloss:0.6772 dlossA:0.7062 dlossQ:0.9555 exploreP:0.9153\n",
      "Episode:42 meanR:21.2558 R:20.0000 rate:0.0400 gloss:0.6770 dlossA:0.7087 dlossQ:0.9523 exploreP:0.9135\n",
      "Episode:43 meanR:21.1818 R:18.0000 rate:0.0360 gloss:0.6777 dlossA:0.7076 dlossQ:0.9538 exploreP:0.9119\n",
      "Episode:44 meanR:21.8444 R:51.0000 rate:0.1020 gloss:0.6818 dlossA:0.7055 dlossQ:0.9560 exploreP:0.9073\n",
      "Episode:45 meanR:21.8913 R:24.0000 rate:0.0480 gloss:0.6853 dlossA:0.7051 dlossQ:0.9586 exploreP:0.9052\n",
      "Episode:46 meanR:22.2766 R:40.0000 rate:0.0800 gloss:0.6845 dlossA:0.7055 dlossQ:0.9606 exploreP:0.9016\n",
      "Episode:47 meanR:22.6250 R:39.0000 rate:0.0780 gloss:0.6795 dlossA:0.7075 dlossQ:0.9605 exploreP:0.8981\n",
      "Episode:48 meanR:22.9592 R:39.0000 rate:0.0780 gloss:0.6829 dlossA:0.7077 dlossQ:0.9614 exploreP:0.8947\n",
      "Episode:49 meanR:23.4400 R:47.0000 rate:0.0940 gloss:0.6780 dlossA:0.7063 dlossQ:0.9547 exploreP:0.8905\n",
      "Episode:50 meanR:23.1961 R:11.0000 rate:0.0220 gloss:0.6853 dlossA:0.7064 dlossQ:0.9667 exploreP:0.8895\n",
      "Episode:51 meanR:23.5385 R:41.0000 rate:0.0820 gloss:0.6799 dlossA:0.7064 dlossQ:0.9600 exploreP:0.8859\n",
      "Episode:52 meanR:23.3019 R:11.0000 rate:0.0220 gloss:0.6839 dlossA:0.7070 dlossQ:0.9630 exploreP:0.8850\n",
      "Episode:53 meanR:23.1481 R:15.0000 rate:0.0300 gloss:0.6759 dlossA:0.7096 dlossQ:0.9573 exploreP:0.8837\n",
      "Episode:54 meanR:23.5818 R:47.0000 rate:0.0940 gloss:0.6758 dlossA:0.7064 dlossQ:0.9580 exploreP:0.8796\n",
      "Episode:55 meanR:23.5000 R:19.0000 rate:0.0380 gloss:0.6736 dlossA:0.7056 dlossQ:0.9582 exploreP:0.8779\n",
      "Episode:56 meanR:23.7719 R:39.0000 rate:0.0780 gloss:0.6867 dlossA:0.7073 dlossQ:0.9631 exploreP:0.8745\n",
      "Episode:57 meanR:23.6034 R:14.0000 rate:0.0280 gloss:0.6772 dlossA:0.7043 dlossQ:0.9694 exploreP:0.8733\n",
      "Episode:58 meanR:23.5424 R:20.0000 rate:0.0400 gloss:0.6992 dlossA:0.7040 dlossQ:1.0014 exploreP:0.8716\n",
      "Episode:59 meanR:23.2833 R:8.0000 rate:0.0160 gloss:0.6871 dlossA:0.7052 dlossQ:0.9688 exploreP:0.8709\n",
      "Episode:60 meanR:23.0984 R:12.0000 rate:0.0240 gloss:0.6761 dlossA:0.7031 dlossQ:0.9644 exploreP:0.8699\n",
      "Episode:61 meanR:23.3065 R:36.0000 rate:0.0720 gloss:0.6891 dlossA:0.7066 dlossQ:0.9643 exploreP:0.8668\n",
      "Episode:62 meanR:23.3968 R:29.0000 rate:0.0580 gloss:0.6848 dlossA:0.7091 dlossQ:0.9612 exploreP:0.8643\n",
      "Episode:63 meanR:23.2188 R:12.0000 rate:0.0240 gloss:0.6800 dlossA:0.7081 dlossQ:0.9636 exploreP:0.8633\n",
      "Episode:64 meanR:23.4923 R:41.0000 rate:0.0820 gloss:0.6766 dlossA:0.7085 dlossQ:0.9537 exploreP:0.8598\n",
      "Episode:65 meanR:23.8333 R:46.0000 rate:0.0920 gloss:0.6853 dlossA:0.7093 dlossQ:0.9657 exploreP:0.8559\n",
      "Episode:66 meanR:23.6418 R:11.0000 rate:0.0220 gloss:0.6829 dlossA:0.7057 dlossQ:0.9729 exploreP:0.8550\n",
      "Episode:67 meanR:23.7647 R:32.0000 rate:0.0640 gloss:0.6757 dlossA:0.7060 dlossQ:0.9603 exploreP:0.8523\n",
      "Episode:68 meanR:23.5797 R:11.0000 rate:0.0220 gloss:0.6558 dlossA:0.7061 dlossQ:0.9472 exploreP:0.8513\n",
      "Episode:69 meanR:23.4857 R:17.0000 rate:0.0340 gloss:0.6749 dlossA:0.7038 dlossQ:0.9647 exploreP:0.8499\n",
      "Episode:70 meanR:23.3521 R:14.0000 rate:0.0280 gloss:0.6756 dlossA:0.7073 dlossQ:0.9498 exploreP:0.8487\n",
      "Episode:71 meanR:23.2361 R:15.0000 rate:0.0300 gloss:0.6746 dlossA:0.7048 dlossQ:0.9592 exploreP:0.8475\n",
      "Episode:72 meanR:23.6301 R:52.0000 rate:0.1040 gloss:0.6772 dlossA:0.7063 dlossQ:0.9585 exploreP:0.8431\n",
      "Episode:73 meanR:23.7297 R:31.0000 rate:0.0620 gloss:0.6716 dlossA:0.7071 dlossQ:0.9563 exploreP:0.8406\n",
      "Episode:74 meanR:24.1333 R:54.0000 rate:0.1080 gloss:0.6743 dlossA:0.7064 dlossQ:0.9596 exploreP:0.8361\n",
      "Episode:75 meanR:24.3421 R:40.0000 rate:0.0800 gloss:0.6841 dlossA:0.7076 dlossQ:0.9698 exploreP:0.8328\n",
      "Episode:76 meanR:24.2468 R:17.0000 rate:0.0340 gloss:0.6798 dlossA:0.7061 dlossQ:0.9642 exploreP:0.8314\n",
      "Episode:77 meanR:24.1154 R:14.0000 rate:0.0280 gloss:0.6870 dlossA:0.7084 dlossQ:0.9650 exploreP:0.8302\n",
      "Episode:78 meanR:24.2532 R:35.0000 rate:0.0700 gloss:0.6847 dlossA:0.7062 dlossQ:0.9717 exploreP:0.8274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:24.3750 R:34.0000 rate:0.0680 gloss:0.6712 dlossA:0.7064 dlossQ:0.9536 exploreP:0.8246\n",
      "Episode:80 meanR:24.2469 R:14.0000 rate:0.0280 gloss:0.6734 dlossA:0.7038 dlossQ:0.9752 exploreP:0.8235\n",
      "Episode:81 meanR:24.2927 R:28.0000 rate:0.0560 gloss:0.6807 dlossA:0.7069 dlossQ:0.9572 exploreP:0.8212\n",
      "Episode:82 meanR:24.1325 R:11.0000 rate:0.0220 gloss:0.6901 dlossA:0.7052 dlossQ:0.9913 exploreP:0.8203\n",
      "Episode:83 meanR:24.0119 R:14.0000 rate:0.0280 gloss:0.6888 dlossA:0.7089 dlossQ:0.9645 exploreP:0.8192\n",
      "Episode:84 meanR:24.5059 R:66.0000 rate:0.1320 gloss:0.6734 dlossA:0.7057 dlossQ:0.9605 exploreP:0.8138\n",
      "Episode:85 meanR:24.4419 R:19.0000 rate:0.0380 gloss:0.6716 dlossA:0.7064 dlossQ:0.9583 exploreP:0.8123\n",
      "Episode:86 meanR:24.3678 R:18.0000 rate:0.0360 gloss:0.6740 dlossA:0.7073 dlossQ:0.9541 exploreP:0.8109\n",
      "Episode:87 meanR:24.5341 R:39.0000 rate:0.0780 gloss:0.6770 dlossA:0.7070 dlossQ:0.9622 exploreP:0.8078\n",
      "Episode:88 meanR:24.3933 R:12.0000 rate:0.0240 gloss:0.6680 dlossA:0.6998 dlossQ:0.9675 exploreP:0.8068\n",
      "Episode:89 meanR:24.3333 R:19.0000 rate:0.0380 gloss:0.6676 dlossA:0.7074 dlossQ:0.9512 exploreP:0.8053\n",
      "Episode:90 meanR:24.3407 R:25.0000 rate:0.0500 gloss:0.6732 dlossA:0.7069 dlossQ:0.9620 exploreP:0.8033\n",
      "Episode:91 meanR:24.5543 R:44.0000 rate:0.0880 gloss:0.6817 dlossA:0.7067 dlossQ:0.9685 exploreP:0.7998\n",
      "Episode:92 meanR:24.6667 R:35.0000 rate:0.0700 gloss:0.6745 dlossA:0.7046 dlossQ:0.9605 exploreP:0.7971\n",
      "Episode:93 meanR:24.6489 R:23.0000 rate:0.0460 gloss:0.6858 dlossA:0.7059 dlossQ:0.9769 exploreP:0.7953\n",
      "Episode:94 meanR:24.6526 R:25.0000 rate:0.0500 gloss:0.6704 dlossA:0.7044 dlossQ:0.9651 exploreP:0.7933\n",
      "Episode:95 meanR:25.0417 R:62.0000 rate:0.1240 gloss:0.6845 dlossA:0.7072 dlossQ:0.9803 exploreP:0.7885\n",
      "Episode:96 meanR:25.5979 R:79.0000 rate:0.1580 gloss:0.6759 dlossA:0.7052 dlossQ:0.9661 exploreP:0.7823\n",
      "Episode:97 meanR:26.3571 R:100.0000 rate:0.2000 gloss:0.6784 dlossA:0.7067 dlossQ:0.9628 exploreP:0.7746\n",
      "Episode:98 meanR:26.3535 R:26.0000 rate:0.0520 gloss:0.6831 dlossA:0.7066 dlossQ:0.9662 exploreP:0.7727\n",
      "Episode:99 meanR:26.8300 R:74.0000 rate:0.1480 gloss:0.6782 dlossA:0.7072 dlossQ:0.9658 exploreP:0.7670\n",
      "Episode:100 meanR:27.2900 R:69.0000 rate:0.1380 gloss:0.6767 dlossA:0.7071 dlossQ:0.9621 exploreP:0.7618\n",
      "Episode:101 meanR:27.3100 R:12.0000 rate:0.0240 gloss:0.6742 dlossA:0.7074 dlossQ:0.9604 exploreP:0.7609\n",
      "Episode:102 meanR:27.6300 R:43.0000 rate:0.0860 gloss:0.6818 dlossA:0.7070 dlossQ:0.9710 exploreP:0.7577\n",
      "Episode:103 meanR:27.5400 R:22.0000 rate:0.0440 gloss:0.6764 dlossA:0.7101 dlossQ:0.9559 exploreP:0.7561\n",
      "Episode:104 meanR:27.6800 R:27.0000 rate:0.0540 gloss:0.6774 dlossA:0.7073 dlossQ:0.9686 exploreP:0.7540\n",
      "Episode:105 meanR:27.8100 R:27.0000 rate:0.0540 gloss:0.6814 dlossA:0.7087 dlossQ:0.9715 exploreP:0.7520\n",
      "Episode:106 meanR:27.8100 R:12.0000 rate:0.0240 gloss:0.6874 dlossA:0.7060 dlossQ:0.9710 exploreP:0.7512\n",
      "Episode:107 meanR:27.8100 R:22.0000 rate:0.0440 gloss:0.6793 dlossA:0.7021 dlossQ:0.9765 exploreP:0.7495\n",
      "Episode:108 meanR:28.2300 R:65.0000 rate:0.1300 gloss:0.6832 dlossA:0.7076 dlossQ:0.9734 exploreP:0.7447\n",
      "Episode:109 meanR:28.2600 R:21.0000 rate:0.0420 gloss:0.6720 dlossA:0.7084 dlossQ:0.9556 exploreP:0.7432\n",
      "Episode:110 meanR:28.5200 R:38.0000 rate:0.0760 gloss:0.6764 dlossA:0.7062 dlossQ:0.9630 exploreP:0.7404\n",
      "Episode:111 meanR:28.5400 R:13.0000 rate:0.0260 gloss:0.7012 dlossA:0.7075 dlossQ:0.9669 exploreP:0.7395\n",
      "Episode:112 meanR:28.7900 R:92.0000 rate:0.1840 gloss:0.6850 dlossA:0.7082 dlossQ:0.9698 exploreP:0.7328\n",
      "Episode:113 meanR:29.9400 R:127.0000 rate:0.2540 gloss:0.6803 dlossA:0.7073 dlossQ:0.9717 exploreP:0.7237\n",
      "Episode:114 meanR:30.2800 R:62.0000 rate:0.1240 gloss:0.6773 dlossA:0.7081 dlossQ:0.9655 exploreP:0.7192\n",
      "Episode:115 meanR:30.3700 R:28.0000 rate:0.0560 gloss:0.6738 dlossA:0.7046 dlossQ:0.9649 exploreP:0.7173\n",
      "Episode:116 meanR:31.1100 R:96.0000 rate:0.1920 gloss:0.6782 dlossA:0.7069 dlossQ:0.9678 exploreP:0.7105\n",
      "Episode:117 meanR:31.1400 R:18.0000 rate:0.0360 gloss:0.6801 dlossA:0.7067 dlossQ:0.9752 exploreP:0.7092\n",
      "Episode:118 meanR:31.9200 R:92.0000 rate:0.1840 gloss:0.6792 dlossA:0.7080 dlossQ:0.9690 exploreP:0.7028\n",
      "Episode:119 meanR:32.4300 R:82.0000 rate:0.1640 gloss:0.6818 dlossA:0.7081 dlossQ:0.9700 exploreP:0.6972\n",
      "Episode:120 meanR:32.6400 R:50.0000 rate:0.1000 gloss:0.6810 dlossA:0.7096 dlossQ:0.9727 exploreP:0.6938\n",
      "Episode:121 meanR:32.8400 R:42.0000 rate:0.0840 gloss:0.6823 dlossA:0.7069 dlossQ:0.9707 exploreP:0.6909\n",
      "Episode:122 meanR:33.5500 R:85.0000 rate:0.1700 gloss:0.6798 dlossA:0.7089 dlossQ:0.9652 exploreP:0.6851\n",
      "Episode:123 meanR:33.9400 R:52.0000 rate:0.1040 gloss:0.6842 dlossA:0.7082 dlossQ:0.9716 exploreP:0.6816\n",
      "Episode:124 meanR:34.3300 R:70.0000 rate:0.1400 gloss:0.6766 dlossA:0.7080 dlossQ:0.9670 exploreP:0.6769\n",
      "Episode:125 meanR:34.9000 R:94.0000 rate:0.1880 gloss:0.6845 dlossA:0.7089 dlossQ:0.9730 exploreP:0.6707\n",
      "Episode:126 meanR:35.6200 R:84.0000 rate:0.1680 gloss:0.6804 dlossA:0.7104 dlossQ:0.9692 exploreP:0.6652\n",
      "Episode:127 meanR:36.2100 R:77.0000 rate:0.1540 gloss:0.6849 dlossA:0.7101 dlossQ:0.9719 exploreP:0.6602\n",
      "Episode:128 meanR:36.2100 R:22.0000 rate:0.0440 gloss:0.6840 dlossA:0.7106 dlossQ:0.9736 exploreP:0.6587\n",
      "Episode:129 meanR:36.3000 R:26.0000 rate:0.0520 gloss:0.6877 dlossA:0.7049 dlossQ:0.9967 exploreP:0.6570\n",
      "Episode:130 meanR:36.3100 R:13.0000 rate:0.0260 gloss:0.6753 dlossA:0.7092 dlossQ:0.9681 exploreP:0.6562\n",
      "Episode:131 meanR:36.1100 R:24.0000 rate:0.0480 gloss:0.6870 dlossA:0.7060 dlossQ:0.9905 exploreP:0.6546\n",
      "Episode:132 meanR:36.3500 R:36.0000 rate:0.0720 gloss:0.6771 dlossA:0.7099 dlossQ:0.9654 exploreP:0.6523\n",
      "Episode:133 meanR:37.1500 R:91.0000 rate:0.1820 gloss:0.6818 dlossA:0.7090 dlossQ:0.9741 exploreP:0.6465\n",
      "Episode:134 meanR:37.6100 R:89.0000 rate:0.1780 gloss:0.6741 dlossA:0.7080 dlossQ:0.9677 exploreP:0.6409\n",
      "Episode:135 meanR:37.5800 R:12.0000 rate:0.0240 gloss:0.6824 dlossA:0.7084 dlossQ:0.9775 exploreP:0.6401\n",
      "Episode:136 meanR:37.6800 R:22.0000 rate:0.0440 gloss:0.6819 dlossA:0.7104 dlossQ:0.9584 exploreP:0.6387\n",
      "Episode:137 meanR:37.5500 R:33.0000 rate:0.0660 gloss:0.6827 dlossA:0.7082 dlossQ:0.9594 exploreP:0.6367\n",
      "Episode:138 meanR:37.9400 R:58.0000 rate:0.1160 gloss:0.6880 dlossA:0.7070 dlossQ:0.9838 exploreP:0.6330\n",
      "Episode:139 meanR:38.5300 R:84.0000 rate:0.1680 gloss:0.6838 dlossA:0.7087 dlossQ:0.9785 exploreP:0.6278\n",
      "Episode:140 meanR:39.3000 R:88.0000 rate:0.1760 gloss:0.6832 dlossA:0.7095 dlossQ:0.9700 exploreP:0.6224\n",
      "Episode:141 meanR:40.9100 R:182.0000 rate:0.3640 gloss:0.6818 dlossA:0.7103 dlossQ:0.9721 exploreP:0.6114\n",
      "Episode:142 meanR:42.0400 R:133.0000 rate:0.2660 gloss:0.6821 dlossA:0.7097 dlossQ:0.9689 exploreP:0.6034\n",
      "Episode:143 meanR:42.4900 R:63.0000 rate:0.1260 gloss:0.6798 dlossA:0.7100 dlossQ:0.9697 exploreP:0.5997\n",
      "Episode:144 meanR:42.3000 R:32.0000 rate:0.0640 gloss:0.6780 dlossA:0.7104 dlossQ:0.9631 exploreP:0.5978\n",
      "Episode:145 meanR:43.0900 R:103.0000 rate:0.2060 gloss:0.6833 dlossA:0.7106 dlossQ:0.9699 exploreP:0.5918\n",
      "Episode:146 meanR:42.9800 R:29.0000 rate:0.0580 gloss:0.6907 dlossA:0.7108 dlossQ:0.9776 exploreP:0.5901\n",
      "Episode:147 meanR:43.7100 R:112.0000 rate:0.2240 gloss:0.6847 dlossA:0.7098 dlossQ:0.9746 exploreP:0.5836\n",
      "Episode:148 meanR:43.9900 R:67.0000 rate:0.1340 gloss:0.6834 dlossA:0.7102 dlossQ:0.9704 exploreP:0.5798\n",
      "Episode:149 meanR:43.9400 R:42.0000 rate:0.0840 gloss:0.6869 dlossA:0.7108 dlossQ:0.9746 exploreP:0.5774\n",
      "Episode:150 meanR:43.9600 R:13.0000 rate:0.0260 gloss:0.6746 dlossA:0.7052 dlossQ:0.9785 exploreP:0.5767\n",
      "Episode:151 meanR:43.9200 R:37.0000 rate:0.0740 gloss:0.6844 dlossA:0.7103 dlossQ:0.9688 exploreP:0.5746\n",
      "Episode:152 meanR:44.6600 R:85.0000 rate:0.1700 gloss:0.6832 dlossA:0.7102 dlossQ:0.9765 exploreP:0.5698\n",
      "Episode:153 meanR:45.5200 R:101.0000 rate:0.2020 gloss:0.6810 dlossA:0.7101 dlossQ:0.9693 exploreP:0.5642\n",
      "Episode:154 meanR:45.9300 R:88.0000 rate:0.1760 gloss:0.6890 dlossA:0.7122 dlossQ:0.9702 exploreP:0.5593\n",
      "Episode:155 meanR:46.3600 R:62.0000 rate:0.1240 gloss:0.6869 dlossA:0.7126 dlossQ:0.9674 exploreP:0.5559\n",
      "Episode:156 meanR:46.8400 R:87.0000 rate:0.1740 gloss:0.6882 dlossA:0.7104 dlossQ:0.9766 exploreP:0.5512\n",
      "Episode:157 meanR:46.9600 R:26.0000 rate:0.0520 gloss:0.6785 dlossA:0.7099 dlossQ:0.9710 exploreP:0.5498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:47.8200 R:106.0000 rate:0.2120 gloss:0.6891 dlossA:0.7126 dlossQ:0.9715 exploreP:0.5441\n",
      "Episode:159 meanR:48.5800 R:84.0000 rate:0.1680 gloss:0.6882 dlossA:0.7112 dlossQ:0.9797 exploreP:0.5396\n",
      "Episode:160 meanR:49.3500 R:89.0000 rate:0.1780 gloss:0.6839 dlossA:0.7099 dlossQ:0.9727 exploreP:0.5350\n",
      "Episode:161 meanR:49.7500 R:76.0000 rate:0.1520 gloss:0.6797 dlossA:0.7118 dlossQ:0.9669 exploreP:0.5310\n",
      "Episode:162 meanR:49.6600 R:20.0000 rate:0.0400 gloss:0.7007 dlossA:0.7117 dlossQ:0.9911 exploreP:0.5299\n",
      "Episode:163 meanR:50.5200 R:98.0000 rate:0.1960 gloss:0.6816 dlossA:0.7117 dlossQ:0.9696 exploreP:0.5249\n",
      "Episode:164 meanR:50.3900 R:28.0000 rate:0.0560 gloss:0.6810 dlossA:0.7105 dlossQ:0.9715 exploreP:0.5234\n",
      "Episode:165 meanR:51.0800 R:115.0000 rate:0.2300 gloss:0.6891 dlossA:0.7120 dlossQ:0.9733 exploreP:0.5176\n",
      "Episode:166 meanR:51.8600 R:89.0000 rate:0.1780 gloss:0.6850 dlossA:0.7138 dlossQ:0.9649 exploreP:0.5131\n",
      "Episode:167 meanR:52.8500 R:131.0000 rate:0.2620 gloss:0.6907 dlossA:0.7140 dlossQ:0.9736 exploreP:0.5065\n",
      "Episode:168 meanR:53.7500 R:101.0000 rate:0.2020 gloss:0.6854 dlossA:0.7124 dlossQ:0.9703 exploreP:0.5015\n",
      "Episode:169 meanR:53.8500 R:27.0000 rate:0.0540 gloss:0.6778 dlossA:0.7096 dlossQ:0.9796 exploreP:0.5002\n",
      "Episode:170 meanR:54.3900 R:68.0000 rate:0.1360 gloss:0.6916 dlossA:0.7149 dlossQ:0.9694 exploreP:0.4969\n",
      "Episode:171 meanR:55.0300 R:79.0000 rate:0.1580 gloss:0.6913 dlossA:0.7170 dlossQ:0.9722 exploreP:0.4930\n",
      "Episode:172 meanR:55.0700 R:56.0000 rate:0.1120 gloss:0.6829 dlossA:0.7133 dlossQ:0.9700 exploreP:0.4903\n",
      "Episode:173 meanR:55.6300 R:87.0000 rate:0.1740 gloss:0.6942 dlossA:0.7148 dlossQ:0.9763 exploreP:0.4862\n",
      "Episode:174 meanR:55.6000 R:51.0000 rate:0.1020 gloss:0.6900 dlossA:0.7118 dlossQ:0.9682 exploreP:0.4838\n",
      "Episode:175 meanR:56.0300 R:83.0000 rate:0.1660 gloss:0.6868 dlossA:0.7124 dlossQ:0.9772 exploreP:0.4798\n",
      "Episode:176 meanR:56.5000 R:64.0000 rate:0.1280 gloss:0.6872 dlossA:0.7111 dlossQ:0.9771 exploreP:0.4768\n",
      "Episode:177 meanR:57.9300 R:157.0000 rate:0.3140 gloss:0.6882 dlossA:0.7131 dlossQ:0.9742 exploreP:0.4696\n",
      "Episode:178 meanR:59.0300 R:145.0000 rate:0.2900 gloss:0.6869 dlossA:0.7136 dlossQ:0.9738 exploreP:0.4630\n",
      "Episode:179 meanR:58.9300 R:24.0000 rate:0.0480 gloss:0.6844 dlossA:0.7095 dlossQ:0.9792 exploreP:0.4619\n",
      "Episode:180 meanR:59.8000 R:101.0000 rate:0.2020 gloss:0.6920 dlossA:0.7162 dlossQ:0.9751 exploreP:0.4573\n",
      "Episode:181 meanR:60.6200 R:110.0000 rate:0.2200 gloss:0.7013 dlossA:0.7150 dlossQ:0.9807 exploreP:0.4524\n",
      "Episode:182 meanR:62.4800 R:197.0000 rate:0.3940 gloss:0.6918 dlossA:0.7154 dlossQ:0.9753 exploreP:0.4438\n",
      "Episode:183 meanR:63.5400 R:120.0000 rate:0.2400 gloss:0.6947 dlossA:0.7137 dlossQ:0.9845 exploreP:0.4386\n",
      "Episode:184 meanR:63.7700 R:89.0000 rate:0.1780 gloss:0.6934 dlossA:0.7138 dlossQ:0.9753 exploreP:0.4348\n",
      "Episode:185 meanR:65.1200 R:154.0000 rate:0.3080 gloss:0.6937 dlossA:0.7149 dlossQ:0.9755 exploreP:0.4283\n",
      "Episode:186 meanR:66.0500 R:111.0000 rate:0.2220 gloss:0.6907 dlossA:0.7158 dlossQ:0.9750 exploreP:0.4237\n",
      "Episode:187 meanR:66.7500 R:109.0000 rate:0.2180 gloss:0.6912 dlossA:0.7147 dlossQ:0.9752 exploreP:0.4192\n",
      "Episode:188 meanR:67.3000 R:67.0000 rate:0.1340 gloss:0.6905 dlossA:0.7157 dlossQ:0.9722 exploreP:0.4165\n",
      "Episode:189 meanR:68.3200 R:121.0000 rate:0.2420 gloss:0.6912 dlossA:0.7140 dlossQ:0.9806 exploreP:0.4116\n",
      "Episode:190 meanR:69.2600 R:119.0000 rate:0.2380 gloss:0.6911 dlossA:0.7147 dlossQ:0.9824 exploreP:0.4069\n",
      "Episode:191 meanR:70.8000 R:198.0000 rate:0.3960 gloss:0.6939 dlossA:0.7169 dlossQ:0.9789 exploreP:0.3991\n",
      "Episode:192 meanR:72.2600 R:181.0000 rate:0.3620 gloss:0.6939 dlossA:0.7171 dlossQ:0.9759 exploreP:0.3921\n",
      "Episode:193 meanR:73.6000 R:157.0000 rate:0.3140 gloss:0.6881 dlossA:0.7158 dlossQ:0.9688 exploreP:0.3862\n",
      "Episode:194 meanR:74.3800 R:103.0000 rate:0.2060 gloss:0.6942 dlossA:0.7172 dlossQ:0.9832 exploreP:0.3823\n",
      "Episode:195 meanR:75.1900 R:143.0000 rate:0.2860 gloss:0.6954 dlossA:0.7184 dlossQ:0.9755 exploreP:0.3770\n",
      "Episode:196 meanR:75.2500 R:85.0000 rate:0.1700 gloss:0.7052 dlossA:0.7178 dlossQ:0.9763 exploreP:0.3739\n",
      "Episode:197 meanR:75.1000 R:85.0000 rate:0.1700 gloss:0.7013 dlossA:0.7179 dlossQ:0.9783 exploreP:0.3708\n",
      "Episode:198 meanR:76.2300 R:139.0000 rate:0.2780 gloss:0.6987 dlossA:0.7185 dlossQ:0.9785 exploreP:0.3658\n",
      "Episode:199 meanR:76.8600 R:137.0000 rate:0.2740 gloss:0.6979 dlossA:0.7181 dlossQ:0.9776 exploreP:0.3610\n",
      "Episode:200 meanR:76.4600 R:29.0000 rate:0.0580 gloss:0.6980 dlossA:0.7190 dlossQ:0.9707 exploreP:0.3600\n",
      "Episode:201 meanR:77.7700 R:143.0000 rate:0.2860 gloss:0.6949 dlossA:0.7186 dlossQ:0.9748 exploreP:0.3550\n",
      "Episode:202 meanR:78.4600 R:112.0000 rate:0.2240 gloss:0.6971 dlossA:0.7183 dlossQ:0.9763 exploreP:0.3512\n",
      "Episode:203 meanR:79.1400 R:90.0000 rate:0.1800 gloss:0.6928 dlossA:0.7170 dlossQ:0.9758 exploreP:0.3481\n",
      "Episode:204 meanR:81.1200 R:225.0000 rate:0.4500 gloss:0.6959 dlossA:0.7182 dlossQ:0.9770 exploreP:0.3406\n",
      "Episode:205 meanR:82.6400 R:179.0000 rate:0.3580 gloss:0.6976 dlossA:0.7191 dlossQ:0.9719 exploreP:0.3347\n",
      "Episode:206 meanR:84.4200 R:190.0000 rate:0.3800 gloss:0.6999 dlossA:0.7211 dlossQ:0.9739 exploreP:0.3286\n",
      "Episode:207 meanR:85.1200 R:92.0000 rate:0.1840 gloss:0.6980 dlossA:0.7194 dlossQ:0.9714 exploreP:0.3257\n",
      "Episode:208 meanR:85.6300 R:116.0000 rate:0.2320 gloss:0.6934 dlossA:0.7209 dlossQ:0.9672 exploreP:0.3221\n",
      "Episode:209 meanR:87.2400 R:182.0000 rate:0.3640 gloss:0.7022 dlossA:0.7217 dlossQ:0.9722 exploreP:0.3164\n",
      "Episode:210 meanR:89.1600 R:230.0000 rate:0.4600 gloss:0.6982 dlossA:0.7214 dlossQ:0.9669 exploreP:0.3095\n",
      "Episode:211 meanR:90.4400 R:141.0000 rate:0.2820 gloss:0.7066 dlossA:0.7227 dlossQ:0.9745 exploreP:0.3053\n",
      "Episode:212 meanR:90.3300 R:81.0000 rate:0.1620 gloss:0.7011 dlossA:0.7237 dlossQ:0.9738 exploreP:0.3029\n",
      "Episode:213 meanR:90.8700 R:181.0000 rate:0.3620 gloss:0.6999 dlossA:0.7219 dlossQ:0.9756 exploreP:0.2976\n",
      "Episode:214 meanR:92.0300 R:178.0000 rate:0.3560 gloss:0.6981 dlossA:0.7223 dlossQ:0.9754 exploreP:0.2926\n",
      "Episode:215 meanR:93.0400 R:129.0000 rate:0.2580 gloss:0.7043 dlossA:0.7238 dlossQ:0.9685 exploreP:0.2889\n",
      "Episode:216 meanR:93.8500 R:177.0000 rate:0.3540 gloss:0.7040 dlossA:0.7244 dlossQ:0.9752 exploreP:0.2840\n",
      "Episode:217 meanR:96.8300 R:316.0000 rate:0.6320 gloss:0.7066 dlossA:0.7242 dlossQ:0.9741 exploreP:0.2755\n",
      "Episode:218 meanR:97.5500 R:164.0000 rate:0.3280 gloss:0.7044 dlossA:0.7253 dlossQ:0.9742 exploreP:0.2712\n",
      "Episode:219 meanR:97.6200 R:89.0000 rate:0.1780 gloss:0.7090 dlossA:0.7270 dlossQ:0.9647 exploreP:0.2689\n",
      "Episode:220 meanR:98.4600 R:134.0000 rate:0.2680 gloss:0.7076 dlossA:0.7279 dlossQ:0.9646 exploreP:0.2654\n",
      "Episode:221 meanR:100.0700 R:203.0000 rate:0.4060 gloss:0.7155 dlossA:0.7333 dlossQ:0.9601 exploreP:0.2603\n",
      "Episode:222 meanR:99.9200 R:70.0000 rate:0.1400 gloss:0.7178 dlossA:0.7334 dlossQ:0.9605 exploreP:0.2586\n",
      "Episode:223 meanR:100.5000 R:110.0000 rate:0.2200 gloss:0.7138 dlossA:0.7330 dlossQ:0.9654 exploreP:0.2558\n",
      "Episode:224 meanR:100.8900 R:109.0000 rate:0.2180 gloss:0.7131 dlossA:0.7320 dlossQ:0.9683 exploreP:0.2532\n",
      "Episode:225 meanR:100.9700 R:102.0000 rate:0.2040 gloss:0.7214 dlossA:0.7379 dlossQ:0.9651 exploreP:0.2507\n",
      "Episode:226 meanR:101.4500 R:132.0000 rate:0.2640 gloss:0.7233 dlossA:0.7372 dlossQ:0.9638 exploreP:0.2476\n",
      "Episode:227 meanR:102.1000 R:142.0000 rate:0.2840 gloss:0.7282 dlossA:0.7395 dlossQ:0.9594 exploreP:0.2442\n",
      "Episode:228 meanR:104.1900 R:231.0000 rate:0.4620 gloss:0.7228 dlossA:0.7386 dlossQ:0.9603 exploreP:0.2389\n",
      "Episode:229 meanR:105.0300 R:110.0000 rate:0.2200 gloss:0.7227 dlossA:0.7374 dlossQ:0.9633 exploreP:0.2364\n",
      "Episode:230 meanR:106.0100 R:111.0000 rate:0.2220 gloss:0.7251 dlossA:0.7384 dlossQ:0.9605 exploreP:0.2339\n",
      "Episode:231 meanR:107.1200 R:135.0000 rate:0.2700 gloss:0.7247 dlossA:0.7408 dlossQ:0.9592 exploreP:0.2309\n",
      "Episode:232 meanR:108.9600 R:220.0000 rate:0.4400 gloss:0.7279 dlossA:0.7445 dlossQ:0.9520 exploreP:0.2260\n",
      "Episode:233 meanR:109.9600 R:191.0000 rate:0.3820 gloss:0.7383 dlossA:0.7462 dlossQ:0.9599 exploreP:0.2220\n",
      "Episode:234 meanR:111.0800 R:201.0000 rate:0.4020 gloss:0.7263 dlossA:0.7441 dlossQ:0.9540 exploreP:0.2177\n",
      "Episode:235 meanR:112.2000 R:124.0000 rate:0.2480 gloss:0.7304 dlossA:0.7446 dlossQ:0.9517 exploreP:0.2152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:114.5400 R:256.0000 rate:0.5120 gloss:0.7362 dlossA:0.7472 dlossQ:0.9527 exploreP:0.2100\n",
      "Episode:237 meanR:115.6200 R:141.0000 rate:0.2820 gloss:0.7337 dlossA:0.7497 dlossQ:0.9512 exploreP:0.2072\n",
      "Episode:238 meanR:116.7500 R:171.0000 rate:0.3420 gloss:0.7499 dlossA:0.7561 dlossQ:0.9437 exploreP:0.2039\n",
      "Episode:239 meanR:117.0900 R:118.0000 rate:0.2360 gloss:0.7479 dlossA:0.7580 dlossQ:0.9436 exploreP:0.2016\n",
      "Episode:240 meanR:117.4200 R:121.0000 rate:0.2420 gloss:0.7436 dlossA:0.7526 dlossQ:0.9452 exploreP:0.1993\n",
      "Episode:241 meanR:118.6700 R:307.0000 rate:0.6140 gloss:0.7479 dlossA:0.7565 dlossQ:0.9394 exploreP:0.1936\n",
      "Episode:242 meanR:119.4900 R:215.0000 rate:0.4300 gloss:0.7522 dlossA:0.7581 dlossQ:0.9436 exploreP:0.1896\n",
      "Episode:243 meanR:120.7200 R:186.0000 rate:0.3720 gloss:0.7557 dlossA:0.7630 dlossQ:0.9379 exploreP:0.1863\n",
      "Episode:244 meanR:121.6600 R:126.0000 rate:0.2520 gloss:0.7660 dlossA:0.7644 dlossQ:0.9421 exploreP:0.1841\n",
      "Episode:245 meanR:122.7500 R:212.0000 rate:0.4240 gloss:0.7603 dlossA:0.7620 dlossQ:0.9441 exploreP:0.1805\n",
      "Episode:246 meanR:124.4100 R:195.0000 rate:0.3900 gloss:0.7614 dlossA:0.7631 dlossQ:0.9409 exploreP:0.1772\n",
      "Episode:247 meanR:126.4600 R:317.0000 rate:0.6340 gloss:0.7677 dlossA:0.7680 dlossQ:0.9370 exploreP:0.1720\n",
      "Episode:248 meanR:127.3900 R:160.0000 rate:0.3200 gloss:0.7697 dlossA:0.7701 dlossQ:0.9401 exploreP:0.1694\n",
      "Episode:249 meanR:128.1000 R:113.0000 rate:0.2260 gloss:0.7748 dlossA:0.7685 dlossQ:0.9405 exploreP:0.1676\n",
      "Episode:250 meanR:129.2200 R:125.0000 rate:0.2500 gloss:0.7777 dlossA:0.7744 dlossQ:0.9397 exploreP:0.1656\n",
      "Episode:251 meanR:130.2500 R:140.0000 rate:0.2800 gloss:0.7804 dlossA:0.7741 dlossQ:0.9380 exploreP:0.1635\n",
      "Episode:252 meanR:132.9600 R:356.0000 rate:0.7120 gloss:0.7841 dlossA:0.7756 dlossQ:0.9310 exploreP:0.1581\n",
      "Episode:253 meanR:133.3000 R:135.0000 rate:0.2700 gloss:0.7884 dlossA:0.7816 dlossQ:0.9249 exploreP:0.1561\n",
      "Episode:254 meanR:135.1500 R:273.0000 rate:0.5460 gloss:0.7959 dlossA:0.7878 dlossQ:0.9275 exploreP:0.1522\n",
      "Episode:255 meanR:137.7200 R:319.0000 rate:0.6380 gloss:0.8013 dlossA:0.7910 dlossQ:0.9256 exploreP:0.1477\n",
      "Episode:256 meanR:140.5100 R:366.0000 rate:0.7320 gloss:0.8089 dlossA:0.7919 dlossQ:0.9248 exploreP:0.1428\n",
      "Episode:257 meanR:142.9800 R:273.0000 rate:0.5460 gloss:0.8199 dlossA:0.8052 dlossQ:0.9259 exploreP:0.1392\n",
      "Episode:258 meanR:144.9900 R:307.0000 rate:0.6140 gloss:0.8180 dlossA:0.8041 dlossQ:0.9193 exploreP:0.1353\n",
      "Episode:259 meanR:146.2200 R:207.0000 rate:0.4140 gloss:0.8219 dlossA:0.8007 dlossQ:0.9188 exploreP:0.1327\n",
      "Episode:260 meanR:146.4600 R:113.0000 rate:0.2260 gloss:0.8241 dlossA:0.8019 dlossQ:0.9149 exploreP:0.1314\n",
      "Episode:261 meanR:147.8100 R:211.0000 rate:0.4220 gloss:0.8396 dlossA:0.8125 dlossQ:0.9129 exploreP:0.1288\n",
      "Episode:262 meanR:148.9300 R:132.0000 rate:0.2640 gloss:0.8488 dlossA:0.8227 dlossQ:0.9143 exploreP:0.1273\n",
      "Episode:263 meanR:149.9000 R:195.0000 rate:0.3900 gloss:0.8366 dlossA:0.8088 dlossQ:0.9099 exploreP:0.1250\n",
      "Episode:264 meanR:152.9500 R:333.0000 rate:0.6660 gloss:0.8523 dlossA:0.8205 dlossQ:0.9071 exploreP:0.1212\n",
      "Episode:265 meanR:154.8100 R:301.0000 rate:0.6020 gloss:0.8660 dlossA:0.8288 dlossQ:0.9061 exploreP:0.1179\n",
      "Episode:266 meanR:156.5400 R:262.0000 rate:0.5240 gloss:0.8706 dlossA:0.8342 dlossQ:0.9075 exploreP:0.1151\n",
      "Episode:267 meanR:156.4800 R:125.0000 rate:0.2500 gloss:0.8714 dlossA:0.8359 dlossQ:0.9062 exploreP:0.1138\n",
      "Episode:268 meanR:159.0200 R:355.0000 rate:0.7100 gloss:0.8724 dlossA:0.8363 dlossQ:0.9019 exploreP:0.1102\n",
      "Episode:269 meanR:162.3200 R:357.0000 rate:0.7140 gloss:0.8775 dlossA:0.8385 dlossQ:0.8980 exploreP:0.1067\n",
      "Episode:270 meanR:164.1900 R:255.0000 rate:0.5100 gloss:0.8975 dlossA:0.8523 dlossQ:0.8910 exploreP:0.1043\n",
      "Episode:271 meanR:165.1400 R:174.0000 rate:0.3480 gloss:0.9080 dlossA:0.8542 dlossQ:0.8883 exploreP:0.1026\n",
      "Episode:272 meanR:167.6900 R:311.0000 rate:0.6220 gloss:0.9121 dlossA:0.8629 dlossQ:0.8868 exploreP:0.0998\n",
      "Episode:273 meanR:171.8200 R:500.0000 rate:1.0000 gloss:0.9236 dlossA:0.8685 dlossQ:0.8855 exploreP:0.0954\n",
      "Episode:274 meanR:176.3100 R:500.0000 rate:1.0000 gloss:0.9367 dlossA:0.8791 dlossQ:0.8805 exploreP:0.0913\n",
      "Episode:275 meanR:180.4800 R:500.0000 rate:1.0000 gloss:0.9583 dlossA:0.8801 dlossQ:0.8763 exploreP:0.0873\n",
      "Episode:276 meanR:181.3400 R:150.0000 rate:0.3000 gloss:0.9835 dlossA:0.9074 dlossQ:0.8737 exploreP:0.0861\n",
      "Episode:277 meanR:181.8500 R:208.0000 rate:0.4160 gloss:0.9842 dlossA:0.9136 dlossQ:0.8716 exploreP:0.0846\n",
      "Episode:278 meanR:185.4000 R:500.0000 rate:1.0000 gloss:1.0002 dlossA:0.9161 dlossQ:0.8721 exploreP:0.0809\n",
      "Episode:279 meanR:190.1600 R:500.0000 rate:1.0000 gloss:1.0149 dlossA:0.9222 dlossQ:0.8671 exploreP:0.0775\n",
      "Episode:280 meanR:193.5500 R:440.0000 rate:0.8800 gloss:1.0455 dlossA:0.9338 dlossQ:0.8681 exploreP:0.0746\n",
      "Episode:281 meanR:195.8900 R:344.0000 rate:0.6880 gloss:1.0667 dlossA:0.9508 dlossQ:0.8622 exploreP:0.0724\n",
      "Episode:282 meanR:195.4300 R:151.0000 rate:0.3020 gloss:1.0834 dlossA:0.9829 dlossQ:0.8617 exploreP:0.0715\n",
      "Episode:283 meanR:197.4400 R:321.0000 rate:0.6420 gloss:1.0981 dlossA:0.9705 dlossQ:0.8628 exploreP:0.0695\n",
      "Episode:284 meanR:200.4400 R:389.0000 rate:0.7780 gloss:1.1125 dlossA:0.9935 dlossQ:0.8566 exploreP:0.0672\n",
      "Episode:285 meanR:200.5300 R:163.0000 rate:0.3260 gloss:1.1165 dlossA:0.9854 dlossQ:0.8564 exploreP:0.0663\n",
      "Episode:286 meanR:200.7600 R:134.0000 rate:0.2680 gloss:1.1320 dlossA:1.0029 dlossQ:0.8563 exploreP:0.0656\n",
      "Episode:287 meanR:200.9300 R:126.0000 rate:0.2520 gloss:1.1488 dlossA:1.0205 dlossQ:0.8622 exploreP:0.0649\n",
      "Episode:288 meanR:203.3600 R:310.0000 rate:0.6200 gloss:1.1568 dlossA:1.0030 dlossQ:0.8578 exploreP:0.0632\n",
      "Episode:289 meanR:205.2000 R:305.0000 rate:0.6100 gloss:1.1670 dlossA:1.0225 dlossQ:0.8463 exploreP:0.0616\n",
      "Episode:290 meanR:208.8500 R:484.0000 rate:0.9680 gloss:1.1994 dlossA:1.0432 dlossQ:0.8509 exploreP:0.0592\n",
      "Episode:291 meanR:211.6200 R:475.0000 rate:0.9500 gloss:1.2303 dlossA:1.0650 dlossQ:0.8542 exploreP:0.0569\n",
      "Episode:292 meanR:211.3400 R:153.0000 rate:0.3060 gloss:1.2495 dlossA:1.0762 dlossQ:0.8493 exploreP:0.0562\n",
      "Episode:293 meanR:214.7700 R:500.0000 rate:1.0000 gloss:1.2747 dlossA:1.0830 dlossQ:0.8515 exploreP:0.0539\n",
      "Episode:294 meanR:215.7800 R:204.0000 rate:0.4080 gloss:1.3116 dlossA:1.1524 dlossQ:0.8497 exploreP:0.0530\n",
      "Episode:295 meanR:216.7100 R:236.0000 rate:0.4720 gloss:1.3132 dlossA:1.1079 dlossQ:0.8492 exploreP:0.0520\n",
      "Episode:296 meanR:217.2300 R:137.0000 rate:0.2740 gloss:1.3204 dlossA:1.1188 dlossQ:0.8440 exploreP:0.0515\n",
      "Episode:297 meanR:221.3800 R:500.0000 rate:1.0000 gloss:1.3479 dlossA:1.1526 dlossQ:0.8474 exploreP:0.0494\n",
      "Episode:298 meanR:224.9900 R:500.0000 rate:1.0000 gloss:1.3940 dlossA:1.1618 dlossQ:0.8496 exploreP:0.0475\n",
      "Episode:299 meanR:225.4800 R:186.0000 rate:0.3720 gloss:1.4234 dlossA:1.1990 dlossQ:0.8427 exploreP:0.0468\n",
      "Episode:300 meanR:229.7300 R:454.0000 rate:0.9080 gloss:1.4564 dlossA:1.2344 dlossQ:0.8492 exploreP:0.0452\n",
      "Episode:301 meanR:231.7700 R:347.0000 rate:0.6940 gloss:1.4700 dlossA:1.2464 dlossQ:0.8433 exploreP:0.0440\n",
      "Episode:302 meanR:232.4100 R:176.0000 rate:0.3520 gloss:1.4867 dlossA:1.2274 dlossQ:0.8457 exploreP:0.0434\n",
      "Episode:303 meanR:233.0900 R:158.0000 rate:0.3160 gloss:1.5044 dlossA:1.2551 dlossQ:0.8429 exploreP:0.0429\n",
      "Episode:304 meanR:232.2000 R:136.0000 rate:0.2720 gloss:1.5158 dlossA:1.2586 dlossQ:0.8430 exploreP:0.0424\n",
      "Episode:305 meanR:235.4100 R:500.0000 rate:1.0000 gloss:1.5380 dlossA:1.3154 dlossQ:0.8429 exploreP:0.0408\n",
      "Episode:306 meanR:238.5100 R:500.0000 rate:1.0000 gloss:1.5780 dlossA:1.3215 dlossQ:0.8480 exploreP:0.0393\n",
      "Episode:307 meanR:242.5900 R:500.0000 rate:1.0000 gloss:1.6143 dlossA:1.3374 dlossQ:0.8419 exploreP:0.0379\n",
      "Episode:308 meanR:246.4300 R:500.0000 rate:1.0000 gloss:1.6499 dlossA:1.3385 dlossQ:0.8396 exploreP:0.0365\n",
      "Episode:309 meanR:249.6100 R:500.0000 rate:1.0000 gloss:1.7092 dlossA:1.4061 dlossQ:0.8360 exploreP:0.0353\n",
      "Episode:310 meanR:248.9000 R:159.0000 rate:0.3180 gloss:1.7522 dlossA:1.4682 dlossQ:0.8348 exploreP:0.0349\n",
      "Episode:311 meanR:252.4900 R:500.0000 rate:1.0000 gloss:1.7671 dlossA:1.4855 dlossQ:0.8450 exploreP:0.0336\n",
      "Episode:312 meanR:256.6800 R:500.0000 rate:1.0000 gloss:1.8059 dlossA:1.5309 dlossQ:0.8375 exploreP:0.0325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:313 meanR:256.9700 R:210.0000 rate:0.4200 gloss:1.8315 dlossA:1.5261 dlossQ:0.8362 exploreP:0.0320\n",
      "Episode:314 meanR:257.0000 R:181.0000 rate:0.3620 gloss:1.8665 dlossA:1.5751 dlossQ:0.8436 exploreP:0.0316\n",
      "Episode:315 meanR:260.7100 R:500.0000 rate:1.0000 gloss:1.8670 dlossA:1.5204 dlossQ:0.8348 exploreP:0.0306\n",
      "Episode:316 meanR:262.5800 R:364.0000 rate:0.7280 gloss:1.9278 dlossA:1.5429 dlossQ:0.8319 exploreP:0.0298\n",
      "Episode:317 meanR:260.9200 R:150.0000 rate:0.3000 gloss:1.9636 dlossA:1.5960 dlossQ:0.8310 exploreP:0.0295\n",
      "Episode:318 meanR:264.2800 R:500.0000 rate:1.0000 gloss:2.0041 dlossA:1.6867 dlossQ:0.8356 exploreP:0.0286\n",
      "Episode:319 meanR:268.3900 R:500.0000 rate:1.0000 gloss:2.0434 dlossA:1.6199 dlossQ:0.8314 exploreP:0.0277\n",
      "Episode:320 meanR:272.0500 R:500.0000 rate:1.0000 gloss:2.1137 dlossA:1.7431 dlossQ:0.8218 exploreP:0.0268\n",
      "Episode:321 meanR:275.0200 R:500.0000 rate:1.0000 gloss:2.1907 dlossA:1.7794 dlossQ:0.8387 exploreP:0.0260\n",
      "Episode:322 meanR:279.3200 R:500.0000 rate:1.0000 gloss:2.2478 dlossA:1.8630 dlossQ:0.8322 exploreP:0.0252\n",
      "Episode:323 meanR:283.2200 R:500.0000 rate:1.0000 gloss:2.3237 dlossA:1.9379 dlossQ:0.8478 exploreP:0.0245\n",
      "Episode:324 meanR:287.1300 R:500.0000 rate:1.0000 gloss:2.3854 dlossA:2.0084 dlossQ:0.8591 exploreP:0.0238\n",
      "Episode:325 meanR:291.1100 R:500.0000 rate:1.0000 gloss:2.4545 dlossA:2.0275 dlossQ:0.8667 exploreP:0.0231\n",
      "Episode:326 meanR:294.7900 R:500.0000 rate:1.0000 gloss:2.5035 dlossA:2.1073 dlossQ:0.8862 exploreP:0.0225\n",
      "Episode:327 meanR:298.3700 R:500.0000 rate:1.0000 gloss:2.5672 dlossA:2.1372 dlossQ:0.8625 exploreP:0.0219\n",
      "Episode:328 meanR:297.4500 R:139.0000 rate:0.2780 gloss:2.6291 dlossA:2.2232 dlossQ:0.8885 exploreP:0.0217\n",
      "Episode:329 meanR:301.3500 R:500.0000 rate:1.0000 gloss:2.6566 dlossA:2.1358 dlossQ:0.8853 exploreP:0.0211\n",
      "Episode:330 meanR:303.7700 R:353.0000 rate:0.7060 gloss:2.7926 dlossA:2.3075 dlossQ:0.9318 exploreP:0.0207\n",
      "Episode:331 meanR:303.8400 R:142.0000 rate:0.2840 gloss:2.8115 dlossA:2.3844 dlossQ:0.8998 exploreP:0.0206\n",
      "Episode:332 meanR:306.6400 R:500.0000 rate:1.0000 gloss:2.8303 dlossA:2.3954 dlossQ:0.9548 exploreP:0.0201\n",
      "Episode:333 meanR:309.7300 R:500.0000 rate:1.0000 gloss:2.8729 dlossA:2.4132 dlossQ:0.9011 exploreP:0.0196\n",
      "Episode:334 meanR:312.7200 R:500.0000 rate:1.0000 gloss:2.9744 dlossA:2.4695 dlossQ:0.9068 exploreP:0.0191\n",
      "Episode:335 meanR:316.4800 R:500.0000 rate:1.0000 gloss:3.0617 dlossA:2.5479 dlossQ:0.9391 exploreP:0.0187\n",
      "Episode:336 meanR:315.5400 R:162.0000 rate:0.3240 gloss:3.1072 dlossA:2.5310 dlossQ:0.9265 exploreP:0.0185\n",
      "Episode:337 meanR:319.1300 R:500.0000 rate:1.0000 gloss:3.1978 dlossA:2.6251 dlossQ:0.9258 exploreP:0.0181\n",
      "Episode:338 meanR:322.4200 R:500.0000 rate:1.0000 gloss:3.3277 dlossA:2.7478 dlossQ:1.0053 exploreP:0.0177\n",
      "Episode:339 meanR:324.3400 R:310.0000 rate:0.6200 gloss:3.3881 dlossA:2.8374 dlossQ:0.9455 exploreP:0.0175\n",
      "Episode:340 meanR:328.1300 R:500.0000 rate:1.0000 gloss:3.5071 dlossA:2.9943 dlossQ:1.0036 exploreP:0.0171\n",
      "Episode:341 meanR:326.6900 R:163.0000 rate:0.3260 gloss:3.5585 dlossA:2.8968 dlossQ:0.9601 exploreP:0.0170\n",
      "Episode:342 meanR:329.5400 R:500.0000 rate:1.0000 gloss:3.6164 dlossA:3.0164 dlossQ:1.0305 exploreP:0.0167\n",
      "Episode:343 meanR:329.1800 R:150.0000 rate:0.3000 gloss:3.7085 dlossA:3.0966 dlossQ:1.0239 exploreP:0.0166\n",
      "Episode:344 meanR:332.9200 R:500.0000 rate:1.0000 gloss:3.7696 dlossA:3.2582 dlossQ:1.1437 exploreP:0.0162\n",
      "Episode:345 meanR:332.5600 R:176.0000 rate:0.3520 gloss:3.7915 dlossA:3.2790 dlossQ:1.0167 exploreP:0.0161\n",
      "Episode:346 meanR:332.1900 R:158.0000 rate:0.3160 gloss:3.7765 dlossA:3.1184 dlossQ:1.0927 exploreP:0.0160\n",
      "Episode:347 meanR:334.0200 R:500.0000 rate:1.0000 gloss:3.9221 dlossA:3.3846 dlossQ:1.0642 exploreP:0.0157\n",
      "Episode:348 meanR:337.4200 R:500.0000 rate:1.0000 gloss:3.9938 dlossA:3.4119 dlossQ:1.0616 exploreP:0.0155\n",
      "Episode:349 meanR:339.1100 R:282.0000 rate:0.5640 gloss:4.1825 dlossA:3.6576 dlossQ:1.1623 exploreP:0.0153\n",
      "Episode:350 meanR:342.8600 R:500.0000 rate:1.0000 gloss:4.1384 dlossA:3.5583 dlossQ:1.1685 exploreP:0.0150\n",
      "Episode:351 meanR:346.4600 R:500.0000 rate:1.0000 gloss:4.2235 dlossA:3.4657 dlossQ:1.1204 exploreP:0.0148\n",
      "Episode:352 meanR:347.9000 R:500.0000 rate:1.0000 gloss:4.3018 dlossA:3.5369 dlossQ:1.1687 exploreP:0.0146\n",
      "Episode:353 meanR:351.5500 R:500.0000 rate:1.0000 gloss:4.4866 dlossA:3.6982 dlossQ:1.1664 exploreP:0.0143\n",
      "Episode:354 meanR:350.6500 R:183.0000 rate:0.3660 gloss:4.5904 dlossA:3.7965 dlossQ:1.4243 exploreP:0.0143\n",
      "Episode:355 meanR:350.2500 R:279.0000 rate:0.5580 gloss:4.5845 dlossA:3.7738 dlossQ:1.1968 exploreP:0.0141\n",
      "Episode:356 meanR:351.5900 R:500.0000 rate:1.0000 gloss:4.7609 dlossA:3.8966 dlossQ:1.2232 exploreP:0.0139\n",
      "Episode:357 meanR:352.2000 R:334.0000 rate:0.6680 gloss:4.8781 dlossA:4.1438 dlossQ:1.3153 exploreP:0.0138\n",
      "Episode:358 meanR:351.0500 R:192.0000 rate:0.3840 gloss:4.9108 dlossA:4.0500 dlossQ:1.2299 exploreP:0.0137\n",
      "Episode:359 meanR:353.9800 R:500.0000 rate:1.0000 gloss:5.0451 dlossA:4.0697 dlossQ:1.2871 exploreP:0.0136\n",
      "Episode:360 meanR:355.9500 R:310.0000 rate:0.6200 gloss:5.2175 dlossA:4.4182 dlossQ:1.3901 exploreP:0.0135\n",
      "Episode:361 meanR:358.8400 R:500.0000 rate:1.0000 gloss:5.2918 dlossA:4.3656 dlossQ:1.3153 exploreP:0.0133\n",
      "Episode:362 meanR:362.5200 R:500.0000 rate:1.0000 gloss:5.3760 dlossA:4.3853 dlossQ:1.5819 exploreP:0.0131\n",
      "Episode:363 meanR:362.5100 R:194.0000 rate:0.3880 gloss:5.5062 dlossA:4.5761 dlossQ:1.3961 exploreP:0.0131\n",
      "Episode:364 meanR:362.3000 R:312.0000 rate:0.6240 gloss:5.6054 dlossA:4.6815 dlossQ:1.4898 exploreP:0.0130\n",
      "Episode:365 meanR:361.5400 R:225.0000 rate:0.4500 gloss:5.6382 dlossA:4.6144 dlossQ:1.5007 exploreP:0.0129\n",
      "Episode:366 meanR:363.9200 R:500.0000 rate:1.0000 gloss:5.7154 dlossA:4.6769 dlossQ:1.5079 exploreP:0.0128\n",
      "Episode:367 meanR:364.9300 R:226.0000 rate:0.4520 gloss:5.8135 dlossA:4.7262 dlossQ:1.4606 exploreP:0.0127\n",
      "Episode:368 meanR:364.0000 R:262.0000 rate:0.5240 gloss:6.0031 dlossA:5.0456 dlossQ:2.3002 exploreP:0.0126\n",
      "Episode:369 meanR:365.4300 R:500.0000 rate:1.0000 gloss:5.8363 dlossA:4.8381 dlossQ:1.4888 exploreP:0.0125\n",
      "Episode:370 meanR:364.6000 R:172.0000 rate:0.3440 gloss:5.7580 dlossA:4.5666 dlossQ:1.6725 exploreP:0.0125\n",
      "Episode:371 meanR:367.8600 R:500.0000 rate:1.0000 gloss:5.8510 dlossA:4.8752 dlossQ:1.5566 exploreP:0.0123\n",
      "Episode:372 meanR:369.7500 R:500.0000 rate:1.0000 gloss:6.0184 dlossA:4.7761 dlossQ:1.5644 exploreP:0.0122\n",
      "Episode:373 meanR:369.7500 R:500.0000 rate:1.0000 gloss:6.0023 dlossA:4.7077 dlossQ:1.6646 exploreP:0.0121\n",
      "Episode:374 meanR:366.8500 R:210.0000 rate:0.4200 gloss:6.1993 dlossA:4.7075 dlossQ:1.5099 exploreP:0.0121\n",
      "Episode:375 meanR:363.9900 R:214.0000 rate:0.4280 gloss:6.3904 dlossA:5.1062 dlossQ:1.5673 exploreP:0.0120\n",
      "Episode:376 meanR:367.4900 R:500.0000 rate:1.0000 gloss:6.3443 dlossA:4.5868 dlossQ:1.6895 exploreP:0.0119\n",
      "Episode:377 meanR:369.1300 R:372.0000 rate:0.7440 gloss:6.5907 dlossA:5.0666 dlossQ:1.6488 exploreP:0.0119\n",
      "Episode:378 meanR:369.1300 R:500.0000 rate:1.0000 gloss:6.8397 dlossA:4.9734 dlossQ:1.7674 exploreP:0.0118\n",
      "Episode:379 meanR:367.4400 R:331.0000 rate:0.6620 gloss:7.2930 dlossA:5.8039 dlossQ:1.9255 exploreP:0.0117\n",
      "Episode:380 meanR:368.0400 R:500.0000 rate:1.0000 gloss:7.2864 dlossA:5.3330 dlossQ:1.8876 exploreP:0.0116\n",
      "Episode:381 meanR:369.6000 R:500.0000 rate:1.0000 gloss:7.4887 dlossA:5.8287 dlossQ:2.7830 exploreP:0.0115\n",
      "Episode:382 meanR:373.0900 R:500.0000 rate:1.0000 gloss:7.4946 dlossA:5.7844 dlossQ:2.0669 exploreP:0.0115\n",
      "Episode:383 meanR:374.8800 R:500.0000 rate:1.0000 gloss:7.5286 dlossA:5.6501 dlossQ:2.7983 exploreP:0.0114\n",
      "Episode:384 meanR:375.9900 R:500.0000 rate:1.0000 gloss:7.4503 dlossA:5.3908 dlossQ:2.1598 exploreP:0.0113\n",
      "Episode:385 meanR:379.3600 R:500.0000 rate:1.0000 gloss:7.6993 dlossA:5.7274 dlossQ:2.4780 exploreP:0.0113\n",
      "Episode:386 meanR:383.0200 R:500.0000 rate:1.0000 gloss:7.9154 dlossA:6.1080 dlossQ:2.3108 exploreP:0.0112\n",
      "Episode:387 meanR:386.7600 R:500.0000 rate:1.0000 gloss:8.0735 dlossA:6.1030 dlossQ:2.3995 exploreP:0.0111\n",
      "Episode:388 meanR:386.8400 R:318.0000 rate:0.6360 gloss:8.1513 dlossA:5.6893 dlossQ:2.3493 exploreP:0.0111\n",
      "Episode:389 meanR:388.7900 R:500.0000 rate:1.0000 gloss:8.5378 dlossA:6.7551 dlossQ:2.5841 exploreP:0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:390 meanR:388.9500 R:500.0000 rate:1.0000 gloss:8.4236 dlossA:5.9231 dlossQ:2.4095 exploreP:0.0110\n",
      "Episode:391 meanR:389.2000 R:500.0000 rate:1.0000 gloss:8.7637 dlossA:6.0484 dlossQ:2.6191 exploreP:0.0110\n",
      "Episode:392 meanR:392.6700 R:500.0000 rate:1.0000 gloss:9.2068 dlossA:6.5388 dlossQ:2.6325 exploreP:0.0109\n",
      "Episode:393 meanR:390.3100 R:264.0000 rate:0.5280 gloss:9.4086 dlossA:6.4933 dlossQ:2.7917 exploreP:0.0109\n",
      "Episode:394 meanR:391.1700 R:290.0000 rate:0.5800 gloss:9.6113 dlossA:6.9908 dlossQ:3.9264 exploreP:0.0109\n",
      "Episode:395 meanR:393.8100 R:500.0000 rate:1.0000 gloss:9.5794 dlossA:6.8828 dlossQ:3.0777 exploreP:0.0108\n",
      "Episode:396 meanR:395.1700 R:273.0000 rate:0.5460 gloss:9.6829 dlossA:6.6487 dlossQ:3.2053 exploreP:0.0108\n",
      "Episode:397 meanR:395.1700 R:500.0000 rate:1.0000 gloss:10.2161 dlossA:7.2026 dlossQ:3.2662 exploreP:0.0108\n",
      "Episode:398 meanR:392.6100 R:244.0000 rate:0.4880 gloss:10.4846 dlossA:8.1352 dlossQ:4.3114 exploreP:0.0107\n",
      "Episode:399 meanR:395.7500 R:500.0000 rate:1.0000 gloss:10.4612 dlossA:7.4802 dlossQ:3.7403 exploreP:0.0107\n",
      "Episode:400 meanR:396.2100 R:500.0000 rate:1.0000 gloss:10.4929 dlossA:7.3166 dlossQ:3.5348 exploreP:0.0107\n",
      "Episode:401 meanR:394.8700 R:213.0000 rate:0.4260 gloss:10.7182 dlossA:7.3575 dlossQ:5.0976 exploreP:0.0107\n",
      "Episode:402 meanR:398.1100 R:500.0000 rate:1.0000 gloss:10.6793 dlossA:7.2963 dlossQ:3.6373 exploreP:0.0106\n",
      "Episode:403 meanR:401.5300 R:500.0000 rate:1.0000 gloss:11.0128 dlossA:7.8952 dlossQ:4.2610 exploreP:0.0106\n",
      "Episode:404 meanR:405.1700 R:500.0000 rate:1.0000 gloss:11.2455 dlossA:8.0818 dlossQ:4.2081 exploreP:0.0106\n",
      "Episode:405 meanR:405.1700 R:500.0000 rate:1.0000 gloss:11.3642 dlossA:7.2163 dlossQ:4.0744 exploreP:0.0105\n",
      "Episode:406 meanR:403.8900 R:372.0000 rate:0.7440 gloss:11.7808 dlossA:8.1058 dlossQ:4.8257 exploreP:0.0105\n",
      "Episode:407 meanR:403.8900 R:500.0000 rate:1.0000 gloss:11.8716 dlossA:7.6321 dlossQ:4.4727 exploreP:0.0105\n",
      "Episode:408 meanR:401.4100 R:252.0000 rate:0.5040 gloss:12.1895 dlossA:8.1598 dlossQ:4.7426 exploreP:0.0105\n",
      "Episode:409 meanR:401.4100 R:500.0000 rate:1.0000 gloss:12.6700 dlossA:8.8231 dlossQ:4.5137 exploreP:0.0105\n",
      "Episode:410 meanR:402.8000 R:298.0000 rate:0.5960 gloss:12.5981 dlossA:8.3021 dlossQ:5.0503 exploreP:0.0104\n",
      "Episode:411 meanR:400.7400 R:294.0000 rate:0.5880 gloss:12.7750 dlossA:8.5619 dlossQ:5.8851 exploreP:0.0104\n",
      "Episode:412 meanR:400.7400 R:500.0000 rate:1.0000 gloss:13.1103 dlossA:8.5864 dlossQ:4.9825 exploreP:0.0104\n",
      "Episode:413 meanR:403.6400 R:500.0000 rate:1.0000 gloss:13.4778 dlossA:8.5844 dlossQ:6.3815 exploreP:0.0104\n",
      "Episode:414 meanR:406.8300 R:500.0000 rate:1.0000 gloss:13.6718 dlossA:8.9149 dlossQ:5.2856 exploreP:0.0104\n",
      "Episode:415 meanR:406.8300 R:500.0000 rate:1.0000 gloss:13.9768 dlossA:8.7057 dlossQ:6.6305 exploreP:0.0104\n",
      "Episode:416 meanR:408.1900 R:500.0000 rate:1.0000 gloss:13.9416 dlossA:8.7580 dlossQ:5.4269 exploreP:0.0103\n",
      "Episode:417 meanR:411.6900 R:500.0000 rate:1.0000 gloss:14.2426 dlossA:8.5357 dlossQ:6.2380 exploreP:0.0103\n",
      "Episode:418 meanR:411.6900 R:500.0000 rate:1.0000 gloss:14.2420 dlossA:8.8683 dlossQ:7.4587 exploreP:0.0103\n",
      "Episode:419 meanR:408.4100 R:172.0000 rate:0.3440 gloss:15.1924 dlossA:10.3270 dlossQ:7.3643 exploreP:0.0103\n",
      "Episode:420 meanR:408.4100 R:500.0000 rate:1.0000 gloss:14.4189 dlossA:8.7079 dlossQ:6.1182 exploreP:0.0103\n",
      "Episode:421 meanR:408.4100 R:500.0000 rate:1.0000 gloss:14.4139 dlossA:8.4626 dlossQ:6.6128 exploreP:0.0103\n",
      "Episode:422 meanR:408.4100 R:500.0000 rate:1.0000 gloss:15.1361 dlossA:9.5293 dlossQ:7.2282 exploreP:0.0103\n",
      "Episode:423 meanR:408.4100 R:500.0000 rate:1.0000 gloss:15.1553 dlossA:9.0212 dlossQ:6.5238 exploreP:0.0102\n",
      "Episode:424 meanR:408.4100 R:500.0000 rate:1.0000 gloss:15.5926 dlossA:9.7068 dlossQ:9.9183 exploreP:0.0102\n",
      "Episode:425 meanR:408.4100 R:500.0000 rate:1.0000 gloss:14.5089 dlossA:7.5813 dlossQ:5.9295 exploreP:0.0102\n",
      "Episode:426 meanR:408.4100 R:500.0000 rate:1.0000 gloss:14.7812 dlossA:8.2386 dlossQ:6.6044 exploreP:0.0102\n",
      "Episode:427 meanR:408.4100 R:500.0000 rate:1.0000 gloss:14.6246 dlossA:8.8636 dlossQ:5.7501 exploreP:0.0102\n",
      "Episode:428 meanR:412.0200 R:500.0000 rate:1.0000 gloss:13.9157 dlossA:7.9178 dlossQ:6.2051 exploreP:0.0102\n",
      "Episode:429 meanR:412.0200 R:500.0000 rate:1.0000 gloss:13.7831 dlossA:7.4358 dlossQ:4.9010 exploreP:0.0102\n",
      "Episode:430 meanR:413.4900 R:500.0000 rate:1.0000 gloss:13.9334 dlossA:7.5424 dlossQ:4.7528 exploreP:0.0102\n",
      "Episode:431 meanR:414.4200 R:235.0000 rate:0.4700 gloss:14.3359 dlossA:8.1632 dlossQ:5.0711 exploreP:0.0102\n",
      "Episode:432 meanR:414.4200 R:500.0000 rate:1.0000 gloss:13.8739 dlossA:7.1381 dlossQ:5.1970 exploreP:0.0102\n",
      "Episode:433 meanR:410.8900 R:147.0000 rate:0.2940 gloss:13.6713 dlossA:6.5168 dlossQ:4.5446 exploreP:0.0102\n",
      "Episode:434 meanR:410.8900 R:500.0000 rate:1.0000 gloss:14.5989 dlossA:7.9867 dlossQ:4.8290 exploreP:0.0101\n",
      "Episode:435 meanR:408.8100 R:292.0000 rate:0.5840 gloss:14.1799 dlossA:6.9647 dlossQ:5.1554 exploreP:0.0101\n",
      "Episode:436 meanR:412.1900 R:500.0000 rate:1.0000 gloss:14.9017 dlossA:7.6116 dlossQ:5.7908 exploreP:0.0101\n",
      "Episode:437 meanR:412.1900 R:500.0000 rate:1.0000 gloss:15.3405 dlossA:7.4987 dlossQ:5.5788 exploreP:0.0101\n",
      "Episode:438 meanR:412.1900 R:500.0000 rate:1.0000 gloss:15.7943 dlossA:7.4609 dlossQ:5.7066 exploreP:0.0101\n",
      "Episode:439 meanR:414.0900 R:500.0000 rate:1.0000 gloss:16.7110 dlossA:8.5207 dlossQ:8.8862 exploreP:0.0101\n",
      "Episode:440 meanR:414.0900 R:500.0000 rate:1.0000 gloss:16.0277 dlossA:7.9209 dlossQ:8.6626 exploreP:0.0101\n",
      "Episode:441 meanR:417.4600 R:500.0000 rate:1.0000 gloss:15.7326 dlossA:7.1449 dlossQ:6.7752 exploreP:0.0101\n",
      "Episode:442 meanR:413.8300 R:137.0000 rate:0.2740 gloss:15.7827 dlossA:6.9148 dlossQ:5.9952 exploreP:0.0101\n",
      "Episode:443 meanR:417.3300 R:500.0000 rate:1.0000 gloss:16.1189 dlossA:7.2661 dlossQ:6.4099 exploreP:0.0101\n",
      "Episode:444 meanR:417.3300 R:500.0000 rate:1.0000 gloss:16.8843 dlossA:7.9289 dlossQ:7.6418 exploreP:0.0101\n",
      "Episode:445 meanR:420.5700 R:500.0000 rate:1.0000 gloss:17.0320 dlossA:7.0516 dlossQ:6.3256 exploreP:0.0101\n",
      "Episode:446 meanR:422.6900 R:370.0000 rate:0.7400 gloss:17.7984 dlossA:7.3066 dlossQ:8.4406 exploreP:0.0101\n",
      "Episode:447 meanR:422.6900 R:500.0000 rate:1.0000 gloss:18.1327 dlossA:8.3531 dlossQ:8.5937 exploreP:0.0101\n",
      "Episode:448 meanR:422.6900 R:500.0000 rate:1.0000 gloss:18.3412 dlossA:7.5097 dlossQ:7.9306 exploreP:0.0101\n",
      "Episode:449 meanR:424.8700 R:500.0000 rate:1.0000 gloss:18.7799 dlossA:8.6698 dlossQ:13.8252 exploreP:0.0101\n",
      "Episode:450 meanR:421.4300 R:156.0000 rate:0.3120 gloss:17.5894 dlossA:7.3332 dlossQ:8.6018 exploreP:0.0101\n",
      "Episode:451 meanR:421.4300 R:500.0000 rate:1.0000 gloss:17.4636 dlossA:6.7669 dlossQ:8.3225 exploreP:0.0101\n",
      "Episode:452 meanR:421.4300 R:500.0000 rate:1.0000 gloss:17.6977 dlossA:6.9403 dlossQ:7.9059 exploreP:0.0101\n",
      "Episode:453 meanR:419.7900 R:336.0000 rate:0.6720 gloss:18.2711 dlossA:7.3514 dlossQ:9.8843 exploreP:0.0101\n",
      "Episode:454 meanR:422.9600 R:500.0000 rate:1.0000 gloss:17.6100 dlossA:6.7028 dlossQ:7.2702 exploreP:0.0101\n",
      "Episode:455 meanR:421.6900 R:152.0000 rate:0.3040 gloss:18.8202 dlossA:8.2678 dlossQ:13.7413 exploreP:0.0101\n",
      "Episode:456 meanR:421.6900 R:500.0000 rate:1.0000 gloss:18.2547 dlossA:7.1467 dlossQ:8.6099 exploreP:0.0101\n",
      "Episode:457 meanR:419.9800 R:163.0000 rate:0.3260 gloss:19.2022 dlossA:9.0236 dlossQ:7.3208 exploreP:0.0101\n",
      "Episode:458 meanR:422.1000 R:404.0000 rate:0.8080 gloss:18.1226 dlossA:6.6882 dlossQ:7.8053 exploreP:0.0101\n",
      "Episode:459 meanR:418.8800 R:178.0000 rate:0.3560 gloss:19.5805 dlossA:7.9294 dlossQ:10.0105 exploreP:0.0101\n",
      "Episode:460 meanR:419.3800 R:360.0000 rate:0.7200 gloss:18.6812 dlossA:7.1436 dlossQ:8.8414 exploreP:0.0101\n",
      "Episode:461 meanR:419.3800 R:500.0000 rate:1.0000 gloss:18.8895 dlossA:6.8518 dlossQ:7.5418 exploreP:0.0100\n",
      "Episode:462 meanR:419.3800 R:500.0000 rate:1.0000 gloss:19.2214 dlossA:7.1975 dlossQ:9.2556 exploreP:0.0100\n",
      "Episode:463 meanR:420.5600 R:312.0000 rate:0.6240 gloss:18.9405 dlossA:5.7533 dlossQ:7.3792 exploreP:0.0100\n",
      "Episode:464 meanR:422.4400 R:500.0000 rate:1.0000 gloss:19.6711 dlossA:6.9238 dlossQ:15.4015 exploreP:0.0100\n",
      "Episode:465 meanR:425.1900 R:500.0000 rate:1.0000 gloss:18.6931 dlossA:6.1263 dlossQ:7.7787 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:466 meanR:423.8700 R:368.0000 rate:0.7360 gloss:18.5177 dlossA:5.6874 dlossQ:8.0170 exploreP:0.0100\n",
      "Episode:467 meanR:426.6100 R:500.0000 rate:1.0000 gloss:18.9184 dlossA:6.8769 dlossQ:9.6847 exploreP:0.0100\n",
      "Episode:468 meanR:428.9900 R:500.0000 rate:1.0000 gloss:18.8711 dlossA:5.6069 dlossQ:8.5960 exploreP:0.0100\n",
      "Episode:469 meanR:427.8300 R:384.0000 rate:0.7680 gloss:18.8266 dlossA:5.3497 dlossQ:11.8100 exploreP:0.0100\n",
      "Episode:470 meanR:431.1100 R:500.0000 rate:1.0000 gloss:19.0994 dlossA:6.1732 dlossQ:7.9480 exploreP:0.0100\n",
      "Episode:471 meanR:429.4500 R:334.0000 rate:0.6680 gloss:19.1615 dlossA:5.6892 dlossQ:7.9406 exploreP:0.0100\n",
      "Episode:472 meanR:426.4100 R:196.0000 rate:0.3920 gloss:19.6430 dlossA:6.3687 dlossQ:8.8485 exploreP:0.0100\n",
      "Episode:473 meanR:426.4100 R:500.0000 rate:1.0000 gloss:19.6851 dlossA:5.0327 dlossQ:9.7191 exploreP:0.0100\n",
      "Episode:474 meanR:429.3100 R:500.0000 rate:1.0000 gloss:20.6921 dlossA:6.5710 dlossQ:9.8215 exploreP:0.0100\n",
      "Episode:475 meanR:430.1100 R:294.0000 rate:0.5880 gloss:20.9723 dlossA:6.8613 dlossQ:10.4403 exploreP:0.0100\n",
      "Episode:476 meanR:430.1100 R:500.0000 rate:1.0000 gloss:20.9657 dlossA:5.2018 dlossQ:10.1191 exploreP:0.0100\n",
      "Episode:477 meanR:431.3900 R:500.0000 rate:1.0000 gloss:21.2964 dlossA:5.5718 dlossQ:10.0247 exploreP:0.0100\n",
      "Episode:478 meanR:431.3900 R:500.0000 rate:1.0000 gloss:22.6266 dlossA:6.0390 dlossQ:11.2421 exploreP:0.0100\n",
      "Episode:479 meanR:433.0800 R:500.0000 rate:1.0000 gloss:22.4828 dlossA:5.3724 dlossQ:11.9504 exploreP:0.0100\n",
      "Episode:480 meanR:433.0800 R:500.0000 rate:1.0000 gloss:23.6688 dlossA:6.1938 dlossQ:13.9481 exploreP:0.0100\n",
      "Episode:481 meanR:433.0800 R:500.0000 rate:1.0000 gloss:23.5076 dlossA:5.5830 dlossQ:11.8361 exploreP:0.0100\n",
      "Episode:482 meanR:433.0800 R:500.0000 rate:1.0000 gloss:23.5097 dlossA:5.0800 dlossQ:24.0591 exploreP:0.0100\n",
      "Episode:483 meanR:433.0800 R:500.0000 rate:1.0000 gloss:22.5056 dlossA:3.7634 dlossQ:11.4492 exploreP:0.0100\n",
      "Episode:484 meanR:433.0800 R:500.0000 rate:1.0000 gloss:22.3363 dlossA:3.5989 dlossQ:12.2079 exploreP:0.0100\n",
      "Episode:485 meanR:433.0800 R:500.0000 rate:1.0000 gloss:22.2145 dlossA:3.3674 dlossQ:12.0827 exploreP:0.0100\n",
      "Episode:486 meanR:431.5400 R:346.0000 rate:0.6920 gloss:22.6462 dlossA:4.1707 dlossQ:13.0599 exploreP:0.0100\n",
      "Episode:487 meanR:431.5400 R:500.0000 rate:1.0000 gloss:21.9994 dlossA:3.0869 dlossQ:12.3273 exploreP:0.0100\n",
      "Episode:488 meanR:433.3600 R:500.0000 rate:1.0000 gloss:22.2598 dlossA:3.8727 dlossQ:22.6196 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
