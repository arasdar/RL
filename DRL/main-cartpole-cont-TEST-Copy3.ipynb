{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e2)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 6), (52, 4), (52,), (52, 4), (52,), (52,), (52,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:24.0000 R:24.0000 rate:0.0480 gloss:0.6395 dlossA:0.6360 dlossQ:1.1719 exploreP:0.9976\n",
      "Episode:1 meanR:22.0000 R:20.0000 rate:0.0400 gloss:0.6593 dlossA:0.6513 dlossQ:1.1578 exploreP:0.9957\n",
      "Episode:2 meanR:19.0000 R:13.0000 rate:0.0260 gloss:0.6552 dlossA:0.6468 dlossQ:1.1518 exploreP:0.9944\n",
      "Episode:3 meanR:19.2500 R:20.0000 rate:0.0400 gloss:0.6462 dlossA:0.6395 dlossQ:1.1624 exploreP:0.9924\n",
      "Episode:4 meanR:17.6000 R:11.0000 rate:0.0220 gloss:0.6463 dlossA:0.6414 dlossQ:1.1641 exploreP:0.9913\n",
      "Episode:5 meanR:17.5000 R:17.0000 rate:0.0340 gloss:0.6303 dlossA:0.6337 dlossQ:1.1602 exploreP:0.9897\n",
      "Episode:6 meanR:17.2857 R:16.0000 rate:0.0320 gloss:0.6548 dlossA:0.6474 dlossQ:1.1483 exploreP:0.9881\n",
      "Episode:7 meanR:17.3750 R:18.0000 rate:0.0360 gloss:0.6496 dlossA:0.6419 dlossQ:1.1662 exploreP:0.9863\n",
      "Episode:8 meanR:16.5556 R:10.0000 rate:0.0200 gloss:0.6282 dlossA:0.6323 dlossQ:1.1895 exploreP:0.9854\n",
      "Episode:9 meanR:16.1000 R:12.0000 rate:0.0240 gloss:0.6519 dlossA:0.6534 dlossQ:1.1334 exploreP:0.9842\n",
      "Episode:10 meanR:17.4545 R:31.0000 rate:0.0620 gloss:0.6590 dlossA:0.6546 dlossQ:1.1291 exploreP:0.9812\n",
      "Episode:11 meanR:17.0000 R:12.0000 rate:0.0240 gloss:0.6395 dlossA:0.6362 dlossQ:1.1848 exploreP:0.9800\n",
      "Episode:12 meanR:17.5385 R:24.0000 rate:0.0480 gloss:0.6546 dlossA:0.6427 dlossQ:1.1915 exploreP:0.9777\n",
      "Episode:13 meanR:16.9286 R:9.0000 rate:0.0180 gloss:0.6624 dlossA:0.6517 dlossQ:1.1527 exploreP:0.9768\n",
      "Episode:14 meanR:17.5333 R:26.0000 rate:0.0520 gloss:0.6490 dlossA:0.6474 dlossQ:1.1501 exploreP:0.9743\n",
      "Episode:15 meanR:17.5000 R:17.0000 rate:0.0340 gloss:0.6336 dlossA:0.6338 dlossQ:1.1754 exploreP:0.9727\n",
      "Episode:16 meanR:18.0588 R:27.0000 rate:0.0540 gloss:0.6473 dlossA:0.6435 dlossQ:1.1602 exploreP:0.9701\n",
      "Episode:17 meanR:18.7778 R:31.0000 rate:0.0620 gloss:0.6599 dlossA:0.6473 dlossQ:1.1765 exploreP:0.9671\n",
      "Episode:18 meanR:18.5263 R:14.0000 rate:0.0280 gloss:0.6532 dlossA:0.6441 dlossQ:1.1750 exploreP:0.9658\n",
      "Episode:19 meanR:18.7500 R:23.0000 rate:0.0460 gloss:0.6439 dlossA:0.6368 dlossQ:1.1771 exploreP:0.9636\n",
      "Episode:20 meanR:19.5714 R:36.0000 rate:0.0720 gloss:0.6495 dlossA:0.6410 dlossQ:1.1783 exploreP:0.9601\n",
      "Episode:21 meanR:21.8636 R:70.0000 rate:0.1400 gloss:0.6435 dlossA:0.6373 dlossQ:1.1807 exploreP:0.9535\n",
      "Episode:22 meanR:21.6087 R:16.0000 rate:0.0320 gloss:0.6425 dlossA:0.6396 dlossQ:1.1695 exploreP:0.9520\n",
      "Episode:23 meanR:22.2083 R:36.0000 rate:0.0720 gloss:0.6530 dlossA:0.6426 dlossQ:1.1821 exploreP:0.9486\n",
      "Episode:24 meanR:21.8400 R:13.0000 rate:0.0260 gloss:0.6592 dlossA:0.6500 dlossQ:1.1573 exploreP:0.9474\n",
      "Episode:25 meanR:21.8077 R:21.0000 rate:0.0420 gloss:0.6436 dlossA:0.6352 dlossQ:1.2222 exploreP:0.9454\n",
      "Episode:26 meanR:21.8889 R:24.0000 rate:0.0480 gloss:0.6568 dlossA:0.6495 dlossQ:1.1609 exploreP:0.9432\n",
      "Episode:27 meanR:21.8214 R:20.0000 rate:0.0400 gloss:0.6519 dlossA:0.6438 dlossQ:1.1745 exploreP:0.9413\n",
      "Episode:28 meanR:21.6207 R:16.0000 rate:0.0320 gloss:0.6662 dlossA:0.6506 dlossQ:1.1711 exploreP:0.9398\n",
      "Episode:29 meanR:21.2667 R:11.0000 rate:0.0220 gloss:0.6654 dlossA:0.6512 dlossQ:1.1719 exploreP:0.9388\n",
      "Episode:30 meanR:21.1613 R:18.0000 rate:0.0360 gloss:0.6690 dlossA:0.6557 dlossQ:1.1585 exploreP:0.9371\n",
      "Episode:31 meanR:20.9375 R:14.0000 rate:0.0280 gloss:0.6315 dlossA:0.6302 dlossQ:1.1840 exploreP:0.9358\n",
      "Episode:32 meanR:20.9394 R:21.0000 rate:0.0420 gloss:0.6573 dlossA:0.6470 dlossQ:1.1679 exploreP:0.9339\n",
      "Episode:33 meanR:20.8235 R:17.0000 rate:0.0340 gloss:0.6511 dlossA:0.6444 dlossQ:1.1771 exploreP:0.9323\n",
      "Episode:34 meanR:20.5429 R:11.0000 rate:0.0220 gloss:0.6609 dlossA:0.6413 dlossQ:1.2246 exploreP:0.9313\n",
      "Episode:35 meanR:20.9167 R:34.0000 rate:0.0680 gloss:0.6387 dlossA:0.6347 dlossQ:1.1957 exploreP:0.9282\n",
      "Episode:36 meanR:20.9189 R:21.0000 rate:0.0420 gloss:0.6511 dlossA:0.6428 dlossQ:1.1824 exploreP:0.9263\n",
      "Episode:37 meanR:20.7895 R:16.0000 rate:0.0320 gloss:0.6634 dlossA:0.6445 dlossQ:1.2073 exploreP:0.9248\n",
      "Episode:38 meanR:20.6410 R:15.0000 rate:0.0300 gloss:0.6543 dlossA:0.6468 dlossQ:1.1673 exploreP:0.9234\n",
      "Episode:39 meanR:20.3750 R:10.0000 rate:0.0200 gloss:0.6591 dlossA:0.6431 dlossQ:1.1897 exploreP:0.9225\n",
      "Episode:40 meanR:20.1951 R:13.0000 rate:0.0260 gloss:0.6605 dlossA:0.6423 dlossQ:1.1924 exploreP:0.9213\n",
      "Episode:41 meanR:20.0000 R:12.0000 rate:0.0240 gloss:0.6532 dlossA:0.6473 dlossQ:1.1701 exploreP:0.9202\n",
      "Episode:42 meanR:20.1628 R:27.0000 rate:0.0540 gloss:0.6474 dlossA:0.6402 dlossQ:1.1857 exploreP:0.9178\n",
      "Episode:43 meanR:20.1136 R:18.0000 rate:0.0360 gloss:0.6402 dlossA:0.6400 dlossQ:1.1583 exploreP:0.9162\n",
      "Episode:44 meanR:20.0000 R:15.0000 rate:0.0300 gloss:0.6401 dlossA:0.6375 dlossQ:1.1781 exploreP:0.9148\n",
      "Episode:45 meanR:19.8696 R:14.0000 rate:0.0280 gloss:0.6511 dlossA:0.6454 dlossQ:1.1585 exploreP:0.9135\n",
      "Episode:46 meanR:19.9362 R:23.0000 rate:0.0460 gloss:0.6413 dlossA:0.6382 dlossQ:1.1850 exploreP:0.9115\n",
      "Episode:47 meanR:19.8333 R:15.0000 rate:0.0300 gloss:0.6419 dlossA:0.6408 dlossQ:1.1630 exploreP:0.9101\n",
      "Episode:48 meanR:20.1224 R:34.0000 rate:0.0680 gloss:0.6420 dlossA:0.6381 dlossQ:1.1797 exploreP:0.9070\n",
      "Episode:49 meanR:19.9400 R:11.0000 rate:0.0220 gloss:0.6594 dlossA:0.6518 dlossQ:1.1561 exploreP:0.9061\n",
      "Episode:50 meanR:19.8235 R:14.0000 rate:0.0280 gloss:0.6455 dlossA:0.6404 dlossQ:1.1739 exploreP:0.9048\n",
      "Episode:51 meanR:19.6154 R:9.0000 rate:0.0180 gloss:0.6605 dlossA:0.6494 dlossQ:1.1598 exploreP:0.9040\n",
      "Episode:52 meanR:19.9623 R:38.0000 rate:0.0760 gloss:0.6516 dlossA:0.6478 dlossQ:1.1635 exploreP:0.9006\n",
      "Episode:53 meanR:19.8519 R:14.0000 rate:0.0280 gloss:0.6495 dlossA:0.6439 dlossQ:1.1719 exploreP:0.8994\n",
      "Episode:54 meanR:19.9636 R:26.0000 rate:0.0520 gloss:0.6523 dlossA:0.6471 dlossQ:1.1612 exploreP:0.8971\n",
      "Episode:55 meanR:20.7857 R:66.0000 rate:0.1320 gloss:0.6346 dlossA:0.6353 dlossQ:1.1773 exploreP:0.8912\n",
      "Episode:56 meanR:21.0877 R:38.0000 rate:0.0760 gloss:0.6562 dlossA:0.6504 dlossQ:1.1566 exploreP:0.8879\n",
      "Episode:57 meanR:21.1724 R:26.0000 rate:0.0520 gloss:0.6475 dlossA:0.6438 dlossQ:1.1547 exploreP:0.8856\n",
      "Episode:58 meanR:21.3220 R:30.0000 rate:0.0600 gloss:0.6362 dlossA:0.6334 dlossQ:1.1875 exploreP:0.8830\n",
      "Episode:59 meanR:21.2167 R:15.0000 rate:0.0300 gloss:0.6411 dlossA:0.6378 dlossQ:1.1776 exploreP:0.8817\n",
      "Episode:60 meanR:21.1639 R:18.0000 rate:0.0360 gloss:0.6430 dlossA:0.6408 dlossQ:1.1730 exploreP:0.8801\n",
      "Episode:61 meanR:21.2581 R:27.0000 rate:0.0540 gloss:0.6332 dlossA:0.6340 dlossQ:1.1821 exploreP:0.8778\n",
      "Episode:62 meanR:21.6984 R:49.0000 rate:0.0980 gloss:0.6350 dlossA:0.6354 dlossQ:1.1764 exploreP:0.8735\n",
      "Episode:63 meanR:22.2031 R:54.0000 rate:0.1080 gloss:0.6337 dlossA:0.6358 dlossQ:1.1762 exploreP:0.8689\n",
      "Episode:64 meanR:22.1231 R:17.0000 rate:0.0340 gloss:0.6380 dlossA:0.6380 dlossQ:1.1664 exploreP:0.8674\n",
      "Episode:65 meanR:22.5000 R:47.0000 rate:0.0940 gloss:0.6420 dlossA:0.6430 dlossQ:1.1619 exploreP:0.8634\n",
      "Episode:66 meanR:22.6567 R:33.0000 rate:0.0660 gloss:0.6375 dlossA:0.6401 dlossQ:1.1593 exploreP:0.8606\n",
      "Episode:67 meanR:22.7941 R:32.0000 rate:0.0640 gloss:0.6441 dlossA:0.6457 dlossQ:1.1466 exploreP:0.8579\n",
      "Episode:68 meanR:22.6812 R:15.0000 rate:0.0300 gloss:0.6277 dlossA:0.6315 dlossQ:1.1734 exploreP:0.8566\n",
      "Episode:69 meanR:22.6143 R:18.0000 rate:0.0360 gloss:0.6454 dlossA:0.6445 dlossQ:1.1630 exploreP:0.8551\n",
      "Episode:70 meanR:23.4225 R:80.0000 rate:0.1600 gloss:0.6314 dlossA:0.6354 dlossQ:1.1718 exploreP:0.8483\n",
      "Episode:71 meanR:23.4444 R:25.0000 rate:0.0500 gloss:0.6286 dlossA:0.6316 dlossQ:1.2067 exploreP:0.8462\n",
      "Episode:72 meanR:23.2466 R:9.0000 rate:0.0180 gloss:0.6163 dlossA:0.6177 dlossQ:1.2770 exploreP:0.8455\n",
      "Episode:73 meanR:23.0811 R:11.0000 rate:0.0220 gloss:0.6166 dlossA:0.6300 dlossQ:1.1480 exploreP:0.8446\n",
      "Episode:74 meanR:23.2267 R:34.0000 rate:0.0680 gloss:0.6340 dlossA:0.6362 dlossQ:1.1766 exploreP:0.8417\n",
      "Episode:75 meanR:23.0921 R:13.0000 rate:0.0260 gloss:0.6485 dlossA:0.6483 dlossQ:1.1522 exploreP:0.8406\n",
      "Episode:76 meanR:22.9221 R:10.0000 rate:0.0200 gloss:0.6386 dlossA:0.6412 dlossQ:1.1709 exploreP:0.8398\n",
      "Episode:77 meanR:23.0641 R:34.0000 rate:0.0680 gloss:0.6355 dlossA:0.6409 dlossQ:1.1590 exploreP:0.8370\n",
      "Episode:78 meanR:23.1772 R:32.0000 rate:0.0640 gloss:0.6278 dlossA:0.6347 dlossQ:1.1656 exploreP:0.8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:23.9125 R:82.0000 rate:0.1640 gloss:0.6261 dlossA:0.6324 dlossQ:1.1655 exploreP:0.8276\n",
      "Episode:80 meanR:23.7901 R:14.0000 rate:0.0280 gloss:0.6311 dlossA:0.6365 dlossQ:1.1577 exploreP:0.8265\n",
      "Episode:81 meanR:23.7439 R:20.0000 rate:0.0400 gloss:0.6096 dlossA:0.6272 dlossQ:1.1803 exploreP:0.8249\n",
      "Episode:82 meanR:23.5904 R:11.0000 rate:0.0220 gloss:0.6335 dlossA:0.6384 dlossQ:1.1536 exploreP:0.8240\n",
      "Episode:83 meanR:23.4167 R:9.0000 rate:0.0180 gloss:0.6382 dlossA:0.6486 dlossQ:1.1327 exploreP:0.8232\n",
      "Episode:84 meanR:23.5059 R:31.0000 rate:0.0620 gloss:0.6121 dlossA:0.6238 dlossQ:1.1835 exploreP:0.8207\n",
      "Episode:85 meanR:23.5930 R:31.0000 rate:0.0620 gloss:0.6226 dlossA:0.6339 dlossQ:1.1624 exploreP:0.8182\n",
      "Episode:86 meanR:23.4943 R:15.0000 rate:0.0300 gloss:0.6264 dlossA:0.6380 dlossQ:1.1688 exploreP:0.8170\n",
      "Episode:87 meanR:23.5341 R:27.0000 rate:0.0540 gloss:0.6314 dlossA:0.6359 dlossQ:1.1560 exploreP:0.8148\n",
      "Episode:88 meanR:23.6517 R:34.0000 rate:0.0680 gloss:0.6217 dlossA:0.6341 dlossQ:1.1366 exploreP:0.8121\n",
      "Episode:89 meanR:23.5556 R:15.0000 rate:0.0300 gloss:0.6150 dlossA:0.6250 dlossQ:1.1764 exploreP:0.8109\n",
      "Episode:90 meanR:23.7033 R:37.0000 rate:0.0740 gloss:0.6188 dlossA:0.6287 dlossQ:1.1663 exploreP:0.8079\n",
      "Episode:91 meanR:23.6957 R:23.0000 rate:0.0460 gloss:0.6256 dlossA:0.6382 dlossQ:1.1457 exploreP:0.8061\n",
      "Episode:92 meanR:23.7097 R:25.0000 rate:0.0500 gloss:0.6237 dlossA:0.6343 dlossQ:1.1805 exploreP:0.8041\n",
      "Episode:93 meanR:23.7872 R:31.0000 rate:0.0620 gloss:0.6271 dlossA:0.6391 dlossQ:1.1513 exploreP:0.8016\n",
      "Episode:94 meanR:23.6632 R:12.0000 rate:0.0240 gloss:0.6330 dlossA:0.6405 dlossQ:1.1470 exploreP:0.8007\n",
      "Episode:95 meanR:23.6562 R:23.0000 rate:0.0460 gloss:0.6220 dlossA:0.6241 dlossQ:1.1978 exploreP:0.7989\n",
      "Episode:96 meanR:23.7835 R:36.0000 rate:0.0720 gloss:0.6273 dlossA:0.6377 dlossQ:1.1476 exploreP:0.7960\n",
      "Episode:97 meanR:23.9796 R:43.0000 rate:0.0860 gloss:0.6241 dlossA:0.6364 dlossQ:1.1511 exploreP:0.7927\n",
      "Episode:98 meanR:24.0404 R:30.0000 rate:0.0600 gloss:0.6228 dlossA:0.6365 dlossQ:1.1340 exploreP:0.7903\n",
      "Episode:99 meanR:24.5600 R:76.0000 rate:0.1520 gloss:0.6227 dlossA:0.6286 dlossQ:1.1839 exploreP:0.7844\n",
      "Episode:100 meanR:24.5100 R:19.0000 rate:0.0380 gloss:0.6111 dlossA:0.6239 dlossQ:1.1728 exploreP:0.7829\n",
      "Episode:101 meanR:24.7300 R:42.0000 rate:0.0840 gloss:0.6205 dlossA:0.6319 dlossQ:1.1654 exploreP:0.7797\n",
      "Episode:102 meanR:24.7200 R:12.0000 rate:0.0240 gloss:0.6120 dlossA:0.6358 dlossQ:1.1211 exploreP:0.7788\n",
      "Episode:103 meanR:24.6100 R:9.0000 rate:0.0180 gloss:0.6055 dlossA:0.6249 dlossQ:1.1718 exploreP:0.7781\n",
      "Episode:104 meanR:24.9900 R:49.0000 rate:0.0980 gloss:0.6149 dlossA:0.6347 dlossQ:1.1562 exploreP:0.7743\n",
      "Episode:105 meanR:25.4200 R:60.0000 rate:0.1200 gloss:0.6108 dlossA:0.6287 dlossQ:1.1538 exploreP:0.7698\n",
      "Episode:106 meanR:25.6200 R:36.0000 rate:0.0720 gloss:0.6045 dlossA:0.6220 dlossQ:1.1659 exploreP:0.7670\n",
      "Episode:107 meanR:25.9600 R:52.0000 rate:0.1040 gloss:0.6089 dlossA:0.6293 dlossQ:1.1521 exploreP:0.7631\n",
      "Episode:108 meanR:26.2700 R:41.0000 rate:0.0820 gloss:0.6064 dlossA:0.6233 dlossQ:1.1694 exploreP:0.7600\n",
      "Episode:109 meanR:26.5100 R:36.0000 rate:0.0720 gloss:0.5984 dlossA:0.6234 dlossQ:1.1567 exploreP:0.7573\n",
      "Episode:110 meanR:26.4800 R:28.0000 rate:0.0560 gloss:0.6043 dlossA:0.6291 dlossQ:1.1379 exploreP:0.7552\n",
      "Episode:111 meanR:26.4800 R:12.0000 rate:0.0240 gloss:0.6007 dlossA:0.6243 dlossQ:1.1334 exploreP:0.7543\n",
      "Episode:112 meanR:26.7500 R:51.0000 rate:0.1020 gloss:0.5936 dlossA:0.6214 dlossQ:1.1419 exploreP:0.7506\n",
      "Episode:113 meanR:27.1300 R:47.0000 rate:0.0940 gloss:0.6070 dlossA:0.6302 dlossQ:1.1311 exploreP:0.7471\n",
      "Episode:114 meanR:27.4000 R:53.0000 rate:0.1060 gloss:0.6017 dlossA:0.6267 dlossQ:1.1349 exploreP:0.7432\n",
      "Episode:115 meanR:27.3300 R:10.0000 rate:0.0200 gloss:0.5990 dlossA:0.6274 dlossQ:1.1248 exploreP:0.7425\n",
      "Episode:116 meanR:27.1800 R:12.0000 rate:0.0240 gloss:0.5971 dlossA:0.6261 dlossQ:1.1445 exploreP:0.7416\n",
      "Episode:117 meanR:27.0500 R:18.0000 rate:0.0360 gloss:0.5993 dlossA:0.6287 dlossQ:1.1200 exploreP:0.7403\n",
      "Episode:118 meanR:27.4800 R:57.0000 rate:0.1140 gloss:0.5977 dlossA:0.6257 dlossQ:1.1286 exploreP:0.7361\n",
      "Episode:119 meanR:27.7800 R:53.0000 rate:0.1060 gloss:0.5976 dlossA:0.6256 dlossQ:1.1311 exploreP:0.7323\n",
      "Episode:120 meanR:27.7000 R:28.0000 rate:0.0560 gloss:0.5966 dlossA:0.6174 dlossQ:1.1651 exploreP:0.7303\n",
      "Episode:121 meanR:27.2000 R:20.0000 rate:0.0400 gloss:0.5997 dlossA:0.6229 dlossQ:1.1380 exploreP:0.7288\n",
      "Episode:122 meanR:27.2100 R:17.0000 rate:0.0340 gloss:0.5853 dlossA:0.6263 dlossQ:1.0853 exploreP:0.7276\n",
      "Episode:123 meanR:27.9600 R:111.0000 rate:0.2220 gloss:0.5969 dlossA:0.6231 dlossQ:1.1420 exploreP:0.7197\n",
      "Episode:124 meanR:28.1300 R:30.0000 rate:0.0600 gloss:0.5990 dlossA:0.6222 dlossQ:1.1702 exploreP:0.7175\n",
      "Episode:125 meanR:28.7400 R:82.0000 rate:0.1640 gloss:0.5874 dlossA:0.6193 dlossQ:1.1471 exploreP:0.7118\n",
      "Episode:126 meanR:28.6900 R:19.0000 rate:0.0380 gloss:0.5994 dlossA:0.6274 dlossQ:1.1510 exploreP:0.7104\n",
      "Episode:127 meanR:28.6200 R:13.0000 rate:0.0260 gloss:0.5907 dlossA:0.6181 dlossQ:1.1314 exploreP:0.7095\n",
      "Episode:128 meanR:28.6200 R:16.0000 rate:0.0320 gloss:0.5824 dlossA:0.6239 dlossQ:1.0921 exploreP:0.7084\n",
      "Episode:129 meanR:28.9600 R:45.0000 rate:0.0900 gloss:0.5886 dlossA:0.6225 dlossQ:1.1343 exploreP:0.7053\n",
      "Episode:130 meanR:29.2800 R:50.0000 rate:0.1000 gloss:0.5890 dlossA:0.6201 dlossQ:1.1391 exploreP:0.7018\n",
      "Episode:131 meanR:29.2500 R:11.0000 rate:0.0220 gloss:0.5977 dlossA:0.6256 dlossQ:1.1327 exploreP:0.7010\n",
      "Episode:132 meanR:29.2200 R:18.0000 rate:0.0360 gloss:0.5874 dlossA:0.6252 dlossQ:1.1027 exploreP:0.6998\n",
      "Episode:133 meanR:29.1600 R:11.0000 rate:0.0220 gloss:0.6044 dlossA:0.6287 dlossQ:1.1213 exploreP:0.6990\n",
      "Episode:134 meanR:29.5700 R:52.0000 rate:0.1040 gloss:0.5889 dlossA:0.6222 dlossQ:1.1366 exploreP:0.6955\n",
      "Episode:135 meanR:29.6500 R:42.0000 rate:0.0840 gloss:0.5879 dlossA:0.6232 dlossQ:1.1464 exploreP:0.6926\n",
      "Episode:136 meanR:30.0200 R:58.0000 rate:0.1160 gloss:0.5860 dlossA:0.6179 dlossQ:1.1721 exploreP:0.6886\n",
      "Episode:137 meanR:30.2300 R:37.0000 rate:0.0740 gloss:0.5937 dlossA:0.6246 dlossQ:1.1207 exploreP:0.6861\n",
      "Episode:138 meanR:30.2300 R:15.0000 rate:0.0300 gloss:0.5834 dlossA:0.6196 dlossQ:1.1283 exploreP:0.6851\n",
      "Episode:139 meanR:30.3300 R:20.0000 rate:0.0400 gloss:0.5887 dlossA:0.6248 dlossQ:1.1091 exploreP:0.6838\n",
      "Episode:140 meanR:30.6200 R:42.0000 rate:0.0840 gloss:0.5909 dlossA:0.6292 dlossQ:1.0979 exploreP:0.6810\n",
      "Episode:141 meanR:31.3600 R:86.0000 rate:0.1720 gloss:0.5832 dlossA:0.6216 dlossQ:1.1338 exploreP:0.6752\n",
      "Episode:142 meanR:31.7600 R:67.0000 rate:0.1340 gloss:0.5893 dlossA:0.6227 dlossQ:1.1282 exploreP:0.6708\n",
      "Episode:143 meanR:31.9700 R:39.0000 rate:0.0780 gloss:0.5812 dlossA:0.6186 dlossQ:1.1226 exploreP:0.6682\n",
      "Episode:144 meanR:32.7400 R:92.0000 rate:0.1840 gloss:0.5777 dlossA:0.6183 dlossQ:1.1217 exploreP:0.6622\n",
      "Episode:145 meanR:33.1600 R:56.0000 rate:0.1120 gloss:0.5862 dlossA:0.6225 dlossQ:1.1345 exploreP:0.6585\n",
      "Episode:146 meanR:33.2400 R:31.0000 rate:0.0620 gloss:0.5835 dlossA:0.6187 dlossQ:1.1312 exploreP:0.6565\n",
      "Episode:147 meanR:33.9000 R:81.0000 rate:0.1620 gloss:0.5798 dlossA:0.6195 dlossQ:1.1263 exploreP:0.6513\n",
      "Episode:148 meanR:33.7600 R:20.0000 rate:0.0400 gloss:0.5879 dlossA:0.6133 dlossQ:1.1419 exploreP:0.6500\n",
      "Episode:149 meanR:34.0400 R:39.0000 rate:0.0780 gloss:0.5865 dlossA:0.6231 dlossQ:1.1029 exploreP:0.6475\n",
      "Episode:150 meanR:34.2300 R:33.0000 rate:0.0660 gloss:0.5844 dlossA:0.6252 dlossQ:1.1190 exploreP:0.6454\n",
      "Episode:151 meanR:34.4200 R:28.0000 rate:0.0560 gloss:0.5765 dlossA:0.6189 dlossQ:1.1043 exploreP:0.6437\n",
      "Episode:152 meanR:34.2600 R:22.0000 rate:0.0440 gloss:0.5868 dlossA:0.6247 dlossQ:1.1200 exploreP:0.6423\n",
      "Episode:153 meanR:35.2400 R:112.0000 rate:0.2240 gloss:0.5845 dlossA:0.6206 dlossQ:1.1311 exploreP:0.6352\n",
      "Episode:154 meanR:35.2200 R:24.0000 rate:0.0480 gloss:0.5819 dlossA:0.6246 dlossQ:1.1048 exploreP:0.6337\n",
      "Episode:155 meanR:34.7400 R:18.0000 rate:0.0360 gloss:0.5760 dlossA:0.6190 dlossQ:1.1065 exploreP:0.6326\n",
      "Episode:156 meanR:34.5000 R:14.0000 rate:0.0280 gloss:0.5832 dlossA:0.6269 dlossQ:1.1262 exploreP:0.6317\n",
      "Episode:157 meanR:34.5000 R:26.0000 rate:0.0520 gloss:0.5793 dlossA:0.6192 dlossQ:1.1324 exploreP:0.6301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:34.3800 R:18.0000 rate:0.0360 gloss:0.5640 dlossA:0.6131 dlossQ:1.1126 exploreP:0.6290\n",
      "Episode:159 meanR:34.8200 R:59.0000 rate:0.1180 gloss:0.5812 dlossA:0.6194 dlossQ:1.1165 exploreP:0.6254\n",
      "Episode:160 meanR:34.8500 R:21.0000 rate:0.0420 gloss:0.5743 dlossA:0.6157 dlossQ:1.1281 exploreP:0.6241\n",
      "Episode:161 meanR:35.1900 R:61.0000 rate:0.1220 gloss:0.5800 dlossA:0.6163 dlossQ:1.1313 exploreP:0.6203\n",
      "Episode:162 meanR:35.6200 R:92.0000 rate:0.1840 gloss:0.5813 dlossA:0.6170 dlossQ:1.1416 exploreP:0.6147\n",
      "Episode:163 meanR:35.5200 R:44.0000 rate:0.0880 gloss:0.5818 dlossA:0.6161 dlossQ:1.1327 exploreP:0.6121\n",
      "Episode:164 meanR:35.9400 R:59.0000 rate:0.1180 gloss:0.5785 dlossA:0.6148 dlossQ:1.1424 exploreP:0.6085\n",
      "Episode:165 meanR:35.9300 R:46.0000 rate:0.0920 gloss:0.5847 dlossA:0.6224 dlossQ:1.1175 exploreP:0.6058\n",
      "Episode:166 meanR:35.7200 R:12.0000 rate:0.0240 gloss:0.5716 dlossA:0.6052 dlossQ:1.2019 exploreP:0.6051\n",
      "Episode:167 meanR:35.6800 R:28.0000 rate:0.0560 gloss:0.5856 dlossA:0.6249 dlossQ:1.1109 exploreP:0.6034\n",
      "Episode:168 meanR:35.7600 R:23.0000 rate:0.0460 gloss:0.5714 dlossA:0.6080 dlossQ:1.1349 exploreP:0.6021\n",
      "Episode:169 meanR:35.8400 R:26.0000 rate:0.0520 gloss:0.5789 dlossA:0.6174 dlossQ:1.1383 exploreP:0.6005\n",
      "Episode:170 meanR:35.4900 R:45.0000 rate:0.0900 gloss:0.5661 dlossA:0.6077 dlossQ:1.1578 exploreP:0.5979\n",
      "Episode:171 meanR:35.4600 R:22.0000 rate:0.0440 gloss:0.5784 dlossA:0.6179 dlossQ:1.1262 exploreP:0.5966\n",
      "Episode:172 meanR:35.5500 R:18.0000 rate:0.0360 gloss:0.5671 dlossA:0.6066 dlossQ:1.1895 exploreP:0.5955\n",
      "Episode:173 meanR:35.7700 R:33.0000 rate:0.0660 gloss:0.5755 dlossA:0.6121 dlossQ:1.1477 exploreP:0.5936\n",
      "Episode:174 meanR:35.6500 R:22.0000 rate:0.0440 gloss:0.5825 dlossA:0.6183 dlossQ:1.1207 exploreP:0.5923\n",
      "Episode:175 meanR:35.8400 R:32.0000 rate:0.0640 gloss:0.5880 dlossA:0.6304 dlossQ:1.1071 exploreP:0.5905\n",
      "Episode:176 meanR:36.1900 R:45.0000 rate:0.0900 gloss:0.5785 dlossA:0.6155 dlossQ:1.1247 exploreP:0.5878\n",
      "Episode:177 meanR:36.1500 R:30.0000 rate:0.0600 gloss:0.5773 dlossA:0.6170 dlossQ:1.1428 exploreP:0.5861\n",
      "Episode:178 meanR:36.3800 R:55.0000 rate:0.1100 gloss:0.5752 dlossA:0.6170 dlossQ:1.1211 exploreP:0.5830\n",
      "Episode:179 meanR:35.7000 R:14.0000 rate:0.0280 gloss:0.5700 dlossA:0.6159 dlossQ:1.1133 exploreP:0.5822\n",
      "Episode:180 meanR:35.7800 R:22.0000 rate:0.0440 gloss:0.5866 dlossA:0.6267 dlossQ:1.1090 exploreP:0.5809\n",
      "Episode:181 meanR:36.1100 R:53.0000 rate:0.1060 gloss:0.5740 dlossA:0.6106 dlossQ:1.1504 exploreP:0.5779\n",
      "Episode:182 meanR:36.4100 R:41.0000 rate:0.0820 gloss:0.5774 dlossA:0.6206 dlossQ:1.1043 exploreP:0.5756\n",
      "Episode:183 meanR:36.9300 R:61.0000 rate:0.1220 gloss:0.5801 dlossA:0.6155 dlossQ:1.1215 exploreP:0.5721\n",
      "Episode:184 meanR:36.7200 R:10.0000 rate:0.0200 gloss:0.5747 dlossA:0.6173 dlossQ:1.1542 exploreP:0.5716\n",
      "Episode:185 meanR:37.1100 R:70.0000 rate:0.1400 gloss:0.5820 dlossA:0.6207 dlossQ:1.1245 exploreP:0.5676\n",
      "Episode:186 meanR:37.1900 R:23.0000 rate:0.0460 gloss:0.5836 dlossA:0.6238 dlossQ:1.1340 exploreP:0.5664\n",
      "Episode:187 meanR:37.0800 R:16.0000 rate:0.0320 gloss:0.5600 dlossA:0.6099 dlossQ:1.1683 exploreP:0.5655\n",
      "Episode:188 meanR:37.4900 R:75.0000 rate:0.1500 gloss:0.5697 dlossA:0.6105 dlossQ:1.1724 exploreP:0.5613\n",
      "Episode:189 meanR:37.7600 R:42.0000 rate:0.0840 gloss:0.5762 dlossA:0.6175 dlossQ:1.1323 exploreP:0.5590\n",
      "Episode:190 meanR:38.4800 R:109.0000 rate:0.2180 gloss:0.5822 dlossA:0.6232 dlossQ:1.1150 exploreP:0.5531\n",
      "Episode:191 meanR:38.7500 R:50.0000 rate:0.1000 gloss:0.5800 dlossA:0.6174 dlossQ:1.1232 exploreP:0.5503\n",
      "Episode:192 meanR:38.8900 R:39.0000 rate:0.0780 gloss:0.5818 dlossA:0.6174 dlossQ:1.1289 exploreP:0.5482\n",
      "Episode:193 meanR:39.0700 R:49.0000 rate:0.0980 gloss:0.5829 dlossA:0.6213 dlossQ:1.1147 exploreP:0.5456\n",
      "Episode:194 meanR:39.4500 R:50.0000 rate:0.1000 gloss:0.5800 dlossA:0.6168 dlossQ:1.1361 exploreP:0.5429\n",
      "Episode:195 meanR:39.8700 R:65.0000 rate:0.1300 gloss:0.5781 dlossA:0.6196 dlossQ:1.1292 exploreP:0.5395\n",
      "Episode:196 meanR:40.0300 R:52.0000 rate:0.1040 gloss:0.5798 dlossA:0.6205 dlossQ:1.1313 exploreP:0.5367\n",
      "Episode:197 meanR:39.9600 R:36.0000 rate:0.0720 gloss:0.5810 dlossA:0.6189 dlossQ:1.1342 exploreP:0.5348\n",
      "Episode:198 meanR:40.3100 R:65.0000 rate:0.1300 gloss:0.5715 dlossA:0.6175 dlossQ:1.1059 exploreP:0.5314\n",
      "Episode:199 meanR:39.7500 R:20.0000 rate:0.0400 gloss:0.5666 dlossA:0.6095 dlossQ:1.1650 exploreP:0.5304\n",
      "Episode:200 meanR:39.6800 R:12.0000 rate:0.0240 gloss:0.5790 dlossA:0.6131 dlossQ:1.1390 exploreP:0.5298\n",
      "Episode:201 meanR:39.8100 R:55.0000 rate:0.1100 gloss:0.5776 dlossA:0.6098 dlossQ:1.1763 exploreP:0.5269\n",
      "Episode:202 meanR:40.1300 R:44.0000 rate:0.0880 gloss:0.5672 dlossA:0.6146 dlossQ:1.1243 exploreP:0.5247\n",
      "Episode:203 meanR:40.7900 R:75.0000 rate:0.1500 gloss:0.5753 dlossA:0.6183 dlossQ:1.1197 exploreP:0.5208\n",
      "Episode:204 meanR:40.8100 R:51.0000 rate:0.1020 gloss:0.5628 dlossA:0.6093 dlossQ:1.1371 exploreP:0.5182\n",
      "Episode:205 meanR:40.4600 R:25.0000 rate:0.0500 gloss:0.5889 dlossA:0.6100 dlossQ:1.1953 exploreP:0.5169\n",
      "Episode:206 meanR:40.8900 R:79.0000 rate:0.1580 gloss:0.5693 dlossA:0.6134 dlossQ:1.1330 exploreP:0.5130\n",
      "Episode:207 meanR:40.5400 R:17.0000 rate:0.0340 gloss:0.5670 dlossA:0.6145 dlossQ:1.1082 exploreP:0.5121\n",
      "Episode:208 meanR:40.6700 R:54.0000 rate:0.1080 gloss:0.5701 dlossA:0.6134 dlossQ:1.1204 exploreP:0.5094\n",
      "Episode:209 meanR:41.5000 R:119.0000 rate:0.2380 gloss:0.5672 dlossA:0.6105 dlossQ:1.1447 exploreP:0.5035\n",
      "Episode:210 meanR:41.8500 R:63.0000 rate:0.1260 gloss:0.5658 dlossA:0.6139 dlossQ:1.1239 exploreP:0.5004\n",
      "Episode:211 meanR:42.3700 R:64.0000 rate:0.1280 gloss:0.5620 dlossA:0.6128 dlossQ:1.1231 exploreP:0.4973\n",
      "Episode:212 meanR:42.3000 R:44.0000 rate:0.0880 gloss:0.5642 dlossA:0.6090 dlossQ:1.1496 exploreP:0.4951\n",
      "Episode:213 meanR:42.5000 R:67.0000 rate:0.1340 gloss:0.5676 dlossA:0.6160 dlossQ:1.1090 exploreP:0.4919\n",
      "Episode:214 meanR:42.7000 R:73.0000 rate:0.1460 gloss:0.5664 dlossA:0.6129 dlossQ:1.1209 exploreP:0.4884\n",
      "Episode:215 meanR:43.1900 R:59.0000 rate:0.1180 gloss:0.5689 dlossA:0.6157 dlossQ:1.1153 exploreP:0.4856\n",
      "Episode:216 meanR:43.3600 R:29.0000 rate:0.0580 gloss:0.5735 dlossA:0.6155 dlossQ:1.1422 exploreP:0.4842\n",
      "Episode:217 meanR:44.5800 R:140.0000 rate:0.2800 gloss:0.5595 dlossA:0.6085 dlossQ:1.1295 exploreP:0.4776\n",
      "Episode:218 meanR:44.5700 R:56.0000 rate:0.1120 gloss:0.5505 dlossA:0.6050 dlossQ:1.1920 exploreP:0.4750\n",
      "Episode:219 meanR:44.7200 R:68.0000 rate:0.1360 gloss:0.5620 dlossA:0.6111 dlossQ:1.1371 exploreP:0.4718\n",
      "Episode:220 meanR:44.7500 R:31.0000 rate:0.0620 gloss:0.5534 dlossA:0.6015 dlossQ:1.1322 exploreP:0.4704\n",
      "Episode:221 meanR:44.8500 R:30.0000 rate:0.0600 gloss:0.5590 dlossA:0.6108 dlossQ:1.1280 exploreP:0.4690\n",
      "Episode:222 meanR:45.4000 R:72.0000 rate:0.1440 gloss:0.5588 dlossA:0.6104 dlossQ:1.1285 exploreP:0.4657\n",
      "Episode:223 meanR:44.8600 R:57.0000 rate:0.1140 gloss:0.5658 dlossA:0.6144 dlossQ:1.1053 exploreP:0.4631\n",
      "Episode:224 meanR:45.5700 R:101.0000 rate:0.2020 gloss:0.5589 dlossA:0.6095 dlossQ:1.1267 exploreP:0.4586\n",
      "Episode:225 meanR:45.2100 R:46.0000 rate:0.0920 gloss:0.5607 dlossA:0.6191 dlossQ:1.1193 exploreP:0.4565\n",
      "Episode:226 meanR:45.6500 R:63.0000 rate:0.1260 gloss:0.5636 dlossA:0.6151 dlossQ:1.1141 exploreP:0.4537\n",
      "Episode:227 meanR:46.0800 R:56.0000 rate:0.1120 gloss:0.5577 dlossA:0.6078 dlossQ:1.1248 exploreP:0.4512\n",
      "Episode:228 meanR:46.8400 R:92.0000 rate:0.1840 gloss:0.5605 dlossA:0.6096 dlossQ:1.1377 exploreP:0.4472\n",
      "Episode:229 meanR:46.9000 R:51.0000 rate:0.1020 gloss:0.5498 dlossA:0.6052 dlossQ:1.1239 exploreP:0.4450\n",
      "Episode:230 meanR:46.7400 R:34.0000 rate:0.0680 gloss:0.5580 dlossA:0.6072 dlossQ:1.1376 exploreP:0.4435\n",
      "Episode:231 meanR:47.0000 R:37.0000 rate:0.0740 gloss:0.5581 dlossA:0.6085 dlossQ:1.1187 exploreP:0.4419\n",
      "Episode:232 meanR:47.6400 R:82.0000 rate:0.1640 gloss:0.5572 dlossA:0.6109 dlossQ:1.1228 exploreP:0.4384\n",
      "Episode:233 meanR:48.1200 R:59.0000 rate:0.1180 gloss:0.5620 dlossA:0.6137 dlossQ:1.1165 exploreP:0.4359\n",
      "Episode:234 meanR:48.3900 R:79.0000 rate:0.1580 gloss:0.5557 dlossA:0.6076 dlossQ:1.1153 exploreP:0.4325\n",
      "Episode:235 meanR:48.7500 R:78.0000 rate:0.1560 gloss:0.5668 dlossA:0.6137 dlossQ:1.1290 exploreP:0.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:48.7300 R:56.0000 rate:0.1120 gloss:0.5615 dlossA:0.6073 dlossQ:1.1361 exploreP:0.4269\n",
      "Episode:237 meanR:49.1800 R:82.0000 rate:0.1640 gloss:0.5635 dlossA:0.6115 dlossQ:1.1374 exploreP:0.4235\n",
      "Episode:238 meanR:49.8900 R:86.0000 rate:0.1720 gloss:0.5680 dlossA:0.6093 dlossQ:1.1385 exploreP:0.4199\n",
      "Episode:239 meanR:49.8900 R:20.0000 rate:0.0400 gloss:0.5621 dlossA:0.6170 dlossQ:1.1208 exploreP:0.4191\n",
      "Episode:240 meanR:50.3900 R:92.0000 rate:0.1840 gloss:0.5606 dlossA:0.6111 dlossQ:1.1151 exploreP:0.4154\n",
      "Episode:241 meanR:49.7000 R:17.0000 rate:0.0340 gloss:0.5547 dlossA:0.6064 dlossQ:1.1376 exploreP:0.4147\n",
      "Episode:242 meanR:49.3000 R:27.0000 rate:0.0540 gloss:0.5595 dlossA:0.6032 dlossQ:1.1879 exploreP:0.4136\n",
      "Episode:243 meanR:49.7200 R:81.0000 rate:0.1620 gloss:0.5469 dlossA:0.6055 dlossQ:1.1174 exploreP:0.4103\n",
      "Episode:244 meanR:49.8200 R:102.0000 rate:0.2040 gloss:0.5668 dlossA:0.6126 dlossQ:1.1340 exploreP:0.4063\n",
      "Episode:245 meanR:50.1100 R:85.0000 rate:0.1700 gloss:0.5727 dlossA:0.6173 dlossQ:1.1174 exploreP:0.4029\n",
      "Episode:246 meanR:50.4100 R:61.0000 rate:0.1220 gloss:0.5628 dlossA:0.6130 dlossQ:1.1346 exploreP:0.4005\n",
      "Episode:247 meanR:49.8900 R:29.0000 rate:0.0580 gloss:0.5449 dlossA:0.6048 dlossQ:1.1384 exploreP:0.3994\n",
      "Episode:248 meanR:50.2300 R:54.0000 rate:0.1080 gloss:0.5697 dlossA:0.6102 dlossQ:1.1455 exploreP:0.3973\n",
      "Episode:249 meanR:50.1900 R:35.0000 rate:0.0700 gloss:0.5496 dlossA:0.5968 dlossQ:1.2064 exploreP:0.3959\n",
      "Episode:250 meanR:50.0400 R:18.0000 rate:0.0360 gloss:0.5433 dlossA:0.5909 dlossQ:1.2287 exploreP:0.3953\n",
      "Episode:251 meanR:50.5000 R:74.0000 rate:0.1480 gloss:0.5619 dlossA:0.6107 dlossQ:1.1307 exploreP:0.3924\n",
      "Episode:252 meanR:51.0900 R:81.0000 rate:0.1620 gloss:0.5559 dlossA:0.6083 dlossQ:1.1348 exploreP:0.3893\n",
      "Episode:253 meanR:50.7500 R:78.0000 rate:0.1560 gloss:0.5559 dlossA:0.6121 dlossQ:1.1182 exploreP:0.3864\n",
      "Episode:254 meanR:51.1500 R:64.0000 rate:0.1280 gloss:0.5556 dlossA:0.6126 dlossQ:1.1088 exploreP:0.3840\n",
      "Episode:255 meanR:51.6400 R:67.0000 rate:0.1340 gloss:0.5513 dlossA:0.6048 dlossQ:1.1321 exploreP:0.3815\n",
      "Episode:256 meanR:52.0900 R:59.0000 rate:0.1180 gloss:0.5561 dlossA:0.6102 dlossQ:1.1205 exploreP:0.3793\n",
      "Episode:257 meanR:52.8200 R:99.0000 rate:0.1980 gloss:0.5623 dlossA:0.6099 dlossQ:1.1342 exploreP:0.3757\n",
      "Episode:258 meanR:53.5500 R:91.0000 rate:0.1820 gloss:0.5442 dlossA:0.6032 dlossQ:1.1346 exploreP:0.3723\n",
      "Episode:259 meanR:53.9400 R:98.0000 rate:0.1960 gloss:0.5556 dlossA:0.6065 dlossQ:1.1467 exploreP:0.3688\n",
      "Episode:260 meanR:54.3800 R:65.0000 rate:0.1300 gloss:0.5513 dlossA:0.6064 dlossQ:1.1320 exploreP:0.3665\n",
      "Episode:261 meanR:54.3300 R:56.0000 rate:0.1120 gloss:0.5636 dlossA:0.6064 dlossQ:1.1364 exploreP:0.3645\n",
      "Episode:262 meanR:54.2300 R:82.0000 rate:0.1640 gloss:0.5587 dlossA:0.6084 dlossQ:1.1412 exploreP:0.3616\n",
      "Episode:263 meanR:54.7700 R:98.0000 rate:0.1960 gloss:0.5562 dlossA:0.6087 dlossQ:1.1324 exploreP:0.3582\n",
      "Episode:264 meanR:55.3000 R:112.0000 rate:0.2240 gloss:0.5531 dlossA:0.6064 dlossQ:1.1455 exploreP:0.3543\n",
      "Episode:265 meanR:55.6100 R:77.0000 rate:0.1540 gloss:0.5561 dlossA:0.6087 dlossQ:1.1369 exploreP:0.3517\n",
      "Episode:266 meanR:56.2700 R:78.0000 rate:0.1560 gloss:0.5582 dlossA:0.6063 dlossQ:1.1548 exploreP:0.3490\n",
      "Episode:267 meanR:57.2300 R:124.0000 rate:0.2480 gloss:0.5391 dlossA:0.6046 dlossQ:1.1199 exploreP:0.3448\n",
      "Episode:268 meanR:58.2700 R:127.0000 rate:0.2540 gloss:0.5506 dlossA:0.6067 dlossQ:1.1286 exploreP:0.3406\n",
      "Episode:269 meanR:59.1900 R:118.0000 rate:0.2360 gloss:0.5516 dlossA:0.6116 dlossQ:1.1463 exploreP:0.3367\n",
      "Episode:270 meanR:59.6700 R:93.0000 rate:0.1860 gloss:0.5523 dlossA:0.6089 dlossQ:1.1248 exploreP:0.3337\n",
      "Episode:271 meanR:60.1400 R:69.0000 rate:0.1380 gloss:0.5461 dlossA:0.6069 dlossQ:1.1083 exploreP:0.3315\n",
      "Episode:272 meanR:60.6100 R:65.0000 rate:0.1300 gloss:0.5567 dlossA:0.6053 dlossQ:1.1682 exploreP:0.3294\n",
      "Episode:273 meanR:60.7500 R:47.0000 rate:0.0940 gloss:0.5464 dlossA:0.6073 dlossQ:1.1316 exploreP:0.3279\n",
      "Episode:274 meanR:61.3400 R:81.0000 rate:0.1620 gloss:0.5520 dlossA:0.6100 dlossQ:1.1195 exploreP:0.3253\n",
      "Episode:275 meanR:61.5900 R:57.0000 rate:0.1140 gloss:0.5470 dlossA:0.6019 dlossQ:1.1428 exploreP:0.3235\n",
      "Episode:276 meanR:61.8900 R:75.0000 rate:0.1500 gloss:0.5483 dlossA:0.6028 dlossQ:1.1329 exploreP:0.3212\n",
      "Episode:277 meanR:62.6800 R:109.0000 rate:0.2180 gloss:0.5409 dlossA:0.6052 dlossQ:1.1357 exploreP:0.3178\n",
      "Episode:278 meanR:63.1900 R:106.0000 rate:0.2120 gloss:0.5331 dlossA:0.5988 dlossQ:1.1283 exploreP:0.3146\n",
      "Episode:279 meanR:64.3000 R:125.0000 rate:0.2500 gloss:0.5372 dlossA:0.6007 dlossQ:1.1421 exploreP:0.3108\n",
      "Episode:280 meanR:65.1400 R:106.0000 rate:0.2120 gloss:0.5463 dlossA:0.6067 dlossQ:1.1247 exploreP:0.3076\n",
      "Episode:281 meanR:65.4300 R:82.0000 rate:0.1640 gloss:0.5448 dlossA:0.6036 dlossQ:1.1555 exploreP:0.3052\n",
      "Episode:282 meanR:65.6600 R:64.0000 rate:0.1280 gloss:0.5399 dlossA:0.6010 dlossQ:1.1245 exploreP:0.3033\n",
      "Episode:283 meanR:65.8100 R:76.0000 rate:0.1520 gloss:0.5556 dlossA:0.6040 dlossQ:1.1742 exploreP:0.3011\n",
      "Episode:284 meanR:66.4600 R:75.0000 rate:0.1500 gloss:0.5509 dlossA:0.6077 dlossQ:1.1329 exploreP:0.2989\n",
      "Episode:285 meanR:66.5100 R:75.0000 rate:0.1500 gloss:0.5446 dlossA:0.6072 dlossQ:1.1419 exploreP:0.2967\n",
      "Episode:286 meanR:67.2300 R:95.0000 rate:0.1900 gloss:0.5413 dlossA:0.6026 dlossQ:1.1323 exploreP:0.2940\n",
      "Episode:287 meanR:67.8300 R:76.0000 rate:0.1520 gloss:0.5289 dlossA:0.5984 dlossQ:1.1435 exploreP:0.2919\n",
      "Episode:288 meanR:67.2300 R:15.0000 rate:0.0300 gloss:0.5374 dlossA:0.6078 dlossQ:1.1001 exploreP:0.2915\n",
      "Episode:289 meanR:67.7700 R:96.0000 rate:0.1920 gloss:0.5399 dlossA:0.6065 dlossQ:1.1217 exploreP:0.2888\n",
      "Episode:290 meanR:67.8900 R:121.0000 rate:0.2420 gloss:0.5324 dlossA:0.5999 dlossQ:1.1332 exploreP:0.2854\n",
      "Episode:291 meanR:68.6800 R:129.0000 rate:0.2580 gloss:0.5353 dlossA:0.6052 dlossQ:1.1228 exploreP:0.2819\n",
      "Episode:292 meanR:69.1100 R:82.0000 rate:0.1640 gloss:0.5451 dlossA:0.6024 dlossQ:1.1549 exploreP:0.2797\n",
      "Episode:293 meanR:69.3000 R:68.0000 rate:0.1360 gloss:0.5331 dlossA:0.6012 dlossQ:1.1361 exploreP:0.2778\n",
      "Episode:294 meanR:69.5700 R:77.0000 rate:0.1540 gloss:0.5377 dlossA:0.6060 dlossQ:1.1333 exploreP:0.2758\n",
      "Episode:295 meanR:69.8200 R:90.0000 rate:0.1800 gloss:0.5308 dlossA:0.5995 dlossQ:1.1315 exploreP:0.2734\n",
      "Episode:296 meanR:70.1400 R:84.0000 rate:0.1680 gloss:0.5344 dlossA:0.6003 dlossQ:1.1147 exploreP:0.2712\n",
      "Episode:297 meanR:71.0200 R:124.0000 rate:0.2480 gloss:0.5356 dlossA:0.5995 dlossQ:1.1545 exploreP:0.2680\n",
      "Episode:298 meanR:71.0300 R:66.0000 rate:0.1320 gloss:0.5244 dlossA:0.6049 dlossQ:1.1402 exploreP:0.2663\n",
      "Episode:299 meanR:71.9200 R:109.0000 rate:0.2180 gloss:0.5296 dlossA:0.5983 dlossQ:1.1288 exploreP:0.2635\n",
      "Episode:300 meanR:72.6200 R:82.0000 rate:0.1640 gloss:0.5258 dlossA:0.6001 dlossQ:1.1523 exploreP:0.2614\n",
      "Episode:301 meanR:72.7800 R:71.0000 rate:0.1420 gloss:0.5286 dlossA:0.5993 dlossQ:1.1194 exploreP:0.2597\n",
      "Episode:302 meanR:73.0800 R:74.0000 rate:0.1480 gloss:0.5331 dlossA:0.6079 dlossQ:1.1230 exploreP:0.2578\n",
      "Episode:303 meanR:73.4900 R:116.0000 rate:0.2320 gloss:0.5347 dlossA:0.6037 dlossQ:1.1197 exploreP:0.2550\n",
      "Episode:304 meanR:73.7400 R:76.0000 rate:0.1520 gloss:0.5410 dlossA:0.6047 dlossQ:1.1175 exploreP:0.2531\n",
      "Episode:305 meanR:74.5300 R:104.0000 rate:0.2080 gloss:0.5259 dlossA:0.5985 dlossQ:1.1384 exploreP:0.2506\n",
      "Episode:306 meanR:74.7600 R:102.0000 rate:0.2040 gloss:0.5378 dlossA:0.6043 dlossQ:1.1310 exploreP:0.2482\n",
      "Episode:307 meanR:75.3900 R:80.0000 rate:0.1600 gloss:0.5236 dlossA:0.6016 dlossQ:1.1251 exploreP:0.2463\n",
      "Episode:308 meanR:76.1100 R:126.0000 rate:0.2520 gloss:0.5127 dlossA:0.5944 dlossQ:1.1657 exploreP:0.2433\n",
      "Episode:309 meanR:75.7500 R:83.0000 rate:0.1660 gloss:0.5161 dlossA:0.5970 dlossQ:1.1289 exploreP:0.2414\n",
      "Episode:310 meanR:76.1200 R:100.0000 rate:0.2000 gloss:0.5311 dlossA:0.6033 dlossQ:1.1192 exploreP:0.2391\n",
      "Episode:311 meanR:76.2600 R:78.0000 rate:0.1560 gloss:0.5114 dlossA:0.5924 dlossQ:1.1307 exploreP:0.2373\n",
      "Episode:312 meanR:76.4500 R:63.0000 rate:0.1260 gloss:0.5209 dlossA:0.6003 dlossQ:1.1067 exploreP:0.2359\n",
      "Episode:313 meanR:76.6100 R:83.0000 rate:0.1660 gloss:0.5197 dlossA:0.5966 dlossQ:1.1417 exploreP:0.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:314 meanR:77.0300 R:115.0000 rate:0.2300 gloss:0.5315 dlossA:0.6002 dlossQ:1.1434 exploreP:0.2314\n",
      "Episode:315 meanR:77.0100 R:57.0000 rate:0.1140 gloss:0.5238 dlossA:0.6038 dlossQ:1.1342 exploreP:0.2302\n",
      "Episode:316 meanR:77.7200 R:100.0000 rate:0.2000 gloss:0.5128 dlossA:0.5925 dlossQ:1.1314 exploreP:0.2280\n",
      "Episode:317 meanR:77.3800 R:106.0000 rate:0.2120 gloss:0.5121 dlossA:0.5968 dlossQ:1.1219 exploreP:0.2257\n",
      "Episode:318 meanR:78.3800 R:156.0000 rate:0.3120 gloss:0.5153 dlossA:0.5982 dlossQ:1.1145 exploreP:0.2223\n",
      "Episode:319 meanR:79.6100 R:191.0000 rate:0.3820 gloss:0.5035 dlossA:0.5931 dlossQ:1.1194 exploreP:0.2183\n",
      "Episode:320 meanR:80.8200 R:152.0000 rate:0.3040 gloss:0.5144 dlossA:0.5962 dlossQ:1.1268 exploreP:0.2152\n",
      "Episode:321 meanR:81.2700 R:75.0000 rate:0.1500 gloss:0.5099 dlossA:0.5925 dlossQ:1.1489 exploreP:0.2137\n",
      "Episode:322 meanR:81.5500 R:100.0000 rate:0.2000 gloss:0.4985 dlossA:0.5950 dlossQ:1.1173 exploreP:0.2116\n",
      "Episode:323 meanR:81.8800 R:90.0000 rate:0.1800 gloss:0.5038 dlossA:0.5923 dlossQ:1.1303 exploreP:0.2098\n",
      "Episode:324 meanR:82.4000 R:153.0000 rate:0.3060 gloss:0.5030 dlossA:0.5938 dlossQ:1.1425 exploreP:0.2068\n",
      "Episode:325 meanR:82.7600 R:82.0000 rate:0.1640 gloss:0.4987 dlossA:0.5933 dlossQ:1.1268 exploreP:0.2052\n",
      "Episode:326 meanR:83.0900 R:96.0000 rate:0.1920 gloss:0.5216 dlossA:0.6044 dlossQ:1.1259 exploreP:0.2033\n",
      "Episode:327 meanR:83.8600 R:133.0000 rate:0.2660 gloss:0.5053 dlossA:0.5952 dlossQ:1.1343 exploreP:0.2008\n",
      "Episode:328 meanR:83.6000 R:66.0000 rate:0.1320 gloss:0.5055 dlossA:0.5880 dlossQ:1.1588 exploreP:0.1995\n",
      "Episode:329 meanR:83.9900 R:90.0000 rate:0.1800 gloss:0.5163 dlossA:0.5967 dlossQ:1.1227 exploreP:0.1978\n",
      "Episode:330 meanR:84.5200 R:87.0000 rate:0.1740 gloss:0.5118 dlossA:0.5993 dlossQ:1.1102 exploreP:0.1962\n",
      "Episode:331 meanR:86.3900 R:224.0000 rate:0.4480 gloss:0.4966 dlossA:0.5925 dlossQ:1.1092 exploreP:0.1921\n",
      "Episode:332 meanR:87.6300 R:206.0000 rate:0.4120 gloss:0.4950 dlossA:0.5874 dlossQ:1.1329 exploreP:0.1883\n",
      "Episode:333 meanR:88.5800 R:154.0000 rate:0.3080 gloss:0.5027 dlossA:0.5923 dlossQ:1.1186 exploreP:0.1856\n",
      "Episode:334 meanR:88.8000 R:101.0000 rate:0.2020 gloss:0.4822 dlossA:0.5881 dlossQ:1.1464 exploreP:0.1839\n",
      "Episode:335 meanR:90.6200 R:260.0000 rate:0.5200 gloss:0.4871 dlossA:0.5883 dlossQ:1.1474 exploreP:0.1794\n",
      "Episode:336 meanR:90.7200 R:66.0000 rate:0.1320 gloss:0.4880 dlossA:0.5875 dlossQ:1.1244 exploreP:0.1783\n",
      "Episode:337 meanR:90.6400 R:74.0000 rate:0.1480 gloss:0.4966 dlossA:0.5952 dlossQ:1.0983 exploreP:0.1770\n",
      "Episode:338 meanR:91.0500 R:127.0000 rate:0.2540 gloss:0.4924 dlossA:0.5907 dlossQ:1.1303 exploreP:0.1749\n",
      "Episode:339 meanR:91.6500 R:80.0000 rate:0.1600 gloss:0.4893 dlossA:0.5883 dlossQ:1.1129 exploreP:0.1736\n",
      "Episode:340 meanR:93.0000 R:227.0000 rate:0.4540 gloss:0.4863 dlossA:0.5892 dlossQ:1.1898 exploreP:0.1699\n",
      "Episode:341 meanR:94.0900 R:126.0000 rate:0.2520 gloss:0.4838 dlossA:0.5875 dlossQ:1.1459 exploreP:0.1679\n",
      "Episode:342 meanR:95.3000 R:148.0000 rate:0.2960 gloss:0.4774 dlossA:0.5861 dlossQ:1.1125 exploreP:0.1656\n",
      "Episode:343 meanR:96.7500 R:226.0000 rate:0.4520 gloss:0.4699 dlossA:0.5858 dlossQ:1.1193 exploreP:0.1621\n",
      "Episode:344 meanR:97.1400 R:141.0000 rate:0.2820 gloss:0.4619 dlossA:0.5812 dlossQ:1.1463 exploreP:0.1600\n",
      "Episode:345 meanR:98.2700 R:198.0000 rate:0.3960 gloss:0.4621 dlossA:0.5825 dlossQ:1.1178 exploreP:0.1571\n",
      "Episode:346 meanR:98.6900 R:103.0000 rate:0.2060 gloss:0.4637 dlossA:0.5806 dlossQ:1.1870 exploreP:0.1556\n",
      "Episode:347 meanR:99.4200 R:102.0000 rate:0.2040 gloss:0.4475 dlossA:0.5846 dlossQ:1.1388 exploreP:0.1541\n",
      "Episode:348 meanR:100.1400 R:126.0000 rate:0.2520 gloss:0.4607 dlossA:0.5856 dlossQ:1.1274 exploreP:0.1523\n",
      "Episode:349 meanR:100.6700 R:88.0000 rate:0.1760 gloss:0.4572 dlossA:0.5845 dlossQ:1.1259 exploreP:0.1510\n",
      "Episode:350 meanR:101.8700 R:138.0000 rate:0.2760 gloss:0.4481 dlossA:0.5781 dlossQ:1.1489 exploreP:0.1491\n",
      "Episode:351 meanR:102.7200 R:159.0000 rate:0.3180 gloss:0.4522 dlossA:0.5864 dlossQ:1.0958 exploreP:0.1469\n",
      "Episode:352 meanR:102.8200 R:91.0000 rate:0.1820 gloss:0.4252 dlossA:0.5763 dlossQ:1.1408 exploreP:0.1457\n",
      "Episode:353 meanR:103.3200 R:128.0000 rate:0.2560 gloss:0.4591 dlossA:0.5858 dlossQ:1.1294 exploreP:0.1439\n",
      "Episode:354 meanR:104.3200 R:164.0000 rate:0.3280 gloss:0.4356 dlossA:0.5801 dlossQ:1.1268 exploreP:0.1418\n",
      "Episode:355 meanR:104.6500 R:100.0000 rate:0.2000 gloss:0.4428 dlossA:0.5802 dlossQ:1.0996 exploreP:0.1405\n",
      "Episode:356 meanR:104.7300 R:67.0000 rate:0.1340 gloss:0.4431 dlossA:0.5821 dlossQ:1.0981 exploreP:0.1396\n",
      "Episode:357 meanR:106.1700 R:243.0000 rate:0.4860 gloss:0.4357 dlossA:0.5814 dlossQ:1.1106 exploreP:0.1365\n",
      "Episode:358 meanR:106.0400 R:78.0000 rate:0.1560 gloss:0.4458 dlossA:0.5824 dlossQ:1.1397 exploreP:0.1355\n",
      "Episode:359 meanR:106.2300 R:117.0000 rate:0.2340 gloss:0.4174 dlossA:0.5747 dlossQ:1.1374 exploreP:0.1340\n",
      "Episode:360 meanR:106.6800 R:110.0000 rate:0.2200 gloss:0.4205 dlossA:0.5796 dlossQ:1.1498 exploreP:0.1327\n",
      "Episode:361 meanR:107.4200 R:130.0000 rate:0.2600 gloss:0.4247 dlossA:0.5805 dlossQ:1.0961 exploreP:0.1311\n",
      "Episode:362 meanR:107.4800 R:88.0000 rate:0.1760 gloss:0.4337 dlossA:0.5803 dlossQ:1.1081 exploreP:0.1300\n",
      "Episode:363 meanR:107.5900 R:109.0000 rate:0.2180 gloss:0.4254 dlossA:0.5832 dlossQ:1.1014 exploreP:0.1287\n",
      "Episode:364 meanR:107.3100 R:84.0000 rate:0.1680 gloss:0.4210 dlossA:0.5786 dlossQ:1.1051 exploreP:0.1277\n",
      "Episode:365 meanR:107.4900 R:95.0000 rate:0.1900 gloss:0.4258 dlossA:0.5798 dlossQ:1.1267 exploreP:0.1266\n",
      "Episode:366 meanR:107.5300 R:82.0000 rate:0.1640 gloss:0.4240 dlossA:0.5792 dlossQ:1.1285 exploreP:0.1257\n",
      "Episode:367 meanR:107.8400 R:155.0000 rate:0.3100 gloss:0.4221 dlossA:0.5778 dlossQ:1.1179 exploreP:0.1239\n",
      "Episode:368 meanR:107.3400 R:77.0000 rate:0.1540 gloss:0.4212 dlossA:0.5767 dlossQ:1.0981 exploreP:0.1230\n",
      "Episode:369 meanR:107.4800 R:132.0000 rate:0.2640 gloss:0.4166 dlossA:0.5789 dlossQ:1.1468 exploreP:0.1215\n",
      "Episode:370 meanR:108.6100 R:206.0000 rate:0.4120 gloss:0.4159 dlossA:0.5720 dlossQ:1.1504 exploreP:0.1193\n",
      "Episode:371 meanR:108.7000 R:78.0000 rate:0.1560 gloss:0.4213 dlossA:0.5805 dlossQ:1.1386 exploreP:0.1184\n",
      "Episode:372 meanR:109.0300 R:98.0000 rate:0.1960 gloss:0.4127 dlossA:0.5766 dlossQ:1.1244 exploreP:0.1174\n",
      "Episode:373 meanR:109.5700 R:101.0000 rate:0.2020 gloss:0.3998 dlossA:0.5697 dlossQ:1.0974 exploreP:0.1163\n",
      "Episode:374 meanR:109.5400 R:78.0000 rate:0.1560 gloss:0.4369 dlossA:0.5958 dlossQ:1.0607 exploreP:0.1154\n",
      "Episode:375 meanR:109.8600 R:89.0000 rate:0.1780 gloss:0.4008 dlossA:0.5701 dlossQ:1.0900 exploreP:0.1145\n",
      "Episode:376 meanR:109.8700 R:76.0000 rate:0.1520 gloss:0.3891 dlossA:0.5631 dlossQ:1.1298 exploreP:0.1137\n",
      "Episode:377 meanR:109.5600 R:78.0000 rate:0.1560 gloss:0.4000 dlossA:0.5686 dlossQ:1.1162 exploreP:0.1129\n",
      "Episode:378 meanR:109.9700 R:147.0000 rate:0.2940 gloss:0.3765 dlossA:0.5603 dlossQ:1.1580 exploreP:0.1114\n",
      "Episode:379 meanR:110.9300 R:221.0000 rate:0.4420 gloss:0.3936 dlossA:0.5700 dlossQ:1.1436 exploreP:0.1092\n",
      "Episode:380 meanR:111.0700 R:120.0000 rate:0.2400 gloss:0.3881 dlossA:0.5688 dlossQ:1.1560 exploreP:0.1080\n",
      "Episode:381 meanR:111.0300 R:78.0000 rate:0.1560 gloss:0.4163 dlossA:0.5817 dlossQ:1.0916 exploreP:0.1073\n",
      "Episode:382 meanR:111.4000 R:101.0000 rate:0.2020 gloss:0.4240 dlossA:0.5843 dlossQ:1.1003 exploreP:0.1063\n",
      "Episode:383 meanR:111.5500 R:91.0000 rate:0.1820 gloss:0.4053 dlossA:0.5783 dlossQ:1.1148 exploreP:0.1054\n",
      "Episode:384 meanR:111.3300 R:53.0000 rate:0.1060 gloss:0.4495 dlossA:0.5945 dlossQ:1.1103 exploreP:0.1049\n",
      "Episode:385 meanR:111.3300 R:75.0000 rate:0.1500 gloss:0.4100 dlossA:0.5809 dlossQ:1.0891 exploreP:0.1042\n",
      "Episode:386 meanR:111.1300 R:75.0000 rate:0.1500 gloss:0.4083 dlossA:0.5824 dlossQ:1.0974 exploreP:0.1035\n",
      "Episode:387 meanR:111.6800 R:131.0000 rate:0.2620 gloss:0.4214 dlossA:0.5748 dlossQ:1.2175 exploreP:0.1023\n",
      "Episode:388 meanR:112.3400 R:81.0000 rate:0.1620 gloss:0.3889 dlossA:0.5703 dlossQ:1.1275 exploreP:0.1015\n",
      "Episode:389 meanR:111.9900 R:61.0000 rate:0.1220 gloss:0.3921 dlossA:0.5813 dlossQ:1.1497 exploreP:0.1010\n",
      "Episode:390 meanR:111.3900 R:61.0000 rate:0.1220 gloss:0.4168 dlossA:0.5757 dlossQ:1.1572 exploreP:0.1004\n",
      "Episode:391 meanR:110.8800 R:78.0000 rate:0.1560 gloss:0.4071 dlossA:0.5806 dlossQ:1.1016 exploreP:0.0997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:392 meanR:110.9900 R:93.0000 rate:0.1860 gloss:0.3893 dlossA:0.5787 dlossQ:1.1082 exploreP:0.0989\n",
      "Episode:393 meanR:111.3800 R:107.0000 rate:0.2140 gloss:0.3905 dlossA:0.5816 dlossQ:1.1786 exploreP:0.0979\n",
      "Episode:394 meanR:111.4200 R:81.0000 rate:0.1620 gloss:0.3948 dlossA:0.5758 dlossQ:1.1470 exploreP:0.0972\n",
      "Episode:395 meanR:111.3900 R:87.0000 rate:0.1740 gloss:0.3866 dlossA:0.5853 dlossQ:1.1703 exploreP:0.0965\n",
      "Episode:396 meanR:111.3300 R:78.0000 rate:0.1560 gloss:0.3920 dlossA:0.5754 dlossQ:1.1133 exploreP:0.0958\n",
      "Episode:397 meanR:110.7800 R:69.0000 rate:0.1380 gloss:0.3850 dlossA:0.5760 dlossQ:1.1608 exploreP:0.0952\n",
      "Episode:398 meanR:111.0500 R:93.0000 rate:0.1860 gloss:0.4062 dlossA:0.5832 dlossQ:1.1436 exploreP:0.0944\n",
      "Episode:399 meanR:110.6900 R:73.0000 rate:0.1460 gloss:0.3813 dlossA:0.5701 dlossQ:1.1124 exploreP:0.0938\n",
      "Episode:400 meanR:110.9200 R:105.0000 rate:0.2100 gloss:0.3560 dlossA:0.5619 dlossQ:1.1475 exploreP:0.0929\n",
      "Episode:401 meanR:110.8700 R:66.0000 rate:0.1320 gloss:0.3611 dlossA:0.5724 dlossQ:1.0797 exploreP:0.0924\n",
      "Episode:402 meanR:111.1300 R:100.0000 rate:0.2000 gloss:0.3802 dlossA:0.5851 dlossQ:1.0820 exploreP:0.0916\n",
      "Episode:403 meanR:110.7100 R:74.0000 rate:0.1480 gloss:0.3402 dlossA:0.5664 dlossQ:1.1067 exploreP:0.0910\n",
      "Episode:404 meanR:111.4000 R:145.0000 rate:0.2900 gloss:0.3693 dlossA:0.5736 dlossQ:1.1220 exploreP:0.0898\n",
      "Episode:405 meanR:111.2900 R:93.0000 rate:0.1860 gloss:0.3588 dlossA:0.5654 dlossQ:1.1136 exploreP:0.0891\n",
      "Episode:406 meanR:111.8100 R:154.0000 rate:0.3080 gloss:0.3388 dlossA:0.5662 dlossQ:1.1523 exploreP:0.0879\n",
      "Episode:407 meanR:112.9100 R:190.0000 rate:0.3800 gloss:0.3378 dlossA:0.5704 dlossQ:1.1350 exploreP:0.0864\n",
      "Episode:408 meanR:112.0900 R:44.0000 rate:0.0880 gloss:0.3582 dlossA:0.5709 dlossQ:1.0890 exploreP:0.0861\n",
      "Episode:409 meanR:112.4100 R:115.0000 rate:0.2300 gloss:0.2981 dlossA:0.5513 dlossQ:1.2027 exploreP:0.0852\n",
      "Episode:410 meanR:112.6400 R:123.0000 rate:0.2460 gloss:0.3155 dlossA:0.5567 dlossQ:1.1260 exploreP:0.0843\n",
      "Episode:411 meanR:112.9600 R:110.0000 rate:0.2200 gloss:0.3423 dlossA:0.5648 dlossQ:1.0888 exploreP:0.0835\n",
      "Episode:412 meanR:113.0900 R:76.0000 rate:0.1520 gloss:0.3363 dlossA:0.5694 dlossQ:1.0912 exploreP:0.0829\n",
      "Episode:413 meanR:114.9400 R:268.0000 rate:0.5360 gloss:0.2993 dlossA:0.5572 dlossQ:1.2141 exploreP:0.0810\n",
      "Episode:414 meanR:115.2600 R:147.0000 rate:0.2940 gloss:0.3087 dlossA:0.5543 dlossQ:1.1557 exploreP:0.0799\n",
      "Episode:415 meanR:115.5200 R:83.0000 rate:0.1660 gloss:0.3014 dlossA:0.5596 dlossQ:1.1321 exploreP:0.0794\n",
      "Episode:416 meanR:115.1500 R:63.0000 rate:0.1260 gloss:0.3190 dlossA:0.5675 dlossQ:1.0743 exploreP:0.0789\n",
      "Episode:417 meanR:114.9300 R:84.0000 rate:0.1680 gloss:0.2623 dlossA:0.5459 dlossQ:1.1798 exploreP:0.0783\n",
      "Episode:418 meanR:114.2700 R:90.0000 rate:0.1800 gloss:0.2793 dlossA:0.5522 dlossQ:1.2093 exploreP:0.0777\n",
      "Episode:419 meanR:113.4600 R:110.0000 rate:0.2200 gloss:0.3230 dlossA:0.5666 dlossQ:1.2031 exploreP:0.0770\n",
      "Episode:420 meanR:113.1100 R:117.0000 rate:0.2340 gloss:0.3144 dlossA:0.5583 dlossQ:1.1919 exploreP:0.0762\n",
      "Episode:421 meanR:113.4500 R:109.0000 rate:0.2180 gloss:0.3208 dlossA:0.5654 dlossQ:1.1069 exploreP:0.0755\n",
      "Episode:422 meanR:113.1200 R:67.0000 rate:0.1340 gloss:0.3144 dlossA:0.5645 dlossQ:1.1356 exploreP:0.0751\n",
      "Episode:423 meanR:113.1400 R:92.0000 rate:0.1840 gloss:0.3077 dlossA:0.5648 dlossQ:1.1367 exploreP:0.0745\n",
      "Episode:424 meanR:112.9800 R:137.0000 rate:0.2740 gloss:0.2913 dlossA:0.5585 dlossQ:1.1487 exploreP:0.0736\n",
      "Episode:425 meanR:113.0000 R:84.0000 rate:0.1680 gloss:0.3060 dlossA:0.5623 dlossQ:1.0978 exploreP:0.0730\n",
      "Episode:426 meanR:112.7100 R:67.0000 rate:0.1340 gloss:0.2793 dlossA:0.5556 dlossQ:1.1880 exploreP:0.0726\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
