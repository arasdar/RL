{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rated DPG for Cartpole\n",
    "\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aras/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1111):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: 0.01 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rewards = [each[3] for each in batch]\n",
    "print('rewards:', np.max(np.array(rewards))/100, np.min(np.array(rewards))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates')\n",
    "    return states, actions, targetQs, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator/Controller: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/Dopamine: Reward function/planner/naviator/advisor/supervisor/cortical columns\n",
    "def discriminator(states, actions, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return rewards logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, actions, targetQs, rates):\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                              labels=actions_labels)\n",
    "    targetQs = tf.reshape(targetQs, shape=[-1, 1])\n",
    "    gloss = tf.reduce_mean(neg_log_prob * targetQs) # DPG: r+(gamma*nextQ)\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states)\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, reuse=True) # Qs\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dlossA = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=rates)) # 0-1\n",
    "    dlossA += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                     labels=rates)) # 0-1\n",
    "    dlossA /= 2\n",
    "    dlossQ = tf.reduce_mean(tf.square(gQs - targetQs)) # DQN\n",
    "    dlossQ += tf.reduce_mean(tf.square(dQs - targetQs)) # DQN\n",
    "    dlossQ /= 2\n",
    "    return actions_logits, gQs, gloss, dlossA, dlossQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizating/training/learning G & D\n",
    "def model_opt(g_loss, d_lossA, d_lossQ, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_optA = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossA, var_list=d_vars)\n",
    "        d_optQ = tf.train.AdamOptimizer(d_learning_rate).minimize(d_lossQ, var_list=d_vars)\n",
    "\n",
    "    return g_opt, d_optA, d_optQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.rates = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.Qs_logits, self.g_loss, self.d_lossA, self.d_lossQ = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_optA, self.d_optQ = model_opt(g_loss=self.g_loss, \n",
    "                                                         d_lossA=self.d_lossA, \n",
    "                                                         d_lossQ=self.d_lossQ, \n",
    "                                                         g_learning_rate=g_learning_rate, \n",
    "                                                         d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 500\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/goal\n",
    "        total_reward = 0 # reset\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "rates = np.array([each[5] for each in batch])\n",
    "batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "states = np.array([each[0] for each in batch])\n",
    "actions = np.array([each[1] for each in batch])\n",
    "next_states = np.array([each[2] for each in batch])\n",
    "rewards = np.array([each[3] for each in batch])\n",
    "dones = np.array([each[4] for each in batch])\n",
    "rates = np.array([each[5] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((92, 6), (92, 4), (92,), (92, 4), (92,), (92,), (92,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape, \\\n",
    "states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:24.0000 R:24.0000 rate:0.0480 gloss:0.6918 dlossA:0.8147 dlossQ:0.8218 exploreP:0.9976\n",
      "Episode:1 meanR:18.5000 R:13.0000 rate:0.0260 gloss:0.6876 dlossA:0.8123 dlossQ:0.8223 exploreP:0.9963\n",
      "Episode:2 meanR:16.0000 R:11.0000 rate:0.0220 gloss:0.7007 dlossA:0.8128 dlossQ:0.8237 exploreP:0.9953\n",
      "Episode:3 meanR:15.7500 R:15.0000 rate:0.0300 gloss:0.6939 dlossA:0.8124 dlossQ:0.8239 exploreP:0.9938\n",
      "Episode:4 meanR:15.8000 R:16.0000 rate:0.0320 gloss:0.7062 dlossA:0.8148 dlossQ:0.8287 exploreP:0.9922\n",
      "Episode:5 meanR:15.6667 R:15.0000 rate:0.0300 gloss:0.6957 dlossA:0.8108 dlossQ:0.8282 exploreP:0.9907\n",
      "Episode:6 meanR:17.1429 R:26.0000 rate:0.0520 gloss:0.7091 dlossA:0.8165 dlossQ:0.8347 exploreP:0.9882\n",
      "Episode:7 meanR:16.7500 R:14.0000 rate:0.0280 gloss:0.7199 dlossA:0.8208 dlossQ:0.8304 exploreP:0.9868\n",
      "Episode:8 meanR:17.0000 R:19.0000 rate:0.0380 gloss:0.7171 dlossA:0.8175 dlossQ:0.8367 exploreP:0.9850\n",
      "Episode:9 meanR:17.3000 R:20.0000 rate:0.0400 gloss:0.7152 dlossA:0.8103 dlossQ:0.8466 exploreP:0.9830\n",
      "Episode:10 meanR:17.7273 R:22.0000 rate:0.0440 gloss:0.7164 dlossA:0.8146 dlossQ:0.8365 exploreP:0.9809\n",
      "Episode:11 meanR:18.8333 R:31.0000 rate:0.0620 gloss:0.7269 dlossA:0.8165 dlossQ:0.8386 exploreP:0.9779\n",
      "Episode:12 meanR:18.2308 R:11.0000 rate:0.0220 gloss:0.7180 dlossA:0.8165 dlossQ:0.8457 exploreP:0.9768\n",
      "Episode:13 meanR:20.0000 R:43.0000 rate:0.0860 gloss:0.7315 dlossA:0.8201 dlossQ:0.8441 exploreP:0.9727\n",
      "Episode:14 meanR:20.2000 R:23.0000 rate:0.0460 gloss:0.7471 dlossA:0.8250 dlossQ:0.8518 exploreP:0.9705\n",
      "Episode:15 meanR:20.8750 R:31.0000 rate:0.0620 gloss:0.7473 dlossA:0.8287 dlossQ:0.8438 exploreP:0.9675\n",
      "Episode:16 meanR:22.4118 R:47.0000 rate:0.0940 gloss:0.7540 dlossA:0.8292 dlossQ:0.8530 exploreP:0.9630\n",
      "Episode:17 meanR:22.0556 R:16.0000 rate:0.0320 gloss:0.7463 dlossA:0.8251 dlossQ:0.8603 exploreP:0.9615\n",
      "Episode:18 meanR:21.4737 R:11.0000 rate:0.0220 gloss:0.7711 dlossA:0.8292 dlossQ:0.8755 exploreP:0.9604\n",
      "Episode:19 meanR:21.1500 R:15.0000 rate:0.0300 gloss:0.7572 dlossA:0.8369 dlossQ:0.8417 exploreP:0.9590\n",
      "Episode:20 meanR:21.0476 R:19.0000 rate:0.0380 gloss:0.7593 dlossA:0.8321 dlossQ:0.8552 exploreP:0.9572\n",
      "Episode:21 meanR:21.4545 R:30.0000 rate:0.0600 gloss:0.7488 dlossA:0.8173 dlossQ:0.8817 exploreP:0.9544\n",
      "Episode:22 meanR:21.6957 R:27.0000 rate:0.0540 gloss:0.7619 dlossA:0.8291 dlossQ:0.8704 exploreP:0.9518\n",
      "Episode:23 meanR:21.8333 R:25.0000 rate:0.0500 gloss:0.7656 dlossA:0.8302 dlossQ:0.8793 exploreP:0.9495\n",
      "Episode:24 meanR:21.6000 R:16.0000 rate:0.0320 gloss:0.7685 dlossA:0.8344 dlossQ:0.8708 exploreP:0.9480\n",
      "Episode:25 meanR:22.3462 R:41.0000 rate:0.0820 gloss:0.7592 dlossA:0.8285 dlossQ:0.8715 exploreP:0.9441\n",
      "Episode:26 meanR:22.2963 R:21.0000 rate:0.0420 gloss:0.7737 dlossA:0.8211 dlossQ:0.9084 exploreP:0.9422\n",
      "Episode:27 meanR:22.2857 R:22.0000 rate:0.0440 gloss:0.7709 dlossA:0.8334 dlossQ:0.8704 exploreP:0.9401\n",
      "Episode:28 meanR:22.1724 R:19.0000 rate:0.0380 gloss:0.7666 dlossA:0.8308 dlossQ:0.8739 exploreP:0.9383\n",
      "Episode:29 meanR:22.5667 R:34.0000 rate:0.0680 gloss:0.7649 dlossA:0.8304 dlossQ:0.8786 exploreP:0.9352\n",
      "Episode:30 meanR:22.3871 R:17.0000 rate:0.0340 gloss:0.7724 dlossA:0.8355 dlossQ:0.8784 exploreP:0.9336\n",
      "Episode:31 meanR:22.0312 R:11.0000 rate:0.0220 gloss:0.7741 dlossA:0.8401 dlossQ:0.8747 exploreP:0.9326\n",
      "Episode:32 meanR:21.6970 R:11.0000 rate:0.0220 gloss:0.7879 dlossA:0.8351 dlossQ:0.8979 exploreP:0.9316\n",
      "Episode:33 meanR:21.7353 R:23.0000 rate:0.0460 gloss:0.7746 dlossA:0.8339 dlossQ:0.8836 exploreP:0.9295\n",
      "Episode:34 meanR:21.4286 R:11.0000 rate:0.0220 gloss:0.7636 dlossA:0.8255 dlossQ:0.8980 exploreP:0.9285\n",
      "Episode:35 meanR:21.1389 R:11.0000 rate:0.0220 gloss:0.7517 dlossA:0.8354 dlossQ:0.8549 exploreP:0.9275\n",
      "Episode:36 meanR:20.9730 R:15.0000 rate:0.0300 gloss:0.7568 dlossA:0.8293 dlossQ:0.8753 exploreP:0.9261\n",
      "Episode:37 meanR:20.9474 R:20.0000 rate:0.0400 gloss:0.7671 dlossA:0.8424 dlossQ:0.8602 exploreP:0.9243\n",
      "Episode:38 meanR:20.7179 R:12.0000 rate:0.0240 gloss:0.7627 dlossA:0.8416 dlossQ:0.8401 exploreP:0.9232\n",
      "Episode:39 meanR:20.6500 R:18.0000 rate:0.0360 gloss:0.7668 dlossA:0.8336 dlossQ:0.8747 exploreP:0.9215\n",
      "Episode:40 meanR:20.5854 R:18.0000 rate:0.0360 gloss:0.7693 dlossA:0.8320 dlossQ:0.8877 exploreP:0.9199\n",
      "Episode:41 meanR:20.9286 R:35.0000 rate:0.0700 gloss:0.7793 dlossA:0.8358 dlossQ:0.8892 exploreP:0.9167\n",
      "Episode:42 meanR:20.8372 R:17.0000 rate:0.0340 gloss:0.7784 dlossA:0.8271 dlossQ:0.8832 exploreP:0.9152\n",
      "Episode:43 meanR:20.6591 R:13.0000 rate:0.0260 gloss:0.7740 dlossA:0.8363 dlossQ:0.8759 exploreP:0.9140\n",
      "Episode:44 meanR:20.6667 R:21.0000 rate:0.0420 gloss:0.7684 dlossA:0.8389 dlossQ:0.8651 exploreP:0.9121\n",
      "Episode:45 meanR:20.7174 R:23.0000 rate:0.0460 gloss:0.7857 dlossA:0.8346 dlossQ:0.9030 exploreP:0.9100\n",
      "Episode:46 meanR:20.8085 R:25.0000 rate:0.0500 gloss:0.8002 dlossA:0.8375 dlossQ:0.9079 exploreP:0.9078\n",
      "Episode:47 meanR:20.7083 R:16.0000 rate:0.0320 gloss:0.7900 dlossA:0.8322 dlossQ:0.9071 exploreP:0.9063\n",
      "Episode:48 meanR:20.7959 R:25.0000 rate:0.0500 gloss:0.7931 dlossA:0.8392 dlossQ:0.8994 exploreP:0.9041\n",
      "Episode:49 meanR:21.1600 R:39.0000 rate:0.0780 gloss:0.7860 dlossA:0.8378 dlossQ:0.8952 exploreP:0.9006\n",
      "Episode:50 meanR:21.2353 R:25.0000 rate:0.0500 gloss:0.7838 dlossA:0.8345 dlossQ:0.8894 exploreP:0.8984\n",
      "Episode:51 meanR:21.0577 R:12.0000 rate:0.0240 gloss:0.7793 dlossA:0.8350 dlossQ:0.9013 exploreP:0.8973\n",
      "Episode:52 meanR:21.3396 R:36.0000 rate:0.0720 gloss:0.7864 dlossA:0.8401 dlossQ:0.8925 exploreP:0.8941\n",
      "Episode:53 meanR:21.2778 R:18.0000 rate:0.0360 gloss:0.7949 dlossA:0.8295 dlossQ:0.9249 exploreP:0.8925\n",
      "Episode:54 meanR:21.4182 R:29.0000 rate:0.0580 gloss:0.7832 dlossA:0.8400 dlossQ:0.8900 exploreP:0.8900\n",
      "Episode:55 meanR:21.8036 R:43.0000 rate:0.0860 gloss:0.7814 dlossA:0.8321 dlossQ:0.8960 exploreP:0.8862\n",
      "Episode:56 meanR:21.8246 R:23.0000 rate:0.0460 gloss:0.7822 dlossA:0.8322 dlossQ:0.8936 exploreP:0.8842\n",
      "Episode:57 meanR:21.7759 R:19.0000 rate:0.0380 gloss:0.7871 dlossA:0.8361 dlossQ:0.8896 exploreP:0.8825\n",
      "Episode:58 meanR:22.1186 R:42.0000 rate:0.0840 gloss:0.7875 dlossA:0.8426 dlossQ:0.8946 exploreP:0.8789\n",
      "Episode:59 meanR:22.0667 R:19.0000 rate:0.0380 gloss:0.7750 dlossA:0.8378 dlossQ:0.8904 exploreP:0.8772\n",
      "Episode:60 meanR:22.2131 R:31.0000 rate:0.0620 gloss:0.7815 dlossA:0.8444 dlossQ:0.8779 exploreP:0.8745\n",
      "Episode:61 meanR:22.0323 R:11.0000 rate:0.0220 gloss:0.7755 dlossA:0.8522 dlossQ:0.8570 exploreP:0.8736\n",
      "Episode:62 meanR:22.0635 R:24.0000 rate:0.0480 gloss:0.7931 dlossA:0.8394 dlossQ:0.9085 exploreP:0.8715\n",
      "Episode:63 meanR:21.9531 R:15.0000 rate:0.0300 gloss:0.7986 dlossA:0.8409 dlossQ:0.9088 exploreP:0.8702\n",
      "Episode:64 meanR:22.2462 R:41.0000 rate:0.0820 gloss:0.7862 dlossA:0.8416 dlossQ:0.8961 exploreP:0.8667\n",
      "Episode:65 meanR:22.1212 R:14.0000 rate:0.0280 gloss:0.7733 dlossA:0.8383 dlossQ:0.8859 exploreP:0.8655\n",
      "Episode:66 meanR:21.9851 R:13.0000 rate:0.0260 gloss:0.7733 dlossA:0.8401 dlossQ:0.8923 exploreP:0.8644\n",
      "Episode:67 meanR:21.9559 R:20.0000 rate:0.0400 gloss:0.7954 dlossA:0.8391 dlossQ:0.9087 exploreP:0.8627\n",
      "Episode:68 meanR:22.1159 R:33.0000 rate:0.0660 gloss:0.7883 dlossA:0.8361 dlossQ:0.9115 exploreP:0.8599\n",
      "Episode:69 meanR:22.5000 R:49.0000 rate:0.0980 gloss:0.7949 dlossA:0.8453 dlossQ:0.9013 exploreP:0.8557\n",
      "Episode:70 meanR:22.3662 R:13.0000 rate:0.0260 gloss:0.7929 dlossA:0.8356 dlossQ:0.9305 exploreP:0.8546\n",
      "Episode:71 meanR:22.5278 R:34.0000 rate:0.0680 gloss:0.7977 dlossA:0.8407 dlossQ:0.9230 exploreP:0.8518\n",
      "Episode:72 meanR:22.4247 R:15.0000 rate:0.0300 gloss:0.7981 dlossA:0.8441 dlossQ:0.9124 exploreP:0.8505\n",
      "Episode:73 meanR:22.3919 R:20.0000 rate:0.0400 gloss:0.7814 dlossA:0.8383 dlossQ:0.8979 exploreP:0.8488\n",
      "Episode:74 meanR:22.5867 R:37.0000 rate:0.0740 gloss:0.7947 dlossA:0.8412 dlossQ:0.9154 exploreP:0.8457\n",
      "Episode:75 meanR:22.5000 R:16.0000 rate:0.0320 gloss:0.7834 dlossA:0.8436 dlossQ:0.8931 exploreP:0.8444\n",
      "Episode:76 meanR:22.4416 R:18.0000 rate:0.0360 gloss:0.7883 dlossA:0.8405 dlossQ:0.8999 exploreP:0.8429\n",
      "Episode:77 meanR:22.3333 R:14.0000 rate:0.0280 gloss:0.8129 dlossA:0.8452 dlossQ:0.9403 exploreP:0.8417\n",
      "Episode:78 meanR:22.1899 R:11.0000 rate:0.0220 gloss:0.8029 dlossA:0.8363 dlossQ:0.9417 exploreP:0.8408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:79 meanR:22.1375 R:18.0000 rate:0.0360 gloss:0.7964 dlossA:0.8405 dlossQ:0.9196 exploreP:0.8393\n",
      "Episode:80 meanR:22.1358 R:22.0000 rate:0.0440 gloss:0.7985 dlossA:0.8378 dlossQ:0.9271 exploreP:0.8375\n",
      "Episode:81 meanR:22.1707 R:25.0000 rate:0.0500 gloss:0.7883 dlossA:0.8435 dlossQ:0.8819 exploreP:0.8354\n",
      "Episode:82 meanR:22.3133 R:34.0000 rate:0.0680 gloss:0.7972 dlossA:0.8462 dlossQ:0.9052 exploreP:0.8326\n",
      "Episode:83 meanR:22.4524 R:34.0000 rate:0.0680 gloss:0.7941 dlossA:0.8441 dlossQ:0.9048 exploreP:0.8298\n",
      "Episode:84 meanR:22.9765 R:67.0000 rate:0.1340 gloss:0.7986 dlossA:0.8421 dlossQ:0.9115 exploreP:0.8244\n",
      "Episode:85 meanR:23.1628 R:39.0000 rate:0.0780 gloss:0.7936 dlossA:0.8389 dlossQ:0.9158 exploreP:0.8212\n",
      "Episode:86 meanR:23.1494 R:22.0000 rate:0.0440 gloss:0.8032 dlossA:0.8459 dlossQ:0.9066 exploreP:0.8194\n",
      "Episode:87 meanR:23.1477 R:23.0000 rate:0.0460 gloss:0.7952 dlossA:0.8400 dlossQ:0.9180 exploreP:0.8175\n",
      "Episode:88 meanR:23.3483 R:41.0000 rate:0.0820 gloss:0.7930 dlossA:0.8382 dlossQ:0.9182 exploreP:0.8142\n",
      "Episode:89 meanR:23.3778 R:26.0000 rate:0.0520 gloss:0.8052 dlossA:0.8419 dlossQ:0.9291 exploreP:0.8122\n",
      "Episode:90 meanR:23.2198 R:9.0000 rate:0.0180 gloss:0.7974 dlossA:0.8472 dlossQ:0.9090 exploreP:0.8114\n",
      "Episode:91 meanR:23.4022 R:40.0000 rate:0.0800 gloss:0.7920 dlossA:0.8407 dlossQ:0.9155 exploreP:0.8082\n",
      "Episode:92 meanR:23.3656 R:20.0000 rate:0.0400 gloss:0.7895 dlossA:0.8390 dlossQ:0.9089 exploreP:0.8066\n",
      "Episode:93 meanR:23.2979 R:17.0000 rate:0.0340 gloss:0.8079 dlossA:0.8465 dlossQ:0.9255 exploreP:0.8053\n",
      "Episode:94 meanR:23.4947 R:42.0000 rate:0.0840 gloss:0.8036 dlossA:0.8457 dlossQ:0.9117 exploreP:0.8020\n",
      "Episode:95 meanR:23.4271 R:17.0000 rate:0.0340 gloss:0.7923 dlossA:0.8420 dlossQ:0.9135 exploreP:0.8006\n",
      "Episode:96 meanR:23.6804 R:48.0000 rate:0.0960 gloss:0.7964 dlossA:0.8454 dlossQ:0.9097 exploreP:0.7968\n",
      "Episode:97 meanR:23.7143 R:27.0000 rate:0.0540 gloss:0.8078 dlossA:0.8373 dlossQ:0.9473 exploreP:0.7947\n",
      "Episode:98 meanR:23.6768 R:20.0000 rate:0.0400 gloss:0.7959 dlossA:0.8432 dlossQ:0.9219 exploreP:0.7931\n",
      "Episode:99 meanR:23.7100 R:27.0000 rate:0.0540 gloss:0.8028 dlossA:0.8422 dlossQ:0.9266 exploreP:0.7910\n",
      "Episode:100 meanR:23.7500 R:28.0000 rate:0.0560 gloss:0.7908 dlossA:0.8411 dlossQ:0.8959 exploreP:0.7888\n",
      "Episode:101 meanR:23.7700 R:15.0000 rate:0.0300 gloss:0.8015 dlossA:0.8471 dlossQ:0.9124 exploreP:0.7877\n",
      "Episode:102 meanR:23.8700 R:21.0000 rate:0.0420 gloss:0.8009 dlossA:0.8444 dlossQ:0.9237 exploreP:0.7860\n",
      "Episode:103 meanR:24.1600 R:44.0000 rate:0.0880 gloss:0.7915 dlossA:0.8404 dlossQ:0.9147 exploreP:0.7826\n",
      "Episode:104 meanR:24.1400 R:14.0000 rate:0.0280 gloss:0.7783 dlossA:0.8440 dlossQ:0.8844 exploreP:0.7816\n",
      "Episode:105 meanR:24.2000 R:21.0000 rate:0.0420 gloss:0.7953 dlossA:0.8490 dlossQ:0.9003 exploreP:0.7799\n",
      "Episode:106 meanR:24.1500 R:21.0000 rate:0.0420 gloss:0.8007 dlossA:0.8379 dlossQ:0.9327 exploreP:0.7783\n",
      "Episode:107 meanR:24.3400 R:33.0000 rate:0.0660 gloss:0.7975 dlossA:0.8437 dlossQ:0.9231 exploreP:0.7758\n",
      "Episode:108 meanR:24.3200 R:17.0000 rate:0.0340 gloss:0.7964 dlossA:0.8426 dlossQ:0.9292 exploreP:0.7745\n",
      "Episode:109 meanR:24.5800 R:46.0000 rate:0.0920 gloss:0.8006 dlossA:0.8466 dlossQ:0.9169 exploreP:0.7710\n",
      "Episode:110 meanR:24.7400 R:38.0000 rate:0.0760 gloss:0.8176 dlossA:0.8426 dlossQ:0.9585 exploreP:0.7681\n",
      "Episode:111 meanR:24.5900 R:16.0000 rate:0.0320 gloss:0.8162 dlossA:0.8376 dlossQ:0.9622 exploreP:0.7669\n",
      "Episode:112 meanR:24.7100 R:23.0000 rate:0.0460 gloss:0.7991 dlossA:0.8447 dlossQ:0.9291 exploreP:0.7651\n",
      "Episode:113 meanR:24.4800 R:20.0000 rate:0.0400 gloss:0.7983 dlossA:0.8416 dlossQ:0.9248 exploreP:0.7636\n",
      "Episode:114 meanR:24.7400 R:49.0000 rate:0.0980 gloss:0.7909 dlossA:0.8440 dlossQ:0.9060 exploreP:0.7599\n",
      "Episode:115 meanR:24.5500 R:12.0000 rate:0.0240 gloss:0.7916 dlossA:0.8437 dlossQ:0.9108 exploreP:0.7590\n",
      "Episode:116 meanR:24.2400 R:16.0000 rate:0.0320 gloss:0.7712 dlossA:0.8437 dlossQ:0.8785 exploreP:0.7579\n",
      "Episode:117 meanR:24.4700 R:39.0000 rate:0.0780 gloss:0.7928 dlossA:0.8362 dlossQ:0.9157 exploreP:0.7549\n",
      "Episode:118 meanR:24.5900 R:23.0000 rate:0.0460 gloss:0.8026 dlossA:0.8493 dlossQ:0.8980 exploreP:0.7532\n",
      "Episode:119 meanR:24.6600 R:22.0000 rate:0.0440 gloss:0.8122 dlossA:0.8448 dlossQ:0.9390 exploreP:0.7516\n",
      "Episode:120 meanR:24.8900 R:42.0000 rate:0.0840 gloss:0.8007 dlossA:0.8490 dlossQ:0.9051 exploreP:0.7485\n",
      "Episode:121 meanR:24.8700 R:28.0000 rate:0.0560 gloss:0.8052 dlossA:0.8465 dlossQ:0.9187 exploreP:0.7464\n",
      "Episode:122 meanR:25.1000 R:50.0000 rate:0.1000 gloss:0.8191 dlossA:0.8524 dlossQ:0.9363 exploreP:0.7428\n",
      "Episode:123 meanR:25.2300 R:38.0000 rate:0.0760 gloss:0.8103 dlossA:0.8462 dlossQ:0.9397 exploreP:0.7400\n",
      "Episode:124 meanR:25.2500 R:18.0000 rate:0.0360 gloss:0.8167 dlossA:0.8458 dlossQ:0.9494 exploreP:0.7387\n",
      "Episode:125 meanR:25.1200 R:28.0000 rate:0.0560 gloss:0.8141 dlossA:0.8482 dlossQ:0.9431 exploreP:0.7366\n",
      "Episode:126 meanR:25.2100 R:30.0000 rate:0.0600 gloss:0.8034 dlossA:0.8432 dlossQ:0.9288 exploreP:0.7344\n",
      "Episode:127 meanR:25.1800 R:19.0000 rate:0.0380 gloss:0.8026 dlossA:0.8450 dlossQ:0.9287 exploreP:0.7331\n",
      "Episode:128 meanR:25.3400 R:35.0000 rate:0.0700 gloss:0.8092 dlossA:0.8465 dlossQ:0.9406 exploreP:0.7305\n",
      "Episode:129 meanR:25.2300 R:23.0000 rate:0.0460 gloss:0.8008 dlossA:0.8472 dlossQ:0.9245 exploreP:0.7289\n",
      "Episode:130 meanR:25.3700 R:31.0000 rate:0.0620 gloss:0.8097 dlossA:0.8456 dlossQ:0.9515 exploreP:0.7267\n",
      "Episode:131 meanR:25.5300 R:27.0000 rate:0.0540 gloss:0.8087 dlossA:0.8433 dlossQ:0.9341 exploreP:0.7247\n",
      "Episode:132 meanR:26.0600 R:64.0000 rate:0.1280 gloss:0.7993 dlossA:0.8487 dlossQ:0.9190 exploreP:0.7202\n",
      "Episode:133 meanR:25.9600 R:13.0000 rate:0.0260 gloss:0.8068 dlossA:0.8494 dlossQ:0.9295 exploreP:0.7192\n",
      "Episode:134 meanR:26.0900 R:24.0000 rate:0.0480 gloss:0.7955 dlossA:0.8474 dlossQ:0.9082 exploreP:0.7175\n",
      "Episode:135 meanR:26.1200 R:14.0000 rate:0.0280 gloss:0.7990 dlossA:0.8472 dlossQ:0.9088 exploreP:0.7166\n",
      "Episode:136 meanR:26.7300 R:76.0000 rate:0.1520 gloss:0.8079 dlossA:0.8466 dlossQ:0.9374 exploreP:0.7112\n",
      "Episode:137 meanR:26.8000 R:27.0000 rate:0.0540 gloss:0.8054 dlossA:0.8397 dlossQ:0.9374 exploreP:0.7093\n",
      "Episode:138 meanR:26.8700 R:19.0000 rate:0.0380 gloss:0.7910 dlossA:0.8433 dlossQ:0.9155 exploreP:0.7080\n",
      "Episode:139 meanR:26.8500 R:16.0000 rate:0.0320 gloss:0.8083 dlossA:0.8432 dlossQ:0.9469 exploreP:0.7069\n",
      "Episode:140 meanR:27.0100 R:34.0000 rate:0.0680 gloss:0.7876 dlossA:0.8438 dlossQ:0.9154 exploreP:0.7045\n",
      "Episode:141 meanR:26.8800 R:22.0000 rate:0.0440 gloss:0.7961 dlossA:0.8426 dlossQ:0.9236 exploreP:0.7030\n",
      "Episode:142 meanR:27.0500 R:34.0000 rate:0.0680 gloss:0.7906 dlossA:0.8378 dlossQ:0.9208 exploreP:0.7006\n",
      "Episode:143 meanR:27.1300 R:21.0000 rate:0.0420 gloss:0.7882 dlossA:0.8403 dlossQ:0.9147 exploreP:0.6992\n",
      "Episode:144 meanR:27.1100 R:19.0000 rate:0.0380 gloss:0.7906 dlossA:0.8433 dlossQ:0.9153 exploreP:0.6979\n",
      "Episode:145 meanR:27.0300 R:15.0000 rate:0.0300 gloss:0.8079 dlossA:0.8427 dlossQ:0.9486 exploreP:0.6968\n",
      "Episode:146 meanR:26.9600 R:18.0000 rate:0.0360 gloss:0.7984 dlossA:0.8457 dlossQ:0.9272 exploreP:0.6956\n",
      "Episode:147 meanR:26.9500 R:15.0000 rate:0.0300 gloss:0.7953 dlossA:0.8435 dlossQ:0.9102 exploreP:0.6946\n",
      "Episode:148 meanR:27.0700 R:37.0000 rate:0.0740 gloss:0.7982 dlossA:0.8466 dlossQ:0.9112 exploreP:0.6921\n",
      "Episode:149 meanR:26.8700 R:19.0000 rate:0.0380 gloss:0.7854 dlossA:0.8419 dlossQ:0.9030 exploreP:0.6908\n",
      "Episode:150 meanR:26.8700 R:25.0000 rate:0.0500 gloss:0.7985 dlossA:0.8415 dlossQ:0.9404 exploreP:0.6891\n",
      "Episode:151 meanR:27.1700 R:42.0000 rate:0.0840 gloss:0.8028 dlossA:0.8444 dlossQ:0.9332 exploreP:0.6862\n",
      "Episode:152 meanR:26.9900 R:18.0000 rate:0.0360 gloss:0.7902 dlossA:0.8435 dlossQ:0.8963 exploreP:0.6850\n",
      "Episode:153 meanR:26.9700 R:16.0000 rate:0.0320 gloss:0.7884 dlossA:0.8452 dlossQ:0.9138 exploreP:0.6839\n",
      "Episode:154 meanR:27.1100 R:43.0000 rate:0.0860 gloss:0.7952 dlossA:0.8416 dlossQ:0.9302 exploreP:0.6810\n",
      "Episode:155 meanR:26.9800 R:30.0000 rate:0.0600 gloss:0.7876 dlossA:0.8395 dlossQ:0.9239 exploreP:0.6790\n",
      "Episode:156 meanR:27.1700 R:42.0000 rate:0.0840 gloss:0.7910 dlossA:0.8446 dlossQ:0.9214 exploreP:0.6762\n",
      "Episode:157 meanR:27.1400 R:16.0000 rate:0.0320 gloss:0.7881 dlossA:0.8386 dlossQ:0.9179 exploreP:0.6751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:158 meanR:27.0900 R:37.0000 rate:0.0740 gloss:0.8042 dlossA:0.8458 dlossQ:0.9363 exploreP:0.6727\n",
      "Episode:159 meanR:27.3200 R:42.0000 rate:0.0840 gloss:0.7958 dlossA:0.8446 dlossQ:0.9239 exploreP:0.6699\n",
      "Episode:160 meanR:27.9700 R:96.0000 rate:0.1920 gloss:0.7988 dlossA:0.8394 dlossQ:0.9379 exploreP:0.6636\n",
      "Episode:161 meanR:27.9800 R:12.0000 rate:0.0240 gloss:0.7945 dlossA:0.8357 dlossQ:0.9365 exploreP:0.6628\n",
      "Episode:162 meanR:27.9300 R:19.0000 rate:0.0380 gloss:0.7874 dlossA:0.8253 dlossQ:0.9439 exploreP:0.6616\n",
      "Episode:163 meanR:28.0800 R:30.0000 rate:0.0600 gloss:0.7910 dlossA:0.8428 dlossQ:0.9141 exploreP:0.6596\n",
      "Episode:164 meanR:27.8500 R:18.0000 rate:0.0360 gloss:0.7793 dlossA:0.8416 dlossQ:0.9091 exploreP:0.6585\n",
      "Episode:165 meanR:27.9300 R:22.0000 rate:0.0440 gloss:0.7884 dlossA:0.8439 dlossQ:0.9066 exploreP:0.6570\n",
      "Episode:166 meanR:28.7000 R:90.0000 rate:0.1800 gloss:0.7950 dlossA:0.8400 dlossQ:0.9157 exploreP:0.6512\n",
      "Episode:167 meanR:29.6300 R:113.0000 rate:0.2260 gloss:0.8073 dlossA:0.8481 dlossQ:0.9227 exploreP:0.6440\n",
      "Episode:168 meanR:29.7000 R:40.0000 rate:0.0800 gloss:0.7973 dlossA:0.8491 dlossQ:0.9226 exploreP:0.6415\n",
      "Episode:169 meanR:29.6400 R:43.0000 rate:0.0860 gloss:0.7978 dlossA:0.8401 dlossQ:0.9332 exploreP:0.6388\n",
      "Episode:170 meanR:29.7700 R:26.0000 rate:0.0520 gloss:0.7844 dlossA:0.8393 dlossQ:0.9229 exploreP:0.6372\n",
      "Episode:171 meanR:30.2800 R:85.0000 rate:0.1700 gloss:0.7962 dlossA:0.8419 dlossQ:0.9261 exploreP:0.6319\n",
      "Episode:172 meanR:31.3100 R:118.0000 rate:0.2360 gloss:0.7970 dlossA:0.8394 dlossQ:0.9364 exploreP:0.6246\n",
      "Episode:173 meanR:31.8100 R:70.0000 rate:0.1400 gloss:0.7804 dlossA:0.8368 dlossQ:0.9122 exploreP:0.6203\n",
      "Episode:174 meanR:31.9700 R:53.0000 rate:0.1060 gloss:0.7842 dlossA:0.8406 dlossQ:0.9030 exploreP:0.6170\n",
      "Episode:175 meanR:32.0200 R:21.0000 rate:0.0420 gloss:0.7883 dlossA:0.8401 dlossQ:0.9227 exploreP:0.6158\n",
      "Episode:176 meanR:32.2700 R:43.0000 rate:0.0860 gloss:0.7760 dlossA:0.8367 dlossQ:0.9090 exploreP:0.6132\n",
      "Episode:177 meanR:32.5900 R:46.0000 rate:0.0920 gloss:0.7980 dlossA:0.8388 dlossQ:0.9415 exploreP:0.6104\n",
      "Episode:178 meanR:33.1100 R:63.0000 rate:0.1260 gloss:0.7977 dlossA:0.8386 dlossQ:0.9389 exploreP:0.6066\n",
      "Episode:179 meanR:33.0700 R:14.0000 rate:0.0280 gloss:0.7949 dlossA:0.8326 dlossQ:0.9475 exploreP:0.6058\n",
      "Episode:180 meanR:33.2000 R:35.0000 rate:0.0700 gloss:0.7835 dlossA:0.8411 dlossQ:0.9080 exploreP:0.6037\n",
      "Episode:181 meanR:33.7300 R:78.0000 rate:0.1560 gloss:0.7899 dlossA:0.8317 dlossQ:0.9312 exploreP:0.5991\n",
      "Episode:182 meanR:34.0200 R:63.0000 rate:0.1260 gloss:0.7987 dlossA:0.8382 dlossQ:0.9244 exploreP:0.5954\n",
      "Episode:183 meanR:34.3600 R:68.0000 rate:0.1360 gloss:0.7849 dlossA:0.8370 dlossQ:0.9274 exploreP:0.5914\n",
      "Episode:184 meanR:34.4100 R:72.0000 rate:0.1440 gloss:0.7790 dlossA:0.8361 dlossQ:0.9022 exploreP:0.5873\n",
      "Episode:185 meanR:34.1900 R:17.0000 rate:0.0340 gloss:0.8031 dlossA:0.8185 dlossQ:0.9729 exploreP:0.5863\n",
      "Episode:186 meanR:34.3000 R:33.0000 rate:0.0660 gloss:0.7746 dlossA:0.8332 dlossQ:0.9055 exploreP:0.5844\n",
      "Episode:187 meanR:34.3100 R:24.0000 rate:0.0480 gloss:0.7930 dlossA:0.8423 dlossQ:0.9180 exploreP:0.5830\n",
      "Episode:188 meanR:34.1200 R:22.0000 rate:0.0440 gloss:0.7927 dlossA:0.8358 dlossQ:0.9356 exploreP:0.5818\n",
      "Episode:189 meanR:34.2100 R:35.0000 rate:0.0700 gloss:0.7758 dlossA:0.8343 dlossQ:0.8991 exploreP:0.5798\n",
      "Episode:190 meanR:34.5300 R:41.0000 rate:0.0820 gloss:0.7783 dlossA:0.8335 dlossQ:0.9038 exploreP:0.5774\n",
      "Episode:191 meanR:34.9600 R:83.0000 rate:0.1660 gloss:0.7949 dlossA:0.8395 dlossQ:0.9230 exploreP:0.5727\n",
      "Episode:192 meanR:35.4900 R:73.0000 rate:0.1460 gloss:0.7959 dlossA:0.8403 dlossQ:0.9268 exploreP:0.5686\n",
      "Episode:193 meanR:35.8700 R:55.0000 rate:0.1100 gloss:0.7874 dlossA:0.8361 dlossQ:0.9085 exploreP:0.5656\n",
      "Episode:194 meanR:35.7300 R:28.0000 rate:0.0560 gloss:0.8073 dlossA:0.8434 dlossQ:0.9106 exploreP:0.5640\n",
      "Episode:195 meanR:36.3400 R:78.0000 rate:0.1560 gloss:0.7952 dlossA:0.8371 dlossQ:0.9288 exploreP:0.5597\n",
      "Episode:196 meanR:36.7200 R:86.0000 rate:0.1720 gloss:0.7962 dlossA:0.8384 dlossQ:0.9293 exploreP:0.5550\n",
      "Episode:197 meanR:36.7400 R:29.0000 rate:0.0580 gloss:0.7885 dlossA:0.8371 dlossQ:0.9135 exploreP:0.5534\n",
      "Episode:198 meanR:37.3200 R:78.0000 rate:0.1560 gloss:0.7924 dlossA:0.8364 dlossQ:0.9291 exploreP:0.5492\n",
      "Episode:199 meanR:37.9900 R:94.0000 rate:0.1880 gloss:0.7908 dlossA:0.8365 dlossQ:0.9292 exploreP:0.5442\n",
      "Episode:200 meanR:38.6500 R:94.0000 rate:0.1880 gloss:0.7917 dlossA:0.8408 dlossQ:0.9266 exploreP:0.5392\n",
      "Episode:201 meanR:39.4000 R:90.0000 rate:0.1800 gloss:0.7977 dlossA:0.8393 dlossQ:0.9364 exploreP:0.5344\n",
      "Episode:202 meanR:39.9900 R:80.0000 rate:0.1600 gloss:0.7911 dlossA:0.8407 dlossQ:0.9282 exploreP:0.5302\n",
      "Episode:203 meanR:40.1800 R:63.0000 rate:0.1260 gloss:0.7968 dlossA:0.8386 dlossQ:0.9422 exploreP:0.5270\n",
      "Episode:204 meanR:40.7200 R:68.0000 rate:0.1360 gloss:0.7985 dlossA:0.8418 dlossQ:0.9422 exploreP:0.5235\n",
      "Episode:205 meanR:41.7400 R:123.0000 rate:0.2460 gloss:0.7975 dlossA:0.8387 dlossQ:0.9334 exploreP:0.5172\n",
      "Episode:206 meanR:41.7700 R:24.0000 rate:0.0480 gloss:0.7896 dlossA:0.8396 dlossQ:0.9196 exploreP:0.5160\n",
      "Episode:207 meanR:42.0600 R:62.0000 rate:0.1240 gloss:0.7999 dlossA:0.8411 dlossQ:0.9405 exploreP:0.5129\n",
      "Episode:208 meanR:42.1100 R:22.0000 rate:0.0440 gloss:0.7762 dlossA:0.8337 dlossQ:0.9058 exploreP:0.5118\n",
      "Episode:209 meanR:42.4900 R:84.0000 rate:0.1680 gloss:0.7971 dlossA:0.8357 dlossQ:0.9366 exploreP:0.5076\n",
      "Episode:210 meanR:42.7200 R:61.0000 rate:0.1220 gloss:0.7950 dlossA:0.8425 dlossQ:0.9197 exploreP:0.5045\n",
      "Episode:211 meanR:43.6800 R:112.0000 rate:0.2240 gloss:0.7891 dlossA:0.8355 dlossQ:0.9343 exploreP:0.4990\n",
      "Episode:212 meanR:44.1600 R:71.0000 rate:0.1420 gloss:0.7913 dlossA:0.8376 dlossQ:0.9213 exploreP:0.4956\n",
      "Episode:213 meanR:44.3100 R:35.0000 rate:0.0700 gloss:0.7902 dlossA:0.8362 dlossQ:0.9252 exploreP:0.4939\n",
      "Episode:214 meanR:44.5800 R:76.0000 rate:0.1520 gloss:0.7830 dlossA:0.8302 dlossQ:0.9321 exploreP:0.4902\n",
      "Episode:215 meanR:45.2600 R:80.0000 rate:0.1600 gloss:0.8121 dlossA:0.8417 dlossQ:0.9433 exploreP:0.4864\n",
      "Episode:216 meanR:46.4900 R:139.0000 rate:0.2780 gloss:0.7890 dlossA:0.8344 dlossQ:0.9249 exploreP:0.4798\n",
      "Episode:217 meanR:46.2600 R:16.0000 rate:0.0320 gloss:0.7968 dlossA:0.8415 dlossQ:0.9219 exploreP:0.4790\n",
      "Episode:218 meanR:46.5700 R:54.0000 rate:0.1080 gloss:0.7936 dlossA:0.8329 dlossQ:0.9432 exploreP:0.4765\n",
      "Episode:219 meanR:46.8700 R:52.0000 rate:0.1040 gloss:0.7862 dlossA:0.8311 dlossQ:0.9301 exploreP:0.4741\n",
      "Episode:220 meanR:47.1900 R:74.0000 rate:0.1480 gloss:0.7929 dlossA:0.8376 dlossQ:0.9394 exploreP:0.4707\n",
      "Episode:221 meanR:47.2700 R:36.0000 rate:0.0720 gloss:0.8025 dlossA:0.8424 dlossQ:0.9406 exploreP:0.4690\n",
      "Episode:222 meanR:47.6900 R:92.0000 rate:0.1840 gloss:0.8014 dlossA:0.8369 dlossQ:0.9308 exploreP:0.4648\n",
      "Episode:223 meanR:48.2100 R:90.0000 rate:0.1800 gloss:0.7940 dlossA:0.8357 dlossQ:0.9306 exploreP:0.4607\n",
      "Episode:224 meanR:48.8500 R:82.0000 rate:0.1640 gloss:0.7936 dlossA:0.8376 dlossQ:0.9183 exploreP:0.4571\n",
      "Episode:225 meanR:49.3800 R:81.0000 rate:0.1620 gloss:0.7949 dlossA:0.8415 dlossQ:0.9201 exploreP:0.4535\n",
      "Episode:226 meanR:49.5600 R:48.0000 rate:0.0960 gloss:0.7976 dlossA:0.8333 dlossQ:0.9379 exploreP:0.4513\n",
      "Episode:227 meanR:51.3000 R:193.0000 rate:0.3860 gloss:0.8044 dlossA:0.8399 dlossQ:0.9320 exploreP:0.4429\n",
      "Episode:228 meanR:52.1600 R:121.0000 rate:0.2420 gloss:0.8066 dlossA:0.8404 dlossQ:0.9449 exploreP:0.4377\n",
      "Episode:229 meanR:52.1700 R:24.0000 rate:0.0480 gloss:0.7903 dlossA:0.8339 dlossQ:0.9285 exploreP:0.4367\n",
      "Episode:230 meanR:52.3300 R:47.0000 rate:0.0940 gloss:0.8009 dlossA:0.8363 dlossQ:0.9370 exploreP:0.4347\n",
      "Episode:231 meanR:52.9900 R:93.0000 rate:0.1860 gloss:0.7950 dlossA:0.8343 dlossQ:0.9400 exploreP:0.4307\n",
      "Episode:232 meanR:53.1700 R:82.0000 rate:0.1640 gloss:0.7991 dlossA:0.8370 dlossQ:0.9504 exploreP:0.4273\n",
      "Episode:233 meanR:53.8500 R:81.0000 rate:0.1620 gloss:0.8079 dlossA:0.8395 dlossQ:0.9474 exploreP:0.4239\n",
      "Episode:234 meanR:54.4000 R:79.0000 rate:0.1580 gloss:0.7981 dlossA:0.8374 dlossQ:0.9320 exploreP:0.4207\n",
      "Episode:235 meanR:55.1200 R:86.0000 rate:0.1720 gloss:0.7961 dlossA:0.8352 dlossQ:0.9363 exploreP:0.4172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:236 meanR:56.0700 R:171.0000 rate:0.3420 gloss:0.8049 dlossA:0.8416 dlossQ:0.9441 exploreP:0.4103\n",
      "Episode:237 meanR:56.6600 R:86.0000 rate:0.1720 gloss:0.8057 dlossA:0.8399 dlossQ:0.9287 exploreP:0.4068\n",
      "Episode:238 meanR:57.7000 R:123.0000 rate:0.2460 gloss:0.7963 dlossA:0.8342 dlossQ:0.9359 exploreP:0.4020\n",
      "Episode:239 meanR:58.3500 R:81.0000 rate:0.1620 gloss:0.8179 dlossA:0.8433 dlossQ:0.9471 exploreP:0.3988\n",
      "Episode:240 meanR:59.4900 R:148.0000 rate:0.2960 gloss:0.8033 dlossA:0.8397 dlossQ:0.9342 exploreP:0.3931\n",
      "Episode:241 meanR:60.0900 R:82.0000 rate:0.1640 gloss:0.8072 dlossA:0.8372 dlossQ:0.9470 exploreP:0.3900\n",
      "Episode:242 meanR:61.2800 R:153.0000 rate:0.3060 gloss:0.8117 dlossA:0.8362 dlossQ:0.9485 exploreP:0.3842\n",
      "Episode:243 meanR:62.1100 R:104.0000 rate:0.2080 gloss:0.8009 dlossA:0.8375 dlossQ:0.9205 exploreP:0.3803\n",
      "Episode:244 meanR:62.5800 R:66.0000 rate:0.1320 gloss:0.7982 dlossA:0.8368 dlossQ:0.9285 exploreP:0.3779\n",
      "Episode:245 meanR:63.7700 R:134.0000 rate:0.2680 gloss:0.8096 dlossA:0.8416 dlossQ:0.9385 exploreP:0.3730\n",
      "Episode:246 meanR:64.2800 R:69.0000 rate:0.1380 gloss:0.8168 dlossA:0.8417 dlossQ:0.9443 exploreP:0.3705\n",
      "Episode:247 meanR:64.8700 R:74.0000 rate:0.1480 gloss:0.8078 dlossA:0.8341 dlossQ:0.9433 exploreP:0.3678\n",
      "Episode:248 meanR:65.5900 R:109.0000 rate:0.2180 gloss:0.8113 dlossA:0.8382 dlossQ:0.9438 exploreP:0.3640\n",
      "Episode:249 meanR:66.2000 R:80.0000 rate:0.1600 gloss:0.8069 dlossA:0.8392 dlossQ:0.9345 exploreP:0.3611\n",
      "Episode:250 meanR:68.0400 R:209.0000 rate:0.4180 gloss:0.8080 dlossA:0.8404 dlossQ:0.9405 exploreP:0.3539\n",
      "Episode:251 meanR:69.3800 R:176.0000 rate:0.3520 gloss:0.8122 dlossA:0.8388 dlossQ:0.9469 exploreP:0.3479\n",
      "Episode:252 meanR:69.9100 R:71.0000 rate:0.1420 gloss:0.8063 dlossA:0.8389 dlossQ:0.9333 exploreP:0.3455\n",
      "Episode:253 meanR:70.9800 R:123.0000 rate:0.2460 gloss:0.8130 dlossA:0.8411 dlossQ:0.9456 exploreP:0.3414\n",
      "Episode:254 meanR:72.2500 R:170.0000 rate:0.3400 gloss:0.8151 dlossA:0.8409 dlossQ:0.9512 exploreP:0.3358\n",
      "Episode:255 meanR:74.8800 R:293.0000 rate:0.5860 gloss:0.8167 dlossA:0.8426 dlossQ:0.9489 exploreP:0.3264\n",
      "Episode:256 meanR:75.6400 R:118.0000 rate:0.2360 gloss:0.8198 dlossA:0.8464 dlossQ:0.9492 exploreP:0.3227\n",
      "Episode:257 meanR:77.3000 R:182.0000 rate:0.3640 gloss:0.8353 dlossA:0.8454 dlossQ:0.9736 exploreP:0.3170\n",
      "Episode:258 meanR:78.2300 R:130.0000 rate:0.2600 gloss:0.8274 dlossA:0.8455 dlossQ:0.9509 exploreP:0.3131\n",
      "Episode:259 meanR:79.3600 R:155.0000 rate:0.3100 gloss:0.8244 dlossA:0.8442 dlossQ:0.9563 exploreP:0.3084\n",
      "Episode:260 meanR:79.9200 R:152.0000 rate:0.3040 gloss:0.8267 dlossA:0.8480 dlossQ:0.9546 exploreP:0.3039\n",
      "Episode:261 meanR:81.2700 R:147.0000 rate:0.2940 gloss:0.8336 dlossA:0.8492 dlossQ:0.9560 exploreP:0.2996\n",
      "Episode:262 meanR:85.0700 R:399.0000 rate:0.7980 gloss:0.8363 dlossA:0.8480 dlossQ:0.9601 exploreP:0.2883\n",
      "Episode:263 meanR:85.5400 R:77.0000 rate:0.1540 gloss:0.8261 dlossA:0.8517 dlossQ:0.9423 exploreP:0.2862\n",
      "Episode:264 meanR:87.7200 R:236.0000 rate:0.4720 gloss:0.8333 dlossA:0.8461 dlossQ:0.9521 exploreP:0.2797\n",
      "Episode:265 meanR:90.7300 R:323.0000 rate:0.6460 gloss:0.8395 dlossA:0.8490 dlossQ:0.9592 exploreP:0.2712\n",
      "Episode:266 meanR:92.9800 R:315.0000 rate:0.6300 gloss:0.8513 dlossA:0.8553 dlossQ:0.9562 exploreP:0.2631\n",
      "Episode:267 meanR:94.8100 R:296.0000 rate:0.5920 gloss:0.8586 dlossA:0.8586 dlossQ:0.9696 exploreP:0.2557\n",
      "Episode:268 meanR:95.5600 R:115.0000 rate:0.2300 gloss:0.8429 dlossA:0.8614 dlossQ:0.9363 exploreP:0.2529\n",
      "Episode:269 meanR:99.5500 R:442.0000 rate:0.8840 gloss:0.8600 dlossA:0.8616 dlossQ:0.9637 exploreP:0.2424\n",
      "Episode:270 meanR:100.7500 R:146.0000 rate:0.2920 gloss:0.8660 dlossA:0.8620 dlossQ:0.9622 exploreP:0.2390\n",
      "Episode:271 meanR:104.4200 R:452.0000 rate:0.9040 gloss:0.8697 dlossA:0.8663 dlossQ:0.9647 exploreP:0.2289\n",
      "Episode:272 meanR:108.2400 R:500.0000 rate:1.0000 gloss:0.8859 dlossA:0.8719 dlossQ:0.9743 exploreP:0.2182\n",
      "Episode:273 meanR:108.3100 R:77.0000 rate:0.1540 gloss:0.8831 dlossA:0.8598 dlossQ:0.9684 exploreP:0.2166\n",
      "Episode:274 meanR:112.7800 R:500.0000 rate:1.0000 gloss:0.8995 dlossA:0.8839 dlossQ:0.9755 exploreP:0.2065\n",
      "Episode:275 meanR:117.5700 R:500.0000 rate:1.0000 gloss:0.9086 dlossA:0.8864 dlossQ:0.9723 exploreP:0.1969\n",
      "Episode:276 meanR:121.1400 R:400.0000 rate:0.8000 gloss:0.9213 dlossA:0.8949 dlossQ:0.9722 exploreP:0.1896\n",
      "Episode:277 meanR:123.7700 R:309.0000 rate:0.6180 gloss:0.9373 dlossA:0.8960 dlossQ:0.9733 exploreP:0.1841\n",
      "Episode:278 meanR:128.1400 R:500.0000 rate:1.0000 gloss:0.9613 dlossA:0.9130 dlossQ:0.9833 exploreP:0.1757\n",
      "Episode:279 meanR:132.5700 R:457.0000 rate:0.9140 gloss:0.9667 dlossA:0.9192 dlossQ:0.9801 exploreP:0.1683\n",
      "Episode:280 meanR:137.2200 R:500.0000 rate:1.0000 gloss:0.9965 dlossA:0.9341 dlossQ:0.9889 exploreP:0.1605\n",
      "Episode:281 meanR:141.4400 R:500.0000 rate:1.0000 gloss:1.0112 dlossA:0.9533 dlossQ:0.9914 exploreP:0.1532\n",
      "Episode:282 meanR:144.9100 R:410.0000 rate:0.8200 gloss:1.0264 dlossA:0.9554 dlossQ:0.9936 exploreP:0.1474\n",
      "Episode:283 meanR:147.0400 R:281.0000 rate:0.5620 gloss:1.0433 dlossA:0.9693 dlossQ:0.9976 exploreP:0.1436\n",
      "Episode:284 meanR:151.3200 R:500.0000 rate:1.0000 gloss:1.0656 dlossA:0.9750 dlossQ:1.0005 exploreP:0.1371\n",
      "Episode:285 meanR:156.1500 R:500.0000 rate:1.0000 gloss:1.0946 dlossA:0.9962 dlossQ:1.0145 exploreP:0.1309\n",
      "Episode:286 meanR:160.0300 R:421.0000 rate:0.8420 gloss:1.1152 dlossA:1.0116 dlossQ:1.0088 exploreP:0.1259\n",
      "Episode:287 meanR:164.7900 R:500.0000 rate:1.0000 gloss:1.1427 dlossA:1.0281 dlossQ:1.0179 exploreP:0.1203\n",
      "Episode:288 meanR:169.5700 R:500.0000 rate:1.0000 gloss:1.1757 dlossA:1.0425 dlossQ:1.0273 exploreP:0.1149\n",
      "Episode:289 meanR:174.2200 R:500.0000 rate:1.0000 gloss:1.2150 dlossA:1.0717 dlossQ:1.0288 exploreP:0.1098\n",
      "Episode:290 meanR:178.8100 R:500.0000 rate:1.0000 gloss:1.2486 dlossA:1.1098 dlossQ:1.0283 exploreP:0.1049\n",
      "Episode:291 meanR:182.9800 R:500.0000 rate:1.0000 gloss:1.2840 dlossA:1.1129 dlossQ:1.0313 exploreP:0.1003\n",
      "Episode:292 meanR:184.4600 R:221.0000 rate:0.4420 gloss:1.3221 dlossA:1.1562 dlossQ:1.0475 exploreP:0.0983\n",
      "Episode:293 meanR:188.9100 R:500.0000 rate:1.0000 gloss:1.3375 dlossA:1.1566 dlossQ:1.0255 exploreP:0.0940\n",
      "Episode:294 meanR:193.6300 R:500.0000 rate:1.0000 gloss:1.3869 dlossA:1.2199 dlossQ:1.0270 exploreP:0.0899\n",
      "Episode:295 meanR:197.8500 R:500.0000 rate:1.0000 gloss:1.4249 dlossA:1.2021 dlossQ:1.0354 exploreP:0.0860\n",
      "Episode:296 meanR:201.9900 R:500.0000 rate:1.0000 gloss:1.4697 dlossA:1.2404 dlossQ:1.0362 exploreP:0.0823\n",
      "Episode:297 meanR:205.5400 R:384.0000 rate:0.7680 gloss:1.5238 dlossA:1.3009 dlossQ:1.0667 exploreP:0.0796\n",
      "Episode:298 meanR:209.7600 R:500.0000 rate:1.0000 gloss:1.5394 dlossA:1.3171 dlossQ:1.0350 exploreP:0.0762\n",
      "Episode:299 meanR:213.8200 R:500.0000 rate:1.0000 gloss:1.5967 dlossA:1.3748 dlossQ:1.0539 exploreP:0.0730\n",
      "Episode:300 meanR:217.8800 R:500.0000 rate:1.0000 gloss:1.6197 dlossA:1.4021 dlossQ:1.0420 exploreP:0.0699\n",
      "Episode:301 meanR:221.9800 R:500.0000 rate:1.0000 gloss:1.6489 dlossA:1.4080 dlossQ:1.0391 exploreP:0.0670\n",
      "Episode:302 meanR:226.1800 R:500.0000 rate:1.0000 gloss:1.6897 dlossA:1.4639 dlossQ:1.0438 exploreP:0.0642\n",
      "Episode:303 meanR:230.5500 R:500.0000 rate:1.0000 gloss:1.7187 dlossA:1.4598 dlossQ:1.0255 exploreP:0.0615\n",
      "Episode:304 meanR:234.8700 R:500.0000 rate:1.0000 gloss:1.7700 dlossA:1.5126 dlossQ:1.0452 exploreP:0.0590\n",
      "Episode:305 meanR:238.6400 R:500.0000 rate:1.0000 gloss:1.8120 dlossA:1.5404 dlossQ:1.0379 exploreP:0.0566\n",
      "Episode:306 meanR:241.0800 R:268.0000 rate:0.5360 gloss:1.8431 dlossA:1.5480 dlossQ:1.0448 exploreP:0.0554\n",
      "Episode:307 meanR:245.4600 R:500.0000 rate:1.0000 gloss:1.8777 dlossA:1.6092 dlossQ:1.0360 exploreP:0.0532\n",
      "Episode:308 meanR:250.2400 R:500.0000 rate:1.0000 gloss:1.9291 dlossA:1.5993 dlossQ:1.0567 exploreP:0.0511\n",
      "Episode:309 meanR:254.4000 R:500.0000 rate:1.0000 gloss:1.9649 dlossA:1.6877 dlossQ:1.0357 exploreP:0.0491\n",
      "Episode:310 meanR:258.7900 R:500.0000 rate:1.0000 gloss:1.9897 dlossA:1.6726 dlossQ:1.0254 exploreP:0.0472\n",
      "Episode:311 meanR:262.6700 R:500.0000 rate:1.0000 gloss:2.0737 dlossA:1.7711 dlossQ:1.0626 exploreP:0.0454\n",
      "Episode:312 meanR:266.9600 R:500.0000 rate:1.0000 gloss:2.1421 dlossA:1.7743 dlossQ:1.0708 exploreP:0.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:313 meanR:269.6800 R:307.0000 rate:0.6140 gloss:2.1660 dlossA:1.8474 dlossQ:1.0781 exploreP:0.0426\n",
      "Episode:314 meanR:273.9200 R:500.0000 rate:1.0000 gloss:2.2320 dlossA:1.8940 dlossQ:1.0881 exploreP:0.0410\n",
      "Episode:315 meanR:277.5100 R:439.0000 rate:0.8780 gloss:2.2815 dlossA:1.9802 dlossQ:1.1118 exploreP:0.0397\n",
      "Episode:316 meanR:280.4700 R:435.0000 rate:0.8700 gloss:2.2950 dlossA:1.9101 dlossQ:1.1148 exploreP:0.0384\n",
      "Episode:317 meanR:285.3100 R:500.0000 rate:1.0000 gloss:2.3758 dlossA:2.0814 dlossQ:1.1402 exploreP:0.0370\n",
      "Episode:318 meanR:288.5300 R:376.0000 rate:0.7520 gloss:2.4059 dlossA:2.0844 dlossQ:1.1110 exploreP:0.0360\n",
      "Episode:319 meanR:292.5500 R:454.0000 rate:0.9080 gloss:2.4087 dlossA:2.0396 dlossQ:1.0747 exploreP:0.0349\n",
      "Episode:320 meanR:296.8100 R:500.0000 rate:1.0000 gloss:2.4908 dlossA:2.1749 dlossQ:1.1349 exploreP:0.0337\n",
      "Episode:321 meanR:300.9000 R:445.0000 rate:0.8900 gloss:2.5551 dlossA:2.2839 dlossQ:1.1434 exploreP:0.0326\n",
      "Episode:322 meanR:304.9800 R:500.0000 rate:1.0000 gloss:2.5609 dlossA:2.2083 dlossQ:1.1210 exploreP:0.0315\n",
      "Episode:323 meanR:308.5800 R:450.0000 rate:0.9000 gloss:2.6517 dlossA:2.2936 dlossQ:1.1457 exploreP:0.0306\n",
      "Episode:324 meanR:312.7600 R:500.0000 rate:1.0000 gloss:2.6643 dlossA:2.2705 dlossQ:1.2209 exploreP:0.0296\n",
      "Episode:325 meanR:316.9500 R:500.0000 rate:1.0000 gloss:2.7214 dlossA:2.2103 dlossQ:1.1492 exploreP:0.0286\n",
      "Episode:326 meanR:321.4700 R:500.0000 rate:1.0000 gloss:2.8076 dlossA:2.3651 dlossQ:1.1376 exploreP:0.0277\n",
      "Episode:327 meanR:322.3900 R:285.0000 rate:0.5700 gloss:2.8943 dlossA:2.4465 dlossQ:1.1819 exploreP:0.0272\n",
      "Episode:328 meanR:326.1800 R:500.0000 rate:1.0000 gloss:2.9473 dlossA:2.5802 dlossQ:1.1626 exploreP:0.0264\n",
      "Episode:329 meanR:330.9400 R:500.0000 rate:1.0000 gloss:3.0302 dlossA:2.6043 dlossQ:1.2393 exploreP:0.0256\n",
      "Episode:330 meanR:335.4700 R:500.0000 rate:1.0000 gloss:3.1009 dlossA:2.6218 dlossQ:1.2174 exploreP:0.0248\n",
      "Episode:331 meanR:339.5400 R:500.0000 rate:1.0000 gloss:3.2277 dlossA:2.8589 dlossQ:1.2651 exploreP:0.0241\n",
      "Episode:332 meanR:343.7200 R:500.0000 rate:1.0000 gloss:3.2790 dlossA:2.8907 dlossQ:1.2750 exploreP:0.0234\n",
      "Episode:333 meanR:347.9100 R:500.0000 rate:1.0000 gloss:3.3702 dlossA:2.8456 dlossQ:1.2926 exploreP:0.0228\n",
      "Episode:334 meanR:352.1200 R:500.0000 rate:1.0000 gloss:3.4554 dlossA:2.9399 dlossQ:1.2907 exploreP:0.0221\n",
      "Episode:335 meanR:356.2600 R:500.0000 rate:1.0000 gloss:3.5597 dlossA:2.9519 dlossQ:1.3668 exploreP:0.0215\n",
      "Episode:336 meanR:359.1200 R:457.0000 rate:0.9140 gloss:3.6682 dlossA:3.1490 dlossQ:1.3641 exploreP:0.0210\n",
      "Episode:337 meanR:363.2600 R:500.0000 rate:1.0000 gloss:3.7121 dlossA:3.0919 dlossQ:1.2542 exploreP:0.0205\n",
      "Episode:338 meanR:367.0300 R:500.0000 rate:1.0000 gloss:3.8204 dlossA:3.1929 dlossQ:1.2756 exploreP:0.0200\n",
      "Episode:339 meanR:371.2200 R:500.0000 rate:1.0000 gloss:3.9938 dlossA:3.3743 dlossQ:1.3324 exploreP:0.0195\n",
      "Episode:340 meanR:374.7400 R:500.0000 rate:1.0000 gloss:4.1004 dlossA:3.3982 dlossQ:1.3619 exploreP:0.0190\n",
      "Episode:341 meanR:378.9200 R:500.0000 rate:1.0000 gloss:4.2670 dlossA:3.5667 dlossQ:1.4353 exploreP:0.0186\n",
      "Episode:342 meanR:382.3900 R:500.0000 rate:1.0000 gloss:4.4554 dlossA:3.8226 dlossQ:1.5740 exploreP:0.0182\n",
      "Episode:343 meanR:386.3500 R:500.0000 rate:1.0000 gloss:4.5847 dlossA:3.9092 dlossQ:1.5625 exploreP:0.0178\n",
      "Episode:344 meanR:390.6900 R:500.0000 rate:1.0000 gloss:4.6850 dlossA:3.9012 dlossQ:1.5742 exploreP:0.0174\n",
      "Episode:345 meanR:393.0600 R:371.0000 rate:0.7420 gloss:4.9636 dlossA:4.4675 dlossQ:1.6861 exploreP:0.0171\n",
      "Episode:346 meanR:397.3700 R:500.0000 rate:1.0000 gloss:4.9907 dlossA:4.2254 dlossQ:1.7464 exploreP:0.0168\n",
      "Episode:347 meanR:401.6300 R:500.0000 rate:1.0000 gloss:5.1782 dlossA:4.6074 dlossQ:2.0605 exploreP:0.0164\n",
      "Episode:348 meanR:405.5400 R:500.0000 rate:1.0000 gloss:5.1189 dlossA:4.2687 dlossQ:1.7071 exploreP:0.0161\n",
      "Episode:349 meanR:409.7400 R:500.0000 rate:1.0000 gloss:5.2320 dlossA:4.2965 dlossQ:1.8431 exploreP:0.0158\n",
      "Episode:350 meanR:412.6500 R:500.0000 rate:1.0000 gloss:5.3579 dlossA:4.4367 dlossQ:1.8053 exploreP:0.0156\n",
      "Episode:351 meanR:415.8900 R:500.0000 rate:1.0000 gloss:5.4773 dlossA:4.5252 dlossQ:1.8314 exploreP:0.0153\n",
      "Episode:352 meanR:420.1800 R:500.0000 rate:1.0000 gloss:5.7768 dlossA:5.0587 dlossQ:2.0205 exploreP:0.0150\n",
      "Episode:353 meanR:423.9500 R:500.0000 rate:1.0000 gloss:5.8562 dlossA:4.9307 dlossQ:2.1247 exploreP:0.0148\n",
      "Episode:354 meanR:427.2500 R:500.0000 rate:1.0000 gloss:5.7678 dlossA:4.5411 dlossQ:1.8025 exploreP:0.0145\n",
      "Episode:355 meanR:429.3200 R:500.0000 rate:1.0000 gloss:6.0317 dlossA:4.9416 dlossQ:1.8632 exploreP:0.0143\n",
      "Episode:356 meanR:433.1400 R:500.0000 rate:1.0000 gloss:6.1489 dlossA:4.7455 dlossQ:1.7774 exploreP:0.0141\n",
      "Episode:357 meanR:436.3200 R:500.0000 rate:1.0000 gloss:6.4821 dlossA:5.1386 dlossQ:2.0640 exploreP:0.0139\n",
      "Episode:358 meanR:440.0200 R:500.0000 rate:1.0000 gloss:6.8306 dlossA:5.5947 dlossQ:2.0761 exploreP:0.0137\n",
      "Episode:359 meanR:443.4700 R:500.0000 rate:1.0000 gloss:7.0275 dlossA:5.6303 dlossQ:2.0974 exploreP:0.0135\n",
      "Episode:360 meanR:446.9500 R:500.0000 rate:1.0000 gloss:7.2377 dlossA:5.6880 dlossQ:2.3828 exploreP:0.0134\n",
      "Episode:361 meanR:450.4800 R:500.0000 rate:1.0000 gloss:7.6993 dlossA:6.4278 dlossQ:2.3925 exploreP:0.0132\n",
      "Episode:362 meanR:451.4900 R:500.0000 rate:1.0000 gloss:8.0187 dlossA:6.5531 dlossQ:2.8813 exploreP:0.0130\n",
      "Episode:363 meanR:453.3000 R:258.0000 rate:0.5160 gloss:7.7300 dlossA:5.7373 dlossQ:2.3580 exploreP:0.0130\n",
      "Episode:364 meanR:455.9400 R:500.0000 rate:1.0000 gloss:8.2176 dlossA:7.0595 dlossQ:2.6054 exploreP:0.0128\n",
      "Episode:365 meanR:457.7100 R:500.0000 rate:1.0000 gloss:7.9119 dlossA:5.8123 dlossQ:2.4015 exploreP:0.0127\n",
      "Episode:366 meanR:457.6700 R:311.0000 rate:0.6220 gloss:8.4336 dlossA:6.4275 dlossQ:2.4609 exploreP:0.0126\n",
      "Episode:367 meanR:459.7100 R:500.0000 rate:1.0000 gloss:8.8074 dlossA:7.0190 dlossQ:2.9839 exploreP:0.0125\n",
      "Episode:368 meanR:463.5600 R:500.0000 rate:1.0000 gloss:8.9206 dlossA:6.8797 dlossQ:2.7740 exploreP:0.0124\n",
      "Episode:369 meanR:464.1400 R:500.0000 rate:1.0000 gloss:9.1261 dlossA:7.1028 dlossQ:2.8402 exploreP:0.0122\n",
      "Episode:370 meanR:467.6800 R:500.0000 rate:1.0000 gloss:9.5587 dlossA:7.5173 dlossQ:3.0652 exploreP:0.0121\n",
      "Episode:371 meanR:468.1600 R:500.0000 rate:1.0000 gloss:9.7564 dlossA:7.4266 dlossQ:3.3898 exploreP:0.0120\n",
      "Episode:372 meanR:468.1600 R:500.0000 rate:1.0000 gloss:10.2068 dlossA:7.9125 dlossQ:3.2483 exploreP:0.0119\n",
      "Episode:373 meanR:472.3900 R:500.0000 rate:1.0000 gloss:10.4764 dlossA:7.9746 dlossQ:3.7007 exploreP:0.0118\n",
      "Episode:374 meanR:472.3900 R:500.0000 rate:1.0000 gloss:10.5714 dlossA:7.9001 dlossQ:3.7214 exploreP:0.0117\n",
      "Episode:375 meanR:472.3900 R:500.0000 rate:1.0000 gloss:11.0427 dlossA:8.5094 dlossQ:4.3284 exploreP:0.0117\n",
      "Episode:376 meanR:473.3900 R:500.0000 rate:1.0000 gloss:11.0962 dlossA:8.4751 dlossQ:4.4089 exploreP:0.0116\n",
      "Episode:377 meanR:475.3000 R:500.0000 rate:1.0000 gloss:11.0059 dlossA:8.1875 dlossQ:4.3125 exploreP:0.0115\n",
      "Episode:378 meanR:475.3000 R:500.0000 rate:1.0000 gloss:11.1612 dlossA:8.5702 dlossQ:4.4839 exploreP:0.0114\n",
      "Episode:379 meanR:475.7300 R:500.0000 rate:1.0000 gloss:11.0848 dlossA:8.1374 dlossQ:4.7250 exploreP:0.0114\n",
      "Episode:380 meanR:475.7300 R:500.0000 rate:1.0000 gloss:11.1040 dlossA:8.0529 dlossQ:4.8216 exploreP:0.0113\n",
      "Episode:381 meanR:475.7300 R:500.0000 rate:1.0000 gloss:10.6858 dlossA:7.3312 dlossQ:5.8963 exploreP:0.0112\n",
      "Episode:382 meanR:476.6300 R:500.0000 rate:1.0000 gloss:10.6436 dlossA:7.4670 dlossQ:4.5913 exploreP:0.0112\n",
      "Episode:383 meanR:478.8200 R:500.0000 rate:1.0000 gloss:10.8507 dlossA:7.5723 dlossQ:5.0591 exploreP:0.0111\n",
      "Episode:384 meanR:478.8200 R:500.0000 rate:1.0000 gloss:10.8126 dlossA:7.4927 dlossQ:4.9603 exploreP:0.0111\n",
      "Episode:385 meanR:478.8200 R:500.0000 rate:1.0000 gloss:11.0916 dlossA:7.7251 dlossQ:5.3487 exploreP:0.0110\n",
      "Episode:386 meanR:479.6100 R:500.0000 rate:1.0000 gloss:11.3734 dlossA:8.1521 dlossQ:5.4886 exploreP:0.0110\n",
      "Episode:387 meanR:479.6100 R:500.0000 rate:1.0000 gloss:11.0718 dlossA:7.5907 dlossQ:6.6492 exploreP:0.0109\n",
      "Episode:388 meanR:479.6100 R:500.0000 rate:1.0000 gloss:10.5900 dlossA:7.1127 dlossQ:5.5149 exploreP:0.0109\n",
      "Episode:389 meanR:479.6100 R:500.0000 rate:1.0000 gloss:10.8390 dlossA:7.0962 dlossQ:5.5101 exploreP:0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:390 meanR:479.6100 R:500.0000 rate:1.0000 gloss:10.9712 dlossA:7.3724 dlossQ:5.9958 exploreP:0.0108\n",
      "Episode:391 meanR:479.6100 R:500.0000 rate:1.0000 gloss:11.1476 dlossA:7.0602 dlossQ:6.3019 exploreP:0.0107\n",
      "Episode:392 meanR:482.4000 R:500.0000 rate:1.0000 gloss:11.2794 dlossA:7.6495 dlossQ:6.3674 exploreP:0.0107\n",
      "Episode:393 meanR:482.4000 R:500.0000 rate:1.0000 gloss:11.1646 dlossA:7.1444 dlossQ:6.2138 exploreP:0.0107\n",
      "Episode:394 meanR:482.4000 R:500.0000 rate:1.0000 gloss:11.5273 dlossA:7.5822 dlossQ:6.2722 exploreP:0.0106\n",
      "Episode:395 meanR:482.4000 R:500.0000 rate:1.0000 gloss:11.8812 dlossA:7.9329 dlossQ:7.2163 exploreP:0.0106\n",
      "Episode:396 meanR:481.0800 R:368.0000 rate:0.7360 gloss:11.8701 dlossA:8.1490 dlossQ:7.1873 exploreP:0.0106\n",
      "Episode:397 meanR:482.2400 R:500.0000 rate:1.0000 gloss:11.7318 dlossA:7.1927 dlossQ:6.6994 exploreP:0.0106\n",
      "Episode:398 meanR:482.2400 R:500.0000 rate:1.0000 gloss:11.9148 dlossA:7.0516 dlossQ:6.9687 exploreP:0.0105\n",
      "Episode:399 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.7370 dlossA:7.8877 dlossQ:8.3288 exploreP:0.0105\n",
      "Episode:400 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.4032 dlossA:7.5965 dlossQ:8.4683 exploreP:0.0105\n",
      "Episode:401 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.6824 dlossA:7.9976 dlossQ:8.1554 exploreP:0.0105\n",
      "Episode:402 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.6553 dlossA:8.0641 dlossQ:8.4701 exploreP:0.0104\n",
      "Episode:403 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.3248 dlossA:7.5546 dlossQ:8.7569 exploreP:0.0104\n",
      "Episode:404 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.3027 dlossA:7.7311 dlossQ:8.9447 exploreP:0.0104\n",
      "Episode:405 meanR:482.2400 R:500.0000 rate:1.0000 gloss:12.2072 dlossA:7.7587 dlossQ:9.5016 exploreP:0.0104\n",
      "Episode:406 meanR:484.5600 R:500.0000 rate:1.0000 gloss:11.6965 dlossA:6.8796 dlossQ:9.1701 exploreP:0.0104\n",
      "Episode:407 meanR:484.5600 R:500.0000 rate:1.0000 gloss:11.7130 dlossA:6.9380 dlossQ:9.4543 exploreP:0.0103\n",
      "Episode:408 meanR:484.5600 R:500.0000 rate:1.0000 gloss:11.3084 dlossA:6.5039 dlossQ:8.5241 exploreP:0.0103\n",
      "Episode:409 meanR:482.7400 R:318.0000 rate:0.6360 gloss:11.5067 dlossA:6.1965 dlossQ:9.4648 exploreP:0.0103\n",
      "Episode:410 meanR:482.7400 R:500.0000 rate:1.0000 gloss:11.2530 dlossA:5.8822 dlossQ:8.7903 exploreP:0.0103\n",
      "Episode:411 meanR:482.7400 R:500.0000 rate:1.0000 gloss:11.9341 dlossA:6.7003 dlossQ:9.2797 exploreP:0.0103\n",
      "Episode:412 meanR:482.7400 R:500.0000 rate:1.0000 gloss:11.6182 dlossA:6.4803 dlossQ:10.3649 exploreP:0.0103\n",
      "Episode:413 meanR:484.6700 R:500.0000 rate:1.0000 gloss:11.2747 dlossA:6.1127 dlossQ:9.3566 exploreP:0.0103\n",
      "Episode:414 meanR:484.6700 R:500.0000 rate:1.0000 gloss:11.2702 dlossA:6.0672 dlossQ:9.9100 exploreP:0.0102\n",
      "Episode:415 meanR:485.2800 R:500.0000 rate:1.0000 gloss:10.8434 dlossA:5.6316 dlossQ:9.5527 exploreP:0.0102\n",
      "Episode:416 meanR:485.9300 R:500.0000 rate:1.0000 gloss:11.2121 dlossA:6.1261 dlossQ:10.1018 exploreP:0.0102\n",
      "Episode:417 meanR:485.9300 R:500.0000 rate:1.0000 gloss:11.0306 dlossA:5.3225 dlossQ:9.1817 exploreP:0.0102\n",
      "Episode:418 meanR:487.1700 R:500.0000 rate:1.0000 gloss:11.7382 dlossA:6.0450 dlossQ:10.5015 exploreP:0.0102\n",
      "Episode:419 meanR:487.6300 R:500.0000 rate:1.0000 gloss:12.6538 dlossA:6.7486 dlossQ:10.8221 exploreP:0.0102\n",
      "Episode:420 meanR:487.6300 R:500.0000 rate:1.0000 gloss:12.2523 dlossA:6.3320 dlossQ:11.1386 exploreP:0.0102\n",
      "Episode:421 meanR:488.1800 R:500.0000 rate:1.0000 gloss:12.8438 dlossA:6.9964 dlossQ:11.4789 exploreP:0.0102\n",
      "Episode:422 meanR:488.1800 R:500.0000 rate:1.0000 gloss:12.4242 dlossA:6.0004 dlossQ:11.9926 exploreP:0.0102\n",
      "Episode:423 meanR:488.6800 R:500.0000 rate:1.0000 gloss:12.9649 dlossA:6.1386 dlossQ:11.5460 exploreP:0.0102\n",
      "Episode:424 meanR:488.6800 R:500.0000 rate:1.0000 gloss:12.8995 dlossA:6.2678 dlossQ:14.4484 exploreP:0.0101\n",
      "Episode:425 meanR:488.6800 R:500.0000 rate:1.0000 gloss:12.4222 dlossA:5.9214 dlossQ:11.4652 exploreP:0.0101\n",
      "Episode:426 meanR:488.6800 R:500.0000 rate:1.0000 gloss:11.6050 dlossA:5.0359 dlossQ:12.1530 exploreP:0.0101\n",
      "Episode:427 meanR:490.8300 R:500.0000 rate:1.0000 gloss:12.0746 dlossA:5.7124 dlossQ:11.8688 exploreP:0.0101\n",
      "Episode:428 meanR:490.8300 R:500.0000 rate:1.0000 gloss:11.5341 dlossA:5.6200 dlossQ:13.7989 exploreP:0.0101\n",
      "Episode:429 meanR:490.8300 R:500.0000 rate:1.0000 gloss:11.2358 dlossA:5.1633 dlossQ:11.4576 exploreP:0.0101\n",
      "Episode:430 meanR:490.8300 R:500.0000 rate:1.0000 gloss:10.5263 dlossA:4.2050 dlossQ:11.9004 exploreP:0.0101\n",
      "Episode:431 meanR:490.8300 R:500.0000 rate:1.0000 gloss:11.1274 dlossA:4.5224 dlossQ:11.6949 exploreP:0.0101\n",
      "Episode:432 meanR:490.8300 R:500.0000 rate:1.0000 gloss:11.5000 dlossA:4.8215 dlossQ:12.2507 exploreP:0.0101\n",
      "Episode:433 meanR:490.8300 R:500.0000 rate:1.0000 gloss:11.3594 dlossA:4.2259 dlossQ:11.7099 exploreP:0.0101\n",
      "Episode:434 meanR:490.8300 R:500.0000 rate:1.0000 gloss:12.7379 dlossA:6.3925 dlossQ:14.4389 exploreP:0.0101\n",
      "Episode:435 meanR:490.8300 R:500.0000 rate:1.0000 gloss:12.1736 dlossA:5.0197 dlossQ:12.9638 exploreP:0.0101\n",
      "Episode:436 meanR:491.2600 R:500.0000 rate:1.0000 gloss:12.8543 dlossA:5.5481 dlossQ:14.7887 exploreP:0.0101\n",
      "Episode:437 meanR:491.2600 R:500.0000 rate:1.0000 gloss:11.9407 dlossA:4.1595 dlossQ:13.5955 exploreP:0.0101\n",
      "Episode:438 meanR:491.0800 R:482.0000 rate:0.9640 gloss:12.2835 dlossA:4.4513 dlossQ:14.2737 exploreP:0.0101\n",
      "Episode:439 meanR:491.0800 R:500.0000 rate:1.0000 gloss:13.6388 dlossA:5.6616 dlossQ:15.4390 exploreP:0.0101\n",
      "Episode:440 meanR:491.0800 R:500.0000 rate:1.0000 gloss:13.5089 dlossA:5.2289 dlossQ:16.8539 exploreP:0.0101\n",
      "Episode:441 meanR:491.0800 R:500.0000 rate:1.0000 gloss:13.9160 dlossA:4.8851 dlossQ:15.8463 exploreP:0.0101\n",
      "Episode:442 meanR:491.0800 R:500.0000 rate:1.0000 gloss:14.2083 dlossA:5.8666 dlossQ:16.8543 exploreP:0.0101\n",
      "Episode:443 meanR:491.0800 R:500.0000 rate:1.0000 gloss:13.8574 dlossA:4.5920 dlossQ:15.3545 exploreP:0.0101\n",
      "Episode:444 meanR:491.0800 R:500.0000 rate:1.0000 gloss:13.7734 dlossA:4.5413 dlossQ:16.4945 exploreP:0.0101\n",
      "Episode:445 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.7879 dlossA:4.2721 dlossQ:16.0833 exploreP:0.0101\n",
      "Episode:446 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.4553 dlossA:4.0356 dlossQ:17.3208 exploreP:0.0100\n",
      "Episode:447 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.8797 dlossA:4.5363 dlossQ:16.4043 exploreP:0.0100\n",
      "Episode:448 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.6219 dlossA:4.3299 dlossQ:17.9706 exploreP:0.0100\n",
      "Episode:449 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.6410 dlossA:4.1871 dlossQ:15.9598 exploreP:0.0100\n",
      "Episode:450 meanR:492.3700 R:500.0000 rate:1.0000 gloss:12.5142 dlossA:3.3788 dlossQ:16.7580 exploreP:0.0100\n",
      "Episode:451 meanR:492.3700 R:500.0000 rate:1.0000 gloss:12.6376 dlossA:3.3550 dlossQ:15.6328 exploreP:0.0100\n",
      "Episode:452 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.9266 dlossA:4.4073 dlossQ:16.2074 exploreP:0.0100\n",
      "Episode:453 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.0420 dlossA:3.2402 dlossQ:16.2558 exploreP:0.0100\n",
      "Episode:454 meanR:492.3700 R:500.0000 rate:1.0000 gloss:13.7785 dlossA:3.2920 dlossQ:17.4563 exploreP:0.0100\n",
      "Episode:455 meanR:492.3700 R:500.0000 rate:1.0000 gloss:14.5556 dlossA:3.3410 dlossQ:17.5507 exploreP:0.0100\n",
      "Episode:456 meanR:492.3700 R:500.0000 rate:1.0000 gloss:14.7847 dlossA:3.0624 dlossQ:18.6917 exploreP:0.0100\n",
      "Episode:457 meanR:491.7500 R:438.0000 rate:0.8760 gloss:14.9362 dlossA:3.4793 dlossQ:18.5448 exploreP:0.0100\n",
      "Episode:458 meanR:491.7500 R:500.0000 rate:1.0000 gloss:15.6559 dlossA:3.5228 dlossQ:20.8633 exploreP:0.0100\n",
      "Episode:459 meanR:491.7500 R:500.0000 rate:1.0000 gloss:14.7983 dlossA:2.7914 dlossQ:20.3754 exploreP:0.0100\n",
      "Episode:460 meanR:491.7500 R:500.0000 rate:1.0000 gloss:14.7902 dlossA:3.0893 dlossQ:24.1397 exploreP:0.0100\n",
      "Episode:461 meanR:491.5900 R:484.0000 rate:0.9680 gloss:14.0805 dlossA:2.1563 dlossQ:18.4908 exploreP:0.0100\n",
      "Episode:462 meanR:491.5900 R:500.0000 rate:1.0000 gloss:13.3141 dlossA:1.7519 dlossQ:18.8630 exploreP:0.0100\n",
      "Episode:463 meanR:494.0100 R:500.0000 rate:1.0000 gloss:13.8900 dlossA:2.2107 dlossQ:17.5269 exploreP:0.0100\n",
      "Episode:464 meanR:494.0100 R:500.0000 rate:1.0000 gloss:13.6882 dlossA:2.4669 dlossQ:19.1022 exploreP:0.0100\n",
      "Episode:465 meanR:494.0100 R:500.0000 rate:1.0000 gloss:12.9438 dlossA:1.6924 dlossQ:17.9246 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:466 meanR:495.9000 R:500.0000 rate:1.0000 gloss:12.8966 dlossA:1.6538 dlossQ:18.6030 exploreP:0.0100\n",
      "Episode:467 meanR:495.9000 R:500.0000 rate:1.0000 gloss:13.1633 dlossA:1.1739 dlossQ:18.4821 exploreP:0.0100\n",
      "Episode:468 meanR:495.9000 R:500.0000 rate:1.0000 gloss:13.5419 dlossA:1.0809 dlossQ:20.9079 exploreP:0.0100\n",
      "Episode:469 meanR:495.9000 R:500.0000 rate:1.0000 gloss:14.1367 dlossA:0.7915 dlossQ:21.2342 exploreP:0.0100\n",
      "Episode:470 meanR:495.9000 R:500.0000 rate:1.0000 gloss:14.4994 dlossA:0.7421 dlossQ:21.8392 exploreP:0.0100\n",
      "Episode:471 meanR:495.9000 R:500.0000 rate:1.0000 gloss:14.2920 dlossA:0.9754 dlossQ:21.7188 exploreP:0.0100\n",
      "Episode:472 meanR:495.9000 R:500.0000 rate:1.0000 gloss:13.8289 dlossA:0.6224 dlossQ:21.6232 exploreP:0.0100\n",
      "Episode:473 meanR:495.5800 R:468.0000 rate:0.9360 gloss:13.1203 dlossA:0.4730 dlossQ:19.0900 exploreP:0.0100\n",
      "Episode:474 meanR:495.5800 R:500.0000 rate:1.0000 gloss:12.3819 dlossA:0.5355 dlossQ:17.9817 exploreP:0.0100\n",
      "Episode:475 meanR:495.5800 R:500.0000 rate:1.0000 gloss:12.5751 dlossA:0.4425 dlossQ:16.7587 exploreP:0.0100\n",
      "Episode:476 meanR:495.5800 R:500.0000 rate:1.0000 gloss:11.2256 dlossA:0.2493 dlossQ:15.9552 exploreP:0.0100\n",
      "Episode:477 meanR:495.5800 R:500.0000 rate:1.0000 gloss:12.4581 dlossA:0.3138 dlossQ:16.1052 exploreP:0.0100\n",
      "Episode:478 meanR:495.5800 R:500.0000 rate:1.0000 gloss:12.1239 dlossA:0.2319 dlossQ:16.3195 exploreP:0.0100\n",
      "Episode:479 meanR:495.5800 R:500.0000 rate:1.0000 gloss:11.6296 dlossA:0.1628 dlossQ:15.4591 exploreP:0.0100\n",
      "Episode:480 meanR:495.5800 R:500.0000 rate:1.0000 gloss:11.6565 dlossA:0.2560 dlossQ:15.6340 exploreP:0.0100\n",
      "Episode:481 meanR:494.9000 R:432.0000 rate:0.8640 gloss:11.0615 dlossA:0.1811 dlossQ:14.3648 exploreP:0.0100\n",
      "Episode:482 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.8511 dlossA:0.1575 dlossQ:14.0329 exploreP:0.0100\n",
      "Episode:483 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.8949 dlossA:0.1607 dlossQ:14.5867 exploreP:0.0100\n",
      "Episode:484 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.8519 dlossA:0.2439 dlossQ:14.0348 exploreP:0.0100\n",
      "Episode:485 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.6393 dlossA:0.1295 dlossQ:13.5368 exploreP:0.0100\n",
      "Episode:486 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.8403 dlossA:0.0935 dlossQ:14.5866 exploreP:0.0100\n",
      "Episode:487 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.0932 dlossA:0.1786 dlossQ:13.3773 exploreP:0.0100\n",
      "Episode:488 meanR:494.9000 R:500.0000 rate:1.0000 gloss:9.9272 dlossA:0.2019 dlossQ:12.5708 exploreP:0.0100\n",
      "Episode:489 meanR:494.9000 R:500.0000 rate:1.0000 gloss:10.1088 dlossA:0.1314 dlossQ:12.7148 exploreP:0.0100\n",
      "Episode:490 meanR:494.9000 R:500.0000 rate:1.0000 gloss:9.9571 dlossA:0.1782 dlossQ:13.5141 exploreP:0.0100\n",
      "Episode:491 meanR:494.9000 R:500.0000 rate:1.0000 gloss:8.7318 dlossA:0.2049 dlossQ:11.5041 exploreP:0.0100\n",
      "Episode:492 meanR:494.9000 R:500.0000 rate:1.0000 gloss:9.3381 dlossA:0.1362 dlossQ:11.9061 exploreP:0.0100\n",
      "Episode:493 meanR:494.9000 R:500.0000 rate:1.0000 gloss:9.0991 dlossA:0.1637 dlossQ:12.5407 exploreP:0.0100\n",
      "Episode:494 meanR:494.9000 R:500.0000 rate:1.0000 gloss:9.9414 dlossA:0.1876 dlossQ:12.2774 exploreP:0.0100\n",
      "Episode:495 meanR:494.9000 R:500.0000 rate:1.0000 gloss:8.9401 dlossA:0.1299 dlossQ:11.5925 exploreP:0.0100\n",
      "Episode:496 meanR:496.2200 R:500.0000 rate:1.0000 gloss:9.0261 dlossA:0.1236 dlossQ:11.8139 exploreP:0.0100\n",
      "Episode:497 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.5996 dlossA:0.1222 dlossQ:11.6776 exploreP:0.0100\n",
      "Episode:498 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.7406 dlossA:0.2296 dlossQ:10.8004 exploreP:0.0100\n",
      "Episode:499 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.0819 dlossA:0.1362 dlossQ:10.7516 exploreP:0.0100\n",
      "Episode:500 meanR:496.2200 R:500.0000 rate:1.0000 gloss:9.0331 dlossA:0.1557 dlossQ:11.8721 exploreP:0.0100\n",
      "Episode:501 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.3316 dlossA:0.1856 dlossQ:10.7059 exploreP:0.0100\n",
      "Episode:502 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.2313 dlossA:0.1649 dlossQ:10.4781 exploreP:0.0100\n",
      "Episode:503 meanR:496.2200 R:500.0000 rate:1.0000 gloss:8.0370 dlossA:0.1843 dlossQ:10.2620 exploreP:0.0100\n",
      "Episode:504 meanR:496.2200 R:500.0000 rate:1.0000 gloss:7.8236 dlossA:0.1609 dlossQ:10.2119 exploreP:0.0100\n",
      "Episode:505 meanR:495.7700 R:455.0000 rate:0.9100 gloss:7.6738 dlossA:0.0981 dlossQ:10.7574 exploreP:0.0100\n",
      "Episode:506 meanR:495.7700 R:500.0000 rate:1.0000 gloss:8.1167 dlossA:0.1947 dlossQ:10.3790 exploreP:0.0100\n",
      "Episode:507 meanR:495.7700 R:500.0000 rate:1.0000 gloss:7.1083 dlossA:0.1850 dlossQ:8.9740 exploreP:0.0100\n",
      "Episode:508 meanR:495.7700 R:500.0000 rate:1.0000 gloss:7.5574 dlossA:0.1675 dlossQ:10.1438 exploreP:0.0100\n",
      "Episode:509 meanR:497.5900 R:500.0000 rate:1.0000 gloss:7.2436 dlossA:0.1426 dlossQ:9.7823 exploreP:0.0100\n",
      "Episode:510 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.9019 dlossA:0.1650 dlossQ:9.6962 exploreP:0.0100\n",
      "Episode:511 meanR:497.5900 R:500.0000 rate:1.0000 gloss:7.0802 dlossA:0.1293 dlossQ:10.2588 exploreP:0.0100\n",
      "Episode:512 meanR:497.5900 R:500.0000 rate:1.0000 gloss:7.1718 dlossA:0.1524 dlossQ:10.1826 exploreP:0.0100\n",
      "Episode:513 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.5843 dlossA:0.1463 dlossQ:9.3081 exploreP:0.0100\n",
      "Episode:514 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.9658 dlossA:0.1899 dlossQ:9.5453 exploreP:0.0100\n",
      "Episode:515 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.4835 dlossA:0.1328 dlossQ:9.3594 exploreP:0.0100\n",
      "Episode:516 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.6661 dlossA:0.1599 dlossQ:9.6736 exploreP:0.0100\n",
      "Episode:517 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.1542 dlossA:0.1506 dlossQ:9.4673 exploreP:0.0100\n",
      "Episode:518 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.3989 dlossA:0.1908 dlossQ:9.1947 exploreP:0.0100\n",
      "Episode:519 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.2641 dlossA:0.1093 dlossQ:9.4628 exploreP:0.0100\n",
      "Episode:520 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.6732 dlossA:0.1479 dlossQ:9.6292 exploreP:0.0100\n",
      "Episode:521 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.9279 dlossA:0.1197 dlossQ:9.1611 exploreP:0.0100\n",
      "Episode:522 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.0650 dlossA:0.1074 dlossQ:9.8999 exploreP:0.0100\n",
      "Episode:523 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.2301 dlossA:0.0766 dlossQ:10.1921 exploreP:0.0100\n",
      "Episode:524 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.8295 dlossA:0.1427 dlossQ:9.4032 exploreP:0.0100\n",
      "Episode:525 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.5326 dlossA:0.0888 dlossQ:8.7710 exploreP:0.0100\n",
      "Episode:526 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.6831 dlossA:0.0792 dlossQ:9.6851 exploreP:0.0100\n",
      "Episode:527 meanR:497.5900 R:500.0000 rate:1.0000 gloss:6.0259 dlossA:0.0671 dlossQ:10.4699 exploreP:0.0100\n",
      "Episode:528 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.7393 dlossA:0.0932 dlossQ:9.1983 exploreP:0.0100\n",
      "Episode:529 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.6290 dlossA:0.0794 dlossQ:8.8630 exploreP:0.0100\n",
      "Episode:530 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.5537 dlossA:0.0982 dlossQ:8.6185 exploreP:0.0100\n",
      "Episode:531 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.3602 dlossA:0.0714 dlossQ:8.7472 exploreP:0.0100\n",
      "Episode:532 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.5750 dlossA:0.0938 dlossQ:8.9473 exploreP:0.0100\n",
      "Episode:533 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.1585 dlossA:0.0638 dlossQ:8.8431 exploreP:0.0100\n",
      "Episode:534 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.4747 dlossA:0.0752 dlossQ:9.5828 exploreP:0.0100\n",
      "Episode:535 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.3239 dlossA:0.1033 dlossQ:8.5350 exploreP:0.0100\n",
      "Episode:536 meanR:497.5900 R:500.0000 rate:1.0000 gloss:4.9980 dlossA:0.0671 dlossQ:8.3855 exploreP:0.0100\n",
      "Episode:537 meanR:497.5900 R:500.0000 rate:1.0000 gloss:5.3525 dlossA:0.0792 dlossQ:9.7110 exploreP:0.0100\n",
      "Episode:538 meanR:497.7700 R:500.0000 rate:1.0000 gloss:5.2787 dlossA:0.0553 dlossQ:9.1593 exploreP:0.0100\n",
      "Episode:539 meanR:497.0100 R:424.0000 rate:0.8480 gloss:5.1522 dlossA:0.0737 dlossQ:8.6706 exploreP:0.0100\n",
      "Episode:540 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.8397 dlossA:0.0713 dlossQ:8.5784 exploreP:0.0100\n",
      "Episode:541 meanR:497.0100 R:500.0000 rate:1.0000 gloss:5.2605 dlossA:0.0449 dlossQ:9.2149 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:542 meanR:497.0100 R:500.0000 rate:1.0000 gloss:5.0905 dlossA:0.0599 dlossQ:9.1899 exploreP:0.0100\n",
      "Episode:543 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.9998 dlossA:0.0715 dlossQ:8.3337 exploreP:0.0100\n",
      "Episode:544 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.9518 dlossA:0.0604 dlossQ:8.6622 exploreP:0.0100\n",
      "Episode:545 meanR:497.0100 R:500.0000 rate:1.0000 gloss:5.1528 dlossA:0.0645 dlossQ:9.4601 exploreP:0.0100\n",
      "Episode:546 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.9596 dlossA:0.0519 dlossQ:8.8906 exploreP:0.0100\n",
      "Episode:547 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.9506 dlossA:0.0782 dlossQ:8.8076 exploreP:0.0100\n",
      "Episode:548 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.9114 dlossA:0.0458 dlossQ:9.0742 exploreP:0.0100\n",
      "Episode:549 meanR:497.0100 R:500.0000 rate:1.0000 gloss:5.1609 dlossA:0.0607 dlossQ:9.4502 exploreP:0.0100\n",
      "Episode:550 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.6140 dlossA:0.0617 dlossQ:8.5802 exploreP:0.0100\n",
      "Episode:551 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.6502 dlossA:0.0762 dlossQ:9.0052 exploreP:0.0100\n",
      "Episode:552 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.7491 dlossA:0.0773 dlossQ:8.8412 exploreP:0.0100\n",
      "Episode:553 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.6048 dlossA:0.0487 dlossQ:9.1248 exploreP:0.0100\n",
      "Episode:554 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.5639 dlossA:0.0935 dlossQ:8.7279 exploreP:0.0100\n",
      "Episode:555 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.5036 dlossA:0.0576 dlossQ:8.7353 exploreP:0.0100\n",
      "Episode:556 meanR:497.0100 R:500.0000 rate:1.0000 gloss:4.5611 dlossA:0.0737 dlossQ:9.2273 exploreP:0.0100\n",
      "Episode:557 meanR:497.6300 R:500.0000 rate:1.0000 gloss:4.5394 dlossA:0.0653 dlossQ:8.9985 exploreP:0.0100\n",
      "Episode:558 meanR:497.6300 R:500.0000 rate:1.0000 gloss:4.5249 dlossA:0.0699 dlossQ:9.2180 exploreP:0.0100\n",
      "Episode:559 meanR:497.6300 R:500.0000 rate:1.0000 gloss:4.5539 dlossA:0.0327 dlossQ:9.4312 exploreP:0.0100\n",
      "Episode:560 meanR:497.6300 R:500.0000 rate:1.0000 gloss:4.6797 dlossA:0.0622 dlossQ:9.4005 exploreP:0.0100\n",
      "Episode:561 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3598 dlossA:0.0724 dlossQ:9.1603 exploreP:0.0100\n",
      "Episode:562 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3732 dlossA:0.0828 dlossQ:8.8876 exploreP:0.0100\n",
      "Episode:563 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3914 dlossA:0.0418 dlossQ:9.2948 exploreP:0.0100\n",
      "Episode:564 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.5987 dlossA:0.0675 dlossQ:9.5430 exploreP:0.0100\n",
      "Episode:565 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.5043 dlossA:0.0630 dlossQ:9.4984 exploreP:0.0100\n",
      "Episode:566 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3903 dlossA:0.0882 dlossQ:9.3467 exploreP:0.0100\n",
      "Episode:567 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3446 dlossA:0.0392 dlossQ:9.3523 exploreP:0.0100\n",
      "Episode:568 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.4897 dlossA:0.0679 dlossQ:9.9373 exploreP:0.0100\n",
      "Episode:569 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.4286 dlossA:0.0437 dlossQ:9.4257 exploreP:0.0100\n",
      "Episode:570 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.4722 dlossA:0.0720 dlossQ:9.8250 exploreP:0.0100\n",
      "Episode:571 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.3467 dlossA:0.0752 dlossQ:9.5638 exploreP:0.0100\n",
      "Episode:572 meanR:497.7900 R:500.0000 rate:1.0000 gloss:4.2715 dlossA:0.0841 dlossQ:9.3423 exploreP:0.0100\n",
      "Episode:573 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.2299 dlossA:0.0540 dlossQ:9.5744 exploreP:0.0100\n",
      "Episode:574 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.2732 dlossA:0.0782 dlossQ:9.5964 exploreP:0.0100\n",
      "Episode:575 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.4571 dlossA:0.0633 dlossQ:9.8946 exploreP:0.0100\n",
      "Episode:576 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.4272 dlossA:0.0707 dlossQ:9.9240 exploreP:0.0100\n",
      "Episode:577 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.1784 dlossA:0.0533 dlossQ:9.7616 exploreP:0.0100\n",
      "Episode:578 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.4514 dlossA:0.0712 dlossQ:9.9251 exploreP:0.0100\n",
      "Episode:579 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.3860 dlossA:0.0653 dlossQ:10.0090 exploreP:0.0100\n",
      "Episode:580 meanR:498.1100 R:500.0000 rate:1.0000 gloss:4.3561 dlossA:0.0626 dlossQ:10.0110 exploreP:0.0100\n",
      "Episode:581 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2930 dlossA:0.0498 dlossQ:10.1805 exploreP:0.0100\n",
      "Episode:582 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3226 dlossA:0.0542 dlossQ:10.2121 exploreP:0.0100\n",
      "Episode:583 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.1847 dlossA:0.0852 dlossQ:9.8344 exploreP:0.0100\n",
      "Episode:584 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2463 dlossA:0.0625 dlossQ:10.0523 exploreP:0.0100\n",
      "Episode:585 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3490 dlossA:0.0635 dlossQ:10.2546 exploreP:0.0100\n",
      "Episode:586 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3935 dlossA:0.0543 dlossQ:10.3310 exploreP:0.0100\n",
      "Episode:587 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.4489 dlossA:0.0790 dlossQ:10.4194 exploreP:0.0100\n",
      "Episode:588 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2538 dlossA:0.0658 dlossQ:9.9907 exploreP:0.0100\n",
      "Episode:589 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3050 dlossA:0.0956 dlossQ:9.9781 exploreP:0.0100\n",
      "Episode:590 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3146 dlossA:0.0990 dlossQ:10.0277 exploreP:0.0100\n",
      "Episode:591 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3526 dlossA:0.0872 dlossQ:10.3911 exploreP:0.0100\n",
      "Episode:592 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.4203 dlossA:0.0581 dlossQ:10.6177 exploreP:0.0100\n",
      "Episode:593 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2805 dlossA:0.0951 dlossQ:10.0597 exploreP:0.0100\n",
      "Episode:594 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.4200 dlossA:0.0881 dlossQ:10.0452 exploreP:0.0100\n",
      "Episode:595 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.4067 dlossA:0.0506 dlossQ:10.4548 exploreP:0.0100\n",
      "Episode:596 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3599 dlossA:0.0824 dlossQ:10.2658 exploreP:0.0100\n",
      "Episode:597 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3834 dlossA:0.0502 dlossQ:10.5430 exploreP:0.0100\n",
      "Episode:598 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.1904 dlossA:0.0715 dlossQ:10.6321 exploreP:0.0100\n",
      "Episode:599 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2495 dlossA:0.0615 dlossQ:10.6320 exploreP:0.0100\n",
      "Episode:600 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2480 dlossA:0.0837 dlossQ:10.3826 exploreP:0.0100\n",
      "Episode:601 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.3006 dlossA:0.0976 dlossQ:10.4107 exploreP:0.0100\n",
      "Episode:602 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2410 dlossA:0.0586 dlossQ:10.6860 exploreP:0.0100\n",
      "Episode:603 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.4144 dlossA:0.0683 dlossQ:10.6960 exploreP:0.0100\n",
      "Episode:604 meanR:498.7900 R:500.0000 rate:1.0000 gloss:4.2082 dlossA:0.0702 dlossQ:10.6862 exploreP:0.0100\n",
      "Episode:605 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.3289 dlossA:0.0559 dlossQ:10.9156 exploreP:0.0100\n",
      "Episode:606 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.1437 dlossA:0.0759 dlossQ:10.6810 exploreP:0.0100\n",
      "Episode:607 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2951 dlossA:0.0638 dlossQ:10.8490 exploreP:0.0100\n",
      "Episode:608 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.3568 dlossA:0.0695 dlossQ:10.9862 exploreP:0.0100\n",
      "Episode:609 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2804 dlossA:0.0695 dlossQ:10.9940 exploreP:0.0100\n",
      "Episode:610 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2216 dlossA:0.0682 dlossQ:10.9648 exploreP:0.0100\n",
      "Episode:611 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2734 dlossA:0.0634 dlossQ:10.9811 exploreP:0.0100\n",
      "Episode:612 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2938 dlossA:0.0784 dlossQ:11.0259 exploreP:0.0100\n",
      "Episode:613 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2655 dlossA:0.0759 dlossQ:10.9030 exploreP:0.0100\n",
      "Episode:614 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2742 dlossA:0.0747 dlossQ:10.7993 exploreP:0.0100\n",
      "Episode:615 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.3231 dlossA:0.0680 dlossQ:11.1698 exploreP:0.0100\n",
      "Episode:616 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2994 dlossA:0.0555 dlossQ:11.2952 exploreP:0.0100\n",
      "Episode:617 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2016 dlossA:0.0616 dlossQ:11.0563 exploreP:0.0100\n",
      "Episode:618 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2043 dlossA:0.0676 dlossQ:11.0436 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:619 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.0894 dlossA:0.0661 dlossQ:10.7998 exploreP:0.0100\n",
      "Episode:620 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2190 dlossA:0.0865 dlossQ:10.9096 exploreP:0.0100\n",
      "Episode:621 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.1527 dlossA:0.0701 dlossQ:11.0350 exploreP:0.0100\n",
      "Episode:622 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2130 dlossA:0.0508 dlossQ:11.2918 exploreP:0.0100\n",
      "Episode:623 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.1671 dlossA:0.0601 dlossQ:11.0944 exploreP:0.0100\n",
      "Episode:624 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2266 dlossA:0.0359 dlossQ:11.4765 exploreP:0.0100\n",
      "Episode:625 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.1309 dlossA:0.0814 dlossQ:10.9112 exploreP:0.0100\n",
      "Episode:626 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.2578 dlossA:0.0587 dlossQ:11.3358 exploreP:0.0100\n",
      "Episode:627 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.1898 dlossA:0.0734 dlossQ:11.0465 exploreP:0.0100\n",
      "Episode:628 meanR:499.2400 R:500.0000 rate:1.0000 gloss:4.3145 dlossA:0.0554 dlossQ:11.2141 exploreP:0.0100\n",
      "Episode:629 meanR:498.9700 R:473.0000 rate:0.9460 gloss:4.2245 dlossA:0.0929 dlossQ:10.8713 exploreP:0.0100\n",
      "Episode:630 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.2766 dlossA:0.0779 dlossQ:10.9996 exploreP:0.0100\n",
      "Episode:631 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.0838 dlossA:0.0817 dlossQ:11.0233 exploreP:0.0100\n",
      "Episode:632 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.1403 dlossA:0.0775 dlossQ:10.9823 exploreP:0.0100\n",
      "Episode:633 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.1313 dlossA:0.0816 dlossQ:10.9747 exploreP:0.0100\n",
      "Episode:634 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.0441 dlossA:0.0863 dlossQ:10.9836 exploreP:0.0100\n",
      "Episode:635 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.1146 dlossA:0.0814 dlossQ:11.0542 exploreP:0.0100\n",
      "Episode:636 meanR:498.9700 R:500.0000 rate:1.0000 gloss:3.9855 dlossA:0.0977 dlossQ:10.6351 exploreP:0.0100\n",
      "Episode:637 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.0974 dlossA:0.0839 dlossQ:11.1422 exploreP:0.0100\n",
      "Episode:638 meanR:498.9700 R:500.0000 rate:1.0000 gloss:4.0218 dlossA:0.0677 dlossQ:11.1781 exploreP:0.0100\n",
      "Episode:639 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9937 dlossA:0.0616 dlossQ:11.2591 exploreP:0.0100\n",
      "Episode:640 meanR:499.7300 R:500.0000 rate:1.0000 gloss:4.1076 dlossA:0.0700 dlossQ:11.4093 exploreP:0.0100\n",
      "Episode:641 meanR:499.7300 R:500.0000 rate:1.0000 gloss:4.1833 dlossA:0.0449 dlossQ:11.6004 exploreP:0.0100\n",
      "Episode:642 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9770 dlossA:0.0633 dlossQ:11.2858 exploreP:0.0100\n",
      "Episode:643 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9660 dlossA:0.0644 dlossQ:10.9617 exploreP:0.0100\n",
      "Episode:644 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9219 dlossA:0.0634 dlossQ:11.0627 exploreP:0.0100\n",
      "Episode:645 meanR:499.7300 R:500.0000 rate:1.0000 gloss:4.0185 dlossA:0.0601 dlossQ:11.3242 exploreP:0.0100\n",
      "Episode:646 meanR:499.7300 R:500.0000 rate:1.0000 gloss:4.0187 dlossA:0.0879 dlossQ:10.9085 exploreP:0.0100\n",
      "Episode:647 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9779 dlossA:0.0685 dlossQ:11.2752 exploreP:0.0100\n",
      "Episode:648 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.7933 dlossA:0.1062 dlossQ:10.9714 exploreP:0.0100\n",
      "Episode:649 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9006 dlossA:0.0745 dlossQ:10.9941 exploreP:0.0100\n",
      "Episode:650 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9405 dlossA:0.0670 dlossQ:11.3288 exploreP:0.0100\n",
      "Episode:651 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9118 dlossA:0.0550 dlossQ:11.2336 exploreP:0.0100\n",
      "Episode:652 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9797 dlossA:0.0887 dlossQ:11.0765 exploreP:0.0100\n",
      "Episode:653 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.8857 dlossA:0.0598 dlossQ:11.1271 exploreP:0.0100\n",
      "Episode:654 meanR:499.7300 R:500.0000 rate:1.0000 gloss:4.0239 dlossA:0.0579 dlossQ:11.5061 exploreP:0.0100\n",
      "Episode:655 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.9986 dlossA:0.0424 dlossQ:11.5465 exploreP:0.0100\n",
      "Episode:656 meanR:499.7300 R:500.0000 rate:1.0000 gloss:3.8401 dlossA:0.0795 dlossQ:11.0479 exploreP:0.0100\n",
      "Episode:657 meanR:499.6600 R:493.0000 rate:0.9860 gloss:3.9762 dlossA:0.0574 dlossQ:11.2005 exploreP:0.0100\n",
      "Episode:658 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9716 dlossA:0.0779 dlossQ:11.2669 exploreP:0.0100\n",
      "Episode:659 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7440 dlossA:0.0958 dlossQ:10.5880 exploreP:0.0100\n",
      "Episode:660 meanR:499.6600 R:500.0000 rate:1.0000 gloss:4.0223 dlossA:0.0498 dlossQ:11.4950 exploreP:0.0100\n",
      "Episode:661 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8409 dlossA:0.0843 dlossQ:10.6922 exploreP:0.0100\n",
      "Episode:662 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9437 dlossA:0.0685 dlossQ:10.9023 exploreP:0.0100\n",
      "Episode:663 meanR:499.6600 R:500.0000 rate:1.0000 gloss:4.1514 dlossA:0.0464 dlossQ:11.5184 exploreP:0.0100\n",
      "Episode:664 meanR:499.6600 R:500.0000 rate:1.0000 gloss:4.0154 dlossA:0.0398 dlossQ:11.3583 exploreP:0.0100\n",
      "Episode:665 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9680 dlossA:0.0661 dlossQ:11.1454 exploreP:0.0100\n",
      "Episode:666 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8810 dlossA:0.0706 dlossQ:11.0971 exploreP:0.0100\n",
      "Episode:667 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9802 dlossA:0.0665 dlossQ:11.2037 exploreP:0.0100\n",
      "Episode:668 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8915 dlossA:0.0485 dlossQ:11.4386 exploreP:0.0100\n",
      "Episode:669 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9290 dlossA:0.0587 dlossQ:11.4087 exploreP:0.0100\n",
      "Episode:670 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8715 dlossA:0.0908 dlossQ:11.1820 exploreP:0.0100\n",
      "Episode:671 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8438 dlossA:0.0550 dlossQ:11.0432 exploreP:0.0100\n",
      "Episode:672 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8355 dlossA:0.0624 dlossQ:11.0834 exploreP:0.0100\n",
      "Episode:673 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8700 dlossA:0.0592 dlossQ:11.4122 exploreP:0.0100\n",
      "Episode:674 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8518 dlossA:0.0507 dlossQ:11.4968 exploreP:0.0100\n",
      "Episode:675 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8975 dlossA:0.0291 dlossQ:11.8858 exploreP:0.0100\n",
      "Episode:676 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8470 dlossA:0.0397 dlossQ:11.7709 exploreP:0.0100\n",
      "Episode:677 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9194 dlossA:0.0417 dlossQ:11.3857 exploreP:0.0100\n",
      "Episode:678 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8160 dlossA:0.0535 dlossQ:11.6621 exploreP:0.0100\n",
      "Episode:679 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7449 dlossA:0.0582 dlossQ:11.4667 exploreP:0.0100\n",
      "Episode:680 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7854 dlossA:0.0538 dlossQ:11.4527 exploreP:0.0100\n",
      "Episode:681 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9320 dlossA:0.0477 dlossQ:12.0004 exploreP:0.0100\n",
      "Episode:682 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9453 dlossA:0.0414 dlossQ:12.0056 exploreP:0.0100\n",
      "Episode:683 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7020 dlossA:0.0526 dlossQ:11.5358 exploreP:0.0100\n",
      "Episode:684 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8613 dlossA:0.0290 dlossQ:12.3071 exploreP:0.0100\n",
      "Episode:685 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.9504 dlossA:0.0326 dlossQ:11.8014 exploreP:0.0100\n",
      "Episode:686 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6879 dlossA:0.0503 dlossQ:11.3644 exploreP:0.0100\n",
      "Episode:687 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7751 dlossA:0.0544 dlossQ:11.6568 exploreP:0.0100\n",
      "Episode:688 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6411 dlossA:0.0523 dlossQ:11.6500 exploreP:0.0100\n",
      "Episode:689 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7977 dlossA:0.0300 dlossQ:11.7011 exploreP:0.0100\n",
      "Episode:690 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7243 dlossA:0.0266 dlossQ:12.3962 exploreP:0.0100\n",
      "Episode:691 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7384 dlossA:0.0331 dlossQ:11.6114 exploreP:0.0100\n",
      "Episode:692 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7384 dlossA:0.0526 dlossQ:12.0604 exploreP:0.0100\n",
      "Episode:693 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6594 dlossA:0.0375 dlossQ:11.8181 exploreP:0.0100\n",
      "Episode:694 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7553 dlossA:0.0432 dlossQ:11.9679 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:695 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7745 dlossA:0.0286 dlossQ:12.5777 exploreP:0.0100\n",
      "Episode:696 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5805 dlossA:0.0356 dlossQ:12.1473 exploreP:0.0100\n",
      "Episode:697 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.8295 dlossA:0.0273 dlossQ:12.4984 exploreP:0.0100\n",
      "Episode:698 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7316 dlossA:0.0362 dlossQ:11.9266 exploreP:0.0100\n",
      "Episode:699 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7157 dlossA:0.0237 dlossQ:12.1974 exploreP:0.0100\n",
      "Episode:700 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5852 dlossA:0.0445 dlossQ:11.1264 exploreP:0.0100\n",
      "Episode:701 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6121 dlossA:0.0280 dlossQ:12.3119 exploreP:0.0100\n",
      "Episode:702 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7123 dlossA:0.0404 dlossQ:11.5727 exploreP:0.0100\n",
      "Episode:703 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6431 dlossA:0.0409 dlossQ:12.7774 exploreP:0.0100\n",
      "Episode:704 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6023 dlossA:0.0408 dlossQ:11.8428 exploreP:0.0100\n",
      "Episode:705 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.7203 dlossA:0.0306 dlossQ:12.4115 exploreP:0.0100\n",
      "Episode:706 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6040 dlossA:0.0196 dlossQ:12.3220 exploreP:0.0100\n",
      "Episode:707 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5760 dlossA:0.0209 dlossQ:12.3864 exploreP:0.0100\n",
      "Episode:708 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6142 dlossA:0.0166 dlossQ:12.5307 exploreP:0.0100\n",
      "Episode:709 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5778 dlossA:0.0129 dlossQ:12.2462 exploreP:0.0100\n",
      "Episode:710 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6223 dlossA:0.0147 dlossQ:12.4337 exploreP:0.0100\n",
      "Episode:711 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5911 dlossA:0.0170 dlossQ:12.1518 exploreP:0.0100\n",
      "Episode:712 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.6531 dlossA:0.0128 dlossQ:12.4131 exploreP:0.0100\n",
      "Episode:713 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5185 dlossA:0.0075 dlossQ:11.9867 exploreP:0.0100\n",
      "Episode:714 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.4811 dlossA:0.0209 dlossQ:11.5020 exploreP:0.0100\n",
      "Episode:715 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3940 dlossA:0.0253 dlossQ:11.3810 exploreP:0.0100\n",
      "Episode:716 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.5132 dlossA:0.0183 dlossQ:11.6353 exploreP:0.0100\n",
      "Episode:717 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.4937 dlossA:0.0087 dlossQ:11.4337 exploreP:0.0100\n",
      "Episode:718 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3669 dlossA:0.0148 dlossQ:11.7511 exploreP:0.0100\n",
      "Episode:719 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3050 dlossA:0.0189 dlossQ:10.7501 exploreP:0.0100\n",
      "Episode:720 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3562 dlossA:0.0073 dlossQ:11.5122 exploreP:0.0100\n",
      "Episode:721 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3974 dlossA:0.0148 dlossQ:11.6291 exploreP:0.0100\n",
      "Episode:722 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3603 dlossA:0.0094 dlossQ:11.0167 exploreP:0.0100\n",
      "Episode:723 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3618 dlossA:0.0173 dlossQ:11.1720 exploreP:0.0100\n",
      "Episode:724 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.2851 dlossA:0.0182 dlossQ:10.9932 exploreP:0.0100\n",
      "Episode:725 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.4021 dlossA:0.0183 dlossQ:11.3235 exploreP:0.0100\n",
      "Episode:726 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3771 dlossA:0.0218 dlossQ:11.1121 exploreP:0.0100\n",
      "Episode:727 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.3550 dlossA:0.0183 dlossQ:11.1179 exploreP:0.0100\n",
      "Episode:728 meanR:499.6600 R:500.0000 rate:1.0000 gloss:3.2869 dlossA:0.0199 dlossQ:10.9422 exploreP:0.0100\n",
      "Episode:729 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3439 dlossA:0.0271 dlossQ:11.1779 exploreP:0.0100\n",
      "Episode:730 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3507 dlossA:0.0166 dlossQ:11.2459 exploreP:0.0100\n",
      "Episode:731 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3713 dlossA:0.0200 dlossQ:11.1532 exploreP:0.0100\n",
      "Episode:732 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3782 dlossA:0.0088 dlossQ:11.5519 exploreP:0.0100\n",
      "Episode:733 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3375 dlossA:0.0312 dlossQ:11.1891 exploreP:0.0100\n",
      "Episode:734 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3693 dlossA:0.0162 dlossQ:11.3016 exploreP:0.0100\n",
      "Episode:735 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3549 dlossA:0.0237 dlossQ:11.1099 exploreP:0.0100\n",
      "Episode:736 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2789 dlossA:0.0261 dlossQ:11.2801 exploreP:0.0100\n",
      "Episode:737 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3457 dlossA:0.0241 dlossQ:11.4554 exploreP:0.0100\n",
      "Episode:738 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.4095 dlossA:0.0124 dlossQ:11.8524 exploreP:0.0100\n",
      "Episode:739 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3341 dlossA:0.0166 dlossQ:11.8030 exploreP:0.0100\n",
      "Episode:740 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2103 dlossA:0.0346 dlossQ:11.4694 exploreP:0.0100\n",
      "Episode:741 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3086 dlossA:0.0311 dlossQ:11.2498 exploreP:0.0100\n",
      "Episode:742 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3270 dlossA:0.0104 dlossQ:12.0029 exploreP:0.0100\n",
      "Episode:743 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3270 dlossA:0.0160 dlossQ:11.9836 exploreP:0.0100\n",
      "Episode:744 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3570 dlossA:0.0096 dlossQ:12.1564 exploreP:0.0100\n",
      "Episode:745 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2501 dlossA:0.0204 dlossQ:11.8788 exploreP:0.0100\n",
      "Episode:746 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2753 dlossA:0.0247 dlossQ:11.5068 exploreP:0.0100\n",
      "Episode:747 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2876 dlossA:0.0187 dlossQ:11.8254 exploreP:0.0100\n",
      "Episode:748 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3285 dlossA:0.0175 dlossQ:11.9280 exploreP:0.0100\n",
      "Episode:749 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2142 dlossA:0.0269 dlossQ:11.8413 exploreP:0.0100\n",
      "Episode:750 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3120 dlossA:0.0121 dlossQ:12.1953 exploreP:0.0100\n",
      "Episode:751 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2968 dlossA:0.0280 dlossQ:11.7920 exploreP:0.0100\n",
      "Episode:752 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2585 dlossA:0.0140 dlossQ:12.0751 exploreP:0.0100\n",
      "Episode:753 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.2728 dlossA:0.0213 dlossQ:11.7189 exploreP:0.0100\n",
      "Episode:754 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3743 dlossA:0.0241 dlossQ:11.8344 exploreP:0.0100\n",
      "Episode:755 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3887 dlossA:0.0210 dlossQ:12.1408 exploreP:0.0100\n",
      "Episode:756 meanR:499.9300 R:500.0000 rate:1.0000 gloss:3.3158 dlossA:0.0163 dlossQ:12.0132 exploreP:0.0100\n",
      "Episode:757 meanR:500.0000 R:500.0000 rate:1.0000 gloss:3.3266 dlossA:0.0340 dlossQ:11.8300 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list = [] # goal\n",
    "rewards_list, gloss_list, dlossA_list, dlossQ_list = [], [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        gloss_batch, dlossA_batch, dlossQ_batch= [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "        total_reward = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "                #print(action)\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits) # adding epsilon*noise\n",
    "                #print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), -1])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the last played episode\n",
    "            if done is True:\n",
    "                rate = total_reward/ goal # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][5] == -1: # double-check if it is empty and it is not rated!\n",
    "                        memory.buffer[-1-idx][5] = rate # rate each SA pair\n",
    "            \n",
    "            # Training using a max rated batch\n",
    "            while True:\n",
    "                idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "                batch = np.array(memory.buffer)[idx*batch_size:(idx+1)*batch_size]\n",
    "                rates = np.array([each[5] for each in batch])\n",
    "                if (np.max(rates)*0.9) > 0: # non-rated data -1\n",
    "                    break\n",
    "            batch = batch[rates >= (np.max(rates)*0.9)]\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            next_states = np.array([each[2] for each in batch])\n",
    "            rewards = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            rates = np.array([each[5] for each in batch])            \n",
    "            #print(states.shape, actions.shape, next_states.shape, rewards.shape, dones.shape, rates.shape)\n",
    "            nextQs_logits = sess.run(model.Qs_logits, feed_dict = {model.states: next_states})\n",
    "            #nextQs = np.max(nextQs_logits, axis=1) * (1-dones) # DQN\n",
    "            nextQs = nextQs_logits.reshape([-1]) * (1-dones) # DPG\n",
    "            targetQs = rewards + (gamma * nextQs)\n",
    "            gloss, dlossA, dlossQ, _, _, _ = sess.run([model.g_loss, model.d_lossA, model.d_lossQ, \n",
    "                                                       model.g_opt, model.d_optA, model.d_optQ],\n",
    "                                                      feed_dict = {model.states: states, \n",
    "                                                                   model.actions: actions,\n",
    "                                                                   model.targetQs: targetQs, \n",
    "                                                                   model.rates: rates})\n",
    "            gloss_batch.append(gloss)\n",
    "            dlossA_batch.append(dlossA)\n",
    "            dlossQ_batch.append(dlossQ)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dlossA:{:.4f}'.format(np.mean(dlossA_batch)),\n",
    "              'dlossQ:{:.4f}'.format(np.mean(dlossQ_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dlossA_list.append([ep, np.mean(dlossA_batch)])\n",
    "        dlossQ_list.append([ep, np.mean(dlossQ_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-a0b0af09d9f3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-a0b0af09d9f3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    plt.ylabel('G losses')`\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossA_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dlossQ_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
