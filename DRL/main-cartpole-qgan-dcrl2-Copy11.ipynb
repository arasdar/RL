{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    rewards = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "    dones = tf.placeholder(tf.float32, [None], name='dones')\n",
    "    rates = tf.placeholder(tf.float32, [None], name='rates') # success rate\n",
    "    return states, actions, next_states, rewards, dones, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('actor', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(state_size, action_size, hidden_size, gamma,\n",
    "               states, actions, next_states, rewards, dones, rates):\n",
    "    actions_logits = actor(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    aloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=actions_logits, \n",
    "                                                                      labels=actions_labels))\n",
    "    ###############################################\n",
    "    next_states_logits = generator(actions=actions_logits, hidden_size=hidden_size, state_size=state_size)\n",
    "    next_states_labels = tf.nn.sigmoid(next_states)\n",
    "    aloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, \n",
    "                                                                    labels=next_states_labels))\n",
    "    ####################################################\n",
    "    dQs = discriminator(actions=actions_labels, hidden_size=hidden_size, states=states, action_size=action_size)\n",
    "    rates = tf.reshape(rates, shape=[-1, 1])\n",
    "    dloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dQs, # GAN\n",
    "                                                                   labels=rates)) # 0-1\n",
    "    ####################################################\n",
    "    gQs = discriminator(actions=actions_logits, hidden_size=hidden_size, states=states, action_size=action_size, \n",
    "                        reuse=True)\n",
    "    dloss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.zeros_like(gQs))) # 0-1\n",
    "    aloss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs, # GAN\n",
    "                                                                    labels=tf.ones_like(gQs))) # 0-1\n",
    "    #####################################################\n",
    "    next_actions_logits = actor(states=next_states, hidden_size=hidden_size, action_size=action_size, reuse=True)\n",
    "    gQs2 = discriminator(actions=next_actions_logits, hidden_size=hidden_size, states=next_states, \n",
    "                         action_size=action_size, reuse=True)\n",
    "    gQs2 = tf.reshape(gQs2, shape=[-1]) * (1-dones)\n",
    "    aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs2, # GAN\n",
    "                                                                     labels=tf.ones_like(gQs2))) # 0-1\n",
    "    #     ##################################################### REPEATING\n",
    "    #     for _ in range(10):\n",
    "    #         next_states_logits = generator(actions=next_actions_logits, hidden_size=hidden_size, state_size=state_size, \n",
    "    #                                        reuse=True)\n",
    "    #         next_actions_logits = actor(states=next_states_logits, hidden_size=hidden_size, action_size=action_size, \n",
    "    #                                     reuse=True)\n",
    "    #         gQs3 = discriminator(actions=next_actions_logits, hidden_size=hidden_size, states=next_states_logits, \n",
    "    #                              action_size=action_size, reuse=True)\n",
    "    #         dones = tf.concat(axis=0, values=[dones[1:], tf.zeros(shape=[1])])\n",
    "    #         gQs3 = tf.reshape(gQs3, shape=[-1]) * dones\n",
    "    #         aloss2 += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gQs3, # GAN\n",
    "    #                                                                          labels=tf.ones_like(gQs3))) # 0-1\n",
    "    return actions_logits, aloss, dloss, aloss2, (1-dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(a_loss, a_loss2, d_loss, a_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    a_vars = [var for var in t_vars if var.name.startswith('actor')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        a_opt = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss, var_list=a_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        a_opt2 = tf.train.AdamOptimizer(a_learning_rate).minimize(a_loss2, var_list=a_vars)\n",
    "    return a_opt, d_opt, a_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, a_learning_rate, d_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.rewards, self.dones, self.rates = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.a_loss, self.d_loss, self.a_loss2, self.dones__ = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init\n",
    "            states=self.states, actions=self.actions, next_states=self.next_states, #model input \n",
    "            rewards=self.rewards, dones=self.dones, rates=self.rates) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.a_opt, self.d_opt, self.a_opt2 = model_opt(a_loss=self.a_loss, \n",
    "                                                        a_loss2=self.a_loss2, \n",
    "                                                        d_loss=self.d_loss,\n",
    "                                                        a_learning_rate=a_learning_rate,\n",
    "                                                        d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample(buffer, batch_size):\n",
    "#     idx = np.random.choice(np.arange(len(buffer)), size=batch_size, replace=False)\n",
    "#     return [buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size) # data batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 4*2             # number of units in each Q-network hidden layer\n",
    "a_learning_rate = 1e-4         # Q-network learning rate\n",
    "d_learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e5)            # memory capacity\n",
    "batch_size = int(1e3)             # experience mini-batch size: 200/500 a successfull episode size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size,\n",
    "              a_learning_rate=a_learning_rate, d_learning_rate=d_learning_rate, gamma=gamma)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "total_reward = 0\n",
    "num_step = 0\n",
    "for _ in range(memory_size):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rate = -1\n",
    "    memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "    num_step += 1 # memory incremented\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        rate = total_reward/500\n",
    "        for idx in range(num_step): # episode length\n",
    "            if memory.buffer[-1-idx][-1] == -1:\n",
    "                memory.buffer[-1-idx][-1] = rate\n",
    "        total_reward = 0 # reset\n",
    "        num_step = 0 # reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:20.0000 R:20.0000 rate:0.0400 aloss:1.4075 dloss:1.2406 aloss2:1.3956 exploreP:0.9980\n",
      "Episode:1 meanR:38.5000 R:57.0000 rate:0.1140 aloss:1.3956 dloss:1.2289 aloss2:1.3905 exploreP:0.9924\n",
      "Episode:2 meanR:30.0000 R:13.0000 rate:0.0260 aloss:1.3865 dloss:1.2202 aloss2:1.3861 exploreP:0.9911\n",
      "Episode:3 meanR:25.0000 R:10.0000 rate:0.0200 aloss:1.4018 dloss:1.2215 aloss2:1.3729 exploreP:0.9901\n",
      "Episode:4 meanR:28.4000 R:42.0000 rate:0.0840 aloss:1.3985 dloss:1.2136 aloss2:1.3943 exploreP:0.9860\n",
      "Episode:5 meanR:25.5000 R:11.0000 rate:0.0220 aloss:1.3971 dloss:1.2058 aloss2:1.3765 exploreP:0.9850\n",
      "Episode:6 meanR:23.8571 R:14.0000 rate:0.0280 aloss:1.4032 dloss:1.1953 aloss2:1.4051 exploreP:0.9836\n",
      "Episode:7 meanR:24.3750 R:28.0000 rate:0.0560 aloss:1.3887 dloss:1.1842 aloss2:1.4396 exploreP:0.9809\n",
      "Episode:8 meanR:24.4444 R:25.0000 rate:0.0500 aloss:1.3913 dloss:1.1850 aloss2:1.4082 exploreP:0.9785\n",
      "Episode:9 meanR:22.8000 R:8.0000 rate:0.0160 aloss:1.3976 dloss:1.2009 aloss2:1.4102 exploreP:0.9777\n",
      "Episode:10 meanR:22.3636 R:18.0000 rate:0.0360 aloss:1.3716 dloss:1.1653 aloss2:1.4524 exploreP:0.9759\n",
      "Episode:11 meanR:21.3333 R:10.0000 rate:0.0200 aloss:1.3856 dloss:1.1620 aloss2:1.4626 exploreP:0.9750\n",
      "Episode:12 meanR:21.0000 R:17.0000 rate:0.0340 aloss:1.3776 dloss:1.1479 aloss2:1.4563 exploreP:0.9733\n",
      "Episode:13 meanR:20.7143 R:17.0000 rate:0.0340 aloss:1.3907 dloss:1.1480 aloss2:1.4823 exploreP:0.9717\n",
      "Episode:14 meanR:20.2000 R:13.0000 rate:0.0260 aloss:1.3818 dloss:1.1417 aloss2:1.4615 exploreP:0.9705\n",
      "Episode:15 meanR:19.5000 R:9.0000 rate:0.0180 aloss:1.3876 dloss:1.1458 aloss2:1.4533 exploreP:0.9696\n",
      "Episode:16 meanR:19.5294 R:20.0000 rate:0.0400 aloss:1.4112 dloss:1.1531 aloss2:1.4442 exploreP:0.9677\n",
      "Episode:17 meanR:20.4444 R:36.0000 rate:0.0720 aloss:1.3826 dloss:1.1238 aloss2:1.4939 exploreP:0.9642\n",
      "Episode:18 meanR:19.8947 R:10.0000 rate:0.0200 aloss:1.3751 dloss:1.1129 aloss2:1.5093 exploreP:0.9633\n",
      "Episode:19 meanR:20.0500 R:23.0000 rate:0.0460 aloss:1.3765 dloss:1.1128 aloss2:1.5172 exploreP:0.9611\n",
      "Episode:20 meanR:20.2857 R:25.0000 rate:0.0500 aloss:1.3832 dloss:1.1042 aloss2:1.5062 exploreP:0.9587\n",
      "Episode:21 meanR:19.9091 R:12.0000 rate:0.0240 aloss:1.3721 dloss:1.0926 aloss2:1.5320 exploreP:0.9576\n",
      "Episode:22 meanR:20.2174 R:27.0000 rate:0.0540 aloss:1.3916 dloss:1.0969 aloss2:1.5032 exploreP:0.9550\n",
      "Episode:23 meanR:20.1250 R:18.0000 rate:0.0360 aloss:1.3863 dloss:1.0943 aloss2:1.5245 exploreP:0.9533\n",
      "Episode:24 meanR:19.8000 R:12.0000 rate:0.0240 aloss:1.3835 dloss:1.0791 aloss2:1.5710 exploreP:0.9522\n",
      "Episode:25 meanR:19.6154 R:15.0000 rate:0.0300 aloss:1.4426 dloss:1.0831 aloss2:1.5349 exploreP:0.9508\n",
      "Episode:26 meanR:19.6667 R:21.0000 rate:0.0420 aloss:1.3792 dloss:1.0647 aloss2:1.5855 exploreP:0.9488\n",
      "Episode:27 meanR:19.5714 R:17.0000 rate:0.0340 aloss:1.3984 dloss:1.0531 aloss2:1.5922 exploreP:0.9472\n",
      "Episode:28 meanR:19.4828 R:17.0000 rate:0.0340 aloss:1.3835 dloss:1.0445 aloss2:1.6146 exploreP:0.9456\n",
      "Episode:29 meanR:19.3667 R:16.0000 rate:0.0320 aloss:1.3759 dloss:1.0584 aloss2:1.5957 exploreP:0.9441\n",
      "Episode:30 meanR:19.1613 R:13.0000 rate:0.0260 aloss:1.3852 dloss:1.0349 aloss2:1.6229 exploreP:0.9429\n",
      "Episode:31 meanR:19.0625 R:16.0000 rate:0.0320 aloss:1.3913 dloss:1.0419 aloss2:1.6133 exploreP:0.9414\n",
      "Episode:32 meanR:19.2121 R:24.0000 rate:0.0480 aloss:1.3772 dloss:1.0251 aloss2:1.6579 exploreP:0.9392\n",
      "Episode:33 meanR:19.5588 R:31.0000 rate:0.0620 aloss:1.4415 dloss:1.0332 aloss2:1.6566 exploreP:0.9363\n",
      "Episode:34 meanR:19.2571 R:9.0000 rate:0.0180 aloss:1.3812 dloss:1.0100 aloss2:1.7183 exploreP:0.9355\n",
      "Episode:35 meanR:19.3611 R:23.0000 rate:0.0460 aloss:1.3798 dloss:1.0010 aloss2:1.7035 exploreP:0.9333\n",
      "Episode:36 meanR:19.1351 R:11.0000 rate:0.0220 aloss:1.4113 dloss:0.9735 aloss2:1.7665 exploreP:0.9323\n",
      "Episode:37 meanR:19.0526 R:16.0000 rate:0.0320 aloss:1.3746 dloss:0.9792 aloss2:1.7389 exploreP:0.9309\n",
      "Episode:38 meanR:18.9744 R:16.0000 rate:0.0320 aloss:1.4006 dloss:0.9964 aloss2:1.7237 exploreP:0.9294\n",
      "Episode:39 meanR:18.8500 R:14.0000 rate:0.0280 aloss:1.3784 dloss:0.9753 aloss2:1.7770 exploreP:0.9281\n",
      "Episode:40 meanR:18.7805 R:16.0000 rate:0.0320 aloss:1.3712 dloss:0.9690 aloss2:1.7541 exploreP:0.9266\n",
      "Episode:41 meanR:18.7857 R:19.0000 rate:0.0380 aloss:1.3854 dloss:0.9756 aloss2:1.7726 exploreP:0.9249\n",
      "Episode:42 meanR:18.9070 R:24.0000 rate:0.0480 aloss:1.3980 dloss:0.9561 aloss2:1.7731 exploreP:0.9227\n",
      "Episode:43 meanR:18.7500 R:12.0000 rate:0.0240 aloss:1.3975 dloss:0.9539 aloss2:1.7919 exploreP:0.9216\n",
      "Episode:44 meanR:19.7556 R:64.0000 rate:0.1280 aloss:1.3946 dloss:0.9397 aloss2:1.8528 exploreP:0.9158\n",
      "Episode:45 meanR:19.7174 R:18.0000 rate:0.0360 aloss:1.3895 dloss:0.9060 aloss2:1.9033 exploreP:0.9142\n",
      "Episode:46 meanR:19.7447 R:21.0000 rate:0.0420 aloss:1.3808 dloss:0.9139 aloss2:1.9284 exploreP:0.9123\n",
      "Episode:47 meanR:19.7292 R:19.0000 rate:0.0380 aloss:1.3894 dloss:0.9012 aloss2:1.9189 exploreP:0.9105\n",
      "Episode:48 meanR:19.6531 R:16.0000 rate:0.0320 aloss:1.3891 dloss:0.8932 aloss2:1.9532 exploreP:0.9091\n",
      "Episode:49 meanR:19.5200 R:13.0000 rate:0.0260 aloss:1.3859 dloss:0.8949 aloss2:1.9776 exploreP:0.9079\n",
      "Episode:50 meanR:19.6275 R:25.0000 rate:0.0500 aloss:1.3952 dloss:0.8708 aloss2:2.0031 exploreP:0.9057\n",
      "Episode:51 meanR:19.5577 R:16.0000 rate:0.0320 aloss:1.3757 dloss:0.8590 aloss2:2.1031 exploreP:0.9043\n",
      "Episode:52 meanR:19.5283 R:18.0000 rate:0.0360 aloss:1.3723 dloss:0.8648 aloss2:2.0914 exploreP:0.9027\n",
      "Episode:53 meanR:19.5926 R:23.0000 rate:0.0460 aloss:1.3845 dloss:0.8471 aloss2:2.0746 exploreP:0.9006\n",
      "Episode:54 meanR:19.8182 R:32.0000 rate:0.0640 aloss:1.3910 dloss:0.8498 aloss2:2.1004 exploreP:0.8978\n",
      "Episode:55 meanR:20.2857 R:46.0000 rate:0.0920 aloss:1.3954 dloss:0.8389 aloss2:2.1233 exploreP:0.8937\n",
      "Episode:56 meanR:20.2807 R:20.0000 rate:0.0400 aloss:1.3812 dloss:0.8404 aloss2:2.1414 exploreP:0.8919\n",
      "Episode:57 meanR:20.1724 R:14.0000 rate:0.0280 aloss:1.3843 dloss:0.7942 aloss2:2.1996 exploreP:0.8907\n",
      "Episode:58 meanR:20.0678 R:14.0000 rate:0.0280 aloss:1.3744 dloss:0.7943 aloss2:2.2256 exploreP:0.8895\n",
      "Episode:59 meanR:20.4000 R:40.0000 rate:0.0800 aloss:1.4001 dloss:0.8097 aloss2:2.2562 exploreP:0.8859\n",
      "Episode:60 meanR:20.5246 R:28.0000 rate:0.0560 aloss:1.3860 dloss:0.8066 aloss2:2.2698 exploreP:0.8835\n",
      "Episode:61 meanR:21.2903 R:68.0000 rate:0.1360 aloss:1.3856 dloss:0.7794 aloss2:2.3423 exploreP:0.8776\n",
      "Episode:62 meanR:21.1587 R:13.0000 rate:0.0260 aloss:1.3784 dloss:0.7469 aloss2:2.5005 exploreP:0.8765\n",
      "Episode:63 meanR:21.1562 R:21.0000 rate:0.0420 aloss:1.3933 dloss:0.7558 aloss2:2.4080 exploreP:0.8746\n",
      "Episode:64 meanR:21.1231 R:19.0000 rate:0.0380 aloss:1.3841 dloss:0.7595 aloss2:2.4231 exploreP:0.8730\n",
      "Episode:65 meanR:21.4242 R:41.0000 rate:0.0820 aloss:1.3957 dloss:0.7522 aloss2:2.4250 exploreP:0.8695\n",
      "Episode:66 meanR:21.3284 R:15.0000 rate:0.0300 aloss:1.3774 dloss:0.7515 aloss2:2.5056 exploreP:0.8682\n",
      "Episode:67 meanR:21.6324 R:42.0000 rate:0.0840 aloss:1.3936 dloss:0.7401 aloss2:2.5832 exploreP:0.8646\n",
      "Episode:68 meanR:21.5217 R:14.0000 rate:0.0280 aloss:1.3882 dloss:0.7461 aloss2:2.6005 exploreP:0.8634\n",
      "Episode:69 meanR:21.4714 R:18.0000 rate:0.0360 aloss:1.3799 dloss:0.7328 aloss2:2.5875 exploreP:0.8618\n",
      "Episode:70 meanR:21.4789 R:22.0000 rate:0.0440 aloss:1.3859 dloss:0.7513 aloss2:2.5727 exploreP:0.8600\n",
      "Episode:71 meanR:21.9722 R:57.0000 rate:0.1140 aloss:1.3999 dloss:0.7306 aloss2:2.6669 exploreP:0.8551\n",
      "Episode:72 meanR:22.0000 R:24.0000 rate:0.0480 aloss:1.3877 dloss:0.6989 aloss2:2.6884 exploreP:0.8531\n",
      "Episode:73 meanR:21.9054 R:15.0000 rate:0.0300 aloss:1.3804 dloss:0.7450 aloss2:2.7469 exploreP:0.8519\n",
      "Episode:74 meanR:21.8667 R:19.0000 rate:0.0380 aloss:1.4054 dloss:0.7384 aloss2:2.6863 exploreP:0.8503\n",
      "Episode:75 meanR:21.7368 R:12.0000 rate:0.0240 aloss:1.3868 dloss:0.7029 aloss2:2.7223 exploreP:0.8492\n",
      "Episode:76 meanR:21.6104 R:12.0000 rate:0.0240 aloss:1.3878 dloss:0.6826 aloss2:2.8732 exploreP:0.8482\n",
      "Episode:77 meanR:21.5128 R:14.0000 rate:0.0280 aloss:1.3959 dloss:0.7061 aloss2:2.7935 exploreP:0.8471\n",
      "Episode:78 meanR:21.5823 R:27.0000 rate:0.0540 aloss:1.3957 dloss:0.6892 aloss2:2.7671 exploreP:0.8448\n",
      "Episode:79 meanR:21.6375 R:26.0000 rate:0.0520 aloss:1.3883 dloss:0.7100 aloss2:2.8667 exploreP:0.8426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:80 meanR:21.5802 R:17.0000 rate:0.0340 aloss:1.3941 dloss:0.6673 aloss2:2.8336 exploreP:0.8412\n",
      "Episode:81 meanR:21.6585 R:28.0000 rate:0.0560 aloss:1.3885 dloss:0.6801 aloss2:2.8790 exploreP:0.8389\n",
      "Episode:82 meanR:21.6145 R:18.0000 rate:0.0360 aloss:1.3841 dloss:0.6891 aloss2:2.9920 exploreP:0.8374\n",
      "Episode:83 meanR:21.8214 R:39.0000 rate:0.0780 aloss:1.4071 dloss:0.6796 aloss2:2.8799 exploreP:0.8342\n",
      "Episode:84 meanR:21.8471 R:24.0000 rate:0.0480 aloss:1.3924 dloss:0.6704 aloss2:3.0599 exploreP:0.8322\n",
      "Episode:85 meanR:21.7442 R:13.0000 rate:0.0260 aloss:1.3926 dloss:0.6721 aloss2:3.0336 exploreP:0.8311\n",
      "Episode:86 meanR:21.7126 R:19.0000 rate:0.0380 aloss:1.4062 dloss:0.6636 aloss2:2.9219 exploreP:0.8296\n",
      "Episode:87 meanR:21.6818 R:19.0000 rate:0.0380 aloss:1.3976 dloss:0.6483 aloss2:3.0250 exploreP:0.8280\n",
      "Episode:88 meanR:21.8202 R:34.0000 rate:0.0680 aloss:1.4075 dloss:0.6426 aloss2:3.1073 exploreP:0.8253\n",
      "Episode:89 meanR:21.9667 R:35.0000 rate:0.0700 aloss:1.3956 dloss:0.6717 aloss2:3.1690 exploreP:0.8224\n",
      "Episode:90 meanR:22.0110 R:26.0000 rate:0.0520 aloss:1.4004 dloss:0.6380 aloss2:3.2033 exploreP:0.8203\n",
      "Episode:91 meanR:21.9565 R:17.0000 rate:0.0340 aloss:1.3810 dloss:0.6257 aloss2:3.3931 exploreP:0.8189\n",
      "Episode:92 meanR:21.9247 R:19.0000 rate:0.0380 aloss:1.3947 dloss:0.6136 aloss2:3.2797 exploreP:0.8174\n",
      "Episode:93 meanR:21.9255 R:22.0000 rate:0.0440 aloss:1.4004 dloss:0.6101 aloss2:3.3821 exploreP:0.8156\n",
      "Episode:94 meanR:21.8421 R:14.0000 rate:0.0280 aloss:1.4006 dloss:0.5981 aloss2:3.3635 exploreP:0.8145\n",
      "Episode:95 meanR:21.7604 R:14.0000 rate:0.0280 aloss:1.4049 dloss:0.6100 aloss2:3.1925 exploreP:0.8134\n",
      "Episode:96 meanR:21.7320 R:19.0000 rate:0.0380 aloss:1.4140 dloss:0.6464 aloss2:3.4256 exploreP:0.8118\n",
      "Episode:97 meanR:21.6837 R:17.0000 rate:0.0340 aloss:1.4106 dloss:0.5984 aloss2:3.3620 exploreP:0.8105\n",
      "Episode:98 meanR:21.6364 R:17.0000 rate:0.0340 aloss:1.3810 dloss:0.5800 aloss2:3.5592 exploreP:0.8091\n",
      "Episode:99 meanR:21.6200 R:20.0000 rate:0.0400 aloss:1.3850 dloss:0.6164 aloss2:3.5730 exploreP:0.8075\n",
      "Episode:100 meanR:21.6000 R:18.0000 rate:0.0360 aloss:1.3944 dloss:0.5918 aloss2:3.6598 exploreP:0.8061\n",
      "Episode:101 meanR:21.2900 R:26.0000 rate:0.0520 aloss:1.3974 dloss:0.6063 aloss2:3.5291 exploreP:0.8040\n",
      "Episode:102 meanR:21.3200 R:16.0000 rate:0.0320 aloss:1.4233 dloss:0.6066 aloss2:3.5402 exploreP:0.8027\n",
      "Episode:103 meanR:21.3700 R:15.0000 rate:0.0300 aloss:1.3998 dloss:0.5940 aloss2:3.6574 exploreP:0.8016\n",
      "Episode:104 meanR:21.0500 R:10.0000 rate:0.0200 aloss:1.4004 dloss:0.5599 aloss2:3.7437 exploreP:0.8008\n",
      "Episode:105 meanR:21.1100 R:17.0000 rate:0.0340 aloss:1.4136 dloss:0.6072 aloss2:3.6697 exploreP:0.7994\n",
      "Episode:106 meanR:21.1000 R:13.0000 rate:0.0260 aloss:1.4366 dloss:0.5641 aloss2:3.5576 exploreP:0.7984\n",
      "Episode:107 meanR:21.0700 R:25.0000 rate:0.0500 aloss:1.4020 dloss:0.5803 aloss2:3.6247 exploreP:0.7964\n",
      "Episode:108 meanR:21.0200 R:20.0000 rate:0.0400 aloss:1.3909 dloss:0.6074 aloss2:3.7438 exploreP:0.7949\n",
      "Episode:109 meanR:21.0500 R:11.0000 rate:0.0220 aloss:1.3953 dloss:0.5587 aloss2:3.8165 exploreP:0.7940\n",
      "Episode:110 meanR:21.1100 R:24.0000 rate:0.0480 aloss:1.3939 dloss:0.5708 aloss2:3.9439 exploreP:0.7921\n",
      "Episode:111 meanR:21.1200 R:11.0000 rate:0.0220 aloss:1.4259 dloss:0.5929 aloss2:3.8052 exploreP:0.7913\n",
      "Episode:112 meanR:21.1700 R:22.0000 rate:0.0440 aloss:1.4001 dloss:0.5804 aloss2:3.8260 exploreP:0.7895\n",
      "Episode:113 meanR:21.2400 R:24.0000 rate:0.0480 aloss:1.3838 dloss:0.5546 aloss2:3.9327 exploreP:0.7877\n",
      "Episode:114 meanR:21.2100 R:10.0000 rate:0.0200 aloss:1.3959 dloss:0.5318 aloss2:3.9020 exploreP:0.7869\n",
      "Episode:115 meanR:21.2800 R:16.0000 rate:0.0320 aloss:1.3958 dloss:0.5358 aloss2:3.9254 exploreP:0.7857\n",
      "Episode:116 meanR:21.3900 R:31.0000 rate:0.0620 aloss:1.3992 dloss:0.5453 aloss2:4.0247 exploreP:0.7833\n",
      "Episode:117 meanR:21.1700 R:14.0000 rate:0.0280 aloss:1.3922 dloss:0.5672 aloss2:4.0376 exploreP:0.7822\n",
      "Episode:118 meanR:21.2900 R:22.0000 rate:0.0440 aloss:1.4007 dloss:0.5569 aloss2:4.1025 exploreP:0.7805\n",
      "Episode:119 meanR:21.2400 R:18.0000 rate:0.0360 aloss:1.3992 dloss:0.5680 aloss2:4.2007 exploreP:0.7791\n",
      "Episode:120 meanR:21.1600 R:17.0000 rate:0.0340 aloss:1.3915 dloss:0.5745 aloss2:4.3772 exploreP:0.7778\n",
      "Episode:121 meanR:21.4900 R:45.0000 rate:0.0900 aloss:1.3972 dloss:0.5547 aloss2:4.2037 exploreP:0.7743\n",
      "Episode:122 meanR:21.4000 R:18.0000 rate:0.0360 aloss:1.4123 dloss:0.5347 aloss2:4.2521 exploreP:0.7730\n",
      "Episode:123 meanR:21.4200 R:20.0000 rate:0.0400 aloss:1.3847 dloss:0.5808 aloss2:4.3883 exploreP:0.7714\n",
      "Episode:124 meanR:21.7200 R:42.0000 rate:0.0840 aloss:1.3958 dloss:0.5240 aloss2:4.2564 exploreP:0.7682\n",
      "Episode:125 meanR:21.7500 R:18.0000 rate:0.0360 aloss:1.3978 dloss:0.5139 aloss2:4.4351 exploreP:0.7669\n",
      "Episode:126 meanR:21.6900 R:15.0000 rate:0.0300 aloss:1.3952 dloss:0.5730 aloss2:4.5330 exploreP:0.7657\n",
      "Episode:127 meanR:21.6900 R:17.0000 rate:0.0340 aloss:1.4170 dloss:0.5225 aloss2:4.2735 exploreP:0.7645\n",
      "Episode:128 meanR:21.6500 R:13.0000 rate:0.0260 aloss:1.3879 dloss:0.5547 aloss2:4.2082 exploreP:0.7635\n",
      "Episode:129 meanR:21.7000 R:21.0000 rate:0.0420 aloss:1.3946 dloss:0.5230 aloss2:4.3288 exploreP:0.7619\n",
      "Episode:130 meanR:21.7200 R:15.0000 rate:0.0300 aloss:1.4002 dloss:0.5648 aloss2:4.4723 exploreP:0.7608\n",
      "Episode:131 meanR:21.7800 R:22.0000 rate:0.0440 aloss:1.4027 dloss:0.5137 aloss2:4.3695 exploreP:0.7591\n",
      "Episode:132 meanR:21.8300 R:29.0000 rate:0.0580 aloss:1.3904 dloss:0.5168 aloss2:4.3886 exploreP:0.7570\n",
      "Episode:133 meanR:21.8100 R:29.0000 rate:0.0580 aloss:1.3773 dloss:0.5138 aloss2:4.4568 exploreP:0.7548\n",
      "Episode:134 meanR:22.0600 R:34.0000 rate:0.0680 aloss:1.3831 dloss:0.5216 aloss2:4.4760 exploreP:0.7523\n",
      "Episode:135 meanR:22.1900 R:36.0000 rate:0.0720 aloss:1.3929 dloss:0.5228 aloss2:4.4718 exploreP:0.7496\n",
      "Episode:136 meanR:22.4600 R:38.0000 rate:0.0760 aloss:1.3758 dloss:0.5358 aloss2:4.5769 exploreP:0.7468\n",
      "Episode:137 meanR:22.6100 R:31.0000 rate:0.0620 aloss:1.3725 dloss:0.5157 aloss2:4.6076 exploreP:0.7445\n",
      "Episode:138 meanR:22.6300 R:18.0000 rate:0.0360 aloss:1.3714 dloss:0.5169 aloss2:4.5476 exploreP:0.7432\n",
      "Episode:139 meanR:22.6600 R:17.0000 rate:0.0340 aloss:1.3729 dloss:0.5341 aloss2:4.7516 exploreP:0.7419\n",
      "Episode:140 meanR:22.9500 R:45.0000 rate:0.0900 aloss:1.3770 dloss:0.5122 aloss2:4.5778 exploreP:0.7387\n",
      "Episode:141 meanR:22.9900 R:23.0000 rate:0.0460 aloss:1.3725 dloss:0.5306 aloss2:4.7336 exploreP:0.7370\n",
      "Episode:142 meanR:23.6900 R:94.0000 rate:0.1880 aloss:1.3762 dloss:0.5191 aloss2:4.6688 exploreP:0.7302\n",
      "Episode:143 meanR:23.7300 R:16.0000 rate:0.0320 aloss:1.3726 dloss:0.5467 aloss2:4.7724 exploreP:0.7290\n",
      "Episode:144 meanR:23.3000 R:21.0000 rate:0.0420 aloss:1.3744 dloss:0.5025 aloss2:4.7468 exploreP:0.7275\n",
      "Episode:145 meanR:23.3800 R:26.0000 rate:0.0520 aloss:1.3809 dloss:0.5164 aloss2:4.7814 exploreP:0.7257\n",
      "Episode:146 meanR:23.7100 R:54.0000 rate:0.1080 aloss:1.3730 dloss:0.5084 aloss2:4.8002 exploreP:0.7218\n",
      "Episode:147 meanR:23.8300 R:31.0000 rate:0.0620 aloss:1.3805 dloss:0.5097 aloss2:4.8182 exploreP:0.7196\n",
      "Episode:148 meanR:24.0000 R:33.0000 rate:0.0660 aloss:1.3749 dloss:0.5129 aloss2:4.7701 exploreP:0.7173\n",
      "Episode:149 meanR:24.1300 R:26.0000 rate:0.0520 aloss:1.3732 dloss:0.5282 aloss2:4.9844 exploreP:0.7154\n",
      "Episode:150 meanR:24.4500 R:57.0000 rate:0.1140 aloss:1.3759 dloss:0.5076 aloss2:4.8872 exploreP:0.7114\n",
      "Episode:151 meanR:24.6300 R:34.0000 rate:0.0680 aloss:1.3772 dloss:0.5177 aloss2:5.0391 exploreP:0.7090\n",
      "Episode:152 meanR:24.6300 R:18.0000 rate:0.0360 aloss:1.3738 dloss:0.5102 aloss2:4.8420 exploreP:0.7078\n",
      "Episode:153 meanR:24.5900 R:19.0000 rate:0.0380 aloss:1.3728 dloss:0.5003 aloss2:4.9617 exploreP:0.7065\n",
      "Episode:154 meanR:24.7100 R:44.0000 rate:0.0880 aloss:1.3696 dloss:0.5392 aloss2:5.0540 exploreP:0.7034\n",
      "Episode:155 meanR:24.8500 R:60.0000 rate:0.1200 aloss:1.3743 dloss:0.5173 aloss2:5.0649 exploreP:0.6993\n",
      "Episode:156 meanR:25.2600 R:61.0000 rate:0.1220 aloss:1.3718 dloss:0.5098 aloss2:4.9883 exploreP:0.6951\n",
      "Episode:157 meanR:25.3500 R:23.0000 rate:0.0460 aloss:1.3828 dloss:0.5468 aloss2:4.8997 exploreP:0.6935\n",
      "Episode:158 meanR:25.4200 R:21.0000 rate:0.0420 aloss:1.3719 dloss:0.4797 aloss2:4.9071 exploreP:0.6921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:159 meanR:25.2000 R:18.0000 rate:0.0360 aloss:1.3729 dloss:0.5233 aloss2:4.9257 exploreP:0.6908\n",
      "Episode:160 meanR:25.2900 R:37.0000 rate:0.0740 aloss:1.3721 dloss:0.4974 aloss2:4.9973 exploreP:0.6883\n",
      "Episode:161 meanR:25.0100 R:40.0000 rate:0.0800 aloss:1.3739 dloss:0.4946 aloss2:4.9851 exploreP:0.6856\n",
      "Episode:162 meanR:25.0000 R:12.0000 rate:0.0240 aloss:1.3957 dloss:0.4984 aloss2:4.9261 exploreP:0.6848\n",
      "Episode:163 meanR:25.3600 R:57.0000 rate:0.1140 aloss:1.3706 dloss:0.4872 aloss2:5.0001 exploreP:0.6810\n",
      "Episode:164 meanR:25.5700 R:40.0000 rate:0.0800 aloss:1.3765 dloss:0.5049 aloss2:5.1271 exploreP:0.6783\n",
      "Episode:165 meanR:25.4800 R:32.0000 rate:0.0640 aloss:1.3732 dloss:0.5037 aloss2:5.1192 exploreP:0.6761\n",
      "Episode:166 meanR:25.6500 R:32.0000 rate:0.0640 aloss:1.3698 dloss:0.5108 aloss2:5.1286 exploreP:0.6740\n",
      "Episode:167 meanR:25.6100 R:38.0000 rate:0.0760 aloss:1.3756 dloss:0.4898 aloss2:5.1291 exploreP:0.6715\n",
      "Episode:168 meanR:25.6100 R:14.0000 rate:0.0280 aloss:1.3820 dloss:0.5446 aloss2:5.1785 exploreP:0.6706\n",
      "Episode:169 meanR:25.9100 R:48.0000 rate:0.0960 aloss:1.3786 dloss:0.4947 aloss2:5.0948 exploreP:0.6674\n",
      "Episode:170 meanR:26.0700 R:38.0000 rate:0.0760 aloss:1.3863 dloss:0.5199 aloss2:5.0693 exploreP:0.6649\n",
      "Episode:171 meanR:25.6200 R:12.0000 rate:0.0240 aloss:1.3814 dloss:0.5211 aloss2:5.0973 exploreP:0.6641\n",
      "Episode:172 meanR:26.2000 R:82.0000 rate:0.1640 aloss:1.3750 dloss:0.4947 aloss2:5.1396 exploreP:0.6588\n",
      "Episode:173 meanR:26.1900 R:14.0000 rate:0.0280 aloss:1.3833 dloss:0.5049 aloss2:5.2027 exploreP:0.6579\n",
      "Episode:174 meanR:26.3700 R:37.0000 rate:0.0740 aloss:1.3779 dloss:0.5014 aloss2:5.1524 exploreP:0.6555\n",
      "Episode:175 meanR:26.4900 R:24.0000 rate:0.0480 aloss:1.3805 dloss:0.4907 aloss2:5.2898 exploreP:0.6539\n",
      "Episode:176 meanR:26.4800 R:11.0000 rate:0.0220 aloss:1.3751 dloss:0.5103 aloss2:5.2874 exploreP:0.6532\n",
      "Episode:177 meanR:26.6000 R:26.0000 rate:0.0520 aloss:1.3789 dloss:0.5021 aloss2:5.2037 exploreP:0.6516\n",
      "Episode:178 meanR:27.3700 R:104.0000 rate:0.2080 aloss:1.3800 dloss:0.4952 aloss2:5.2482 exploreP:0.6449\n",
      "Episode:179 meanR:27.4000 R:29.0000 rate:0.0580 aloss:1.3738 dloss:0.5180 aloss2:5.2175 exploreP:0.6431\n",
      "Episode:180 meanR:27.4900 R:26.0000 rate:0.0520 aloss:1.3853 dloss:0.4816 aloss2:5.1969 exploreP:0.6414\n",
      "Episode:181 meanR:27.7700 R:56.0000 rate:0.1120 aloss:1.3773 dloss:0.4975 aloss2:5.2766 exploreP:0.6379\n",
      "Episode:182 meanR:27.9200 R:33.0000 rate:0.0660 aloss:1.3776 dloss:0.4962 aloss2:5.1885 exploreP:0.6358\n",
      "Episode:183 meanR:28.6000 R:107.0000 rate:0.2140 aloss:1.3761 dloss:0.4903 aloss2:5.2960 exploreP:0.6292\n",
      "Episode:184 meanR:28.6400 R:28.0000 rate:0.0560 aloss:1.3738 dloss:0.4791 aloss2:5.1977 exploreP:0.6275\n",
      "Episode:185 meanR:29.7200 R:121.0000 rate:0.2420 aloss:1.3746 dloss:0.4709 aloss2:5.2965 exploreP:0.6200\n",
      "Episode:186 meanR:30.3600 R:83.0000 rate:0.1660 aloss:1.3709 dloss:0.4859 aloss2:5.3520 exploreP:0.6150\n",
      "Episode:187 meanR:30.6000 R:43.0000 rate:0.0860 aloss:1.3749 dloss:0.5093 aloss2:5.3517 exploreP:0.6124\n",
      "Episode:188 meanR:30.9800 R:72.0000 rate:0.1440 aloss:1.3726 dloss:0.4750 aloss2:5.3658 exploreP:0.6081\n",
      "Episode:189 meanR:31.0100 R:38.0000 rate:0.0760 aloss:1.3761 dloss:0.4993 aloss2:5.3536 exploreP:0.6058\n",
      "Episode:190 meanR:31.0300 R:28.0000 rate:0.0560 aloss:1.3916 dloss:0.4679 aloss2:5.3517 exploreP:0.6041\n",
      "Episode:191 meanR:31.1800 R:32.0000 rate:0.0640 aloss:1.3714 dloss:0.5006 aloss2:5.3197 exploreP:0.6022\n",
      "Episode:192 meanR:31.4800 R:49.0000 rate:0.0980 aloss:1.3855 dloss:0.5041 aloss2:5.2926 exploreP:0.5993\n",
      "Episode:193 meanR:31.9000 R:64.0000 rate:0.1280 aloss:1.3795 dloss:0.4794 aloss2:5.3394 exploreP:0.5956\n",
      "Episode:194 meanR:32.2900 R:53.0000 rate:0.1060 aloss:1.3764 dloss:0.4906 aloss2:5.3394 exploreP:0.5925\n",
      "Episode:195 meanR:33.0600 R:91.0000 rate:0.1820 aloss:1.3770 dloss:0.5034 aloss2:5.3767 exploreP:0.5872\n",
      "Episode:196 meanR:33.3000 R:43.0000 rate:0.0860 aloss:1.3713 dloss:0.4857 aloss2:5.3578 exploreP:0.5847\n",
      "Episode:197 meanR:33.5300 R:40.0000 rate:0.0800 aloss:1.3747 dloss:0.4831 aloss2:5.3724 exploreP:0.5824\n",
      "Episode:198 meanR:33.8400 R:48.0000 rate:0.0960 aloss:1.3681 dloss:0.4976 aloss2:5.3697 exploreP:0.5797\n",
      "Episode:199 meanR:33.8400 R:20.0000 rate:0.0400 aloss:1.3833 dloss:0.4684 aloss2:5.3669 exploreP:0.5786\n",
      "Episode:200 meanR:33.9300 R:27.0000 rate:0.0540 aloss:1.3748 dloss:0.5017 aloss2:5.3333 exploreP:0.5770\n",
      "Episode:201 meanR:33.9200 R:25.0000 rate:0.0500 aloss:1.3795 dloss:0.4868 aloss2:5.3378 exploreP:0.5756\n",
      "Episode:202 meanR:34.2900 R:53.0000 rate:0.1060 aloss:1.3789 dloss:0.4892 aloss2:5.3545 exploreP:0.5726\n",
      "Episode:203 meanR:34.7000 R:56.0000 rate:0.1120 aloss:1.3708 dloss:0.4747 aloss2:5.3409 exploreP:0.5695\n",
      "Episode:204 meanR:34.9100 R:31.0000 rate:0.0620 aloss:1.3733 dloss:0.4950 aloss2:5.3947 exploreP:0.5677\n",
      "Episode:205 meanR:35.1200 R:38.0000 rate:0.0760 aloss:1.3825 dloss:0.4749 aloss2:5.3645 exploreP:0.5656\n",
      "Episode:206 meanR:35.2500 R:26.0000 rate:0.0520 aloss:1.3725 dloss:0.4942 aloss2:5.3829 exploreP:0.5642\n",
      "Episode:207 meanR:35.2900 R:29.0000 rate:0.0580 aloss:1.3595 dloss:0.4780 aloss2:5.3858 exploreP:0.5626\n",
      "Episode:208 meanR:35.2500 R:16.0000 rate:0.0320 aloss:1.3764 dloss:0.4727 aloss2:5.4398 exploreP:0.5617\n",
      "Episode:209 meanR:35.6000 R:46.0000 rate:0.0920 aloss:1.3757 dloss:0.4761 aloss2:5.3640 exploreP:0.5592\n",
      "Episode:210 meanR:35.9100 R:55.0000 rate:0.1100 aloss:1.3766 dloss:0.5055 aloss2:5.4252 exploreP:0.5562\n",
      "Episode:211 meanR:36.2100 R:41.0000 rate:0.0820 aloss:1.3665 dloss:0.4964 aloss2:5.4131 exploreP:0.5539\n",
      "Episode:212 meanR:36.2500 R:26.0000 rate:0.0520 aloss:1.3758 dloss:0.4614 aloss2:5.3397 exploreP:0.5525\n",
      "Episode:213 meanR:36.7000 R:69.0000 rate:0.1380 aloss:1.3699 dloss:0.5041 aloss2:5.3683 exploreP:0.5488\n",
      "Episode:214 meanR:37.0700 R:47.0000 rate:0.0940 aloss:1.3793 dloss:0.4860 aloss2:5.3773 exploreP:0.5463\n",
      "Episode:215 meanR:37.9600 R:105.0000 rate:0.2100 aloss:1.3774 dloss:0.4850 aloss2:5.3777 exploreP:0.5407\n",
      "Episode:216 meanR:38.0400 R:39.0000 rate:0.0780 aloss:1.3815 dloss:0.4614 aloss2:5.4186 exploreP:0.5386\n",
      "Episode:217 meanR:38.1500 R:25.0000 rate:0.0500 aloss:1.3693 dloss:0.4656 aloss2:5.4147 exploreP:0.5373\n",
      "Episode:218 meanR:38.7100 R:78.0000 rate:0.1560 aloss:1.3783 dloss:0.4844 aloss2:5.4464 exploreP:0.5332\n",
      "Episode:219 meanR:39.0800 R:55.0000 rate:0.1100 aloss:1.3665 dloss:0.4848 aloss2:5.4078 exploreP:0.5303\n",
      "Episode:220 meanR:39.6900 R:78.0000 rate:0.1560 aloss:1.3718 dloss:0.5035 aloss2:5.4047 exploreP:0.5263\n",
      "Episode:221 meanR:40.4400 R:120.0000 rate:0.2400 aloss:1.3664 dloss:0.4838 aloss2:5.3796 exploreP:0.5201\n",
      "Episode:222 meanR:40.6700 R:41.0000 rate:0.0820 aloss:1.3858 dloss:0.4698 aloss2:5.3666 exploreP:0.5180\n",
      "Episode:223 meanR:40.8900 R:42.0000 rate:0.0840 aloss:1.3795 dloss:0.4865 aloss2:5.3837 exploreP:0.5159\n",
      "Episode:224 meanR:41.7200 R:125.0000 rate:0.2500 aloss:1.3740 dloss:0.4893 aloss2:5.3976 exploreP:0.5096\n",
      "Episode:225 meanR:41.9800 R:44.0000 rate:0.0880 aloss:1.3737 dloss:0.4798 aloss2:5.3682 exploreP:0.5074\n",
      "Episode:226 meanR:41.9500 R:12.0000 rate:0.0240 aloss:1.3921 dloss:0.4628 aloss2:5.4171 exploreP:0.5068\n",
      "Episode:227 meanR:43.6100 R:183.0000 rate:0.3660 aloss:1.3837 dloss:0.4747 aloss2:5.4244 exploreP:0.4978\n",
      "Episode:228 meanR:43.8400 R:36.0000 rate:0.0720 aloss:1.3865 dloss:0.5063 aloss2:5.4075 exploreP:0.4960\n",
      "Episode:229 meanR:44.4000 R:77.0000 rate:0.1540 aloss:1.3881 dloss:0.4906 aloss2:5.3857 exploreP:0.4923\n",
      "Episode:230 meanR:44.4900 R:24.0000 rate:0.0480 aloss:1.3969 dloss:0.4647 aloss2:5.3734 exploreP:0.4912\n",
      "Episode:231 meanR:44.7600 R:49.0000 rate:0.0980 aloss:1.3789 dloss:0.4797 aloss2:5.3932 exploreP:0.4888\n",
      "Episode:232 meanR:44.9600 R:49.0000 rate:0.0980 aloss:1.3679 dloss:0.5320 aloss2:5.3863 exploreP:0.4865\n",
      "Episode:233 meanR:45.3200 R:65.0000 rate:0.1300 aloss:1.3855 dloss:0.4840 aloss2:5.3470 exploreP:0.4834\n",
      "Episode:234 meanR:46.1700 R:119.0000 rate:0.2380 aloss:1.3884 dloss:0.4886 aloss2:5.3355 exploreP:0.4778\n",
      "Episode:235 meanR:46.5200 R:71.0000 rate:0.1420 aloss:1.3822 dloss:0.4976 aloss2:5.3606 exploreP:0.4745\n",
      "Episode:236 meanR:46.7700 R:63.0000 rate:0.1260 aloss:1.3915 dloss:0.5142 aloss2:5.3398 exploreP:0.4716\n",
      "Episode:237 meanR:47.4700 R:101.0000 rate:0.2020 aloss:1.4083 dloss:0.4957 aloss2:5.2913 exploreP:0.4669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:238 meanR:47.4300 R:14.0000 rate:0.0280 aloss:1.3970 dloss:0.5264 aloss2:5.3390 exploreP:0.4663\n",
      "Episode:239 meanR:48.0700 R:81.0000 rate:0.1620 aloss:1.3835 dloss:0.5045 aloss2:5.3218 exploreP:0.4626\n",
      "Episode:240 meanR:48.5300 R:91.0000 rate:0.1820 aloss:1.3836 dloss:0.5155 aloss2:5.3167 exploreP:0.4585\n",
      "Episode:241 meanR:48.9600 R:66.0000 rate:0.1320 aloss:1.3857 dloss:0.4764 aloss2:5.3084 exploreP:0.4555\n",
      "Episode:242 meanR:48.8900 R:87.0000 rate:0.1740 aloss:1.4000 dloss:0.5069 aloss2:5.3309 exploreP:0.4517\n",
      "Episode:243 meanR:49.2000 R:47.0000 rate:0.0940 aloss:1.4176 dloss:0.5102 aloss2:5.2783 exploreP:0.4496\n",
      "Episode:244 meanR:49.6000 R:61.0000 rate:0.1220 aloss:1.4046 dloss:0.4910 aloss2:5.3176 exploreP:0.4469\n",
      "Episode:245 meanR:49.7300 R:39.0000 rate:0.0780 aloss:1.3832 dloss:0.5251 aloss2:5.3291 exploreP:0.4452\n",
      "Episode:246 meanR:49.4200 R:23.0000 rate:0.0460 aloss:1.3911 dloss:0.4560 aloss2:5.3151 exploreP:0.4442\n",
      "Episode:247 meanR:49.6400 R:53.0000 rate:0.1060 aloss:1.4077 dloss:0.4789 aloss2:5.3524 exploreP:0.4419\n",
      "Episode:248 meanR:50.0700 R:76.0000 rate:0.1520 aloss:1.4001 dloss:0.4942 aloss2:5.3548 exploreP:0.4387\n",
      "Episode:249 meanR:50.3200 R:51.0000 rate:0.1020 aloss:1.3949 dloss:0.5045 aloss2:5.3614 exploreP:0.4365\n",
      "Episode:250 meanR:50.3700 R:62.0000 rate:0.1240 aloss:1.3951 dloss:0.4976 aloss2:5.3360 exploreP:0.4339\n",
      "Episode:251 meanR:50.7400 R:71.0000 rate:0.1420 aloss:1.3977 dloss:0.5220 aloss2:5.3184 exploreP:0.4309\n",
      "Episode:252 meanR:51.2000 R:64.0000 rate:0.1280 aloss:1.4012 dloss:0.4959 aloss2:5.3160 exploreP:0.4282\n",
      "Episode:253 meanR:51.7500 R:74.0000 rate:0.1480 aloss:1.4101 dloss:0.4806 aloss2:5.3299 exploreP:0.4251\n",
      "Episode:254 meanR:52.7600 R:145.0000 rate:0.2900 aloss:1.4063 dloss:0.5036 aloss2:5.3322 exploreP:0.4191\n",
      "Episode:255 meanR:52.3700 R:21.0000 rate:0.0420 aloss:1.3967 dloss:0.4793 aloss2:5.3388 exploreP:0.4183\n",
      "Episode:256 meanR:53.4300 R:167.0000 rate:0.3340 aloss:1.4152 dloss:0.5125 aloss2:5.3170 exploreP:0.4115\n",
      "Episode:257 meanR:53.8800 R:68.0000 rate:0.1360 aloss:1.4115 dloss:0.5146 aloss2:5.2896 exploreP:0.4088\n",
      "Episode:258 meanR:54.6400 R:97.0000 rate:0.1940 aloss:1.3985 dloss:0.4999 aloss2:5.2897 exploreP:0.4049\n",
      "Episode:259 meanR:55.8500 R:139.0000 rate:0.2780 aloss:1.4151 dloss:0.5089 aloss2:5.2890 exploreP:0.3995\n",
      "Episode:260 meanR:56.9500 R:147.0000 rate:0.2940 aloss:1.4189 dloss:0.4921 aloss2:5.3032 exploreP:0.3938\n",
      "Episode:261 meanR:57.7000 R:115.0000 rate:0.2300 aloss:1.4152 dloss:0.4985 aloss2:5.3596 exploreP:0.3894\n",
      "Episode:262 meanR:58.3300 R:75.0000 rate:0.1500 aloss:1.4180 dloss:0.4981 aloss2:5.3339 exploreP:0.3866\n",
      "Episode:263 meanR:58.6100 R:85.0000 rate:0.1700 aloss:1.4036 dloss:0.4965 aloss2:5.3260 exploreP:0.3834\n",
      "Episode:264 meanR:59.4500 R:124.0000 rate:0.2480 aloss:1.4125 dloss:0.5109 aloss2:5.3055 exploreP:0.3788\n",
      "Episode:265 meanR:60.0800 R:95.0000 rate:0.1900 aloss:1.4036 dloss:0.5039 aloss2:5.3257 exploreP:0.3753\n",
      "Episode:266 meanR:60.2700 R:51.0000 rate:0.1020 aloss:1.4206 dloss:0.5171 aloss2:5.3098 exploreP:0.3734\n",
      "Episode:267 meanR:60.4600 R:57.0000 rate:0.1140 aloss:1.4080 dloss:0.5287 aloss2:5.2635 exploreP:0.3714\n",
      "Episode:268 meanR:61.1800 R:86.0000 rate:0.1720 aloss:1.4296 dloss:0.5197 aloss2:5.2628 exploreP:0.3683\n",
      "Episode:269 meanR:61.9100 R:121.0000 rate:0.2420 aloss:1.4171 dloss:0.5190 aloss2:5.2681 exploreP:0.3640\n",
      "Episode:270 meanR:62.1800 R:65.0000 rate:0.1300 aloss:1.4372 dloss:0.4870 aloss2:5.2687 exploreP:0.3617\n",
      "Episode:271 meanR:62.7500 R:69.0000 rate:0.1380 aloss:1.4518 dloss:0.5246 aloss2:5.3037 exploreP:0.3593\n",
      "Episode:272 meanR:62.9700 R:104.0000 rate:0.2080 aloss:1.4291 dloss:0.5118 aloss2:5.2813 exploreP:0.3556\n",
      "Episode:273 meanR:63.7300 R:90.0000 rate:0.1800 aloss:1.4002 dloss:0.5109 aloss2:5.2845 exploreP:0.3525\n",
      "Episode:274 meanR:64.0600 R:70.0000 rate:0.1400 aloss:1.4089 dloss:0.5182 aloss2:5.2481 exploreP:0.3502\n",
      "Episode:275 meanR:64.5600 R:74.0000 rate:0.1480 aloss:1.4162 dloss:0.5232 aloss2:5.2357 exploreP:0.3476\n",
      "Episode:276 meanR:65.5700 R:112.0000 rate:0.2240 aloss:1.4073 dloss:0.5051 aloss2:5.2527 exploreP:0.3439\n",
      "Episode:277 meanR:65.9700 R:66.0000 rate:0.1320 aloss:1.4183 dloss:0.4879 aloss2:5.2867 exploreP:0.3417\n",
      "Episode:278 meanR:65.5000 R:57.0000 rate:0.1140 aloss:1.4322 dloss:0.5015 aloss2:5.3072 exploreP:0.3398\n",
      "Episode:279 meanR:66.8400 R:163.0000 rate:0.3260 aloss:1.4254 dloss:0.4866 aloss2:5.3256 exploreP:0.3345\n",
      "Episode:280 meanR:67.2200 R:64.0000 rate:0.1280 aloss:1.4187 dloss:0.4996 aloss2:5.3704 exploreP:0.3324\n",
      "Episode:281 meanR:67.2900 R:63.0000 rate:0.1260 aloss:1.4021 dloss:0.5175 aloss2:5.3653 exploreP:0.3304\n",
      "Episode:282 meanR:67.5400 R:58.0000 rate:0.1160 aloss:1.4220 dloss:0.5087 aloss2:5.3241 exploreP:0.3285\n",
      "Episode:283 meanR:67.5200 R:105.0000 rate:0.2100 aloss:1.4278 dloss:0.5210 aloss2:5.2997 exploreP:0.3252\n",
      "Episode:284 meanR:68.0300 R:79.0000 rate:0.1580 aloss:1.4296 dloss:0.5174 aloss2:5.2960 exploreP:0.3227\n",
      "Episode:285 meanR:67.2700 R:45.0000 rate:0.0900 aloss:1.4322 dloss:0.5170 aloss2:5.2849 exploreP:0.3213\n",
      "Episode:286 meanR:67.2200 R:78.0000 rate:0.1560 aloss:1.4092 dloss:0.5571 aloss2:5.2647 exploreP:0.3189\n",
      "Episode:287 meanR:67.9300 R:114.0000 rate:0.2280 aloss:1.4328 dloss:0.5079 aloss2:5.2066 exploreP:0.3154\n",
      "Episode:288 meanR:67.8900 R:68.0000 rate:0.1360 aloss:1.4250 dloss:0.5001 aloss2:5.2608 exploreP:0.3133\n",
      "Episode:289 meanR:68.2000 R:69.0000 rate:0.1380 aloss:1.4320 dloss:0.4801 aloss2:5.2900 exploreP:0.3112\n",
      "Episode:290 meanR:68.7700 R:85.0000 rate:0.1700 aloss:1.4137 dloss:0.5181 aloss2:5.3202 exploreP:0.3087\n",
      "Episode:291 meanR:68.8400 R:39.0000 rate:0.0780 aloss:1.4813 dloss:0.4733 aloss2:5.3189 exploreP:0.3075\n",
      "Episode:292 meanR:69.0000 R:65.0000 rate:0.1300 aloss:1.4304 dloss:0.4736 aloss2:5.3502 exploreP:0.3056\n",
      "Episode:293 meanR:68.8400 R:48.0000 rate:0.0960 aloss:1.4257 dloss:0.4877 aloss2:5.3644 exploreP:0.3042\n",
      "Episode:294 meanR:69.0700 R:76.0000 rate:0.1520 aloss:1.4200 dloss:0.5142 aloss2:5.3750 exploreP:0.3020\n",
      "Episode:295 meanR:69.1600 R:100.0000 rate:0.2000 aloss:1.4397 dloss:0.4904 aloss2:5.3505 exploreP:0.2991\n",
      "Episode:296 meanR:69.0900 R:36.0000 rate:0.0720 aloss:1.4493 dloss:0.4866 aloss2:5.3376 exploreP:0.2980\n",
      "Episode:297 meanR:69.3900 R:70.0000 rate:0.1400 aloss:1.4486 dloss:0.4967 aloss2:5.3513 exploreP:0.2960\n",
      "Episode:298 meanR:69.3000 R:39.0000 rate:0.0780 aloss:1.4249 dloss:0.5097 aloss2:5.3684 exploreP:0.2949\n",
      "Episode:299 meanR:70.1700 R:107.0000 rate:0.2140 aloss:1.4186 dloss:0.5052 aloss2:5.3437 exploreP:0.2919\n",
      "Episode:300 meanR:70.6100 R:71.0000 rate:0.1420 aloss:1.4162 dloss:0.5369 aloss2:5.3322 exploreP:0.2899\n",
      "Episode:301 meanR:70.9400 R:58.0000 rate:0.1160 aloss:1.4011 dloss:0.5432 aloss2:5.2617 exploreP:0.2882\n",
      "Episode:302 meanR:71.3600 R:95.0000 rate:0.1900 aloss:1.4017 dloss:0.5044 aloss2:5.2530 exploreP:0.2856\n",
      "Episode:303 meanR:71.9700 R:117.0000 rate:0.2340 aloss:1.4022 dloss:0.5347 aloss2:5.2559 exploreP:0.2824\n",
      "Episode:304 meanR:72.8800 R:122.0000 rate:0.2440 aloss:1.4310 dloss:0.5111 aloss2:5.2589 exploreP:0.2791\n",
      "Episode:305 meanR:73.5800 R:108.0000 rate:0.2160 aloss:1.4340 dloss:0.5123 aloss2:5.2620 exploreP:0.2762\n",
      "Episode:306 meanR:74.1500 R:83.0000 rate:0.1660 aloss:1.4454 dloss:0.4905 aloss2:5.2722 exploreP:0.2740\n",
      "Episode:307 meanR:74.8200 R:96.0000 rate:0.1920 aloss:1.4131 dloss:0.5186 aloss2:5.2865 exploreP:0.2715\n",
      "Episode:308 meanR:75.2300 R:57.0000 rate:0.1140 aloss:1.4286 dloss:0.5001 aloss2:5.3002 exploreP:0.2700\n",
      "Episode:309 meanR:75.3200 R:55.0000 rate:0.1100 aloss:1.4442 dloss:0.4926 aloss2:5.3015 exploreP:0.2686\n",
      "Episode:310 meanR:76.1300 R:136.0000 rate:0.2720 aloss:1.4138 dloss:0.5142 aloss2:5.2765 exploreP:0.2651\n",
      "Episode:311 meanR:76.1300 R:41.0000 rate:0.0820 aloss:1.4124 dloss:0.5206 aloss2:5.2941 exploreP:0.2640\n",
      "Episode:312 meanR:76.5400 R:67.0000 rate:0.1340 aloss:1.4224 dloss:0.5250 aloss2:5.2736 exploreP:0.2623\n",
      "Episode:313 meanR:76.9600 R:111.0000 rate:0.2220 aloss:1.4134 dloss:0.5321 aloss2:5.2740 exploreP:0.2596\n",
      "Episode:314 meanR:77.8100 R:132.0000 rate:0.2640 aloss:1.4352 dloss:0.4967 aloss2:5.2666 exploreP:0.2563\n",
      "Episode:315 meanR:77.8800 R:112.0000 rate:0.2240 aloss:1.4259 dloss:0.4990 aloss2:5.3092 exploreP:0.2535\n",
      "Episode:316 meanR:78.5400 R:105.0000 rate:0.2100 aloss:1.4176 dloss:0.5412 aloss2:5.2978 exploreP:0.2510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:317 meanR:79.0900 R:80.0000 rate:0.1600 aloss:1.4268 dloss:0.4980 aloss2:5.2603 exploreP:0.2491\n",
      "Episode:318 meanR:79.4800 R:117.0000 rate:0.2340 aloss:1.4142 dloss:0.5314 aloss2:5.2693 exploreP:0.2463\n",
      "Episode:319 meanR:80.2700 R:134.0000 rate:0.2680 aloss:1.4092 dloss:0.5346 aloss2:5.2445 exploreP:0.2432\n",
      "Episode:320 meanR:80.5100 R:102.0000 rate:0.2040 aloss:1.4167 dloss:0.5198 aloss2:5.2137 exploreP:0.2408\n",
      "Episode:321 meanR:80.6000 R:129.0000 rate:0.2580 aloss:1.4237 dloss:0.5231 aloss2:5.1950 exploreP:0.2378\n",
      "Episode:322 meanR:81.1500 R:96.0000 rate:0.1920 aloss:1.4248 dloss:0.4906 aloss2:5.2531 exploreP:0.2357\n",
      "Episode:323 meanR:81.6800 R:95.0000 rate:0.1900 aloss:1.4493 dloss:0.4972 aloss2:5.2873 exploreP:0.2335\n",
      "Episode:324 meanR:81.2800 R:85.0000 rate:0.1700 aloss:1.4062 dloss:0.5222 aloss2:5.3143 exploreP:0.2316\n",
      "Episode:325 meanR:82.0000 R:116.0000 rate:0.2320 aloss:1.4081 dloss:0.5411 aloss2:5.2790 exploreP:0.2291\n",
      "Episode:326 meanR:83.3900 R:151.0000 rate:0.3020 aloss:1.4194 dloss:0.5303 aloss2:5.2051 exploreP:0.2258\n",
      "Episode:327 meanR:82.3200 R:76.0000 rate:0.1520 aloss:1.4196 dloss:0.5343 aloss2:5.2007 exploreP:0.2242\n",
      "Episode:328 meanR:83.1100 R:115.0000 rate:0.2300 aloss:1.4225 dloss:0.5142 aloss2:5.2426 exploreP:0.2217\n",
      "Episode:329 meanR:84.4800 R:214.0000 rate:0.4280 aloss:1.4185 dloss:0.5355 aloss2:5.2180 exploreP:0.2172\n",
      "Episode:330 meanR:85.1000 R:86.0000 rate:0.1720 aloss:1.4092 dloss:0.5216 aloss2:5.1983 exploreP:0.2155\n",
      "Episode:331 meanR:85.4900 R:88.0000 rate:0.1760 aloss:1.4073 dloss:0.5269 aloss2:5.2242 exploreP:0.2137\n",
      "Episode:332 meanR:86.1500 R:115.0000 rate:0.2300 aloss:1.4099 dloss:0.5477 aloss2:5.1821 exploreP:0.2113\n",
      "Episode:333 meanR:86.6800 R:118.0000 rate:0.2360 aloss:1.4186 dloss:0.5190 aloss2:5.1827 exploreP:0.2090\n",
      "Episode:334 meanR:85.7900 R:30.0000 rate:0.0600 aloss:1.4194 dloss:0.5339 aloss2:5.2080 exploreP:0.2084\n",
      "Episode:335 meanR:86.0300 R:95.0000 rate:0.1900 aloss:1.4262 dloss:0.5427 aloss2:5.2107 exploreP:0.2065\n",
      "Episode:336 meanR:86.2800 R:88.0000 rate:0.1760 aloss:1.4058 dloss:0.5645 aloss2:5.1628 exploreP:0.2048\n",
      "Episode:337 meanR:86.1100 R:84.0000 rate:0.1680 aloss:1.4205 dloss:0.5424 aloss2:5.1492 exploreP:0.2031\n",
      "Episode:338 meanR:87.0100 R:104.0000 rate:0.2080 aloss:1.4184 dloss:0.5476 aloss2:5.1243 exploreP:0.2011\n",
      "Episode:339 meanR:87.6400 R:144.0000 rate:0.2880 aloss:1.4197 dloss:0.5582 aloss2:5.1206 exploreP:0.1984\n",
      "Episode:340 meanR:87.8400 R:111.0000 rate:0.2220 aloss:1.4180 dloss:0.5320 aloss2:5.1219 exploreP:0.1963\n",
      "Episode:341 meanR:89.0200 R:184.0000 rate:0.3680 aloss:1.4019 dloss:0.5380 aloss2:5.1416 exploreP:0.1929\n",
      "Episode:342 meanR:89.1900 R:104.0000 rate:0.2080 aloss:1.4099 dloss:0.5499 aloss2:5.1517 exploreP:0.1910\n",
      "Episode:343 meanR:89.4900 R:77.0000 rate:0.1540 aloss:1.4068 dloss:0.5254 aloss2:5.1075 exploreP:0.1896\n",
      "Episode:344 meanR:91.3500 R:247.0000 rate:0.4940 aloss:1.4238 dloss:0.5334 aloss2:5.1729 exploreP:0.1853\n",
      "Episode:345 meanR:92.0100 R:105.0000 rate:0.2100 aloss:1.4235 dloss:0.5524 aloss2:5.1821 exploreP:0.1834\n",
      "Episode:346 meanR:93.1800 R:140.0000 rate:0.2800 aloss:1.4193 dloss:0.5560 aloss2:5.1257 exploreP:0.1810\n",
      "Episode:347 meanR:93.4600 R:81.0000 rate:0.1620 aloss:1.3976 dloss:0.5856 aloss2:5.0894 exploreP:0.1796\n",
      "Episode:348 meanR:94.7900 R:209.0000 rate:0.4180 aloss:1.4201 dloss:0.5376 aloss2:5.0914 exploreP:0.1761\n",
      "Episode:349 meanR:95.1500 R:87.0000 rate:0.1740 aloss:1.4239 dloss:0.5149 aloss2:5.1349 exploreP:0.1747\n",
      "Episode:350 meanR:95.7300 R:120.0000 rate:0.2400 aloss:1.4076 dloss:0.5525 aloss2:5.1541 exploreP:0.1727\n",
      "Episode:351 meanR:95.8800 R:86.0000 rate:0.1720 aloss:1.4002 dloss:0.5249 aloss2:5.1225 exploreP:0.1713\n",
      "Episode:352 meanR:96.1300 R:89.0000 rate:0.1780 aloss:1.4173 dloss:0.5311 aloss2:5.1606 exploreP:0.1699\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "# aloss_list, dloss_list, aloss2_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0 # each episode\n",
    "        aloss_batch, dloss_batch, aloss2_batch = [], [], []\n",
    "        state = env.reset() # each episode\n",
    "        num_step = 0 # each episode\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "                action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rate = -1\n",
    "            memory.buffer.append([state, action, next_state, reward, float(done), rate])\n",
    "            num_step += 1 # momory added\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Rating the memory\n",
    "            if done is True:\n",
    "                rate = total_reward/500 # update rate at the end/ when episode is done\n",
    "                for idx in range(num_step): # episode length\n",
    "                    if memory.buffer[-1-idx][-1] == -1: # double-check the landmark/marked indexes\n",
    "                        memory.buffer[-1-idx][-1] = rate # rate the trajectory/data\n",
    "                        \n",
    "            # Training with the maxrated minibatch\n",
    "            batch = memory.buffer\n",
    "            #for idx in range(memory_size// batch_size):\n",
    "            idx = np.random.choice(np.arange(memory_size// batch_size))\n",
    "            states = np.array([each[0] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            actions = np.array([each[1] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            next_states = np.array([each[2] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rewards = np.array([each[3] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            dones = np.array([each[4] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            rates = np.array([each[5] for each in batch])[idx*batch_size:(idx+1)*batch_size]\n",
    "            states = states[rates >= np.max(rates)]\n",
    "            actions = actions[rates >= np.max(rates)]\n",
    "            next_states = next_states[rates >= np.max(rates)]\n",
    "            rewards = rewards[rates >= np.max(rates)]\n",
    "            dones = dones[rates >= np.max(rates)]\n",
    "            rates = rates[rates >= np.max(rates)]\n",
    "            aloss, dloss, _, _ = sess.run([model.a_loss, model.d_loss, model.a_opt, model.d_opt],\n",
    "                                          feed_dict = {model.states: states, \n",
    "                                                       model.actions: actions,\n",
    "                                                       model.next_states: next_states,\n",
    "                                                       model.rewards: rewards,\n",
    "                                                       model.dones: dones,\n",
    "                                                       model.rates: rates})\n",
    "            aloss2, _, dones__ = sess.run([model.a_loss2, model.a_opt2, model.dones__], \n",
    "                                 feed_dict = {model.states: states, \n",
    "                                              model.actions: actions,\n",
    "                                              model.next_states: next_states,\n",
    "                                              model.rewards: rewards,\n",
    "                                              model.dones: dones,\n",
    "                                              model.rates: rates})\n",
    "            #print(dones__)\n",
    "            aloss_batch.append(aloss)\n",
    "            dloss_batch.append(dloss)\n",
    "            aloss2_batch.append(aloss2)\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'rate:{:.4f}'.format(rate),\n",
    "              'aloss:{:.4f}'.format(np.mean(aloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'aloss2:{:.4f}'.format(np.mean(aloss2_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        # gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        # dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 481.0\n",
      "total_reward: 481.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print('total_reward: {}'.format(total_reward))\n",
    "                break\n",
    "                \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
