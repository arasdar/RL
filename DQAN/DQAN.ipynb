{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQAN: DQN (Deep Q-Nets) + GAN (Gen. Adv. Nets)\n",
    "\n",
    "In this notebook, we'll combine a DQN (deep Q-net) with GAN (generative adverserial net) that can learn to play games through reinforcement learning without any reward function. We'll call this network DQAN (deep Q adverserial net). \n",
    "Adverserial nets learn to maximize the current reward based the past rewards.\n",
    "Q-net learns to maximize the future rewards based on the current reward.\n",
    "Given a task and known when the task is done or failed, we should be able to learn the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[ 0.00222694 -0.22294398 -0.01775856  0.29077372] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00223194 -0.02757337 -0.01194309 -0.00745672] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00278341  0.16771781 -0.01209222 -0.30388378] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00057095 -0.02722974 -0.0181699  -0.01503886] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 2.63533246e-05  1.68148013e-01 -1.84706775e-02 -3.13398773e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00338931  0.36352815 -0.02473865 -0.61184906] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01065988  0.55898696 -0.03697563 -0.91221999] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02183962  0.75458915 -0.05522003 -1.21629107] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.0369314   0.95037813 -0.07954586 -1.52575334] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.05593896  1.14636506 -0.11006092 -1.84216597] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.146365057513399 -1.8421659689688057\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    # Given data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # Target Q values for training\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, state_size, action_size, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(actions, action_size, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=True)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=True)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # logits for loss and reward/prob/out\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, action_size, hidden_size, state_size, targetQs, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param states: real input states or observations given\n",
    "    :param actions: real actions given\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # The fake/generated actions\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size, \n",
    "                               state_size=state_size)\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    d_logits_fake = discriminator(actions=actions_fake, hidden_size=hidden_size, action_size=action_size)\n",
    "\n",
    "    # The real onehot encoded actions\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    d_logits_real = discriminator(actions=actions_real, hidden_size=hidden_size, action_size=action_size, \n",
    "                                  reuse=True)\n",
    "\n",
    "    # Training the rewarding function\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Train the generate to maximize the current reward 0-1\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Train the generator to maximize the future rewards: Bellman equations: loss (targetQ - Q)^2\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # The generated rewards for Bellman equation\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "\n",
    "    return d_loss, g_loss, q_loss, actions_logits, Qs, rewards_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for reward function\n",
    "    :param g_loss: Generator/Q-value loss Tensor for action & next state predicton\n",
    "    :param q_loss: Value loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=g_vars)\n",
    "\n",
    "    return d_opt, g_opt, q_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.d_loss, self.g_loss, self.q_loss, self.actions_logits, self.Qs, self.rewards_fake = model_loss(\n",
    "            state_size=state_size, action_size=action_size, actions=self.actions, states=self.states, \n",
    "            hidden_size=hidden_size, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.d_opt, self.g_opt, self.q_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, \n",
    "                                                       q_loss=self.q_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 2000          # max number of episodes to learn from\n",
    "max_steps = 200               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = DQAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 5.0 Average reward fake: 0.5388237833976746 Training d_loss: 1.3556 Training g_loss: 0.6301 Training q_loss: 0.5420 Explore P: 0.9995\n",
      "Episode: 1 Total reward: 12.0 Average reward fake: 0.49096670746803284 Training d_loss: 1.2933 Training g_loss: 0.7199 Training q_loss: 0.9145 Explore P: 0.9983\n",
      "Episode: 2 Total reward: 20.0 Average reward fake: 0.4301253855228424 Training d_loss: 1.1584 Training g_loss: 0.8514 Training q_loss: 2.1964 Explore P: 0.9963\n",
      "Episode: 3 Total reward: 15.0 Average reward fake: 0.4589077830314636 Training d_loss: 1.1449 Training g_loss: 0.7866 Training q_loss: 2.7261 Explore P: 0.9949\n",
      "Episode: 4 Total reward: 15.0 Average reward fake: 0.5174843072891235 Training d_loss: 1.2871 Training g_loss: 0.6998 Training q_loss: 5.9500 Explore P: 0.9934\n",
      "Episode: 5 Total reward: 16.0 Average reward fake: 0.4305177628993988 Training d_loss: 1.2738 Training g_loss: 0.8692 Training q_loss: 7.6100 Explore P: 0.9918\n",
      "Episode: 6 Total reward: 13.0 Average reward fake: 0.5037295818328857 Training d_loss: 1.3481 Training g_loss: 0.6862 Training q_loss: 25.7198 Explore P: 0.9905\n",
      "Episode: 7 Total reward: 10.0 Average reward fake: 0.452383428812027 Training d_loss: 1.2550 Training g_loss: 0.8269 Training q_loss: 45.5000 Explore P: 0.9896\n",
      "Episode: 8 Total reward: 13.0 Average reward fake: 0.48877501487731934 Training d_loss: 1.2589 Training g_loss: 0.7285 Training q_loss: 90.5145 Explore P: 0.9883\n",
      "Episode: 9 Total reward: 26.0 Average reward fake: 0.507609486579895 Training d_loss: 1.2830 Training g_loss: 0.6987 Training q_loss: 204.6315 Explore P: 0.9857\n",
      "Episode: 10 Total reward: 18.0 Average reward fake: 0.46319857239723206 Training d_loss: 1.3257 Training g_loss: 0.7747 Training q_loss: 512.8188 Explore P: 0.9840\n",
      "Episode: 11 Total reward: 17.0 Average reward fake: 0.5031522512435913 Training d_loss: 1.4142 Training g_loss: 0.7095 Training q_loss: 180.6910 Explore P: 0.9823\n",
      "Episode: 12 Total reward: 13.0 Average reward fake: 0.514363169670105 Training d_loss: 1.5611 Training g_loss: 0.6940 Training q_loss: 220.4801 Explore P: 0.9811\n",
      "Episode: 13 Total reward: 26.0 Average reward fake: 0.45973873138427734 Training d_loss: 1.3106 Training g_loss: 0.7948 Training q_loss: 551.8180 Explore P: 0.9786\n",
      "Episode: 14 Total reward: 11.0 Average reward fake: 0.4910677969455719 Training d_loss: 1.3215 Training g_loss: 0.7568 Training q_loss: 830.4457 Explore P: 0.9775\n",
      "Episode: 15 Total reward: 12.0 Average reward fake: 0.5253003239631653 Training d_loss: 1.4071 Training g_loss: 0.6762 Training q_loss: 2566.6716 Explore P: 0.9763\n",
      "Episode: 16 Total reward: 30.0 Average reward fake: 0.5068828463554382 Training d_loss: 1.3178 Training g_loss: 0.6790 Training q_loss: 17584.7695 Explore P: 0.9734\n",
      "Episode: 17 Total reward: 9.0 Average reward fake: 0.48073989152908325 Training d_loss: 1.3061 Training g_loss: 0.7480 Training q_loss: 621.8243 Explore P: 0.9726\n",
      "Episode: 18 Total reward: 9.0 Average reward fake: 0.5062569975852966 Training d_loss: 1.4356 Training g_loss: 0.6868 Training q_loss: 7623.1641 Explore P: 0.9717\n",
      "Episode: 19 Total reward: 18.0 Average reward fake: 0.47465386986732483 Training d_loss: 1.2666 Training g_loss: 0.7620 Training q_loss: 2335.8293 Explore P: 0.9700\n",
      "Episode: 20 Total reward: 24.0 Average reward fake: 0.4643831253051758 Training d_loss: 1.4629 Training g_loss: 0.7908 Training q_loss: 1822.9945 Explore P: 0.9677\n",
      "Episode: 21 Total reward: 33.0 Average reward fake: 0.471569299697876 Training d_loss: 1.3376 Training g_loss: 0.7361 Training q_loss: 1915.2682 Explore P: 0.9645\n",
      "Episode: 22 Total reward: 22.0 Average reward fake: 0.4487701952457428 Training d_loss: 1.2613 Training g_loss: 0.8368 Training q_loss: 1095.1991 Explore P: 0.9624\n",
      "Episode: 23 Total reward: 9.0 Average reward fake: 0.44969066977500916 Training d_loss: 1.3484 Training g_loss: 0.8315 Training q_loss: 1493.6975 Explore P: 0.9616\n",
      "Episode: 24 Total reward: 17.0 Average reward fake: 0.4489498734474182 Training d_loss: 1.2298 Training g_loss: 0.8061 Training q_loss: 18641.2930 Explore P: 0.9599\n",
      "Episode: 25 Total reward: 15.0 Average reward fake: 0.47957438230514526 Training d_loss: 1.3003 Training g_loss: 0.7597 Training q_loss: 992.4158 Explore P: 0.9585\n",
      "Episode: 26 Total reward: 13.0 Average reward fake: 0.48974642157554626 Training d_loss: 1.3489 Training g_loss: 0.7490 Training q_loss: 3135.3489 Explore P: 0.9573\n",
      "Episode: 27 Total reward: 27.0 Average reward fake: 0.5145448446273804 Training d_loss: 1.3219 Training g_loss: 0.6854 Training q_loss: 418.3027 Explore P: 0.9547\n",
      "Episode: 28 Total reward: 26.0 Average reward fake: 0.47776341438293457 Training d_loss: 1.2510 Training g_loss: 0.7694 Training q_loss: 638.4117 Explore P: 0.9523\n",
      "Episode: 29 Total reward: 10.0 Average reward fake: 0.371452271938324 Training d_loss: 1.1557 Training g_loss: 1.0166 Training q_loss: 1210.7931 Explore P: 0.9513\n",
      "Episode: 30 Total reward: 15.0 Average reward fake: 0.4327177107334137 Training d_loss: 0.9845 Training g_loss: 0.8738 Training q_loss: 1160.4801 Explore P: 0.9499\n",
      "Episode: 31 Total reward: 16.0 Average reward fake: 0.30053266882896423 Training d_loss: 1.1978 Training g_loss: 1.2070 Training q_loss: 2995.7341 Explore P: 0.9484\n",
      "Episode: 32 Total reward: 34.0 Average reward fake: 0.393248975276947 Training d_loss: 1.0578 Training g_loss: 0.9965 Training q_loss: 59164.1133 Explore P: 0.9452\n",
      "Episode: 33 Total reward: 16.0 Average reward fake: 0.4419288635253906 Training d_loss: 1.3242 Training g_loss: 0.8151 Training q_loss: 1629.4248 Explore P: 0.9437\n",
      "Episode: 34 Total reward: 22.0 Average reward fake: 0.4530559480190277 Training d_loss: 1.3996 Training g_loss: 0.8714 Training q_loss: 7923.7407 Explore P: 0.9417\n",
      "Episode: 35 Total reward: 11.0 Average reward fake: 0.3552398681640625 Training d_loss: 1.0559 Training g_loss: 1.0600 Training q_loss: 2671.9470 Explore P: 0.9407\n",
      "Episode: 36 Total reward: 22.0 Average reward fake: 0.3613009750843048 Training d_loss: 1.3754 Training g_loss: 1.0060 Training q_loss: 7175.2852 Explore P: 0.9386\n",
      "Episode: 37 Total reward: 28.0 Average reward fake: 0.39961162209510803 Training d_loss: 0.9088 Training g_loss: 0.9842 Training q_loss: 32507.7070 Explore P: 0.9360\n",
      "Episode: 38 Total reward: 11.0 Average reward fake: 0.31784695386886597 Training d_loss: 0.9079 Training g_loss: 1.1342 Training q_loss: 6695.6538 Explore P: 0.9350\n",
      "Episode: 39 Total reward: 12.0 Average reward fake: 0.3231564164161682 Training d_loss: 0.9962 Training g_loss: 1.1248 Training q_loss: 5760.7246 Explore P: 0.9339\n",
      "Episode: 40 Total reward: 50.0 Average reward fake: 0.3341830372810364 Training d_loss: 0.7633 Training g_loss: 1.1114 Training q_loss: 20325.4199 Explore P: 0.9293\n",
      "Episode: 41 Total reward: 18.0 Average reward fake: 0.33519771695137024 Training d_loss: 1.2191 Training g_loss: 1.0795 Training q_loss: 211057.5938 Explore P: 0.9276\n",
      "Episode: 42 Total reward: 29.0 Average reward fake: 0.34199103713035583 Training d_loss: 0.9790 Training g_loss: 1.0755 Training q_loss: 54503.7305 Explore P: 0.9250\n",
      "Episode: 43 Total reward: 12.0 Average reward fake: 0.4215157926082611 Training d_loss: 1.4574 Training g_loss: 0.8987 Training q_loss: 47726.8594 Explore P: 0.9239\n",
      "Episode: 44 Total reward: 17.0 Average reward fake: 0.3118917644023895 Training d_loss: 0.8607 Training g_loss: 1.1955 Training q_loss: 150471.1406 Explore P: 0.9223\n",
      "Episode: 45 Total reward: 50.0 Average reward fake: 0.3045603632926941 Training d_loss: 1.1990 Training g_loss: 1.1664 Training q_loss: 220977.9062 Explore P: 0.9178\n",
      "Episode: 46 Total reward: 13.0 Average reward fake: 0.3863435387611389 Training d_loss: 0.8750 Training g_loss: 0.9495 Training q_loss: 54648.8047 Explore P: 0.9166\n",
      "Episode: 47 Total reward: 16.0 Average reward fake: 0.28411775827407837 Training d_loss: 0.8435 Training g_loss: 1.2507 Training q_loss: 1242592.3750 Explore P: 0.9152\n",
      "Episode: 48 Total reward: 13.0 Average reward fake: 0.3705276846885681 Training d_loss: 0.7662 Training g_loss: 0.9822 Training q_loss: 252836.2031 Explore P: 0.9140\n",
      "Episode: 49 Total reward: 36.0 Average reward fake: 0.3466513156890869 Training d_loss: 0.8524 Training g_loss: 1.0680 Training q_loss: 4095989.2500 Explore P: 0.9107\n",
      "Episode: 50 Total reward: 10.0 Average reward fake: 0.3610888123512268 Training d_loss: 1.1624 Training g_loss: 1.0100 Training q_loss: 3438439.5000 Explore P: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51 Total reward: 12.0 Average reward fake: 0.3582882881164551 Training d_loss: 0.7544 Training g_loss: 1.0230 Training q_loss: 2180215.5000 Explore P: 0.9088\n",
      "Episode: 52 Total reward: 13.0 Average reward fake: 0.3536151051521301 Training d_loss: 1.0615 Training g_loss: 1.0521 Training q_loss: 544316.8125 Explore P: 0.9076\n",
      "Episode: 53 Total reward: 18.0 Average reward fake: 0.3740648925304413 Training d_loss: 1.0597 Training g_loss: 0.9943 Training q_loss: 258439.9688 Explore P: 0.9060\n",
      "Episode: 54 Total reward: 14.0 Average reward fake: 0.339749276638031 Training d_loss: 1.1717 Training g_loss: 1.0621 Training q_loss: 281842.2188 Explore P: 0.9047\n",
      "Episode: 55 Total reward: 36.0 Average reward fake: 0.3528996407985687 Training d_loss: 1.0610 Training g_loss: 1.0554 Training q_loss: 234653.7188 Explore P: 0.9015\n",
      "Episode: 56 Total reward: 20.0 Average reward fake: 0.38108938932418823 Training d_loss: 1.0594 Training g_loss: 0.9604 Training q_loss: 242256.0469 Explore P: 0.8997\n",
      "Episode: 57 Total reward: 9.0 Average reward fake: 0.36301010847091675 Training d_loss: 0.5539 Training g_loss: 1.0454 Training q_loss: 729307.5000 Explore P: 0.8989\n",
      "Episode: 58 Total reward: 7.0 Average reward fake: 0.29470112919807434 Training d_loss: 1.0829 Training g_loss: 1.2367 Training q_loss: 938022.3125 Explore P: 0.8983\n",
      "Episode: 59 Total reward: 31.0 Average reward fake: 0.34578782320022583 Training d_loss: 0.9560 Training g_loss: 1.0738 Training q_loss: 11439610.0000 Explore P: 0.8955\n",
      "Episode: 60 Total reward: 15.0 Average reward fake: 0.2897419333457947 Training d_loss: 0.7147 Training g_loss: 1.2554 Training q_loss: 3153289.7500 Explore P: 0.8942\n",
      "Episode: 61 Total reward: 15.0 Average reward fake: 0.38311928510665894 Training d_loss: 1.0592 Training g_loss: 0.9537 Training q_loss: 424141.3438 Explore P: 0.8929\n",
      "Episode: 62 Total reward: 37.0 Average reward fake: 0.3238341212272644 Training d_loss: 1.2935 Training g_loss: 1.1284 Training q_loss: 199748.5625 Explore P: 0.8896\n",
      "Episode: 63 Total reward: 24.0 Average reward fake: 0.3747721314430237 Training d_loss: 0.9608 Training g_loss: 0.9872 Training q_loss: 146323.6406 Explore P: 0.8875\n",
      "Episode: 64 Total reward: 30.0 Average reward fake: 0.34389370679855347 Training d_loss: 1.1689 Training g_loss: 1.0671 Training q_loss: 670982.7500 Explore P: 0.8849\n",
      "Episode: 65 Total reward: 14.0 Average reward fake: 0.32909563183784485 Training d_loss: 0.9552 Training g_loss: 1.1034 Training q_loss: 5283991.0000 Explore P: 0.8837\n",
      "Episode: 66 Total reward: 32.0 Average reward fake: 0.2999146580696106 Training d_loss: 1.0794 Training g_loss: 1.1743 Training q_loss: 288912.2812 Explore P: 0.8809\n",
      "Episode: 67 Total reward: 20.0 Average reward fake: 0.3949694037437439 Training d_loss: 1.0601 Training g_loss: 0.9329 Training q_loss: 291563.3125 Explore P: 0.8791\n",
      "Episode: 68 Total reward: 58.0 Average reward fake: 0.35401779413223267 Training d_loss: 0.9564 Training g_loss: 1.0564 Training q_loss: 317290.7812 Explore P: 0.8741\n",
      "Episode: 69 Total reward: 34.0 Average reward fake: 0.3594112992286682 Training d_loss: 0.9572 Training g_loss: 1.0343 Training q_loss: 178398.3594 Explore P: 0.8712\n",
      "Episode: 70 Total reward: 49.0 Average reward fake: 0.3364606499671936 Training d_loss: 1.2817 Training g_loss: 1.0922 Training q_loss: 141634.1250 Explore P: 0.8670\n",
      "Episode: 71 Total reward: 27.0 Average reward fake: 0.41439276933670044 Training d_loss: 1.1519 Training g_loss: 0.8833 Training q_loss: 174700.6406 Explore P: 0.8647\n",
      "Episode: 72 Total reward: 13.0 Average reward fake: 0.3248046040534973 Training d_loss: 1.0676 Training g_loss: 1.1258 Training q_loss: 128979.3750 Explore P: 0.8636\n",
      "Episode: 73 Total reward: 10.0 Average reward fake: 0.3107166588306427 Training d_loss: 0.7230 Training g_loss: 1.1627 Training q_loss: 222135.7500 Explore P: 0.8627\n",
      "Episode: 74 Total reward: 14.0 Average reward fake: 0.3673999309539795 Training d_loss: 0.8586 Training g_loss: 1.0098 Training q_loss: 274337.7500 Explore P: 0.8615\n",
      "Episode: 75 Total reward: 24.0 Average reward fake: 0.36725685000419617 Training d_loss: 0.8586 Training g_loss: 0.9926 Training q_loss: 310226.5938 Explore P: 0.8595\n",
      "Episode: 76 Total reward: 10.0 Average reward fake: 0.3631454408168793 Training d_loss: 1.0591 Training g_loss: 1.0125 Training q_loss: 6351154.0000 Explore P: 0.8586\n",
      "Episode: 77 Total reward: 19.0 Average reward fake: 0.3468347191810608 Training d_loss: 1.0614 Training g_loss: 1.0583 Training q_loss: 253783.9531 Explore P: 0.8570\n",
      "Episode: 78 Total reward: 8.0 Average reward fake: 0.3620004653930664 Training d_loss: 1.0592 Training g_loss: 1.0110 Training q_loss: 271663.0312 Explore P: 0.8563\n",
      "Episode: 79 Total reward: 10.0 Average reward fake: 0.3593418300151825 Training d_loss: 0.9571 Training g_loss: 1.0262 Training q_loss: 360352.6562 Explore P: 0.8555\n",
      "Episode: 80 Total reward: 28.0 Average reward fake: 0.39705222845077515 Training d_loss: 0.8755 Training g_loss: 0.9159 Training q_loss: 315989.2812 Explore P: 0.8531\n",
      "Episode: 81 Total reward: 28.0 Average reward fake: 0.3045971989631653 Training d_loss: 0.7201 Training g_loss: 1.1806 Training q_loss: 5213003.0000 Explore P: 0.8508\n",
      "Episode: 82 Total reward: 13.0 Average reward fake: 0.38981005549430847 Training d_loss: 0.8710 Training g_loss: 0.9372 Training q_loss: 376866.9375 Explore P: 0.8497\n",
      "Episode: 83 Total reward: 25.0 Average reward fake: 0.3061373233795166 Training d_loss: 0.8391 Training g_loss: 1.1931 Training q_loss: 1270640.6250 Explore P: 0.8476\n",
      "Episode: 84 Total reward: 13.0 Average reward fake: 0.33912187814712524 Training d_loss: 1.0631 Training g_loss: 1.0684 Training q_loss: 239218.5469 Explore P: 0.8465\n",
      "Episode: 85 Total reward: 13.0 Average reward fake: 0.34556013345718384 Training d_loss: 0.7429 Training g_loss: 1.0791 Training q_loss: 5907471.0000 Explore P: 0.8454\n",
      "Episode: 86 Total reward: 17.0 Average reward fake: 0.32705098390579224 Training d_loss: 1.4020 Training g_loss: 1.1034 Training q_loss: 6506107.0000 Explore P: 0.8440\n",
      "Episode: 87 Total reward: 19.0 Average reward fake: 0.35281041264533997 Training d_loss: 1.1645 Training g_loss: 1.0515 Training q_loss: 207851.0469 Explore P: 0.8424\n",
      "Episode: 88 Total reward: 13.0 Average reward fake: 0.31834012269973755 Training d_loss: 0.8412 Training g_loss: 1.1478 Training q_loss: 159596.9219 Explore P: 0.8413\n",
      "Episode: 89 Total reward: 8.0 Average reward fake: 0.3445644974708557 Training d_loss: 0.9553 Training g_loss: 1.0539 Training q_loss: 2412401.7500 Explore P: 0.8406\n",
      "Episode: 90 Total reward: 19.0 Average reward fake: 0.3408186733722687 Training d_loss: 1.1703 Training g_loss: 1.0641 Training q_loss: 2360121.5000 Explore P: 0.8391\n",
      "Episode: 91 Total reward: 28.0 Average reward fake: 0.3665221333503723 Training d_loss: 0.9585 Training g_loss: 1.0162 Training q_loss: 167005.0156 Explore P: 0.8368\n",
      "Episode: 92 Total reward: 9.0 Average reward fake: 0.34988588094711304 Training d_loss: 0.9558 Training g_loss: 1.0473 Training q_loss: 192896.5781 Explore P: 0.8360\n",
      "Episode: 93 Total reward: 11.0 Average reward fake: 0.3324585556983948 Training d_loss: 1.0650 Training g_loss: 1.1029 Training q_loss: 3683233.2500 Explore P: 0.8351\n",
      "Episode: 94 Total reward: 12.0 Average reward fake: 0.3249046206474304 Training d_loss: 0.7303 Training g_loss: 1.1274 Training q_loss: 151715.3750 Explore P: 0.8341\n",
      "Episode: 95 Total reward: 19.0 Average reward fake: 0.3003605008125305 Training d_loss: 0.7181 Training g_loss: 1.2110 Training q_loss: 134588.9219 Explore P: 0.8325\n",
      "Episode: 96 Total reward: 26.0 Average reward fake: 0.3998948931694031 Training d_loss: 0.8774 Training g_loss: 0.9317 Training q_loss: 6175367.0000 Explore P: 0.8304\n",
      "Episode: 97 Total reward: 9.0 Average reward fake: 0.32056817412376404 Training d_loss: 1.2967 Training g_loss: 1.1445 Training q_loss: 15942550.0000 Explore P: 0.8297\n",
      "Episode: 98 Total reward: 8.0 Average reward fake: 0.3530001938343048 Training d_loss: 1.0602 Training g_loss: 1.0203 Training q_loss: 118046.8906 Explore P: 0.8290\n",
      "Episode: 99 Total reward: 63.0 Average reward fake: 0.3588722348213196 Training d_loss: 1.0594 Training g_loss: 1.0152 Training q_loss: 117706.2734 Explore P: 0.8239\n",
      "Episode: 100 Total reward: 13.0 Average reward fake: 0.3532070219516754 Training d_loss: 1.0602 Training g_loss: 1.0536 Training q_loss: 64964.1680 Explore P: 0.8228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 101 Total reward: 24.0 Average reward fake: 0.3515334725379944 Training d_loss: 1.0605 Training g_loss: 1.0405 Training q_loss: 5253190.5000 Explore P: 0.8209\n",
      "Episode: 102 Total reward: 19.0 Average reward fake: 0.3589852452278137 Training d_loss: 0.7521 Training g_loss: 1.0335 Training q_loss: 5764673.5000 Explore P: 0.8193\n",
      "Episode: 103 Total reward: 37.0 Average reward fake: 0.37266018986701965 Training d_loss: 0.8612 Training g_loss: 0.9965 Training q_loss: 7451657.5000 Explore P: 0.8163\n",
      "Episode: 104 Total reward: 21.0 Average reward fake: 0.389329731464386 Training d_loss: 0.9649 Training g_loss: 0.9339 Training q_loss: 73707.2266 Explore P: 0.8146\n",
      "Episode: 105 Total reward: 17.0 Average reward fake: 0.33311206102371216 Training d_loss: 1.0647 Training g_loss: 1.1058 Training q_loss: 49363.9727 Explore P: 0.8133\n",
      "Episode: 106 Total reward: 19.0 Average reward fake: 0.3300195336341858 Training d_loss: 0.9549 Training g_loss: 1.1082 Training q_loss: 4491161.0000 Explore P: 0.8118\n",
      "Episode: 107 Total reward: 15.0 Average reward fake: 0.3894714415073395 Training d_loss: 0.6821 Training g_loss: 0.9514 Training q_loss: 74421.1484 Explore P: 0.8106\n",
      "Episode: 108 Total reward: 15.0 Average reward fake: 0.3312801718711853 Training d_loss: 0.9548 Training g_loss: 1.0913 Training q_loss: 103516.2969 Explore P: 0.8094\n",
      "Episode: 109 Total reward: 22.0 Average reward fake: 0.3353636562824249 Training d_loss: 0.6271 Training g_loss: 1.1069 Training q_loss: 61915.7578 Explore P: 0.8076\n",
      "Episode: 110 Total reward: 14.0 Average reward fake: 0.32639676332473755 Training d_loss: 0.9550 Training g_loss: 1.1136 Training q_loss: 78803.7031 Explore P: 0.8065\n",
      "Episode: 111 Total reward: 36.0 Average reward fake: 0.3682428300380707 Training d_loss: 0.8589 Training g_loss: 1.0054 Training q_loss: 2960437.7500 Explore P: 0.8036\n",
      "Episode: 112 Total reward: 10.0 Average reward fake: 0.3439452052116394 Training d_loss: 1.0619 Training g_loss: 1.0640 Training q_loss: 2667244.7500 Explore P: 0.8028\n",
      "Episode: 113 Total reward: 13.0 Average reward fake: 0.331801176071167 Training d_loss: 0.9548 Training g_loss: 1.0892 Training q_loss: 49775.9219 Explore P: 0.8018\n",
      "Episode: 114 Total reward: 36.0 Average reward fake: 0.335634708404541 Training d_loss: 1.0640 Training g_loss: 1.0948 Training q_loss: 66865.3984 Explore P: 0.7990\n",
      "Episode: 115 Total reward: 22.0 Average reward fake: 0.35496169328689575 Training d_loss: 1.1635 Training g_loss: 1.0236 Training q_loss: 7110203.0000 Explore P: 0.7972\n",
      "Episode: 116 Total reward: 14.0 Average reward fake: 0.37234026193618774 Training d_loss: 0.9598 Training g_loss: 0.9900 Training q_loss: 8848846.0000 Explore P: 0.7961\n",
      "Episode: 117 Total reward: 15.0 Average reward fake: 0.32739028334617615 Training d_loss: 1.2899 Training g_loss: 1.1217 Training q_loss: 16281.8623 Explore P: 0.7949\n",
      "Episode: 118 Total reward: 47.0 Average reward fake: 0.35171884298324585 Training d_loss: 1.3739 Training g_loss: 1.0411 Training q_loss: 21902.2051 Explore P: 0.7913\n",
      "Episode: 119 Total reward: 13.0 Average reward fake: 0.35697275400161743 Training d_loss: 0.8536 Training g_loss: 1.0234 Training q_loss: 2650211.5000 Explore P: 0.7902\n",
      "Episode: 120 Total reward: 9.0 Average reward fake: 0.3750544488430023 Training d_loss: 0.8624 Training g_loss: 0.9929 Training q_loss: 2779365.5000 Explore P: 0.7895\n",
      "Episode: 121 Total reward: 18.0 Average reward fake: 0.33642953634262085 Training d_loss: 0.9548 Training g_loss: 1.0840 Training q_loss: 29756.1602 Explore P: 0.7881\n",
      "Episode: 122 Total reward: 19.0 Average reward fake: 0.3301119804382324 Training d_loss: 1.2873 Training g_loss: 1.1121 Training q_loss: 1381571.5000 Explore P: 0.7867\n",
      "Episode: 123 Total reward: 20.0 Average reward fake: 0.38668662309646606 Training d_loss: 1.0590 Training g_loss: 0.9570 Training q_loss: 9548.6094 Explore P: 0.7851\n",
      "Episode: 124 Total reward: 11.0 Average reward fake: 0.3395358920097351 Training d_loss: 0.9549 Training g_loss: 1.0880 Training q_loss: 19391.5781 Explore P: 0.7843\n",
      "Episode: 125 Total reward: 12.0 Average reward fake: 0.3599472641944885 Training d_loss: 0.8550 Training g_loss: 1.0168 Training q_loss: 13257.2500 Explore P: 0.7833\n",
      "Episode: 126 Total reward: 13.0 Average reward fake: 0.3324001133441925 Training d_loss: 0.9548 Training g_loss: 1.1074 Training q_loss: 22105.2695 Explore P: 0.7823\n",
      "Episode: 127 Total reward: 10.0 Average reward fake: 0.3040176033973694 Training d_loss: 0.7197 Training g_loss: 1.2136 Training q_loss: 12350.7168 Explore P: 0.7816\n",
      "Episode: 128 Total reward: 20.0 Average reward fake: 0.32978612184524536 Training d_loss: 0.9548 Training g_loss: 1.0933 Training q_loss: 16803.6172 Explore P: 0.7800\n",
      "Episode: 129 Total reward: 14.0 Average reward fake: 0.3416683077812195 Training d_loss: 0.9550 Training g_loss: 1.0859 Training q_loss: 6838.0601 Explore P: 0.7789\n",
      "Episode: 130 Total reward: 13.0 Average reward fake: 0.3329088091850281 Training d_loss: 1.3947 Training g_loss: 1.0910 Training q_loss: 11206.6064 Explore P: 0.7779\n",
      "Episode: 131 Total reward: 16.0 Average reward fake: 0.3265017867088318 Training d_loss: 1.2907 Training g_loss: 1.1147 Training q_loss: 7750.5249 Explore P: 0.7767\n",
      "Episode: 132 Total reward: 8.0 Average reward fake: 0.35477206110954285 Training d_loss: 0.9563 Training g_loss: 1.0112 Training q_loss: 8864.4893 Explore P: 0.7761\n",
      "Episode: 133 Total reward: 8.0 Average reward fake: 0.3778941035270691 Training d_loss: 0.9612 Training g_loss: 0.9824 Training q_loss: 1527325.1250 Explore P: 0.7755\n",
      "Episode: 134 Total reward: 11.0 Average reward fake: 0.31388548016548157 Training d_loss: 1.0720 Training g_loss: 1.1629 Training q_loss: 1800142.2500 Explore P: 0.7746\n",
      "Episode: 135 Total reward: 50.0 Average reward fake: 0.32196253538131714 Training d_loss: 0.9552 Training g_loss: 1.1353 Training q_loss: 22896.4238 Explore P: 0.7708\n",
      "Episode: 136 Total reward: 16.0 Average reward fake: 0.326164186000824 Training d_loss: 0.9550 Training g_loss: 1.1035 Training q_loss: 17496.4160 Explore P: 0.7696\n",
      "Episode: 137 Total reward: 17.0 Average reward fake: 0.5210402011871338 Training d_loss: 2.5469 Training g_loss: 0.7299 Training q_loss: 3189666.7500 Explore P: 0.7683\n",
      "Episode: 138 Total reward: 18.0 Average reward fake: 0.341336727142334 Training d_loss: 1.1703 Training g_loss: 1.0617 Training q_loss: 11293.8359 Explore P: 0.7670\n",
      "Episode: 139 Total reward: 14.0 Average reward fake: 0.34583452343940735 Training d_loss: 0.8500 Training g_loss: 1.0713 Training q_loss: 995308.6250 Explore P: 0.7659\n",
      "Episode: 140 Total reward: 13.0 Average reward fake: 0.310997873544693 Training d_loss: 1.0739 Training g_loss: 1.1678 Training q_loss: 1671507.8750 Explore P: 0.7649\n",
      "Episode: 141 Total reward: 15.0 Average reward fake: 0.35970526933670044 Training d_loss: 1.1619 Training g_loss: 1.0177 Training q_loss: 9568.5605 Explore P: 0.7638\n",
      "Episode: 142 Total reward: 8.0 Average reward fake: 0.3822351396083832 Training d_loss: 0.7710 Training g_loss: 0.9660 Training q_loss: 9371.6162 Explore P: 0.7632\n",
      "Episode: 143 Total reward: 10.0 Average reward fake: 0.4505007266998291 Training d_loss: 1.4958 Training g_loss: 0.8509 Training q_loss: 1386480.7500 Explore P: 0.7624\n",
      "Episode: 144 Total reward: 17.0 Average reward fake: 0.3801036775112152 Training d_loss: 1.1560 Training g_loss: 0.9744 Training q_loss: 801364.8750 Explore P: 0.7611\n",
      "Episode: 145 Total reward: 33.0 Average reward fake: 0.3552230894565582 Training d_loss: 1.1644 Training g_loss: 1.0200 Training q_loss: 1303607.7500 Explore P: 0.7587\n",
      "Episode: 146 Total reward: 12.0 Average reward fake: 0.3473809063434601 Training d_loss: 1.0633 Training g_loss: 1.0831 Training q_loss: 1064012.5000 Explore P: 0.7578\n",
      "Episode: 147 Total reward: 32.0 Average reward fake: 0.38532280921936035 Training d_loss: 1.0610 Training g_loss: 0.9560 Training q_loss: 19131.4062 Explore P: 0.7554\n",
      "Episode: 148 Total reward: 13.0 Average reward fake: 0.34230881929397583 Training d_loss: 0.9578 Training g_loss: 1.0561 Training q_loss: 3400829.5000 Explore P: 0.7544\n",
      "Episode: 149 Total reward: 16.0 Average reward fake: 0.37749555706977844 Training d_loss: 1.1573 Training g_loss: 0.9694 Training q_loss: 13240.5781 Explore P: 0.7532\n",
      "Episode: 150 Total reward: 9.0 Average reward fake: 0.36462563276290894 Training d_loss: 0.9601 Training g_loss: 1.0228 Training q_loss: 12657.8018 Explore P: 0.7526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 151 Total reward: 14.0 Average reward fake: 0.316053181886673 Training d_loss: 0.9575 Training g_loss: 1.1453 Training q_loss: 2334490.2500 Explore P: 0.7515\n",
      "Episode: 152 Total reward: 13.0 Average reward fake: 0.34241461753845215 Training d_loss: 0.7427 Training g_loss: 1.0736 Training q_loss: 2005002.7500 Explore P: 0.7506\n",
      "Episode: 153 Total reward: 19.0 Average reward fake: 0.3723856806755066 Training d_loss: 1.1580 Training g_loss: 0.9836 Training q_loss: 1083659.3750 Explore P: 0.7492\n",
      "Episode: 154 Total reward: 13.0 Average reward fake: 0.37017446756362915 Training d_loss: 1.1586 Training g_loss: 0.9888 Training q_loss: 1762105.3750 Explore P: 0.7482\n",
      "Episode: 155 Total reward: 14.0 Average reward fake: 0.3570994734764099 Training d_loss: 1.0604 Training g_loss: 1.0388 Training q_loss: 11258.4395 Explore P: 0.7472\n",
      "Episode: 156 Total reward: 16.0 Average reward fake: 0.35888364911079407 Training d_loss: 0.8555 Training g_loss: 1.0079 Training q_loss: 17676.1406 Explore P: 0.7460\n",
      "Episode: 157 Total reward: 8.0 Average reward fake: 0.3632257580757141 Training d_loss: 1.0596 Training g_loss: 1.0218 Training q_loss: 14426.1406 Explore P: 0.7454\n",
      "Episode: 158 Total reward: 11.0 Average reward fake: 0.34454110264778137 Training d_loss: 0.7432 Training g_loss: 1.0683 Training q_loss: 10931.0908 Explore P: 0.7446\n",
      "Episode: 159 Total reward: 44.0 Average reward fake: 0.36398956179618835 Training d_loss: 1.0597 Training g_loss: 1.0161 Training q_loss: 931075.6875 Explore P: 0.7414\n",
      "Episode: 160 Total reward: 16.0 Average reward fake: 0.35361120104789734 Training d_loss: 0.7499 Training g_loss: 1.0461 Training q_loss: 18224.7227 Explore P: 0.7402\n",
      "Episode: 161 Total reward: 12.0 Average reward fake: 0.35775938630104065 Training d_loss: 0.8554 Training g_loss: 1.0274 Training q_loss: 17233.1133 Explore P: 0.7393\n",
      "Episode: 162 Total reward: 17.0 Average reward fake: 0.3885384202003479 Training d_loss: 1.2490 Training g_loss: 0.9312 Training q_loss: 17824.3184 Explore P: 0.7381\n",
      "Episode: 163 Total reward: 43.0 Average reward fake: 0.389814555644989 Training d_loss: 0.9673 Training g_loss: 0.9459 Training q_loss: 852551.3750 Explore P: 0.7350\n",
      "Episode: 164 Total reward: 11.0 Average reward fake: 0.3461838662624359 Training d_loss: 1.1686 Training g_loss: 1.0578 Training q_loss: 1419828.5000 Explore P: 0.7342\n",
      "Episode: 165 Total reward: 14.0 Average reward fake: 0.37090450525283813 Training d_loss: 1.0599 Training g_loss: 0.9796 Training q_loss: 1361859.3750 Explore P: 0.7331\n",
      "Episode: 166 Total reward: 25.0 Average reward fake: 0.3370746970176697 Training d_loss: 1.1731 Training g_loss: 1.0804 Training q_loss: 26040.5352 Explore P: 0.7313\n",
      "Episode: 167 Total reward: 21.0 Average reward fake: 0.36510998010635376 Training d_loss: 0.6576 Training g_loss: 1.0305 Training q_loss: 20640.2969 Explore P: 0.7298\n",
      "Episode: 168 Total reward: 19.0 Average reward fake: 0.36674216389656067 Training d_loss: 1.1596 Training g_loss: 0.9818 Training q_loss: 873295.3125 Explore P: 0.7285\n",
      "Episode: 169 Total reward: 17.0 Average reward fake: 0.38679245114326477 Training d_loss: 0.8700 Training g_loss: 0.9617 Training q_loss: 20633.9844 Explore P: 0.7272\n",
      "Episode: 170 Total reward: 11.0 Average reward fake: 0.3205227255821228 Training d_loss: 0.9561 Training g_loss: 1.1535 Training q_loss: 848340.0000 Explore P: 0.7264\n",
      "Episode: 171 Total reward: 12.0 Average reward fake: 0.3172338008880615 Training d_loss: 0.7271 Training g_loss: 1.1452 Training q_loss: 19858.4395 Explore P: 0.7256\n",
      "Episode: 172 Total reward: 22.0 Average reward fake: 0.38979023694992065 Training d_loss: 1.0598 Training g_loss: 0.9423 Training q_loss: 539472.2500 Explore P: 0.7240\n",
      "Episode: 173 Total reward: 33.0 Average reward fake: 0.36908894777297974 Training d_loss: 1.0595 Training g_loss: 0.9877 Training q_loss: 374721.6875 Explore P: 0.7217\n",
      "Episode: 174 Total reward: 15.0 Average reward fake: 0.372183233499527 Training d_loss: 1.0594 Training g_loss: 0.9962 Training q_loss: 15051.1924 Explore P: 0.7206\n",
      "Episode: 175 Total reward: 25.0 Average reward fake: 0.36837059259414673 Training d_loss: 1.0594 Training g_loss: 0.9959 Training q_loss: 17575.6367 Explore P: 0.7188\n",
      "Episode: 176 Total reward: 11.0 Average reward fake: 0.3577956557273865 Training d_loss: 1.1629 Training g_loss: 1.0330 Training q_loss: 16094.8701 Explore P: 0.7180\n",
      "Episode: 177 Total reward: 15.0 Average reward fake: 0.35774844884872437 Training d_loss: 1.1628 Training g_loss: 1.0205 Training q_loss: 13485.7207 Explore P: 0.7170\n",
      "Episode: 178 Total reward: 16.0 Average reward fake: 0.34778785705566406 Training d_loss: 0.9562 Training g_loss: 1.0536 Training q_loss: 487178.5625 Explore P: 0.7159\n",
      "Episode: 179 Total reward: 14.0 Average reward fake: 0.4497264325618744 Training d_loss: 1.3335 Training g_loss: 1.0908 Training q_loss: 20040.0742 Explore P: 0.7149\n",
      "Episode: 180 Total reward: 12.0 Average reward fake: 0.22995924949645996 Training d_loss: 0.8462 Training g_loss: 1.5904 Training q_loss: 26209.2773 Explore P: 0.7140\n",
      "Episode: 181 Total reward: 13.0 Average reward fake: 0.3254860043525696 Training d_loss: 0.8687 Training g_loss: 1.0771 Training q_loss: 444445.2500 Explore P: 0.7131\n",
      "Episode: 182 Total reward: 10.0 Average reward fake: 0.35698196291923523 Training d_loss: 1.1798 Training g_loss: 1.0479 Training q_loss: 765442.8750 Explore P: 0.7124\n",
      "Episode: 183 Total reward: 19.0 Average reward fake: 0.36753684282302856 Training d_loss: 1.2474 Training g_loss: 1.0781 Training q_loss: 441753.3125 Explore P: 0.7111\n",
      "Episode: 184 Total reward: 14.0 Average reward fake: 0.3315926194190979 Training d_loss: 0.7535 Training g_loss: 1.1100 Training q_loss: 18002.9727 Explore P: 0.7101\n",
      "Episode: 185 Total reward: 11.0 Average reward fake: 0.3244056701660156 Training d_loss: 0.9810 Training g_loss: 1.1191 Training q_loss: 22197.9277 Explore P: 0.7093\n",
      "Episode: 186 Total reward: 10.0 Average reward fake: 0.4897702634334564 Training d_loss: 1.3388 Training g_loss: 0.8363 Training q_loss: 20691.7402 Explore P: 0.7086\n",
      "Episode: 187 Total reward: 43.0 Average reward fake: 0.4842912256717682 Training d_loss: 1.3886 Training g_loss: 0.7310 Training q_loss: 15962.4531 Explore P: 0.7056\n",
      "Episode: 188 Total reward: 35.0 Average reward fake: 0.48047152161598206 Training d_loss: 1.3280 Training g_loss: 0.7536 Training q_loss: 569481.6250 Explore P: 0.7032\n",
      "Episode: 189 Total reward: 44.0 Average reward fake: 0.3990994393825531 Training d_loss: 1.2613 Training g_loss: 0.9261 Training q_loss: 11743.7637 Explore P: 0.7001\n",
      "Episode: 190 Total reward: 23.0 Average reward fake: 0.34973305463790894 Training d_loss: 0.8482 Training g_loss: 1.0598 Training q_loss: 306844.4688 Explore P: 0.6986\n",
      "Episode: 191 Total reward: 14.0 Average reward fake: 0.42241978645324707 Training d_loss: 1.1793 Training g_loss: 0.9847 Training q_loss: 217396.9688 Explore P: 0.6976\n",
      "Episode: 192 Total reward: 11.0 Average reward fake: 0.45717865228652954 Training d_loss: 1.5010 Training g_loss: 0.9150 Training q_loss: 20752.6250 Explore P: 0.6968\n",
      "Episode: 193 Total reward: 11.0 Average reward fake: 0.5159533619880676 Training d_loss: 1.6827 Training g_loss: 0.7468 Training q_loss: 18787.9570 Explore P: 0.6961\n",
      "Episode: 194 Total reward: 13.0 Average reward fake: 0.48421892523765564 Training d_loss: 1.5807 Training g_loss: 0.7445 Training q_loss: 149802.0312 Explore P: 0.6952\n",
      "Episode: 195 Total reward: 21.0 Average reward fake: 0.42876631021499634 Training d_loss: 1.4264 Training g_loss: 0.8480 Training q_loss: 266880.4375 Explore P: 0.6938\n",
      "Episode: 196 Total reward: 13.0 Average reward fake: 0.4318985044956207 Training d_loss: 1.3581 Training g_loss: 0.8393 Training q_loss: 37480.4922 Explore P: 0.6929\n",
      "Episode: 197 Total reward: 10.0 Average reward fake: 0.43217840790748596 Training d_loss: 1.3709 Training g_loss: 0.8385 Training q_loss: 139474.7812 Explore P: 0.6922\n",
      "Episode: 198 Total reward: 10.0 Average reward fake: 0.4275704026222229 Training d_loss: 1.3355 Training g_loss: 0.8520 Training q_loss: 299307.3125 Explore P: 0.6915\n",
      "Episode: 199 Total reward: 10.0 Average reward fake: 0.4183322489261627 Training d_loss: 1.2766 Training g_loss: 0.8745 Training q_loss: 22345.0820 Explore P: 0.6908\n",
      "Episode: 200 Total reward: 11.0 Average reward fake: 0.4046573042869568 Training d_loss: 1.3423 Training g_loss: 0.9062 Training q_loss: 21543.5195 Explore P: 0.6901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 201 Total reward: 15.0 Average reward fake: 0.43040671944618225 Training d_loss: 1.2745 Training g_loss: 0.8579 Training q_loss: 12676.9082 Explore P: 0.6891\n",
      "Episode: 202 Total reward: 10.0 Average reward fake: 0.4212876260280609 Training d_loss: 1.3303 Training g_loss: 0.8844 Training q_loss: 166532.3281 Explore P: 0.6884\n",
      "Episode: 203 Total reward: 8.0 Average reward fake: 0.5832405686378479 Training d_loss: 1.3190 Training g_loss: 0.5857 Training q_loss: 141719.6406 Explore P: 0.6878\n",
      "Episode: 204 Total reward: 11.0 Average reward fake: 0.6719344854354858 Training d_loss: 1.8044 Training g_loss: 0.4134 Training q_loss: 14665.1426 Explore P: 0.6871\n",
      "Episode: 205 Total reward: 32.0 Average reward fake: 0.496285617351532 Training d_loss: 1.3800 Training g_loss: 0.7093 Training q_loss: 121943.5625 Explore P: 0.6849\n",
      "Episode: 206 Total reward: 13.0 Average reward fake: 0.41837078332901 Training d_loss: 1.3572 Training g_loss: 0.8893 Training q_loss: 11143.4424 Explore P: 0.6841\n",
      "Episode: 207 Total reward: 12.0 Average reward fake: 0.3870944082736969 Training d_loss: 1.3614 Training g_loss: 0.9651 Training q_loss: 18148.6602 Explore P: 0.6832\n",
      "Episode: 208 Total reward: 12.0 Average reward fake: 0.4140004515647888 Training d_loss: 1.3537 Training g_loss: 0.9054 Training q_loss: 16071.4707 Explore P: 0.6824\n",
      "Episode: 209 Total reward: 15.0 Average reward fake: 0.43780431151390076 Training d_loss: 1.3470 Training g_loss: 0.8533 Training q_loss: 16398.6016 Explore P: 0.6814\n",
      "Episode: 210 Total reward: 11.0 Average reward fake: 0.4758126139640808 Training d_loss: 1.4310 Training g_loss: 0.7883 Training q_loss: 106003.0156 Explore P: 0.6807\n",
      "Episode: 211 Total reward: 11.0 Average reward fake: 0.5131750106811523 Training d_loss: 1.5103 Training g_loss: 0.7187 Training q_loss: 19484.1914 Explore P: 0.6800\n",
      "Episode: 212 Total reward: 10.0 Average reward fake: 0.4795178771018982 Training d_loss: 1.2378 Training g_loss: 0.7773 Training q_loss: 27764.3750 Explore P: 0.6793\n",
      "Episode: 213 Total reward: 9.0 Average reward fake: 0.4981827735900879 Training d_loss: 1.4736 Training g_loss: 0.7390 Training q_loss: 22201.8652 Explore P: 0.6787\n",
      "Episode: 214 Total reward: 10.0 Average reward fake: 0.4692971110343933 Training d_loss: 1.4050 Training g_loss: 0.7878 Training q_loss: 16085.5029 Explore P: 0.6780\n",
      "Episode: 215 Total reward: 8.0 Average reward fake: 0.5685655474662781 Training d_loss: 1.4535 Training g_loss: 0.5857 Training q_loss: 95882.2969 Explore P: 0.6775\n",
      "Episode: 216 Total reward: 13.0 Average reward fake: 0.525966465473175 Training d_loss: 1.4311 Training g_loss: 0.6540 Training q_loss: 16589.4180 Explore P: 0.6766\n",
      "Episode: 217 Total reward: 16.0 Average reward fake: 0.5441867709159851 Training d_loss: 1.4237 Training g_loss: 0.6132 Training q_loss: 88804.2969 Explore P: 0.6755\n",
      "Episode: 218 Total reward: 23.0 Average reward fake: 0.4931271970272064 Training d_loss: 1.3703 Training g_loss: 0.7109 Training q_loss: 25021.0527 Explore P: 0.6740\n",
      "Episode: 219 Total reward: 16.0 Average reward fake: 0.46497002243995667 Training d_loss: 1.3421 Training g_loss: 0.7691 Training q_loss: 20729.3066 Explore P: 0.6730\n",
      "Episode: 220 Total reward: 14.0 Average reward fake: 0.44442471861839294 Training d_loss: 1.3122 Training g_loss: 0.8128 Training q_loss: 30358.4629 Explore P: 0.6720\n",
      "Episode: 221 Total reward: 14.0 Average reward fake: 0.45195475220680237 Training d_loss: 1.3255 Training g_loss: 0.7982 Training q_loss: 270622.9375 Explore P: 0.6711\n",
      "Episode: 222 Total reward: 9.0 Average reward fake: 0.4402281641960144 Training d_loss: 1.3011 Training g_loss: 0.8249 Training q_loss: 22122.3125 Explore P: 0.6705\n",
      "Episode: 223 Total reward: 15.0 Average reward fake: 0.5008087158203125 Training d_loss: 1.3779 Training g_loss: 0.7006 Training q_loss: 105697.6719 Explore P: 0.6695\n",
      "Episode: 224 Total reward: 10.0 Average reward fake: 0.5221511125564575 Training d_loss: 1.3829 Training g_loss: 0.6561 Training q_loss: 81258.6562 Explore P: 0.6689\n",
      "Episode: 225 Total reward: 12.0 Average reward fake: 0.5495840311050415 Training d_loss: 1.4920 Training g_loss: 0.6016 Training q_loss: 137984.3906 Explore P: 0.6681\n",
      "Episode: 226 Total reward: 11.0 Average reward fake: 0.5205827951431274 Training d_loss: 1.4027 Training g_loss: 0.6543 Training q_loss: 23074.3535 Explore P: 0.6673\n",
      "Episode: 227 Total reward: 31.0 Average reward fake: 0.5205557942390442 Training d_loss: 1.3929 Training g_loss: 0.6534 Training q_loss: 24869.5762 Explore P: 0.6653\n",
      "Episode: 228 Total reward: 11.0 Average reward fake: 0.5190139412879944 Training d_loss: 1.3774 Training g_loss: 0.6559 Training q_loss: 18557.2715 Explore P: 0.6646\n",
      "Episode: 229 Total reward: 11.0 Average reward fake: 0.5245450735092163 Training d_loss: 1.3892 Training g_loss: 0.6450 Training q_loss: 83336.4844 Explore P: 0.6639\n",
      "Episode: 230 Total reward: 11.0 Average reward fake: 0.5078725814819336 Training d_loss: 1.3319 Training g_loss: 0.6782 Training q_loss: 78009.5078 Explore P: 0.6631\n",
      "Episode: 231 Total reward: 16.0 Average reward fake: 0.5142385959625244 Training d_loss: 1.3309 Training g_loss: 0.6666 Training q_loss: 26579.0684 Explore P: 0.6621\n",
      "Episode: 232 Total reward: 11.0 Average reward fake: 0.535139262676239 Training d_loss: 1.3972 Training g_loss: 0.6278 Training q_loss: 91858.0781 Explore P: 0.6614\n",
      "Episode: 233 Total reward: 12.0 Average reward fake: 0.5437406897544861 Training d_loss: 1.3686 Training g_loss: 0.6126 Training q_loss: 175906.6406 Explore P: 0.6606\n",
      "Episode: 234 Total reward: 18.0 Average reward fake: 0.5182204246520996 Training d_loss: 1.3790 Training g_loss: 0.6653 Training q_loss: 73317.5391 Explore P: 0.6594\n",
      "Episode: 235 Total reward: 9.0 Average reward fake: 0.49881115555763245 Training d_loss: 1.3246 Training g_loss: 0.7044 Training q_loss: 331966.6562 Explore P: 0.6589\n",
      "Episode: 236 Total reward: 13.0 Average reward fake: 0.49745163321495056 Training d_loss: 1.3939 Training g_loss: 0.7091 Training q_loss: 19464.3750 Explore P: 0.6580\n",
      "Episode: 237 Total reward: 21.0 Average reward fake: 0.5180010199546814 Training d_loss: 1.4283 Training g_loss: 0.6716 Training q_loss: 18274.6230 Explore P: 0.6566\n",
      "Episode: 238 Total reward: 12.0 Average reward fake: 0.5541041493415833 Training d_loss: 1.4859 Training g_loss: 0.5971 Training q_loss: 25362.8809 Explore P: 0.6559\n",
      "Episode: 239 Total reward: 23.0 Average reward fake: 0.49225297570228577 Training d_loss: 1.4064 Training g_loss: 0.7120 Training q_loss: 195506.0781 Explore P: 0.6544\n",
      "Episode: 240 Total reward: 11.0 Average reward fake: 0.4767734408378601 Training d_loss: 1.3939 Training g_loss: 0.7426 Training q_loss: 14210.8564 Explore P: 0.6537\n",
      "Episode: 241 Total reward: 13.0 Average reward fake: 0.46662601828575134 Training d_loss: 1.3868 Training g_loss: 0.7637 Training q_loss: 22351.1758 Explore P: 0.6528\n",
      "Episode: 242 Total reward: 15.0 Average reward fake: 0.457526832818985 Training d_loss: 1.3857 Training g_loss: 0.7829 Training q_loss: 20291.4102 Explore P: 0.6519\n",
      "Episode: 243 Total reward: 12.0 Average reward fake: 0.451526015996933 Training d_loss: 1.3766 Training g_loss: 0.7960 Training q_loss: 16538.9727 Explore P: 0.6511\n",
      "Episode: 244 Total reward: 19.0 Average reward fake: 0.44082704186439514 Training d_loss: 1.3656 Training g_loss: 0.8204 Training q_loss: 81069.2344 Explore P: 0.6499\n",
      "Episode: 245 Total reward: 10.0 Average reward fake: 0.44245004653930664 Training d_loss: 1.3563 Training g_loss: 0.8176 Training q_loss: 31504.9004 Explore P: 0.6493\n",
      "Episode: 246 Total reward: 14.0 Average reward fake: 0.4303472638130188 Training d_loss: 1.3660 Training g_loss: 0.8438 Training q_loss: 21899.2773 Explore P: 0.6484\n",
      "Episode: 247 Total reward: 17.0 Average reward fake: 0.42977848649024963 Training d_loss: 1.2990 Training g_loss: 0.8514 Training q_loss: 26814.4492 Explore P: 0.6473\n",
      "Episode: 248 Total reward: 14.0 Average reward fake: 0.3994525969028473 Training d_loss: 1.1954 Training g_loss: 0.9206 Training q_loss: 22150.3086 Explore P: 0.6464\n",
      "Episode: 249 Total reward: 10.0 Average reward fake: 0.3872523605823517 Training d_loss: 1.2975 Training g_loss: 0.9512 Training q_loss: 22433.9238 Explore P: 0.6457\n",
      "Episode: 250 Total reward: 16.0 Average reward fake: 0.5322357416152954 Training d_loss: 1.4177 Training g_loss: 0.6622 Training q_loss: 193126.0938 Explore P: 0.6447\n",
      "Episode: 251 Total reward: 14.0 Average reward fake: 0.5945173501968384 Training d_loss: 1.6266 Training g_loss: 0.5363 Training q_loss: 19838.9883 Explore P: 0.6438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 252 Total reward: 11.0 Average reward fake: 0.5905638933181763 Training d_loss: 1.5069 Training g_loss: 0.5356 Training q_loss: 98573.9688 Explore P: 0.6431\n",
      "Episode: 253 Total reward: 26.0 Average reward fake: 0.5471531748771667 Training d_loss: 1.4159 Training g_loss: 0.6053 Training q_loss: 25863.8555 Explore P: 0.6415\n",
      "Episode: 254 Total reward: 14.0 Average reward fake: 0.5356682538986206 Training d_loss: 1.4108 Training g_loss: 0.6253 Training q_loss: 26468.0898 Explore P: 0.6406\n",
      "Episode: 255 Total reward: 16.0 Average reward fake: 0.5294852256774902 Training d_loss: 1.3813 Training g_loss: 0.6366 Training q_loss: 20277.6309 Explore P: 0.6396\n",
      "Episode: 256 Total reward: 8.0 Average reward fake: 0.5288880467414856 Training d_loss: 1.3621 Training g_loss: 0.6375 Training q_loss: 88570.2031 Explore P: 0.6391\n",
      "Episode: 257 Total reward: 20.0 Average reward fake: 0.5318588018417358 Training d_loss: 1.3473 Training g_loss: 0.6332 Training q_loss: 72386.8047 Explore P: 0.6379\n",
      "Episode: 258 Total reward: 27.0 Average reward fake: 0.5353502035140991 Training d_loss: 1.3226 Training g_loss: 0.6321 Training q_loss: 14563.3623 Explore P: 0.6362\n",
      "Episode: 259 Total reward: 7.0 Average reward fake: 0.504588782787323 Training d_loss: 1.3341 Training g_loss: 0.6942 Training q_loss: 23219.8418 Explore P: 0.6357\n",
      "Episode: 260 Total reward: 21.0 Average reward fake: 0.45782676339149475 Training d_loss: 1.3387 Training g_loss: 0.7907 Training q_loss: 76162.2812 Explore P: 0.6344\n",
      "Episode: 261 Total reward: 12.0 Average reward fake: 0.4791782796382904 Training d_loss: 1.4398 Training g_loss: 0.7533 Training q_loss: 83232.2891 Explore P: 0.6337\n",
      "Episode: 262 Total reward: 14.0 Average reward fake: 0.48357000946998596 Training d_loss: 1.3294 Training g_loss: 0.7428 Training q_loss: 18357.3340 Explore P: 0.6328\n",
      "Episode: 263 Total reward: 9.0 Average reward fake: 0.5439110994338989 Training d_loss: 1.4606 Training g_loss: 0.6243 Training q_loss: 22896.9121 Explore P: 0.6322\n",
      "Episode: 264 Total reward: 9.0 Average reward fake: 0.5730989575386047 Training d_loss: 1.5829 Training g_loss: 0.5644 Training q_loss: 28618.2441 Explore P: 0.6317\n",
      "Episode: 265 Total reward: 11.0 Average reward fake: 0.5242518186569214 Training d_loss: 1.4402 Training g_loss: 0.6530 Training q_loss: 22847.8086 Explore P: 0.6310\n",
      "Episode: 266 Total reward: 13.0 Average reward fake: 0.4845658242702484 Training d_loss: 1.4022 Training g_loss: 0.7295 Training q_loss: 25476.1816 Explore P: 0.6302\n",
      "Episode: 267 Total reward: 11.0 Average reward fake: 0.46486911177635193 Training d_loss: 1.3807 Training g_loss: 0.7690 Training q_loss: 60796.0117 Explore P: 0.6295\n",
      "Episode: 268 Total reward: 32.0 Average reward fake: 0.4326418936252594 Training d_loss: 1.3785 Training g_loss: 0.8393 Training q_loss: 180357.7500 Explore P: 0.6275\n",
      "Episode: 269 Total reward: 16.0 Average reward fake: 0.42221444845199585 Training d_loss: 1.3343 Training g_loss: 0.8631 Training q_loss: 69798.1484 Explore P: 0.6265\n",
      "Episode: 270 Total reward: 11.0 Average reward fake: 0.415010541677475 Training d_loss: 1.3899 Training g_loss: 0.8803 Training q_loss: 25171.9453 Explore P: 0.6259\n",
      "Episode: 271 Total reward: 14.0 Average reward fake: 0.3979673683643341 Training d_loss: 1.3244 Training g_loss: 0.9236 Training q_loss: 130858.7266 Explore P: 0.6250\n",
      "Episode: 272 Total reward: 14.0 Average reward fake: 0.3850594162940979 Training d_loss: 0.9414 Training g_loss: 0.9581 Training q_loss: 74356.5469 Explore P: 0.6241\n",
      "Episode: 273 Total reward: 14.0 Average reward fake: 0.35977527499198914 Training d_loss: 0.8702 Training g_loss: 1.0293 Training q_loss: 22421.6465 Explore P: 0.6233\n",
      "Episode: 274 Total reward: 16.0 Average reward fake: 0.3438301980495453 Training d_loss: 0.7475 Training g_loss: 1.0720 Training q_loss: 35220.5898 Explore P: 0.6223\n",
      "Episode: 275 Total reward: 33.0 Average reward fake: 0.37727516889572144 Training d_loss: 0.8656 Training g_loss: 0.9723 Training q_loss: 19592.7793 Explore P: 0.6203\n",
      "Episode: 276 Total reward: 10.0 Average reward fake: 0.3789738714694977 Training d_loss: 0.9630 Training g_loss: 0.9716 Training q_loss: 32042.9316 Explore P: 0.6197\n",
      "Episode: 277 Total reward: 28.0 Average reward fake: 0.3772728443145752 Training d_loss: 1.1567 Training g_loss: 0.9778 Training q_loss: 18294.5820 Explore P: 0.6180\n",
      "Episode: 278 Total reward: 10.0 Average reward fake: 0.37683218717575073 Training d_loss: 1.0594 Training g_loss: 0.9760 Training q_loss: 65969.7734 Explore P: 0.6173\n",
      "Episode: 279 Total reward: 15.0 Average reward fake: 0.36395692825317383 Training d_loss: 1.2615 Training g_loss: 1.0127 Training q_loss: 18303.2227 Explore P: 0.6164\n",
      "Episode: 280 Total reward: 11.0 Average reward fake: 0.3568802773952484 Training d_loss: 1.2661 Training g_loss: 1.0328 Training q_loss: 20001.0195 Explore P: 0.6158\n",
      "Episode: 281 Total reward: 7.0 Average reward fake: 0.35815197229385376 Training d_loss: 0.6501 Training g_loss: 1.0276 Training q_loss: 25931.6719 Explore P: 0.6153\n",
      "Episode: 282 Total reward: 12.0 Average reward fake: 0.361518919467926 Training d_loss: 1.0598 Training g_loss: 1.0170 Training q_loss: 24121.0332 Explore P: 0.6146\n",
      "Episode: 283 Total reward: 11.0 Average reward fake: 0.36111870408058167 Training d_loss: 0.9580 Training g_loss: 1.0223 Training q_loss: 14623.6953 Explore P: 0.6140\n",
      "Episode: 284 Total reward: 9.0 Average reward fake: 0.3463413119316101 Training d_loss: 1.1678 Training g_loss: 1.0633 Training q_loss: 20192.0664 Explore P: 0.6134\n",
      "Episode: 285 Total reward: 30.0 Average reward fake: 0.36164623498916626 Training d_loss: 1.0596 Training g_loss: 1.0151 Training q_loss: 14764.3154 Explore P: 0.6116\n",
      "Episode: 286 Total reward: 13.0 Average reward fake: 0.3593965172767639 Training d_loss: 1.2642 Training g_loss: 1.0240 Training q_loss: 28486.7773 Explore P: 0.6108\n",
      "Episode: 287 Total reward: 9.0 Average reward fake: 0.35297298431396484 Training d_loss: 1.0606 Training g_loss: 1.0397 Training q_loss: 30720.3340 Explore P: 0.6103\n",
      "Episode: 288 Total reward: 16.0 Average reward fake: 0.35745054483413696 Training d_loss: 1.0599 Training g_loss: 1.0295 Training q_loss: 15002.1973 Explore P: 0.6093\n",
      "Episode: 289 Total reward: 18.0 Average reward fake: 0.35171884298324585 Training d_loss: 1.1651 Training g_loss: 1.0427 Training q_loss: 25610.9629 Explore P: 0.6082\n",
      "Episode: 290 Total reward: 21.0 Average reward fake: 0.3463338017463684 Training d_loss: 1.1676 Training g_loss: 1.0602 Training q_loss: 26720.4883 Explore P: 0.6070\n",
      "Episode: 291 Total reward: 34.0 Average reward fake: 0.3453097343444824 Training d_loss: 0.7431 Training g_loss: 1.0644 Training q_loss: 45011.7188 Explore P: 0.6050\n",
      "Episode: 292 Total reward: 10.0 Average reward fake: 0.3441062271595001 Training d_loss: 1.0621 Training g_loss: 1.0627 Training q_loss: 127550.3281 Explore P: 0.6044\n",
      "Episode: 293 Total reward: 12.0 Average reward fake: 0.3603207767009735 Training d_loss: 1.0595 Training g_loss: 1.0164 Training q_loss: 29342.6152 Explore P: 0.6037\n",
      "Episode: 294 Total reward: 25.0 Average reward fake: 0.36327481269836426 Training d_loss: 0.8568 Training g_loss: 1.0151 Training q_loss: 20942.7695 Explore P: 0.6022\n",
      "Episode: 295 Total reward: 29.0 Average reward fake: 0.34805768728256226 Training d_loss: 0.9558 Training g_loss: 1.0542 Training q_loss: 20707.7402 Explore P: 0.6005\n",
      "Episode: 296 Total reward: 19.0 Average reward fake: 0.3448109030723572 Training d_loss: 1.1683 Training g_loss: 1.0620 Training q_loss: 23621.4512 Explore P: 0.5993\n",
      "Episode: 297 Total reward: 13.0 Average reward fake: 0.3449023365974426 Training d_loss: 0.6362 Training g_loss: 1.0687 Training q_loss: 28172.2910 Explore P: 0.5986\n",
      "Episode: 298 Total reward: 8.0 Average reward fake: 0.34315985441207886 Training d_loss: 0.8484 Training g_loss: 1.0690 Training q_loss: 32382.8711 Explore P: 0.5981\n",
      "Episode: 299 Total reward: 19.0 Average reward fake: 0.35863643884658813 Training d_loss: 1.0596 Training g_loss: 1.0199 Training q_loss: 19386.0273 Explore P: 0.5970\n",
      "Episode: 300 Total reward: 22.0 Average reward fake: 0.36894315481185913 Training d_loss: 0.6601 Training g_loss: 0.9975 Training q_loss: 24073.3906 Explore P: 0.5957\n",
      "Episode: 301 Total reward: 16.0 Average reward fake: 0.357255756855011 Training d_loss: 1.0597 Training g_loss: 1.0265 Training q_loss: 29081.5625 Explore P: 0.5948\n",
      "Episode: 302 Total reward: 13.0 Average reward fake: 0.3559167683124542 Training d_loss: 1.0599 Training g_loss: 1.0330 Training q_loss: 109831.8125 Explore P: 0.5940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 303 Total reward: 9.0 Average reward fake: 0.3571712374687195 Training d_loss: 1.0597 Training g_loss: 1.0289 Training q_loss: 19609.5586 Explore P: 0.5935\n",
      "Episode: 304 Total reward: 27.0 Average reward fake: 0.37902823090553284 Training d_loss: 0.9617 Training g_loss: 0.9675 Training q_loss: 22935.7930 Explore P: 0.5919\n",
      "Episode: 305 Total reward: 33.0 Average reward fake: 0.3846898674964905 Training d_loss: 0.6769 Training g_loss: 0.9577 Training q_loss: 22924.5352 Explore P: 0.5900\n",
      "Episode: 306 Total reward: 16.0 Average reward fake: 0.3706488013267517 Training d_loss: 1.1579 Training g_loss: 0.9902 Training q_loss: 27104.9805 Explore P: 0.5891\n",
      "Episode: 307 Total reward: 9.0 Average reward fake: 0.3702675700187683 Training d_loss: 1.3567 Training g_loss: 0.9932 Training q_loss: 27867.4180 Explore P: 0.5885\n",
      "Episode: 308 Total reward: 14.0 Average reward fake: 0.37061381340026855 Training d_loss: 1.0587 Training g_loss: 0.9899 Training q_loss: 19032.9141 Explore P: 0.5877\n",
      "Episode: 309 Total reward: 21.0 Average reward fake: 0.3565118610858917 Training d_loss: 1.0598 Training g_loss: 1.0333 Training q_loss: 22810.1992 Explore P: 0.5865\n",
      "Episode: 310 Total reward: 14.0 Average reward fake: 0.3519130349159241 Training d_loss: 1.1649 Training g_loss: 1.0435 Training q_loss: 19411.5840 Explore P: 0.5857\n",
      "Episode: 311 Total reward: 40.0 Average reward fake: 0.6089617013931274 Training d_loss: 2.3526 Training g_loss: 0.6219 Training q_loss: 64061.9609 Explore P: 0.5834\n",
      "Episode: 312 Total reward: 15.0 Average reward fake: 0.4825907349586487 Training d_loss: 1.3941 Training g_loss: 0.7705 Training q_loss: 26696.2070 Explore P: 0.5826\n",
      "Episode: 313 Total reward: 14.0 Average reward fake: 0.40321946144104004 Training d_loss: 1.4104 Training g_loss: 0.9195 Training q_loss: 18307.1523 Explore P: 0.5818\n",
      "Episode: 314 Total reward: 22.0 Average reward fake: 0.43271851539611816 Training d_loss: 1.3912 Training g_loss: 0.8404 Training q_loss: 41641.4258 Explore P: 0.5805\n",
      "Episode: 315 Total reward: 88.0 Average reward fake: 0.5070492029190063 Training d_loss: 1.3600 Training g_loss: 0.6810 Training q_loss: 14558.5625 Explore P: 0.5755\n",
      "Episode: 316 Total reward: 35.0 Average reward fake: 0.5394953489303589 Training d_loss: 1.5199 Training g_loss: 0.6247 Training q_loss: 27611.8555 Explore P: 0.5735\n",
      "Episode: 317 Total reward: 44.0 Average reward fake: 0.48706740140914917 Training d_loss: 1.3742 Training g_loss: 0.7195 Training q_loss: 18478.9570 Explore P: 0.5710\n",
      "Episode: 318 Total reward: 43.0 Average reward fake: 0.48655813932418823 Training d_loss: 1.3978 Training g_loss: 0.7209 Training q_loss: 18750.1367 Explore P: 0.5686\n",
      "Episode: 319 Total reward: 24.0 Average reward fake: 0.4922824800014496 Training d_loss: 1.3695 Training g_loss: 0.7086 Training q_loss: 25593.0918 Explore P: 0.5673\n",
      "Episode: 320 Total reward: 41.0 Average reward fake: 0.5105959177017212 Training d_loss: 1.3984 Training g_loss: 0.6726 Training q_loss: 23609.0977 Explore P: 0.5650\n",
      "Episode: 321 Total reward: 29.0 Average reward fake: 0.4995538592338562 Training d_loss: 1.3872 Training g_loss: 0.6942 Training q_loss: 24372.5059 Explore P: 0.5634\n",
      "Episode: 322 Total reward: 33.0 Average reward fake: 0.5002621412277222 Training d_loss: 1.3834 Training g_loss: 0.6913 Training q_loss: 25272.8613 Explore P: 0.5616\n",
      "Episode: 323 Total reward: 46.0 Average reward fake: 0.5202557444572449 Training d_loss: 1.4299 Training g_loss: 0.6554 Training q_loss: 23527.7031 Explore P: 0.5591\n",
      "Episode: 324 Total reward: 14.0 Average reward fake: 0.5032569169998169 Training d_loss: 1.3980 Training g_loss: 0.6878 Training q_loss: 15431.3145 Explore P: 0.5583\n",
      "Episode: 325 Total reward: 22.0 Average reward fake: 0.48656439781188965 Training d_loss: 1.3766 Training g_loss: 0.7240 Training q_loss: 17266.3945 Explore P: 0.5571\n",
      "Episode: 326 Total reward: 64.0 Average reward fake: 0.480879545211792 Training d_loss: 1.4176 Training g_loss: 0.7466 Training q_loss: 15824.4316 Explore P: 0.5536\n",
      "Episode: 327 Total reward: 26.0 Average reward fake: 0.5361878275871277 Training d_loss: 1.4822 Training g_loss: 0.6345 Training q_loss: 9520.2471 Explore P: 0.5522\n",
      "Episode: 328 Total reward: 13.0 Average reward fake: 0.5303242206573486 Training d_loss: 1.4071 Training g_loss: 0.6433 Training q_loss: 9107.4863 Explore P: 0.5515\n",
      "Episode: 329 Total reward: 23.0 Average reward fake: 0.5331824421882629 Training d_loss: 1.4465 Training g_loss: 0.6329 Training q_loss: 6888.9077 Explore P: 0.5502\n",
      "Episode: 330 Total reward: 16.0 Average reward fake: 0.5163151621818542 Training d_loss: 1.4115 Training g_loss: 0.6645 Training q_loss: 17125.3320 Explore P: 0.5494\n",
      "Episode: 331 Total reward: 26.0 Average reward fake: 0.4980652332305908 Training d_loss: 1.3671 Training g_loss: 0.6982 Training q_loss: 13983.4814 Explore P: 0.5480\n",
      "Episode: 332 Total reward: 52.0 Average reward fake: 0.5144768953323364 Training d_loss: 1.4128 Training g_loss: 0.6668 Training q_loss: 15428.6143 Explore P: 0.5452\n",
      "Episode: 333 Total reward: 71.0 Average reward fake: 0.49578768014907837 Training d_loss: 1.3667 Training g_loss: 0.7032 Training q_loss: 6609.2749 Explore P: 0.5414\n",
      "Episode: 334 Total reward: 48.0 Average reward fake: 0.5009822249412537 Training d_loss: 1.3537 Training g_loss: 0.6975 Training q_loss: 14427.3438 Explore P: 0.5389\n",
      "Episode: 335 Total reward: 35.0 Average reward fake: 0.47683945298194885 Training d_loss: 1.4013 Training g_loss: 0.7475 Training q_loss: 9678.9980 Explore P: 0.5370\n",
      "Episode: 336 Total reward: 50.0 Average reward fake: 0.5091155171394348 Training d_loss: 1.4490 Training g_loss: 0.6826 Training q_loss: 10600.0391 Explore P: 0.5344\n",
      "Episode: 337 Total reward: 53.0 Average reward fake: 0.4783709943294525 Training d_loss: 1.3361 Training g_loss: 0.7414 Training q_loss: 12924.0439 Explore P: 0.5316\n",
      "Episode: 338 Total reward: 44.0 Average reward fake: 0.48873281478881836 Training d_loss: 1.3764 Training g_loss: 0.7206 Training q_loss: 6410.6182 Explore P: 0.5293\n",
      "Episode: 339 Total reward: 81.0 Average reward fake: 0.4951581060886383 Training d_loss: 1.3945 Training g_loss: 0.7071 Training q_loss: 13609.1348 Explore P: 0.5251\n",
      "Episode: 340 Total reward: 72.0 Average reward fake: 0.503585159778595 Training d_loss: 1.4257 Training g_loss: 0.6881 Training q_loss: 11675.6611 Explore P: 0.5214\n",
      "Episode: 341 Total reward: 40.0 Average reward fake: 0.4963218569755554 Training d_loss: 1.3763 Training g_loss: 0.7008 Training q_loss: 4951.6689 Explore P: 0.5194\n",
      "Episode: 342 Total reward: 50.0 Average reward fake: 0.5009725689888 Training d_loss: 1.3801 Training g_loss: 0.6919 Training q_loss: 17588.3789 Explore P: 0.5168\n",
      "Episode: 343 Total reward: 17.0 Average reward fake: 0.5011196136474609 Training d_loss: 1.3793 Training g_loss: 0.6921 Training q_loss: 8434.9512 Explore P: 0.5160\n",
      "Episode: 344 Total reward: 59.0 Average reward fake: 0.4999449849128723 Training d_loss: 1.3917 Training g_loss: 0.6941 Training q_loss: 6345.8691 Explore P: 0.5130\n",
      "Episode: 345 Total reward: 26.0 Average reward fake: 0.49661558866500854 Training d_loss: 1.3866 Training g_loss: 0.6997 Training q_loss: 7196.8257 Explore P: 0.5117\n",
      "Episode: 346 Total reward: 43.0 Average reward fake: 0.497781902551651 Training d_loss: 1.3711 Training g_loss: 0.6982 Training q_loss: 2918.5229 Explore P: 0.5095\n",
      "Episode: 347 Total reward: 38.0 Average reward fake: 0.5036101341247559 Training d_loss: 1.3913 Training g_loss: 0.6859 Training q_loss: 4161.1768 Explore P: 0.5077\n",
      "Episode: 348 Total reward: 39.0 Average reward fake: 0.506300151348114 Training d_loss: 1.3712 Training g_loss: 0.6814 Training q_loss: 23674.4180 Explore P: 0.5057\n",
      "Episode: 349 Total reward: 31.0 Average reward fake: 0.49453967809677124 Training d_loss: 1.3657 Training g_loss: 0.7047 Training q_loss: 8356.8145 Explore P: 0.5042\n",
      "Episode: 350 Total reward: 48.0 Average reward fake: 0.5062555074691772 Training d_loss: 1.4188 Training g_loss: 0.6819 Training q_loss: 6366.8008 Explore P: 0.5018\n",
      "Episode: 351 Total reward: 35.0 Average reward fake: 0.5002724528312683 Training d_loss: 1.3791 Training g_loss: 0.6939 Training q_loss: 19689.4824 Explore P: 0.5001\n",
      "Episode: 352 Total reward: 21.0 Average reward fake: 0.5082495808601379 Training d_loss: 1.4172 Training g_loss: 0.6782 Training q_loss: 715.2673 Explore P: 0.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 353 Total reward: 30.0 Average reward fake: 0.4892241358757019 Training d_loss: 1.3835 Training g_loss: 0.7155 Training q_loss: 7931.1143 Explore P: 0.4976\n",
      "Episode: 354 Total reward: 53.0 Average reward fake: 0.5029471516609192 Training d_loss: 1.3889 Training g_loss: 0.6868 Training q_loss: 6537.5747 Explore P: 0.4950\n",
      "Episode: 355 Total reward: 58.0 Average reward fake: 0.5026527047157288 Training d_loss: 1.3567 Training g_loss: 0.6938 Training q_loss: 3406.7859 Explore P: 0.4922\n",
      "Episode: 356 Total reward: 25.0 Average reward fake: 0.4939681589603424 Training d_loss: 1.3895 Training g_loss: 0.7087 Training q_loss: 3875.7942 Explore P: 0.4910\n",
      "Episode: 357 Total reward: 23.0 Average reward fake: 0.4869351387023926 Training d_loss: 1.3985 Training g_loss: 0.7199 Training q_loss: 1946.8033 Explore P: 0.4899\n",
      "Episode: 358 Total reward: 22.0 Average reward fake: 0.4879387319087982 Training d_loss: 1.3847 Training g_loss: 0.7176 Training q_loss: 4372.6846 Explore P: 0.4889\n",
      "Episode: 359 Total reward: 20.0 Average reward fake: 0.48929810523986816 Training d_loss: 1.3815 Training g_loss: 0.7153 Training q_loss: 8468.3018 Explore P: 0.4879\n",
      "Episode: 360 Total reward: 18.0 Average reward fake: 0.4895034432411194 Training d_loss: 1.3943 Training g_loss: 0.7131 Training q_loss: 14485.7969 Explore P: 0.4870\n",
      "Episode: 361 Total reward: 36.0 Average reward fake: 0.5086361765861511 Training d_loss: 1.3767 Training g_loss: 0.6762 Training q_loss: 3038.1304 Explore P: 0.4853\n",
      "Episode: 362 Total reward: 59.0 Average reward fake: 0.5189616084098816 Training d_loss: 1.3977 Training g_loss: 0.6626 Training q_loss: 3619.0659 Explore P: 0.4825\n",
      "Episode: 363 Total reward: 24.0 Average reward fake: 0.48811736702919006 Training d_loss: 1.3776 Training g_loss: 0.7207 Training q_loss: 26355.1758 Explore P: 0.4814\n",
      "Episode: 364 Total reward: 24.0 Average reward fake: 0.4836544990539551 Training d_loss: 1.3703 Training g_loss: 0.7264 Training q_loss: 7804.4189 Explore P: 0.4803\n",
      "Episode: 365 Total reward: 17.0 Average reward fake: 0.4904349744319916 Training d_loss: 1.4015 Training g_loss: 0.7143 Training q_loss: 4575.8555 Explore P: 0.4795\n",
      "Episode: 366 Total reward: 31.0 Average reward fake: 0.48435014486312866 Training d_loss: 1.3852 Training g_loss: 0.7241 Training q_loss: 4413.3906 Explore P: 0.4780\n",
      "Episode: 367 Total reward: 52.0 Average reward fake: 0.5037533640861511 Training d_loss: 1.3831 Training g_loss: 0.6854 Training q_loss: 3925.7512 Explore P: 0.4756\n",
      "Episode: 368 Total reward: 68.0 Average reward fake: 0.5018047094345093 Training d_loss: 1.3833 Training g_loss: 0.6901 Training q_loss: 3370.3228 Explore P: 0.4724\n",
      "Episode: 369 Total reward: 52.0 Average reward fake: 0.5005953907966614 Training d_loss: 1.3878 Training g_loss: 0.6912 Training q_loss: 2062.3667 Explore P: 0.4700\n",
      "Episode: 370 Total reward: 34.0 Average reward fake: 0.5020292401313782 Training d_loss: 1.3834 Training g_loss: 0.6891 Training q_loss: 2735.5718 Explore P: 0.4685\n",
      "Episode: 371 Total reward: 34.0 Average reward fake: 0.5035354495048523 Training d_loss: 1.3832 Training g_loss: 0.6855 Training q_loss: 1936.7068 Explore P: 0.4669\n",
      "Episode: 372 Total reward: 44.0 Average reward fake: 0.4941251873970032 Training d_loss: 1.3848 Training g_loss: 0.7054 Training q_loss: 2049.9844 Explore P: 0.4649\n",
      "Episode: 373 Total reward: 58.0 Average reward fake: 0.4991624355316162 Training d_loss: 1.3854 Training g_loss: 0.6947 Training q_loss: 4116.7744 Explore P: 0.4623\n",
      "Episode: 374 Total reward: 34.0 Average reward fake: 0.49841728806495667 Training d_loss: 1.3853 Training g_loss: 0.6961 Training q_loss: 1977.9235 Explore P: 0.4607\n",
      "Episode: 375 Total reward: 19.0 Average reward fake: 0.5022926330566406 Training d_loss: 1.3839 Training g_loss: 0.6882 Training q_loss: 1714.0924 Explore P: 0.4599\n",
      "Episode: 376 Total reward: 65.0 Average reward fake: 0.49285244941711426 Training d_loss: 1.3839 Training g_loss: 0.7074 Training q_loss: 1578.4285 Explore P: 0.4570\n",
      "Episode: 377 Total reward: 29.0 Average reward fake: 0.5016090869903564 Training d_loss: 1.3726 Training g_loss: 0.6899 Training q_loss: 508.1104 Explore P: 0.4557\n",
      "Episode: 378 Total reward: 27.0 Average reward fake: 0.5053001046180725 Training d_loss: 1.3905 Training g_loss: 0.6835 Training q_loss: 472.3835 Explore P: 0.4545\n",
      "Episode: 379 Total reward: 25.0 Average reward fake: 0.49544915556907654 Training d_loss: 1.3896 Training g_loss: 0.7021 Training q_loss: 1787.1028 Explore P: 0.4534\n",
      "Episode: 380 Total reward: 23.0 Average reward fake: 0.4901748299598694 Training d_loss: 1.3851 Training g_loss: 0.7130 Training q_loss: 2480.2622 Explore P: 0.4524\n",
      "Episode: 381 Total reward: 32.0 Average reward fake: 0.5054219365119934 Training d_loss: 1.3858 Training g_loss: 0.6824 Training q_loss: 9971.5498 Explore P: 0.4509\n",
      "Episode: 382 Total reward: 83.0 Average reward fake: 0.496561199426651 Training d_loss: 1.3695 Training g_loss: 0.7005 Training q_loss: 5384.9673 Explore P: 0.4473\n",
      "Episode: 383 Total reward: 58.0 Average reward fake: 0.5033060312271118 Training d_loss: 1.3975 Training g_loss: 0.6870 Training q_loss: 1365.0004 Explore P: 0.4448\n",
      "Episode: 384 Total reward: 39.0 Average reward fake: 0.5047447085380554 Training d_loss: 1.3865 Training g_loss: 0.6839 Training q_loss: 630.4705 Explore P: 0.4431\n",
      "Episode: 385 Total reward: 27.0 Average reward fake: 0.5030613541603088 Training d_loss: 1.3863 Training g_loss: 0.6876 Training q_loss: 962.5510 Explore P: 0.4419\n",
      "Episode: 386 Total reward: 40.0 Average reward fake: 0.5015285611152649 Training d_loss: 1.3872 Training g_loss: 0.6897 Training q_loss: 983.0302 Explore P: 0.4402\n",
      "Episode: 387 Total reward: 44.0 Average reward fake: 0.49599161744117737 Training d_loss: 1.3712 Training g_loss: 0.7020 Training q_loss: 12681.6934 Explore P: 0.4383\n",
      "Episode: 388 Total reward: 33.0 Average reward fake: 0.4990905225276947 Training d_loss: 1.3999 Training g_loss: 0.6976 Training q_loss: 2066.4565 Explore P: 0.4369\n",
      "Episode: 389 Total reward: 22.0 Average reward fake: 0.48354625701904297 Training d_loss: 1.3882 Training g_loss: 0.7274 Training q_loss: 1946.8422 Explore P: 0.4359\n",
      "Episode: 390 Total reward: 12.0 Average reward fake: 0.46113672852516174 Training d_loss: 1.3164 Training g_loss: 0.7769 Training q_loss: 4485.3447 Explore P: 0.4354\n",
      "Episode: 391 Total reward: 41.0 Average reward fake: 0.5095551013946533 Training d_loss: 1.3571 Training g_loss: 0.6754 Training q_loss: 3595.5247 Explore P: 0.4337\n",
      "Episode: 392 Total reward: 46.0 Average reward fake: 0.5393264293670654 Training d_loss: 1.5061 Training g_loss: 0.6305 Training q_loss: 5737.3306 Explore P: 0.4317\n",
      "Episode: 393 Total reward: 30.0 Average reward fake: 0.47106796503067017 Training d_loss: 1.3882 Training g_loss: 0.7526 Training q_loss: 2826.3286 Explore P: 0.4305\n",
      "Episode: 394 Total reward: 12.0 Average reward fake: 0.4756690561771393 Training d_loss: 1.3888 Training g_loss: 0.7426 Training q_loss: 3382.9480 Explore P: 0.4300\n",
      "Episode: 395 Total reward: 25.0 Average reward fake: 0.47028079628944397 Training d_loss: 1.3714 Training g_loss: 0.7545 Training q_loss: 1276.3586 Explore P: 0.4289\n",
      "Episode: 396 Total reward: 25.0 Average reward fake: 0.4821164608001709 Training d_loss: 1.3974 Training g_loss: 0.7302 Training q_loss: 2044.5254 Explore P: 0.4279\n",
      "Episode: 397 Total reward: 23.0 Average reward fake: 0.5073958039283752 Training d_loss: 1.4352 Training g_loss: 0.6785 Training q_loss: 794.6758 Explore P: 0.4269\n",
      "Episode: 398 Total reward: 44.0 Average reward fake: 0.5116420984268188 Training d_loss: 1.3961 Training g_loss: 0.6701 Training q_loss: 2765.9302 Explore P: 0.4251\n",
      "Episode: 399 Total reward: 112.0 Average reward fake: 0.5065321922302246 Training d_loss: 1.3898 Training g_loss: 0.6810 Training q_loss: 1926.9348 Explore P: 0.4205\n",
      "Episode: 400 Total reward: 13.0 Average reward fake: 0.5009653568267822 Training d_loss: 1.3860 Training g_loss: 0.6918 Training q_loss: 2765.6162 Explore P: 0.4199\n",
      "Episode: 401 Total reward: 17.0 Average reward fake: 0.4946540892124176 Training d_loss: 1.3843 Training g_loss: 0.7053 Training q_loss: 9042.9053 Explore P: 0.4192\n",
      "Episode: 402 Total reward: 20.0 Average reward fake: 0.49189239740371704 Training d_loss: 1.3998 Training g_loss: 0.7091 Training q_loss: 4827.5889 Explore P: 0.4184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 403 Total reward: 48.0 Average reward fake: 0.5044032335281372 Training d_loss: 1.3852 Training g_loss: 0.6845 Training q_loss: 1584.8752 Explore P: 0.4165\n",
      "Episode: 404 Total reward: 34.0 Average reward fake: 0.5015589594841003 Training d_loss: 1.3802 Training g_loss: 0.6905 Training q_loss: 5881.0605 Explore P: 0.4151\n",
      "Episode: 405 Total reward: 43.0 Average reward fake: 0.4983099102973938 Training d_loss: 1.3915 Training g_loss: 0.6968 Training q_loss: 9615.0557 Explore P: 0.4134\n",
      "Episode: 406 Total reward: 22.0 Average reward fake: 0.49977725744247437 Training d_loss: 1.3883 Training g_loss: 0.6943 Training q_loss: 1123.1042 Explore P: 0.4125\n",
      "Episode: 407 Total reward: 60.0 Average reward fake: 0.4930174946784973 Training d_loss: 1.4049 Training g_loss: 0.7069 Training q_loss: 1432.1559 Explore P: 0.4101\n",
      "Episode: 408 Total reward: 53.0 Average reward fake: 0.49399298429489136 Training d_loss: 1.3594 Training g_loss: 0.7072 Training q_loss: 910.4214 Explore P: 0.4079\n",
      "Episode: 409 Total reward: 49.0 Average reward fake: 0.5041452050209045 Training d_loss: 1.4303 Training g_loss: 0.6886 Training q_loss: 1085.0757 Explore P: 0.4060\n",
      "Episode: 410 Total reward: 20.0 Average reward fake: 0.48783358931541443 Training d_loss: 1.3885 Training g_loss: 0.7184 Training q_loss: 985.0325 Explore P: 0.4052\n",
      "Episode: 411 Total reward: 13.0 Average reward fake: 0.4870310425758362 Training d_loss: 1.3849 Training g_loss: 0.7195 Training q_loss: 5510.1084 Explore P: 0.4047\n",
      "Episode: 412 Total reward: 25.0 Average reward fake: 0.48696112632751465 Training d_loss: 1.3619 Training g_loss: 0.7202 Training q_loss: 1153.6919 Explore P: 0.4037\n",
      "Episode: 413 Total reward: 55.0 Average reward fake: 0.5090100169181824 Training d_loss: 1.3893 Training g_loss: 0.6755 Training q_loss: 1773.1453 Explore P: 0.4015\n",
      "Episode: 414 Total reward: 9.0 Average reward fake: 0.5083049535751343 Training d_loss: 1.3869 Training g_loss: 0.6771 Training q_loss: 3967.8599 Explore P: 0.4012\n",
      "Episode: 415 Total reward: 14.0 Average reward fake: 0.5052636861801147 Training d_loss: 1.3789 Training g_loss: 0.6832 Training q_loss: 5378.5820 Explore P: 0.4006\n",
      "Episode: 416 Total reward: 15.0 Average reward fake: 0.49817943572998047 Training d_loss: 1.3754 Training g_loss: 0.6986 Training q_loss: 3416.1235 Explore P: 0.4001\n",
      "Episode: 417 Total reward: 71.0 Average reward fake: 0.5014175176620483 Training d_loss: 1.3996 Training g_loss: 0.6917 Training q_loss: 1509.6953 Explore P: 0.3973\n",
      "Episode: 418 Total reward: 17.0 Average reward fake: 0.49206042289733887 Training d_loss: 1.3866 Training g_loss: 0.7096 Training q_loss: 972.6831 Explore P: 0.3966\n",
      "Episode: 419 Total reward: 19.0 Average reward fake: 0.48622241616249084 Training d_loss: 1.3763 Training g_loss: 0.7220 Training q_loss: 1719.1882 Explore P: 0.3959\n",
      "Episode: 420 Total reward: 22.0 Average reward fake: 0.4926120638847351 Training d_loss: 1.3752 Training g_loss: 0.7083 Training q_loss: 1777.5668 Explore P: 0.3951\n",
      "Episode: 421 Total reward: 52.0 Average reward fake: 0.5043688416481018 Training d_loss: 1.3864 Training g_loss: 0.6845 Training q_loss: 16258.9531 Explore P: 0.3931\n",
      "Episode: 422 Total reward: 15.0 Average reward fake: 0.5043706893920898 Training d_loss: 1.3849 Training g_loss: 0.6844 Training q_loss: 2891.5535 Explore P: 0.3925\n",
      "Episode: 423 Total reward: 74.0 Average reward fake: 0.5028754472732544 Training d_loss: 1.3923 Training g_loss: 0.6875 Training q_loss: 1905.2457 Explore P: 0.3897\n",
      "Episode: 424 Total reward: 17.0 Average reward fake: 0.49974870681762695 Training d_loss: 1.3851 Training g_loss: 0.6938 Training q_loss: 1279.6106 Explore P: 0.3890\n",
      "Episode: 425 Total reward: 26.0 Average reward fake: 0.4991305470466614 Training d_loss: 1.3869 Training g_loss: 0.6950 Training q_loss: 443.4516 Explore P: 0.3880\n",
      "Episode: 426 Total reward: 31.0 Average reward fake: 0.4970913529396057 Training d_loss: 1.3855 Training g_loss: 0.6993 Training q_loss: 707.4465 Explore P: 0.3869\n",
      "Episode: 427 Total reward: 29.0 Average reward fake: 0.49644142389297485 Training d_loss: 1.3916 Training g_loss: 0.7001 Training q_loss: 772.6516 Explore P: 0.3858\n",
      "Episode: 428 Total reward: 65.0 Average reward fake: 0.5031987428665161 Training d_loss: 1.3918 Training g_loss: 0.6868 Training q_loss: 487.0368 Explore P: 0.3833\n",
      "Episode: 429 Total reward: 23.0 Average reward fake: 0.5012214183807373 Training d_loss: 1.3873 Training g_loss: 0.6906 Training q_loss: 183.4583 Explore P: 0.3825\n",
      "Episode: 430 Total reward: 29.0 Average reward fake: 0.5015013217926025 Training d_loss: 1.3853 Training g_loss: 0.6902 Training q_loss: 599.7804 Explore P: 0.3814\n",
      "Episode: 431 Total reward: 30.0 Average reward fake: 0.5011904835700989 Training d_loss: 1.3865 Training g_loss: 0.6909 Training q_loss: 279.6562 Explore P: 0.3803\n",
      "Episode: 432 Total reward: 30.0 Average reward fake: 0.5003882646560669 Training d_loss: 1.3846 Training g_loss: 0.6924 Training q_loss: 546.1536 Explore P: 0.3792\n",
      "Episode: 433 Total reward: 43.0 Average reward fake: 0.49920934438705444 Training d_loss: 1.3863 Training g_loss: 0.6948 Training q_loss: 382.3238 Explore P: 0.3776\n",
      "Episode: 434 Total reward: 34.0 Average reward fake: 0.4994026720523834 Training d_loss: 1.3866 Training g_loss: 0.6943 Training q_loss: 160.2041 Explore P: 0.3764\n",
      "Episode: 435 Total reward: 38.0 Average reward fake: 0.49987468123435974 Training d_loss: 1.3841 Training g_loss: 0.6935 Training q_loss: 581.4776 Explore P: 0.3750\n",
      "Episode: 436 Total reward: 34.0 Average reward fake: 0.49958252906799316 Training d_loss: 1.3875 Training g_loss: 0.6940 Training q_loss: 395.0456 Explore P: 0.3737\n",
      "Episode: 437 Total reward: 58.0 Average reward fake: 0.4995953142642975 Training d_loss: 1.3853 Training g_loss: 0.6942 Training q_loss: 278.5244 Explore P: 0.3716\n",
      "Episode: 438 Total reward: 42.0 Average reward fake: 0.4987761378288269 Training d_loss: 1.3923 Training g_loss: 0.6956 Training q_loss: 2442.0581 Explore P: 0.3701\n",
      "Episode: 439 Total reward: 78.0 Average reward fake: 0.4992232918739319 Training d_loss: 1.3767 Training g_loss: 0.6950 Training q_loss: 197.5393 Explore P: 0.3673\n",
      "Episode: 440 Total reward: 33.0 Average reward fake: 0.5012682676315308 Training d_loss: 1.3933 Training g_loss: 0.6909 Training q_loss: 231.4091 Explore P: 0.3661\n",
      "Episode: 441 Total reward: 25.0 Average reward fake: 0.49716347455978394 Training d_loss: 1.3856 Training g_loss: 0.6991 Training q_loss: 190.6764 Explore P: 0.3652\n",
      "Episode: 442 Total reward: 24.0 Average reward fake: 0.4933534264564514 Training d_loss: 1.3798 Training g_loss: 0.7071 Training q_loss: 448.0106 Explore P: 0.3644\n",
      "Episode: 443 Total reward: 57.0 Average reward fake: 0.5073364973068237 Training d_loss: 1.3968 Training g_loss: 0.6791 Training q_loss: 798.1498 Explore P: 0.3624\n",
      "Episode: 444 Total reward: 35.0 Average reward fake: 0.4946025311946869 Training d_loss: 1.3810 Training g_loss: 0.7058 Training q_loss: 2523.8228 Explore P: 0.3611\n",
      "Episode: 445 Total reward: 19.0 Average reward fake: 0.508899986743927 Training d_loss: 1.4197 Training g_loss: 0.6759 Training q_loss: 581.0955 Explore P: 0.3605\n",
      "Episode: 446 Total reward: 19.0 Average reward fake: 0.5093742609024048 Training d_loss: 1.3951 Training g_loss: 0.6754 Training q_loss: 896.9518 Explore P: 0.3598\n",
      "Episode: 447 Total reward: 24.0 Average reward fake: 0.5014060139656067 Training d_loss: 1.3879 Training g_loss: 0.6909 Training q_loss: 631.8018 Explore P: 0.3590\n",
      "Episode: 448 Total reward: 27.0 Average reward fake: 0.4985024929046631 Training d_loss: 1.3862 Training g_loss: 0.6962 Training q_loss: 6935.5923 Explore P: 0.3580\n",
      "Episode: 449 Total reward: 46.0 Average reward fake: 0.49756231904029846 Training d_loss: 1.3856 Training g_loss: 0.6980 Training q_loss: 133.5567 Explore P: 0.3564\n",
      "Episode: 450 Total reward: 56.0 Average reward fake: 0.5005460977554321 Training d_loss: 1.3864 Training g_loss: 0.6921 Training q_loss: 207.7431 Explore P: 0.3545\n",
      "Episode: 451 Total reward: 43.0 Average reward fake: 0.5003110766410828 Training d_loss: 1.3851 Training g_loss: 0.6925 Training q_loss: 5046.0659 Explore P: 0.3530\n",
      "Episode: 452 Total reward: 56.0 Average reward fake: 0.4987163543701172 Training d_loss: 1.3832 Training g_loss: 0.6957 Training q_loss: 355.7498 Explore P: 0.3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 453 Total reward: 35.0 Average reward fake: 0.4987994134426117 Training d_loss: 1.3810 Training g_loss: 0.6957 Training q_loss: 287.4337 Explore P: 0.3499\n",
      "Episode: 454 Total reward: 30.0 Average reward fake: 0.5001438856124878 Training d_loss: 1.3869 Training g_loss: 0.6933 Training q_loss: 2699.0012 Explore P: 0.3489\n",
      "Episode: 455 Total reward: 21.0 Average reward fake: 0.4993663430213928 Training d_loss: 1.3863 Training g_loss: 0.6944 Training q_loss: 477.6427 Explore P: 0.3482\n",
      "Episode: 456 Total reward: 105.0 Average reward fake: 0.4908375144004822 Training d_loss: 1.3692 Training g_loss: 0.7132 Training q_loss: 380.8844 Explore P: 0.3447\n",
      "Episode: 457 Total reward: 85.0 Average reward fake: 0.5114012956619263 Training d_loss: 1.4272 Training g_loss: 0.6765 Training q_loss: 629.1377 Explore P: 0.3418\n",
      "Episode: 458 Total reward: 18.0 Average reward fake: 0.49789899587631226 Training d_loss: 1.3802 Training g_loss: 0.6994 Training q_loss: 1469.0265 Explore P: 0.3412\n",
      "Episode: 459 Total reward: 17.0 Average reward fake: 0.49580803513526917 Training d_loss: 1.3912 Training g_loss: 0.7019 Training q_loss: 1000.2601 Explore P: 0.3407\n",
      "Episode: 460 Total reward: 23.0 Average reward fake: 0.49538978934288025 Training d_loss: 1.3840 Training g_loss: 0.7022 Training q_loss: 389.3926 Explore P: 0.3399\n",
      "Episode: 461 Total reward: 34.0 Average reward fake: 0.5023672580718994 Training d_loss: 1.3829 Training g_loss: 0.6885 Training q_loss: 358.7093 Explore P: 0.3388\n",
      "Episode: 462 Total reward: 49.0 Average reward fake: 0.50110924243927 Training d_loss: 1.3842 Training g_loss: 0.6910 Training q_loss: 1218.3724 Explore P: 0.3372\n",
      "Episode: 463 Total reward: 30.0 Average reward fake: 0.4979904294013977 Training d_loss: 1.3842 Training g_loss: 0.6974 Training q_loss: 1966.0697 Explore P: 0.3362\n",
      "Episode: 464 Total reward: 33.0 Average reward fake: 0.49918460845947266 Training d_loss: 1.3954 Training g_loss: 0.6951 Training q_loss: 666.9228 Explore P: 0.3351\n",
      "Episode: 465 Total reward: 32.0 Average reward fake: 0.4951159358024597 Training d_loss: 1.3940 Training g_loss: 0.7026 Training q_loss: 707.9281 Explore P: 0.3341\n",
      "Episode: 466 Total reward: 59.0 Average reward fake: 0.4980749487876892 Training d_loss: 1.3676 Training g_loss: 0.6985 Training q_loss: 121.1820 Explore P: 0.3322\n",
      "Episode: 467 Total reward: 44.0 Average reward fake: 0.5024078488349915 Training d_loss: 1.3892 Training g_loss: 0.6892 Training q_loss: 3763.1724 Explore P: 0.3308\n",
      "Episode: 468 Total reward: 26.0 Average reward fake: 0.49076956510543823 Training d_loss: 1.3838 Training g_loss: 0.7129 Training q_loss: 489.1719 Explore P: 0.3299\n",
      "Episode: 469 Total reward: 100.0 Average reward fake: 0.49767881631851196 Training d_loss: 1.3695 Training g_loss: 0.6983 Training q_loss: 255.0517 Explore P: 0.3267\n",
      "Episode: 470 Total reward: 31.0 Average reward fake: 0.5000945329666138 Training d_loss: 1.3865 Training g_loss: 0.6932 Training q_loss: 596.1838 Explore P: 0.3258\n",
      "Episode: 471 Total reward: 28.0 Average reward fake: 0.48407942056655884 Training d_loss: 1.3651 Training g_loss: 0.7281 Training q_loss: 368.2436 Explore P: 0.3249\n",
      "Episode: 472 Total reward: 82.0 Average reward fake: 0.5372484922409058 Training d_loss: 1.4855 Training g_loss: 0.6270 Training q_loss: 906.2662 Explore P: 0.3223\n",
      "Episode: 473 Total reward: 19.0 Average reward fake: 0.5158348083496094 Training d_loss: 1.4122 Training g_loss: 0.6637 Training q_loss: 1091.0110 Explore P: 0.3217\n",
      "Episode: 474 Total reward: 19.0 Average reward fake: 0.5063626170158386 Training d_loss: 1.3946 Training g_loss: 0.6812 Training q_loss: 532.9460 Explore P: 0.3211\n",
      "Episode: 475 Total reward: 65.0 Average reward fake: 0.5004042387008667 Training d_loss: 1.3924 Training g_loss: 0.6927 Training q_loss: 8053.5352 Explore P: 0.3191\n",
      "Episode: 476 Total reward: 78.0 Average reward fake: 0.5036534070968628 Training d_loss: 1.3870 Training g_loss: 0.6865 Training q_loss: 493.3141 Explore P: 0.3167\n",
      "Episode: 477 Total reward: 63.0 Average reward fake: 0.4954414367675781 Training d_loss: 1.3767 Training g_loss: 0.7031 Training q_loss: 553.2501 Explore P: 0.3148\n",
      "Episode: 478 Total reward: 98.0 Average reward fake: 0.49959444999694824 Training d_loss: 1.3857 Training g_loss: 0.6939 Training q_loss: 84.6092 Explore P: 0.3118\n",
      "Episode: 479 Total reward: 32.0 Average reward fake: 0.4985409379005432 Training d_loss: 1.3830 Training g_loss: 0.6961 Training q_loss: 1928.1501 Explore P: 0.3108\n",
      "Episode: 480 Total reward: 30.0 Average reward fake: 0.4929676055908203 Training d_loss: 1.3784 Training g_loss: 0.7077 Training q_loss: 338.4005 Explore P: 0.3099\n",
      "Episode: 481 Total reward: 48.0 Average reward fake: 0.5007948875427246 Training d_loss: 1.3802 Training g_loss: 0.6920 Training q_loss: 930.8185 Explore P: 0.3085\n",
      "Episode: 482 Total reward: 69.0 Average reward fake: 0.4881477355957031 Training d_loss: 1.3807 Training g_loss: 0.7179 Training q_loss: 201.1374 Explore P: 0.3065\n",
      "Episode: 483 Total reward: 57.0 Average reward fake: 0.5022404789924622 Training d_loss: 1.3793 Training g_loss: 0.6896 Training q_loss: 306.8223 Explore P: 0.3048\n",
      "Episode: 484 Total reward: 50.0 Average reward fake: 0.4988187253475189 Training d_loss: 1.3793 Training g_loss: 0.6964 Training q_loss: 4791.1167 Explore P: 0.3033\n",
      "Episode: 485 Total reward: 28.0 Average reward fake: 0.5114158391952515 Training d_loss: 1.3871 Training g_loss: 0.6717 Training q_loss: 582.2211 Explore P: 0.3025\n",
      "Episode: 486 Total reward: 48.0 Average reward fake: 0.4950827956199646 Training d_loss: 1.3759 Training g_loss: 0.7034 Training q_loss: 255.0636 Explore P: 0.3011\n",
      "Episode: 487 Total reward: 57.0 Average reward fake: 0.49968379735946655 Training d_loss: 1.4045 Training g_loss: 0.6954 Training q_loss: 768.2409 Explore P: 0.2994\n",
      "Episode: 488 Total reward: 17.0 Average reward fake: 0.4917459487915039 Training d_loss: 1.3797 Training g_loss: 0.7105 Training q_loss: 1243.4158 Explore P: 0.2989\n",
      "Episode: 489 Total reward: 54.0 Average reward fake: 0.5036857724189758 Training d_loss: 1.3560 Training g_loss: 0.6900 Training q_loss: 679.7601 Explore P: 0.2974\n",
      "Episode: 490 Total reward: 21.0 Average reward fake: 0.5118356943130493 Training d_loss: 1.4044 Training g_loss: 0.6998 Training q_loss: 144.9449 Explore P: 0.2968\n",
      "Episode: 491 Total reward: 17.0 Average reward fake: 0.48252663016319275 Training d_loss: 1.4482 Training g_loss: 0.7473 Training q_loss: 979.3923 Explore P: 0.2963\n",
      "Episode: 492 Total reward: 26.0 Average reward fake: 0.47820955514907837 Training d_loss: 1.3822 Training g_loss: 0.7369 Training q_loss: 523.4427 Explore P: 0.2955\n",
      "Episode: 493 Total reward: 34.0 Average reward fake: 0.49413713812828064 Training d_loss: 1.3738 Training g_loss: 0.7066 Training q_loss: 4726.4697 Explore P: 0.2946\n",
      "Episode: 494 Total reward: 66.0 Average reward fake: 0.4892864227294922 Training d_loss: 1.3901 Training g_loss: 0.7160 Training q_loss: 8185.7158 Explore P: 0.2927\n",
      "Episode: 495 Total reward: 88.0 Average reward fake: 0.5060449838638306 Training d_loss: 1.4056 Training g_loss: 0.6835 Training q_loss: 203.5353 Explore P: 0.2902\n",
      "Episode: 496 Total reward: 16.0 Average reward fake: 0.4913718104362488 Training d_loss: 1.4015 Training g_loss: 0.7139 Training q_loss: 653.8585 Explore P: 0.2898\n",
      "Episode: 497 Total reward: 21.0 Average reward fake: 0.4743141233921051 Training d_loss: 1.3722 Training g_loss: 0.7460 Training q_loss: 3823.5969 Explore P: 0.2892\n",
      "Episode: 498 Total reward: 53.0 Average reward fake: 0.5131827592849731 Training d_loss: 1.3893 Training g_loss: 0.6685 Training q_loss: 477.6343 Explore P: 0.2877\n",
      "Episode: 499 Total reward: 102.0 Average reward fake: 0.49706965684890747 Training d_loss: 1.3664 Training g_loss: 0.7015 Training q_loss: 3326.0007 Explore P: 0.2849\n",
      "Episode: 500 Total reward: 37.0 Average reward fake: 0.48929762840270996 Training d_loss: 1.3859 Training g_loss: 0.7140 Training q_loss: 160.6480 Explore P: 0.2839\n",
      "Episode: 501 Total reward: 68.0 Average reward fake: 0.49467676877975464 Training d_loss: 1.3843 Training g_loss: 0.7075 Training q_loss: 6863.5483 Explore P: 0.2820\n",
      "Episode: 502 Total reward: 26.0 Average reward fake: 0.4950302541255951 Training d_loss: 1.3861 Training g_loss: 0.7021 Training q_loss: 97.2021 Explore P: 0.2813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 503 Total reward: 57.0 Average reward fake: 0.5005965232849121 Training d_loss: 1.3899 Training g_loss: 0.6945 Training q_loss: 255.0407 Explore P: 0.2798\n",
      "Episode: 504 Total reward: 31.0 Average reward fake: 0.4874843955039978 Training d_loss: 1.3651 Training g_loss: 0.7219 Training q_loss: 461.7119 Explore P: 0.2789\n",
      "Episode: 505 Total reward: 54.0 Average reward fake: 0.512789249420166 Training d_loss: 1.3909 Training g_loss: 0.6685 Training q_loss: 12180.1670 Explore P: 0.2775\n",
      "Episode: 506 Total reward: 29.0 Average reward fake: 0.4995841383934021 Training d_loss: 1.3826 Training g_loss: 0.6946 Training q_loss: 354.3730 Explore P: 0.2767\n",
      "Episode: 507 Total reward: 20.0 Average reward fake: 0.4767315983772278 Training d_loss: 1.3603 Training g_loss: 0.7460 Training q_loss: 6531.6631 Explore P: 0.2762\n",
      "Episode: 508 Total reward: 32.0 Average reward fake: 0.5192424654960632 Training d_loss: 1.3855 Training g_loss: 0.6558 Training q_loss: 465.4389 Explore P: 0.2753\n",
      "Episode: 509 Total reward: 20.0 Average reward fake: 0.49734288454055786 Training d_loss: 1.3494 Training g_loss: 0.7035 Training q_loss: 520.6428 Explore P: 0.2748\n",
      "Episode: 510 Total reward: 10.0 Average reward fake: 0.46386685967445374 Training d_loss: 1.3061 Training g_loss: 0.7755 Training q_loss: 343.7466 Explore P: 0.2745\n",
      "Episode: 511 Total reward: 11.0 Average reward fake: 0.44832223653793335 Training d_loss: 1.2145 Training g_loss: 0.8147 Training q_loss: 476.3740 Explore P: 0.2743\n",
      "Episode: 512 Total reward: 130.0 Average reward fake: 0.5057691931724548 Training d_loss: 1.3772 Training g_loss: 0.6858 Training q_loss: 300.6540 Explore P: 0.2708\n",
      "Episode: 513 Total reward: 31.0 Average reward fake: 0.5169738531112671 Training d_loss: 1.3873 Training g_loss: 0.6625 Training q_loss: 756.9860 Explore P: 0.2700\n",
      "Episode: 514 Total reward: 34.0 Average reward fake: 0.4987674355506897 Training d_loss: 1.3931 Training g_loss: 0.6957 Training q_loss: 459.1622 Explore P: 0.2691\n",
      "Episode: 515 Total reward: 37.0 Average reward fake: 0.50030916929245 Training d_loss: 1.3856 Training g_loss: 0.6924 Training q_loss: 231.0896 Explore P: 0.2682\n",
      "Episode: 516 Total reward: 89.0 Average reward fake: 0.47931113839149475 Training d_loss: 1.3390 Training g_loss: 0.7384 Training q_loss: 1024.0476 Explore P: 0.2659\n",
      "Episode: 517 Total reward: 90.0 Average reward fake: 0.5029582977294922 Training d_loss: 1.3560 Training g_loss: 0.6886 Training q_loss: 531.9719 Explore P: 0.2636\n",
      "Episode: 518 Total reward: 22.0 Average reward fake: 0.474763959646225 Training d_loss: 1.2683 Training g_loss: 0.7538 Training q_loss: 489.1386 Explore P: 0.2631\n",
      "Episode: 519 Total reward: 16.0 Average reward fake: 0.43750229477882385 Training d_loss: 1.2180 Training g_loss: 0.8385 Training q_loss: 5656.0195 Explore P: 0.2626\n",
      "Episode: 520 Total reward: 37.0 Average reward fake: 0.5480915904045105 Training d_loss: 1.4193 Training g_loss: 0.6071 Training q_loss: 1367.5349 Explore P: 0.2617\n",
      "Episode: 521 Total reward: 16.0 Average reward fake: 0.510703980922699 Training d_loss: 1.3917 Training g_loss: 0.6758 Training q_loss: 1062.6388 Explore P: 0.2613\n",
      "Episode: 522 Total reward: 21.0 Average reward fake: 0.5057679414749146 Training d_loss: 1.3918 Training g_loss: 0.6824 Training q_loss: 242.5937 Explore P: 0.2608\n",
      "Episode: 523 Total reward: 28.0 Average reward fake: 0.48162347078323364 Training d_loss: 1.3627 Training g_loss: 0.7346 Training q_loss: 289.4173 Explore P: 0.2601\n",
      "Episode: 524 Total reward: 87.0 Average reward fake: 0.4718859791755676 Training d_loss: 1.3619 Training g_loss: 0.7546 Training q_loss: 4643.0845 Explore P: 0.2579\n",
      "Episode: 525 Total reward: 38.0 Average reward fake: 0.5007715225219727 Training d_loss: 1.3996 Training g_loss: 0.6963 Training q_loss: 4972.3496 Explore P: 0.2570\n",
      "Episode: 526 Total reward: 91.0 Average reward fake: 0.5283181071281433 Training d_loss: 1.4363 Training g_loss: 0.6387 Training q_loss: 817.1110 Explore P: 0.2547\n",
      "Episode: 527 Total reward: 44.0 Average reward fake: 0.4967808127403259 Training d_loss: 1.3613 Training g_loss: 0.7010 Training q_loss: 219.4165 Explore P: 0.2537\n",
      "Episode: 528 Total reward: 55.0 Average reward fake: 0.462273508310318 Training d_loss: 1.3500 Training g_loss: 0.7756 Training q_loss: 1223.7518 Explore P: 0.2523\n",
      "Episode: 529 Total reward: 41.0 Average reward fake: 0.49990755319595337 Training d_loss: 1.3864 Training g_loss: 0.6927 Training q_loss: 76.2326 Explore P: 0.2513\n",
      "Episode: 530 Total reward: 36.0 Average reward fake: 0.4314475953578949 Training d_loss: 1.2843 Training g_loss: 0.8559 Training q_loss: 211.4989 Explore P: 0.2505\n",
      "Episode: 531 Total reward: 45.0 Average reward fake: 0.5651080012321472 Training d_loss: 1.4222 Training g_loss: 0.5741 Training q_loss: 339.3692 Explore P: 0.2494\n",
      "Episode: 532 Total reward: 56.0 Average reward fake: 0.5076130032539368 Training d_loss: 1.3804 Training g_loss: 0.6792 Training q_loss: 210.3987 Explore P: 0.2481\n",
      "Episode: 533 Total reward: 63.0 Average reward fake: 0.41859397292137146 Training d_loss: 1.2787 Training g_loss: 0.8816 Training q_loss: 65.8500 Explore P: 0.2466\n",
      "Episode: 534 Total reward: 54.0 Average reward fake: 0.4701654314994812 Training d_loss: 1.3237 Training g_loss: 0.7670 Training q_loss: 223.7508 Explore P: 0.2453\n",
      "Episode: 535 Total reward: 45.0 Average reward fake: 0.47021645307540894 Training d_loss: 1.3553 Training g_loss: 0.7632 Training q_loss: 866.4708 Explore P: 0.2442\n",
      "Episode: 536 Total reward: 79.0 Average reward fake: 0.485373318195343 Training d_loss: 1.4211 Training g_loss: 0.7397 Training q_loss: 205.1135 Explore P: 0.2424\n",
      "Episode: 537 Total reward: 56.0 Average reward fake: 0.45545998215675354 Training d_loss: 1.2873 Training g_loss: 0.8003 Training q_loss: 124.6858 Explore P: 0.2411\n",
      "Episode: 538 Total reward: 36.0 Average reward fake: 0.4414195120334625 Training d_loss: 1.3444 Training g_loss: 0.8374 Training q_loss: 668.0214 Explore P: 0.2403\n",
      "Episode: 539 Total reward: 35.0 Average reward fake: 0.5302063226699829 Training d_loss: 1.3820 Training g_loss: 0.6318 Training q_loss: 1001.5505 Explore P: 0.2395\n",
      "Episode: 540 Total reward: 73.0 Average reward fake: 0.43430835008621216 Training d_loss: 1.2594 Training g_loss: 0.8586 Training q_loss: 94.8611 Explore P: 0.2378\n",
      "Episode: 541 Total reward: 81.0 Average reward fake: 0.46149516105651855 Training d_loss: 1.2717 Training g_loss: 0.7859 Training q_loss: 810.4548 Explore P: 0.2359\n",
      "Episode: 542 Total reward: 58.0 Average reward fake: 0.4600907266139984 Training d_loss: 1.3039 Training g_loss: 0.8045 Training q_loss: 162.0797 Explore P: 0.2346\n",
      "Episode: 543 Total reward: 65.0 Average reward fake: 0.4299280047416687 Training d_loss: 1.2810 Training g_loss: 0.8772 Training q_loss: 48.2598 Explore P: 0.2332\n",
      "Episode: 544 Total reward: 58.0 Average reward fake: 0.5351434946060181 Training d_loss: 1.5001 Training g_loss: 0.6499 Training q_loss: 279.1075 Explore P: 0.2319\n",
      "Episode: 545 Total reward: 80.0 Average reward fake: 0.501196026802063 Training d_loss: 1.3594 Training g_loss: 0.7053 Training q_loss: 77.5874 Explore P: 0.2301\n",
      "Episode: 546 Total reward: 51.0 Average reward fake: 0.5304521322250366 Training d_loss: 1.4485 Training g_loss: 0.6648 Training q_loss: 117.9269 Explore P: 0.2290\n",
      "Episode: 547 Total reward: 112.0 Average reward fake: 0.5135920643806458 Training d_loss: 1.3509 Training g_loss: 0.6708 Training q_loss: 190.1510 Explore P: 0.2266\n",
      "Episode: 548 Total reward: 117.0 Average reward fake: 0.503250002861023 Training d_loss: 1.2454 Training g_loss: 0.7346 Training q_loss: 290.4930 Explore P: 0.2240\n",
      "Episode: 549 Total reward: 36.0 Average reward fake: 0.5074883699417114 Training d_loss: 1.5106 Training g_loss: 0.7214 Training q_loss: 9848.1846 Explore P: 0.2233\n",
      "Episode: 550 Total reward: 68.0 Average reward fake: 0.4727548658847809 Training d_loss: 1.2805 Training g_loss: 0.7787 Training q_loss: 472.0032 Explore P: 0.2218\n",
      "Episode: 551 Total reward: 93.0 Average reward fake: 0.47724002599716187 Training d_loss: 1.3341 Training g_loss: 0.7940 Training q_loss: 1422.2109 Explore P: 0.2199\n",
      "Episode: 552 Total reward: 128.0 Average reward fake: 0.5074597597122192 Training d_loss: 1.3594 Training g_loss: 0.6775 Training q_loss: 398.2308 Explore P: 0.2172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 553 Total reward: 60.0 Average reward fake: 0.48765498399734497 Training d_loss: 1.3686 Training g_loss: 0.7212 Training q_loss: 13916.8594 Explore P: 0.2160\n",
      "Episode: 554 Total reward: 102.0 Average reward fake: 0.5119268298149109 Training d_loss: 1.3702 Training g_loss: 0.6706 Training q_loss: 9790.9863 Explore P: 0.2139\n",
      "Episode: 555 Total reward: 92.0 Average reward fake: 0.4572151303291321 Training d_loss: 1.3154 Training g_loss: 0.8584 Training q_loss: 161.3067 Explore P: 0.2120\n",
      "Episode: 556 Total reward: 79.0 Average reward fake: 0.4745756983757019 Training d_loss: 1.3579 Training g_loss: 0.7475 Training q_loss: 326.9476 Explore P: 0.2104\n",
      "Episode: 557 Total reward: 65.0 Average reward fake: 0.4919326901435852 Training d_loss: 1.3517 Training g_loss: 0.7157 Training q_loss: 198.4046 Explore P: 0.2091\n",
      "Episode: 558 Total reward: 65.0 Average reward fake: 0.5008254647254944 Training d_loss: 1.3338 Training g_loss: 0.6979 Training q_loss: 89.2877 Explore P: 0.2078\n",
      "Episode: 559 Total reward: 74.0 Average reward fake: 0.47464752197265625 Training d_loss: 1.3998 Training g_loss: 0.7397 Training q_loss: 66.8961 Explore P: 0.2064\n",
      "Episode: 560 Total reward: 80.0 Average reward fake: 0.46950197219848633 Training d_loss: 1.3159 Training g_loss: 0.7664 Training q_loss: 110.8331 Explore P: 0.2048\n",
      "Episode: 561 Total reward: 39.0 Average reward fake: 0.521465003490448 Training d_loss: 1.4683 Training g_loss: 0.6640 Training q_loss: 302.7336 Explore P: 0.2040\n",
      "Episode: 562 Total reward: 107.0 Average reward fake: 0.5129626989364624 Training d_loss: 1.3830 Training g_loss: 0.7098 Training q_loss: 695.7201 Explore P: 0.2020\n",
      "Episode: 563 Total reward: 115.0 Average reward fake: 0.341010719537735 Training d_loss: 0.9733 Training g_loss: 1.2086 Training q_loss: 1125.0043 Explore P: 0.1998\n",
      "Episode: 564 Total reward: 31.0 Average reward fake: 0.4286936819553375 Training d_loss: 1.1456 Training g_loss: 0.8809 Training q_loss: 280.4246 Explore P: 0.1992\n",
      "Episode: 565 Total reward: 80.0 Average reward fake: 0.39993205666542053 Training d_loss: 1.1224 Training g_loss: 0.9604 Training q_loss: 139.7773 Explore P: 0.1977\n",
      "Episode: 566 Total reward: 76.0 Average reward fake: 0.4553954005241394 Training d_loss: 1.3205 Training g_loss: 0.9947 Training q_loss: 1569.9478 Explore P: 0.1963\n",
      "Episode: 567 Total reward: 109.0 Average reward fake: 0.59888756275177 Training d_loss: 1.4264 Training g_loss: 0.5367 Training q_loss: 12744.2236 Explore P: 0.1943\n",
      "Episode: 568 Total reward: 58.0 Average reward fake: 0.4480164051055908 Training d_loss: 1.1877 Training g_loss: 0.9233 Training q_loss: 106.4269 Explore P: 0.1932\n",
      "Episode: 569 Total reward: 77.0 Average reward fake: 0.4666561186313629 Training d_loss: 1.2496 Training g_loss: 0.7892 Training q_loss: 82.2778 Explore P: 0.1918\n",
      "Episode: 570 Total reward: 138.0 Average reward fake: 0.48932522535324097 Training d_loss: 1.4198 Training g_loss: 0.7444 Training q_loss: 94.2459 Explore P: 0.1893\n",
      "Episode: 571 Total reward: 199.0 Average reward fake: 0.42753997445106506 Training d_loss: 1.1528 Training g_loss: 1.0293 Training q_loss: 101.7511 Explore P: 0.1858\n",
      "Episode: 572 Total reward: 64.0 Average reward fake: 0.5769239664077759 Training d_loss: 1.4217 Training g_loss: 0.5695 Training q_loss: 197.4391 Explore P: 0.1846\n",
      "Episode: 573 Total reward: 71.0 Average reward fake: 0.5156693458557129 Training d_loss: 1.3511 Training g_loss: 0.6935 Training q_loss: 112.8570 Explore P: 0.1834\n",
      "Episode: 574 Total reward: 95.0 Average reward fake: 0.4798939824104309 Training d_loss: 1.3683 Training g_loss: 0.7989 Training q_loss: 50.8594 Explore P: 0.1818\n",
      "Episode: 575 Total reward: 120.0 Average reward fake: 0.4642147123813629 Training d_loss: 1.3620 Training g_loss: 0.9662 Training q_loss: 104.5751 Explore P: 0.1797\n",
      "Episode: 576 Total reward: 139.0 Average reward fake: 0.40040379762649536 Training d_loss: 1.2006 Training g_loss: 0.9431 Training q_loss: 1204.4269 Explore P: 0.1774\n",
      "Episode: 577 Total reward: 37.0 Average reward fake: 0.3922480642795563 Training d_loss: 1.1414 Training g_loss: 1.1238 Training q_loss: 74.9889 Explore P: 0.1768\n",
      "Episode: 578 Total reward: 156.0 Average reward fake: 0.5163707733154297 Training d_loss: 1.3074 Training g_loss: 0.7012 Training q_loss: 54.5544 Explore P: 0.1742\n",
      "Episode: 579 Total reward: 73.0 Average reward fake: 0.4055844247341156 Training d_loss: 1.1577 Training g_loss: 1.0872 Training q_loss: 106.3851 Explore P: 0.1730\n",
      "Episode: 580 Total reward: 89.0 Average reward fake: 0.5125935077667236 Training d_loss: 1.3067 Training g_loss: 0.6951 Training q_loss: 150.6831 Explore P: 0.1715\n",
      "Episode: 581 Total reward: 118.0 Average reward fake: 0.5116840600967407 Training d_loss: 1.3794 Training g_loss: 0.6707 Training q_loss: 180.4776 Explore P: 0.1696\n",
      "Episode: 582 Total reward: 199.0 Average reward fake: 0.514122724533081 Training d_loss: 1.4291 Training g_loss: 0.9168 Training q_loss: 216.1451 Explore P: 0.1665\n",
      "Episode: 583 Total reward: 63.0 Average reward fake: 0.47127848863601685 Training d_loss: 1.3122 Training g_loss: 0.9839 Training q_loss: 232.8575 Explore P: 0.1655\n",
      "Episode: 584 Total reward: 70.0 Average reward fake: 0.49725526571273804 Training d_loss: 1.3448 Training g_loss: 0.7074 Training q_loss: 846.5969 Explore P: 0.1644\n",
      "Episode: 585 Total reward: 155.0 Average reward fake: 0.4748249053955078 Training d_loss: 1.3704 Training g_loss: 0.8080 Training q_loss: 172.3517 Explore P: 0.1620\n",
      "Episode: 586 Total reward: 163.0 Average reward fake: 0.46806278824806213 Training d_loss: 1.2880 Training g_loss: 0.7771 Training q_loss: 63.3426 Explore P: 0.1596\n",
      "Episode: 587 Total reward: 92.0 Average reward fake: 0.45916348695755005 Training d_loss: 1.1391 Training g_loss: 1.0852 Training q_loss: 147.5760 Explore P: 0.1582\n",
      "Episode: 588 Total reward: 151.0 Average reward fake: 0.45229363441467285 Training d_loss: 1.3289 Training g_loss: 1.0763 Training q_loss: 9484.8770 Explore P: 0.1560\n",
      "Episode: 589 Total reward: 58.0 Average reward fake: 0.44382739067077637 Training d_loss: 1.2650 Training g_loss: 0.8283 Training q_loss: 85.6181 Explore P: 0.1552\n",
      "Episode: 590 Total reward: 72.0 Average reward fake: 0.34176382422447205 Training d_loss: 1.1372 Training g_loss: 1.7636 Training q_loss: 95.7650 Explore P: 0.1541\n",
      "Episode: 591 Total reward: 125.0 Average reward fake: 0.4559597074985504 Training d_loss: 1.3006 Training g_loss: 0.8192 Training q_loss: 334.4708 Explore P: 0.1523\n",
      "Episode: 592 Total reward: 190.0 Average reward fake: 0.5383808016777039 Training d_loss: 1.3903 Training g_loss: 0.6174 Training q_loss: 180.8665 Explore P: 0.1496\n",
      "Episode: 593 Total reward: 199.0 Average reward fake: 0.41848617792129517 Training d_loss: 1.2146 Training g_loss: 1.1573 Training q_loss: 603.8205 Explore P: 0.1469\n",
      "Episode: 594 Total reward: 199.0 Average reward fake: 0.4588180482387543 Training d_loss: 1.3037 Training g_loss: 1.0346 Training q_loss: 107.4563 Explore P: 0.1442\n",
      "Episode: 595 Total reward: 181.0 Average reward fake: 0.43042102456092834 Training d_loss: 1.1942 Training g_loss: 1.1785 Training q_loss: 176.8334 Explore P: 0.1418\n",
      "Episode: 596 Total reward: 199.0 Average reward fake: 0.4208006262779236 Training d_loss: 1.4020 Training g_loss: 1.1918 Training q_loss: 134.1843 Explore P: 0.1392\n",
      "Episode: 597 Total reward: 27.0 Average reward fake: 0.3449562191963196 Training d_loss: 1.2090 Training g_loss: 1.0383 Training q_loss: 521.0282 Explore P: 0.1388\n",
      "Episode: 598 Total reward: 26.0 Average reward fake: 0.40992075204849243 Training d_loss: 1.2831 Training g_loss: 0.9369 Training q_loss: 396.4677 Explore P: 0.1385\n",
      "Episode: 599 Total reward: 72.0 Average reward fake: 0.5159875750541687 Training d_loss: 1.3906 Training g_loss: 0.6638 Training q_loss: 266.1003 Explore P: 0.1376\n",
      "Episode: 600 Total reward: 82.0 Average reward fake: 0.44201070070266724 Training d_loss: 1.2855 Training g_loss: 1.2170 Training q_loss: 154.3225 Explore P: 0.1365\n",
      "Episode: 601 Total reward: 199.0 Average reward fake: 0.4584038257598877 Training d_loss: 1.2927 Training g_loss: 1.0879 Training q_loss: 251.6814 Explore P: 0.1341\n",
      "Episode: 602 Total reward: 75.0 Average reward fake: 0.41325560212135315 Training d_loss: 1.2097 Training g_loss: 1.0944 Training q_loss: 280.9861 Explore P: 0.1331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 603 Total reward: 101.0 Average reward fake: 0.3834112286567688 Training d_loss: 1.2371 Training g_loss: 1.2317 Training q_loss: 267.1182 Explore P: 0.1319\n",
      "Episode: 604 Total reward: 190.0 Average reward fake: 0.4246577322483063 Training d_loss: 1.2445 Training g_loss: 0.9182 Training q_loss: 196.3612 Explore P: 0.1296\n",
      "Episode: 605 Total reward: 199.0 Average reward fake: 0.4988320469856262 Training d_loss: 1.3824 Training g_loss: 0.6977 Training q_loss: 175.8946 Explore P: 0.1272\n",
      "Episode: 606 Total reward: 152.0 Average reward fake: 0.5054890513420105 Training d_loss: 1.3457 Training g_loss: 1.0079 Training q_loss: 363.1105 Explore P: 0.1255\n",
      "Episode: 607 Total reward: 199.0 Average reward fake: 0.4908926486968994 Training d_loss: 1.3667 Training g_loss: 0.7240 Training q_loss: 166.6105 Explore P: 0.1232\n",
      "Episode: 608 Total reward: 199.0 Average reward fake: 0.4083707928657532 Training d_loss: 1.2944 Training g_loss: 1.5753 Training q_loss: 280.5509 Explore P: 0.1210\n",
      "Episode: 609 Total reward: 199.0 Average reward fake: 0.44205889105796814 Training d_loss: 1.3169 Training g_loss: 1.1669 Training q_loss: 537.2697 Explore P: 0.1188\n",
      "Episode: 610 Total reward: 199.0 Average reward fake: 0.5216004848480225 Training d_loss: 1.3893 Training g_loss: 0.6560 Training q_loss: 6226.1230 Explore P: 0.1166\n",
      "Episode: 611 Total reward: 199.0 Average reward fake: 0.5102937817573547 Training d_loss: 1.3686 Training g_loss: 0.6713 Training q_loss: 218.2737 Explore P: 0.1145\n",
      "Episode: 612 Total reward: 199.0 Average reward fake: 0.41211986541748047 Training d_loss: 1.2195 Training g_loss: 1.0226 Training q_loss: 208.0101 Explore P: 0.1125\n",
      "Episode: 613 Total reward: 196.0 Average reward fake: 0.4809788167476654 Training d_loss: 1.2977 Training g_loss: 1.1319 Training q_loss: 56.4116 Explore P: 0.1105\n",
      "Episode: 614 Total reward: 199.0 Average reward fake: 0.5382708311080933 Training d_loss: 1.4766 Training g_loss: 0.6336 Training q_loss: 93.3749 Explore P: 0.1085\n",
      "Episode: 615 Total reward: 193.0 Average reward fake: 0.39865416288375854 Training d_loss: 1.1898 Training g_loss: 1.6299 Training q_loss: 104.6234 Explore P: 0.1066\n",
      "Episode: 616 Total reward: 175.0 Average reward fake: 0.5149108171463013 Training d_loss: 1.3552 Training g_loss: 0.6647 Training q_loss: 40.7733 Explore P: 0.1049\n",
      "Episode: 617 Total reward: 181.0 Average reward fake: 0.49502211809158325 Training d_loss: 1.3556 Training g_loss: 0.7018 Training q_loss: 352.8544 Explore P: 0.1032\n",
      "Episode: 618 Total reward: 170.0 Average reward fake: 0.6108927130699158 Training d_loss: 1.4176 Training g_loss: 0.5241 Training q_loss: 117.5267 Explore P: 0.1017\n",
      "Episode: 619 Total reward: 199.0 Average reward fake: 0.5166308879852295 Training d_loss: 1.3944 Training g_loss: 0.6626 Training q_loss: 168.3589 Explore P: 0.0999\n",
      "Episode: 620 Total reward: 199.0 Average reward fake: 0.48087453842163086 Training d_loss: 1.3474 Training g_loss: 0.7438 Training q_loss: 106.1819 Explore P: 0.0981\n",
      "Episode: 621 Total reward: 149.0 Average reward fake: 0.41794052720069885 Training d_loss: 1.1837 Training g_loss: 0.9143 Training q_loss: 283.8693 Explore P: 0.0968\n",
      "Episode: 622 Total reward: 20.0 Average reward fake: 0.39721250534057617 Training d_loss: 1.5184 Training g_loss: 0.9654 Training q_loss: 921.5905 Explore P: 0.0966\n",
      "Episode: 623 Total reward: 9.0 Average reward fake: 0.3718512952327728 Training d_loss: 1.0902 Training g_loss: 1.0582 Training q_loss: 441.3310 Explore P: 0.0965\n",
      "Episode: 624 Total reward: 14.0 Average reward fake: 0.34648841619491577 Training d_loss: 0.9158 Training g_loss: 1.0636 Training q_loss: 42747.4883 Explore P: 0.0964\n",
      "Episode: 625 Total reward: 12.0 Average reward fake: 0.4178491234779358 Training d_loss: 0.9355 Training g_loss: 0.8673 Training q_loss: 533.5817 Explore P: 0.0963\n",
      "Episode: 626 Total reward: 9.0 Average reward fake: 0.3245082497596741 Training d_loss: 1.0944 Training g_loss: 1.1451 Training q_loss: 538.4573 Explore P: 0.0962\n",
      "Episode: 627 Total reward: 11.0 Average reward fake: 0.35817551612854004 Training d_loss: 1.1749 Training g_loss: 1.0192 Training q_loss: 1825.8414 Explore P: 0.0961\n",
      "Episode: 628 Total reward: 10.0 Average reward fake: 0.37941399216651917 Training d_loss: 1.0685 Training g_loss: 0.9605 Training q_loss: 1280.9420 Explore P: 0.0961\n",
      "Episode: 629 Total reward: 11.0 Average reward fake: 0.27608731389045715 Training d_loss: 0.8483 Training g_loss: 1.2920 Training q_loss: 2120.6401 Explore P: 0.0960\n",
      "Episode: 630 Total reward: 11.0 Average reward fake: 0.30700182914733887 Training d_loss: 1.1963 Training g_loss: 1.1316 Training q_loss: 1729.5104 Explore P: 0.0959\n",
      "Episode: 631 Total reward: 12.0 Average reward fake: 0.31277045607566833 Training d_loss: 0.7282 Training g_loss: 1.1829 Training q_loss: 656.7488 Explore P: 0.0958\n",
      "Episode: 632 Total reward: 16.0 Average reward fake: 0.31208354234695435 Training d_loss: 1.0754 Training g_loss: 1.1583 Training q_loss: 752.4225 Explore P: 0.0956\n",
      "Episode: 633 Total reward: 15.0 Average reward fake: 0.3796565532684326 Training d_loss: 0.9688 Training g_loss: 0.9974 Training q_loss: 344.0186 Explore P: 0.0955\n",
      "Episode: 634 Total reward: 9.0 Average reward fake: 0.29638761281967163 Training d_loss: 0.8506 Training g_loss: 1.2364 Training q_loss: 105652.1094 Explore P: 0.0954\n",
      "Episode: 635 Total reward: 12.0 Average reward fake: 0.32593584060668945 Training d_loss: 0.9638 Training g_loss: 1.0912 Training q_loss: 2514.0776 Explore P: 0.0953\n",
      "Episode: 636 Total reward: 10.0 Average reward fake: 0.43103352189064026 Training d_loss: 1.0574 Training g_loss: 0.9300 Training q_loss: 571.2454 Explore P: 0.0952\n",
      "Episode: 637 Total reward: 15.0 Average reward fake: 0.3044797480106354 Training d_loss: 1.0564 Training g_loss: 1.1911 Training q_loss: 1677.6973 Explore P: 0.0951\n",
      "Episode: 638 Total reward: 18.0 Average reward fake: 0.4131799638271332 Training d_loss: 1.0242 Training g_loss: 0.9923 Training q_loss: 231.3049 Explore P: 0.0950\n",
      "Episode: 639 Total reward: 11.0 Average reward fake: 0.40044498443603516 Training d_loss: 0.9139 Training g_loss: 0.9922 Training q_loss: 635.0413 Explore P: 0.0949\n",
      "Episode: 640 Total reward: 15.0 Average reward fake: 0.30233925580978394 Training d_loss: 0.9901 Training g_loss: 1.1652 Training q_loss: 300.3743 Explore P: 0.0947\n",
      "Episode: 641 Total reward: 13.0 Average reward fake: 0.40488189458847046 Training d_loss: 1.1377 Training g_loss: 0.9690 Training q_loss: 524.4586 Explore P: 0.0946\n",
      "Episode: 642 Total reward: 11.0 Average reward fake: 0.44703131914138794 Training d_loss: 1.5570 Training g_loss: 0.9804 Training q_loss: 397.3507 Explore P: 0.0945\n",
      "Episode: 643 Total reward: 10.0 Average reward fake: 0.3514532446861267 Training d_loss: 1.1020 Training g_loss: 1.0534 Training q_loss: 1693.6207 Explore P: 0.0944\n",
      "Episode: 644 Total reward: 8.0 Average reward fake: 0.3269345760345459 Training d_loss: 1.2108 Training g_loss: 1.1156 Training q_loss: 1349.2493 Explore P: 0.0944\n",
      "Episode: 645 Total reward: 15.0 Average reward fake: 0.41491150856018066 Training d_loss: 0.9013 Training g_loss: 0.8711 Training q_loss: 709.0118 Explore P: 0.0943\n",
      "Episode: 646 Total reward: 13.0 Average reward fake: 0.5462067723274231 Training d_loss: 1.2981 Training g_loss: 0.6860 Training q_loss: 1399.9076 Explore P: 0.0941\n",
      "Episode: 647 Total reward: 21.0 Average reward fake: 0.3675224483013153 Training d_loss: 1.2959 Training g_loss: 1.0607 Training q_loss: 1035.8451 Explore P: 0.0940\n",
      "Episode: 648 Total reward: 180.0 Average reward fake: 0.428283154964447 Training d_loss: 1.2040 Training g_loss: 1.4435 Training q_loss: 558.7748 Explore P: 0.0925\n",
      "Episode: 649 Total reward: 138.0 Average reward fake: 0.33042728900909424 Training d_loss: 1.0662 Training g_loss: 1.9581 Training q_loss: 339.7992 Explore P: 0.0913\n",
      "Episode: 650 Total reward: 177.0 Average reward fake: 0.5163881778717041 Training d_loss: 1.3599 Training g_loss: 0.6825 Training q_loss: 469.5030 Explore P: 0.0899\n",
      "Episode: 651 Total reward: 199.0 Average reward fake: 0.4624273180961609 Training d_loss: 1.2126 Training g_loss: 1.1272 Training q_loss: 343.0894 Explore P: 0.0883\n",
      "Episode: 652 Total reward: 199.0 Average reward fake: 0.49252206087112427 Training d_loss: 1.3880 Training g_loss: 0.7261 Training q_loss: 373.9908 Explore P: 0.0868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 653 Total reward: 199.0 Average reward fake: 0.5038773417472839 Training d_loss: 1.3384 Training g_loss: 1.0996 Training q_loss: 178.5770 Explore P: 0.0853\n",
      "Episode: 654 Total reward: 194.0 Average reward fake: 0.4521295130252838 Training d_loss: 1.4062 Training g_loss: 0.8116 Training q_loss: 269.5097 Explore P: 0.0838\n",
      "Episode: 655 Total reward: 199.0 Average reward fake: 0.4492630064487457 Training d_loss: 1.2938 Training g_loss: 1.2379 Training q_loss: 131.3466 Explore P: 0.0824\n",
      "Episode: 656 Total reward: 199.0 Average reward fake: 0.39636602997779846 Training d_loss: 1.2350 Training g_loss: 0.9761 Training q_loss: 6641.4229 Explore P: 0.0810\n",
      "Episode: 657 Total reward: 199.0 Average reward fake: 0.39676356315612793 Training d_loss: 1.4306 Training g_loss: 0.9532 Training q_loss: 567.2509 Explore P: 0.0796\n",
      "Episode: 658 Total reward: 189.0 Average reward fake: 0.4739746153354645 Training d_loss: 1.3332 Training g_loss: 0.7647 Training q_loss: 487.6556 Explore P: 0.0783\n",
      "Episode: 659 Total reward: 199.0 Average reward fake: 0.5089880228042603 Training d_loss: 1.2065 Training g_loss: 0.7105 Training q_loss: 211.0523 Explore P: 0.0769\n",
      "Episode: 660 Total reward: 199.0 Average reward fake: 0.5428892970085144 Training d_loss: 1.2956 Training g_loss: 0.6242 Training q_loss: 390.9127 Explore P: 0.0756\n",
      "Episode: 661 Total reward: 199.0 Average reward fake: 0.504543662071228 Training d_loss: 1.3542 Training g_loss: 1.1532 Training q_loss: 76.1916 Explore P: 0.0743\n",
      "Episode: 662 Total reward: 199.0 Average reward fake: 0.4971643388271332 Training d_loss: 1.3396 Training g_loss: 0.8147 Training q_loss: 115.4490 Explore P: 0.0730\n",
      "Episode: 663 Total reward: 199.0 Average reward fake: 0.43525463342666626 Training d_loss: 1.2264 Training g_loss: 1.4134 Training q_loss: 219.0815 Explore P: 0.0718\n",
      "Episode: 664 Total reward: 168.0 Average reward fake: 0.3671784996986389 Training d_loss: 1.1722 Training g_loss: 1.8935 Training q_loss: 235.8318 Explore P: 0.0708\n",
      "Episode: 665 Total reward: 199.0 Average reward fake: 0.5062870383262634 Training d_loss: 1.3335 Training g_loss: 1.1312 Training q_loss: 311.6529 Explore P: 0.0696\n",
      "Episode: 666 Total reward: 199.0 Average reward fake: 0.4381760060787201 Training d_loss: 1.3128 Training g_loss: 1.3005 Training q_loss: 340.5989 Explore P: 0.0684\n",
      "Episode: 667 Total reward: 199.0 Average reward fake: 0.4854135513305664 Training d_loss: 1.3426 Training g_loss: 0.7666 Training q_loss: 147.3365 Explore P: 0.0672\n",
      "Episode: 668 Total reward: 199.0 Average reward fake: 0.4886384904384613 Training d_loss: 1.3188 Training g_loss: 1.0920 Training q_loss: 96.8565 Explore P: 0.0661\n",
      "Episode: 669 Total reward: 199.0 Average reward fake: 0.5186840891838074 Training d_loss: 1.3688 Training g_loss: 0.6834 Training q_loss: 267.9990 Explore P: 0.0650\n",
      "Episode: 670 Total reward: 199.0 Average reward fake: 0.45031943917274475 Training d_loss: 1.2842 Training g_loss: 0.8923 Training q_loss: 635.6109 Explore P: 0.0639\n",
      "Episode: 671 Total reward: 199.0 Average reward fake: 0.37605375051498413 Training d_loss: 1.1191 Training g_loss: 2.4466 Training q_loss: 230.3848 Explore P: 0.0629\n",
      "Episode: 672 Total reward: 181.0 Average reward fake: 0.4016263484954834 Training d_loss: 1.1990 Training g_loss: 1.5347 Training q_loss: 201.7347 Explore P: 0.0619\n",
      "Episode: 673 Total reward: 199.0 Average reward fake: 0.5170620679855347 Training d_loss: 1.4198 Training g_loss: 0.6761 Training q_loss: 192.0242 Explore P: 0.0609\n",
      "Episode: 674 Total reward: 199.0 Average reward fake: 0.3180968165397644 Training d_loss: 1.0237 Training g_loss: 2.4961 Training q_loss: 128.2512 Explore P: 0.0599\n",
      "Episode: 675 Total reward: 199.0 Average reward fake: 0.5098494291305542 Training d_loss: 1.3644 Training g_loss: 0.6697 Training q_loss: 72.8250 Explore P: 0.0589\n",
      "Episode: 676 Total reward: 199.0 Average reward fake: 0.44105610251426697 Training d_loss: 1.2742 Training g_loss: 1.3911 Training q_loss: 140.9046 Explore P: 0.0579\n",
      "Episode: 677 Total reward: 199.0 Average reward fake: 0.46538281440734863 Training d_loss: 1.2135 Training g_loss: 1.0985 Training q_loss: 96.8814 Explore P: 0.0570\n",
      "Episode: 678 Total reward: 199.0 Average reward fake: 0.5636075735092163 Training d_loss: 1.4132 Training g_loss: 0.5763 Training q_loss: 6141.6880 Explore P: 0.0561\n",
      "Episode: 679 Total reward: 199.0 Average reward fake: 0.5163937211036682 Training d_loss: 1.3208 Training g_loss: 1.2713 Training q_loss: 65.0140 Explore P: 0.0552\n",
      "Episode: 680 Total reward: 199.0 Average reward fake: 0.4778805375099182 Training d_loss: 1.2948 Training g_loss: 0.7567 Training q_loss: 142.3609 Explore P: 0.0543\n",
      "Episode: 681 Total reward: 199.0 Average reward fake: 0.441946417093277 Training d_loss: 1.2671 Training g_loss: 1.4659 Training q_loss: 117.5245 Explore P: 0.0534\n",
      "Episode: 682 Total reward: 199.0 Average reward fake: 0.3936101496219635 Training d_loss: 1.1731 Training g_loss: 2.2909 Training q_loss: 56.0607 Explore P: 0.0525\n",
      "Episode: 683 Total reward: 199.0 Average reward fake: 0.4751412868499756 Training d_loss: 1.3101 Training g_loss: 1.2550 Training q_loss: 214.6077 Explore P: 0.0517\n",
      "Episode: 684 Total reward: 199.0 Average reward fake: 0.5069783329963684 Training d_loss: 1.3784 Training g_loss: 0.6735 Training q_loss: 71.2409 Explore P: 0.0509\n",
      "Episode: 685 Total reward: 199.0 Average reward fake: 0.4236845374107361 Training d_loss: 1.2493 Training g_loss: 1.7988 Training q_loss: 63.5817 Explore P: 0.0501\n",
      "Episode: 686 Total reward: 199.0 Average reward fake: 0.45609474182128906 Training d_loss: 1.2470 Training g_loss: 1.5372 Training q_loss: 245.5667 Explore P: 0.0493\n",
      "Episode: 687 Total reward: 199.0 Average reward fake: 0.5137230157852173 Training d_loss: 1.3171 Training g_loss: 1.4001 Training q_loss: 200.5014 Explore P: 0.0485\n",
      "Episode: 688 Total reward: 199.0 Average reward fake: 0.4890437126159668 Training d_loss: 1.2936 Training g_loss: 0.7939 Training q_loss: 89.7421 Explore P: 0.0478\n",
      "Episode: 689 Total reward: 199.0 Average reward fake: 0.46239718794822693 Training d_loss: 1.2851 Training g_loss: 2.2629 Training q_loss: 157.3178 Explore P: 0.0470\n",
      "Episode: 690 Total reward: 199.0 Average reward fake: 0.4348301291465759 Training d_loss: 1.2808 Training g_loss: 1.5682 Training q_loss: 80.6269 Explore P: 0.0463\n",
      "Episode: 691 Total reward: 182.0 Average reward fake: 0.4589802324771881 Training d_loss: 1.3315 Training g_loss: 0.7835 Training q_loss: 106.7586 Explore P: 0.0456\n",
      "Episode: 692 Total reward: 199.0 Average reward fake: 0.42363429069519043 Training d_loss: 1.1454 Training g_loss: 2.2996 Training q_loss: 46.2915 Explore P: 0.0449\n",
      "Episode: 693 Total reward: 183.0 Average reward fake: 0.4328485429286957 Training d_loss: 1.2953 Training g_loss: 1.6821 Training q_loss: 577.3193 Explore P: 0.0443\n",
      "Episode: 694 Total reward: 199.0 Average reward fake: 0.3151479661464691 Training d_loss: 1.3129 Training g_loss: 1.9128 Training q_loss: 90.7001 Explore P: 0.0436\n",
      "Episode: 695 Total reward: 199.0 Average reward fake: 0.4338095784187317 Training d_loss: 1.2934 Training g_loss: 1.6277 Training q_loss: 41.1339 Explore P: 0.0430\n",
      "Episode: 696 Total reward: 199.0 Average reward fake: 0.47009173035621643 Training d_loss: 1.3188 Training g_loss: 0.8052 Training q_loss: 209.6539 Explore P: 0.0423\n",
      "Episode: 697 Total reward: 199.0 Average reward fake: 0.4878675937652588 Training d_loss: 1.3058 Training g_loss: 0.8666 Training q_loss: 45.6815 Explore P: 0.0417\n",
      "Episode: 698 Total reward: 199.0 Average reward fake: 0.4590446352958679 Training d_loss: 1.1749 Training g_loss: 1.8098 Training q_loss: 75.3452 Explore P: 0.0410\n",
      "Episode: 699 Total reward: 199.0 Average reward fake: 0.4990466237068176 Training d_loss: 1.3060 Training g_loss: 1.6205 Training q_loss: 26.2974 Explore P: 0.0404\n",
      "Episode: 700 Total reward: 199.0 Average reward fake: 0.5313788652420044 Training d_loss: 1.3140 Training g_loss: 1.2748 Training q_loss: 64.0930 Explore P: 0.0398\n",
      "Episode: 701 Total reward: 199.0 Average reward fake: 0.48423418402671814 Training d_loss: 1.3282 Training g_loss: 1.7809 Training q_loss: 84.9393 Explore P: 0.0392\n",
      "Episode: 702 Total reward: 199.0 Average reward fake: 0.4507881999015808 Training d_loss: 1.2926 Training g_loss: 1.8267 Training q_loss: 66.7633 Explore P: 0.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 703 Total reward: 199.0 Average reward fake: 0.49377408623695374 Training d_loss: 1.3191 Training g_loss: 0.7611 Training q_loss: 151.2204 Explore P: 0.0381\n",
      "Episode: 704 Total reward: 197.0 Average reward fake: 0.38235655426979065 Training d_loss: 1.0743 Training g_loss: 2.0383 Training q_loss: 32.9227 Explore P: 0.0376\n",
      "Episode: 705 Total reward: 167.0 Average reward fake: 0.26564639806747437 Training d_loss: 0.9398 Training g_loss: 4.4013 Training q_loss: 66.1194 Explore P: 0.0371\n",
      "Episode: 706 Total reward: 199.0 Average reward fake: 0.4088762402534485 Training d_loss: 1.1665 Training g_loss: 2.1936 Training q_loss: 44.1580 Explore P: 0.0366\n",
      "Episode: 707 Total reward: 199.0 Average reward fake: 0.37487703561782837 Training d_loss: 1.1508 Training g_loss: 3.0339 Training q_loss: 88.9330 Explore P: 0.0360\n",
      "Episode: 708 Total reward: 199.0 Average reward fake: 0.5430618524551392 Training d_loss: 1.3515 Training g_loss: 1.6713 Training q_loss: 37.7320 Explore P: 0.0355\n",
      "Episode: 709 Total reward: 199.0 Average reward fake: 0.4959282875061035 Training d_loss: 1.3120 Training g_loss: 1.7856 Training q_loss: 36.3096 Explore P: 0.0350\n",
      "Episode: 710 Total reward: 199.0 Average reward fake: 0.35075005888938904 Training d_loss: 1.0440 Training g_loss: 3.1899 Training q_loss: 30.6232 Explore P: 0.0345\n",
      "Episode: 711 Total reward: 199.0 Average reward fake: 0.47069019079208374 Training d_loss: 1.1988 Training g_loss: 1.6424 Training q_loss: 2274.4292 Explore P: 0.0340\n",
      "Episode: 712 Total reward: 199.0 Average reward fake: 0.42168861627578735 Training d_loss: 1.2236 Training g_loss: 1.8598 Training q_loss: 10.7855 Explore P: 0.0336\n",
      "Episode: 713 Total reward: 199.0 Average reward fake: 0.537798285484314 Training d_loss: 1.3782 Training g_loss: 0.6489 Training q_loss: 28.6465 Explore P: 0.0331\n",
      "Episode: 714 Total reward: 153.0 Average reward fake: 0.4860538840293884 Training d_loss: 1.3031 Training g_loss: 2.4882 Training q_loss: 93.3015 Explore P: 0.0328\n",
      "Episode: 715 Total reward: 184.0 Average reward fake: 0.44157958030700684 Training d_loss: 1.3028 Training g_loss: 1.9917 Training q_loss: 65.9062 Explore P: 0.0323\n",
      "Episode: 716 Total reward: 199.0 Average reward fake: 0.3891904950141907 Training d_loss: 1.1065 Training g_loss: 2.5641 Training q_loss: 29.1939 Explore P: 0.0319\n",
      "Episode: 717 Total reward: 199.0 Average reward fake: 0.5193301439285278 Training d_loss: 1.3562 Training g_loss: 1.8622 Training q_loss: 45.1789 Explore P: 0.0315\n",
      "Episode: 718 Total reward: 199.0 Average reward fake: 0.5072653293609619 Training d_loss: 1.2926 Training g_loss: 3.1742 Training q_loss: 130.3290 Explore P: 0.0311\n",
      "Episode: 719 Total reward: 199.0 Average reward fake: 0.48032107949256897 Training d_loss: 1.1954 Training g_loss: 2.0531 Training q_loss: 37.0096 Explore P: 0.0306\n",
      "Episode: 720 Total reward: 199.0 Average reward fake: 0.4550931453704834 Training d_loss: 1.2106 Training g_loss: 0.7938 Training q_loss: 33.2335 Explore P: 0.0302\n",
      "Episode: 721 Total reward: 170.0 Average reward fake: 0.36825793981552124 Training d_loss: 1.0784 Training g_loss: 3.6748 Training q_loss: 176.1780 Explore P: 0.0299\n",
      "Episode: 722 Total reward: 199.0 Average reward fake: 0.3497214615345001 Training d_loss: 1.0409 Training g_loss: 1.1262 Training q_loss: 51.5523 Explore P: 0.0295\n",
      "Episode: 723 Total reward: 16.0 Average reward fake: 0.3016117215156555 Training d_loss: 0.8521 Training g_loss: 1.2003 Training q_loss: 71.3104 Explore P: 0.0295\n",
      "Episode: 724 Total reward: 9.0 Average reward fake: 0.35189834237098694 Training d_loss: 0.9634 Training g_loss: 1.0199 Training q_loss: 237.6198 Explore P: 0.0294\n",
      "Episode: 725 Total reward: 11.0 Average reward fake: 0.3797956109046936 Training d_loss: 0.7752 Training g_loss: 0.9947 Training q_loss: 752.9854 Explore P: 0.0294\n",
      "Episode: 726 Total reward: 8.0 Average reward fake: 0.33931490778923035 Training d_loss: 0.8519 Training g_loss: 1.0951 Training q_loss: 855.8766 Explore P: 0.0294\n",
      "Episode: 727 Total reward: 9.0 Average reward fake: 0.29174545407295227 Training d_loss: 0.8420 Training g_loss: 1.2512 Training q_loss: 1414.6648 Explore P: 0.0294\n",
      "Episode: 728 Total reward: 9.0 Average reward fake: 0.2960042953491211 Training d_loss: 0.9625 Training g_loss: 1.1945 Training q_loss: 2440.8318 Explore P: 0.0294\n",
      "Episode: 729 Total reward: 11.0 Average reward fake: 0.3413657248020172 Training d_loss: 0.6361 Training g_loss: 1.0902 Training q_loss: 2658.7273 Explore P: 0.0294\n",
      "Episode: 730 Total reward: 9.0 Average reward fake: 0.31728416681289673 Training d_loss: 1.1864 Training g_loss: 1.1283 Training q_loss: 6328.7261 Explore P: 0.0293\n",
      "Episode: 731 Total reward: 9.0 Average reward fake: 0.38596293330192566 Training d_loss: 0.8704 Training g_loss: 0.9337 Training q_loss: 7433.6064 Explore P: 0.0293\n",
      "Episode: 732 Total reward: 8.0 Average reward fake: 0.3510770797729492 Training d_loss: 0.6443 Training g_loss: 1.0900 Training q_loss: 7731.3320 Explore P: 0.0293\n",
      "Episode: 733 Total reward: 12.0 Average reward fake: 0.27738118171691895 Training d_loss: 0.7118 Training g_loss: 1.3008 Training q_loss: 10789.0830 Explore P: 0.0293\n",
      "Episode: 734 Total reward: 11.0 Average reward fake: 0.34180310368537903 Training d_loss: 1.0635 Training g_loss: 1.0299 Training q_loss: 10327.8291 Explore P: 0.0293\n",
      "Episode: 735 Total reward: 9.0 Average reward fake: 0.40280357003211975 Training d_loss: 0.9714 Training g_loss: 0.9209 Training q_loss: 22736.8477 Explore P: 0.0292\n",
      "Episode: 736 Total reward: 14.0 Average reward fake: 0.2889622747898102 Training d_loss: 0.7151 Training g_loss: 1.2329 Training q_loss: 14617.0801 Explore P: 0.0292\n",
      "Episode: 737 Total reward: 9.0 Average reward fake: 0.3099822998046875 Training d_loss: 0.9577 Training g_loss: 1.1635 Training q_loss: 27288.3379 Explore P: 0.0292\n",
      "Episode: 738 Total reward: 11.0 Average reward fake: 0.3438112139701843 Training d_loss: 0.9561 Training g_loss: 1.0550 Training q_loss: 66502.0781 Explore P: 0.0292\n",
      "Episode: 739 Total reward: 8.0 Average reward fake: 0.36292049288749695 Training d_loss: 1.1609 Training g_loss: 1.0117 Training q_loss: 86574.3203 Explore P: 0.0292\n",
      "Episode: 740 Total reward: 7.0 Average reward fake: 0.34356144070625305 Training d_loss: 0.7426 Training g_loss: 1.0824 Training q_loss: 172873.1406 Explore P: 0.0291\n",
      "Episode: 741 Total reward: 9.0 Average reward fake: 0.3130730986595154 Training d_loss: 1.1889 Training g_loss: 1.1536 Training q_loss: 231192.6719 Explore P: 0.0291\n",
      "Episode: 742 Total reward: 8.0 Average reward fake: 0.33199167251586914 Training d_loss: 0.9555 Training g_loss: 1.0935 Training q_loss: 445479.5938 Explore P: 0.0291\n",
      "Episode: 743 Total reward: 10.0 Average reward fake: 0.36087682843208313 Training d_loss: 0.9579 Training g_loss: 1.0305 Training q_loss: 22123878.0000 Explore P: 0.0291\n",
      "Episode: 744 Total reward: 8.0 Average reward fake: 0.33053964376449585 Training d_loss: 0.6237 Training g_loss: 1.1364 Training q_loss: 729419.8750 Explore P: 0.0291\n",
      "Episode: 745 Total reward: 9.0 Average reward fake: 0.3042542636394501 Training d_loss: 0.8394 Training g_loss: 1.1780 Training q_loss: 428534.1875 Explore P: 0.0291\n",
      "Episode: 746 Total reward: 9.0 Average reward fake: 0.34743601083755493 Training d_loss: 0.9560 Training g_loss: 1.0495 Training q_loss: 308691.3438 Explore P: 0.0290\n",
      "Episode: 747 Total reward: 8.0 Average reward fake: 0.30960968136787415 Training d_loss: 0.8401 Training g_loss: 1.1954 Training q_loss: 253550.2500 Explore P: 0.0290\n",
      "Episode: 748 Total reward: 11.0 Average reward fake: 0.3214750289916992 Training d_loss: 0.9557 Training g_loss: 1.1349 Training q_loss: 344736.0938 Explore P: 0.0290\n",
      "Episode: 749 Total reward: 10.0 Average reward fake: 0.35188740491867065 Training d_loss: 1.0607 Training g_loss: 1.0307 Training q_loss: 338354.3750 Explore P: 0.0290\n",
      "Episode: 750 Total reward: 9.0 Average reward fake: 0.3646307587623596 Training d_loss: 0.7568 Training g_loss: 1.0211 Training q_loss: 550389.3750 Explore P: 0.0290\n",
      "Episode: 751 Total reward: 9.0 Average reward fake: 0.3380431532859802 Training d_loss: 1.1720 Training g_loss: 1.0763 Training q_loss: 268978.4375 Explore P: 0.0290\n",
      "Episode: 752 Total reward: 9.0 Average reward fake: 0.3314639925956726 Training d_loss: 1.0655 Training g_loss: 1.1182 Training q_loss: 367783.9062 Explore P: 0.0289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 753 Total reward: 9.0 Average reward fake: 0.3010030686855316 Training d_loss: 1.0788 Training g_loss: 1.1880 Training q_loss: 321992.1562 Explore P: 0.0289\n",
      "Episode: 754 Total reward: 11.0 Average reward fake: 0.33519867062568665 Training d_loss: 0.6274 Training g_loss: 1.1001 Training q_loss: 359293.4688 Explore P: 0.0289\n",
      "Episode: 755 Total reward: 11.0 Average reward fake: 0.2885369658470154 Training d_loss: 0.8380 Training g_loss: 1.2579 Training q_loss: 181230.3906 Explore P: 0.0289\n",
      "Episode: 756 Total reward: 7.0 Average reward fake: 0.29305723309516907 Training d_loss: 0.7155 Training g_loss: 1.2188 Training q_loss: 189928.5469 Explore P: 0.0289\n",
      "Episode: 757 Total reward: 11.0 Average reward fake: 0.29532742500305176 Training d_loss: 0.9602 Training g_loss: 1.2068 Training q_loss: 91280.7344 Explore P: 0.0288\n",
      "Episode: 758 Total reward: 10.0 Average reward fake: 0.35570529103279114 Training d_loss: 1.0600 Training g_loss: 1.0194 Training q_loss: 65278.3047 Explore P: 0.0288\n",
      "Episode: 759 Total reward: 11.0 Average reward fake: 0.34808167815208435 Training d_loss: 0.9558 Training g_loss: 1.0541 Training q_loss: 100601.8359 Explore P: 0.0288\n",
      "Episode: 760 Total reward: 9.0 Average reward fake: 0.3631057143211365 Training d_loss: 0.8567 Training g_loss: 1.0045 Training q_loss: 75256.9609 Explore P: 0.0288\n",
      "Episode: 761 Total reward: 10.0 Average reward fake: 0.3532126545906067 Training d_loss: 0.7483 Training g_loss: 1.0508 Training q_loss: 109186.8984 Explore P: 0.0288\n",
      "Episode: 762 Total reward: 8.0 Average reward fake: 0.3649559020996094 Training d_loss: 1.0590 Training g_loss: 1.0004 Training q_loss: 81831.3281 Explore P: 0.0288\n",
      "Episode: 763 Total reward: 9.0 Average reward fake: 0.3825570046901703 Training d_loss: 0.9628 Training g_loss: 0.9762 Training q_loss: 16436432.0000 Explore P: 0.0287\n",
      "Episode: 764 Total reward: 11.0 Average reward fake: 0.315330445766449 Training d_loss: 0.9561 Training g_loss: 1.1779 Training q_loss: 88831.4766 Explore P: 0.0287\n",
      "Episode: 765 Total reward: 9.0 Average reward fake: 0.3040357232093811 Training d_loss: 0.8390 Training g_loss: 1.1781 Training q_loss: 72713.1016 Explore P: 0.0287\n",
      "Episode: 766 Total reward: 11.0 Average reward fake: 0.37015074491500854 Training d_loss: 1.0588 Training g_loss: 0.9774 Training q_loss: 73657.5000 Explore P: 0.0287\n",
      "Episode: 767 Total reward: 14.0 Average reward fake: 0.35317474603652954 Training d_loss: 0.7482 Training g_loss: 1.0555 Training q_loss: 67909.5781 Explore P: 0.0287\n",
      "Episode: 768 Total reward: 11.0 Average reward fake: 0.3551085591316223 Training d_loss: 0.8531 Training g_loss: 1.0273 Training q_loss: 67076.1797 Explore P: 0.0286\n",
      "Episode: 769 Total reward: 8.0 Average reward fake: 0.36722224950790405 Training d_loss: 0.8586 Training g_loss: 1.0168 Training q_loss: 68330.0234 Explore P: 0.0286\n",
      "Episode: 770 Total reward: 9.0 Average reward fake: 0.32145214080810547 Training d_loss: 0.9555 Training g_loss: 1.1541 Training q_loss: 64071.1758 Explore P: 0.0286\n",
      "Episode: 771 Total reward: 9.0 Average reward fake: 0.2907647490501404 Training d_loss: 1.0849 Training g_loss: 1.2112 Training q_loss: 51579.6875 Explore P: 0.0286\n",
      "Episode: 772 Total reward: 9.0 Average reward fake: 0.37430915236473083 Training d_loss: 0.8622 Training g_loss: 0.9641 Training q_loss: 31220.1504 Explore P: 0.0286\n",
      "Episode: 773 Total reward: 8.0 Average reward fake: 0.3805040419101715 Training d_loss: 1.0588 Training g_loss: 0.9843 Training q_loss: 23618.3379 Explore P: 0.0286\n",
      "Episode: 774 Total reward: 9.0 Average reward fake: 0.3205046057701111 Training d_loss: 0.6143 Training g_loss: 1.1631 Training q_loss: 10154.5879 Explore P: 0.0285\n",
      "Episode: 775 Total reward: 10.0 Average reward fake: 0.3824809491634369 Training d_loss: 1.5925 Training g_loss: 1.0214 Training q_loss: 151468.7344 Explore P: 0.0285\n",
      "Episode: 776 Total reward: 10.0 Average reward fake: 0.3412620425224304 Training d_loss: 0.9556 Training g_loss: 1.0700 Training q_loss: 14954.9248 Explore P: 0.0285\n",
      "Episode: 777 Total reward: 10.0 Average reward fake: 0.34453558921813965 Training d_loss: 1.2757 Training g_loss: 1.0700 Training q_loss: 13826.5859 Explore P: 0.0285\n",
      "Episode: 778 Total reward: 12.0 Average reward fake: 0.3445958197116852 Training d_loss: 0.9598 Training g_loss: 1.0492 Training q_loss: 4093.3052 Explore P: 0.0285\n",
      "Episode: 779 Total reward: 9.0 Average reward fake: 0.3562147617340088 Training d_loss: 0.9633 Training g_loss: 1.0301 Training q_loss: 1921.8586 Explore P: 0.0284\n",
      "Episode: 780 Total reward: 9.0 Average reward fake: 0.36465829610824585 Training d_loss: 1.2643 Training g_loss: 0.9885 Training q_loss: 2196.9160 Explore P: 0.0284\n",
      "Episode: 781 Total reward: 11.0 Average reward fake: 0.4345049262046814 Training d_loss: 1.3095 Training g_loss: 0.8985 Training q_loss: 66605.3281 Explore P: 0.0284\n",
      "Episode: 782 Total reward: 12.0 Average reward fake: 0.36063456535339355 Training d_loss: 1.1718 Training g_loss: 1.0196 Training q_loss: 3041.1414 Explore P: 0.0284\n",
      "Episode: 783 Total reward: 14.0 Average reward fake: 0.39171820878982544 Training d_loss: 1.3606 Training g_loss: 1.0134 Training q_loss: 21723.0840 Explore P: 0.0284\n",
      "Episode: 784 Total reward: 18.0 Average reward fake: 0.35440200567245483 Training d_loss: 1.2785 Training g_loss: 1.0388 Training q_loss: 4726.2783 Explore P: 0.0283\n",
      "Episode: 785 Total reward: 79.0 Average reward fake: 0.41444873809814453 Training d_loss: 1.6859 Training g_loss: 0.9254 Training q_loss: 180078.0469 Explore P: 0.0282\n",
      "Episode: 786 Total reward: 18.0 Average reward fake: 0.38329362869262695 Training d_loss: 1.0073 Training g_loss: 0.9624 Training q_loss: 14911.1348 Explore P: 0.0282\n",
      "Episode: 787 Total reward: 34.0 Average reward fake: 0.3997983932495117 Training d_loss: 1.0829 Training g_loss: 0.9511 Training q_loss: 42455.3242 Explore P: 0.0281\n",
      "Episode: 788 Total reward: 16.0 Average reward fake: 0.36519891023635864 Training d_loss: 1.0315 Training g_loss: 1.0428 Training q_loss: 133143.8438 Explore P: 0.0281\n",
      "Episode: 789 Total reward: 22.0 Average reward fake: 0.38246655464172363 Training d_loss: 1.2738 Training g_loss: 1.0200 Training q_loss: 31826.6875 Explore P: 0.0280\n",
      "Episode: 790 Total reward: 32.0 Average reward fake: 0.6217997670173645 Training d_loss: 1.6436 Training g_loss: 0.7434 Training q_loss: 85469.9219 Explore P: 0.0280\n",
      "Episode: 791 Total reward: 56.0 Average reward fake: 0.40533652901649475 Training d_loss: 1.2071 Training g_loss: 0.9228 Training q_loss: 47762.6445 Explore P: 0.0279\n",
      "Episode: 792 Total reward: 33.0 Average reward fake: 0.42979440093040466 Training d_loss: 1.1357 Training g_loss: 0.8587 Training q_loss: 41216.2891 Explore P: 0.0278\n",
      "Episode: 793 Total reward: 63.0 Average reward fake: 0.3058376908302307 Training d_loss: 0.8978 Training g_loss: 1.1545 Training q_loss: 7485.2432 Explore P: 0.0277\n",
      "Episode: 794 Total reward: 24.0 Average reward fake: 0.34906822443008423 Training d_loss: 1.0177 Training g_loss: 1.0352 Training q_loss: 8772.5332 Explore P: 0.0276\n",
      "Episode: 795 Total reward: 17.0 Average reward fake: 0.3586013913154602 Training d_loss: 0.8895 Training g_loss: 1.0607 Training q_loss: 5973.8950 Explore P: 0.0276\n",
      "Episode: 796 Total reward: 17.0 Average reward fake: 0.3267975449562073 Training d_loss: 0.9696 Training g_loss: 1.0787 Training q_loss: 4317.6772 Explore P: 0.0276\n",
      "Episode: 797 Total reward: 20.0 Average reward fake: 0.27378368377685547 Training d_loss: 1.1031 Training g_loss: 1.2710 Training q_loss: 3659.3835 Explore P: 0.0276\n",
      "Episode: 798 Total reward: 15.0 Average reward fake: 0.2995656132698059 Training d_loss: 1.0843 Training g_loss: 1.1825 Training q_loss: 5557.5454 Explore P: 0.0275\n",
      "Episode: 799 Total reward: 15.0 Average reward fake: 0.3427290618419647 Training d_loss: 1.0658 Training g_loss: 1.0785 Training q_loss: 6867778.5000 Explore P: 0.0275\n",
      "Episode: 800 Total reward: 24.0 Average reward fake: 0.36547356843948364 Training d_loss: 0.7648 Training g_loss: 0.9918 Training q_loss: 11366.8379 Explore P: 0.0275\n",
      "Episode: 801 Total reward: 13.0 Average reward fake: 0.32345515489578247 Training d_loss: 0.7393 Training g_loss: 1.1416 Training q_loss: 45902.6758 Explore P: 0.0274\n",
      "Episode: 802 Total reward: 15.0 Average reward fake: 0.335968554019928 Training d_loss: 1.0687 Training g_loss: 1.0532 Training q_loss: 1255.6047 Explore P: 0.0274\n",
      "Episode: 803 Total reward: 15.0 Average reward fake: 0.368708074092865 Training d_loss: 0.9637 Training g_loss: 1.0094 Training q_loss: 18597412.0000 Explore P: 0.0274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 804 Total reward: 14.0 Average reward fake: 0.32707592844963074 Training d_loss: 0.6260 Training g_loss: 1.1247 Training q_loss: 13140941.0000 Explore P: 0.0274\n",
      "Episode: 805 Total reward: 113.0 Average reward fake: 0.4506555199623108 Training d_loss: 1.2275 Training g_loss: 0.8976 Training q_loss: 5378.8154 Explore P: 0.0272\n",
      "Episode: 806 Total reward: 81.0 Average reward fake: 0.33591052889823914 Training d_loss: 0.9271 Training g_loss: 1.0964 Training q_loss: 18101.8535 Explore P: 0.0270\n",
      "Episode: 807 Total reward: 25.0 Average reward fake: 0.4055918753147125 Training d_loss: 1.1914 Training g_loss: 0.9426 Training q_loss: 5136.9697 Explore P: 0.0270\n",
      "Episode: 808 Total reward: 14.0 Average reward fake: 0.3310618996620178 Training d_loss: 0.8846 Training g_loss: 1.1061 Training q_loss: 4373.3564 Explore P: 0.0270\n",
      "Episode: 809 Total reward: 11.0 Average reward fake: 0.3169332444667816 Training d_loss: 1.1981 Training g_loss: 1.1436 Training q_loss: 890.3758 Explore P: 0.0269\n",
      "Episode: 810 Total reward: 13.0 Average reward fake: 0.3340681195259094 Training d_loss: 1.2888 Training g_loss: 1.0866 Training q_loss: 1884.9006 Explore P: 0.0269\n",
      "Episode: 811 Total reward: 9.0 Average reward fake: 0.3416750729084015 Training d_loss: 0.9652 Training g_loss: 1.0843 Training q_loss: 12215.7061 Explore P: 0.0269\n",
      "Episode: 812 Total reward: 8.0 Average reward fake: 0.33522576093673706 Training d_loss: 0.9639 Training g_loss: 1.0878 Training q_loss: 1486.0817 Explore P: 0.0269\n",
      "Episode: 813 Total reward: 10.0 Average reward fake: 0.32832640409469604 Training d_loss: 0.9624 Training g_loss: 1.1075 Training q_loss: 1100.7043 Explore P: 0.0269\n",
      "Episode: 814 Total reward: 11.0 Average reward fake: 0.33997783064842224 Training d_loss: 0.8544 Training g_loss: 1.0765 Training q_loss: 3218.9141 Explore P: 0.0269\n",
      "Episode: 815 Total reward: 9.0 Average reward fake: 0.3354479670524597 Training d_loss: 0.9600 Training g_loss: 1.0902 Training q_loss: 3085.5017 Explore P: 0.0268\n",
      "Episode: 816 Total reward: 9.0 Average reward fake: 0.30877965688705444 Training d_loss: 0.6117 Training g_loss: 1.1987 Training q_loss: 3313.7339 Explore P: 0.0268\n",
      "Episode: 817 Total reward: 10.0 Average reward fake: 0.26866859197616577 Training d_loss: 1.1046 Training g_loss: 1.2919 Training q_loss: 1482.3147 Explore P: 0.0268\n",
      "Episode: 818 Total reward: 10.0 Average reward fake: 0.3447301983833313 Training d_loss: 1.2760 Training g_loss: 1.0369 Training q_loss: 2111.9426 Explore P: 0.0268\n",
      "Episode: 819 Total reward: 8.0 Average reward fake: 0.3524736762046814 Training d_loss: 1.1663 Training g_loss: 1.0458 Training q_loss: 128892.6875 Explore P: 0.0268\n",
      "Episode: 820 Total reward: 11.0 Average reward fake: 0.3409857153892517 Training d_loss: 0.9576 Training g_loss: 1.0704 Training q_loss: 1853.9935 Explore P: 0.0268\n",
      "Episode: 821 Total reward: 11.0 Average reward fake: 0.3352054953575134 Training d_loss: 1.1748 Training g_loss: 1.0903 Training q_loss: 1246.6365 Explore P: 0.0267\n",
      "Episode: 822 Total reward: 10.0 Average reward fake: 0.31431815028190613 Training d_loss: 1.1888 Training g_loss: 1.1692 Training q_loss: 6395.0845 Explore P: 0.0267\n",
      "Episode: 823 Total reward: 11.0 Average reward fake: 0.3131747841835022 Training d_loss: 1.0739 Training g_loss: 1.1568 Training q_loss: 2381.8308 Explore P: 0.0267\n",
      "Episode: 824 Total reward: 9.0 Average reward fake: 0.31847184896469116 Training d_loss: 1.1855 Training g_loss: 1.1299 Training q_loss: 4004.7852 Explore P: 0.0267\n",
      "Episode: 825 Total reward: 8.0 Average reward fake: 0.3258787989616394 Training d_loss: 0.7331 Training g_loss: 1.1300 Training q_loss: 11997561.0000 Explore P: 0.0267\n",
      "Episode: 826 Total reward: 10.0 Average reward fake: 0.32668015360832214 Training d_loss: 0.8450 Training g_loss: 1.1027 Training q_loss: 784.9469 Explore P: 0.0267\n",
      "Episode: 827 Total reward: 10.0 Average reward fake: 0.3528061807155609 Training d_loss: 0.9576 Training g_loss: 1.0523 Training q_loss: 3023.8430 Explore P: 0.0266\n",
      "Episode: 828 Total reward: 10.0 Average reward fake: 0.3043508529663086 Training d_loss: 0.9596 Training g_loss: 1.1974 Training q_loss: 3056053.7500 Explore P: 0.0266\n",
      "Episode: 829 Total reward: 9.0 Average reward fake: 0.3075208067893982 Training d_loss: 1.1941 Training g_loss: 1.1813 Training q_loss: 2704.4985 Explore P: 0.0266\n",
      "Episode: 830 Total reward: 9.0 Average reward fake: 0.3262002170085907 Training d_loss: 0.7334 Training g_loss: 1.1270 Training q_loss: 3863.5098 Explore P: 0.0266\n",
      "Episode: 831 Total reward: 11.0 Average reward fake: 0.3209790885448456 Training d_loss: 1.1836 Training g_loss: 1.1227 Training q_loss: 2241.4512 Explore P: 0.0266\n",
      "Episode: 832 Total reward: 10.0 Average reward fake: 0.35076990723609924 Training d_loss: 0.8529 Training g_loss: 1.0466 Training q_loss: 6102010.5000 Explore P: 0.0266\n",
      "Episode: 833 Total reward: 12.0 Average reward fake: 0.3203105330467224 Training d_loss: 0.8432 Training g_loss: 1.1489 Training q_loss: 1730.3264 Explore P: 0.0265\n",
      "Episode: 834 Total reward: 10.0 Average reward fake: 0.36242741346359253 Training d_loss: 0.9589 Training g_loss: 0.9969 Training q_loss: 118311.2500 Explore P: 0.0265\n",
      "Episode: 835 Total reward: 12.0 Average reward fake: 0.3919927775859833 Training d_loss: 0.6876 Training g_loss: 0.9421 Training q_loss: 959.0590 Explore P: 0.0265\n",
      "Episode: 836 Total reward: 12.0 Average reward fake: 0.3296261429786682 Training d_loss: 0.6266 Training g_loss: 1.1338 Training q_loss: 2711.1533 Explore P: 0.0265\n",
      "Episode: 837 Total reward: 11.0 Average reward fake: 0.3430272936820984 Training d_loss: 1.0410 Training g_loss: 1.1871 Training q_loss: 5187112.5000 Explore P: 0.0265\n",
      "Episode: 838 Total reward: 10.0 Average reward fake: 0.2923201620578766 Training d_loss: 0.8445 Training g_loss: 1.2139 Training q_loss: 3294.6809 Explore P: 0.0265\n",
      "Episode: 839 Total reward: 8.0 Average reward fake: 0.31602877378463745 Training d_loss: 0.8480 Training g_loss: 1.1597 Training q_loss: 2641.9287 Explore P: 0.0264\n",
      "Episode: 840 Total reward: 12.0 Average reward fake: 0.32630568742752075 Training d_loss: 0.9623 Training g_loss: 1.1275 Training q_loss: 5331.1064 Explore P: 0.0264\n",
      "Episode: 841 Total reward: 11.0 Average reward fake: 0.3550260663032532 Training d_loss: 0.9642 Training g_loss: 1.0232 Training q_loss: 10086.8994 Explore P: 0.0264\n",
      "Episode: 842 Total reward: 10.0 Average reward fake: 0.36475563049316406 Training d_loss: 0.6687 Training g_loss: 1.0240 Training q_loss: 2237241.5000 Explore P: 0.0264\n",
      "Episode: 843 Total reward: 10.0 Average reward fake: 0.3408423066139221 Training d_loss: 1.2811 Training g_loss: 1.0731 Training q_loss: 6157.4478 Explore P: 0.0264\n",
      "Episode: 844 Total reward: 10.0 Average reward fake: 0.3405435085296631 Training d_loss: 0.7497 Training g_loss: 1.0845 Training q_loss: 4406307.0000 Explore P: 0.0264\n",
      "Episode: 845 Total reward: 9.0 Average reward fake: 0.33700793981552124 Training d_loss: 1.2837 Training g_loss: 1.0746 Training q_loss: 3460.5962 Explore P: 0.0263\n",
      "Episode: 846 Total reward: 12.0 Average reward fake: 0.35692542791366577 Training d_loss: 0.9621 Training g_loss: 1.0311 Training q_loss: 7396905.5000 Explore P: 0.0263\n",
      "Episode: 847 Total reward: 10.0 Average reward fake: 0.3446134626865387 Training d_loss: 0.8545 Training g_loss: 1.0794 Training q_loss: 5206.9478 Explore P: 0.0263\n",
      "Episode: 848 Total reward: 9.0 Average reward fake: 0.3130297064781189 Training d_loss: 0.8454 Training g_loss: 1.1726 Training q_loss: 8993.9678 Explore P: 0.0263\n",
      "Episode: 849 Total reward: 9.0 Average reward fake: 0.3111133277416229 Training d_loss: 0.8445 Training g_loss: 1.1599 Training q_loss: 10341.2598 Explore P: 0.0263\n",
      "Episode: 850 Total reward: 10.0 Average reward fake: 0.3368605971336365 Training d_loss: 0.6341 Training g_loss: 1.0898 Training q_loss: 7412.8154 Explore P: 0.0263\n",
      "Episode: 851 Total reward: 11.0 Average reward fake: 0.3083621859550476 Training d_loss: 1.1941 Training g_loss: 1.1688 Training q_loss: 16484.4180 Explore P: 0.0262\n",
      "Episode: 852 Total reward: 10.0 Average reward fake: 0.3405260443687439 Training d_loss: 0.9578 Training g_loss: 1.0691 Training q_loss: 12736642.0000 Explore P: 0.0262\n",
      "Episode: 853 Total reward: 11.0 Average reward fake: 0.37346455454826355 Training d_loss: 0.9626 Training g_loss: 0.9875 Training q_loss: 6440.2158 Explore P: 0.0262\n",
      "Episode: 854 Total reward: 8.0 Average reward fake: 0.35465139150619507 Training d_loss: 0.6492 Training g_loss: 1.0562 Training q_loss: 80034.5547 Explore P: 0.0262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 855 Total reward: 11.0 Average reward fake: 0.3009806275367737 Training d_loss: 1.0804 Training g_loss: 1.2024 Training q_loss: 15206094.0000 Explore P: 0.0262\n",
      "Episode: 856 Total reward: 9.0 Average reward fake: 0.30523499846458435 Training d_loss: 1.0785 Training g_loss: 1.1729 Training q_loss: 6213.6777 Explore P: 0.0262\n",
      "Episode: 857 Total reward: 12.0 Average reward fake: 0.369243323802948 Training d_loss: 1.0625 Training g_loss: 0.9864 Training q_loss: 2183.0464 Explore P: 0.0261\n",
      "Episode: 858 Total reward: 13.0 Average reward fake: 0.36755600571632385 Training d_loss: 0.7687 Training g_loss: 1.0119 Training q_loss: 1090.1156 Explore P: 0.0261\n",
      "Episode: 859 Total reward: 15.0 Average reward fake: 0.3265523314476013 Training d_loss: 1.0762 Training g_loss: 1.1305 Training q_loss: 1755.3711 Explore P: 0.0261\n",
      "Episode: 860 Total reward: 10.0 Average reward fake: 0.30411362648010254 Training d_loss: 0.7386 Training g_loss: 1.1990 Training q_loss: 7269.1846 Explore P: 0.0261\n",
      "Episode: 861 Total reward: 12.0 Average reward fake: 0.3014654517173767 Training d_loss: 0.8588 Training g_loss: 1.1911 Training q_loss: 897.7224 Explore P: 0.0261\n",
      "Episode: 862 Total reward: 16.0 Average reward fake: 0.3507138192653656 Training d_loss: 1.0759 Training g_loss: 1.0396 Training q_loss: 886.0743 Explore P: 0.0260\n",
      "Episode: 863 Total reward: 15.0 Average reward fake: 0.3395153880119324 Training d_loss: 0.6725 Training g_loss: 1.1029 Training q_loss: 6333.3364 Explore P: 0.0260\n",
      "Episode: 864 Total reward: 9.0 Average reward fake: 0.37552106380462646 Training d_loss: 1.1337 Training g_loss: 1.0593 Training q_loss: 3465.7336 Explore P: 0.0260\n",
      "Episode: 865 Total reward: 9.0 Average reward fake: 0.37358441948890686 Training d_loss: 1.3584 Training g_loss: 1.0566 Training q_loss: 5131.2861 Explore P: 0.0260\n",
      "Episode: 866 Total reward: 16.0 Average reward fake: 0.33458060026168823 Training d_loss: 0.9829 Training g_loss: 1.0969 Training q_loss: 637.4212 Explore P: 0.0260\n",
      "Episode: 867 Total reward: 14.0 Average reward fake: 0.3206079602241516 Training d_loss: 0.9830 Training g_loss: 1.1360 Training q_loss: 3595217.5000 Explore P: 0.0259\n",
      "Episode: 868 Total reward: 8.0 Average reward fake: 0.42002588510513306 Training d_loss: 1.1411 Training g_loss: 0.9067 Training q_loss: 20179.5977 Explore P: 0.0259\n",
      "Episode: 869 Total reward: 8.0 Average reward fake: 0.4662289023399353 Training d_loss: 1.4109 Training g_loss: 0.7955 Training q_loss: 12968571.0000 Explore P: 0.0259\n",
      "Episode: 870 Total reward: 8.0 Average reward fake: 0.4070033133029938 Training d_loss: 0.8317 Training g_loss: 0.9173 Training q_loss: 1872.4309 Explore P: 0.0259\n",
      "Episode: 871 Total reward: 12.0 Average reward fake: 0.2783147692680359 Training d_loss: 0.9451 Training g_loss: 1.9310 Training q_loss: 3488.8132 Explore P: 0.0259\n",
      "Episode: 872 Total reward: 12.0 Average reward fake: 0.3437522053718567 Training d_loss: 1.0820 Training g_loss: 1.0543 Training q_loss: 1649.4078 Explore P: 0.0259\n",
      "Episode: 873 Total reward: 14.0 Average reward fake: 0.3472842276096344 Training d_loss: 1.0844 Training g_loss: 1.0661 Training q_loss: 2225.0520 Explore P: 0.0258\n",
      "Episode: 874 Total reward: 14.0 Average reward fake: 0.3296385407447815 Training d_loss: 0.8864 Training g_loss: 1.1017 Training q_loss: 9074.6270 Explore P: 0.0258\n",
      "Episode: 875 Total reward: 10.0 Average reward fake: 0.3478524386882782 Training d_loss: 0.7905 Training g_loss: 1.0631 Training q_loss: 1175.5651 Explore P: 0.0258\n",
      "Episode: 876 Total reward: 9.0 Average reward fake: 0.339270681142807 Training d_loss: 1.0866 Training g_loss: 1.0812 Training q_loss: 4736.6948 Explore P: 0.0258\n",
      "Episode: 877 Total reward: 10.0 Average reward fake: 0.39850491285324097 Training d_loss: 1.2337 Training g_loss: 0.9773 Training q_loss: 15433.0596 Explore P: 0.0258\n",
      "Episode: 878 Total reward: 10.0 Average reward fake: 0.41101759672164917 Training d_loss: 1.3324 Training g_loss: 0.9449 Training q_loss: 3728.2085 Explore P: 0.0258\n",
      "Episode: 879 Total reward: 9.0 Average reward fake: 0.3481736183166504 Training d_loss: 1.1851 Training g_loss: 1.0565 Training q_loss: 2850.5872 Explore P: 0.0257\n",
      "Episode: 880 Total reward: 12.0 Average reward fake: 0.39240774512290955 Training d_loss: 1.3293 Training g_loss: 1.0072 Training q_loss: 27976.6660 Explore P: 0.0257\n",
      "Episode: 881 Total reward: 16.0 Average reward fake: 0.41399580240249634 Training d_loss: 1.5794 Training g_loss: 0.9161 Training q_loss: 8169.4517 Explore P: 0.0257\n",
      "Episode: 882 Total reward: 58.0 Average reward fake: 0.4224374294281006 Training d_loss: 1.2920 Training g_loss: 0.8797 Training q_loss: 25867.2129 Explore P: 0.0256\n",
      "Episode: 883 Total reward: 15.0 Average reward fake: 0.3035394549369812 Training d_loss: 1.1400 Training g_loss: 1.2159 Training q_loss: 31971.1758 Explore P: 0.0256\n",
      "Episode: 884 Total reward: 17.0 Average reward fake: 0.3461918830871582 Training d_loss: 0.9291 Training g_loss: 1.0802 Training q_loss: 23852.5605 Explore P: 0.0256\n",
      "Episode: 885 Total reward: 46.0 Average reward fake: 0.35856279730796814 Training d_loss: 1.2212 Training g_loss: 1.0104 Training q_loss: 21886.6211 Explore P: 0.0255\n",
      "Episode: 886 Total reward: 25.0 Average reward fake: 0.38532447814941406 Training d_loss: 0.9145 Training g_loss: 1.0076 Training q_loss: 1684113.6250 Explore P: 0.0254\n",
      "Episode: 887 Total reward: 19.0 Average reward fake: 0.45817580819129944 Training d_loss: 1.3341 Training g_loss: 0.7962 Training q_loss: 18395.1816 Explore P: 0.0254\n",
      "Episode: 888 Total reward: 24.0 Average reward fake: 0.4823668599128723 Training d_loss: 1.5525 Training g_loss: 0.8310 Training q_loss: 36063.9102 Explore P: 0.0254\n",
      "Episode: 889 Total reward: 33.0 Average reward fake: 0.3323717415332794 Training d_loss: 0.9747 Training g_loss: 1.1162 Training q_loss: 9077888.0000 Explore P: 0.0253\n",
      "Episode: 890 Total reward: 34.0 Average reward fake: 0.40051403641700745 Training d_loss: 1.0966 Training g_loss: 0.9543 Training q_loss: 13699.1113 Explore P: 0.0253\n",
      "Episode: 891 Total reward: 199.0 Average reward fake: 0.4112868309020996 Training d_loss: 1.0190 Training g_loss: 0.9362 Training q_loss: 34884.1367 Explore P: 0.0250\n",
      "Episode: 892 Total reward: 36.0 Average reward fake: 0.3550109267234802 Training d_loss: 1.0461 Training g_loss: 1.0298 Training q_loss: 3930.5293 Explore P: 0.0249\n",
      "Episode: 893 Total reward: 132.0 Average reward fake: 0.44309496879577637 Training d_loss: 1.2812 Training g_loss: 0.8421 Training q_loss: 14465.4736 Explore P: 0.0247\n",
      "Episode: 894 Total reward: 32.0 Average reward fake: 0.36980634927749634 Training d_loss: 1.1984 Training g_loss: 1.0452 Training q_loss: 1651932.2500 Explore P: 0.0247\n",
      "Episode: 895 Total reward: 29.0 Average reward fake: 0.38531750440597534 Training d_loss: 1.4483 Training g_loss: 0.9755 Training q_loss: 21112.3223 Explore P: 0.0246\n",
      "Episode: 896 Total reward: 21.0 Average reward fake: 0.409552663564682 Training d_loss: 1.3505 Training g_loss: 0.9365 Training q_loss: 6808.8188 Explore P: 0.0246\n",
      "Episode: 897 Total reward: 20.0 Average reward fake: 0.3679036498069763 Training d_loss: 1.3964 Training g_loss: 1.0431 Training q_loss: 24376.1641 Explore P: 0.0246\n",
      "Episode: 898 Total reward: 114.0 Average reward fake: 0.3766666650772095 Training d_loss: 1.1795 Training g_loss: 1.0004 Training q_loss: 17794.8047 Explore P: 0.0244\n",
      "Episode: 899 Total reward: 92.0 Average reward fake: 0.5838330388069153 Training d_loss: 1.4022 Training g_loss: 0.5439 Training q_loss: 4364.6807 Explore P: 0.0243\n",
      "Episode: 900 Total reward: 37.0 Average reward fake: 0.36210402846336365 Training d_loss: 1.2962 Training g_loss: 1.0112 Training q_loss: 3904.2917 Explore P: 0.0242\n",
      "Episode: 901 Total reward: 54.0 Average reward fake: 0.3819606900215149 Training d_loss: 1.1811 Training g_loss: 0.9852 Training q_loss: 13480.8418 Explore P: 0.0241\n",
      "Episode: 902 Total reward: 57.0 Average reward fake: 0.4583272933959961 Training d_loss: 1.1617 Training g_loss: 0.8267 Training q_loss: 4044.4219 Explore P: 0.0241\n",
      "Episode: 903 Total reward: 33.0 Average reward fake: 0.4002651572227478 Training d_loss: 1.1221 Training g_loss: 0.9594 Training q_loss: 8871.4434 Explore P: 0.0240\n",
      "Episode: 904 Total reward: 24.0 Average reward fake: 0.33556246757507324 Training d_loss: 1.0666 Training g_loss: 1.0825 Training q_loss: 8774.1084 Explore P: 0.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 905 Total reward: 37.0 Average reward fake: 0.4187045693397522 Training d_loss: 1.1026 Training g_loss: 0.8925 Training q_loss: 5436.8560 Explore P: 0.0239\n",
      "Episode: 906 Total reward: 45.0 Average reward fake: 0.5073176622390747 Training d_loss: 1.5434 Training g_loss: 0.7320 Training q_loss: 243574.5469 Explore P: 0.0239\n",
      "Episode: 907 Total reward: 37.0 Average reward fake: 0.36496812105178833 Training d_loss: 1.3091 Training g_loss: 1.0117 Training q_loss: 1175.8796 Explore P: 0.0238\n",
      "Episode: 908 Total reward: 18.0 Average reward fake: 0.3227717876434326 Training d_loss: 0.9649 Training g_loss: 1.1264 Training q_loss: 3597.0481 Explore P: 0.0238\n",
      "Episode: 909 Total reward: 18.0 Average reward fake: 0.39723077416419983 Training d_loss: 1.1295 Training g_loss: 0.9197 Training q_loss: 3141.5356 Explore P: 0.0238\n",
      "Episode: 910 Total reward: 27.0 Average reward fake: 0.3509829640388489 Training d_loss: 1.1203 Training g_loss: 1.0294 Training q_loss: 761069.8125 Explore P: 0.0237\n",
      "Episode: 911 Total reward: 27.0 Average reward fake: 0.39218077063560486 Training d_loss: 1.2766 Training g_loss: 0.9717 Training q_loss: 4040.7358 Explore P: 0.0237\n",
      "Episode: 912 Total reward: 12.0 Average reward fake: 0.385364294052124 Training d_loss: 1.0446 Training g_loss: 0.9537 Training q_loss: 15229.1924 Explore P: 0.0237\n",
      "Episode: 913 Total reward: 18.0 Average reward fake: 0.37736302614212036 Training d_loss: 1.0904 Training g_loss: 1.0412 Training q_loss: 4380.9033 Explore P: 0.0237\n",
      "Episode: 914 Total reward: 22.0 Average reward fake: 0.40602999925613403 Training d_loss: 1.0013 Training g_loss: 0.9513 Training q_loss: 7239.0679 Explore P: 0.0236\n",
      "Episode: 915 Total reward: 22.0 Average reward fake: 0.3957677483558655 Training d_loss: 1.4574 Training g_loss: 0.9680 Training q_loss: 4156.2964 Explore P: 0.0236\n",
      "Episode: 916 Total reward: 28.0 Average reward fake: 0.32732757925987244 Training d_loss: 0.7364 Training g_loss: 1.1267 Training q_loss: 24334.0098 Explore P: 0.0236\n",
      "Episode: 917 Total reward: 22.0 Average reward fake: 0.35401445627212524 Training d_loss: 1.1223 Training g_loss: 1.0411 Training q_loss: 2955.3169 Explore P: 0.0235\n",
      "Episode: 918 Total reward: 89.0 Average reward fake: 0.48511379957199097 Training d_loss: 1.3940 Training g_loss: 0.7424 Training q_loss: 894999.8125 Explore P: 0.0234\n",
      "Episode: 919 Total reward: 199.0 Average reward fake: 0.5336405038833618 Training d_loss: 1.3800 Training g_loss: 0.6328 Training q_loss: 12660.5762 Explore P: 0.0231\n",
      "Episode: 920 Total reward: 125.0 Average reward fake: 0.4468369483947754 Training d_loss: 1.2619 Training g_loss: 0.8232 Training q_loss: 3584.2954 Explore P: 0.0230\n",
      "Episode: 921 Total reward: 55.0 Average reward fake: 0.4758132994174957 Training d_loss: 1.1905 Training g_loss: 0.7589 Training q_loss: 1031456.3125 Explore P: 0.0229\n",
      "Episode: 922 Total reward: 100.0 Average reward fake: 0.5386983752250671 Training d_loss: 1.4013 Training g_loss: 0.6207 Training q_loss: 7517.2822 Explore P: 0.0228\n",
      "Episode: 923 Total reward: 33.0 Average reward fake: 0.4791713356971741 Training d_loss: 1.3452 Training g_loss: 0.7508 Training q_loss: 53934.3242 Explore P: 0.0227\n",
      "Episode: 924 Total reward: 54.0 Average reward fake: 0.5228707790374756 Training d_loss: 1.3875 Training g_loss: 0.6708 Training q_loss: 19510.8496 Explore P: 0.0227\n",
      "Episode: 925 Total reward: 48.0 Average reward fake: 0.4803580343723297 Training d_loss: 1.3532 Training g_loss: 0.7693 Training q_loss: 19147.4434 Explore P: 0.0226\n",
      "Episode: 926 Total reward: 29.0 Average reward fake: 0.42943358421325684 Training d_loss: 1.4003 Training g_loss: 1.8566 Training q_loss: 16352.1455 Explore P: 0.0226\n",
      "Episode: 927 Total reward: 88.0 Average reward fake: 0.5047905445098877 Training d_loss: 1.4310 Training g_loss: 0.7060 Training q_loss: 5108.4731 Explore P: 0.0225\n",
      "Episode: 928 Total reward: 144.0 Average reward fake: 0.4176792502403259 Training d_loss: 1.2772 Training g_loss: 0.8783 Training q_loss: 8430.5830 Explore P: 0.0223\n",
      "Episode: 929 Total reward: 163.0 Average reward fake: 0.3248787820339203 Training d_loss: 1.1817 Training g_loss: 2.6598 Training q_loss: 11411.9551 Explore P: 0.0221\n",
      "Episode: 930 Total reward: 107.0 Average reward fake: 0.43020516633987427 Training d_loss: 1.2887 Training g_loss: 1.1961 Training q_loss: 1531.9707 Explore P: 0.0220\n",
      "Episode: 931 Total reward: 55.0 Average reward fake: 0.47431325912475586 Training d_loss: 1.3499 Training g_loss: 0.7531 Training q_loss: 10950.0537 Explore P: 0.0219\n",
      "Episode: 932 Total reward: 48.0 Average reward fake: 0.48986849188804626 Training d_loss: 1.2992 Training g_loss: 0.7475 Training q_loss: 34151.2734 Explore P: 0.0218\n",
      "Episode: 933 Total reward: 83.0 Average reward fake: 0.43751320242881775 Training d_loss: 1.2465 Training g_loss: 1.7424 Training q_loss: 7471.7173 Explore P: 0.0217\n",
      "Episode: 934 Total reward: 69.0 Average reward fake: 0.5139095187187195 Training d_loss: 1.2598 Training g_loss: 0.6821 Training q_loss: 8633.3135 Explore P: 0.0217\n",
      "Episode: 935 Total reward: 74.0 Average reward fake: 0.47343701124191284 Training d_loss: 1.3991 Training g_loss: 0.7756 Training q_loss: 13050.3584 Explore P: 0.0216\n",
      "Episode: 936 Total reward: 101.0 Average reward fake: 0.430753618478775 Training d_loss: 1.4045 Training g_loss: 0.8725 Training q_loss: 8101.7002 Explore P: 0.0215\n",
      "Episode: 937 Total reward: 125.0 Average reward fake: 0.47115832567214966 Training d_loss: 1.3945 Training g_loss: 0.7481 Training q_loss: 6614.7979 Explore P: 0.0213\n",
      "Episode: 938 Total reward: 199.0 Average reward fake: 0.5112506151199341 Training d_loss: 1.3865 Training g_loss: 0.6740 Training q_loss: 1999.5730 Explore P: 0.0211\n",
      "Episode: 939 Total reward: 83.0 Average reward fake: 0.38937908411026 Training d_loss: 1.4086 Training g_loss: 0.9216 Training q_loss: 10619.7109 Explore P: 0.0210\n",
      "Episode: 940 Total reward: 13.0 Average reward fake: 0.31585878133773804 Training d_loss: 1.1371 Training g_loss: 1.1829 Training q_loss: 13931.3018 Explore P: 0.0210\n",
      "Episode: 941 Total reward: 32.0 Average reward fake: 0.3548726439476013 Training d_loss: 1.2357 Training g_loss: 1.0475 Training q_loss: 138624.9375 Explore P: 0.0209\n",
      "Episode: 942 Total reward: 120.0 Average reward fake: 0.526715874671936 Training d_loss: 1.4019 Training g_loss: 0.6385 Training q_loss: 2697.3113 Explore P: 0.0208\n",
      "Episode: 943 Total reward: 189.0 Average reward fake: 0.5387493371963501 Training d_loss: 1.3967 Training g_loss: 0.6223 Training q_loss: 1809.8259 Explore P: 0.0206\n",
      "Episode: 944 Total reward: 57.0 Average reward fake: 0.4245906472206116 Training d_loss: 1.2176 Training g_loss: 0.9068 Training q_loss: 7986.1353 Explore P: 0.0206\n",
      "Episode: 945 Total reward: 97.0 Average reward fake: 0.5235919952392578 Training d_loss: 1.3876 Training g_loss: 0.6491 Training q_loss: 2326.4033 Explore P: 0.0205\n",
      "Episode: 946 Total reward: 85.0 Average reward fake: 0.4622439742088318 Training d_loss: 1.2806 Training g_loss: 1.6912 Training q_loss: 1063.7634 Explore P: 0.0204\n",
      "Episode: 947 Total reward: 70.0 Average reward fake: 0.4265274107456207 Training d_loss: 1.3910 Training g_loss: 0.8830 Training q_loss: 3997.8540 Explore P: 0.0203\n",
      "Episode: 948 Total reward: 89.0 Average reward fake: 0.5456315279006958 Training d_loss: 1.4250 Training g_loss: 0.6092 Training q_loss: 2679.4517 Explore P: 0.0202\n",
      "Episode: 949 Total reward: 88.0 Average reward fake: 0.3335670828819275 Training d_loss: 1.2492 Training g_loss: 1.1431 Training q_loss: 7800.1460 Explore P: 0.0201\n",
      "Episode: 950 Total reward: 23.0 Average reward fake: 0.4266994893550873 Training d_loss: 1.3475 Training g_loss: 0.8795 Training q_loss: 16377.8174 Explore P: 0.0201\n",
      "Episode: 951 Total reward: 146.0 Average reward fake: 0.47623133659362793 Training d_loss: 1.3512 Training g_loss: 0.7553 Training q_loss: 1783.3518 Explore P: 0.0199\n",
      "Episode: 952 Total reward: 47.0 Average reward fake: 0.46848225593566895 Training d_loss: 1.3830 Training g_loss: 0.7735 Training q_loss: 41918.0273 Explore P: 0.0199\n",
      "Episode: 953 Total reward: 139.0 Average reward fake: 0.5160786509513855 Training d_loss: 1.4507 Training g_loss: 0.6544 Training q_loss: 802.3354 Explore P: 0.0198\n",
      "Episode: 954 Total reward: 159.0 Average reward fake: 0.4342692792415619 Training d_loss: 1.1507 Training g_loss: 0.8541 Training q_loss: 3417.5093 Explore P: 0.0196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 955 Total reward: 26.0 Average reward fake: 0.29919877648353577 Training d_loss: 0.9062 Training g_loss: 2.7338 Training q_loss: 4552.7100 Explore P: 0.0196\n",
      "Episode: 956 Total reward: 199.0 Average reward fake: 0.49241048097610474 Training d_loss: 1.3400 Training g_loss: 0.7222 Training q_loss: 13873.7471 Explore P: 0.0194\n",
      "Episode: 957 Total reward: 84.0 Average reward fake: 0.5182303190231323 Training d_loss: 1.3556 Training g_loss: 0.6609 Training q_loss: 529.8613 Explore P: 0.0193\n",
      "Episode: 958 Total reward: 152.0 Average reward fake: 0.5172727704048157 Training d_loss: 1.3872 Training g_loss: 0.6585 Training q_loss: 59275.5859 Explore P: 0.0192\n",
      "Episode: 959 Total reward: 125.0 Average reward fake: 0.5048814415931702 Training d_loss: 1.3762 Training g_loss: 0.6848 Training q_loss: 2735.9438 Explore P: 0.0191\n",
      "Episode: 960 Total reward: 199.0 Average reward fake: 0.4221152365207672 Training d_loss: 1.2595 Training g_loss: 1.3144 Training q_loss: 42858.9688 Explore P: 0.0189\n",
      "Episode: 961 Total reward: 100.0 Average reward fake: 0.400185763835907 Training d_loss: 1.3115 Training g_loss: 0.9380 Training q_loss: 4851.2041 Explore P: 0.0188\n",
      "Episode: 962 Total reward: 172.0 Average reward fake: 0.35318607091903687 Training d_loss: 1.1694 Training g_loss: 1.1666 Training q_loss: 3029.0225 Explore P: 0.0186\n",
      "Episode: 963 Total reward: 145.0 Average reward fake: 0.5130759477615356 Training d_loss: 1.3754 Training g_loss: 0.6691 Training q_loss: 2827.0010 Explore P: 0.0185\n",
      "Episode: 964 Total reward: 199.0 Average reward fake: 0.5124354362487793 Training d_loss: 1.3826 Training g_loss: 0.6696 Training q_loss: 1438.0259 Explore P: 0.0183\n",
      "Episode: 965 Total reward: 151.0 Average reward fake: 0.5056183934211731 Training d_loss: 1.3999 Training g_loss: 0.6824 Training q_loss: 71420.3281 Explore P: 0.0182\n",
      "Episode: 966 Total reward: 114.0 Average reward fake: 0.4607938230037689 Training d_loss: 1.3086 Training g_loss: 1.3271 Training q_loss: 33369.4297 Explore P: 0.0181\n",
      "Episode: 967 Total reward: 120.0 Average reward fake: 0.4716815948486328 Training d_loss: 1.3133 Training g_loss: 1.5658 Training q_loss: 636.7437 Explore P: 0.0180\n",
      "Episode: 968 Total reward: 199.0 Average reward fake: 0.4778047502040863 Training d_loss: 1.3553 Training g_loss: 0.7419 Training q_loss: 1609.3186 Explore P: 0.0179\n",
      "Episode: 969 Total reward: 42.0 Average reward fake: 0.4494100511074066 Training d_loss: 1.5292 Training g_loss: 0.7572 Training q_loss: 1534.2875 Explore P: 0.0178\n",
      "Episode: 970 Total reward: 199.0 Average reward fake: 0.4332631528377533 Training d_loss: 1.2002 Training g_loss: 0.8592 Training q_loss: 749.5418 Explore P: 0.0177\n",
      "Episode: 971 Total reward: 144.0 Average reward fake: 0.5250499248504639 Training d_loss: 1.3862 Training g_loss: 0.6451 Training q_loss: 854.4109 Explore P: 0.0176\n",
      "Episode: 972 Total reward: 89.0 Average reward fake: 0.5081467032432556 Training d_loss: 1.3825 Training g_loss: 0.6819 Training q_loss: 2176.6555 Explore P: 0.0175\n",
      "Episode: 973 Total reward: 172.0 Average reward fake: 0.5274940729141235 Training d_loss: 1.3925 Training g_loss: 0.6415 Training q_loss: 402.2957 Explore P: 0.0174\n",
      "Episode: 974 Total reward: 46.0 Average reward fake: 0.5212369561195374 Training d_loss: 1.3383 Training g_loss: 0.6588 Training q_loss: 1043.4559 Explore P: 0.0173\n",
      "Episode: 975 Total reward: 11.0 Average reward fake: 0.3838130235671997 Training d_loss: 1.1693 Training g_loss: 1.7990 Training q_loss: 1170.8815 Explore P: 0.0173\n",
      "Episode: 976 Total reward: 199.0 Average reward fake: 0.4534772038459778 Training d_loss: 1.2866 Training g_loss: 1.5874 Training q_loss: 26108.9043 Explore P: 0.0172\n",
      "Episode: 977 Total reward: 159.0 Average reward fake: 0.5380021929740906 Training d_loss: 1.3947 Training g_loss: 0.6193 Training q_loss: 3798.0710 Explore P: 0.0171\n",
      "Episode: 978 Total reward: 108.0 Average reward fake: 0.5207618474960327 Training d_loss: 1.3951 Training g_loss: 0.6559 Training q_loss: 922.0308 Explore P: 0.0170\n",
      "Episode: 979 Total reward: 115.0 Average reward fake: 0.34354081749916077 Training d_loss: 1.3975 Training g_loss: 0.9858 Training q_loss: 1079.8500 Explore P: 0.0169\n",
      "Episode: 980 Total reward: 65.0 Average reward fake: 0.2781742215156555 Training d_loss: 1.2773 Training g_loss: 1.2815 Training q_loss: 831.6842 Explore P: 0.0169\n",
      "Episode: 981 Total reward: 8.0 Average reward fake: 0.3329353630542755 Training d_loss: 1.1229 Training g_loss: 1.5410 Training q_loss: 1055.2209 Explore P: 0.0169\n",
      "Episode: 982 Total reward: 137.0 Average reward fake: 0.3609946370124817 Training d_loss: 1.1867 Training g_loss: 1.8950 Training q_loss: 11222.9180 Explore P: 0.0168\n",
      "Episode: 983 Total reward: 199.0 Average reward fake: 0.5240234136581421 Training d_loss: 1.3779 Training g_loss: 0.6506 Training q_loss: 895.4222 Explore P: 0.0166\n",
      "Episode: 984 Total reward: 117.0 Average reward fake: 0.36823570728302 Training d_loss: 1.1811 Training g_loss: 1.9261 Training q_loss: 641.8459 Explore P: 0.0166\n",
      "Episode: 985 Total reward: 154.0 Average reward fake: 0.557584822177887 Training d_loss: 1.4474 Training g_loss: 0.5897 Training q_loss: 4134.2876 Explore P: 0.0165\n",
      "Episode: 986 Total reward: 199.0 Average reward fake: 0.48740655183792114 Training d_loss: 1.2206 Training g_loss: 1.4628 Training q_loss: 498.3372 Explore P: 0.0163\n",
      "Episode: 987 Total reward: 199.0 Average reward fake: 0.4609163701534271 Training d_loss: 1.3071 Training g_loss: 1.5870 Training q_loss: 743.7745 Explore P: 0.0162\n",
      "Episode: 988 Total reward: 65.0 Average reward fake: 0.277640700340271 Training d_loss: 1.2688 Training g_loss: 2.0331 Training q_loss: 683.4069 Explore P: 0.0162\n",
      "Episode: 989 Total reward: 152.0 Average reward fake: 0.4602193832397461 Training d_loss: 1.4091 Training g_loss: 0.7752 Training q_loss: 451.1105 Explore P: 0.0161\n",
      "Episode: 990 Total reward: 121.0 Average reward fake: 0.5382930636405945 Training d_loss: 1.3930 Training g_loss: 0.6216 Training q_loss: 2209.6411 Explore P: 0.0160\n",
      "Episode: 991 Total reward: 9.0 Average reward fake: 0.40222564339637756 Training d_loss: 1.2016 Training g_loss: 2.1428 Training q_loss: 1110.3314 Explore P: 0.0160\n",
      "Episode: 992 Total reward: 40.0 Average reward fake: 0.551636815071106 Training d_loss: 1.3900 Training g_loss: 0.5989 Training q_loss: 1563.2639 Explore P: 0.0160\n",
      "Episode: 993 Total reward: 8.0 Average reward fake: 0.4860788881778717 Training d_loss: 1.2864 Training g_loss: 0.8342 Training q_loss: 1032.0795 Explore P: 0.0160\n",
      "Episode: 994 Total reward: 71.0 Average reward fake: 0.5291028618812561 Training d_loss: 1.3897 Training g_loss: 0.6377 Training q_loss: 8760.7676 Explore P: 0.0159\n",
      "Episode: 995 Total reward: 108.0 Average reward fake: 0.3342036008834839 Training d_loss: 1.2036 Training g_loss: 1.4799 Training q_loss: 1352.2981 Explore P: 0.0159\n",
      "Episode: 996 Total reward: 199.0 Average reward fake: 0.45319119095802307 Training d_loss: 1.2674 Training g_loss: 1.6359 Training q_loss: 10727.6328 Explore P: 0.0158\n",
      "Episode: 997 Total reward: 199.0 Average reward fake: 0.4668095111846924 Training d_loss: 1.3564 Training g_loss: 0.7955 Training q_loss: 964.1165 Explore P: 0.0156\n",
      "Episode: 998 Total reward: 199.0 Average reward fake: 0.47051435708999634 Training d_loss: 1.3070 Training g_loss: 1.6065 Training q_loss: 462.8817 Explore P: 0.0155\n",
      "Episode: 999 Total reward: 199.0 Average reward fake: 0.4205085337162018 Training d_loss: 1.1724 Training g_loss: 2.0435 Training q_loss: 744.3483 Explore P: 0.0154\n",
      "Episode: 1000 Total reward: 199.0 Average reward fake: 0.4820636808872223 Training d_loss: 1.3162 Training g_loss: 1.5667 Training q_loss: 1031.6973 Explore P: 0.0153\n",
      "Episode: 1001 Total reward: 120.0 Average reward fake: 0.5269018411636353 Training d_loss: 1.3513 Training g_loss: 0.6465 Training q_loss: 470.6817 Explore P: 0.0153\n",
      "Episode: 1002 Total reward: 199.0 Average reward fake: 0.5404058694839478 Training d_loss: 1.3844 Training g_loss: 0.6229 Training q_loss: 2359.9761 Explore P: 0.0151\n",
      "Episode: 1003 Total reward: 199.0 Average reward fake: 0.4870675206184387 Training d_loss: 1.3252 Training g_loss: 1.5128 Training q_loss: 427.2381 Explore P: 0.0150\n",
      "Episode: 1004 Total reward: 174.0 Average reward fake: 0.39292675256729126 Training d_loss: 1.2021 Training g_loss: 1.2622 Training q_loss: 39.4490 Explore P: 0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1005 Total reward: 143.0 Average reward fake: 0.5271464586257935 Training d_loss: 1.3544 Training g_loss: 0.6601 Training q_loss: 4281.3696 Explore P: 0.0149\n",
      "Episode: 1006 Total reward: 199.0 Average reward fake: 0.3527378439903259 Training d_loss: 0.9700 Training g_loss: 1.8675 Training q_loss: 252.8072 Explore P: 0.0148\n",
      "Episode: 1007 Total reward: 32.0 Average reward fake: 0.40896812081336975 Training d_loss: 1.1346 Training g_loss: 0.9238 Training q_loss: 30960.2227 Explore P: 0.0148\n",
      "Episode: 1008 Total reward: 199.0 Average reward fake: 0.4065670967102051 Training d_loss: 1.2749 Training g_loss: 1.0334 Training q_loss: 22515.0820 Explore P: 0.0147\n",
      "Episode: 1009 Total reward: 199.0 Average reward fake: 0.5237026214599609 Training d_loss: 1.3460 Training g_loss: 0.6464 Training q_loss: 444.4456 Explore P: 0.0146\n",
      "Episode: 1010 Total reward: 109.0 Average reward fake: 0.591812014579773 Training d_loss: 1.4503 Training g_loss: 0.5331 Training q_loss: 395.8160 Explore P: 0.0145\n",
      "Episode: 1011 Total reward: 199.0 Average reward fake: 0.4076331555843353 Training d_loss: 1.2997 Training g_loss: 1.7833 Training q_loss: 255.1660 Explore P: 0.0145\n",
      "Episode: 1012 Total reward: 199.0 Average reward fake: 0.4367399215698242 Training d_loss: 1.2345 Training g_loss: 2.4534 Training q_loss: 180.6084 Explore P: 0.0144\n",
      "Episode: 1013 Total reward: 81.0 Average reward fake: 0.4407266676425934 Training d_loss: 1.1377 Training g_loss: 1.0516 Training q_loss: 345.1700 Explore P: 0.0143\n",
      "Episode: 1014 Total reward: 43.0 Average reward fake: 0.41439658403396606 Training d_loss: 1.2072 Training g_loss: 1.6918 Training q_loss: 1222.1349 Explore P: 0.0143\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list = []\n",
    "d_loss_list, g_loss_list, q_loss_list = [], [], [] \n",
    "rewards_fake_list = []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward = 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        rewards_fake_mean = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            # feed_dict={model.states: next_states}\n",
    "            # next_actions_logits, next_rewards_fake = sess.run([model.actions_logits, model.rewards_fake], feed_dict)\n",
    "            feed_dict={model.states: states}\n",
    "            rewards_fake = sess.run(model.rewards_fake, feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake rewards or rewarded generated actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "            # print('DEBUGGING', targetQs.shape, next_rewards_fake.shape, next_actions_logits.shape, np.max(next_actions_logits, axis=1).shape)\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.targetQs: targetQs}\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/DQAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/DQAN-cartpole.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    # Save the trained model \n",
    "    saver.restore(sess, 'checkpoints/DQAN-cartpole.ckpt')\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
