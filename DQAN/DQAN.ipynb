{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQAN: DQN (Deep Q-Nets) + GAN (Gen. Adv. Nets)\n",
    "\n",
    "In this notebook, we'll combine a DQN (deep Q-net) with GAN (generative adverserial net) that can learn to play games through reinforcement learning without any reward function. We'll call this network DQAN (deep Q adverserial net). \n",
    "Adverserial nets learn to maximize the current reward based the past rewards.\n",
    "Q-net learns to maximize the future rewards based on the current reward.\n",
    "Given a task and known when the task is done or failed, we should be able to learn the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[ 0.00656088 -0.14912214 -0.04014852  0.26801559] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00357843  0.04654911 -0.0347882  -0.03705524] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00450942  0.24215221 -0.03552931 -0.34050804] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00935246  0.04755332 -0.04233947 -0.05923734] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01030353  0.24325595 -0.04352422 -0.3649723 ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01516865  0.43896855 -0.05082366 -0.67105526] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02394802  0.63475884 -0.06424477 -0.97929721] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.03664319  0.83068047 -0.08383071 -1.29144834] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.0532568   1.02676221 -0.10965968 -1.60915545] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.07379205  1.22299582 -0.14184279 -1.93391319] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.22299582295005 -1.9339131861641197\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    # Given data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # Target Q values for training\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(states, state_size, action_size, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(actions, action_size, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=True)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=True)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # logits for loss and reward/prob/out\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, action_size, hidden_size, state_size, targetQs, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param states: real input states or observations given\n",
    "    :param actions: real actions given\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # The fake/generated actions\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size, \n",
    "                               state_size=state_size)\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    d_logits_fake = discriminator(actions=actions_fake, hidden_size=hidden_size, action_size=action_size)\n",
    "\n",
    "    # The real onehot encoded actions\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    d_logits_real = discriminator(actions=actions_real, hidden_size=hidden_size, action_size=action_size, \n",
    "                                  reuse=True)\n",
    "\n",
    "    # Training the rewarding function\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Train the generate to maximize the current reward 0-1\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Train the generator to maximize the future rewards: Bellman equations: loss (targetQ - Q)^2\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # The generated rewards for Bellman equation\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "\n",
    "    return d_loss, g_loss, q_loss, actions_logits, Qs, rewards_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for reward function\n",
    "    :param g_loss: Generator/Q-value loss Tensor for action & next state predicton\n",
    "    :param q_loss: Value loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=g_vars)\n",
    "\n",
    "    return d_opt, g_opt, q_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.d_loss, self.g_loss, self.q_loss, self.actions_logits, self.Qs, self.rewards_fake = model_loss(\n",
    "            state_size=state_size, action_size=action_size, actions=self.actions, states=self.states, \n",
    "            hidden_size=hidden_size, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.d_opt, self.g_opt, self.q_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, \n",
    "                                                       q_loss=self.q_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = QNetwork(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 2.0 Training d_loss: 1.3830 Training g_loss: 0.6841 Training q_loss: 0.5192 Explore P: 0.9998\n",
      "Episode: 1 Total reward: 19.0 Training d_loss: 1.2352 Training g_loss: 0.7737 Training q_loss: 0.4918 Explore P: 0.9979\n",
      "Episode: 2 Total reward: 9.0 Training d_loss: 1.1859 Training g_loss: 0.7918 Training q_loss: 1.3040 Explore P: 0.9970\n",
      "Episode: 3 Total reward: 14.0 Training d_loss: 1.1632 Training g_loss: 0.7876 Training q_loss: 3.6075 Explore P: 0.9957\n",
      "Episode: 4 Total reward: 46.0 Training d_loss: 1.1178 Training g_loss: 0.9200 Training q_loss: 7.4019 Explore P: 0.9911\n",
      "Episode: 5 Total reward: 17.0 Training d_loss: 1.2273 Training g_loss: 0.8286 Training q_loss: 14.2734 Explore P: 0.9895\n",
      "Episode: 6 Total reward: 12.0 Training d_loss: 1.1514 Training g_loss: 1.0090 Training q_loss: 6.1049 Explore P: 0.9883\n",
      "Episode: 7 Total reward: 16.0 Training d_loss: 1.1359 Training g_loss: 1.0124 Training q_loss: 25.3727 Explore P: 0.9867\n",
      "Episode: 8 Total reward: 20.0 Training d_loss: 1.2666 Training g_loss: 0.7575 Training q_loss: 15.4163 Explore P: 0.9848\n",
      "Episode: 9 Total reward: 12.0 Training d_loss: 1.1990 Training g_loss: 1.0284 Training q_loss: 18.1947 Explore P: 0.9836\n",
      "Episode: 10 Total reward: 19.0 Training d_loss: 1.1516 Training g_loss: 1.0500 Training q_loss: 38.8903 Explore P: 0.9818\n",
      "Episode: 11 Total reward: 18.0 Training d_loss: 1.3780 Training g_loss: 1.2241 Training q_loss: 76.8311 Explore P: 0.9800\n",
      "Episode: 12 Total reward: 20.0 Training d_loss: 1.1611 Training g_loss: 1.1887 Training q_loss: 114.0081 Explore P: 0.9781\n",
      "Episode: 13 Total reward: 16.0 Training d_loss: 1.1083 Training g_loss: 1.3380 Training q_loss: 102.6848 Explore P: 0.9765\n",
      "Episode: 14 Total reward: 15.0 Training d_loss: 1.1869 Training g_loss: 1.0427 Training q_loss: 53.1470 Explore P: 0.9751\n",
      "Episode: 15 Total reward: 15.0 Training d_loss: 1.4344 Training g_loss: 0.5658 Training q_loss: 52.4221 Explore P: 0.9736\n",
      "Episode: 16 Total reward: 16.0 Training d_loss: 1.1821 Training g_loss: 0.8448 Training q_loss: 44.9356 Explore P: 0.9721\n",
      "Episode: 17 Total reward: 19.0 Training d_loss: 1.4012 Training g_loss: 0.5672 Training q_loss: 476.1136 Explore P: 0.9703\n",
      "Episode: 18 Total reward: 13.0 Training d_loss: 1.0929 Training g_loss: 1.2447 Training q_loss: 21.1314 Explore P: 0.9690\n",
      "Episode: 19 Total reward: 24.0 Training d_loss: 1.2066 Training g_loss: 1.0252 Training q_loss: 50.9314 Explore P: 0.9667\n",
      "Episode: 20 Total reward: 19.0 Training d_loss: 1.0905 Training g_loss: 1.4080 Training q_loss: 266.1802 Explore P: 0.9649\n",
      "Episode: 21 Total reward: 18.0 Training d_loss: 1.5718 Training g_loss: 0.6017 Training q_loss: 368.3770 Explore P: 0.9632\n",
      "Episode: 22 Total reward: 21.0 Training d_loss: 1.1864 Training g_loss: 0.9849 Training q_loss: 341.5357 Explore P: 0.9612\n",
      "Episode: 23 Total reward: 19.0 Training d_loss: 1.0665 Training g_loss: 1.5572 Training q_loss: 55.0057 Explore P: 0.9594\n",
      "Episode: 24 Total reward: 14.0 Training d_loss: 1.0923 Training g_loss: 1.4574 Training q_loss: 59.9107 Explore P: 0.9580\n",
      "Episode: 25 Total reward: 14.0 Training d_loss: 1.1202 Training g_loss: 1.5564 Training q_loss: 23.5325 Explore P: 0.9567\n",
      "Episode: 26 Total reward: 37.0 Training d_loss: 1.2069 Training g_loss: 0.8639 Training q_loss: 99.0039 Explore P: 0.9532\n",
      "Episode: 27 Total reward: 24.0 Training d_loss: 1.3359 Training g_loss: 0.7383 Training q_loss: 113.6972 Explore P: 0.9510\n",
      "Episode: 28 Total reward: 16.0 Training d_loss: 1.1116 Training g_loss: 1.0432 Training q_loss: 15.7193 Explore P: 0.9495\n",
      "Episode: 29 Total reward: 16.0 Training d_loss: 1.3467 Training g_loss: 0.6650 Training q_loss: 39.8719 Explore P: 0.9480\n",
      "Episode: 30 Total reward: 13.0 Training d_loss: 1.2463 Training g_loss: 1.1467 Training q_loss: 292.4610 Explore P: 0.9467\n",
      "Episode: 31 Total reward: 15.0 Training d_loss: 1.2896 Training g_loss: 1.0069 Training q_loss: 451.8762 Explore P: 0.9453\n",
      "Episode: 32 Total reward: 15.0 Training d_loss: 1.2147 Training g_loss: 0.9773 Training q_loss: 52.2969 Explore P: 0.9439\n",
      "Episode: 33 Total reward: 30.0 Training d_loss: 1.0561 Training g_loss: 1.6862 Training q_loss: 5.9221 Explore P: 0.9411\n",
      "Episode: 34 Total reward: 8.0 Training d_loss: 1.2797 Training g_loss: 0.6777 Training q_loss: 342.5125 Explore P: 0.9404\n",
      "Episode: 35 Total reward: 68.0 Training d_loss: 1.3269 Training g_loss: 0.7164 Training q_loss: 814.6125 Explore P: 0.9341\n",
      "Episode: 36 Total reward: 39.0 Training d_loss: 0.9595 Training g_loss: 1.8112 Training q_loss: 16.9138 Explore P: 0.9305\n",
      "Episode: 37 Total reward: 26.0 Training d_loss: 1.1870 Training g_loss: 1.1579 Training q_loss: 25.7845 Explore P: 0.9281\n",
      "Episode: 38 Total reward: 18.0 Training d_loss: 1.1278 Training g_loss: 1.1936 Training q_loss: 40.4253 Explore P: 0.9264\n",
      "Episode: 39 Total reward: 29.0 Training d_loss: 1.1408 Training g_loss: 0.9750 Training q_loss: 29.7790 Explore P: 0.9238\n",
      "Episode: 40 Total reward: 9.0 Training d_loss: 1.3771 Training g_loss: 0.9338 Training q_loss: 80.9152 Explore P: 0.9230\n",
      "Episode: 41 Total reward: 17.0 Training d_loss: 1.3981 Training g_loss: 0.7091 Training q_loss: 21.6849 Explore P: 0.9214\n",
      "Episode: 42 Total reward: 29.0 Training d_loss: 1.2229 Training g_loss: 0.9903 Training q_loss: 19.6319 Explore P: 0.9188\n",
      "Episode: 43 Total reward: 31.0 Training d_loss: 1.1263 Training g_loss: 1.7520 Training q_loss: 16.6073 Explore P: 0.9160\n",
      "Episode: 44 Total reward: 16.0 Training d_loss: 1.3830 Training g_loss: 1.0987 Training q_loss: 49.6100 Explore P: 0.9145\n",
      "Episode: 45 Total reward: 9.0 Training d_loss: 1.2498 Training g_loss: 1.3912 Training q_loss: 368.2131 Explore P: 0.9137\n",
      "Episode: 46 Total reward: 38.0 Training d_loss: 1.1379 Training g_loss: 1.8746 Training q_loss: 110.7202 Explore P: 0.9103\n",
      "Episode: 47 Total reward: 29.0 Training d_loss: 1.2610 Training g_loss: 0.6956 Training q_loss: 17.5774 Explore P: 0.9077\n",
      "Episode: 48 Total reward: 35.0 Training d_loss: 1.4663 Training g_loss: 0.5308 Training q_loss: 700.0348 Explore P: 0.9045\n",
      "Episode: 49 Total reward: 35.0 Training d_loss: 1.2816 Training g_loss: 0.8872 Training q_loss: 423.5662 Explore P: 0.9014\n",
      "Episode: 50 Total reward: 14.0 Training d_loss: 1.1986 Training g_loss: 0.8912 Training q_loss: 352.5896 Explore P: 0.9002\n",
      "Episode: 51 Total reward: 15.0 Training d_loss: 1.2181 Training g_loss: 0.7925 Training q_loss: 22.1006 Explore P: 0.8988\n",
      "Episode: 52 Total reward: 8.0 Training d_loss: 1.2193 Training g_loss: 0.8041 Training q_loss: 17.6736 Explore P: 0.8981\n",
      "Episode: 53 Total reward: 10.0 Training d_loss: 1.5970 Training g_loss: 0.7891 Training q_loss: 332.3470 Explore P: 0.8972\n",
      "Episode: 54 Total reward: 29.0 Training d_loss: 1.1635 Training g_loss: 1.7438 Training q_loss: 26.1760 Explore P: 0.8947\n",
      "Episode: 55 Total reward: 22.0 Training d_loss: 1.3496 Training g_loss: 1.1982 Training q_loss: 48.6777 Explore P: 0.8927\n",
      "Episode: 56 Total reward: 29.0 Training d_loss: 1.2482 Training g_loss: 1.1364 Training q_loss: 262.1773 Explore P: 0.8902\n",
      "Episode: 57 Total reward: 12.0 Training d_loss: 1.4638 Training g_loss: 0.7932 Training q_loss: 387.4963 Explore P: 0.8891\n",
      "Episode: 58 Total reward: 42.0 Training d_loss: 1.2553 Training g_loss: 0.7484 Training q_loss: 73.3797 Explore P: 0.8854\n",
      "Episode: 59 Total reward: 15.0 Training d_loss: 1.3060 Training g_loss: 0.8610 Training q_loss: 660.1832 Explore P: 0.8841\n",
      "Episode: 60 Total reward: 11.0 Training d_loss: 1.3243 Training g_loss: 0.9479 Training q_loss: 895.3959 Explore P: 0.8831\n",
      "Episode: 61 Total reward: 19.0 Training d_loss: 1.3118 Training g_loss: 0.9875 Training q_loss: 10.4740 Explore P: 0.8815\n",
      "Episode: 62 Total reward: 10.0 Training d_loss: 1.2188 Training g_loss: 0.9804 Training q_loss: 106.6776 Explore P: 0.8806\n",
      "Episode: 63 Total reward: 25.0 Training d_loss: 1.0804 Training g_loss: 1.0076 Training q_loss: 40.6298 Explore P: 0.8784\n",
      "Episode: 64 Total reward: 24.0 Training d_loss: 1.2540 Training g_loss: 1.3141 Training q_loss: 31.0646 Explore P: 0.8764\n",
      "Episode: 65 Total reward: 28.0 Training d_loss: 1.2112 Training g_loss: 1.8252 Training q_loss: 37.5953 Explore P: 0.8739\n",
      "Episode: 66 Total reward: 16.0 Training d_loss: 1.3368 Training g_loss: 0.7818 Training q_loss: 1005.7402 Explore P: 0.8726\n",
      "Episode: 67 Total reward: 11.0 Training d_loss: 1.2975 Training g_loss: 0.9737 Training q_loss: 771.2864 Explore P: 0.8716\n",
      "Episode: 68 Total reward: 13.0 Training d_loss: 1.1411 Training g_loss: 0.9007 Training q_loss: 710.3876 Explore P: 0.8705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 69 Total reward: 23.0 Training d_loss: 1.3018 Training g_loss: 0.7375 Training q_loss: 28.2213 Explore P: 0.8685\n",
      "Episode: 70 Total reward: 20.0 Training d_loss: 1.3535 Training g_loss: 0.9010 Training q_loss: 107.0749 Explore P: 0.8668\n",
      "Episode: 71 Total reward: 14.0 Training d_loss: 1.2582 Training g_loss: 0.8446 Training q_loss: 32.3859 Explore P: 0.8656\n",
      "Episode: 72 Total reward: 88.0 Training d_loss: 1.4020 Training g_loss: 0.6907 Training q_loss: 72.6068 Explore P: 0.8581\n",
      "Episode: 73 Total reward: 11.0 Training d_loss: 1.2905 Training g_loss: 1.2243 Training q_loss: 18.1432 Explore P: 0.8572\n",
      "Episode: 74 Total reward: 16.0 Training d_loss: 1.1795 Training g_loss: 0.8760 Training q_loss: 31.9393 Explore P: 0.8558\n",
      "Episode: 75 Total reward: 12.0 Training d_loss: 1.3879 Training g_loss: 0.8832 Training q_loss: 45.9666 Explore P: 0.8548\n",
      "Episode: 76 Total reward: 11.0 Training d_loss: 1.3655 Training g_loss: 0.9777 Training q_loss: 24.4077 Explore P: 0.8539\n",
      "Episode: 77 Total reward: 12.0 Training d_loss: 1.3774 Training g_loss: 0.8276 Training q_loss: 533.0783 Explore P: 0.8529\n",
      "Episode: 78 Total reward: 11.0 Training d_loss: 1.4350 Training g_loss: 0.6575 Training q_loss: 44.3410 Explore P: 0.8519\n",
      "Episode: 79 Total reward: 17.0 Training d_loss: 1.3772 Training g_loss: 0.7648 Training q_loss: 16.9529 Explore P: 0.8505\n",
      "Episode: 80 Total reward: 12.0 Training d_loss: 1.2108 Training g_loss: 1.3874 Training q_loss: 11.3829 Explore P: 0.8495\n",
      "Episode: 81 Total reward: 19.0 Training d_loss: 1.3573 Training g_loss: 1.3425 Training q_loss: 31.4772 Explore P: 0.8479\n",
      "Episode: 82 Total reward: 35.0 Training d_loss: 1.2091 Training g_loss: 1.2094 Training q_loss: 89.0387 Explore P: 0.8450\n",
      "Episode: 83 Total reward: 40.0 Training d_loss: 1.3260 Training g_loss: 0.8104 Training q_loss: 13.3316 Explore P: 0.8416\n",
      "Episode: 84 Total reward: 19.0 Training d_loss: 1.3627 Training g_loss: 0.6123 Training q_loss: 1156.5082 Explore P: 0.8401\n",
      "Episode: 85 Total reward: 15.0 Training d_loss: 1.3933 Training g_loss: 0.7756 Training q_loss: 45.9716 Explore P: 0.8388\n",
      "Episode: 86 Total reward: 58.0 Training d_loss: 1.3601 Training g_loss: 0.6521 Training q_loss: 58.3400 Explore P: 0.8340\n",
      "Episode: 87 Total reward: 14.0 Training d_loss: 1.3633 Training g_loss: 0.7251 Training q_loss: 85.0520 Explore P: 0.8329\n",
      "Episode: 88 Total reward: 24.0 Training d_loss: 1.4338 Training g_loss: 0.7369 Training q_loss: 508.1858 Explore P: 0.8309\n",
      "Episode: 89 Total reward: 16.0 Training d_loss: 1.3425 Training g_loss: 0.7784 Training q_loss: 23.2982 Explore P: 0.8296\n",
      "Episode: 90 Total reward: 28.0 Training d_loss: 1.3529 Training g_loss: 0.7971 Training q_loss: 980.1774 Explore P: 0.8273\n",
      "Episode: 91 Total reward: 10.0 Training d_loss: 1.2953 Training g_loss: 1.1222 Training q_loss: 30.0139 Explore P: 0.8265\n",
      "Episode: 92 Total reward: 14.0 Training d_loss: 1.3555 Training g_loss: 0.6614 Training q_loss: 101.5449 Explore P: 0.8253\n",
      "Episode: 93 Total reward: 37.0 Training d_loss: 1.4001 Training g_loss: 0.7433 Training q_loss: 79.7228 Explore P: 0.8223\n",
      "Episode: 94 Total reward: 20.0 Training d_loss: 1.6861 Training g_loss: 0.6238 Training q_loss: 192.2768 Explore P: 0.8207\n",
      "Episode: 95 Total reward: 29.0 Training d_loss: 1.3040 Training g_loss: 1.1993 Training q_loss: 823.7786 Explore P: 0.8184\n",
      "Episode: 96 Total reward: 17.0 Training d_loss: 1.3567 Training g_loss: 0.7306 Training q_loss: 1903.4645 Explore P: 0.8170\n",
      "Episode: 97 Total reward: 144.0 Training d_loss: 1.4023 Training g_loss: 0.5990 Training q_loss: 15.0795 Explore P: 0.8054\n",
      "Episode: 98 Total reward: 73.0 Training d_loss: 1.1849 Training g_loss: 1.5435 Training q_loss: 27.8843 Explore P: 0.7997\n",
      "Episode: 99 Total reward: 23.0 Training d_loss: 1.3211 Training g_loss: 0.7881 Training q_loss: 235.2255 Explore P: 0.7978\n",
      "Episode: 100 Total reward: 80.0 Training d_loss: 1.4268 Training g_loss: 0.9379 Training q_loss: 14.3181 Explore P: 0.7916\n",
      "Episode: 101 Total reward: 42.0 Training d_loss: 1.3302 Training g_loss: 0.6916 Training q_loss: 22.3451 Explore P: 0.7883\n",
      "Episode: 102 Total reward: 83.0 Training d_loss: 1.3984 Training g_loss: 0.6164 Training q_loss: 172.2495 Explore P: 0.7819\n",
      "Episode: 103 Total reward: 14.0 Training d_loss: 1.3879 Training g_loss: 0.6843 Training q_loss: 44.6040 Explore P: 0.7808\n",
      "Episode: 104 Total reward: 30.0 Training d_loss: 1.2768 Training g_loss: 0.8186 Training q_loss: 42.7313 Explore P: 0.7785\n",
      "Episode: 105 Total reward: 20.0 Training d_loss: 1.3053 Training g_loss: 0.7068 Training q_loss: 43.9226 Explore P: 0.7769\n",
      "Episode: 106 Total reward: 41.0 Training d_loss: 1.2284 Training g_loss: 0.9628 Training q_loss: 38.4770 Explore P: 0.7738\n",
      "Episode: 107 Total reward: 67.0 Training d_loss: 1.2778 Training g_loss: 1.0719 Training q_loss: 53.4172 Explore P: 0.7687\n",
      "Episode: 108 Total reward: 67.0 Training d_loss: 1.2421 Training g_loss: 1.2133 Training q_loss: 16.3861 Explore P: 0.7636\n",
      "Episode: 109 Total reward: 10.0 Training d_loss: 1.3303 Training g_loss: 0.8472 Training q_loss: 924.1036 Explore P: 0.7629\n",
      "Episode: 110 Total reward: 48.0 Training d_loss: 1.3902 Training g_loss: 0.6760 Training q_loss: 857.3662 Explore P: 0.7593\n",
      "Episode: 111 Total reward: 14.0 Training d_loss: 1.3461 Training g_loss: 1.2210 Training q_loss: 27.7050 Explore P: 0.7582\n",
      "Episode: 112 Total reward: 67.0 Training d_loss: 1.1898 Training g_loss: 1.5903 Training q_loss: 58.8570 Explore P: 0.7532\n",
      "Episode: 113 Total reward: 24.0 Training d_loss: 1.4332 Training g_loss: 0.6319 Training q_loss: 85.0970 Explore P: 0.7514\n",
      "Episode: 114 Total reward: 24.0 Training d_loss: 1.3820 Training g_loss: 0.6209 Training q_loss: 82.4978 Explore P: 0.7497\n",
      "Episode: 115 Total reward: 31.0 Training d_loss: 1.5153 Training g_loss: 0.7299 Training q_loss: 70.1740 Explore P: 0.7474\n",
      "Episode: 116 Total reward: 28.0 Training d_loss: 1.2241 Training g_loss: 1.8065 Training q_loss: 30.6229 Explore P: 0.7453\n",
      "Episode: 117 Total reward: 19.0 Training d_loss: 1.2518 Training g_loss: 1.7876 Training q_loss: 14.9279 Explore P: 0.7439\n",
      "Episode: 118 Total reward: 15.0 Training d_loss: 1.2587 Training g_loss: 1.1996 Training q_loss: 1587.6284 Explore P: 0.7428\n",
      "Episode: 119 Total reward: 24.0 Training d_loss: 1.3048 Training g_loss: 1.3136 Training q_loss: 2393.2588 Explore P: 0.7411\n",
      "Episode: 120 Total reward: 40.0 Training d_loss: 1.2280 Training g_loss: 1.0510 Training q_loss: 73.6654 Explore P: 0.7381\n",
      "Episode: 121 Total reward: 56.0 Training d_loss: 1.2782 Training g_loss: 1.2681 Training q_loss: 1284.3096 Explore P: 0.7341\n",
      "Episode: 122 Total reward: 73.0 Training d_loss: 1.2676 Training g_loss: 1.0091 Training q_loss: 45.2163 Explore P: 0.7288\n",
      "Episode: 123 Total reward: 25.0 Training d_loss: 1.3752 Training g_loss: 0.6486 Training q_loss: 33.7359 Explore P: 0.7270\n",
      "Episode: 124 Total reward: 26.0 Training d_loss: 0.9443 Training g_loss: 1.1767 Training q_loss: 163.7006 Explore P: 0.7252\n",
      "Episode: 125 Total reward: 29.0 Training d_loss: 1.3862 Training g_loss: 0.7334 Training q_loss: 603.3929 Explore P: 0.7231\n",
      "Episode: 126 Total reward: 30.0 Training d_loss: 1.2834 Training g_loss: 0.7221 Training q_loss: 100.7772 Explore P: 0.7210\n",
      "Episode: 127 Total reward: 9.0 Training d_loss: 1.3407 Training g_loss: 0.8620 Training q_loss: 7.1316 Explore P: 0.7203\n",
      "Episode: 128 Total reward: 43.0 Training d_loss: 1.3300 Training g_loss: 1.1498 Training q_loss: 297.1172 Explore P: 0.7173\n",
      "Episode: 129 Total reward: 18.0 Training d_loss: 1.4061 Training g_loss: 0.6090 Training q_loss: 515.3379 Explore P: 0.7160\n",
      "Episode: 130 Total reward: 47.0 Training d_loss: 1.3658 Training g_loss: 0.6768 Training q_loss: 47.5222 Explore P: 0.7127\n",
      "Episode: 131 Total reward: 12.0 Training d_loss: 1.2768 Training g_loss: 0.7408 Training q_loss: 20.3842 Explore P: 0.7118\n",
      "Episode: 132 Total reward: 21.0 Training d_loss: 1.3207 Training g_loss: 0.7376 Training q_loss: 64.7729 Explore P: 0.7104\n",
      "Episode: 133 Total reward: 32.0 Training d_loss: 1.1905 Training g_loss: 1.3452 Training q_loss: 24.7150 Explore P: 0.7081\n",
      "Episode: 134 Total reward: 47.0 Training d_loss: 1.1773 Training g_loss: 1.2366 Training q_loss: 48.2552 Explore P: 0.7049\n",
      "Episode: 135 Total reward: 67.0 Training d_loss: 1.2632 Training g_loss: 0.8494 Training q_loss: 68.5969 Explore P: 0.7002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 136 Total reward: 101.0 Training d_loss: 1.3125 Training g_loss: 0.7522 Training q_loss: 3226.3022 Explore P: 0.6933\n",
      "Episode: 137 Total reward: 33.0 Training d_loss: 1.4055 Training g_loss: 0.6127 Training q_loss: 57.5578 Explore P: 0.6910\n",
      "Episode: 138 Total reward: 63.0 Training d_loss: 1.1969 Training g_loss: 0.9039 Training q_loss: 194.4655 Explore P: 0.6868\n",
      "Episode: 139 Total reward: 23.0 Training d_loss: 1.4810 Training g_loss: 0.7251 Training q_loss: 6434.2080 Explore P: 0.6852\n",
      "Episode: 140 Total reward: 13.0 Training d_loss: 1.2810 Training g_loss: 0.9302 Training q_loss: 241.2480 Explore P: 0.6843\n",
      "Episode: 141 Total reward: 18.0 Training d_loss: 1.3504 Training g_loss: 0.7097 Training q_loss: 5104.7407 Explore P: 0.6831\n",
      "Episode: 142 Total reward: 31.0 Training d_loss: 1.3604 Training g_loss: 0.7136 Training q_loss: 675.3869 Explore P: 0.6810\n",
      "Episode: 143 Total reward: 121.0 Training d_loss: 1.4356 Training g_loss: 0.6339 Training q_loss: 237.7728 Explore P: 0.6730\n",
      "Episode: 144 Total reward: 30.0 Training d_loss: 1.2882 Training g_loss: 1.2177 Training q_loss: 61.7411 Explore P: 0.6710\n",
      "Episode: 145 Total reward: 15.0 Training d_loss: 1.3316 Training g_loss: 0.7876 Training q_loss: 1227.1354 Explore P: 0.6700\n",
      "Episode: 146 Total reward: 33.0 Training d_loss: 1.3797 Training g_loss: 0.6963 Training q_loss: 52.1807 Explore P: 0.6678\n",
      "Episode: 147 Total reward: 55.0 Training d_loss: 1.3574 Training g_loss: 0.7200 Training q_loss: 2036.8500 Explore P: 0.6642\n",
      "Episode: 148 Total reward: 14.0 Training d_loss: 1.5535 Training g_loss: 0.5930 Training q_loss: 81.8901 Explore P: 0.6633\n",
      "Episode: 149 Total reward: 106.0 Training d_loss: 1.2632 Training g_loss: 1.2129 Training q_loss: 125.5295 Explore P: 0.6564\n",
      "Episode: 150 Total reward: 40.0 Training d_loss: 1.1722 Training g_loss: 1.3156 Training q_loss: 162.5782 Explore P: 0.6538\n",
      "Episode: 151 Total reward: 78.0 Training d_loss: 1.4305 Training g_loss: 0.6656 Training q_loss: 178.3104 Explore P: 0.6488\n",
      "Episode: 152 Total reward: 122.0 Training d_loss: 1.3015 Training g_loss: 0.9191 Training q_loss: 137.4034 Explore P: 0.6411\n",
      "Episode: 153 Total reward: 20.0 Training d_loss: 1.2998 Training g_loss: 0.8147 Training q_loss: 99.4514 Explore P: 0.6398\n",
      "Episode: 154 Total reward: 38.0 Training d_loss: 1.1704 Training g_loss: 0.8682 Training q_loss: 102.9355 Explore P: 0.6374\n",
      "Episode: 155 Total reward: 61.0 Training d_loss: 1.3799 Training g_loss: 0.6719 Training q_loss: 3220.6587 Explore P: 0.6336\n",
      "Episode: 156 Total reward: 105.0 Training d_loss: 1.3555 Training g_loss: 0.6526 Training q_loss: 86.1208 Explore P: 0.6271\n",
      "Episode: 157 Total reward: 54.0 Training d_loss: 1.2930 Training g_loss: 1.1285 Training q_loss: 72.1373 Explore P: 0.6238\n",
      "Episode: 158 Total reward: 112.0 Training d_loss: 1.2406 Training g_loss: 1.1856 Training q_loss: 235.3590 Explore P: 0.6169\n",
      "Episode: 159 Total reward: 65.0 Training d_loss: 1.4447 Training g_loss: 0.6153 Training q_loss: 2388.9668 Explore P: 0.6130\n",
      "Episode: 160 Total reward: 90.0 Training d_loss: 1.2285 Training g_loss: 1.1504 Training q_loss: 155.8553 Explore P: 0.6076\n",
      "Episode: 161 Total reward: 65.0 Training d_loss: 1.3033 Training g_loss: 0.7555 Training q_loss: 215.3814 Explore P: 0.6037\n",
      "Episode: 162 Total reward: 33.0 Training d_loss: 1.3806 Training g_loss: 0.6631 Training q_loss: 121.4264 Explore P: 0.6018\n",
      "Episode: 163 Total reward: 90.0 Training d_loss: 1.2111 Training g_loss: 1.4149 Training q_loss: 2002.5729 Explore P: 0.5965\n",
      "Episode: 164 Total reward: 103.0 Training d_loss: 1.2512 Training g_loss: 1.1379 Training q_loss: 1013.3217 Explore P: 0.5905\n",
      "Episode: 165 Total reward: 15.0 Training d_loss: 1.3767 Training g_loss: 0.6827 Training q_loss: 156.6593 Explore P: 0.5896\n",
      "Episode: 166 Total reward: 31.0 Training d_loss: 1.3294 Training g_loss: 1.0556 Training q_loss: 89.7105 Explore P: 0.5878\n",
      "Episode: 167 Total reward: 51.0 Training d_loss: 1.3796 Training g_loss: 0.6952 Training q_loss: 8249.5762 Explore P: 0.5848\n",
      "Episode: 168 Total reward: 110.0 Training d_loss: 1.3650 Training g_loss: 0.7035 Training q_loss: 197.0042 Explore P: 0.5786\n",
      "Episode: 169 Total reward: 83.0 Training d_loss: 1.3871 Training g_loss: 0.6735 Training q_loss: 2153.4048 Explore P: 0.5739\n",
      "Episode: 170 Total reward: 72.0 Training d_loss: 1.3116 Training g_loss: 1.1345 Training q_loss: 265.8251 Explore P: 0.5698\n",
      "Episode: 171 Total reward: 122.0 Training d_loss: 1.4405 Training g_loss: 0.6254 Training q_loss: 46.8820 Explore P: 0.5630\n",
      "Episode: 172 Total reward: 111.0 Training d_loss: 1.3880 Training g_loss: 0.6752 Training q_loss: 215.8976 Explore P: 0.5569\n",
      "Episode: 173 Total reward: 43.0 Training d_loss: 1.3586 Training g_loss: 0.6818 Training q_loss: 192.8331 Explore P: 0.5546\n",
      "Episode: 174 Total reward: 108.0 Training d_loss: 1.2098 Training g_loss: 1.1531 Training q_loss: 224.4928 Explore P: 0.5487\n",
      "Episode: 175 Total reward: 199.0 Training d_loss: 1.4013 Training g_loss: 0.6750 Training q_loss: 842.7103 Explore P: 0.5381\n",
      "Episode: 176 Total reward: 137.0 Training d_loss: 1.3448 Training g_loss: 0.6659 Training q_loss: 223.1625 Explore P: 0.5309\n",
      "Episode: 177 Total reward: 90.0 Training d_loss: 1.3580 Training g_loss: 0.7968 Training q_loss: 140.4530 Explore P: 0.5263\n",
      "Episode: 178 Total reward: 130.0 Training d_loss: 1.3867 Training g_loss: 0.6746 Training q_loss: 343.8732 Explore P: 0.5196\n",
      "Episode: 179 Total reward: 196.0 Training d_loss: 1.2818 Training g_loss: 0.9593 Training q_loss: 179.5766 Explore P: 0.5097\n",
      "Episode: 180 Total reward: 168.0 Training d_loss: 1.3557 Training g_loss: 0.7359 Training q_loss: 195.5491 Explore P: 0.5014\n",
      "Episode: 181 Total reward: 42.0 Training d_loss: 1.2521 Training g_loss: 0.9314 Training q_loss: 236.7787 Explore P: 0.4993\n",
      "Episode: 182 Total reward: 37.0 Training d_loss: 1.4252 Training g_loss: 0.6894 Training q_loss: 143.0765 Explore P: 0.4975\n",
      "Episode: 183 Total reward: 129.0 Training d_loss: 1.4488 Training g_loss: 0.6261 Training q_loss: 55.3482 Explore P: 0.4913\n",
      "Episode: 184 Total reward: 20.0 Training d_loss: 1.3932 Training g_loss: 0.7261 Training q_loss: 8178.3242 Explore P: 0.4903\n",
      "Episode: 185 Total reward: 90.0 Training d_loss: 1.2202 Training g_loss: 1.4545 Training q_loss: 368.6979 Explore P: 0.4860\n",
      "Episode: 186 Total reward: 126.0 Training d_loss: 1.3507 Training g_loss: 0.7404 Training q_loss: 115.6448 Explore P: 0.4800\n",
      "Episode: 187 Total reward: 199.0 Training d_loss: 1.3404 Training g_loss: 0.7274 Training q_loss: 3656.7671 Explore P: 0.4708\n",
      "Episode: 188 Total reward: 29.0 Training d_loss: 1.3543 Training g_loss: 0.7281 Training q_loss: 27280.5430 Explore P: 0.4694\n",
      "Episode: 189 Total reward: 94.0 Training d_loss: 1.4641 Training g_loss: 0.6547 Training q_loss: 181.3075 Explore P: 0.4651\n",
      "Episode: 190 Total reward: 159.0 Training d_loss: 1.4090 Training g_loss: 0.6680 Training q_loss: 840.6675 Explore P: 0.4580\n",
      "Episode: 191 Total reward: 184.0 Training d_loss: 1.3865 Training g_loss: 0.6818 Training q_loss: 174.1617 Explore P: 0.4498\n",
      "Episode: 192 Total reward: 60.0 Training d_loss: 1.3816 Training g_loss: 0.7115 Training q_loss: 91.9937 Explore P: 0.4472\n",
      "Episode: 193 Total reward: 143.0 Training d_loss: 1.3825 Training g_loss: 0.6835 Training q_loss: 11809.6729 Explore P: 0.4410\n",
      "Episode: 194 Total reward: 199.0 Training d_loss: 1.3265 Training g_loss: 0.9638 Training q_loss: 144.5956 Explore P: 0.4325\n",
      "Episode: 195 Total reward: 199.0 Training d_loss: 1.3508 Training g_loss: 0.7555 Training q_loss: 979.0376 Explore P: 0.4241\n",
      "Episode: 196 Total reward: 132.0 Training d_loss: 1.3029 Training g_loss: 0.9409 Training q_loss: 616.9863 Explore P: 0.4187\n",
      "Episode: 197 Total reward: 149.0 Training d_loss: 1.3908 Training g_loss: 0.6883 Training q_loss: 370.6741 Explore P: 0.4127\n",
      "Episode: 198 Total reward: 128.0 Training d_loss: 1.3540 Training g_loss: 0.7380 Training q_loss: 8138.6226 Explore P: 0.4075\n",
      "Episode: 199 Total reward: 156.0 Training d_loss: 1.3871 Training g_loss: 0.6640 Training q_loss: 887.5330 Explore P: 0.4014\n",
      "Episode: 200 Total reward: 199.0 Training d_loss: 1.2804 Training g_loss: 1.0196 Training q_loss: 646.5802 Explore P: 0.3937\n",
      "Episode: 201 Total reward: 171.0 Training d_loss: 1.3909 Training g_loss: 0.6923 Training q_loss: 132.5122 Explore P: 0.3872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 202 Total reward: 119.0 Training d_loss: 1.3826 Training g_loss: 0.6891 Training q_loss: 29994.9414 Explore P: 0.3827\n",
      "Episode: 203 Total reward: 113.0 Training d_loss: 1.3211 Training g_loss: 0.6886 Training q_loss: 2994.5364 Explore P: 0.3785\n",
      "Episode: 204 Total reward: 158.0 Training d_loss: 1.3935 Training g_loss: 0.6715 Training q_loss: 584.3666 Explore P: 0.3727\n",
      "Episode: 205 Total reward: 199.0 Training d_loss: 1.3959 Training g_loss: 0.6705 Training q_loss: 1003.6022 Explore P: 0.3656\n",
      "Episode: 206 Total reward: 199.0 Training d_loss: 1.4406 Training g_loss: 0.6719 Training q_loss: 834.0086 Explore P: 0.3586\n",
      "Episode: 207 Total reward: 193.0 Training d_loss: 1.3653 Training g_loss: 0.6811 Training q_loss: 299.0294 Explore P: 0.3519\n",
      "Episode: 208 Total reward: 199.0 Training d_loss: 1.3213 Training g_loss: 0.7771 Training q_loss: 449.4674 Explore P: 0.3452\n",
      "Episode: 209 Total reward: 24.0 Training d_loss: 1.3964 Training g_loss: 0.6643 Training q_loss: 462.7298 Explore P: 0.3444\n",
      "Episode: 210 Total reward: 199.0 Training d_loss: 1.3479 Training g_loss: 0.7913 Training q_loss: 160.0890 Explore P: 0.3378\n",
      "Episode: 211 Total reward: 188.0 Training d_loss: 1.3342 Training g_loss: 0.9531 Training q_loss: 607.8818 Explore P: 0.3317\n",
      "Episode: 212 Total reward: 189.0 Training d_loss: 1.3314 Training g_loss: 0.7386 Training q_loss: 529.0272 Explore P: 0.3257\n",
      "Episode: 213 Total reward: 164.0 Training d_loss: 1.3956 Training g_loss: 0.6811 Training q_loss: 642.0812 Explore P: 0.3205\n",
      "Episode: 214 Total reward: 165.0 Training d_loss: 1.3800 Training g_loss: 0.6924 Training q_loss: 224.5833 Explore P: 0.3155\n",
      "Episode: 215 Total reward: 57.0 Training d_loss: 1.4063 Training g_loss: 0.7005 Training q_loss: 1045.3124 Explore P: 0.3137\n",
      "Episode: 216 Total reward: 199.0 Training d_loss: 1.2594 Training g_loss: 0.7845 Training q_loss: 2143.3447 Explore P: 0.3077\n",
      "Episode: 217 Total reward: 198.0 Training d_loss: 1.4194 Training g_loss: 0.6512 Training q_loss: 362.6612 Explore P: 0.3019\n",
      "Episode: 218 Total reward: 166.0 Training d_loss: 1.3513 Training g_loss: 0.7608 Training q_loss: 606.9890 Explore P: 0.2971\n",
      "Episode: 219 Total reward: 168.0 Training d_loss: 1.3755 Training g_loss: 0.7056 Training q_loss: 636.9344 Explore P: 0.2923\n",
      "Episode: 220 Total reward: 199.0 Training d_loss: 1.3579 Training g_loss: 0.6988 Training q_loss: 292.5632 Explore P: 0.2867\n",
      "Episode: 221 Total reward: 163.0 Training d_loss: 1.3463 Training g_loss: 1.1273 Training q_loss: 26351.7598 Explore P: 0.2823\n",
      "Episode: 222 Total reward: 199.0 Training d_loss: 1.1550 Training g_loss: 1.3318 Training q_loss: 193.2809 Explore P: 0.2769\n",
      "Episode: 223 Total reward: 199.0 Training d_loss: 1.3909 Training g_loss: 0.6885 Training q_loss: 250.9369 Explore P: 0.2716\n",
      "Episode: 224 Total reward: 183.0 Training d_loss: 1.3208 Training g_loss: 0.9602 Training q_loss: 1834.0170 Explore P: 0.2669\n",
      "Episode: 225 Total reward: 199.0 Training d_loss: 1.3170 Training g_loss: 0.9773 Training q_loss: 233.6003 Explore P: 0.2618\n",
      "Episode: 226 Total reward: 162.0 Training d_loss: 1.3022 Training g_loss: 1.0425 Training q_loss: 223.4118 Explore P: 0.2578\n",
      "Episode: 227 Total reward: 199.0 Training d_loss: 1.3468 Training g_loss: 0.6893 Training q_loss: 345.8544 Explore P: 0.2529\n",
      "Episode: 228 Total reward: 199.0 Training d_loss: 1.2778 Training g_loss: 1.0296 Training q_loss: 308.2921 Explore P: 0.2481\n",
      "Episode: 229 Total reward: 157.0 Training d_loss: 1.3904 Training g_loss: 0.6502 Training q_loss: 260.5599 Explore P: 0.2444\n",
      "Episode: 230 Total reward: 172.0 Training d_loss: 1.3351 Training g_loss: 0.7454 Training q_loss: 129.3833 Explore P: 0.2404\n",
      "Episode: 231 Total reward: 183.0 Training d_loss: 1.2132 Training g_loss: 1.1123 Training q_loss: 141.2294 Explore P: 0.2362\n",
      "Episode: 232 Total reward: 199.0 Training d_loss: 1.3600 Training g_loss: 1.0271 Training q_loss: 110.5619 Explore P: 0.2318\n",
      "Episode: 233 Total reward: 199.0 Training d_loss: 1.2884 Training g_loss: 1.0316 Training q_loss: 108.5030 Explore P: 0.2274\n",
      "Episode: 234 Total reward: 199.0 Training d_loss: 1.3411 Training g_loss: 1.0401 Training q_loss: 194.9899 Explore P: 0.2231\n",
      "Episode: 235 Total reward: 199.0 Training d_loss: 1.2362 Training g_loss: 1.1417 Training q_loss: 134.8812 Explore P: 0.2189\n",
      "Episode: 236 Total reward: 127.0 Training d_loss: 1.3422 Training g_loss: 0.7961 Training q_loss: 98.2782 Explore P: 0.2163\n",
      "Episode: 237 Total reward: 199.0 Training d_loss: 1.3647 Training g_loss: 0.5651 Training q_loss: 208.5642 Explore P: 0.2122\n",
      "Episode: 238 Total reward: 170.0 Training d_loss: 1.2509 Training g_loss: 1.2812 Training q_loss: 106.1288 Explore P: 0.2088\n",
      "Episode: 239 Total reward: 199.0 Training d_loss: 1.3225 Training g_loss: 0.9433 Training q_loss: 122.6467 Explore P: 0.2049\n",
      "Episode: 240 Total reward: 199.0 Training d_loss: 1.3606 Training g_loss: 0.9414 Training q_loss: 118.6840 Explore P: 0.2011\n",
      "Episode: 241 Total reward: 199.0 Training d_loss: 1.3596 Training g_loss: 0.7057 Training q_loss: 181.0304 Explore P: 0.1973\n",
      "Episode: 242 Total reward: 199.0 Training d_loss: 1.2814 Training g_loss: 1.2195 Training q_loss: 118.4082 Explore P: 0.1936\n",
      "Episode: 243 Total reward: 162.0 Training d_loss: 1.4061 Training g_loss: 0.6614 Training q_loss: 372.3272 Explore P: 0.1907\n",
      "Episode: 244 Total reward: 199.0 Training d_loss: 1.3557 Training g_loss: 1.0961 Training q_loss: 223.2962 Explore P: 0.1871\n",
      "Episode: 245 Total reward: 184.0 Training d_loss: 1.2260 Training g_loss: 1.3208 Training q_loss: 142.3062 Explore P: 0.1839\n",
      "Episode: 246 Total reward: 199.0 Training d_loss: 1.4028 Training g_loss: 0.6076 Training q_loss: 89.6872 Explore P: 0.1804\n",
      "Episode: 247 Total reward: 199.0 Training d_loss: 1.2473 Training g_loss: 1.7262 Training q_loss: 117.4970 Explore P: 0.1771\n",
      "Episode: 248 Total reward: 180.0 Training d_loss: 1.2577 Training g_loss: 1.5195 Training q_loss: 20.3183 Explore P: 0.1741\n",
      "Episode: 249 Total reward: 199.0 Training d_loss: 1.3215 Training g_loss: 0.8569 Training q_loss: 57.0539 Explore P: 0.1709\n",
      "Episode: 250 Total reward: 198.0 Training d_loss: 1.2428 Training g_loss: 1.1724 Training q_loss: 78.9796 Explore P: 0.1677\n",
      "Episode: 251 Total reward: 198.0 Training d_loss: 1.2925 Training g_loss: 0.7472 Training q_loss: 78.3632 Explore P: 0.1646\n",
      "Episode: 252 Total reward: 199.0 Training d_loss: 1.2075 Training g_loss: 1.1285 Training q_loss: 37.4363 Explore P: 0.1616\n",
      "Episode: 253 Total reward: 199.0 Training d_loss: 1.1558 Training g_loss: 1.5496 Training q_loss: 121.4377 Explore P: 0.1586\n",
      "Episode: 254 Total reward: 191.0 Training d_loss: 1.2044 Training g_loss: 2.0823 Training q_loss: 19.9258 Explore P: 0.1558\n",
      "Episode: 255 Total reward: 170.0 Training d_loss: 1.3442 Training g_loss: 1.4696 Training q_loss: 227.8794 Explore P: 0.1533\n",
      "Episode: 256 Total reward: 199.0 Training d_loss: 1.3680 Training g_loss: 0.7199 Training q_loss: 75.0405 Explore P: 0.1505\n",
      "Episode: 257 Total reward: 184.0 Training d_loss: 1.5128 Training g_loss: 0.6987 Training q_loss: 85.7469 Explore P: 0.1479\n",
      "Episode: 258 Total reward: 187.0 Training d_loss: 1.3084 Training g_loss: 2.7047 Training q_loss: 44.1338 Explore P: 0.1454\n",
      "Episode: 259 Total reward: 155.0 Training d_loss: 1.2613 Training g_loss: 2.2885 Training q_loss: 32.2947 Explore P: 0.1433\n",
      "Episode: 260 Total reward: 186.0 Training d_loss: 1.1182 Training g_loss: 1.5620 Training q_loss: 29.2242 Explore P: 0.1408\n",
      "Episode: 261 Total reward: 156.0 Training d_loss: 1.2999 Training g_loss: 2.4398 Training q_loss: 19.5258 Explore P: 0.1388\n",
      "Episode: 262 Total reward: 174.0 Training d_loss: 1.2107 Training g_loss: 2.4958 Training q_loss: 75.7737 Explore P: 0.1366\n",
      "Episode: 263 Total reward: 184.0 Training d_loss: 1.1692 Training g_loss: 0.7947 Training q_loss: 24.3240 Explore P: 0.1343\n",
      "Episode: 264 Total reward: 199.0 Training d_loss: 1.1638 Training g_loss: 3.6075 Training q_loss: 15.7015 Explore P: 0.1318\n",
      "Episode: 265 Total reward: 199.0 Training d_loss: 1.2100 Training g_loss: 1.4807 Training q_loss: 109.2924 Explore P: 0.1294\n",
      "Episode: 266 Total reward: 199.0 Training d_loss: 1.1720 Training g_loss: 2.0260 Training q_loss: 23.3168 Explore P: 0.1271\n",
      "Episode: 267 Total reward: 199.0 Training d_loss: 1.2466 Training g_loss: 2.2888 Training q_loss: 48.2074 Explore P: 0.1248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 268 Total reward: 199.0 Training d_loss: 1.3467 Training g_loss: 0.7126 Training q_loss: 51.4991 Explore P: 0.1225\n",
      "Episode: 269 Total reward: 199.0 Training d_loss: 1.1983 Training g_loss: 3.0473 Training q_loss: 67.4182 Explore P: 0.1203\n",
      "Episode: 270 Total reward: 199.0 Training d_loss: 1.1875 Training g_loss: 2.0257 Training q_loss: 95.5106 Explore P: 0.1181\n",
      "Episode: 271 Total reward: 199.0 Training d_loss: 1.1191 Training g_loss: 2.0690 Training q_loss: 27.3672 Explore P: 0.1160\n",
      "Episode: 272 Total reward: 199.0 Training d_loss: 1.1323 Training g_loss: 3.1628 Training q_loss: 20.4127 Explore P: 0.1139\n",
      "Episode: 273 Total reward: 199.0 Training d_loss: 1.2244 Training g_loss: 0.8163 Training q_loss: 28.3440 Explore P: 0.1119\n",
      "Episode: 274 Total reward: 199.0 Training d_loss: 1.2262 Training g_loss: 1.0968 Training q_loss: 58.5725 Explore P: 0.1099\n",
      "Episode: 275 Total reward: 199.0 Training d_loss: 1.0338 Training g_loss: 4.5659 Training q_loss: 34.8613 Explore P: 0.1079\n",
      "Episode: 276 Total reward: 199.0 Training d_loss: 1.2038 Training g_loss: 2.4076 Training q_loss: 78.3327 Explore P: 0.1060\n",
      "Episode: 277 Total reward: 199.0 Training d_loss: 1.3055 Training g_loss: 0.7995 Training q_loss: 30.1828 Explore P: 0.1041\n",
      "Episode: 278 Total reward: 199.0 Training d_loss: 1.2630 Training g_loss: 1.8927 Training q_loss: 219.9834 Explore P: 0.1022\n",
      "Episode: 279 Total reward: 199.0 Training d_loss: 1.1849 Training g_loss: 3.2395 Training q_loss: 87.7303 Explore P: 0.1004\n",
      "Episode: 280 Total reward: 199.0 Training d_loss: 1.1251 Training g_loss: 2.6179 Training q_loss: 42.4904 Explore P: 0.0986\n",
      "Episode: 281 Total reward: 199.0 Training d_loss: 0.9074 Training g_loss: 1.3508 Training q_loss: 40.3754 Explore P: 0.0969\n",
      "Episode: 282 Total reward: 131.0 Training d_loss: 1.2733 Training g_loss: 2.0409 Training q_loss: 25.2884 Explore P: 0.0957\n",
      "Episode: 283 Total reward: 32.0 Training d_loss: 0.9839 Training g_loss: 0.9868 Training q_loss: 149.6419 Explore P: 0.0955\n",
      "Episode: 284 Total reward: 88.0 Training d_loss: 0.8934 Training g_loss: 0.9399 Training q_loss: 45.4977 Explore P: 0.0947\n",
      "Episode: 285 Total reward: 30.0 Training d_loss: 0.8617 Training g_loss: 1.0475 Training q_loss: 39.1140 Explore P: 0.0945\n",
      "Episode: 286 Total reward: 113.0 Training d_loss: 0.9740 Training g_loss: 1.0522 Training q_loss: 145.7854 Explore P: 0.0935\n",
      "Episode: 287 Total reward: 199.0 Training d_loss: 1.1789 Training g_loss: 0.9654 Training q_loss: 204.1205 Explore P: 0.0919\n",
      "Episode: 288 Total reward: 85.0 Training d_loss: 1.0903 Training g_loss: 1.1212 Training q_loss: 86.3360 Explore P: 0.0912\n",
      "Episode: 289 Total reward: 22.0 Training d_loss: 1.5416 Training g_loss: 0.9565 Training q_loss: 42173.5898 Explore P: 0.0910\n",
      "Episode: 290 Total reward: 75.0 Training d_loss: 1.1520 Training g_loss: 2.9455 Training q_loss: 144.7389 Explore P: 0.0904\n",
      "Episode: 291 Total reward: 159.0 Training d_loss: 1.0922 Training g_loss: 0.9292 Training q_loss: 203.8835 Explore P: 0.0891\n",
      "Episode: 292 Total reward: 16.0 Training d_loss: 0.8781 Training g_loss: 1.1658 Training q_loss: 183.2169 Explore P: 0.0890\n",
      "Episode: 293 Total reward: 117.0 Training d_loss: 1.2770 Training g_loss: 0.8096 Training q_loss: 119.7953 Explore P: 0.0881\n",
      "Episode: 294 Total reward: 32.0 Training d_loss: 1.1007 Training g_loss: 0.9351 Training q_loss: 71.3677 Explore P: 0.0878\n",
      "Episode: 295 Total reward: 122.0 Training d_loss: 1.0615 Training g_loss: 1.1820 Training q_loss: 141.9214 Explore P: 0.0869\n",
      "Episode: 296 Total reward: 196.0 Training d_loss: 1.2216 Training g_loss: 2.6742 Training q_loss: 65.9563 Explore P: 0.0854\n",
      "Episode: 297 Total reward: 199.0 Training d_loss: 1.2759 Training g_loss: 2.5371 Training q_loss: 149.9132 Explore P: 0.0839\n",
      "Episode: 298 Total reward: 199.0 Training d_loss: 1.3178 Training g_loss: 0.7378 Training q_loss: 133.4236 Explore P: 0.0825\n",
      "Episode: 299 Total reward: 199.0 Training d_loss: 1.3948 Training g_loss: 0.6352 Training q_loss: 89.9621 Explore P: 0.0810\n",
      "Episode: 300 Total reward: 199.0 Training d_loss: 1.0543 Training g_loss: 2.8171 Training q_loss: 37.6394 Explore P: 0.0796\n",
      "Episode: 301 Total reward: 199.0 Training d_loss: 1.2135 Training g_loss: 3.4183 Training q_loss: 75.0963 Explore P: 0.0783\n",
      "Episode: 302 Total reward: 182.0 Training d_loss: 1.3331 Training g_loss: 2.0601 Training q_loss: 90.4899 Explore P: 0.0770\n",
      "Episode: 303 Total reward: 199.0 Training d_loss: 1.3077 Training g_loss: 1.7472 Training q_loss: 58.8695 Explore P: 0.0757\n",
      "Episode: 304 Total reward: 177.0 Training d_loss: 1.2718 Training g_loss: 2.0824 Training q_loss: 91.5663 Explore P: 0.0745\n",
      "Episode: 305 Total reward: 199.0 Training d_loss: 1.3000 Training g_loss: 3.4256 Training q_loss: 79.4653 Explore P: 0.0733\n",
      "Episode: 306 Total reward: 184.0 Training d_loss: 1.4395 Training g_loss: 0.7119 Training q_loss: 1021.9177 Explore P: 0.0721\n",
      "Episode: 307 Total reward: 199.0 Training d_loss: 1.0549 Training g_loss: 4.0842 Training q_loss: 33.2089 Explore P: 0.0709\n",
      "Episode: 308 Total reward: 199.0 Training d_loss: 1.0640 Training g_loss: 2.6278 Training q_loss: 71.5911 Explore P: 0.0697\n",
      "Episode: 309 Total reward: 134.0 Training d_loss: 1.0977 Training g_loss: 3.2774 Training q_loss: 40.2348 Explore P: 0.0689\n",
      "Episode: 310 Total reward: 199.0 Training d_loss: 0.9813 Training g_loss: 5.2316 Training q_loss: 89.7530 Explore P: 0.0677\n",
      "Episode: 311 Total reward: 179.0 Training d_loss: 1.4290 Training g_loss: 0.7983 Training q_loss: 98.1856 Explore P: 0.0667\n",
      "Episode: 312 Total reward: 199.0 Training d_loss: 1.0148 Training g_loss: 7.0026 Training q_loss: 146.7687 Explore P: 0.0656\n",
      "Episode: 313 Total reward: 199.0 Training d_loss: 1.0853 Training g_loss: 4.0279 Training q_loss: 32.2559 Explore P: 0.0645\n",
      "Episode: 314 Total reward: 199.0 Training d_loss: 1.1736 Training g_loss: 3.8810 Training q_loss: 103.9404 Explore P: 0.0634\n",
      "Episode: 315 Total reward: 199.0 Training d_loss: 1.0558 Training g_loss: 4.7184 Training q_loss: 43.9718 Explore P: 0.0624\n",
      "Episode: 316 Total reward: 199.0 Training d_loss: 1.1004 Training g_loss: 1.5907 Training q_loss: 48.1387 Explore P: 0.0613\n",
      "Episode: 317 Total reward: 199.0 Training d_loss: 1.2006 Training g_loss: 0.8978 Training q_loss: 119.8584 Explore P: 0.0603\n",
      "Episode: 318 Total reward: 199.0 Training d_loss: 1.1115 Training g_loss: 4.3699 Training q_loss: 61.3307 Explore P: 0.0593\n",
      "Episode: 319 Total reward: 199.0 Training d_loss: 1.2996 Training g_loss: 2.2354 Training q_loss: 3266.3706 Explore P: 0.0584\n",
      "Episode: 320 Total reward: 199.0 Training d_loss: 1.0609 Training g_loss: 4.1761 Training q_loss: 170.9713 Explore P: 0.0574\n",
      "Episode: 321 Total reward: 199.0 Training d_loss: 0.9442 Training g_loss: 5.0170 Training q_loss: 27.4576 Explore P: 0.0565\n",
      "Episode: 322 Total reward: 199.0 Training d_loss: 1.0625 Training g_loss: 3.0304 Training q_loss: 1156.5969 Explore P: 0.0556\n",
      "Episode: 323 Total reward: 199.0 Training d_loss: 1.2249 Training g_loss: 1.1179 Training q_loss: 71.8776 Explore P: 0.0547\n",
      "Episode: 324 Total reward: 199.0 Training d_loss: 1.2462 Training g_loss: 2.4568 Training q_loss: 49.9556 Explore P: 0.0538\n",
      "Episode: 325 Total reward: 199.0 Training d_loss: 1.3746 Training g_loss: 0.6863 Training q_loss: 84.2247 Explore P: 0.0529\n",
      "Episode: 326 Total reward: 199.0 Training d_loss: 1.3043 Training g_loss: 1.8360 Training q_loss: 70.3771 Explore P: 0.0521\n",
      "Episode: 327 Total reward: 199.0 Training d_loss: 1.2850 Training g_loss: 2.4537 Training q_loss: 8.9469 Explore P: 0.0513\n",
      "Episode: 328 Total reward: 199.0 Training d_loss: 1.1953 Training g_loss: 4.4032 Training q_loss: 15.2018 Explore P: 0.0504\n",
      "Episode: 329 Total reward: 199.0 Training d_loss: 1.1379 Training g_loss: 4.3267 Training q_loss: 108.4467 Explore P: 0.0496\n",
      "Episode: 330 Total reward: 199.0 Training d_loss: 1.2381 Training g_loss: 4.5268 Training q_loss: 44.8261 Explore P: 0.0489\n",
      "Episode: 331 Total reward: 199.0 Training d_loss: 1.1376 Training g_loss: 3.2003 Training q_loss: 18.0108 Explore P: 0.0481\n",
      "Episode: 332 Total reward: 199.0 Training d_loss: 1.2563 Training g_loss: 2.7630 Training q_loss: 3.1133 Explore P: 0.0473\n",
      "Episode: 333 Total reward: 199.0 Training d_loss: 1.4243 Training g_loss: 0.5624 Training q_loss: 26.7476 Explore P: 0.0466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 334 Total reward: 199.0 Training d_loss: 1.3016 Training g_loss: 1.2793 Training q_loss: 41.8432 Explore P: 0.0459\n",
      "Episode: 335 Total reward: 199.0 Training d_loss: 1.3295 Training g_loss: 2.8204 Training q_loss: 724.0529 Explore P: 0.0452\n",
      "Episode: 336 Total reward: 199.0 Training d_loss: 1.2822 Training g_loss: 4.2679 Training q_loss: 19.2509 Explore P: 0.0445\n",
      "Episode: 337 Total reward: 199.0 Training d_loss: 1.3504 Training g_loss: 2.6535 Training q_loss: 2336.6318 Explore P: 0.0438\n",
      "Episode: 338 Total reward: 199.0 Training d_loss: 1.1792 Training g_loss: 3.1269 Training q_loss: 78.1063 Explore P: 0.0431\n",
      "Episode: 339 Total reward: 199.0 Training d_loss: 0.9870 Training g_loss: 3.2095 Training q_loss: 26.5622 Explore P: 0.0425\n",
      "Episode: 340 Total reward: 199.0 Training d_loss: 1.3196 Training g_loss: 1.0054 Training q_loss: 85.4963 Explore P: 0.0418\n",
      "Episode: 341 Total reward: 199.0 Training d_loss: 1.3155 Training g_loss: 1.6400 Training q_loss: 1823.3105 Explore P: 0.0412\n",
      "Episode: 342 Total reward: 199.0 Training d_loss: 1.2393 Training g_loss: 3.0094 Training q_loss: 78.8759 Explore P: 0.0406\n",
      "Episode: 343 Total reward: 199.0 Training d_loss: 1.4362 Training g_loss: 0.4641 Training q_loss: 93.4839 Explore P: 0.0400\n",
      "Episode: 344 Total reward: 199.0 Training d_loss: 1.2432 Training g_loss: 3.1192 Training q_loss: 12.6371 Explore P: 0.0394\n",
      "Episode: 345 Total reward: 199.0 Training d_loss: 1.3564 Training g_loss: 1.8135 Training q_loss: 13.1248 Explore P: 0.0388\n",
      "Episode: 346 Total reward: 199.0 Training d_loss: 1.2290 Training g_loss: 3.5586 Training q_loss: 17.4715 Explore P: 0.0383\n",
      "Episode: 347 Total reward: 199.0 Training d_loss: 1.2143 Training g_loss: 1.3934 Training q_loss: 16.1011 Explore P: 0.0377\n",
      "Episode: 348 Total reward: 199.0 Training d_loss: 1.1102 Training g_loss: 7.9762 Training q_loss: 69.0220 Explore P: 0.0372\n",
      "Episode: 349 Total reward: 199.0 Training d_loss: 1.2481 Training g_loss: 5.5791 Training q_loss: 48.7653 Explore P: 0.0366\n",
      "Episode: 350 Total reward: 199.0 Training d_loss: 1.3027 Training g_loss: 3.0402 Training q_loss: 74.4801 Explore P: 0.0361\n",
      "Episode: 351 Total reward: 199.0 Training d_loss: 1.3930 Training g_loss: 0.6343 Training q_loss: 78.4773 Explore P: 0.0356\n",
      "Episode: 352 Total reward: 199.0 Training d_loss: 1.2712 Training g_loss: 2.3505 Training q_loss: 659.3033 Explore P: 0.0351\n",
      "Episode: 353 Total reward: 199.0 Training d_loss: 1.2745 Training g_loss: 1.1551 Training q_loss: 14.4548 Explore P: 0.0346\n",
      "Episode: 354 Total reward: 199.0 Training d_loss: 1.3059 Training g_loss: 3.1206 Training q_loss: 11.5729 Explore P: 0.0341\n",
      "Episode: 355 Total reward: 147.0 Training d_loss: 1.3357 Training g_loss: 3.2405 Training q_loss: 40.3065 Explore P: 0.0338\n",
      "Episode: 356 Total reward: 199.0 Training d_loss: 1.3730 Training g_loss: 0.7076 Training q_loss: 31.9239 Explore P: 0.0333\n",
      "Episode: 357 Total reward: 199.0 Training d_loss: 1.3917 Training g_loss: 0.7008 Training q_loss: 9.3450 Explore P: 0.0328\n",
      "Episode: 358 Total reward: 199.0 Training d_loss: 1.2994 Training g_loss: 3.3983 Training q_loss: 22.1914 Explore P: 0.0324\n",
      "Episode: 359 Total reward: 176.0 Training d_loss: 1.2627 Training g_loss: 0.8673 Training q_loss: 9.0363 Explore P: 0.0320\n",
      "Episode: 360 Total reward: 175.0 Training d_loss: 1.3307 Training g_loss: 3.4137 Training q_loss: 18.5922 Explore P: 0.0316\n",
      "Episode: 361 Total reward: 199.0 Training d_loss: 1.2717 Training g_loss: 3.2907 Training q_loss: 770.9215 Explore P: 0.0312\n",
      "Episode: 362 Total reward: 199.0 Training d_loss: 1.3518 Training g_loss: 0.8090 Training q_loss: 12.8886 Explore P: 0.0308\n",
      "Episode: 363 Total reward: 190.0 Training d_loss: 1.3897 Training g_loss: 0.6496 Training q_loss: 6.8286 Explore P: 0.0304\n",
      "Episode: 364 Total reward: 199.0 Training d_loss: 1.3989 Training g_loss: 0.5947 Training q_loss: 49.1839 Explore P: 0.0300\n",
      "Episode: 365 Total reward: 199.0 Training d_loss: 1.1554 Training g_loss: 3.7111 Training q_loss: 34.2655 Explore P: 0.0296\n",
      "Episode: 366 Total reward: 199.0 Training d_loss: 1.3613 Training g_loss: 0.6812 Training q_loss: 20.2997 Explore P: 0.0292\n",
      "Episode: 367 Total reward: 199.0 Training d_loss: 1.2601 Training g_loss: 1.2680 Training q_loss: 17.5787 Explore P: 0.0288\n",
      "Episode: 368 Total reward: 184.0 Training d_loss: 1.3157 Training g_loss: 3.0912 Training q_loss: 22.1068 Explore P: 0.0285\n",
      "Episode: 369 Total reward: 199.0 Training d_loss: 1.2539 Training g_loss: 1.0099 Training q_loss: 12.7310 Explore P: 0.0281\n",
      "Episode: 370 Total reward: 199.0 Training d_loss: 1.1631 Training g_loss: 4.1657 Training q_loss: 10.7600 Explore P: 0.0277\n",
      "Episode: 371 Total reward: 199.0 Training d_loss: 1.3960 Training g_loss: 0.6441 Training q_loss: 571.8525 Explore P: 0.0274\n",
      "Episode: 372 Total reward: 199.0 Training d_loss: 1.3096 Training g_loss: 3.1319 Training q_loss: 6.7072 Explore P: 0.0271\n",
      "Episode: 373 Total reward: 199.0 Training d_loss: 1.3117 Training g_loss: 1.2634 Training q_loss: 12.5957 Explore P: 0.0267\n",
      "Episode: 374 Total reward: 199.0 Training d_loss: 1.2130 Training g_loss: 6.4328 Training q_loss: 8.5412 Explore P: 0.0264\n",
      "Episode: 375 Total reward: 199.0 Training d_loss: 1.1657 Training g_loss: 4.4859 Training q_loss: 6.4227 Explore P: 0.0261\n",
      "Episode: 376 Total reward: 199.0 Training d_loss: 1.2387 Training g_loss: 4.7856 Training q_loss: 229.4934 Explore P: 0.0258\n",
      "Episode: 377 Total reward: 199.0 Training d_loss: 1.2529 Training g_loss: 0.9797 Training q_loss: 9.5363 Explore P: 0.0254\n",
      "Episode: 378 Total reward: 199.0 Training d_loss: 1.2216 Training g_loss: 6.4910 Training q_loss: 496.9298 Explore P: 0.0251\n",
      "Episode: 379 Total reward: 199.0 Training d_loss: 1.4073 Training g_loss: 0.6072 Training q_loss: 3.5506 Explore P: 0.0248\n",
      "Episode: 380 Total reward: 199.0 Training d_loss: 1.1857 Training g_loss: 4.6210 Training q_loss: 7.5666 Explore P: 0.0245\n",
      "Episode: 381 Total reward: 199.0 Training d_loss: 1.0498 Training g_loss: 7.2339 Training q_loss: 4.4611 Explore P: 0.0243\n",
      "Episode: 382 Total reward: 199.0 Training d_loss: 1.2514 Training g_loss: 1.8008 Training q_loss: 22.1733 Explore P: 0.0240\n",
      "Episode: 383 Total reward: 199.0 Training d_loss: 1.2569 Training g_loss: 4.4868 Training q_loss: 4.9505 Explore P: 0.0237\n",
      "Episode: 384 Total reward: 199.0 Training d_loss: 1.1811 Training g_loss: 10.2639 Training q_loss: 45.3214 Explore P: 0.0234\n",
      "Episode: 385 Total reward: 199.0 Training d_loss: 1.3156 Training g_loss: 3.7425 Training q_loss: 5.4080 Explore P: 0.0232\n",
      "Episode: 386 Total reward: 199.0 Training d_loss: 1.2159 Training g_loss: 1.1315 Training q_loss: 4.1550 Explore P: 0.0229\n",
      "Episode: 387 Total reward: 199.0 Training d_loss: 1.3927 Training g_loss: 0.6315 Training q_loss: 24.2454 Explore P: 0.0227\n",
      "Episode: 388 Total reward: 199.0 Training d_loss: 1.4092 Training g_loss: 0.5940 Training q_loss: 6.8807 Explore P: 0.0224\n",
      "Episode: 389 Total reward: 199.0 Training d_loss: 1.2522 Training g_loss: 4.1584 Training q_loss: 8.5433 Explore P: 0.0222\n",
      "Episode: 390 Total reward: 199.0 Training d_loss: 1.1895 Training g_loss: 6.7957 Training q_loss: 13.6311 Explore P: 0.0219\n",
      "Episode: 391 Total reward: 199.0 Training d_loss: 1.3667 Training g_loss: 0.7428 Training q_loss: 19.9600 Explore P: 0.0217\n",
      "Episode: 392 Total reward: 199.0 Training d_loss: 1.2855 Training g_loss: 2.3206 Training q_loss: 5.7110 Explore P: 0.0215\n",
      "Episode: 393 Total reward: 171.0 Training d_loss: 1.1639 Training g_loss: 3.4568 Training q_loss: 8.6890 Explore P: 0.0213\n",
      "Episode: 394 Total reward: 186.0 Training d_loss: 1.2576 Training g_loss: 3.9472 Training q_loss: 22.8908 Explore P: 0.0211\n",
      "Episode: 395 Total reward: 179.0 Training d_loss: 1.4755 Training g_loss: 0.6112 Training q_loss: 91.1883 Explore P: 0.0209\n",
      "Episode: 396 Total reward: 164.0 Training d_loss: 1.3293 Training g_loss: 0.9373 Training q_loss: 3.2150 Explore P: 0.0207\n",
      "Episode: 397 Total reward: 198.0 Training d_loss: 1.3392 Training g_loss: 4.2426 Training q_loss: 78.0133 Explore P: 0.0205\n",
      "Episode: 398 Total reward: 199.0 Training d_loss: 1.3375 Training g_loss: 0.6744 Training q_loss: 4.9304 Explore P: 0.0203\n",
      "Episode: 399 Total reward: 199.0 Training d_loss: 1.2727 Training g_loss: 7.5499 Training q_loss: 4.3466 Explore P: 0.0201\n",
      "Episode: 400 Total reward: 166.0 Training d_loss: 1.1781 Training g_loss: 2.6727 Training q_loss: 5.8655 Explore P: 0.0199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 401 Total reward: 161.0 Training d_loss: 1.0807 Training g_loss: 2.8373 Training q_loss: 1.8405 Explore P: 0.0197\n",
      "Episode: 402 Total reward: 199.0 Training d_loss: 1.3615 Training g_loss: 0.7670 Training q_loss: 3.3960 Explore P: 0.0195\n",
      "Episode: 403 Total reward: 199.0 Training d_loss: 1.3311 Training g_loss: 0.7843 Training q_loss: 7.8832 Explore P: 0.0194\n",
      "Episode: 404 Total reward: 151.0 Training d_loss: 1.3084 Training g_loss: 0.8694 Training q_loss: 20.0971 Explore P: 0.0192\n",
      "Episode: 405 Total reward: 151.0 Training d_loss: 1.1054 Training g_loss: 1.4140 Training q_loss: 5.6591 Explore P: 0.0191\n",
      "Episode: 406 Total reward: 128.0 Training d_loss: 1.0803 Training g_loss: 5.3300 Training q_loss: 7.7167 Explore P: 0.0190\n",
      "Episode: 407 Total reward: 151.0 Training d_loss: 1.3683 Training g_loss: 1.0054 Training q_loss: 8.3113 Explore P: 0.0188\n",
      "Episode: 408 Total reward: 123.0 Training d_loss: 1.2129 Training g_loss: 0.7252 Training q_loss: 20.0816 Explore P: 0.0187\n",
      "Episode: 409 Total reward: 128.0 Training d_loss: 1.2343 Training g_loss: 4.7226 Training q_loss: 39.8168 Explore P: 0.0186\n",
      "Episode: 410 Total reward: 119.0 Training d_loss: 1.2687 Training g_loss: 4.4195 Training q_loss: 6.7191 Explore P: 0.0185\n",
      "Episode: 411 Total reward: 131.0 Training d_loss: 1.3465 Training g_loss: 0.7569 Training q_loss: 24.1067 Explore P: 0.0184\n",
      "Episode: 412 Total reward: 115.0 Training d_loss: 1.1033 Training g_loss: 7.5625 Training q_loss: 6.6175 Explore P: 0.0183\n",
      "Episode: 413 Total reward: 127.0 Training d_loss: 1.3398 Training g_loss: 1.6873 Training q_loss: 4.3700 Explore P: 0.0182\n",
      "Episode: 414 Total reward: 123.0 Training d_loss: 1.1614 Training g_loss: 4.8818 Training q_loss: 14.9224 Explore P: 0.0181\n",
      "Episode: 415 Total reward: 121.0 Training d_loss: 1.4338 Training g_loss: 0.8364 Training q_loss: 28.2425 Explore P: 0.0180\n",
      "Episode: 416 Total reward: 139.0 Training d_loss: 1.1547 Training g_loss: 1.2597 Training q_loss: 30.7813 Explore P: 0.0179\n",
      "Episode: 417 Total reward: 159.0 Training d_loss: 1.1717 Training g_loss: 4.6875 Training q_loss: 27.8776 Explore P: 0.0178\n",
      "Episode: 418 Total reward: 142.0 Training d_loss: 1.4045 Training g_loss: 0.8127 Training q_loss: 76.4899 Explore P: 0.0177\n",
      "Episode: 419 Total reward: 137.0 Training d_loss: 1.1428 Training g_loss: 3.7816 Training q_loss: 8.6019 Explore P: 0.0176\n",
      "Episode: 420 Total reward: 126.0 Training d_loss: 1.3168 Training g_loss: 0.7425 Training q_loss: 12.5874 Explore P: 0.0175\n",
      "Episode: 421 Total reward: 130.0 Training d_loss: 1.1367 Training g_loss: 8.9277 Training q_loss: 1954.3528 Explore P: 0.0174\n",
      "Episode: 422 Total reward: 143.0 Training d_loss: 1.2724 Training g_loss: 1.1987 Training q_loss: 36.7294 Explore P: 0.0173\n",
      "Episode: 423 Total reward: 142.0 Training d_loss: 1.4234 Training g_loss: 0.4812 Training q_loss: 13.0808 Explore P: 0.0172\n",
      "Episode: 424 Total reward: 138.0 Training d_loss: 1.2525 Training g_loss: 4.9028 Training q_loss: 25.9130 Explore P: 0.0171\n",
      "Episode: 425 Total reward: 129.0 Training d_loss: 1.3769 Training g_loss: 0.8411 Training q_loss: 9.9526 Explore P: 0.0170\n",
      "Episode: 426 Total reward: 121.0 Training d_loss: 1.2905 Training g_loss: 0.8096 Training q_loss: 16.6077 Explore P: 0.0169\n",
      "Episode: 427 Total reward: 144.0 Training d_loss: 1.1706 Training g_loss: 7.4705 Training q_loss: 20.6369 Explore P: 0.0168\n",
      "Episode: 428 Total reward: 130.0 Training d_loss: 1.1464 Training g_loss: 1.7784 Training q_loss: 28.4986 Explore P: 0.0167\n",
      "Episode: 429 Total reward: 127.0 Training d_loss: 1.3917 Training g_loss: 0.6984 Training q_loss: 4.2001 Explore P: 0.0166\n",
      "Episode: 430 Total reward: 137.0 Training d_loss: 1.1954 Training g_loss: 7.2606 Training q_loss: 13.2232 Explore P: 0.0165\n",
      "Episode: 431 Total reward: 167.0 Training d_loss: 1.3505 Training g_loss: 0.7235 Training q_loss: 20.6944 Explore P: 0.0164\n",
      "Episode: 432 Total reward: 142.0 Training d_loss: 1.1956 Training g_loss: 5.5859 Training q_loss: 93.6435 Explore P: 0.0163\n",
      "Episode: 433 Total reward: 150.0 Training d_loss: 1.3420 Training g_loss: 0.7619 Training q_loss: 21.1520 Explore P: 0.0162\n",
      "Episode: 434 Total reward: 177.0 Training d_loss: 1.2889 Training g_loss: 4.3398 Training q_loss: 13.9075 Explore P: 0.0161\n",
      "Episode: 435 Total reward: 123.0 Training d_loss: 1.2719 Training g_loss: 4.9028 Training q_loss: 12.9184 Explore P: 0.0160\n",
      "Episode: 436 Total reward: 145.0 Training d_loss: 1.4041 Training g_loss: 0.5417 Training q_loss: 3.9414 Explore P: 0.0160\n",
      "Episode: 437 Total reward: 131.0 Training d_loss: 1.3207 Training g_loss: 2.6032 Training q_loss: 4.4835 Explore P: 0.0159\n",
      "Episode: 438 Total reward: 150.0 Training d_loss: 1.3559 Training g_loss: 0.6821 Training q_loss: 11.7896 Explore P: 0.0158\n",
      "Episode: 439 Total reward: 180.0 Training d_loss: 1.0584 Training g_loss: 5.2343 Training q_loss: 11.8106 Explore P: 0.0157\n",
      "Episode: 440 Total reward: 160.0 Training d_loss: 1.2517 Training g_loss: 3.3545 Training q_loss: 34.4492 Explore P: 0.0156\n",
      "Episode: 441 Total reward: 133.0 Training d_loss: 1.2963 Training g_loss: 3.8807 Training q_loss: 9.6766 Explore P: 0.0155\n",
      "Episode: 442 Total reward: 140.0 Training d_loss: 1.3639 Training g_loss: 0.7753 Training q_loss: 21.7070 Explore P: 0.0154\n",
      "Episode: 443 Total reward: 127.0 Training d_loss: 1.3095 Training g_loss: 5.0073 Training q_loss: 325.1190 Explore P: 0.0154\n",
      "Episode: 444 Total reward: 148.0 Training d_loss: 1.3028 Training g_loss: 0.8312 Training q_loss: 13.8155 Explore P: 0.0153\n",
      "Episode: 445 Total reward: 127.0 Training d_loss: 1.2551 Training g_loss: 5.1423 Training q_loss: 17.0011 Explore P: 0.0152\n",
      "Episode: 446 Total reward: 146.0 Training d_loss: 1.3453 Training g_loss: 0.7114 Training q_loss: 3.5340 Explore P: 0.0152\n",
      "Episode: 447 Total reward: 149.0 Training d_loss: 1.3026 Training g_loss: 1.5232 Training q_loss: 105.0883 Explore P: 0.0151\n",
      "Episode: 448 Total reward: 137.0 Training d_loss: 1.4369 Training g_loss: 0.7668 Training q_loss: 2.6120 Explore P: 0.0150\n",
      "Episode: 449 Total reward: 133.0 Training d_loss: 1.2366 Training g_loss: 6.1679 Training q_loss: 12.8607 Explore P: 0.0149\n",
      "Episode: 450 Total reward: 138.0 Training d_loss: 1.1212 Training g_loss: 12.3579 Training q_loss: 5.3142 Explore P: 0.0149\n",
      "Episode: 451 Total reward: 130.0 Training d_loss: 1.3688 Training g_loss: 0.6529 Training q_loss: 10.7429 Explore P: 0.0148\n",
      "Episode: 452 Total reward: 142.0 Training d_loss: 1.3045 Training g_loss: 1.2039 Training q_loss: 2.3530 Explore P: 0.0147\n",
      "Episode: 453 Total reward: 116.0 Training d_loss: 1.1664 Training g_loss: 8.8227 Training q_loss: 10.0747 Explore P: 0.0147\n",
      "Episode: 454 Total reward: 149.0 Training d_loss: 1.1052 Training g_loss: 9.6247 Training q_loss: 6.4926 Explore P: 0.0146\n",
      "Episode: 455 Total reward: 116.0 Training d_loss: 1.1649 Training g_loss: 9.4162 Training q_loss: 18.8208 Explore P: 0.0146\n",
      "Episode: 456 Total reward: 128.0 Training d_loss: 1.4264 Training g_loss: 0.5773 Training q_loss: 20.5846 Explore P: 0.0145\n",
      "Episode: 457 Total reward: 122.0 Training d_loss: 1.2794 Training g_loss: 2.8534 Training q_loss: 4.3105 Explore P: 0.0145\n",
      "Episode: 458 Total reward: 112.0 Training d_loss: 1.3974 Training g_loss: 0.5580 Training q_loss: 4.8538 Explore P: 0.0144\n",
      "Episode: 459 Total reward: 122.0 Training d_loss: 1.2155 Training g_loss: 4.9914 Training q_loss: 26.7043 Explore P: 0.0144\n",
      "Episode: 460 Total reward: 120.0 Training d_loss: 1.3120 Training g_loss: 0.8849 Training q_loss: 4.5052 Explore P: 0.0143\n",
      "Episode: 461 Total reward: 111.0 Training d_loss: 1.3715 Training g_loss: 0.6773 Training q_loss: 26.1849 Explore P: 0.0143\n",
      "Episode: 462 Total reward: 101.0 Training d_loss: 1.3920 Training g_loss: 0.6084 Training q_loss: 9.0539 Explore P: 0.0142\n",
      "Episode: 463 Total reward: 148.0 Training d_loss: 1.1567 Training g_loss: 8.4551 Training q_loss: 27.6054 Explore P: 0.0142\n",
      "Episode: 464 Total reward: 108.0 Training d_loss: 1.2914 Training g_loss: 1.6608 Training q_loss: 11.2169 Explore P: 0.0141\n",
      "Episode: 465 Total reward: 113.0 Training d_loss: 1.2919 Training g_loss: 0.9830 Training q_loss: 14.0595 Explore P: 0.0141\n",
      "Episode: 466 Total reward: 134.0 Training d_loss: 1.2119 Training g_loss: 8.7070 Training q_loss: 13.6129 Explore P: 0.0140\n",
      "Episode: 467 Total reward: 115.0 Training d_loss: 0.9657 Training g_loss: 9.6168 Training q_loss: 6.8716 Explore P: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 468 Total reward: 135.0 Training d_loss: 1.2126 Training g_loss: 4.5532 Training q_loss: 24.9202 Explore P: 0.0139\n",
      "Episode: 469 Total reward: 115.0 Training d_loss: 1.4203 Training g_loss: 0.6240 Training q_loss: 4.7768 Explore P: 0.0139\n",
      "Episode: 470 Total reward: 119.0 Training d_loss: 1.2925 Training g_loss: 2.5217 Training q_loss: 25.4296 Explore P: 0.0138\n",
      "Episode: 471 Total reward: 100.0 Training d_loss: 1.1238 Training g_loss: 9.5794 Training q_loss: 3.0434 Explore P: 0.0138\n",
      "Episode: 472 Total reward: 117.0 Training d_loss: 1.3645 Training g_loss: 2.9144 Training q_loss: 19.7103 Explore P: 0.0137\n",
      "Episode: 473 Total reward: 118.0 Training d_loss: 1.1328 Training g_loss: 2.2968 Training q_loss: 23.3469 Explore P: 0.0137\n",
      "Episode: 474 Total reward: 99.0 Training d_loss: 1.0677 Training g_loss: 10.4918 Training q_loss: 12.2072 Explore P: 0.0137\n",
      "Episode: 475 Total reward: 121.0 Training d_loss: 1.3005 Training g_loss: 5.4355 Training q_loss: 19.0380 Explore P: 0.0136\n",
      "Episode: 476 Total reward: 130.0 Training d_loss: 1.2744 Training g_loss: 2.9733 Training q_loss: 17.1438 Explore P: 0.0136\n",
      "Episode: 477 Total reward: 107.0 Training d_loss: 1.2911 Training g_loss: 2.3009 Training q_loss: 20.4710 Explore P: 0.0135\n",
      "Episode: 478 Total reward: 105.0 Training d_loss: 1.3634 Training g_loss: 0.7543 Training q_loss: 15.9548 Explore P: 0.0135\n",
      "Episode: 479 Total reward: 115.0 Training d_loss: 1.1458 Training g_loss: 6.4964 Training q_loss: 16.3755 Explore P: 0.0134\n",
      "Episode: 480 Total reward: 123.0 Training d_loss: 1.2887 Training g_loss: 6.3591 Training q_loss: 3.5906 Explore P: 0.0134\n",
      "Episode: 481 Total reward: 97.0 Training d_loss: 1.2924 Training g_loss: 0.9888 Training q_loss: 105.7801 Explore P: 0.0134\n",
      "Episode: 482 Total reward: 113.0 Training d_loss: 1.3864 Training g_loss: 8.3758 Training q_loss: 113.4552 Explore P: 0.0133\n",
      "Episode: 483 Total reward: 75.0 Training d_loss: 1.1125 Training g_loss: 3.9164 Training q_loss: 8.7241 Explore P: 0.0133\n",
      "Episode: 484 Total reward: 57.0 Training d_loss: 0.9039 Training g_loss: 13.3780 Training q_loss: 19.0337 Explore P: 0.0133\n",
      "Episode: 485 Total reward: 105.0 Training d_loss: 1.1125 Training g_loss: 5.6733 Training q_loss: 11.2243 Explore P: 0.0133\n",
      "Episode: 486 Total reward: 51.0 Training d_loss: 1.1844 Training g_loss: 5.3652 Training q_loss: 4.8904 Explore P: 0.0132\n",
      "Episode: 487 Total reward: 73.0 Training d_loss: 1.3361 Training g_loss: 0.9970 Training q_loss: 61.2717 Explore P: 0.0132\n",
      "Episode: 488 Total reward: 74.0 Training d_loss: 1.3357 Training g_loss: 0.8725 Training q_loss: 53.4531 Explore P: 0.0132\n",
      "Episode: 489 Total reward: 79.0 Training d_loss: 1.2704 Training g_loss: 0.9761 Training q_loss: 58.0651 Explore P: 0.0132\n",
      "Episode: 490 Total reward: 59.0 Training d_loss: 1.1085 Training g_loss: 6.3208 Training q_loss: 39.1967 Explore P: 0.0132\n",
      "Episode: 491 Total reward: 59.0 Training d_loss: 1.3318 Training g_loss: 0.6497 Training q_loss: 24.6316 Explore P: 0.0131\n",
      "Episode: 492 Total reward: 50.0 Training d_loss: 1.1462 Training g_loss: 0.7195 Training q_loss: 22.5772 Explore P: 0.0131\n",
      "Episode: 493 Total reward: 46.0 Training d_loss: 0.9186 Training g_loss: 5.3882 Training q_loss: 14.2307 Explore P: 0.0131\n",
      "Episode: 494 Total reward: 45.0 Training d_loss: 1.0395 Training g_loss: 8.4698 Training q_loss: 1137.1329 Explore P: 0.0131\n",
      "Episode: 495 Total reward: 60.0 Training d_loss: 1.4059 Training g_loss: 0.5772 Training q_loss: 1270.6021 Explore P: 0.0131\n",
      "Episode: 496 Total reward: 54.0 Training d_loss: 0.9876 Training g_loss: 13.4235 Training q_loss: 10.4611 Explore P: 0.0131\n",
      "Episode: 497 Total reward: 44.0 Training d_loss: 1.2130 Training g_loss: 1.3348 Training q_loss: 460.6667 Explore P: 0.0130\n",
      "Episode: 498 Total reward: 48.0 Training d_loss: 1.0638 Training g_loss: 11.6031 Training q_loss: 39.3188 Explore P: 0.0130\n",
      "Episode: 499 Total reward: 47.0 Training d_loss: 1.2476 Training g_loss: 0.7410 Training q_loss: 122.6508 Explore P: 0.0130\n",
      "Episode: 500 Total reward: 43.0 Training d_loss: 1.1082 Training g_loss: 1.4474 Training q_loss: 975.2106 Explore P: 0.0130\n",
      "Episode: 501 Total reward: 45.0 Training d_loss: 0.7755 Training g_loss: 5.5185 Training q_loss: 41.3781 Explore P: 0.0130\n",
      "Episode: 502 Total reward: 44.0 Training d_loss: 0.9803 Training g_loss: 5.4461 Training q_loss: 41.8447 Explore P: 0.0130\n",
      "Episode: 503 Total reward: 52.0 Training d_loss: 1.2255 Training g_loss: 5.3496 Training q_loss: 11.5012 Explore P: 0.0130\n",
      "Episode: 504 Total reward: 45.0 Training d_loss: 1.3070 Training g_loss: 0.9372 Training q_loss: 15.6755 Explore P: 0.0129\n",
      "Episode: 505 Total reward: 47.0 Training d_loss: 1.1356 Training g_loss: 1.4218 Training q_loss: 29.4627 Explore P: 0.0129\n",
      "Episode: 506 Total reward: 52.0 Training d_loss: 1.3286 Training g_loss: 0.6123 Training q_loss: 100.7084 Explore P: 0.0129\n",
      "Episode: 507 Total reward: 42.0 Training d_loss: 1.4679 Training g_loss: 0.9799 Training q_loss: 24.7836 Explore P: 0.0129\n",
      "Episode: 508 Total reward: 54.0 Training d_loss: 1.0958 Training g_loss: 1.1744 Training q_loss: 48.8442 Explore P: 0.0129\n",
      "Episode: 509 Total reward: 44.0 Training d_loss: 1.3861 Training g_loss: 0.5510 Training q_loss: 32.2174 Explore P: 0.0129\n",
      "Episode: 510 Total reward: 45.0 Training d_loss: 1.2519 Training g_loss: 1.3333 Training q_loss: 12.9739 Explore P: 0.0129\n",
      "Episode: 511 Total reward: 39.0 Training d_loss: 1.3919 Training g_loss: 1.0665 Training q_loss: 31.0995 Explore P: 0.0128\n",
      "Episode: 512 Total reward: 36.0 Training d_loss: 1.1074 Training g_loss: 0.9144 Training q_loss: 31.8783 Explore P: 0.0128\n",
      "Episode: 513 Total reward: 41.0 Training d_loss: 1.2727 Training g_loss: 5.5110 Training q_loss: 57.0053 Explore P: 0.0128\n",
      "Episode: 514 Total reward: 84.0 Training d_loss: 1.1799 Training g_loss: 9.3221 Training q_loss: 55.5737 Explore P: 0.0128\n",
      "Episode: 515 Total reward: 92.0 Training d_loss: 1.4635 Training g_loss: 0.5557 Training q_loss: 32.7940 Explore P: 0.0128\n",
      "Episode: 516 Total reward: 122.0 Training d_loss: 1.2865 Training g_loss: 2.1885 Training q_loss: 8.8710 Explore P: 0.0127\n",
      "Episode: 517 Total reward: 114.0 Training d_loss: 1.2168 Training g_loss: 6.2996 Training q_loss: 27.1750 Explore P: 0.0127\n",
      "Episode: 518 Total reward: 128.0 Training d_loss: 1.2218 Training g_loss: 5.9195 Training q_loss: 29.4164 Explore P: 0.0127\n",
      "Episode: 519 Total reward: 192.0 Training d_loss: 1.3214 Training g_loss: 3.0550 Training q_loss: 6.2212 Explore P: 0.0126\n",
      "Episode: 520 Total reward: 141.0 Training d_loss: 1.3813 Training g_loss: 0.5542 Training q_loss: 28.1744 Explore P: 0.0126\n",
      "Episode: 521 Total reward: 199.0 Training d_loss: 1.0971 Training g_loss: 10.8427 Training q_loss: 24.1008 Explore P: 0.0125\n",
      "Episode: 522 Total reward: 197.0 Training d_loss: 1.1529 Training g_loss: 8.5816 Training q_loss: 16.8042 Explore P: 0.0125\n",
      "Episode: 523 Total reward: 199.0 Training d_loss: 1.3754 Training g_loss: 0.6692 Training q_loss: 23.5822 Explore P: 0.0124\n",
      "Episode: 524 Total reward: 199.0 Training d_loss: 1.2968 Training g_loss: 0.7369 Training q_loss: 25.0364 Explore P: 0.0124\n",
      "Episode: 525 Total reward: 171.0 Training d_loss: 1.4141 Training g_loss: 0.5668 Training q_loss: 33.8097 Explore P: 0.0124\n",
      "Episode: 526 Total reward: 175.0 Training d_loss: 1.3062 Training g_loss: 1.9293 Training q_loss: 101.2316 Explore P: 0.0123\n",
      "Episode: 527 Total reward: 199.0 Training d_loss: 1.2981 Training g_loss: 13.2517 Training q_loss: 43.1083 Explore P: 0.0123\n",
      "Episode: 528 Total reward: 199.0 Training d_loss: 1.3319 Training g_loss: 8.7403 Training q_loss: 13.0501 Explore P: 0.0122\n",
      "Episode: 529 Total reward: 129.0 Training d_loss: 1.1594 Training g_loss: 4.6508 Training q_loss: 21.5566 Explore P: 0.0122\n",
      "Episode: 530 Total reward: 176.0 Training d_loss: 1.0137 Training g_loss: 9.6532 Training q_loss: 19.1273 Explore P: 0.0122\n",
      "Episode: 531 Total reward: 149.0 Training d_loss: 1.3494 Training g_loss: 1.3922 Training q_loss: 35.6739 Explore P: 0.0121\n",
      "Episode: 532 Total reward: 199.0 Training d_loss: 1.3871 Training g_loss: 0.6022 Training q_loss: 7.8008 Explore P: 0.0121\n",
      "Episode: 533 Total reward: 152.0 Training d_loss: 0.9895 Training g_loss: 5.5742 Training q_loss: 7.1094 Explore P: 0.0120\n",
      "Episode: 534 Total reward: 198.0 Training d_loss: 1.2226 Training g_loss: 3.9933 Training q_loss: 5.5525 Explore P: 0.0120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 535 Total reward: 199.0 Training d_loss: 0.8786 Training g_loss: 8.1927 Training q_loss: 12.0790 Explore P: 0.0120\n",
      "Episode: 536 Total reward: 199.0 Training d_loss: 1.2757 Training g_loss: 2.3152 Training q_loss: 122.5334 Explore P: 0.0119\n",
      "Episode: 537 Total reward: 184.0 Training d_loss: 1.3826 Training g_loss: 3.1549 Training q_loss: 69.7261 Explore P: 0.0119\n",
      "Episode: 538 Total reward: 199.0 Training d_loss: 1.1168 Training g_loss: 0.9713 Training q_loss: 53.3777 Explore P: 0.0119\n",
      "Episode: 539 Total reward: 171.0 Training d_loss: 1.2143 Training g_loss: 5.9100 Training q_loss: 13.2103 Explore P: 0.0118\n",
      "Episode: 540 Total reward: 199.0 Training d_loss: 1.3258 Training g_loss: 1.8552 Training q_loss: 50.9156 Explore P: 0.0118\n",
      "Episode: 541 Total reward: 199.0 Training d_loss: 1.2100 Training g_loss: 9.8787 Training q_loss: 8.9153 Explore P: 0.0118\n",
      "Episode: 542 Total reward: 199.0 Training d_loss: 1.2780 Training g_loss: 5.7160 Training q_loss: 6.3378 Explore P: 0.0117\n",
      "Episode: 543 Total reward: 199.0 Training d_loss: 1.1732 Training g_loss: 10.9561 Training q_loss: 431.4537 Explore P: 0.0117\n",
      "Episode: 544 Total reward: 199.0 Training d_loss: 1.0121 Training g_loss: 9.6701 Training q_loss: 10.2919 Explore P: 0.0117\n",
      "Episode: 545 Total reward: 199.0 Training d_loss: 1.3187 Training g_loss: 1.0520 Training q_loss: 34.3038 Explore P: 0.0116\n",
      "Episode: 546 Total reward: 199.0 Training d_loss: 1.2793 Training g_loss: 4.2355 Training q_loss: 8.3295 Explore P: 0.0116\n",
      "Episode: 547 Total reward: 199.0 Training d_loss: 1.1977 Training g_loss: 1.9603 Training q_loss: 2.6651 Explore P: 0.0116\n",
      "Episode: 548 Total reward: 199.0 Training d_loss: 1.1868 Training g_loss: 0.9632 Training q_loss: 6.8007 Explore P: 0.0115\n",
      "Episode: 549 Total reward: 199.0 Training d_loss: 1.1997 Training g_loss: 1.3524 Training q_loss: 3.9310 Explore P: 0.0115\n",
      "Episode: 550 Total reward: 199.0 Training d_loss: 1.2192 Training g_loss: 9.4504 Training q_loss: 605.9868 Explore P: 0.0115\n",
      "Episode: 551 Total reward: 199.0 Training d_loss: 1.1514 Training g_loss: 2.2398 Training q_loss: 46.7091 Explore P: 0.0114\n",
      "Episode: 552 Total reward: 199.0 Training d_loss: 1.1957 Training g_loss: 1.6678 Training q_loss: 20.5473 Explore P: 0.0114\n",
      "Episode: 553 Total reward: 199.0 Training d_loss: 1.3424 Training g_loss: 1.0973 Training q_loss: 15.4174 Explore P: 0.0114\n",
      "Episode: 554 Total reward: 199.0 Training d_loss: 1.3094 Training g_loss: 5.4520 Training q_loss: 28.7501 Explore P: 0.0114\n",
      "Episode: 555 Total reward: 199.0 Training d_loss: 1.2438 Training g_loss: 5.6369 Training q_loss: 13.6328 Explore P: 0.0113\n",
      "Episode: 556 Total reward: 199.0 Training d_loss: 1.3112 Training g_loss: 5.8434 Training q_loss: 344.0888 Explore P: 0.0113\n",
      "Episode: 557 Total reward: 199.0 Training d_loss: 1.2247 Training g_loss: 6.0268 Training q_loss: 10.9464 Explore P: 0.0113\n",
      "Episode: 558 Total reward: 199.0 Training d_loss: 1.1978 Training g_loss: 5.8857 Training q_loss: 8.5448 Explore P: 0.0113\n",
      "Episode: 559 Total reward: 199.0 Training d_loss: 1.3813 Training g_loss: 0.6247 Training q_loss: 11.0761 Explore P: 0.0112\n",
      "Episode: 560 Total reward: 159.0 Training d_loss: 1.2508 Training g_loss: 1.9974 Training q_loss: 9.8892 Explore P: 0.0112\n",
      "Episode: 561 Total reward: 199.0 Training d_loss: 1.2526 Training g_loss: 1.7099 Training q_loss: 9.5222 Explore P: 0.0112\n",
      "Episode: 562 Total reward: 199.0 Training d_loss: 1.1636 Training g_loss: 9.0984 Training q_loss: 25.4913 Explore P: 0.0112\n",
      "Episode: 563 Total reward: 199.0 Training d_loss: 1.1346 Training g_loss: 11.2547 Training q_loss: 19.2484 Explore P: 0.0111\n",
      "Episode: 564 Total reward: 185.0 Training d_loss: 1.3932 Training g_loss: 0.5960 Training q_loss: 80.3913 Explore P: 0.0111\n",
      "Episode: 565 Total reward: 199.0 Training d_loss: 1.1439 Training g_loss: 6.6130 Training q_loss: 21.4397 Explore P: 0.0111\n",
      "Episode: 566 Total reward: 180.0 Training d_loss: 1.3761 Training g_loss: 0.5954 Training q_loss: 6.5178 Explore P: 0.0111\n",
      "Episode: 567 Total reward: 199.0 Training d_loss: 0.9503 Training g_loss: 7.2568 Training q_loss: 7.1863 Explore P: 0.0111\n",
      "Episode: 568 Total reward: 168.0 Training d_loss: 1.2525 Training g_loss: 0.8051 Training q_loss: 7.6099 Explore P: 0.0110\n",
      "Episode: 569 Total reward: 194.0 Training d_loss: 1.2045 Training g_loss: 4.4792 Training q_loss: 9.9730 Explore P: 0.0110\n",
      "Episode: 570 Total reward: 129.0 Training d_loss: 1.1775 Training g_loss: 6.8915 Training q_loss: 37.3982 Explore P: 0.0110\n",
      "Episode: 571 Total reward: 154.0 Training d_loss: 1.2424 Training g_loss: 1.1613 Training q_loss: 3.3621 Explore P: 0.0110\n",
      "Episode: 572 Total reward: 137.0 Training d_loss: 1.3229 Training g_loss: 0.9418 Training q_loss: 3.4612 Explore P: 0.0110\n",
      "Episode: 573 Total reward: 199.0 Training d_loss: 1.3439 Training g_loss: 0.6401 Training q_loss: 12.7770 Explore P: 0.0110\n",
      "Episode: 574 Total reward: 179.0 Training d_loss: 1.2875 Training g_loss: 6.8092 Training q_loss: 20.0809 Explore P: 0.0109\n",
      "Episode: 575 Total reward: 136.0 Training d_loss: 1.1206 Training g_loss: 3.8975 Training q_loss: 12.0215 Explore P: 0.0109\n",
      "Episode: 576 Total reward: 164.0 Training d_loss: 1.2853 Training g_loss: 1.2067 Training q_loss: 7.0271 Explore P: 0.0109\n",
      "Episode: 577 Total reward: 144.0 Training d_loss: 1.2557 Training g_loss: 3.2975 Training q_loss: 8.2653 Explore P: 0.0109\n",
      "Episode: 578 Total reward: 182.0 Training d_loss: 1.3815 Training g_loss: 0.6643 Training q_loss: 4.1228 Explore P: 0.0109\n",
      "Episode: 579 Total reward: 199.0 Training d_loss: 1.1665 Training g_loss: 2.0843 Training q_loss: 5.8828 Explore P: 0.0109\n",
      "Episode: 580 Total reward: 199.0 Training d_loss: 1.3106 Training g_loss: 1.5924 Training q_loss: 9.2519 Explore P: 0.0108\n",
      "Episode: 581 Total reward: 199.0 Training d_loss: 1.3113 Training g_loss: 0.7408 Training q_loss: 4.0826 Explore P: 0.0108\n",
      "Episode: 582 Total reward: 199.0 Training d_loss: 1.2805 Training g_loss: 4.8216 Training q_loss: 13.2122 Explore P: 0.0108\n",
      "Episode: 583 Total reward: 158.0 Training d_loss: 1.3197 Training g_loss: 0.8625 Training q_loss: 2.6375 Explore P: 0.0108\n",
      "Episode: 584 Total reward: 171.0 Training d_loss: 1.3021 Training g_loss: 0.6944 Training q_loss: 12.0657 Explore P: 0.0108\n",
      "Episode: 585 Total reward: 142.0 Training d_loss: 1.3382 Training g_loss: 0.8131 Training q_loss: 22.4534 Explore P: 0.0108\n",
      "Episode: 586 Total reward: 146.0 Training d_loss: 1.3770 Training g_loss: 0.6685 Training q_loss: 17.6245 Explore P: 0.0108\n",
      "Episode: 587 Total reward: 199.0 Training d_loss: 1.1891 Training g_loss: 7.5346 Training q_loss: 13.7527 Explore P: 0.0108\n",
      "Episode: 588 Total reward: 125.0 Training d_loss: 1.3162 Training g_loss: 1.2639 Training q_loss: 8.0945 Explore P: 0.0107\n",
      "Episode: 589 Total reward: 180.0 Training d_loss: 1.2413 Training g_loss: 0.9332 Training q_loss: 9.5789 Explore P: 0.0107\n",
      "Episode: 590 Total reward: 140.0 Training d_loss: 1.3878 Training g_loss: 0.8005 Training q_loss: 13.4593 Explore P: 0.0107\n",
      "Episode: 591 Total reward: 151.0 Training d_loss: 1.3662 Training g_loss: 0.7071 Training q_loss: 9.5407 Explore P: 0.0107\n",
      "Episode: 592 Total reward: 124.0 Training d_loss: 1.3048 Training g_loss: 1.1873 Training q_loss: 10.7522 Explore P: 0.0107\n",
      "Episode: 593 Total reward: 118.0 Training d_loss: 1.4220 Training g_loss: 0.5880 Training q_loss: 9.4546 Explore P: 0.0107\n",
      "Episode: 594 Total reward: 177.0 Training d_loss: 1.3871 Training g_loss: 0.6697 Training q_loss: 4.4729 Explore P: 0.0107\n",
      "Episode: 595 Total reward: 151.0 Training d_loss: 1.3674 Training g_loss: 7.2193 Training q_loss: 0.6231 Explore P: 0.0107\n",
      "Episode: 596 Total reward: 138.0 Training d_loss: 1.4184 Training g_loss: 0.5147 Training q_loss: 5.0092 Explore P: 0.0107\n",
      "Episode: 597 Total reward: 192.0 Training d_loss: 1.3308 Training g_loss: 0.7903 Training q_loss: 6.1340 Explore P: 0.0106\n",
      "Episode: 598 Total reward: 129.0 Training d_loss: 1.2533 Training g_loss: 6.8062 Training q_loss: 8.7589 Explore P: 0.0106\n",
      "Episode: 599 Total reward: 199.0 Training d_loss: 1.2410 Training g_loss: 7.5126 Training q_loss: 3.2993 Explore P: 0.0106\n",
      "Episode: 600 Total reward: 159.0 Training d_loss: 1.0767 Training g_loss: 8.7311 Training q_loss: 6.0396 Explore P: 0.0106\n",
      "Episode: 601 Total reward: 171.0 Training d_loss: 1.4530 Training g_loss: 0.5168 Training q_loss: 9.8418 Explore P: 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 602 Total reward: 161.0 Training d_loss: 1.1111 Training g_loss: 13.6625 Training q_loss: 13.9426 Explore P: 0.0106\n",
      "Episode: 603 Total reward: 103.0 Training d_loss: 1.2318 Training g_loss: 7.7513 Training q_loss: 15.6046 Explore P: 0.0106\n",
      "Episode: 604 Total reward: 174.0 Training d_loss: 1.3664 Training g_loss: 0.7044 Training q_loss: 2.3574 Explore P: 0.0106\n",
      "Episode: 605 Total reward: 166.0 Training d_loss: 1.2890 Training g_loss: 0.8984 Training q_loss: 10.6456 Explore P: 0.0106\n",
      "Episode: 606 Total reward: 149.0 Training d_loss: 1.3389 Training g_loss: 1.5417 Training q_loss: 14.6128 Explore P: 0.0106\n",
      "Episode: 607 Total reward: 137.0 Training d_loss: 1.3801 Training g_loss: 0.6366 Training q_loss: 4.6369 Explore P: 0.0106\n",
      "Episode: 608 Total reward: 158.0 Training d_loss: 1.1802 Training g_loss: 8.2289 Training q_loss: 3.6761 Explore P: 0.0105\n",
      "Episode: 609 Total reward: 165.0 Training d_loss: 1.3284 Training g_loss: 1.9137 Training q_loss: 15.2535 Explore P: 0.0105\n",
      "Episode: 610 Total reward: 123.0 Training d_loss: 1.2329 Training g_loss: 7.9890 Training q_loss: 3.8546 Explore P: 0.0105\n",
      "Episode: 611 Total reward: 118.0 Training d_loss: 1.2774 Training g_loss: 1.3855 Training q_loss: 20.6902 Explore P: 0.0105\n",
      "Episode: 612 Total reward: 136.0 Training d_loss: 1.1049 Training g_loss: 16.3846 Training q_loss: 11.2797 Explore P: 0.0105\n",
      "Episode: 613 Total reward: 134.0 Training d_loss: 1.1374 Training g_loss: 2.4477 Training q_loss: 7.6165 Explore P: 0.0105\n",
      "Episode: 614 Total reward: 149.0 Training d_loss: 1.1540 Training g_loss: 7.7241 Training q_loss: 3.7306 Explore P: 0.0105\n",
      "Episode: 615 Total reward: 194.0 Training d_loss: 1.2393 Training g_loss: 1.1416 Training q_loss: 6.5998 Explore P: 0.0105\n",
      "Episode: 616 Total reward: 144.0 Training d_loss: 1.4118 Training g_loss: 0.7300 Training q_loss: 34.1401 Explore P: 0.0105\n",
      "Episode: 617 Total reward: 152.0 Training d_loss: 1.2808 Training g_loss: 0.9730 Training q_loss: 2.4467 Explore P: 0.0105\n",
      "Episode: 618 Total reward: 126.0 Training d_loss: 1.3524 Training g_loss: 0.7926 Training q_loss: 8.6074 Explore P: 0.0105\n",
      "Episode: 619 Total reward: 160.0 Training d_loss: 1.4088 Training g_loss: 0.6224 Training q_loss: 3.3664 Explore P: 0.0105\n",
      "Episode: 620 Total reward: 152.0 Training d_loss: 1.2190 Training g_loss: 9.3404 Training q_loss: 7.2258 Explore P: 0.0105\n",
      "Episode: 621 Total reward: 102.0 Training d_loss: 1.3740 Training g_loss: 0.6495 Training q_loss: 3.2551 Explore P: 0.0105\n",
      "Episode: 622 Total reward: 140.0 Training d_loss: 1.3550 Training g_loss: 0.6294 Training q_loss: 6.9910 Explore P: 0.0104\n",
      "Episode: 623 Total reward: 113.0 Training d_loss: 1.2942 Training g_loss: 2.3343 Training q_loss: 6.6904 Explore P: 0.0104\n",
      "Episode: 624 Total reward: 109.0 Training d_loss: 1.2868 Training g_loss: 7.7192 Training q_loss: 4.9559 Explore P: 0.0104\n",
      "Episode: 625 Total reward: 108.0 Training d_loss: 1.1894 Training g_loss: 10.6376 Training q_loss: 4.9631 Explore P: 0.0104\n",
      "Episode: 626 Total reward: 82.0 Training d_loss: 1.2771 Training g_loss: 7.5066 Training q_loss: 11.0180 Explore P: 0.0104\n",
      "Episode: 627 Total reward: 118.0 Training d_loss: 1.2785 Training g_loss: 6.8173 Training q_loss: 10.1928 Explore P: 0.0104\n",
      "Episode: 628 Total reward: 170.0 Training d_loss: 1.2102 Training g_loss: 7.8390 Training q_loss: 4.0983 Explore P: 0.0104\n",
      "Episode: 629 Total reward: 199.0 Training d_loss: 1.3601 Training g_loss: 0.6609 Training q_loss: 10.8917 Explore P: 0.0104\n",
      "Episode: 630 Total reward: 76.0 Training d_loss: 1.3048 Training g_loss: 0.9894 Training q_loss: 5.0749 Explore P: 0.0104\n",
      "Episode: 631 Total reward: 119.0 Training d_loss: 1.3637 Training g_loss: 0.7029 Training q_loss: 8.3416 Explore P: 0.0104\n",
      "Episode: 632 Total reward: 88.0 Training d_loss: 1.3069 Training g_loss: 0.7513 Training q_loss: 15.5778 Explore P: 0.0104\n",
      "Episode: 633 Total reward: 130.0 Training d_loss: 1.1657 Training g_loss: 22.8067 Training q_loss: 10.4328 Explore P: 0.0104\n",
      "Episode: 634 Total reward: 86.0 Training d_loss: 1.3599 Training g_loss: 0.6987 Training q_loss: 10.0172 Explore P: 0.0104\n",
      "Episode: 635 Total reward: 65.0 Training d_loss: 1.3093 Training g_loss: 7.8339 Training q_loss: 9.3407 Explore P: 0.0104\n",
      "Episode: 636 Total reward: 91.0 Training d_loss: 1.3998 Training g_loss: 0.8698 Training q_loss: 2.8348 Explore P: 0.0104\n",
      "Episode: 637 Total reward: 82.0 Training d_loss: 1.2345 Training g_loss: 1.9788 Training q_loss: 11.9548 Explore P: 0.0104\n",
      "Episode: 638 Total reward: 86.0 Training d_loss: 1.2172 Training g_loss: 15.9609 Training q_loss: 8.9369 Explore P: 0.0104\n",
      "Episode: 639 Total reward: 105.0 Training d_loss: 1.2982 Training g_loss: 0.8012 Training q_loss: 9.1804 Explore P: 0.0104\n",
      "Episode: 640 Total reward: 114.0 Training d_loss: 1.2473 Training g_loss: 0.8233 Training q_loss: 2.7186 Explore P: 0.0104\n",
      "Episode: 641 Total reward: 102.0 Training d_loss: 1.1472 Training g_loss: 3.5404 Training q_loss: 6.8935 Explore P: 0.0104\n",
      "Episode: 642 Total reward: 122.0 Training d_loss: 1.3778 Training g_loss: 0.6778 Training q_loss: 6.1944 Explore P: 0.0104\n",
      "Episode: 643 Total reward: 94.0 Training d_loss: 1.1798 Training g_loss: 2.5627 Training q_loss: 8.0130 Explore P: 0.0104\n",
      "Episode: 644 Total reward: 61.0 Training d_loss: 1.2468 Training g_loss: 8.2216 Training q_loss: 4.0791 Explore P: 0.0104\n",
      "Episode: 645 Total reward: 131.0 Training d_loss: 1.3061 Training g_loss: 0.7973 Training q_loss: 25.4729 Explore P: 0.0103\n",
      "Episode: 646 Total reward: 134.0 Training d_loss: 1.2817 Training g_loss: 1.0789 Training q_loss: 8.6038 Explore P: 0.0103\n",
      "Episode: 647 Total reward: 89.0 Training d_loss: 1.2170 Training g_loss: 1.0622 Training q_loss: 8.3173 Explore P: 0.0103\n",
      "Episode: 648 Total reward: 56.0 Training d_loss: 1.2213 Training g_loss: 8.1756 Training q_loss: 4.5555 Explore P: 0.0103\n",
      "Episode: 649 Total reward: 126.0 Training d_loss: 1.2937 Training g_loss: 1.6557 Training q_loss: 6.6025 Explore P: 0.0103\n",
      "Episode: 650 Total reward: 74.0 Training d_loss: 1.3133 Training g_loss: 1.0072 Training q_loss: 3.2860 Explore P: 0.0103\n",
      "Episode: 651 Total reward: 90.0 Training d_loss: 1.1516 Training g_loss: 10.5741 Training q_loss: 9.0295 Explore P: 0.0103\n",
      "Episode: 652 Total reward: 66.0 Training d_loss: 1.1359 Training g_loss: 7.4712 Training q_loss: 19.8490 Explore P: 0.0103\n",
      "Episode: 653 Total reward: 156.0 Training d_loss: 1.1167 Training g_loss: 23.6078 Training q_loss: 8.0049 Explore P: 0.0103\n",
      "Episode: 654 Total reward: 104.0 Training d_loss: 1.3490 Training g_loss: 0.7159 Training q_loss: 4.0215 Explore P: 0.0103\n",
      "Episode: 655 Total reward: 100.0 Training d_loss: 1.1555 Training g_loss: 4.3268 Training q_loss: 19.1779 Explore P: 0.0103\n",
      "Episode: 656 Total reward: 122.0 Training d_loss: 1.3121 Training g_loss: 0.9130 Training q_loss: 12.5909 Explore P: 0.0103\n",
      "Episode: 657 Total reward: 119.0 Training d_loss: 1.3152 Training g_loss: 8.4407 Training q_loss: 6.8477 Explore P: 0.0103\n",
      "Episode: 658 Total reward: 115.0 Training d_loss: 1.3395 Training g_loss: 0.7376 Training q_loss: 12.2137 Explore P: 0.0103\n",
      "Episode: 659 Total reward: 88.0 Training d_loss: 1.3460 Training g_loss: 8.5133 Training q_loss: 2.5293 Explore P: 0.0103\n",
      "Episode: 660 Total reward: 162.0 Training d_loss: 1.3764 Training g_loss: 0.9115 Training q_loss: 26.9843 Explore P: 0.0103\n",
      "Episode: 661 Total reward: 124.0 Training d_loss: 1.3761 Training g_loss: 0.6319 Training q_loss: 2.6892 Explore P: 0.0103\n",
      "Episode: 662 Total reward: 151.0 Training d_loss: 1.3741 Training g_loss: 0.7354 Training q_loss: 2.5758 Explore P: 0.0103\n",
      "Episode: 663 Total reward: 106.0 Training d_loss: 1.1975 Training g_loss: 18.4391 Training q_loss: 23.0344 Explore P: 0.0103\n",
      "Episode: 664 Total reward: 101.0 Training d_loss: 1.3206 Training g_loss: 1.8155 Training q_loss: 31.6503 Explore P: 0.0103\n",
      "Episode: 665 Total reward: 131.0 Training d_loss: 1.3201 Training g_loss: 0.8329 Training q_loss: 2.8783 Explore P: 0.0103\n",
      "Episode: 666 Total reward: 134.0 Training d_loss: 1.2999 Training g_loss: 2.6609 Training q_loss: 4.7279 Explore P: 0.0103\n",
      "Episode: 667 Total reward: 103.0 Training d_loss: 1.2395 Training g_loss: 10.5597 Training q_loss: 10.2288 Explore P: 0.0103\n",
      "Episode: 668 Total reward: 120.0 Training d_loss: 1.2156 Training g_loss: 2.4820 Training q_loss: 5.4413 Explore P: 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 669 Total reward: 113.0 Training d_loss: 1.2772 Training g_loss: 2.5124 Training q_loss: 5.8666 Explore P: 0.0103\n",
      "Episode: 670 Total reward: 125.0 Training d_loss: 1.4213 Training g_loss: 0.5757 Training q_loss: 3.9240 Explore P: 0.0103\n",
      "Episode: 671 Total reward: 118.0 Training d_loss: 1.1686 Training g_loss: 8.8792 Training q_loss: 5.3128 Explore P: 0.0103\n",
      "Episode: 672 Total reward: 120.0 Training d_loss: 1.3891 Training g_loss: 0.7415 Training q_loss: 1.3676 Explore P: 0.0103\n",
      "Episode: 673 Total reward: 127.0 Training d_loss: 1.2626 Training g_loss: 8.7115 Training q_loss: 5.3105 Explore P: 0.0103\n",
      "Episode: 674 Total reward: 102.0 Training d_loss: 1.3781 Training g_loss: 0.6664 Training q_loss: 5.4769 Explore P: 0.0103\n",
      "Episode: 675 Total reward: 155.0 Training d_loss: 1.3135 Training g_loss: 0.9502 Training q_loss: 2.4129 Explore P: 0.0102\n",
      "Episode: 676 Total reward: 121.0 Training d_loss: 1.2233 Training g_loss: 15.6072 Training q_loss: 54.7657 Explore P: 0.0102\n",
      "Episode: 677 Total reward: 151.0 Training d_loss: 1.3848 Training g_loss: 8.3968 Training q_loss: 6.7144 Explore P: 0.0102\n",
      "Episode: 678 Total reward: 148.0 Training d_loss: 1.2367 Training g_loss: 9.6674 Training q_loss: 2.2271 Explore P: 0.0102\n",
      "Episode: 679 Total reward: 125.0 Training d_loss: 1.3248 Training g_loss: 0.8334 Training q_loss: 6.0634 Explore P: 0.0102\n",
      "Episode: 680 Total reward: 121.0 Training d_loss: 1.3551 Training g_loss: 0.7394 Training q_loss: 5.1766 Explore P: 0.0102\n",
      "Episode: 681 Total reward: 169.0 Training d_loss: 1.2753 Training g_loss: 8.2242 Training q_loss: 11.8959 Explore P: 0.0102\n",
      "Episode: 682 Total reward: 150.0 Training d_loss: 1.2907 Training g_loss: 1.6817 Training q_loss: 11.5760 Explore P: 0.0102\n",
      "Episode: 683 Total reward: 111.0 Training d_loss: 1.2738 Training g_loss: 1.1038 Training q_loss: 2.6539 Explore P: 0.0102\n",
      "Episode: 684 Total reward: 86.0 Training d_loss: 1.2348 Training g_loss: 3.8927 Training q_loss: 12.9903 Explore P: 0.0102\n",
      "Episode: 685 Total reward: 161.0 Training d_loss: 1.2352 Training g_loss: 13.0590 Training q_loss: 17.6136 Explore P: 0.0102\n",
      "Episode: 686 Total reward: 171.0 Training d_loss: 1.1893 Training g_loss: 8.6499 Training q_loss: 21.2211 Explore P: 0.0102\n",
      "Episode: 687 Total reward: 199.0 Training d_loss: 1.4209 Training g_loss: 0.6009 Training q_loss: 7.7687 Explore P: 0.0102\n",
      "Episode: 688 Total reward: 74.0 Training d_loss: 1.2455 Training g_loss: 2.7264 Training q_loss: 2.9432 Explore P: 0.0102\n",
      "Episode: 689 Total reward: 162.0 Training d_loss: 1.4112 Training g_loss: 0.6380 Training q_loss: 17.6662 Explore P: 0.0102\n",
      "Episode: 690 Total reward: 75.0 Training d_loss: 1.3078 Training g_loss: 3.5400 Training q_loss: 9.0691 Explore P: 0.0102\n",
      "Episode: 691 Total reward: 106.0 Training d_loss: 1.2838 Training g_loss: 14.7270 Training q_loss: 16.9111 Explore P: 0.0102\n",
      "Episode: 692 Total reward: 77.0 Training d_loss: 1.2566 Training g_loss: 8.7804 Training q_loss: 7.8613 Explore P: 0.0102\n",
      "Episode: 693 Total reward: 106.0 Training d_loss: 1.3236 Training g_loss: 5.8512 Training q_loss: 12.5481 Explore P: 0.0102\n",
      "Episode: 694 Total reward: 120.0 Training d_loss: 1.1981 Training g_loss: 15.4465 Training q_loss: 2.4662 Explore P: 0.0102\n",
      "Episode: 695 Total reward: 111.0 Training d_loss: 1.1919 Training g_loss: 2.9877 Training q_loss: 5.0418 Explore P: 0.0102\n",
      "Episode: 696 Total reward: 103.0 Training d_loss: 1.3088 Training g_loss: 2.2648 Training q_loss: 12.3700 Explore P: 0.0102\n",
      "Episode: 697 Total reward: 111.0 Training d_loss: 1.3226 Training g_loss: 7.6383 Training q_loss: 8.3946 Explore P: 0.0102\n",
      "Episode: 698 Total reward: 62.0 Training d_loss: 1.2385 Training g_loss: 8.0813 Training q_loss: 3.8455 Explore P: 0.0102\n",
      "Episode: 699 Total reward: 107.0 Training d_loss: 1.4233 Training g_loss: 0.5454 Training q_loss: 4.7599 Explore P: 0.0102\n",
      "Episode: 700 Total reward: 72.0 Training d_loss: 1.1949 Training g_loss: 8.2779 Training q_loss: 11.7030 Explore P: 0.0102\n",
      "Episode: 701 Total reward: 46.0 Training d_loss: 1.0955 Training g_loss: 23.2425 Training q_loss: 9.0692 Explore P: 0.0102\n",
      "Episode: 702 Total reward: 51.0 Training d_loss: 1.2243 Training g_loss: 3.2912 Training q_loss: 23.4270 Explore P: 0.0102\n",
      "Episode: 703 Total reward: 36.0 Training d_loss: 1.2429 Training g_loss: 5.9174 Training q_loss: 4.1188 Explore P: 0.0102\n",
      "Episode: 704 Total reward: 31.0 Training d_loss: 1.0850 Training g_loss: 1.0617 Training q_loss: 62.7118 Explore P: 0.0102\n",
      "Episode: 705 Total reward: 30.0 Training d_loss: 1.0814 Training g_loss: 14.8955 Training q_loss: 28.9309 Explore P: 0.0102\n",
      "Episode: 706 Total reward: 28.0 Training d_loss: 1.3331 Training g_loss: 7.7546 Training q_loss: 51.3218 Explore P: 0.0102\n",
      "Episode: 707 Total reward: 25.0 Training d_loss: 1.3267 Training g_loss: 1.0917 Training q_loss: 77.3557 Explore P: 0.0102\n",
      "Episode: 708 Total reward: 20.0 Training d_loss: 1.0098 Training g_loss: 7.7972 Training q_loss: 138.6143 Explore P: 0.0102\n",
      "Episode: 709 Total reward: 13.0 Training d_loss: 1.1382 Training g_loss: 1.0025 Training q_loss: 370.5305 Explore P: 0.0102\n",
      "Episode: 710 Total reward: 15.0 Training d_loss: 0.9049 Training g_loss: 1.6925 Training q_loss: 438.0439 Explore P: 0.0102\n",
      "Episode: 711 Total reward: 12.0 Training d_loss: 0.9136 Training g_loss: 0.8918 Training q_loss: 971.1915 Explore P: 0.0102\n",
      "Episode: 712 Total reward: 13.0 Training d_loss: 0.9813 Training g_loss: 0.9373 Training q_loss: 4778.6875 Explore P: 0.0102\n",
      "Episode: 713 Total reward: 11.0 Training d_loss: 1.1151 Training g_loss: 1.0942 Training q_loss: 4417.0967 Explore P: 0.0102\n",
      "Episode: 714 Total reward: 11.0 Training d_loss: 0.7184 Training g_loss: 1.3850 Training q_loss: 10877.0908 Explore P: 0.0102\n",
      "Episode: 715 Total reward: 10.0 Training d_loss: 0.7482 Training g_loss: 1.1667 Training q_loss: 10199.0137 Explore P: 0.0102\n",
      "Episode: 716 Total reward: 11.0 Training d_loss: 1.1685 Training g_loss: 0.9656 Training q_loss: 13089.9707 Explore P: 0.0102\n",
      "Episode: 717 Total reward: 7.0 Training d_loss: 0.9622 Training g_loss: 1.3182 Training q_loss: 13843.0371 Explore P: 0.0102\n",
      "Episode: 718 Total reward: 8.0 Training d_loss: 0.9594 Training g_loss: 1.0902 Training q_loss: 24482.6309 Explore P: 0.0102\n",
      "Episode: 719 Total reward: 11.0 Training d_loss: 1.0604 Training g_loss: 0.8564 Training q_loss: 22138.6914 Explore P: 0.0102\n",
      "Episode: 720 Total reward: 8.0 Training d_loss: 0.5849 Training g_loss: 1.3864 Training q_loss: 11949.3447 Explore P: 0.0102\n",
      "Episode: 721 Total reward: 14.0 Training d_loss: 1.0687 Training g_loss: 1.0856 Training q_loss: 7203.9468 Explore P: 0.0102\n",
      "Episode: 722 Total reward: 13.0 Training d_loss: 1.2468 Training g_loss: 0.9805 Training q_loss: 11056.9346 Explore P: 0.0102\n",
      "Episode: 723 Total reward: 14.0 Training d_loss: 0.7216 Training g_loss: 1.3952 Training q_loss: 10258.5791 Explore P: 0.0102\n",
      "Episode: 724 Total reward: 12.0 Training d_loss: 1.1568 Training g_loss: 0.9662 Training q_loss: 12836.5830 Explore P: 0.0102\n",
      "Episode: 725 Total reward: 13.0 Training d_loss: 0.9827 Training g_loss: 1.2212 Training q_loss: 15971.4814 Explore P: 0.0102\n",
      "Episode: 726 Total reward: 12.0 Training d_loss: 0.8615 Training g_loss: 1.4606 Training q_loss: 18028.3320 Explore P: 0.0102\n",
      "Episode: 727 Total reward: 9.0 Training d_loss: 0.8641 Training g_loss: 1.1787 Training q_loss: 21049.0430 Explore P: 0.0102\n",
      "Episode: 728 Total reward: 9.0 Training d_loss: 0.9652 Training g_loss: 1.1046 Training q_loss: 15782.7871 Explore P: 0.0102\n",
      "Episode: 729 Total reward: 15.0 Training d_loss: 0.7188 Training g_loss: 1.3726 Training q_loss: 12015.4238 Explore P: 0.0102\n",
      "Episode: 730 Total reward: 13.0 Training d_loss: 0.7106 Training g_loss: 1.6856 Training q_loss: 16736.4297 Explore P: 0.0102\n",
      "Episode: 731 Total reward: 17.0 Training d_loss: 0.8872 Training g_loss: 1.7116 Training q_loss: 11409.7998 Explore P: 0.0102\n",
      "Episode: 732 Total reward: 18.0 Training d_loss: 0.8631 Training g_loss: 1.5045 Training q_loss: 2397.8257 Explore P: 0.0102\n",
      "Episode: 733 Total reward: 14.0 Training d_loss: 0.8585 Training g_loss: 1.3269 Training q_loss: 3197.5598 Explore P: 0.0102\n",
      "Episode: 734 Total reward: 14.0 Training d_loss: 1.0824 Training g_loss: 1.0509 Training q_loss: 3305.0430 Explore P: 0.0102\n",
      "Episode: 735 Total reward: 14.0 Training d_loss: 1.2123 Training g_loss: 1.0188 Training q_loss: 584677.3750 Explore P: 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 736 Total reward: 13.0 Training d_loss: 0.8071 Training g_loss: 1.8867 Training q_loss: 3609.1929 Explore P: 0.0102\n",
      "Episode: 737 Total reward: 13.0 Training d_loss: 0.8589 Training g_loss: 5.8780 Training q_loss: 3371.3403 Explore P: 0.0102\n",
      "Episode: 738 Total reward: 13.0 Training d_loss: 0.9894 Training g_loss: 1.1664 Training q_loss: 5762.5815 Explore P: 0.0102\n",
      "Episode: 739 Total reward: 15.0 Training d_loss: 1.1236 Training g_loss: 1.3777 Training q_loss: 4924.7305 Explore P: 0.0102\n",
      "Episode: 740 Total reward: 18.0 Training d_loss: 1.0908 Training g_loss: 1.0508 Training q_loss: 7152.7251 Explore P: 0.0102\n",
      "Episode: 741 Total reward: 15.0 Training d_loss: 1.2520 Training g_loss: 1.2780 Training q_loss: 7528.3936 Explore P: 0.0102\n",
      "Episode: 742 Total reward: 15.0 Training d_loss: 1.2200 Training g_loss: 1.2532 Training q_loss: 7517.1377 Explore P: 0.0102\n",
      "Episode: 743 Total reward: 17.0 Training d_loss: 1.0979 Training g_loss: 1.1743 Training q_loss: 1835.1703 Explore P: 0.0102\n",
      "Episode: 744 Total reward: 14.0 Training d_loss: 1.2158 Training g_loss: 4.3812 Training q_loss: 3829.6980 Explore P: 0.0102\n",
      "Episode: 745 Total reward: 18.0 Training d_loss: 1.4644 Training g_loss: 0.8301 Training q_loss: 14665.0674 Explore P: 0.0102\n",
      "Episode: 746 Total reward: 18.0 Training d_loss: 1.3276 Training g_loss: 0.8251 Training q_loss: 147076.4062 Explore P: 0.0102\n",
      "Episode: 747 Total reward: 15.0 Training d_loss: 1.0078 Training g_loss: 1.3418 Training q_loss: 242685.0938 Explore P: 0.0102\n",
      "Episode: 748 Total reward: 20.0 Training d_loss: 1.3916 Training g_loss: 0.8669 Training q_loss: 6943.1797 Explore P: 0.0102\n",
      "Episode: 749 Total reward: 24.0 Training d_loss: 1.0687 Training g_loss: 0.8253 Training q_loss: 6141.5723 Explore P: 0.0102\n",
      "Episode: 750 Total reward: 23.0 Training d_loss: 1.2929 Training g_loss: 0.6271 Training q_loss: 5916.8501 Explore P: 0.0102\n",
      "Episode: 751 Total reward: 18.0 Training d_loss: 1.2943 Training g_loss: 1.0643 Training q_loss: 7063.2578 Explore P: 0.0102\n",
      "Episode: 752 Total reward: 23.0 Training d_loss: 1.1978 Training g_loss: 0.9951 Training q_loss: 4132.8955 Explore P: 0.0102\n",
      "Episode: 753 Total reward: 20.0 Training d_loss: 1.4521 Training g_loss: 0.9054 Training q_loss: 8796.1846 Explore P: 0.0102\n",
      "Episode: 754 Total reward: 14.0 Training d_loss: 1.0475 Training g_loss: 0.9435 Training q_loss: 4213.1045 Explore P: 0.0102\n",
      "Episode: 755 Total reward: 18.0 Training d_loss: 1.0294 Training g_loss: 1.3812 Training q_loss: 5280.3496 Explore P: 0.0102\n",
      "Episode: 756 Total reward: 16.0 Training d_loss: 1.3090 Training g_loss: 0.9002 Training q_loss: 2725.5708 Explore P: 0.0102\n",
      "Episode: 757 Total reward: 16.0 Training d_loss: 1.3777 Training g_loss: 0.7605 Training q_loss: 2296.8867 Explore P: 0.0102\n",
      "Episode: 758 Total reward: 20.0 Training d_loss: 0.9732 Training g_loss: 1.1981 Training q_loss: 7217.0298 Explore P: 0.0102\n",
      "Episode: 759 Total reward: 38.0 Training d_loss: 1.3546 Training g_loss: 0.7488 Training q_loss: 6640.7891 Explore P: 0.0102\n",
      "Episode: 760 Total reward: 28.0 Training d_loss: 1.4149 Training g_loss: 0.6544 Training q_loss: 13626.4082 Explore P: 0.0102\n",
      "Episode: 761 Total reward: 35.0 Training d_loss: 1.1651 Training g_loss: 8.4504 Training q_loss: 2028.3971 Explore P: 0.0102\n",
      "Episode: 762 Total reward: 24.0 Training d_loss: 1.1877 Training g_loss: 1.1414 Training q_loss: 5533.6094 Explore P: 0.0102\n",
      "Episode: 763 Total reward: 26.0 Training d_loss: 1.2671 Training g_loss: 0.6833 Training q_loss: 2801.8413 Explore P: 0.0102\n",
      "Episode: 764 Total reward: 40.0 Training d_loss: 1.3883 Training g_loss: 0.6061 Training q_loss: 1200325.1250 Explore P: 0.0102\n",
      "Episode: 765 Total reward: 34.0 Training d_loss: 1.3219 Training g_loss: 0.8371 Training q_loss: 3755.8020 Explore P: 0.0102\n",
      "Episode: 766 Total reward: 25.0 Training d_loss: 1.1972 Training g_loss: 1.0509 Training q_loss: 2670.7664 Explore P: 0.0102\n",
      "Episode: 767 Total reward: 18.0 Training d_loss: 1.3709 Training g_loss: 0.9373 Training q_loss: 1974216.3750 Explore P: 0.0102\n",
      "Episode: 768 Total reward: 25.0 Training d_loss: 1.1143 Training g_loss: 0.8063 Training q_loss: 4295.3643 Explore P: 0.0102\n",
      "Episode: 769 Total reward: 28.0 Training d_loss: 1.0997 Training g_loss: 1.2779 Training q_loss: 762313.4375 Explore P: 0.0102\n",
      "Episode: 770 Total reward: 31.0 Training d_loss: 1.2847 Training g_loss: 13.9762 Training q_loss: 4455.1904 Explore P: 0.0102\n",
      "Episode: 771 Total reward: 38.0 Training d_loss: 1.1497 Training g_loss: 1.6087 Training q_loss: 4555.8506 Explore P: 0.0102\n",
      "Episode: 772 Total reward: 35.0 Training d_loss: 1.0674 Training g_loss: 0.9613 Training q_loss: 1332.8827 Explore P: 0.0102\n",
      "Episode: 773 Total reward: 44.0 Training d_loss: 1.3962 Training g_loss: 0.5875 Training q_loss: 2424.0024 Explore P: 0.0102\n",
      "Episode: 774 Total reward: 67.0 Training d_loss: 1.2236 Training g_loss: 7.4861 Training q_loss: 2002.5424 Explore P: 0.0102\n",
      "Episode: 775 Total reward: 41.0 Training d_loss: 1.3724 Training g_loss: 0.6162 Training q_loss: 1163.9310 Explore P: 0.0102\n",
      "Episode: 776 Total reward: 55.0 Training d_loss: 1.4003 Training g_loss: 0.9068 Training q_loss: 1375.7858 Explore P: 0.0102\n",
      "Episode: 777 Total reward: 19.0 Training d_loss: 1.2532 Training g_loss: 1.1748 Training q_loss: 3148.9390 Explore P: 0.0102\n",
      "Episode: 778 Total reward: 26.0 Training d_loss: 1.3666 Training g_loss: 7.5018 Training q_loss: 1353.5585 Explore P: 0.0102\n",
      "Episode: 779 Total reward: 31.0 Training d_loss: 1.0864 Training g_loss: 13.9563 Training q_loss: 1321.6775 Explore P: 0.0102\n",
      "Episode: 780 Total reward: 38.0 Training d_loss: 1.0907 Training g_loss: 6.7618 Training q_loss: 896.9664 Explore P: 0.0102\n",
      "Episode: 781 Total reward: 12.0 Training d_loss: 1.2358 Training g_loss: 6.6674 Training q_loss: 1659.7292 Explore P: 0.0102\n",
      "Episode: 782 Total reward: 94.0 Training d_loss: 1.1715 Training g_loss: 1.2010 Training q_loss: 1123.1211 Explore P: 0.0102\n",
      "Episode: 783 Total reward: 21.0 Training d_loss: 1.2153 Training g_loss: 0.9716 Training q_loss: 367.8924 Explore P: 0.0102\n",
      "Episode: 784 Total reward: 18.0 Training d_loss: 0.9844 Training g_loss: 0.8745 Training q_loss: 1000.0170 Explore P: 0.0102\n",
      "Episode: 785 Total reward: 10.0 Training d_loss: 1.1561 Training g_loss: 1.2697 Training q_loss: 1902.8748 Explore P: 0.0102\n",
      "Episode: 786 Total reward: 14.0 Training d_loss: 1.3533 Training g_loss: 0.9019 Training q_loss: 5550.8403 Explore P: 0.0102\n",
      "Episode: 787 Total reward: 17.0 Training d_loss: 1.0253 Training g_loss: 6.6815 Training q_loss: 1561.1918 Explore P: 0.0102\n",
      "Episode: 788 Total reward: 13.0 Training d_loss: 1.1110 Training g_loss: 1.0620 Training q_loss: 1266.1770 Explore P: 0.0102\n",
      "Episode: 789 Total reward: 12.0 Training d_loss: 1.5255 Training g_loss: 0.9146 Training q_loss: 8008.0068 Explore P: 0.0102\n",
      "Episode: 790 Total reward: 15.0 Training d_loss: 1.2184 Training g_loss: 8.9108 Training q_loss: 682.1945 Explore P: 0.0102\n",
      "Episode: 791 Total reward: 36.0 Training d_loss: 1.3152 Training g_loss: 7.6048 Training q_loss: 910.7863 Explore P: 0.0102\n",
      "Episode: 792 Total reward: 26.0 Training d_loss: 1.1221 Training g_loss: 1.1830 Training q_loss: 4162.3447 Explore P: 0.0101\n",
      "Episode: 793 Total reward: 13.0 Training d_loss: 1.4732 Training g_loss: 0.4975 Training q_loss: 917.6915 Explore P: 0.0101\n",
      "Episode: 794 Total reward: 105.0 Training d_loss: 0.9561 Training g_loss: 7.1421 Training q_loss: 623.4944 Explore P: 0.0101\n",
      "Episode: 795 Total reward: 138.0 Training d_loss: 1.3154 Training g_loss: 7.3301 Training q_loss: 813.6531 Explore P: 0.0101\n",
      "Episode: 796 Total reward: 88.0 Training d_loss: 1.0496 Training g_loss: 3.4750 Training q_loss: 1739.1924 Explore P: 0.0101\n",
      "Episode: 797 Total reward: 111.0 Training d_loss: 1.1959 Training g_loss: 0.8215 Training q_loss: 802.5848 Explore P: 0.0101\n",
      "Episode: 798 Total reward: 162.0 Training d_loss: 1.1090 Training g_loss: 1.0675 Training q_loss: 1201.8728 Explore P: 0.0101\n",
      "Episode: 799 Total reward: 90.0 Training d_loss: 1.0957 Training g_loss: 1.0181 Training q_loss: 681.8585 Explore P: 0.0101\n",
      "Episode: 800 Total reward: 76.0 Training d_loss: 1.2946 Training g_loss: 0.8203 Training q_loss: 16456.6484 Explore P: 0.0101\n",
      "Episode: 801 Total reward: 62.0 Training d_loss: 1.4693 Training g_loss: 0.8804 Training q_loss: 1093.3923 Explore P: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 802 Total reward: 98.0 Training d_loss: 1.3980 Training g_loss: 0.5702 Training q_loss: 1250.8759 Explore P: 0.0101\n",
      "Episode: 803 Total reward: 172.0 Training d_loss: 1.2943 Training g_loss: 0.9185 Training q_loss: 598.3201 Explore P: 0.0101\n",
      "Episode: 804 Total reward: 91.0 Training d_loss: 1.2807 Training g_loss: 1.0384 Training q_loss: 769.0616 Explore P: 0.0101\n",
      "Episode: 805 Total reward: 143.0 Training d_loss: 1.0121 Training g_loss: 0.9452 Training q_loss: 456.5778 Explore P: 0.0101\n",
      "Episode: 806 Total reward: 58.0 Training d_loss: 1.2229 Training g_loss: 0.9653 Training q_loss: 1131.1536 Explore P: 0.0101\n",
      "Episode: 807 Total reward: 67.0 Training d_loss: 1.1699 Training g_loss: 7.7926 Training q_loss: 770.8448 Explore P: 0.0101\n",
      "Episode: 808 Total reward: 60.0 Training d_loss: 1.1852 Training g_loss: 0.8651 Training q_loss: 1990.9387 Explore P: 0.0101\n",
      "Episode: 809 Total reward: 67.0 Training d_loss: 1.1553 Training g_loss: 3.5471 Training q_loss: 429.2469 Explore P: 0.0101\n",
      "Episode: 810 Total reward: 81.0 Training d_loss: 1.2571 Training g_loss: 0.7064 Training q_loss: 306.8799 Explore P: 0.0101\n",
      "Episode: 811 Total reward: 125.0 Training d_loss: 1.2764 Training g_loss: 6.7259 Training q_loss: 345.2326 Explore P: 0.0101\n",
      "Episode: 812 Total reward: 70.0 Training d_loss: 1.3653 Training g_loss: 0.6086 Training q_loss: 822.9886 Explore P: 0.0101\n",
      "Episode: 813 Total reward: 64.0 Training d_loss: 1.3450 Training g_loss: 1.7344 Training q_loss: 497.3617 Explore P: 0.0101\n",
      "Episode: 814 Total reward: 199.0 Training d_loss: 1.3234 Training g_loss: 2.7630 Training q_loss: 10095.1455 Explore P: 0.0101\n",
      "Episode: 815 Total reward: 188.0 Training d_loss: 1.3771 Training g_loss: 0.6649 Training q_loss: 98.8269 Explore P: 0.0101\n",
      "Episode: 816 Total reward: 44.0 Training d_loss: 1.2455 Training g_loss: 4.3254 Training q_loss: 282.3161 Explore P: 0.0101\n",
      "Episode: 817 Total reward: 75.0 Training d_loss: 1.1873 Training g_loss: 12.8731 Training q_loss: 288.9841 Explore P: 0.0101\n",
      "Episode: 818 Total reward: 25.0 Training d_loss: 1.2936 Training g_loss: 6.8275 Training q_loss: 318.5078 Explore P: 0.0101\n",
      "Episode: 819 Total reward: 42.0 Training d_loss: 1.3217 Training g_loss: 0.6986 Training q_loss: 1051.5784 Explore P: 0.0101\n",
      "Episode: 820 Total reward: 19.0 Training d_loss: 1.1722 Training g_loss: 1.1691 Training q_loss: 1798.1758 Explore P: 0.0101\n",
      "Episode: 821 Total reward: 19.0 Training d_loss: 1.1450 Training g_loss: 8.5421 Training q_loss: 287.6993 Explore P: 0.0101\n",
      "Episode: 822 Total reward: 30.0 Training d_loss: 1.1016 Training g_loss: 3.3800 Training q_loss: 485.6473 Explore P: 0.0101\n",
      "Episode: 823 Total reward: 16.0 Training d_loss: 1.3530 Training g_loss: 0.7486 Training q_loss: 179.7124 Explore P: 0.0101\n",
      "Episode: 824 Total reward: 20.0 Training d_loss: 1.1803 Training g_loss: 0.8888 Training q_loss: 450.1805 Explore P: 0.0101\n",
      "Episode: 825 Total reward: 22.0 Training d_loss: 1.2287 Training g_loss: 0.7408 Training q_loss: 536.5570 Explore P: 0.0101\n",
      "Episode: 826 Total reward: 18.0 Training d_loss: 0.9934 Training g_loss: 6.9509 Training q_loss: 812.4841 Explore P: 0.0101\n",
      "Episode: 827 Total reward: 16.0 Training d_loss: 1.3299 Training g_loss: 0.7840 Training q_loss: 775.3018 Explore P: 0.0101\n",
      "Episode: 828 Total reward: 39.0 Training d_loss: 1.2717 Training g_loss: 4.5288 Training q_loss: 277.9314 Explore P: 0.0101\n",
      "Episode: 829 Total reward: 163.0 Training d_loss: 1.3003 Training g_loss: 1.2792 Training q_loss: 482.5164 Explore P: 0.0101\n",
      "Episode: 830 Total reward: 24.0 Training d_loss: 1.2972 Training g_loss: 3.6445 Training q_loss: 435.1048 Explore P: 0.0101\n",
      "Episode: 831 Total reward: 24.0 Training d_loss: 1.2448 Training g_loss: 7.2565 Training q_loss: 502.8408 Explore P: 0.0101\n",
      "Episode: 832 Total reward: 23.0 Training d_loss: 1.3299 Training g_loss: 2.1322 Training q_loss: 726.9589 Explore P: 0.0101\n",
      "Episode: 833 Total reward: 63.0 Training d_loss: 1.1688 Training g_loss: 0.7702 Training q_loss: 255.5244 Explore P: 0.0101\n",
      "Episode: 834 Total reward: 32.0 Training d_loss: 1.1949 Training g_loss: 12.0910 Training q_loss: 539.2438 Explore P: 0.0101\n",
      "Episode: 835 Total reward: 30.0 Training d_loss: 1.2646 Training g_loss: 6.8631 Training q_loss: 3500.3516 Explore P: 0.0101\n",
      "Episode: 836 Total reward: 30.0 Training d_loss: 1.2563 Training g_loss: 1.2961 Training q_loss: 190.1563 Explore P: 0.0101\n",
      "Episode: 837 Total reward: 29.0 Training d_loss: 1.2673 Training g_loss: 4.3539 Training q_loss: 251.5762 Explore P: 0.0101\n",
      "Episode: 838 Total reward: 32.0 Training d_loss: 1.3119 Training g_loss: 0.7578 Training q_loss: 433.1358 Explore P: 0.0101\n",
      "Episode: 839 Total reward: 23.0 Training d_loss: 1.2671 Training g_loss: 0.6674 Training q_loss: 265.2585 Explore P: 0.0101\n",
      "Episode: 840 Total reward: 25.0 Training d_loss: 1.2900 Training g_loss: 0.8446 Training q_loss: 1528.6846 Explore P: 0.0101\n",
      "Episode: 841 Total reward: 24.0 Training d_loss: 1.3216 Training g_loss: 4.9558 Training q_loss: 299.0346 Explore P: 0.0101\n",
      "Episode: 842 Total reward: 26.0 Training d_loss: 1.2884 Training g_loss: 6.6401 Training q_loss: 993.1238 Explore P: 0.0101\n",
      "Episode: 843 Total reward: 31.0 Training d_loss: 1.2633 Training g_loss: 0.8557 Training q_loss: 325.5008 Explore P: 0.0101\n",
      "Episode: 844 Total reward: 21.0 Training d_loss: 1.2362 Training g_loss: 6.9017 Training q_loss: 1385.9767 Explore P: 0.0101\n",
      "Episode: 845 Total reward: 16.0 Training d_loss: 1.4280 Training g_loss: 0.6891 Training q_loss: 777.1641 Explore P: 0.0101\n",
      "Episode: 846 Total reward: 28.0 Training d_loss: 1.3922 Training g_loss: 6.6056 Training q_loss: 130.6874 Explore P: 0.0101\n",
      "Episode: 847 Total reward: 29.0 Training d_loss: 1.3772 Training g_loss: 0.6838 Training q_loss: 360.6290 Explore P: 0.0101\n",
      "Episode: 848 Total reward: 28.0 Training d_loss: 1.4084 Training g_loss: 0.6320 Training q_loss: 509.2090 Explore P: 0.0101\n",
      "Episode: 849 Total reward: 25.0 Training d_loss: 1.3494 Training g_loss: 0.7465 Training q_loss: 3820.5991 Explore P: 0.0101\n",
      "Episode: 850 Total reward: 31.0 Training d_loss: 1.3377 Training g_loss: 0.6790 Training q_loss: 3111.8608 Explore P: 0.0101\n",
      "Episode: 851 Total reward: 27.0 Training d_loss: 1.3835 Training g_loss: 0.6913 Training q_loss: 1867.3367 Explore P: 0.0101\n",
      "Episode: 852 Total reward: 22.0 Training d_loss: 1.3491 Training g_loss: 0.8062 Training q_loss: 636.7545 Explore P: 0.0101\n",
      "Episode: 853 Total reward: 17.0 Training d_loss: 1.2655 Training g_loss: 0.6889 Training q_loss: 4663.1353 Explore P: 0.0101\n",
      "Episode: 854 Total reward: 13.0 Training d_loss: 1.2172 Training g_loss: 0.7898 Training q_loss: 653.9340 Explore P: 0.0101\n",
      "Episode: 855 Total reward: 21.0 Training d_loss: 1.3761 Training g_loss: 0.8056 Training q_loss: 4996.9268 Explore P: 0.0101\n",
      "Episode: 856 Total reward: 23.0 Training d_loss: 1.2997 Training g_loss: 2.7825 Training q_loss: 756.6517 Explore P: 0.0101\n",
      "Episode: 857 Total reward: 19.0 Training d_loss: 1.2201 Training g_loss: 0.8850 Training q_loss: 682.7201 Explore P: 0.0101\n",
      "Episode: 858 Total reward: 23.0 Training d_loss: 1.3277 Training g_loss: 0.9482 Training q_loss: 696.8081 Explore P: 0.0101\n",
      "Episode: 859 Total reward: 19.0 Training d_loss: 1.2726 Training g_loss: 1.4590 Training q_loss: 1147.9368 Explore P: 0.0101\n",
      "Episode: 860 Total reward: 19.0 Training d_loss: 1.2883 Training g_loss: 0.8140 Training q_loss: 7110.0166 Explore P: 0.0101\n",
      "Episode: 861 Total reward: 19.0 Training d_loss: 1.2456 Training g_loss: 4.9980 Training q_loss: 7206.7393 Explore P: 0.0101\n",
      "Episode: 862 Total reward: 21.0 Training d_loss: 1.3180 Training g_loss: 4.2118 Training q_loss: 438.2480 Explore P: 0.0101\n",
      "Episode: 863 Total reward: 20.0 Training d_loss: 1.2438 Training g_loss: 12.2665 Training q_loss: 3409.2520 Explore P: 0.0101\n",
      "Episode: 864 Total reward: 20.0 Training d_loss: 1.1325 Training g_loss: 4.4832 Training q_loss: 446.1341 Explore P: 0.0101\n",
      "Episode: 865 Total reward: 18.0 Training d_loss: 1.2338 Training g_loss: 0.9468 Training q_loss: 572.8556 Explore P: 0.0101\n",
      "Episode: 866 Total reward: 17.0 Training d_loss: 1.3858 Training g_loss: 1.0200 Training q_loss: 752.6813 Explore P: 0.0101\n",
      "Episode: 867 Total reward: 17.0 Training d_loss: 1.1426 Training g_loss: 6.4504 Training q_loss: 863.0812 Explore P: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 868 Total reward: 19.0 Training d_loss: 1.3208 Training g_loss: 0.6937 Training q_loss: 97.3609 Explore P: 0.0101\n",
      "Episode: 869 Total reward: 19.0 Training d_loss: 1.3263 Training g_loss: 0.7291 Training q_loss: 545.9496 Explore P: 0.0101\n",
      "Episode: 870 Total reward: 40.0 Training d_loss: 1.2552 Training g_loss: 7.3851 Training q_loss: 1034.3032 Explore P: 0.0101\n",
      "Episode: 871 Total reward: 82.0 Training d_loss: 1.3283 Training g_loss: 0.6322 Training q_loss: 600.4856 Explore P: 0.0101\n",
      "Episode: 872 Total reward: 144.0 Training d_loss: 1.3285 Training g_loss: 0.8279 Training q_loss: 206.7463 Explore P: 0.0101\n",
      "Episode: 873 Total reward: 110.0 Training d_loss: 1.1639 Training g_loss: 0.9871 Training q_loss: 804.6430 Explore P: 0.0101\n",
      "Episode: 874 Total reward: 74.0 Training d_loss: 1.1968 Training g_loss: 0.9688 Training q_loss: 686.9255 Explore P: 0.0101\n",
      "Episode: 875 Total reward: 64.0 Training d_loss: 1.1114 Training g_loss: 0.9779 Training q_loss: 397.3913 Explore P: 0.0101\n",
      "Episode: 876 Total reward: 51.0 Training d_loss: 1.2604 Training g_loss: 0.8875 Training q_loss: 10650.7295 Explore P: 0.0101\n",
      "Episode: 877 Total reward: 97.0 Training d_loss: 1.2616 Training g_loss: 0.9512 Training q_loss: 4577.5376 Explore P: 0.0101\n",
      "Episode: 878 Total reward: 142.0 Training d_loss: 1.1058 Training g_loss: 0.9212 Training q_loss: 525.1547 Explore P: 0.0101\n",
      "Episode: 879 Total reward: 118.0 Training d_loss: 1.0952 Training g_loss: 0.9607 Training q_loss: 657.7537 Explore P: 0.0101\n",
      "Episode: 880 Total reward: 109.0 Training d_loss: 1.2863 Training g_loss: 0.8316 Training q_loss: 279.7897 Explore P: 0.0101\n",
      "Episode: 881 Total reward: 123.0 Training d_loss: 1.1435 Training g_loss: 0.9216 Training q_loss: 6598.3843 Explore P: 0.0101\n",
      "Episode: 882 Total reward: 120.0 Training d_loss: 1.2786 Training g_loss: 0.7408 Training q_loss: 1131.9214 Explore P: 0.0101\n",
      "Episode: 883 Total reward: 133.0 Training d_loss: 1.3327 Training g_loss: 0.7375 Training q_loss: 1250.4246 Explore P: 0.0101\n",
      "Episode: 884 Total reward: 113.0 Training d_loss: 1.1679 Training g_loss: 1.0126 Training q_loss: 380.5545 Explore P: 0.0101\n",
      "Episode: 885 Total reward: 132.0 Training d_loss: 1.3907 Training g_loss: 0.6305 Training q_loss: 729.9063 Explore P: 0.0101\n",
      "Episode: 886 Total reward: 127.0 Training d_loss: 1.2835 Training g_loss: 0.8636 Training q_loss: 667.6677 Explore P: 0.0101\n",
      "Episode: 887 Total reward: 112.0 Training d_loss: 0.9611 Training g_loss: 1.1095 Training q_loss: 11044.7256 Explore P: 0.0101\n",
      "Episode: 888 Total reward: 130.0 Training d_loss: 1.3699 Training g_loss: 0.7272 Training q_loss: 317.9232 Explore P: 0.0101\n",
      "Episode: 889 Total reward: 168.0 Training d_loss: 1.2375 Training g_loss: 1.8042 Training q_loss: 348.1005 Explore P: 0.0101\n",
      "Episode: 890 Total reward: 122.0 Training d_loss: 1.2833 Training g_loss: 0.6439 Training q_loss: 8593.3408 Explore P: 0.0101\n",
      "Episode: 891 Total reward: 155.0 Training d_loss: 1.4035 Training g_loss: 0.8034 Training q_loss: 711.5181 Explore P: 0.0101\n",
      "Episode: 892 Total reward: 123.0 Training d_loss: 1.2990 Training g_loss: 4.9300 Training q_loss: 586.4976 Explore P: 0.0101\n",
      "Episode: 893 Total reward: 144.0 Training d_loss: 1.3398 Training g_loss: 2.1832 Training q_loss: 644.9045 Explore P: 0.0101\n",
      "Episode: 894 Total reward: 199.0 Training d_loss: 1.3845 Training g_loss: 0.7317 Training q_loss: 643.3372 Explore P: 0.0101\n",
      "Episode: 895 Total reward: 123.0 Training d_loss: 1.3875 Training g_loss: 2.8642 Training q_loss: 408.7993 Explore P: 0.0101\n",
      "Episode: 896 Total reward: 178.0 Training d_loss: 1.3749 Training g_loss: 0.7604 Training q_loss: 463.7094 Explore P: 0.0101\n",
      "Episode: 897 Total reward: 101.0 Training d_loss: 1.3149 Training g_loss: 5.8497 Training q_loss: 433.8434 Explore P: 0.0101\n",
      "Episode: 898 Total reward: 97.0 Training d_loss: 1.2768 Training g_loss: 5.7670 Training q_loss: 8762.4883 Explore P: 0.0101\n",
      "Episode: 899 Total reward: 199.0 Training d_loss: 1.1639 Training g_loss: 0.8447 Training q_loss: 48.8379 Explore P: 0.0101\n",
      "Episode: 900 Total reward: 125.0 Training d_loss: 1.4228 Training g_loss: 0.6997 Training q_loss: 9873.6191 Explore P: 0.0101\n",
      "Episode: 901 Total reward: 130.0 Training d_loss: 1.3196 Training g_loss: 0.7352 Training q_loss: 284.3066 Explore P: 0.0101\n",
      "Episode: 902 Total reward: 82.0 Training d_loss: 1.3590 Training g_loss: 0.6892 Training q_loss: 224.8699 Explore P: 0.0101\n",
      "Episode: 903 Total reward: 111.0 Training d_loss: 1.4148 Training g_loss: 0.6098 Training q_loss: 606.0248 Explore P: 0.0101\n",
      "Episode: 904 Total reward: 199.0 Training d_loss: 1.2443 Training g_loss: 1.9166 Training q_loss: 655.9609 Explore P: 0.0101\n",
      "Episode: 905 Total reward: 159.0 Training d_loss: 1.3943 Training g_loss: 0.6417 Training q_loss: 646.2855 Explore P: 0.0101\n",
      "Episode: 906 Total reward: 199.0 Training d_loss: 1.1771 Training g_loss: 7.5403 Training q_loss: 17285.8594 Explore P: 0.0101\n",
      "Episode: 907 Total reward: 199.0 Training d_loss: 1.2410 Training g_loss: 4.8018 Training q_loss: 281.1274 Explore P: 0.0101\n",
      "Episode: 908 Total reward: 199.0 Training d_loss: 1.1510 Training g_loss: 1.1426 Training q_loss: 436.0144 Explore P: 0.0101\n",
      "Episode: 909 Total reward: 199.0 Training d_loss: 1.3821 Training g_loss: 0.7086 Training q_loss: 205.8032 Explore P: 0.0101\n",
      "Episode: 910 Total reward: 159.0 Training d_loss: 1.4419 Training g_loss: 0.5313 Training q_loss: 116.4377 Explore P: 0.0101\n",
      "Episode: 911 Total reward: 199.0 Training d_loss: 1.3725 Training g_loss: 0.6876 Training q_loss: 26896.8906 Explore P: 0.0101\n",
      "Episode: 912 Total reward: 199.0 Training d_loss: 1.3190 Training g_loss: 2.1715 Training q_loss: 204.2461 Explore P: 0.0101\n",
      "Episode: 913 Total reward: 199.0 Training d_loss: 1.3865 Training g_loss: 0.7105 Training q_loss: 292.5189 Explore P: 0.0101\n",
      "Episode: 914 Total reward: 199.0 Training d_loss: 1.3637 Training g_loss: 3.1608 Training q_loss: 219.9728 Explore P: 0.0101\n",
      "Episode: 915 Total reward: 199.0 Training d_loss: 1.3975 Training g_loss: 0.6339 Training q_loss: 347.8421 Explore P: 0.0101\n",
      "Episode: 916 Total reward: 100.0 Training d_loss: 1.1688 Training g_loss: 0.7688 Training q_loss: 738.8713 Explore P: 0.0101\n",
      "Episode: 917 Total reward: 199.0 Training d_loss: 1.2595 Training g_loss: 6.7502 Training q_loss: 319.0805 Explore P: 0.0101\n",
      "Episode: 918 Total reward: 147.0 Training d_loss: 1.2572 Training g_loss: 6.6966 Training q_loss: 149.0845 Explore P: 0.0101\n",
      "Episode: 919 Total reward: 137.0 Training d_loss: 1.3836 Training g_loss: 0.6415 Training q_loss: 174.4645 Explore P: 0.0101\n",
      "Episode: 920 Total reward: 199.0 Training d_loss: 1.4293 Training g_loss: 0.6019 Training q_loss: 225.4077 Explore P: 0.0100\n",
      "Episode: 921 Total reward: 199.0 Training d_loss: 1.3393 Training g_loss: 0.6773 Training q_loss: 868.5156 Explore P: 0.0100\n",
      "Episode: 922 Total reward: 199.0 Training d_loss: 1.3163 Training g_loss: 4.7908 Training q_loss: 163.2893 Explore P: 0.0100\n",
      "Episode: 923 Total reward: 143.0 Training d_loss: 1.3798 Training g_loss: 0.7252 Training q_loss: 298.6087 Explore P: 0.0100\n",
      "Episode: 924 Total reward: 131.0 Training d_loss: 1.3439 Training g_loss: 0.7602 Training q_loss: 245.4193 Explore P: 0.0100\n",
      "Episode: 925 Total reward: 199.0 Training d_loss: 1.3492 Training g_loss: 0.7076 Training q_loss: 458.0850 Explore P: 0.0100\n",
      "Episode: 926 Total reward: 199.0 Training d_loss: 1.3128 Training g_loss: 4.5402 Training q_loss: 380.2906 Explore P: 0.0100\n",
      "Episode: 927 Total reward: 199.0 Training d_loss: 1.3894 Training g_loss: 0.7769 Training q_loss: 207.7140 Explore P: 0.0100\n",
      "Episode: 928 Total reward: 175.0 Training d_loss: 1.3857 Training g_loss: 0.6435 Training q_loss: 198.1256 Explore P: 0.0100\n",
      "Episode: 929 Total reward: 199.0 Training d_loss: 1.3160 Training g_loss: 2.7076 Training q_loss: 86.3670 Explore P: 0.0100\n",
      "Episode: 930 Total reward: 199.0 Training d_loss: 1.1582 Training g_loss: 7.5309 Training q_loss: 223.0343 Explore P: 0.0100\n",
      "Episode: 931 Total reward: 199.0 Training d_loss: 1.3156 Training g_loss: 4.5907 Training q_loss: 70.0026 Explore P: 0.0100\n",
      "Episode: 932 Total reward: 199.0 Training d_loss: 1.2050 Training g_loss: 7.2708 Training q_loss: 137.9759 Explore P: 0.0100\n",
      "Episode: 933 Total reward: 157.0 Training d_loss: 1.6886 Training g_loss: 7.0260 Training q_loss: 272.2664 Explore P: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 934 Total reward: 189.0 Training d_loss: 1.2497 Training g_loss: 0.8565 Training q_loss: 243.2249 Explore P: 0.0100\n",
      "Episode: 935 Total reward: 114.0 Training d_loss: 1.1676 Training g_loss: 2.3771 Training q_loss: 173.5432 Explore P: 0.0100\n",
      "Episode: 936 Total reward: 121.0 Training d_loss: 1.2294 Training g_loss: 5.5262 Training q_loss: 267.0475 Explore P: 0.0100\n",
      "Episode: 937 Total reward: 107.0 Training d_loss: 1.2958 Training g_loss: 4.5340 Training q_loss: 224.7875 Explore P: 0.0100\n",
      "Episode: 938 Total reward: 120.0 Training d_loss: 1.5091 Training g_loss: 5.0365 Training q_loss: 188.8164 Explore P: 0.0100\n",
      "Episode: 939 Total reward: 115.0 Training d_loss: 1.2189 Training g_loss: 2.3378 Training q_loss: 220.6620 Explore P: 0.0100\n",
      "Episode: 940 Total reward: 116.0 Training d_loss: 1.2570 Training g_loss: 4.5294 Training q_loss: 376.4562 Explore P: 0.0100\n",
      "Episode: 941 Total reward: 124.0 Training d_loss: 1.2927 Training g_loss: 0.8304 Training q_loss: 197.4508 Explore P: 0.0100\n",
      "Episode: 942 Total reward: 122.0 Training d_loss: 1.0515 Training g_loss: 3.7428 Training q_loss: 863.4753 Explore P: 0.0100\n",
      "Episode: 943 Total reward: 152.0 Training d_loss: 1.3072 Training g_loss: 4.4044 Training q_loss: 146.3436 Explore P: 0.0100\n",
      "Episode: 944 Total reward: 149.0 Training d_loss: 1.3458 Training g_loss: 4.2066 Training q_loss: 114.3961 Explore P: 0.0100\n",
      "Episode: 945 Total reward: 112.0 Training d_loss: 1.3273 Training g_loss: 0.6368 Training q_loss: 465.0121 Explore P: 0.0100\n",
      "Episode: 946 Total reward: 108.0 Training d_loss: 1.0040 Training g_loss: 17.1344 Training q_loss: 182.4033 Explore P: 0.0100\n",
      "Episode: 947 Total reward: 104.0 Training d_loss: 1.2086 Training g_loss: 6.7820 Training q_loss: 204.6133 Explore P: 0.0100\n",
      "Episode: 948 Total reward: 24.0 Training d_loss: 1.2227 Training g_loss: 2.1994 Training q_loss: 85.0356 Explore P: 0.0100\n",
      "Episode: 949 Total reward: 66.0 Training d_loss: 1.4632 Training g_loss: 0.5540 Training q_loss: 361.3455 Explore P: 0.0100\n",
      "Episode: 950 Total reward: 199.0 Training d_loss: 1.4835 Training g_loss: 4.2283 Training q_loss: 63.1685 Explore P: 0.0100\n",
      "Episode: 951 Total reward: 199.0 Training d_loss: 1.0931 Training g_loss: 6.4434 Training q_loss: 129.9269 Explore P: 0.0100\n",
      "Episode: 952 Total reward: 84.0 Training d_loss: 1.3640 Training g_loss: 0.9672 Training q_loss: 433.6549 Explore P: 0.0100\n",
      "Episode: 953 Total reward: 37.0 Training d_loss: 1.4415 Training g_loss: 0.5870 Training q_loss: 577.4592 Explore P: 0.0100\n",
      "Episode: 954 Total reward: 105.0 Training d_loss: 1.2343 Training g_loss: 0.8063 Training q_loss: 221.2385 Explore P: 0.0100\n",
      "Episode: 955 Total reward: 199.0 Training d_loss: 1.3881 Training g_loss: 0.6554 Training q_loss: 81.8588 Explore P: 0.0100\n",
      "Episode: 956 Total reward: 199.0 Training d_loss: 1.3720 Training g_loss: 0.6482 Training q_loss: 109.3141 Explore P: 0.0100\n",
      "Episode: 957 Total reward: 199.0 Training d_loss: 1.3898 Training g_loss: 0.6323 Training q_loss: 132.8994 Explore P: 0.0100\n",
      "Episode: 958 Total reward: 199.0 Training d_loss: 1.1957 Training g_loss: 8.1251 Training q_loss: 49.0615 Explore P: 0.0100\n",
      "Episode: 959 Total reward: 199.0 Training d_loss: 1.1477 Training g_loss: 1.6436 Training q_loss: 165.5539 Explore P: 0.0100\n",
      "Episode: 960 Total reward: 199.0 Training d_loss: 1.3010 Training g_loss: 0.8891 Training q_loss: 179.2717 Explore P: 0.0100\n",
      "Episode: 961 Total reward: 199.0 Training d_loss: 1.2923 Training g_loss: 0.7396 Training q_loss: 150.4399 Explore P: 0.0100\n",
      "Episode: 962 Total reward: 199.0 Training d_loss: 1.2775 Training g_loss: 4.5170 Training q_loss: 111.9923 Explore P: 0.0100\n",
      "Episode: 963 Total reward: 199.0 Training d_loss: 1.3727 Training g_loss: 0.6838 Training q_loss: 131.0627 Explore P: 0.0100\n",
      "Episode: 964 Total reward: 199.0 Training d_loss: 1.1942 Training g_loss: 7.6599 Training q_loss: 27.6889 Explore P: 0.0100\n",
      "Episode: 965 Total reward: 199.0 Training d_loss: 1.3761 Training g_loss: 0.6903 Training q_loss: 235.9246 Explore P: 0.0100\n",
      "Episode: 966 Total reward: 199.0 Training d_loss: 1.3019 Training g_loss: 0.7475 Training q_loss: 74.3365 Explore P: 0.0100\n",
      "Episode: 967 Total reward: 199.0 Training d_loss: 1.3008 Training g_loss: 2.7618 Training q_loss: 87.5981 Explore P: 0.0100\n",
      "Episode: 968 Total reward: 199.0 Training d_loss: 1.3277 Training g_loss: 0.8583 Training q_loss: 120.4121 Explore P: 0.0100\n",
      "Episode: 969 Total reward: 171.0 Training d_loss: 1.2336 Training g_loss: 4.9083 Training q_loss: 147.7400 Explore P: 0.0100\n",
      "Episode: 970 Total reward: 159.0 Training d_loss: 1.1527 Training g_loss: 8.0924 Training q_loss: 86.5193 Explore P: 0.0100\n",
      "Episode: 971 Total reward: 133.0 Training d_loss: 1.3430 Training g_loss: 4.0645 Training q_loss: 147.2867 Explore P: 0.0100\n",
      "Episode: 972 Total reward: 61.0 Training d_loss: 1.2811 Training g_loss: 4.1457 Training q_loss: 41.9185 Explore P: 0.0100\n",
      "Episode: 973 Total reward: 96.0 Training d_loss: 1.3662 Training g_loss: 4.0630 Training q_loss: 123.0622 Explore P: 0.0100\n",
      "Episode: 974 Total reward: 104.0 Training d_loss: 1.3185 Training g_loss: 0.6894 Training q_loss: 32.0781 Explore P: 0.0100\n",
      "Episode: 975 Total reward: 199.0 Training d_loss: 1.1403 Training g_loss: 2.1857 Training q_loss: 60.0412 Explore P: 0.0100\n",
      "Episode: 976 Total reward: 86.0 Training d_loss: 1.2174 Training g_loss: 1.2333 Training q_loss: 243.7268 Explore P: 0.0100\n",
      "Episode: 977 Total reward: 189.0 Training d_loss: 1.2885 Training g_loss: 3.2235 Training q_loss: 132.0479 Explore P: 0.0100\n",
      "Episode: 978 Total reward: 131.0 Training d_loss: 1.2513 Training g_loss: 0.9853 Training q_loss: 76.2131 Explore P: 0.0100\n",
      "Episode: 979 Total reward: 135.0 Training d_loss: 1.0805 Training g_loss: 9.7751 Training q_loss: 238.5090 Explore P: 0.0100\n",
      "Episode: 980 Total reward: 121.0 Training d_loss: 1.3920 Training g_loss: 0.9561 Training q_loss: 63.0032 Explore P: 0.0100\n",
      "Episode: 981 Total reward: 106.0 Training d_loss: 1.3742 Training g_loss: 0.7375 Training q_loss: 186.8910 Explore P: 0.0100\n",
      "Episode: 982 Total reward: 187.0 Training d_loss: 1.3364 Training g_loss: 0.7155 Training q_loss: 21076.8945 Explore P: 0.0100\n",
      "Episode: 983 Total reward: 134.0 Training d_loss: 1.2687 Training g_loss: 0.8621 Training q_loss: 83.8092 Explore P: 0.0100\n",
      "Episode: 984 Total reward: 199.0 Training d_loss: 1.4261 Training g_loss: 0.6354 Training q_loss: 148.6485 Explore P: 0.0100\n",
      "Episode: 985 Total reward: 148.0 Training d_loss: 1.3752 Training g_loss: 0.7076 Training q_loss: 100.5442 Explore P: 0.0100\n",
      "Episode: 986 Total reward: 199.0 Training d_loss: 1.3258 Training g_loss: 0.8536 Training q_loss: 57.3121 Explore P: 0.0100\n",
      "Episode: 987 Total reward: 195.0 Training d_loss: 1.2060 Training g_loss: 1.2147 Training q_loss: 83.6204 Explore P: 0.0100\n",
      "Episode: 988 Total reward: 159.0 Training d_loss: 1.3657 Training g_loss: 0.6594 Training q_loss: 48.8305 Explore P: 0.0100\n",
      "Episode: 989 Total reward: 199.0 Training d_loss: 1.3074 Training g_loss: 2.2574 Training q_loss: 197.9986 Explore P: 0.0100\n",
      "Episode: 990 Total reward: 199.0 Training d_loss: 1.3847 Training g_loss: 0.5610 Training q_loss: 84.5695 Explore P: 0.0100\n",
      "Episode: 991 Total reward: 199.0 Training d_loss: 1.2269 Training g_loss: 4.4527 Training q_loss: 105.7698 Explore P: 0.0100\n",
      "Episode: 992 Total reward: 199.0 Training d_loss: 1.3222 Training g_loss: 0.6097 Training q_loss: 104.8756 Explore P: 0.0100\n",
      "Episode: 993 Total reward: 179.0 Training d_loss: 1.3643 Training g_loss: 0.5451 Training q_loss: 182.8521 Explore P: 0.0100\n",
      "Episode: 994 Total reward: 199.0 Training d_loss: 1.2227 Training g_loss: 0.9716 Training q_loss: 102.8026 Explore P: 0.0100\n",
      "Episode: 995 Total reward: 191.0 Training d_loss: 1.3042 Training g_loss: 1.2514 Training q_loss: 496.6696 Explore P: 0.0100\n",
      "Episode: 996 Total reward: 199.0 Training d_loss: 1.2858 Training g_loss: 4.1991 Training q_loss: 350.1005 Explore P: 0.0100\n",
      "Episode: 997 Total reward: 199.0 Training d_loss: 1.2409 Training g_loss: 4.2330 Training q_loss: 61.9278 Explore P: 0.0100\n",
      "Episode: 998 Total reward: 199.0 Training d_loss: 1.2435 Training g_loss: 1.2653 Training q_loss: 97.6943 Explore P: 0.0100\n",
      "Episode: 999 Total reward: 199.0 Training d_loss: 1.2447 Training g_loss: 1.3335 Training q_loss: 161.8474 Explore P: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards list for plotting\n",
    "rewards_list = []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward = 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                \n",
    "                # total rewards for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits, next_rewards_fake = sess.run([model.actions_logits, model.rewards_fake], feed_dict)\n",
    "\n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation\n",
    "            targetQs = next_rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "            # print('DEBUGGING', targetQs.shape, next_rewards_fake.shape, next_actions_logits.shape, np.max(next_actions_logits, axis=1).shape)\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.targetQs: targetQs}\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/DQAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cartpole.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for ep in range(test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            # env.render() \n",
    "            \n",
    "            # Get action from Q-network\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            targetQ = sess.run(model.outputs, feed_dict)\n",
    "            action = np.argmax(targetQ)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
