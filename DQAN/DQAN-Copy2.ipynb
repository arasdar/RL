{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQAN (Deep Q-Adverserial Nets): DQN (Deep Q-Nets) + GAN (Gen. Adv. Nets)\n",
    "\n",
    "In this notebook, we'll combine a DQN (deep Q-net) with GAN (generative adverserial net) that can learn to play games through reinforcement learning without any reward function. We'll call this network DQAN (deep Q adverserial net). \n",
    "Adverserial nets learn to maximize the current reward based the past rewards.\n",
    "Q-net learns to maximize the future rewards based on the current reward.\n",
    "Given a task and known when the task is done or failed, we should be able to learn the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[-0.00090499 -0.1928352  -0.03145304  0.32617311] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.0047617   0.00272012 -0.02492958  0.02373954] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00470729  0.19819055 -0.02445479 -0.27670353] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00074348  0.00342587 -0.02998886  0.00816714] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00067497  0.19896478 -0.02982552 -0.29382475] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00330433  0.39449898 -0.03570201 -0.59576301] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01119431  0.59010192 -0.04761727 -0.89947461] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02299635  0.78583572 -0.06560676 -1.20673671] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.03871306  0.98174112 -0.0897415  -1.51923717] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.05834789  1.17782615 -0.12012624 -1.83852972] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.177826153139452 -1.838529723915487\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    # Given data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Actions as output\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # Target Q values for training\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: qfunction/encoder\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits_actions)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: generator/ decoder: actions can be given actions, generated actions\n",
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: descriminator/reward function\n",
    "def discriminator(states, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, action_size, hidden_size, actions, targetQs, state_size, next_states, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param states: real current input states or observations given\n",
    "    :param actions: real actions given\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # Q-learning: Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_logits = generator(actions=actions_fake, state_size=state_size, hidden_size=hidden_size)\n",
    "    \n",
    "    # GAN: Discriminate between fake and real\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(states=next_states_fake, hidden_size=hidden_size, reuse=False)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(states=next_states_real, hidden_size=hidden_size, reuse=True)\n",
    "\n",
    "    # GAN: Adverserial training - D-learning\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # GAN: Adverserial training - G-learning\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, d_loss, g_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.d_loss, self.g_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, actions=self.actions, states=self.states, \n",
    "            next_states=self.next_states, hidden_size=hidden_size, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, \n",
    "                                                       q_loss=self.q_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = DQAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 3.0 Average reward fake: 0.5075462460517883 Average reward real: 0.5098732709884644 Training d_loss: 1.3820 Training g_loss: 0.6783 Training q_loss: 0.3934 Explore P: 0.9997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 16.0 Average reward fake: 0.4952053427696228 Average reward real: 0.489777147769928 Training d_loss: 1.3983 Training g_loss: 0.7068 Training q_loss: 0.3203 Explore P: 0.9981\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 11.0 Average reward fake: 0.4877958297729492 Average reward real: 0.49072346091270447 Training d_loss: 1.3810 Training g_loss: 0.7213 Training q_loss: 0.2446 Explore P: 0.9970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 27.0 Average reward fake: 0.46171092987060547 Average reward real: 0.5315499305725098 Training d_loss: 1.2563 Training g_loss: 0.7815 Training q_loss: 0.3817 Explore P: 0.9944\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 11.0 Average reward fake: 0.4698796272277832 Average reward real: 0.5184186697006226 Training d_loss: 1.2997 Training g_loss: 0.7636 Training q_loss: 1.9648 Explore P: 0.9933\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 20.0 Average reward fake: 0.5067384243011475 Average reward real: 0.4558272957801819 Training d_loss: 1.5201 Training g_loss: 0.7017 Training q_loss: 4.8710 Explore P: 0.9913\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 25.0 Average reward fake: 0.4986640512943268 Average reward real: 0.4757203459739685 Training d_loss: 1.4339 Training g_loss: 0.7040 Training q_loss: 1.8724 Explore P: 0.9889\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 19.0 Average reward fake: 0.4999375343322754 Average reward real: 0.5456620454788208 Training d_loss: 1.2998 Training g_loss: 0.7041 Training q_loss: 5.6905 Explore P: 0.9870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 20.0 Average reward fake: 0.42280906438827515 Average reward real: 0.6891927123069763 Training d_loss: 0.9242 Training g_loss: 0.8812 Training q_loss: 83.7140 Explore P: 0.9851\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 14.0 Average reward fake: 0.3453397750854492 Average reward real: 0.6732240915298462 Training d_loss: 0.8360 Training g_loss: 1.1064 Training q_loss: 159.3795 Explore P: 0.9837\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 14.0 Average reward fake: 0.46581631898880005 Average reward real: 0.6539368629455566 Training d_loss: 1.1049 Training g_loss: 0.8445 Training q_loss: 15.5429 Explore P: 0.9823\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 12.0 Average reward fake: 0.4640544056892395 Average reward real: 0.4773259162902832 Training d_loss: 1.4990 Training g_loss: 0.8376 Training q_loss: 17.3854 Explore P: 0.9812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 16.0 Average reward fake: 0.5487098097801208 Average reward real: 0.4310271143913269 Training d_loss: 1.6696 Training g_loss: 0.6354 Training q_loss: 60.9374 Explore P: 0.9796\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 16.0 Average reward fake: 0.5850109457969666 Average reward real: 0.3487083315849304 Training d_loss: 1.9359 Training g_loss: 0.5987 Training q_loss: 54.8245 Explore P: 0.9781\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 47.0 Average reward fake: 0.40790629386901855 Average reward real: 0.6499353647232056 Training d_loss: 0.9892 Training g_loss: 0.9485 Training q_loss: 384.7792 Explore P: 0.9735\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 11.0 Average reward fake: 0.5244015455245972 Average reward real: 0.593265175819397 Training d_loss: 1.3105 Training g_loss: 0.7214 Training q_loss: 99.1789 Explore P: 0.9725\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 8.0 Average reward fake: 0.4558540880680084 Average reward real: 0.49047064781188965 Training d_loss: 1.3565 Training g_loss: 0.8666 Training q_loss: 739.3341 Explore P: 0.9717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 14.0 Average reward fake: 0.3533034026622772 Average reward real: 0.4946940541267395 Training d_loss: 1.1734 Training g_loss: 1.0668 Training q_loss: 3787.9805 Explore P: 0.9704\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 17.0 Average reward fake: 0.5401115417480469 Average reward real: 0.5767245888710022 Training d_loss: 1.3290 Training g_loss: 0.7305 Training q_loss: 90.1616 Explore P: 0.9687\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 13.0 Average reward fake: 0.4430743157863617 Average reward real: 0.39372479915618896 Training d_loss: 1.5271 Training g_loss: 0.8813 Training q_loss: 66.2973 Explore P: 0.9675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 17.0 Average reward fake: 0.2909243702888489 Average reward real: 0.528006911277771 Training d_loss: 1.0208 Training g_loss: 1.2646 Training q_loss: 1525.6887 Explore P: 0.9659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 12.0 Average reward fake: 0.2170293778181076 Average reward real: 0.7449947595596313 Training d_loss: 0.5567 Training g_loss: 1.5790 Training q_loss: 6.2938 Explore P: 0.9647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 18.0 Average reward fake: 0.3788483440876007 Average reward real: 0.6363021731376648 Training d_loss: 0.9677 Training g_loss: 1.1263 Training q_loss: 10.2285 Explore P: 0.9630\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 22.0 Average reward fake: 0.5436132550239563 Average reward real: 0.5609828233718872 Training d_loss: 1.4273 Training g_loss: 0.6778 Training q_loss: 7.1357 Explore P: 0.9609\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 19.0 Average reward fake: 0.5668443441390991 Average reward real: 0.4019160270690918 Training d_loss: 1.7839 Training g_loss: 0.6101 Training q_loss: 2.5507 Explore P: 0.9591\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 9.0 Average reward fake: 0.4306960999965668 Average reward real: 0.4785185754299164 Training d_loss: 1.3033 Training g_loss: 0.8863 Training q_loss: 4.3457 Explore P: 0.9582\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 9.0 Average reward fake: 0.2929706871509552 Average reward real: 0.5580456852912903 Training d_loss: 0.9336 Training g_loss: 1.2783 Training q_loss: 472.4407 Explore P: 0.9574\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 17.0 Average reward fake: 0.3206610083580017 Average reward real: 0.7034006714820862 Training d_loss: 0.7607 Training g_loss: 1.1946 Training q_loss: 2.4504 Explore P: 0.9558\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 38.0 Average reward fake: 0.5130559206008911 Average reward real: 0.40665286779403687 Training d_loss: 1.6680 Training g_loss: 0.6972 Training q_loss: 1.7046 Explore P: 0.9522\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 23.0 Average reward fake: 0.4624517560005188 Average reward real: 0.5323482155799866 Training d_loss: 1.2679 Training g_loss: 0.7936 Training q_loss: 180.2702 Explore P: 0.9500\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 17.0 Average reward fake: 0.5085260272026062 Average reward real: 0.5572590231895447 Training d_loss: 1.3080 Training g_loss: 0.6857 Training q_loss: 2.9431 Explore P: 0.9484\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 18.0 Average reward fake: 0.5289517641067505 Average reward real: 0.5093077421188354 Training d_loss: 1.4412 Training g_loss: 0.6454 Training q_loss: 2.9541 Explore P: 0.9467\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 15.0 Average reward fake: 0.5358701944351196 Average reward real: 0.4906995892524719 Training d_loss: 1.4892 Training g_loss: 0.6266 Training q_loss: 154.9519 Explore P: 0.9453\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 21.0 Average reward fake: 0.5625723600387573 Average reward real: 0.5010421276092529 Training d_loss: 1.5200 Training g_loss: 0.5815 Training q_loss: 3.0161 Explore P: 0.9434\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 12.0 Average reward fake: 0.5449956059455872 Average reward real: 0.4855140745639801 Training d_loss: 1.5103 Training g_loss: 0.6132 Training q_loss: 128.4251 Explore P: 0.9423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 17.0 Average reward fake: 0.5092687010765076 Average reward real: 0.5099461078643799 Training d_loss: 1.3859 Training g_loss: 0.6836 Training q_loss: 2.1500 Explore P: 0.9407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 12.0 Average reward fake: 0.47567886114120483 Average reward real: 0.5235112905502319 Training d_loss: 1.2986 Training g_loss: 0.7530 Training q_loss: 2.2753 Explore P: 0.9396\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 10.0 Average reward fake: 0.4638034701347351 Average reward real: 0.5195318460464478 Training d_loss: 1.2884 Training g_loss: 0.7741 Training q_loss: 2.7853 Explore P: 0.9386\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 12.0 Average reward fake: 0.4654296934604645 Average reward real: 0.5888561010360718 Training d_loss: 1.1668 Training g_loss: 0.7741 Training q_loss: 4.0473 Explore P: 0.9375\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 24.0 Average reward fake: 0.4745856821537018 Average reward real: 0.54789799451828 Training d_loss: 1.2699 Training g_loss: 0.7511 Training q_loss: 88.3379 Explore P: 0.9353\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 21.0 Average reward fake: 0.48053842782974243 Average reward real: 0.5449432730674744 Training d_loss: 1.2705 Training g_loss: 0.7424 Training q_loss: 2.8483 Explore P: 0.9333\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 10.0 Average reward fake: 0.5017991065979004 Average reward real: 0.5075013041496277 Training d_loss: 1.3872 Training g_loss: 0.6937 Training q_loss: 2.8982 Explore P: 0.9324\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 15.0 Average reward fake: 0.549674391746521 Average reward real: 0.5019339323043823 Training d_loss: 1.4919 Training g_loss: 0.6089 Training q_loss: 2.4936 Explore P: 0.9310\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 9.0 Average reward fake: 0.5548855066299438 Average reward real: 0.44376903772354126 Training d_loss: 1.6227 Training g_loss: 0.5959 Training q_loss: 2.1941 Explore P: 0.9302\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 18.0 Average reward fake: 0.50071120262146 Average reward real: 0.51701420545578 Training d_loss: 1.3557 Training g_loss: 0.7033 Training q_loss: 3.4710 Explore P: 0.9286\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 15.0 Average reward fake: 0.4541109502315521 Average reward real: 0.5410007238388062 Training d_loss: 1.2267 Training g_loss: 0.7951 Training q_loss: 4.4358 Explore P: 0.9272\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 25.0 Average reward fake: 0.46265727281570435 Average reward real: 0.5492198467254639 Training d_loss: 1.2408 Training g_loss: 0.7808 Training q_loss: 1.7422 Explore P: 0.9249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 10.0 Average reward fake: 0.4608004689216614 Average reward real: 0.5248970985412598 Training d_loss: 1.2816 Training g_loss: 0.7820 Training q_loss: 4.1887 Explore P: 0.9240\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 16.0 Average reward fake: 0.5114399194717407 Average reward real: 0.5420728921890259 Training d_loss: 1.3418 Training g_loss: 0.6749 Training q_loss: 24.9609 Explore P: 0.9225\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 27.0 Average reward fake: 0.4999132752418518 Average reward real: 0.4971957206726074 Training d_loss: 1.3957 Training g_loss: 0.6983 Training q_loss: 3.3533 Explore P: 0.9201\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 33.0 Average reward fake: 0.48695802688598633 Average reward real: 0.4936605393886566 Training d_loss: 1.3800 Training g_loss: 0.7265 Training q_loss: 43.4300 Explore P: 0.9171\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 18.0 Average reward fake: 0.47506850957870483 Average reward real: 0.4996282458305359 Training d_loss: 1.3735 Training g_loss: 0.7755 Training q_loss: 22.8034 Explore P: 0.9154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 34.0 Average reward fake: 0.46177929639816284 Average reward real: 0.45220357179641724 Training d_loss: 1.4218 Training g_loss: 0.8096 Training q_loss: 3.0546 Explore P: 0.9124\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 18.0 Average reward fake: 0.4472591280937195 Average reward real: 0.6375129818916321 Training d_loss: 1.0649 Training g_loss: 0.8345 Training q_loss: 3.6050 Explore P: 0.9107\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 30.0 Average reward fake: 0.4535748064517975 Average reward real: 0.4911019206047058 Training d_loss: 1.3804 Training g_loss: 0.8214 Training q_loss: 8.8333 Explore P: 0.9080\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 16.0 Average reward fake: 0.5513678193092346 Average reward real: 0.5085521936416626 Training d_loss: 1.5358 Training g_loss: 0.6159 Training q_loss: 42.3961 Explore P: 0.9066\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 45.0 Average reward fake: 0.5267537832260132 Average reward real: 0.4999713897705078 Training d_loss: 1.4451 Training g_loss: 0.6437 Training q_loss: 6.8488 Explore P: 0.9026\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 14.0 Average reward fake: 0.5059841275215149 Average reward real: 0.5198938846588135 Training d_loss: 1.3602 Training g_loss: 0.6898 Training q_loss: 77.2731 Explore P: 0.9013\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 18.0 Average reward fake: 0.4433358609676361 Average reward real: 0.5264477133750916 Training d_loss: 1.2362 Training g_loss: 0.8329 Training q_loss: 2.8031 Explore P: 0.8997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 21.0 Average reward fake: 0.44410020112991333 Average reward real: 0.515873372554779 Training d_loss: 1.2819 Training g_loss: 0.8234 Training q_loss: 4.8416 Explore P: 0.8979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 29.0 Average reward fake: 0.45968136191368103 Average reward real: 0.46480950713157654 Training d_loss: 1.4465 Training g_loss: 0.7944 Training q_loss: 36.7491 Explore P: 0.8953\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 15.0 Average reward fake: 0.509713888168335 Average reward real: 0.5024805665016174 Training d_loss: 1.4290 Training g_loss: 0.6887 Training q_loss: 1.4717 Explore P: 0.8940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 22.0 Average reward fake: 0.5473809242248535 Average reward real: 0.509141206741333 Training d_loss: 1.4811 Training g_loss: 0.6163 Training q_loss: 33.7547 Explore P: 0.8920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 15.0 Average reward fake: 0.5194061994552612 Average reward real: 0.48037999868392944 Training d_loss: 1.4667 Training g_loss: 0.6676 Training q_loss: 82.2443 Explore P: 0.8907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 14.0 Average reward fake: 0.4154403805732727 Average reward real: 0.5169650912284851 Training d_loss: 1.2032 Training g_loss: 0.8976 Training q_loss: 40.8568 Explore P: 0.8895\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 28.0 Average reward fake: 0.4552479684352875 Average reward real: 0.5684057474136353 Training d_loss: 1.2157 Training g_loss: 0.7960 Training q_loss: 0.9643 Explore P: 0.8870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 15.0 Average reward fake: 0.5056203603744507 Average reward real: 0.48646292090415955 Training d_loss: 1.4694 Training g_loss: 0.6967 Training q_loss: 34.5332 Explore P: 0.8857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 17.0 Average reward fake: 0.509868860244751 Average reward real: 0.47668886184692383 Training d_loss: 1.5054 Training g_loss: 0.6806 Training q_loss: 3.3487 Explore P: 0.8842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 19.0 Average reward fake: 0.5380724668502808 Average reward real: 0.5126553773880005 Training d_loss: 1.4674 Training g_loss: 0.6311 Training q_loss: 2.5696 Explore P: 0.8825\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 16.0 Average reward fake: 0.49061718583106995 Average reward real: 0.47039538621902466 Training d_loss: 1.4334 Training g_loss: 0.7205 Training q_loss: 16.4356 Explore P: 0.8811\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 30.0 Average reward fake: 0.4875944256782532 Average reward real: 0.5174214839935303 Training d_loss: 1.3282 Training g_loss: 0.7214 Training q_loss: 35.9657 Explore P: 0.8785\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 46.0 Average reward fake: 0.4603622853755951 Average reward real: 0.5122183561325073 Training d_loss: 1.3035 Training g_loss: 0.7829 Training q_loss: 32.2211 Explore P: 0.8745\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 66.0 Average reward fake: 0.4634663462638855 Average reward real: 0.49674588441848755 Training d_loss: 1.3554 Training g_loss: 0.7773 Training q_loss: 21.9912 Explore P: 0.8689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 28.0 Average reward fake: 0.48343682289123535 Average reward real: 0.5128352642059326 Training d_loss: 1.3333 Training g_loss: 0.7323 Training q_loss: 18.7174 Explore P: 0.8665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 68.0 Average reward fake: 0.5210146307945251 Average reward real: 0.4457215368747711 Training d_loss: 1.5695 Training g_loss: 0.6568 Training q_loss: 2.6288 Explore P: 0.8607\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 16.0 Average reward fake: 0.48649734258651733 Average reward real: 0.4878918528556824 Training d_loss: 1.4096 Training g_loss: 0.7364 Training q_loss: 1.9595 Explore P: 0.8593\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 17.0 Average reward fake: 0.45298337936401367 Average reward real: 0.506237804889679 Training d_loss: 1.2949 Training g_loss: 0.7960 Training q_loss: 1.1835 Explore P: 0.8579\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 20.0 Average reward fake: 0.4151034355163574 Average reward real: 0.6034284234046936 Training d_loss: 1.0543 Training g_loss: 0.8930 Training q_loss: 2.3180 Explore P: 0.8562\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 24.0 Average reward fake: 0.4479575753211975 Average reward real: 0.5869371294975281 Training d_loss: 1.1764 Training g_loss: 0.8153 Training q_loss: 5.2836 Explore P: 0.8541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 33.0 Average reward fake: 0.4623483121395111 Average reward real: 0.5718841552734375 Training d_loss: 1.2384 Training g_loss: 0.7911 Training q_loss: 0.9501 Explore P: 0.8513\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 88.0 Average reward fake: 0.3907202482223511 Average reward real: 0.6018367409706116 Training d_loss: 1.0155 Training g_loss: 0.9654 Training q_loss: 14.6612 Explore P: 0.8440\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 25.0 Average reward fake: 0.4377671778202057 Average reward real: 0.651684582233429 Training d_loss: 1.0635 Training g_loss: 0.9207 Training q_loss: 2.9546 Explore P: 0.8419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 13.0 Average reward fake: 0.44957834482192993 Average reward real: 0.5088674426078796 Training d_loss: 1.3754 Training g_loss: 0.8242 Training q_loss: 2.9391 Explore P: 0.8408\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 49.0 Average reward fake: 0.5762022137641907 Average reward real: 0.47333043813705444 Training d_loss: 1.6115 Training g_loss: 0.5688 Training q_loss: 13.2034 Explore P: 0.8368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 35.0 Average reward fake: 0.4848043918609619 Average reward real: 0.6708894968032837 Training d_loss: 1.0849 Training g_loss: 0.7517 Training q_loss: 3.3183 Explore P: 0.8339\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 16.0 Average reward fake: 0.5453301668167114 Average reward real: 0.5470648407936096 Training d_loss: 1.4547 Training g_loss: 0.6344 Training q_loss: 3.4242 Explore P: 0.8325\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 24.0 Average reward fake: 0.5343902707099915 Average reward real: 0.5761639475822449 Training d_loss: 1.3328 Training g_loss: 0.6668 Training q_loss: 8.1396 Explore P: 0.8306\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 12.0 Average reward fake: 0.4420132040977478 Average reward real: 0.5007001757621765 Training d_loss: 1.2762 Training g_loss: 0.8315 Training q_loss: 27.9302 Explore P: 0.8296\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 40.0 Average reward fake: 0.5538844466209412 Average reward real: 0.5079801678657532 Training d_loss: 1.5224 Training g_loss: 0.6183 Training q_loss: 1.5847 Explore P: 0.8263\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 28.0 Average reward fake: 0.45022836327552795 Average reward real: 0.4289051592350006 Training d_loss: 1.4465 Training g_loss: 0.8010 Training q_loss: 1.1624 Explore P: 0.8240\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 24.0 Average reward fake: 0.5057294368743896 Average reward real: 0.46929579973220825 Training d_loss: 1.4699 Training g_loss: 0.6905 Training q_loss: 24.8547 Explore P: 0.8221\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 49.0 Average reward fake: 0.4415866732597351 Average reward real: 0.4876983165740967 Training d_loss: 1.3202 Training g_loss: 0.8381 Training q_loss: 50.3055 Explore P: 0.8181\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 44.0 Average reward fake: 0.4072880148887634 Average reward real: 0.49105969071388245 Training d_loss: 1.2476 Training g_loss: 0.9174 Training q_loss: 5.2514 Explore P: 0.8146\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 66.0 Average reward fake: 0.5631589889526367 Average reward real: 0.49262523651123047 Training d_loss: 1.6096 Training g_loss: 0.5948 Training q_loss: 5.6165 Explore P: 0.8093\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 25.0 Average reward fake: 0.5648524761199951 Average reward real: 0.5596535801887512 Training d_loss: 1.4189 Training g_loss: 0.5850 Training q_loss: 2.4922 Explore P: 0.8073\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 32.0 Average reward fake: 0.561259925365448 Average reward real: 0.5860286355018616 Training d_loss: 1.3890 Training g_loss: 0.6021 Training q_loss: 2.3127 Explore P: 0.8047\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 34.0 Average reward fake: 0.4772149622440338 Average reward real: 0.4737885892391205 Training d_loss: 1.4221 Training g_loss: 0.8042 Training q_loss: 2.7164 Explore P: 0.8020\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 22.0 Average reward fake: 0.28150826692581177 Average reward real: 0.6169878244400024 Training d_loss: 0.8183 Training g_loss: 1.3504 Training q_loss: 14.5286 Explore P: 0.8003\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 19.0 Average reward fake: 0.3070385456085205 Average reward real: 0.5205233097076416 Training d_loss: 1.3160 Training g_loss: 1.4602 Training q_loss: 71.7661 Explore P: 0.7988\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 22.0 Average reward fake: 0.5976814031600952 Average reward real: 0.41035470366477966 Training d_loss: 2.1418 Training g_loss: 0.5237 Training q_loss: 3.0021 Explore P: 0.7971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 36.0 Average reward fake: 0.5343033075332642 Average reward real: 0.4875965714454651 Training d_loss: 1.4858 Training g_loss: 0.6373 Training q_loss: 17.9350 Explore P: 0.7942\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 19.0 Average reward fake: 0.48433613777160645 Average reward real: 0.5077880620956421 Training d_loss: 1.3540 Training g_loss: 0.7467 Training q_loss: 82.2672 Explore P: 0.7927\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 22.0 Average reward fake: 0.42822304368019104 Average reward real: 0.5106856822967529 Training d_loss: 1.2704 Training g_loss: 0.8638 Training q_loss: 0.6500 Explore P: 0.7910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 44.0 Average reward fake: 0.43232399225234985 Average reward real: 0.48665112257003784 Training d_loss: 1.3078 Training g_loss: 0.8718 Training q_loss: 62.5346 Explore P: 0.7876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 22.0 Average reward fake: 0.31862783432006836 Average reward real: 0.48880448937416077 Training d_loss: 1.1010 Training g_loss: 1.1705 Training q_loss: 3.1933 Explore P: 0.7859\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 15.0 Average reward fake: 0.3229600787162781 Average reward real: 0.584833562374115 Training d_loss: 0.9385 Training g_loss: 1.1605 Training q_loss: 2.6347 Explore P: 0.7847\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 27.0 Average reward fake: 0.4904860854148865 Average reward real: 0.6091447472572327 Training d_loss: 1.2032 Training g_loss: 0.7443 Training q_loss: 49.7927 Explore P: 0.7826\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 10.0 Average reward fake: 0.5671337842941284 Average reward real: 0.45922261476516724 Training d_loss: 1.6398 Training g_loss: 0.5746 Training q_loss: 1.7536 Explore P: 0.7819\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 32.0 Average reward fake: 0.6681567430496216 Average reward real: 0.5061001181602478 Training d_loss: 1.7845 Training g_loss: 0.4153 Training q_loss: 3.7540 Explore P: 0.7794\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 30.0 Average reward fake: 0.4808040261268616 Average reward real: 0.5905645489692688 Training d_loss: 1.1877 Training g_loss: 0.7460 Training q_loss: 3.2897 Explore P: 0.7771\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 10.0 Average reward fake: 0.4805876314640045 Average reward real: 0.6074345707893372 Training d_loss: 1.1640 Training g_loss: 0.7467 Training q_loss: 4.0790 Explore P: 0.7763\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 65.0 Average reward fake: 0.44099053740501404 Average reward real: 0.5654643177986145 Training d_loss: 1.1542 Training g_loss: 0.8348 Training q_loss: 5.2215 Explore P: 0.7714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 37.0 Average reward fake: 0.5223596692085266 Average reward real: 0.47402477264404297 Training d_loss: 1.4951 Training g_loss: 0.6535 Training q_loss: 2.5136 Explore P: 0.7685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 23.0 Average reward fake: 0.5556254386901855 Average reward real: 0.4814903736114502 Training d_loss: 1.5562 Training g_loss: 0.5941 Training q_loss: 2.5690 Explore P: 0.7668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 17.0 Average reward fake: 0.5060232877731323 Average reward real: 0.5025269985198975 Training d_loss: 1.4024 Training g_loss: 0.6799 Training q_loss: 34.5601 Explore P: 0.7655\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 65.0 Average reward fake: 0.52766352891922 Average reward real: 0.43797340989112854 Training d_loss: 1.5801 Training g_loss: 0.6674 Training q_loss: 3.9428 Explore P: 0.7606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 18.0 Average reward fake: 0.42025095224380493 Average reward real: 0.4975675940513611 Training d_loss: 1.2628 Training g_loss: 0.9068 Training q_loss: 5.6436 Explore P: 0.7593\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 63.0 Average reward fake: 0.5037425756454468 Average reward real: 0.44829076528549194 Training d_loss: 1.5070 Training g_loss: 0.7028 Training q_loss: 5.0046 Explore P: 0.7546\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 19.0 Average reward fake: 0.3854878842830658 Average reward real: 0.5267564058303833 Training d_loss: 1.1502 Training g_loss: 0.9930 Training q_loss: 49.7958 Explore P: 0.7532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 29.0 Average reward fake: 0.562842845916748 Average reward real: 0.5177631974220276 Training d_loss: 1.5023 Training g_loss: 0.5880 Training q_loss: 3.7133 Explore P: 0.7510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 51.0 Average reward fake: 0.47928720712661743 Average reward real: 0.5224600434303284 Training d_loss: 1.3018 Training g_loss: 0.7387 Training q_loss: 2.6224 Explore P: 0.7472\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 44.0 Average reward fake: 0.48186206817626953 Average reward real: 0.5509284138679504 Training d_loss: 1.2564 Training g_loss: 0.7380 Training q_loss: 63.7904 Explore P: 0.7440\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 16.0 Average reward fake: 0.5226058959960938 Average reward real: 0.5113396644592285 Training d_loss: 1.4139 Training g_loss: 0.6526 Training q_loss: 1.8535 Explore P: 0.7428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 65.0 Average reward fake: 0.5237872004508972 Average reward real: 0.4630652368068695 Training d_loss: 1.5145 Training g_loss: 0.6574 Training q_loss: 2.1182 Explore P: 0.7381\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 22.0 Average reward fake: 0.43998169898986816 Average reward real: 0.4913291037082672 Training d_loss: 1.2918 Training g_loss: 0.8349 Training q_loss: 1.3172 Explore P: 0.7365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 18.0 Average reward fake: 0.471731036901474 Average reward real: 0.5170130729675293 Training d_loss: 1.3067 Training g_loss: 0.7681 Training q_loss: 5.3407 Explore P: 0.7352\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 27.0 Average reward fake: 0.48716744780540466 Average reward real: 0.4813947081565857 Training d_loss: 1.4168 Training g_loss: 0.7225 Training q_loss: 3.2984 Explore P: 0.7332\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 102.0 Average reward fake: 0.4418913722038269 Average reward real: 0.5690455436706543 Training d_loss: 1.1885 Training g_loss: 0.8545 Training q_loss: 73.9392 Explore P: 0.7259\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 15.0 Average reward fake: 0.5232558846473694 Average reward real: 0.5564731359481812 Training d_loss: 1.4121 Training g_loss: 0.6917 Training q_loss: 3.7674 Explore P: 0.7248\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 45.0 Average reward fake: 0.5271745324134827 Average reward real: 0.4197794497013092 Training d_loss: 1.6218 Training g_loss: 0.6620 Training q_loss: 4.3694 Explore P: 0.7216\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 70.0 Average reward fake: 0.5094336867332458 Average reward real: 0.5590265989303589 Training d_loss: 1.3726 Training g_loss: 0.7131 Training q_loss: 3.3855 Explore P: 0.7166\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 11.0 Average reward fake: 0.5710980892181396 Average reward real: 0.47616061568260193 Training d_loss: 1.6756 Training g_loss: 0.5746 Training q_loss: 4.2837 Explore P: 0.7159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 31.0 Average reward fake: 0.5690445303916931 Average reward real: 0.5453413128852844 Training d_loss: 1.4512 Training g_loss: 0.5771 Training q_loss: 66.8377 Explore P: 0.7137\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 46.0 Average reward fake: 0.4702112078666687 Average reward real: 0.5048040151596069 Training d_loss: 1.3388 Training g_loss: 0.7768 Training q_loss: 5.1465 Explore P: 0.7104\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 48.0 Average reward fake: 0.3951827883720398 Average reward real: 0.47069066762924194 Training d_loss: 1.2682 Training g_loss: 0.9448 Training q_loss: 1.8922 Explore P: 0.7071\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 47.0 Average reward fake: 0.4219974875450134 Average reward real: 0.5864468812942505 Training d_loss: 1.0862 Training g_loss: 0.8797 Training q_loss: 2.0686 Explore P: 0.7038\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 15.0 Average reward fake: 0.46453553438186646 Average reward real: 0.5237237215042114 Training d_loss: 1.2964 Training g_loss: 0.7820 Training q_loss: 4.4124 Explore P: 0.7028\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 82.0 Average reward fake: 0.5242015719413757 Average reward real: 0.5083555579185486 Training d_loss: 1.4220 Training g_loss: 0.6479 Training q_loss: 6.6507 Explore P: 0.6971\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 75.0 Average reward fake: 0.49482256174087524 Average reward real: 0.600605845451355 Training d_loss: 1.1964 Training g_loss: 0.7105 Training q_loss: 4.3685 Explore P: 0.6920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 20.0 Average reward fake: 0.441974937915802 Average reward real: 0.5665594935417175 Training d_loss: 1.1631 Training g_loss: 0.8315 Training q_loss: 2.2545 Explore P: 0.6906\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 37.0 Average reward fake: 0.44489049911499023 Average reward real: 0.5291858911514282 Training d_loss: 1.2378 Training g_loss: 0.8007 Training q_loss: 113.7272 Explore P: 0.6881\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 14.0 Average reward fake: 0.4466039538383484 Average reward real: 0.572528064250946 Training d_loss: 1.1574 Training g_loss: 0.8150 Training q_loss: 6.6072 Explore P: 0.6872\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 20.0 Average reward fake: 0.4689335227012634 Average reward real: 0.5703941583633423 Training d_loss: 1.2092 Training g_loss: 0.7676 Training q_loss: 7.8801 Explore P: 0.6858\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 84.0 Average reward fake: 0.5167196989059448 Average reward real: 0.5377113223075867 Training d_loss: 1.4282 Training g_loss: 0.7185 Training q_loss: 6.2306 Explore P: 0.6802\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 38.0 Average reward fake: 0.4910746216773987 Average reward real: 0.5264020562171936 Training d_loss: 1.3202 Training g_loss: 0.7113 Training q_loss: 157.5406 Explore P: 0.6776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 103.0 Average reward fake: 0.4968436658382416 Average reward real: 0.4639247953891754 Training d_loss: 1.4588 Training g_loss: 0.6893 Training q_loss: 5.7883 Explore P: 0.6708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 64.0 Average reward fake: 0.51125168800354 Average reward real: 0.3856808543205261 Training d_loss: 1.7110 Training g_loss: 0.6698 Training q_loss: 4.4003 Explore P: 0.6666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 66.0 Average reward fake: 0.49820375442504883 Average reward real: 0.5156146287918091 Training d_loss: 1.3570 Training g_loss: 0.7037 Training q_loss: 6.2309 Explore P: 0.6622\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 33.0 Average reward fake: 0.4725703299045563 Average reward real: 0.5413480997085571 Training d_loss: 1.2599 Training g_loss: 0.7552 Training q_loss: 4.7128 Explore P: 0.6601\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 92.0 Average reward fake: 0.447326123714447 Average reward real: 0.38960331678390503 Training d_loss: 1.5601 Training g_loss: 0.8260 Training q_loss: 4.1327 Explore P: 0.6541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 76.0 Average reward fake: 0.458423912525177 Average reward real: 0.591476321220398 Training d_loss: 1.1490 Training g_loss: 0.8049 Training q_loss: 4.0064 Explore P: 0.6493\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 167.0 Average reward fake: 0.47898024320602417 Average reward real: 0.395972341299057 Training d_loss: 1.6500 Training g_loss: 0.7586 Training q_loss: 4.0901 Explore P: 0.6387\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 76.0 Average reward fake: 0.4578797221183777 Average reward real: 0.5611639022827148 Training d_loss: 1.1918 Training g_loss: 0.7866 Training q_loss: 8.9557 Explore P: 0.6339\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 58.0 Average reward fake: 0.522998571395874 Average reward real: 0.48927822709083557 Training d_loss: 1.4643 Training g_loss: 0.6712 Training q_loss: 415.8405 Explore P: 0.6303\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 103.0 Average reward fake: 0.5021473169326782 Average reward real: 0.4539298415184021 Training d_loss: 1.5149 Training g_loss: 0.7071 Training q_loss: 7.3166 Explore P: 0.6239\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 22.0 Average reward fake: 0.4835086762905121 Average reward real: 0.5152308940887451 Training d_loss: 1.3337 Training g_loss: 0.7266 Training q_loss: 5.0446 Explore P: 0.6226\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 29.0 Average reward fake: 0.47803276777267456 Average reward real: 0.5419520139694214 Training d_loss: 1.2671 Training g_loss: 0.7465 Training q_loss: 8.1670 Explore P: 0.6208\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 17.0 Average reward fake: 0.49195265769958496 Average reward real: 0.5289364457130432 Training d_loss: 1.3165 Training g_loss: 0.7180 Training q_loss: 4.6685 Explore P: 0.6198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 147.0 Average reward fake: 0.42957058548927307 Average reward real: 0.567907452583313 Training d_loss: 1.1392 Training g_loss: 0.8459 Training q_loss: 14.6405 Explore P: 0.6109\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 51.0 Average reward fake: 0.4923872947692871 Average reward real: 0.4739286005496979 Training d_loss: 1.4280 Training g_loss: 0.7010 Training q_loss: 4.8147 Explore P: 0.6078\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 15.0 Average reward fake: 0.5112699866294861 Average reward real: 0.5036092400550842 Training d_loss: 1.4158 Training g_loss: 0.6825 Training q_loss: 18.2254 Explore P: 0.6069\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 72.0 Average reward fake: 0.4261988699436188 Average reward real: 0.473207950592041 Training d_loss: 1.3437 Training g_loss: 0.9311 Training q_loss: 10.4884 Explore P: 0.6027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 165.0 Average reward fake: 0.44502711296081543 Average reward real: 0.5970150232315063 Training d_loss: 1.1535 Training g_loss: 0.8407 Training q_loss: 862.7224 Explore P: 0.5930\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 21.0 Average reward fake: 0.47253647446632385 Average reward real: 0.4282093644142151 Training d_loss: 1.5597 Training g_loss: 0.7420 Training q_loss: 112.7326 Explore P: 0.5917\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 74.0 Average reward fake: 0.5121086239814758 Average reward real: 0.4956692159175873 Training d_loss: 1.4288 Training g_loss: 0.6707 Training q_loss: 22.2597 Explore P: 0.5874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 21.0 Average reward fake: 0.5029188394546509 Average reward real: 0.5093151330947876 Training d_loss: 1.3747 Training g_loss: 0.6875 Training q_loss: 8.7820 Explore P: 0.5862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 138.0 Average reward fake: 0.4769342541694641 Average reward real: 0.5460297465324402 Training d_loss: 1.2651 Training g_loss: 0.8104 Training q_loss: 45.7342 Explore P: 0.5783\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 48.0 Average reward fake: 0.47547444701194763 Average reward real: 0.49145469069480896 Training d_loss: 1.3798 Training g_loss: 0.7556 Training q_loss: 10.5292 Explore P: 0.5756\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 156.0 Average reward fake: 0.549239993095398 Average reward real: 0.5386022925376892 Training d_loss: 1.4240 Training g_loss: 0.5992 Training q_loss: 21.3650 Explore P: 0.5669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 20.0 Average reward fake: 0.5115240812301636 Average reward real: 0.5258992910385132 Training d_loss: 1.3634 Training g_loss: 0.6831 Training q_loss: 12.8725 Explore P: 0.5657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 92.0 Average reward fake: 0.5332473516464233 Average reward real: 0.5002032518386841 Training d_loss: 1.4707 Training g_loss: 0.6612 Training q_loss: 691.9837 Explore P: 0.5607\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 96.0 Average reward fake: 0.5529931783676147 Average reward real: 0.4667319655418396 Training d_loss: 1.5930 Training g_loss: 0.6100 Training q_loss: 5.4675 Explore P: 0.5554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 170.0 Average reward fake: 0.4806954264640808 Average reward real: 0.3766948878765106 Training d_loss: 1.6666 Training g_loss: 0.7676 Training q_loss: 23.8829 Explore P: 0.5462\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 55.0 Average reward fake: 0.3907133936882019 Average reward real: 0.608792245388031 Training d_loss: 0.9944 Training g_loss: 0.9965 Training q_loss: 23.0348 Explore P: 0.5433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 38.0 Average reward fake: 0.6285675764083862 Average reward real: 0.40960603952407837 Training d_loss: 1.9567 Training g_loss: 0.4915 Training q_loss: 24.4256 Explore P: 0.5412\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 178.0 Average reward fake: 0.5158016085624695 Average reward real: 0.3999994695186615 Training d_loss: 1.6494 Training g_loss: 0.6762 Training q_loss: 27.4082 Explore P: 0.5319\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 72.0 Average reward fake: 0.49078044295310974 Average reward real: 0.36549997329711914 Training d_loss: 1.8426 Training g_loss: 0.7242 Training q_loss: 37.0282 Explore P: 0.5281\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 52.0 Average reward fake: 0.5715945959091187 Average reward real: 0.5422471761703491 Training d_loss: 1.4636 Training g_loss: 0.5774 Training q_loss: 75.4096 Explore P: 0.5254\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 138.0 Average reward fake: 0.5223649144172668 Average reward real: 0.4933384358882904 Training d_loss: 1.4470 Training g_loss: 0.6544 Training q_loss: 50.7597 Explore P: 0.5184\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 99.0 Average reward fake: 0.5095468759536743 Average reward real: 0.4658929407596588 Training d_loss: 1.5025 Training g_loss: 0.6756 Training q_loss: 6.5501 Explore P: 0.5134\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 122.0 Average reward fake: 0.4298607409000397 Average reward real: 0.5171598196029663 Training d_loss: 1.2261 Training g_loss: 0.8385 Training q_loss: 223.2468 Explore P: 0.5073\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 161.0 Average reward fake: 0.48590096831321716 Average reward real: 0.543528139591217 Training d_loss: 1.2796 Training g_loss: 0.7200 Training q_loss: 31.2757 Explore P: 0.4993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 52.0 Average reward fake: 0.5236743688583374 Average reward real: 0.47987836599349976 Training d_loss: 1.4800 Training g_loss: 0.6536 Training q_loss: 49.2614 Explore P: 0.4968\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 75.0 Average reward fake: 0.47027188539505005 Average reward real: 0.550248384475708 Training d_loss: 1.2486 Training g_loss: 0.7648 Training q_loss: 1523.1927 Explore P: 0.4931\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 199.0 Average reward fake: 0.48718738555908203 Average reward real: 0.6039396524429321 Training d_loss: 1.3191 Training g_loss: 0.8807 Training q_loss: 44.1095 Explore P: 0.4836\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 185.0 Average reward fake: 0.4385625720024109 Average reward real: 0.5325339436531067 Training d_loss: 1.2206 Training g_loss: 0.8680 Training q_loss: 88.8581 Explore P: 0.4749\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 16.0 Average reward fake: 0.43672770261764526 Average reward real: 0.48595380783081055 Training d_loss: 1.3061 Training g_loss: 0.8477 Training q_loss: 10.7789 Explore P: 0.4742\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 154.0 Average reward fake: 0.5033859014511108 Average reward real: 0.5694661140441895 Training d_loss: 1.2723 Training g_loss: 0.6985 Training q_loss: 27.7557 Explore P: 0.4671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 17.0 Average reward fake: 0.47843289375305176 Average reward real: 0.5032463073730469 Training d_loss: 1.3468 Training g_loss: 0.7462 Training q_loss: 729.8158 Explore P: 0.4663\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 199.0 Average reward fake: 0.4863644242286682 Average reward real: 0.47735515236854553 Training d_loss: 1.4268 Training g_loss: 0.7266 Training q_loss: 35.5006 Explore P: 0.4573\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 65.0 Average reward fake: 0.5115452408790588 Average reward real: 0.49518242478370667 Training d_loss: 1.4318 Training g_loss: 0.6769 Training q_loss: 98.3148 Explore P: 0.4544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 199.0 Average reward fake: 0.4621833860874176 Average reward real: 0.5213952660560608 Training d_loss: 1.2736 Training g_loss: 0.7761 Training q_loss: 118.5408 Explore P: 0.4457\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 199.0 Average reward fake: 0.45790520310401917 Average reward real: 0.5593084692955017 Training d_loss: 1.1984 Training g_loss: 0.7899 Training q_loss: 8.9811 Explore P: 0.4371\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 180.0 Average reward fake: 0.421712726354599 Average reward real: 0.44950491189956665 Training d_loss: 1.3539 Training g_loss: 0.8766 Training q_loss: 29.9793 Explore P: 0.4295\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "d_loss_list, g_loss_list, q_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions}\n",
    "            rewards_fake, rewards_real = sess.run([model.rewards_fake, model.rewards_real], feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions, model.targetQs: targetQs}\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/DQAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/DQAN-cartpole.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 1000\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    # Save the trained model \n",
    "    saver.restore(sess, 'checkpoints/DQAN-cartpole.ckpt')\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
