{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQAN (Deep Q-Adverserial Nets): DQN (Deep Q-Nets) + GAN (Gen. Adv. Nets)\n",
    "\n",
    "In this notebook, we'll combine a DQN (deep Q-net) with GAN (generative adverserial net) that can learn to play games through reinforcement learning without any reward function. We'll call this network DQAN (deep Q adverserial net). \n",
    "Adverserial nets learn to maximize the current reward based the past rewards.\n",
    "Q-net learns to maximize the future rewards based on the current reward.\n",
    "Given a task and known when the task is done or failed, we should be able to learn the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[-0.0048124  -0.2450929   0.03652628  0.32928597] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00971426 -0.05050945  0.043112    0.04834181] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.01072445  0.14396864  0.04407883 -0.23043327] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00784507 -0.05175455  0.03947017  0.07582134] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00888016  0.14277998  0.0409866  -0.20415198] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00602456  0.33729255  0.03690356 -0.48362911] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 7.21286127e-04  5.31874773e-01  2.72309739e-02 -7.64456531e-01] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01135878  0.72661135  0.01194184 -1.04844818] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02589101  0.92157282 -0.00902712 -1.33735872] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.04432246  1.11680731 -0.03577429 -1.63285246] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.11680731300025 -1.6328524636998423\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    # Given data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Actions as output\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # Target Q values for training\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.dense(\n",
    "#     inputs, ????????????????????????\n",
    "#     units, ??????????????????????\n",
    "#     activation=None,\n",
    "#     use_bias=True, OOOOOOOOOOOOOOOOOOOOOOOK\n",
    "#     kernel_initializer=None,\n",
    "#     bias_initializer=tf.zeros_initializer(), OOOOOOOOOOOOOOOK\n",
    "#     kernel_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None,\n",
    "#     kernel_constraint=None,\n",
    "#     bias_constraint=None,\n",
    "#     trainable=True, ??????????????????\n",
    "#     name=None,\n",
    "#     reuse=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q function\n",
    "def generator(states, state_size, action_size, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=training) #training=True ~ batchnorm\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=training) #training=True ~ batchnorm\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits_actions = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits_actions)\n",
    "\n",
    "        # Output layer\n",
    "        logits_next_states = tf.layers.dense(inputs=nl2, units=state_size, trainable=False)        \n",
    "        #predictions = tf.nn.softmax(logits_next_states)\n",
    "\n",
    "        # # Output layer\n",
    "        # logits = tf.layers.dense(inputs=nl2, units=(state_size + action_size))\n",
    "\n",
    "        # # Split the states and actions (opposite of fusion)\n",
    "        # logits_next_states, logits_actions = tf.split(value=logits, axis=1, num_or_size_splits=[state_size, action_size])\n",
    "\n",
    "        return logits_actions, logits_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a reward function: Rt(St+1, at) or Rt(~St+1, ~at)\n",
    "def discriminator(next_states, actions, hidden_size, reuse=False, alpha=0.1): #training=True ~ batchnorm\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Stack/concatenate/fuse actions and states or \n",
    "        # predicted/reconstructed actions and states\n",
    "        x_fused = tf.concat(values=(next_states, actions), axis=1)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(h1, training=True)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(h2, training=True)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # logits for loss and reward/prob/out\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qt(St, At) = Rt(St+1, At) + max(alpha*Qt+1(St+1))\n",
    "def model_loss(states, next_states, state_size, actions, action_size, hidden_size, targetQs, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param states: real current input states or observations given\n",
    "    :param actions: real actions given\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # The fake/generated actions\n",
    "    actions_logits, next_states_logits = generator(states=states, state_size=state_size, hidden_size=hidden_size, \n",
    "                                              action_size=action_size)\n",
    "    #print(actions_logits.shape, next_states_logits.shape)\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(next_states=next_states_fake, actions=actions_fake, hidden_size=hidden_size, reuse=False)\n",
    "\n",
    "    # The real onehot encoded actions\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(next_states=next_states_real, actions=actions_real, hidden_size=hidden_size, reuse=True)\n",
    "\n",
    "    # Training the rewarding function\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Train the generate to maximize the current reward 0-1\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Train the generator to maximize the future rewards: Bellman equations: loss (targetQ - Q)^2\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # The generated rewards for Bellman equation\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return d_loss, g_loss, q_loss, actions_logits, Qs, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for reward function\n",
    "    :param g_loss: Generator/Q-value loss Tensor for action & next state predicton\n",
    "    :param q_loss: Value loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=g_vars)\n",
    "\n",
    "    return d_opt, g_opt, q_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "        #print(self.states, self.next_states, self.actions, self.targetQs)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.d_loss, self.g_loss, self.q_loss, self.actions_logits, self.Qs, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, actions=self.actions, \n",
    "            states=self.states, next_states=self.next_states, \n",
    "            hidden_size=hidden_size, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.d_opt, self.g_opt, self.q_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, \n",
    "                                                       q_loss=self.q_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 3000          # max number of episodes to learn from\n",
    "max_steps = 200               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = DQAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 2.0 Average reward fake: 0.4857206344604492 Average reward real: 0.48793157935142517 Training d_loss: 1.3826 Training g_loss: 0.7213 Training q_loss: 0.3421 Explore P: 0.9998\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 19.0 Average reward fake: 0.4790409207344055 Average reward real: 0.5361893773078918 Training d_loss: 1.2777 Training g_loss: 0.7402 Training q_loss: 0.5650 Explore P: 0.9979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 9.0 Average reward fake: 0.4861293435096741 Average reward real: 0.5334004163742065 Training d_loss: 1.2992 Training g_loss: 0.7248 Training q_loss: 1.2741 Explore P: 0.9970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 38.0 Average reward fake: 0.4821871221065521 Average reward real: 0.5324274897575378 Training d_loss: 1.3025 Training g_loss: 0.7542 Training q_loss: 4.4540 Explore P: 0.9933\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 14.0 Average reward fake: 0.4847755432128906 Average reward real: 0.5632508397102356 Training d_loss: 1.2806 Training g_loss: 0.7563 Training q_loss: 3.3844 Explore P: 0.9919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 50.0 Average reward fake: 0.4283068776130676 Average reward real: 0.5425136685371399 Training d_loss: 1.1893 Training g_loss: 0.9022 Training q_loss: 33.4755 Explore P: 0.9870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 18.0 Average reward fake: 0.3369201123714447 Average reward real: 0.5258254408836365 Training d_loss: 1.0959 Training g_loss: 1.1449 Training q_loss: 16.4606 Explore P: 0.9853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 12.0 Average reward fake: 0.42254191637039185 Average reward real: 0.5162688493728638 Training d_loss: 1.2520 Training g_loss: 0.9338 Training q_loss: 20.0515 Explore P: 0.9841\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 14.0 Average reward fake: 0.3687342703342438 Average reward real: 0.5194190740585327 Training d_loss: 1.1517 Training g_loss: 1.0796 Training q_loss: 259.5577 Explore P: 0.9827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 19.0 Average reward fake: 0.35696661472320557 Average reward real: 0.5443680286407471 Training d_loss: 1.0690 Training g_loss: 1.1721 Training q_loss: 440.2466 Explore P: 0.9809\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 19.0 Average reward fake: 0.6224973201751709 Average reward real: 0.6479455232620239 Training d_loss: 1.4985 Training g_loss: 0.5684 Training q_loss: 72.3618 Explore P: 0.9790\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 13.0 Average reward fake: 0.37451618909835815 Average reward real: 0.5327476859092712 Training d_loss: 1.1492 Training g_loss: 1.0318 Training q_loss: 21.5360 Explore P: 0.9778\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 21.0 Average reward fake: 0.4726167619228363 Average reward real: 0.7664388418197632 Training d_loss: 1.0046 Training g_loss: 0.9366 Training q_loss: 137.3525 Explore P: 0.9757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 42.0 Average reward fake: 0.17984095215797424 Average reward real: 0.9001714587211609 Training d_loss: 0.3149 Training g_loss: 1.9551 Training q_loss: 25.9176 Explore P: 0.9717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 21.0 Average reward fake: 0.5066973567008972 Average reward real: 0.7831504940986633 Training d_loss: 1.4387 Training g_loss: 1.7249 Training q_loss: 256.4227 Explore P: 0.9697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 18.0 Average reward fake: 0.2029992789030075 Average reward real: 0.8239818811416626 Training d_loss: 0.4811 Training g_loss: 2.0792 Training q_loss: 91.5058 Explore P: 0.9680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 46.0 Average reward fake: 0.09397973865270615 Average reward real: 0.924393355846405 Training d_loss: 0.2018 Training g_loss: 3.0413 Training q_loss: 38.3748 Explore P: 0.9636\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 11.0 Average reward fake: 0.31379374861717224 Average reward real: 0.6607580184936523 Training d_loss: 1.2723 Training g_loss: 2.8422 Training q_loss: 1056.6630 Explore P: 0.9625\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 10.0 Average reward fake: 0.11495671421289444 Average reward real: 0.7670198678970337 Training d_loss: 0.6405 Training g_loss: 2.4521 Training q_loss: 502.3740 Explore P: 0.9616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 14.0 Average reward fake: 0.28308039903640747 Average reward real: 0.9069147109985352 Training d_loss: 0.5602 Training g_loss: 2.1935 Training q_loss: 90.1237 Explore P: 0.9602\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 13.0 Average reward fake: 0.20687969028949738 Average reward real: 0.49253568053245544 Training d_loss: 1.8566 Training g_loss: 1.6711 Training q_loss: 52.8506 Explore P: 0.9590\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 11.0 Average reward fake: 0.1765120029449463 Average reward real: 0.679750919342041 Training d_loss: 0.7205 Training g_loss: 1.6845 Training q_loss: 3782.1367 Explore P: 0.9580\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 12.0 Average reward fake: 0.16274303197860718 Average reward real: 0.7751983404159546 Training d_loss: 0.6439 Training g_loss: 1.8472 Training q_loss: 2403.7207 Explore P: 0.9568\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 37.0 Average reward fake: 0.07496147602796555 Average reward real: 0.8563590049743652 Training d_loss: 0.2776 Training g_loss: 2.6004 Training q_loss: 36.1427 Explore P: 0.9533\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 20.0 Average reward fake: 0.04531712085008621 Average reward real: 0.925586998462677 Training d_loss: 0.1295 Training g_loss: 3.1313 Training q_loss: 711.0956 Explore P: 0.9514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 22.0 Average reward fake: 0.03569464758038521 Average reward real: 0.9571079015731812 Training d_loss: 0.0811 Training g_loss: 3.3664 Training q_loss: 1904.9053 Explore P: 0.9494\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 12.0 Average reward fake: 0.03320932760834694 Average reward real: 0.9801109433174133 Training d_loss: 0.0541 Training g_loss: 3.4403 Training q_loss: 1055.0789 Explore P: 0.9482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 35.0 Average reward fake: 0.015718888491392136 Average reward real: 0.9336186647415161 Training d_loss: 0.0927 Training g_loss: 4.1169 Training q_loss: 23.7926 Explore P: 0.9450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 27.0 Average reward fake: 0.010746536776423454 Average reward real: 0.9924175143241882 Training d_loss: 0.0184 Training g_loss: 4.5637 Training q_loss: 26.5652 Explore P: 0.9424\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 25.0 Average reward fake: 0.00788805726915598 Average reward real: 0.9872304797172546 Training d_loss: 0.0209 Training g_loss: 4.8638 Training q_loss: 559.9628 Explore P: 0.9401\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 12.0 Average reward fake: 0.007401513401418924 Average reward real: 0.9821997880935669 Training d_loss: 0.0256 Training g_loss: 4.8886 Training q_loss: 20.2401 Explore P: 0.9390\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 12.0 Average reward fake: 0.0075980001129209995 Average reward real: 0.9859344363212585 Training d_loss: 0.0219 Training g_loss: 4.8946 Training q_loss: 9.3379 Explore P: 0.9379\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 20.0 Average reward fake: 0.005264383740723133 Average reward real: 0.9861570596694946 Training d_loss: 0.0194 Training g_loss: 5.2332 Training q_loss: 532.9093 Explore P: 0.9360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 18.0 Average reward fake: 0.006845719181001186 Average reward real: 0.9902799725532532 Training d_loss: 0.0167 Training g_loss: 4.9967 Training q_loss: 522.2694 Explore P: 0.9344\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 29.0 Average reward fake: 0.026989828795194626 Average reward real: 0.8754910230636597 Training d_loss: 0.1893 Training g_loss: 5.9582 Training q_loss: 27.3379 Explore P: 0.9317\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 15.0 Average reward fake: 0.040586359798908234 Average reward real: 0.964234471321106 Training d_loss: 0.0791 Training g_loss: 3.4213 Training q_loss: 334.5760 Explore P: 0.9303\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 31.0 Average reward fake: 0.05247749760746956 Average reward real: 0.9493364095687866 Training d_loss: 0.1131 Training g_loss: 3.2473 Training q_loss: 1069.0718 Explore P: 0.9275\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 11.0 Average reward fake: 0.018815865740180016 Average reward real: 0.9498255848884583 Training d_loss: 0.0738 Training g_loss: 4.0295 Training q_loss: 51.8697 Explore P: 0.9264\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 19.0 Average reward fake: 0.020470771938562393 Average reward real: 0.9938859939575195 Training d_loss: 0.0268 Training g_loss: 3.9856 Training q_loss: 94.6715 Explore P: 0.9247\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 31.0 Average reward fake: 0.01643853262066841 Average reward real: 0.9241212010383606 Training d_loss: 0.1053 Training g_loss: 4.0736 Training q_loss: 47.0828 Explore P: 0.9219\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 33.0 Average reward fake: 0.003133268328383565 Average reward real: 0.9951993823051453 Training d_loss: 0.0080 Training g_loss: 5.7868 Training q_loss: 49.5708 Explore P: 0.9189\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 15.0 Average reward fake: 0.0024130616802722216 Average reward real: 0.9949037432670593 Training d_loss: 0.0076 Training g_loss: 6.0428 Training q_loss: 1112.0936 Explore P: 0.9175\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 23.0 Average reward fake: 0.0021006048191338778 Average reward real: 0.9977852702140808 Training d_loss: 0.0043 Training g_loss: 6.1674 Training q_loss: 25.4464 Explore P: 0.9154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 14.0 Average reward fake: 0.001949709258042276 Average reward real: 0.997855544090271 Training d_loss: 0.0041 Training g_loss: 6.2413 Training q_loss: 17.9022 Explore P: 0.9142\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 15.0 Average reward fake: 0.0018104190239682794 Average reward real: 0.998214602470398 Training d_loss: 0.0036 Training g_loss: 6.3259 Training q_loss: 28.4263 Explore P: 0.9128\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 37.0 Average reward fake: 0.0013290971983224154 Average reward real: 0.9990848302841187 Training d_loss: 0.0022 Training g_loss: 6.6223 Training q_loss: 758.3319 Explore P: 0.9095\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 18.0 Average reward fake: 0.0012542642652988434 Average reward real: 0.9988151788711548 Training d_loss: 0.0024 Training g_loss: 6.6910 Training q_loss: 706.6235 Explore P: 0.9079\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 9.0 Average reward fake: 0.001158001134172082 Average reward real: 0.9985052943229675 Training d_loss: 0.0027 Training g_loss: 6.7720 Training q_loss: 19.1774 Explore P: 0.9070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 16.0 Average reward fake: 0.0009750787867233157 Average reward real: 0.9936079978942871 Training d_loss: 0.0075 Training g_loss: 6.9384 Training q_loss: 911.0749 Explore P: 0.9056\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 18.0 Average reward fake: 0.0009568231180310249 Average reward real: 0.998630702495575 Training d_loss: 0.0023 Training g_loss: 6.9585 Training q_loss: 17.0766 Explore P: 0.9040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 23.0 Average reward fake: 0.002517489017918706 Average reward real: 0.998762309551239 Training d_loss: 0.0038 Training g_loss: 6.1453 Training q_loss: 32.1107 Explore P: 0.9019\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 31.0 Average reward fake: 0.00556743610650301 Average reward real: 0.9765628576278687 Training d_loss: 0.0313 Training g_loss: 5.6817 Training q_loss: 42.1460 Explore P: 0.8992\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 13.0 Average reward fake: 0.02303997054696083 Average reward real: 0.9739112854003906 Training d_loss: 0.0507 Training g_loss: 6.0052 Training q_loss: 1027.7455 Explore P: 0.8980\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 27.0 Average reward fake: 0.020156927406787872 Average reward real: 0.9920517206192017 Training d_loss: 0.0293 Training g_loss: 6.5229 Training q_loss: 164.9339 Explore P: 0.8956\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 14.0 Average reward fake: 0.03030386008322239 Average reward real: 0.9933006167411804 Training d_loss: 0.0388 Training g_loss: 6.4931 Training q_loss: 1512.9159 Explore P: 0.8944\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 32.0 Average reward fake: 0.034315258264541626 Average reward real: 0.9003372192382812 Training d_loss: 0.1427 Training g_loss: 5.4478 Training q_loss: 111.3185 Explore P: 0.8916\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 36.0 Average reward fake: 0.1070004552602768 Average reward real: 0.873884379863739 Training d_loss: 0.2691 Training g_loss: 2.9183 Training q_loss: 663.6777 Explore P: 0.8884\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 14.0 Average reward fake: 0.2671949863433838 Average reward real: 0.9296520948410034 Training d_loss: 0.4925 Training g_loss: 2.6235 Training q_loss: 1005.1419 Explore P: 0.8872\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 38.0 Average reward fake: 0.02393641136586666 Average reward real: 0.916601300239563 Training d_loss: 0.1180 Training g_loss: 4.9907 Training q_loss: 8428.0137 Explore P: 0.8838\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 17.0 Average reward fake: 0.09972196817398071 Average reward real: 0.975100040435791 Training d_loss: 0.1385 Training g_loss: 3.1235 Training q_loss: 238.4576 Explore P: 0.8824\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 20.0 Average reward fake: 0.3165455758571625 Average reward real: 0.5638200044631958 Training d_loss: 1.5644 Training g_loss: 1.6945 Training q_loss: 447.4294 Explore P: 0.8806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 24.0 Average reward fake: 0.17568467557430267 Average reward real: 0.8025795221328735 Training d_loss: 0.4786 Training g_loss: 4.5434 Training q_loss: 559.9658 Explore P: 0.8785\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 11.0 Average reward fake: 0.06338340044021606 Average reward real: 0.8110578656196594 Training d_loss: 0.3812 Training g_loss: 3.9160 Training q_loss: 3344.1809 Explore P: 0.8776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 17.0 Average reward fake: 0.11188594996929169 Average reward real: 0.825150191783905 Training d_loss: 0.3913 Training g_loss: 3.4216 Training q_loss: 7433.5342 Explore P: 0.8761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 21.0 Average reward fake: 0.08047499507665634 Average reward real: 0.8452900648117065 Training d_loss: 0.4003 Training g_loss: 2.5690 Training q_loss: 831.7051 Explore P: 0.8743\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 9.0 Average reward fake: 0.019045095890760422 Average reward real: 0.8124756813049316 Training d_loss: 0.2957 Training g_loss: 4.1600 Training q_loss: 1309.4427 Explore P: 0.8735\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 20.0 Average reward fake: 0.13132227957248688 Average reward real: 0.9604441523551941 Training d_loss: 0.2593 Training g_loss: 5.6691 Training q_loss: 1036.0066 Explore P: 0.8718\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 13.0 Average reward fake: 0.11878464370965958 Average reward real: 0.8929404020309448 Training d_loss: 0.3162 Training g_loss: 6.7041 Training q_loss: 1034.1560 Explore P: 0.8707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 11.0 Average reward fake: 0.08135100454092026 Average reward real: 0.9412912130355835 Training d_loss: 0.2317 Training g_loss: 7.2446 Training q_loss: 2155.4900 Explore P: 0.8697\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 18.0 Average reward fake: 0.11191314458847046 Average reward real: 0.9220808744430542 Training d_loss: 0.3312 Training g_loss: 5.2811 Training q_loss: 1093.0222 Explore P: 0.8682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 13.0 Average reward fake: 0.23422174155712128 Average reward real: 0.7822372317314148 Training d_loss: 0.8907 Training g_loss: 3.7306 Training q_loss: 2612.0793 Explore P: 0.8671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 20.0 Average reward fake: 0.16909615695476532 Average reward real: 0.7002133727073669 Training d_loss: 0.7023 Training g_loss: 3.4263 Training q_loss: 2686.6580 Explore P: 0.8653\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 21.0 Average reward fake: 0.2498294860124588 Average reward real: 0.7834848165512085 Training d_loss: 0.9920 Training g_loss: 2.2509 Training q_loss: 64451.0938 Explore P: 0.8636\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 45.0 Average reward fake: 0.25774234533309937 Average reward real: 0.9098347425460815 Training d_loss: 0.4452 Training g_loss: 2.2085 Training q_loss: 2576.8628 Explore P: 0.8597\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 21.0 Average reward fake: 0.25904497504234314 Average reward real: 0.7866309881210327 Training d_loss: 0.5732 Training g_loss: 1.6403 Training q_loss: 7158.2593 Explore P: 0.8579\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 31.0 Average reward fake: 0.3173903822898865 Average reward real: 0.730305016040802 Training d_loss: 1.0602 Training g_loss: 1.6194 Training q_loss: 4509.8320 Explore P: 0.8553\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 14.0 Average reward fake: 0.22150492668151855 Average reward real: 0.7660334706306458 Training d_loss: 0.6039 Training g_loss: 1.8146 Training q_loss: 2300.5127 Explore P: 0.8541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 11.0 Average reward fake: 0.3615429103374481 Average reward real: 0.9425945281982422 Training d_loss: 0.5241 Training g_loss: 1.2693 Training q_loss: 2016.5618 Explore P: 0.8532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 10.0 Average reward fake: 0.23829412460327148 Average reward real: 0.7143763303756714 Training d_loss: 1.2257 Training g_loss: 1.6852 Training q_loss: 85730.7578 Explore P: 0.8524\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 14.0 Average reward fake: 0.23227381706237793 Average reward real: 0.7339387536048889 Training d_loss: 0.7497 Training g_loss: 1.7945 Training q_loss: 2214.4971 Explore P: 0.8512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 21.0 Average reward fake: 0.45893993973731995 Average reward real: 0.6454989314079285 Training d_loss: 1.2416 Training g_loss: 0.9541 Training q_loss: 2180.5496 Explore P: 0.8494\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 12.0 Average reward fake: 0.1066984161734581 Average reward real: 0.5331041216850281 Training d_loss: 1.7713 Training g_loss: 2.2659 Training q_loss: 6200.5903 Explore P: 0.8484\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 13.0 Average reward fake: 0.3024778664112091 Average reward real: 0.8717435002326965 Training d_loss: 0.5197 Training g_loss: 1.4267 Training q_loss: 2698.5640 Explore P: 0.8473\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 25.0 Average reward fake: 0.3243761956691742 Average reward real: 0.7314772009849548 Training d_loss: 0.9547 Training g_loss: 1.2340 Training q_loss: 2646.4023 Explore P: 0.8452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 23.0 Average reward fake: 0.2379385530948639 Average reward real: 0.7798672914505005 Training d_loss: 0.6183 Training g_loss: 1.6549 Training q_loss: 3745.6030 Explore P: 0.8433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 17.0 Average reward fake: 0.23789522051811218 Average reward real: 0.6372271180152893 Training d_loss: 0.9523 Training g_loss: 1.5225 Training q_loss: 2705.2786 Explore P: 0.8419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 15.0 Average reward fake: 0.454122394323349 Average reward real: 0.8279455304145813 Training d_loss: 0.8630 Training g_loss: 0.8234 Training q_loss: 77334.3203 Explore P: 0.8406\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 57.0 Average reward fake: 0.38997676968574524 Average reward real: 0.6855890154838562 Training d_loss: 0.9899 Training g_loss: 0.9228 Training q_loss: 1764.8416 Explore P: 0.8359\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 37.0 Average reward fake: 0.30701762437820435 Average reward real: 0.7237831354141235 Training d_loss: 0.8355 Training g_loss: 1.2525 Training q_loss: 2312.7380 Explore P: 0.8329\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 16.0 Average reward fake: 0.39706680178642273 Average reward real: 0.5567058324813843 Training d_loss: 1.2108 Training g_loss: 0.9523 Training q_loss: 1769.8220 Explore P: 0.8316\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 34.0 Average reward fake: 0.3821144700050354 Average reward real: 0.5292750597000122 Training d_loss: 1.1974 Training g_loss: 0.9722 Training q_loss: 1422.8748 Explore P: 0.8288\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 22.0 Average reward fake: 0.3451685607433319 Average reward real: 0.5631647109985352 Training d_loss: 1.2077 Training g_loss: 1.0508 Training q_loss: 1276.1855 Explore P: 0.8270\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 17.0 Average reward fake: 0.22489003837108612 Average reward real: 0.5890467166900635 Training d_loss: 0.9612 Training g_loss: 1.5748 Training q_loss: 1513.7434 Explore P: 0.8256\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 11.0 Average reward fake: 0.2837497889995575 Average reward real: 0.831872284412384 Training d_loss: 0.5809 Training g_loss: 1.2960 Training q_loss: 85043.9141 Explore P: 0.8247\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 32.0 Average reward fake: 0.4360080659389496 Average reward real: 0.7237445116043091 Training d_loss: 0.9768 Training g_loss: 0.8569 Training q_loss: 1060.9236 Explore P: 0.8221\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 18.0 Average reward fake: 0.32657375931739807 Average reward real: 0.7951170206069946 Training d_loss: 0.7389 Training g_loss: 1.1080 Training q_loss: 43455.7305 Explore P: 0.8206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 21.0 Average reward fake: 0.33359894156455994 Average reward real: 0.6018767952919006 Training d_loss: 1.0591 Training g_loss: 1.1186 Training q_loss: 50052.5078 Explore P: 0.8189\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 27.0 Average reward fake: 0.3625059723854065 Average reward real: 0.8017281293869019 Training d_loss: 0.7747 Training g_loss: 1.0559 Training q_loss: 722.8555 Explore P: 0.8167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 26.0 Average reward fake: 0.41844862699508667 Average reward real: 0.6459211111068726 Training d_loss: 1.0808 Training g_loss: 0.8885 Training q_loss: 64564.9453 Explore P: 0.8146\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 19.0 Average reward fake: 0.3663392961025238 Average reward real: 0.7940128445625305 Training d_loss: 0.8044 Training g_loss: 1.0041 Training q_loss: 311.1393 Explore P: 0.8131\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 14.0 Average reward fake: 0.3574117124080658 Average reward real: 0.5308444499969482 Training d_loss: 1.2200 Training g_loss: 0.9978 Training q_loss: 27788.7598 Explore P: 0.8120\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 30.0 Average reward fake: 0.34231165051460266 Average reward real: 0.6548937559127808 Training d_loss: 1.0063 Training g_loss: 1.0912 Training q_loss: 21950.5469 Explore P: 0.8096\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 20.0 Average reward fake: 0.38398119807243347 Average reward real: 0.6872460246086121 Training d_loss: 0.9778 Training g_loss: 0.9681 Training q_loss: 95.0629 Explore P: 0.8080\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 36.0 Average reward fake: 0.9948075413703918 Average reward real: 0.6884905695915222 Training d_loss: 5.7522 Training g_loss: 0.0136 Training q_loss: 93.3261 Explore P: 0.8051\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 22.0 Average reward fake: 0.29232650995254517 Average reward real: 0.564572811126709 Training d_loss: 1.3461 Training g_loss: 1.1531 Training q_loss: 13386.7061 Explore P: 0.8034\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 14.0 Average reward fake: 0.2140432596206665 Average reward real: 0.6738048791885376 Training d_loss: 0.7096 Training g_loss: 1.6641 Training q_loss: 22716.9902 Explore P: 0.8023\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 38.0 Average reward fake: 0.4395429491996765 Average reward real: 0.3672320246696472 Training d_loss: 1.9052 Training g_loss: 0.9248 Training q_loss: 291.2176 Explore P: 0.7993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 13.0 Average reward fake: 0.23299479484558105 Average reward real: 0.7412494421005249 Training d_loss: 0.6047 Training g_loss: 1.5560 Training q_loss: 217.0621 Explore P: 0.7982\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 12.0 Average reward fake: 0.30688703060150146 Average reward real: 0.7801744937896729 Training d_loss: 0.8302 Training g_loss: 1.3671 Training q_loss: 303.3697 Explore P: 0.7973\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 19.0 Average reward fake: 0.3429836332798004 Average reward real: 0.788124680519104 Training d_loss: 0.7203 Training g_loss: 1.1188 Training q_loss: 199.4006 Explore P: 0.7958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 22.0 Average reward fake: 0.3882291316986084 Average reward real: 0.8218196630477905 Training d_loss: 0.7391 Training g_loss: 1.1035 Training q_loss: 30586.9316 Explore P: 0.7941\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 11.0 Average reward fake: 0.36611494421958923 Average reward real: 0.3857005536556244 Training d_loss: 2.1465 Training g_loss: 1.1706 Training q_loss: 127.1109 Explore P: 0.7932\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 15.0 Average reward fake: 0.277535617351532 Average reward real: 0.6169198751449585 Training d_loss: 0.8832 Training g_loss: 1.2499 Training q_loss: 139.2806 Explore P: 0.7920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 16.0 Average reward fake: 0.5224406719207764 Average reward real: 0.7274768948554993 Training d_loss: 1.4572 Training g_loss: 0.7990 Training q_loss: 37873.9375 Explore P: 0.7908\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 9.0 Average reward fake: 0.8797081708908081 Average reward real: 0.5091325044631958 Training d_loss: 3.1129 Training g_loss: 0.1973 Training q_loss: 388.3475 Explore P: 0.7901\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 14.0 Average reward fake: 0.1790730059146881 Average reward real: 0.5370428562164307 Training d_loss: 0.9327 Training g_loss: 1.7778 Training q_loss: 898.9618 Explore P: 0.7890\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 48.0 Average reward fake: 0.3045367896556854 Average reward real: 0.7632613182067871 Training d_loss: 0.7294 Training g_loss: 1.2031 Training q_loss: 33292.9258 Explore P: 0.7853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 17.0 Average reward fake: 0.3115319609642029 Average reward real: 0.7854615449905396 Training d_loss: 0.6964 Training g_loss: 1.1647 Training q_loss: 6486.5757 Explore P: 0.7839\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 11.0 Average reward fake: 0.4826633930206299 Average reward real: 0.7061629891395569 Training d_loss: 1.3459 Training g_loss: 0.8157 Training q_loss: 19424.9414 Explore P: 0.7831\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 8.0 Average reward fake: 0.5692710280418396 Average reward real: 0.7369166612625122 Training d_loss: 1.5228 Training g_loss: 0.6790 Training q_loss: 77.3247 Explore P: 0.7825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 25.0 Average reward fake: 0.35640788078308105 Average reward real: 0.46519261598587036 Training d_loss: 1.2507 Training g_loss: 1.0853 Training q_loss: 484.5842 Explore P: 0.7806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 12.0 Average reward fake: 0.3202805817127228 Average reward real: 0.41883236169815063 Training d_loss: 1.6362 Training g_loss: 1.1433 Training q_loss: 565.1914 Explore P: 0.7796\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 25.0 Average reward fake: 0.24100665748119354 Average reward real: 0.47961434721946716 Training d_loss: 1.1741 Training g_loss: 1.4392 Training q_loss: 297.5637 Explore P: 0.7777\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 17.0 Average reward fake: 0.3464033901691437 Average reward real: 0.5623215436935425 Training d_loss: 1.1568 Training g_loss: 1.0690 Training q_loss: 214.0247 Explore P: 0.7764\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 12.0 Average reward fake: 0.3850421905517578 Average reward real: 0.5963481664657593 Training d_loss: 1.1200 Training g_loss: 0.9679 Training q_loss: 22631.4102 Explore P: 0.7755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 20.0 Average reward fake: 0.3592471480369568 Average reward real: 0.6519069075584412 Training d_loss: 0.9928 Training g_loss: 1.0309 Training q_loss: 240.9147 Explore P: 0.7740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 17.0 Average reward fake: 0.3360869288444519 Average reward real: 0.8933402895927429 Training d_loss: 0.5668 Training g_loss: 1.1090 Training q_loss: 231.9209 Explore P: 0.7727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 13.0 Average reward fake: 0.30707675218582153 Average reward real: 0.6949967741966248 Training d_loss: 0.8796 Training g_loss: 1.1882 Training q_loss: 16286.4971 Explore P: 0.7717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 13.0 Average reward fake: 0.3136771321296692 Average reward real: 0.5058414936065674 Training d_loss: 1.2143 Training g_loss: 1.1540 Training q_loss: 128.3022 Explore P: 0.7707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 34.0 Average reward fake: 0.32039889693260193 Average reward real: 0.49838972091674805 Training d_loss: 1.2581 Training g_loss: 1.1385 Training q_loss: 157.5067 Explore P: 0.7681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 33.0 Average reward fake: 0.6854321956634521 Average reward real: 0.6044315099716187 Training d_loss: 1.6786 Training g_loss: 0.4186 Training q_loss: 524.1870 Explore P: 0.7656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 8.0 Average reward fake: 0.4995691776275635 Average reward real: 0.4907422959804535 Training d_loss: 1.4086 Training g_loss: 0.7324 Training q_loss: 542.3137 Explore P: 0.7650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 16.0 Average reward fake: 0.35409417748451233 Average reward real: 0.5070043206214905 Training d_loss: 1.1714 Training g_loss: 1.0494 Training q_loss: 341.3556 Explore P: 0.7638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 26.0 Average reward fake: 0.3677171468734741 Average reward real: 0.6214123964309692 Training d_loss: 1.0070 Training g_loss: 1.0000 Training q_loss: 6458.8081 Explore P: 0.7618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 12.0 Average reward fake: 0.4127940237522125 Average reward real: 0.5509434938430786 Training d_loss: 1.2414 Training g_loss: 0.9121 Training q_loss: 406.2848 Explore P: 0.7609\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 9.0 Average reward fake: 0.4750359058380127 Average reward real: 0.6713874936103821 Training d_loss: 1.1672 Training g_loss: 0.7943 Training q_loss: 147.9224 Explore P: 0.7602\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 34.0 Average reward fake: 0.5932791233062744 Average reward real: 0.518376350402832 Training d_loss: 1.5691 Training g_loss: 0.5560 Training q_loss: 152.3903 Explore P: 0.7577\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 26.0 Average reward fake: 0.3489205241203308 Average reward real: 0.37554940581321716 Training d_loss: 1.4519 Training g_loss: 1.0594 Training q_loss: 9082.9707 Explore P: 0.7558\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 24.0 Average reward fake: 0.3221326470375061 Average reward real: 0.5310887098312378 Training d_loss: 1.1072 Training g_loss: 1.1356 Training q_loss: 217.9126 Explore P: 0.7540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 26.0 Average reward fake: 0.376272052526474 Average reward real: 0.512431263923645 Training d_loss: 1.2898 Training g_loss: 1.0273 Training q_loss: 89.7060 Explore P: 0.7520\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 23.0 Average reward fake: 0.7147653698921204 Average reward real: 0.48251086473464966 Training d_loss: 2.1943 Training g_loss: 0.3806 Training q_loss: 170.9080 Explore P: 0.7503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 9.0 Average reward fake: 0.594764232635498 Average reward real: 0.5318373441696167 Training d_loss: 1.6578 Training g_loss: 0.5631 Training q_loss: 3033.2944 Explore P: 0.7497\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 15.0 Average reward fake: 0.5615708827972412 Average reward real: 0.5066324472427368 Training d_loss: 1.5156 Training g_loss: 0.5925 Training q_loss: 3363.3098 Explore P: 0.7486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 12.0 Average reward fake: 0.4990619719028473 Average reward real: 0.5281511545181274 Training d_loss: 1.3323 Training g_loss: 0.7047 Training q_loss: 5293.1323 Explore P: 0.7477\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 15.0 Average reward fake: 0.47503939270973206 Average reward real: 0.5227013826370239 Training d_loss: 1.3167 Training g_loss: 0.7538 Training q_loss: 4331.4297 Explore P: 0.7466\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 84.0 Average reward fake: 0.4818090498447418 Average reward real: 0.4882315695285797 Training d_loss: 1.3754 Training g_loss: 0.7501 Training q_loss: 204.6311 Explore P: 0.7404\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 29.0 Average reward fake: 0.3920128643512726 Average reward real: 0.5009894967079163 Training d_loss: 1.2414 Training g_loss: 0.9496 Training q_loss: 1370.5195 Explore P: 0.7383\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 33.0 Average reward fake: 0.5626043081283569 Average reward real: 0.5687817931175232 Training d_loss: 1.4730 Training g_loss: 0.6078 Training q_loss: 191.7586 Explore P: 0.7359\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 15.0 Average reward fake: 0.5641118288040161 Average reward real: 0.5398539900779724 Training d_loss: 1.4662 Training g_loss: 0.5846 Training q_loss: 357.1009 Explore P: 0.7348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 15.0 Average reward fake: 0.5323126316070557 Average reward real: 0.5532580018043518 Training d_loss: 1.3563 Training g_loss: 0.6353 Training q_loss: 5927.2905 Explore P: 0.7337\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 18.0 Average reward fake: 0.5121484994888306 Average reward real: 0.5295053720474243 Training d_loss: 1.3619 Training g_loss: 0.6758 Training q_loss: 268.4520 Explore P: 0.7324\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 16.0 Average reward fake: 0.49020320177078247 Average reward real: 0.49347352981567383 Training d_loss: 1.3992 Training g_loss: 0.7186 Training q_loss: 935.9142 Explore P: 0.7313\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 16.0 Average reward fake: 0.527411937713623 Average reward real: 0.5387226343154907 Training d_loss: 1.3918 Training g_loss: 0.6491 Training q_loss: 81.3162 Explore P: 0.7301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 48.0 Average reward fake: 0.4801059663295746 Average reward real: 0.45427584648132324 Training d_loss: 1.4594 Training g_loss: 0.7700 Training q_loss: 2565.3296 Explore P: 0.7267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 14.0 Average reward fake: 0.43292173743247986 Average reward real: 0.4528554379940033 Training d_loss: 1.3666 Training g_loss: 0.8635 Training q_loss: 754.4774 Explore P: 0.7257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 13.0 Average reward fake: 0.3697804808616638 Average reward real: 0.490406334400177 Training d_loss: 1.1919 Training g_loss: 1.0280 Training q_loss: 1137.3750 Explore P: 0.7247\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 11.0 Average reward fake: 0.36994630098342896 Average reward real: 0.4471687376499176 Training d_loss: 1.3112 Training g_loss: 1.0250 Training q_loss: 885.6873 Explore P: 0.7239\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 29.0 Average reward fake: 0.5477677583694458 Average reward real: 0.5470129251480103 Training d_loss: 1.4245 Training g_loss: 0.6170 Training q_loss: 1319.0149 Explore P: 0.7219\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 14.0 Average reward fake: 0.5180178880691528 Average reward real: 0.5246345400810242 Training d_loss: 1.3852 Training g_loss: 0.6671 Training q_loss: 2961.8350 Explore P: 0.7209\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 14.0 Average reward fake: 0.508725106716156 Average reward real: 0.5042039752006531 Training d_loss: 1.4007 Training g_loss: 0.6857 Training q_loss: 164.8212 Explore P: 0.7199\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 12.0 Average reward fake: 0.5184776186943054 Average reward real: 0.5324693322181702 Training d_loss: 1.3654 Training g_loss: 0.6596 Training q_loss: 150.7093 Explore P: 0.7190\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 38.0 Average reward fake: 0.47569188475608826 Average reward real: 0.5328116416931152 Training d_loss: 1.2985 Training g_loss: 0.7597 Training q_loss: 800.6892 Explore P: 0.7163\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 13.0 Average reward fake: 0.47547999024391174 Average reward real: 0.5146859288215637 Training d_loss: 1.3276 Training g_loss: 0.7524 Training q_loss: 108.6393 Explore P: 0.7154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 34.0 Average reward fake: 0.5025621652603149 Average reward real: 0.5047529935836792 Training d_loss: 1.3953 Training g_loss: 0.6974 Training q_loss: 167.5672 Explore P: 0.7130\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 41.0 Average reward fake: 0.4785689413547516 Average reward real: 0.48079466819763184 Training d_loss: 1.3861 Training g_loss: 0.7379 Training q_loss: 65.7166 Explore P: 0.7102\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 23.0 Average reward fake: 0.49253496527671814 Average reward real: 0.48570236563682556 Training d_loss: 1.4024 Training g_loss: 0.7067 Training q_loss: 116.4304 Explore P: 0.7085\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 58.0 Average reward fake: 0.49597740173339844 Average reward real: 0.5029596090316772 Training d_loss: 1.3736 Training g_loss: 0.7023 Training q_loss: 2872.5566 Explore P: 0.7045\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 37.0 Average reward fake: 0.4831382632255554 Average reward real: 0.4903552532196045 Training d_loss: 1.3772 Training g_loss: 0.7350 Training q_loss: 2464.6450 Explore P: 0.7019\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 65.0 Average reward fake: 0.4926789700984955 Average reward real: 0.5045850872993469 Training d_loss: 1.3633 Training g_loss: 0.7133 Training q_loss: 6287.1431 Explore P: 0.6975\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 46.0 Average reward fake: 0.48634833097457886 Average reward real: 0.5024539828300476 Training d_loss: 1.3555 Training g_loss: 0.7253 Training q_loss: 82.6826 Explore P: 0.6943\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 106.0 Average reward fake: 0.4911669194698334 Average reward real: 0.5054983496665955 Training d_loss: 1.3613 Training g_loss: 0.7141 Training q_loss: 315.2794 Explore P: 0.6871\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 29.0 Average reward fake: 0.5169877409934998 Average reward real: 0.4887772500514984 Training d_loss: 1.4449 Training g_loss: 0.6654 Training q_loss: 62.7774 Explore P: 0.6851\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 32.0 Average reward fake: 0.49633607268333435 Average reward real: 0.5166206955909729 Training d_loss: 1.3489 Training g_loss: 0.7042 Training q_loss: 77.3790 Explore P: 0.6830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 30.0 Average reward fake: 0.4817740321159363 Average reward real: 0.4661791920661926 Training d_loss: 1.4412 Training g_loss: 0.7440 Training q_loss: 1559.1760 Explore P: 0.6810\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 50.0 Average reward fake: 0.5189191102981567 Average reward real: 0.5364866256713867 Training d_loss: 1.3589 Training g_loss: 0.6593 Training q_loss: 2158.5881 Explore P: 0.6776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 24.0 Average reward fake: 0.48757752776145935 Average reward real: 0.48373961448669434 Training d_loss: 1.3984 Training g_loss: 0.7276 Training q_loss: 470.5263 Explore P: 0.6760\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 31.0 Average reward fake: 0.4455742835998535 Average reward real: 0.4981580376625061 Training d_loss: 1.2946 Training g_loss: 0.8314 Training q_loss: 4290.2744 Explore P: 0.6739\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 47.0 Average reward fake: 0.5009905695915222 Average reward real: 0.5040945410728455 Training d_loss: 1.3864 Training g_loss: 0.7019 Training q_loss: 101.1640 Explore P: 0.6708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 33.0 Average reward fake: 0.4372301995754242 Average reward real: 0.5218589305877686 Training d_loss: 1.2388 Training g_loss: 0.8603 Training q_loss: 100.8745 Explore P: 0.6687\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 22.0 Average reward fake: 0.5466461181640625 Average reward real: 0.49167633056640625 Training d_loss: 1.5087 Training g_loss: 0.6114 Training q_loss: 90.1590 Explore P: 0.6672\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 26.0 Average reward fake: 0.4650149941444397 Average reward real: 0.48499029874801636 Training d_loss: 1.3569 Training g_loss: 0.7747 Training q_loss: 112.7888 Explore P: 0.6655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 15.0 Average reward fake: 0.5192524194717407 Average reward real: 0.5204927325248718 Training d_loss: 1.3894 Training g_loss: 0.6594 Training q_loss: 212.8146 Explore P: 0.6645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 45.0 Average reward fake: 0.4909808039665222 Average reward real: 0.5321427583694458 Training d_loss: 1.3108 Training g_loss: 0.7193 Training q_loss: 68.2256 Explore P: 0.6616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 38.0 Average reward fake: 0.5260109901428223 Average reward real: 0.5030769109725952 Training d_loss: 1.4411 Training g_loss: 0.6555 Training q_loss: 112.5218 Explore P: 0.6591\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 26.0 Average reward fake: 0.5167390704154968 Average reward real: 0.5160118341445923 Training d_loss: 1.3918 Training g_loss: 0.6632 Training q_loss: 99.7986 Explore P: 0.6574\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 66.0 Average reward fake: 0.519392728805542 Average reward real: 0.4848322868347168 Training d_loss: 1.4623 Training g_loss: 0.6620 Training q_loss: 237.2868 Explore P: 0.6532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 17.0 Average reward fake: 0.5101035833358765 Average reward real: 0.5147465467453003 Training d_loss: 1.3789 Training g_loss: 0.6782 Training q_loss: 147.7136 Explore P: 0.6521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 23.0 Average reward fake: 0.47422972321510315 Average reward real: 0.5019156336784363 Training d_loss: 1.3418 Training g_loss: 0.7676 Training q_loss: 206.9288 Explore P: 0.6506\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 31.0 Average reward fake: 0.523684024810791 Average reward real: 0.5180593729019165 Training d_loss: 1.4008 Training g_loss: 0.6497 Training q_loss: 1574.0496 Explore P: 0.6486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 56.0 Average reward fake: 0.4815329611301422 Average reward real: 0.5127205848693848 Training d_loss: 1.3386 Training g_loss: 0.7635 Training q_loss: 62.8741 Explore P: 0.6451\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 32.0 Average reward fake: 0.4891274869441986 Average reward real: 0.5080695748329163 Training d_loss: 1.3531 Training g_loss: 0.7194 Training q_loss: 1656.0593 Explore P: 0.6430\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 26.0 Average reward fake: 0.5266059637069702 Average reward real: 0.5150457620620728 Training d_loss: 1.4132 Training g_loss: 0.6460 Training q_loss: 94.1061 Explore P: 0.6414\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 21.0 Average reward fake: 0.46711307764053345 Average reward real: 0.4728073477745056 Training d_loss: 1.4453 Training g_loss: 0.8097 Training q_loss: 245.8636 Explore P: 0.6401\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 37.0 Average reward fake: 0.5053250789642334 Average reward real: 0.5138357281684875 Training d_loss: 1.4034 Training g_loss: 0.7026 Training q_loss: 97.4955 Explore P: 0.6377\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 13.0 Average reward fake: 0.48857003450393677 Average reward real: 0.5356072187423706 Training d_loss: 1.3219 Training g_loss: 0.7294 Training q_loss: 164.8970 Explore P: 0.6369\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 33.0 Average reward fake: 0.5133499503135681 Average reward real: 0.5544124841690063 Training d_loss: 1.3240 Training g_loss: 0.6883 Training q_loss: 67.6657 Explore P: 0.6348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 52.0 Average reward fake: 0.4752345681190491 Average reward real: 0.5138475298881531 Training d_loss: 1.3176 Training g_loss: 0.7549 Training q_loss: 725.0107 Explore P: 0.6316\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 45.0 Average reward fake: 0.46479901671409607 Average reward real: 0.4847899377346039 Training d_loss: 1.3506 Training g_loss: 0.7796 Training q_loss: 1718.8402 Explore P: 0.6288\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 56.0 Average reward fake: 0.4728773534297943 Average reward real: 0.5283764600753784 Training d_loss: 1.3029 Training g_loss: 0.7963 Training q_loss: 102.2484 Explore P: 0.6254\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 8.0 Average reward fake: 0.559884250164032 Average reward real: 0.5192031264305115 Training d_loss: 1.4838 Training g_loss: 0.5856 Training q_loss: 73.5443 Explore P: 0.6249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 35.0 Average reward fake: 0.5168954133987427 Average reward real: 0.5220125913619995 Training d_loss: 1.3788 Training g_loss: 0.6582 Training q_loss: 2267.4773 Explore P: 0.6227\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 19.0 Average reward fake: 0.5414999723434448 Average reward real: 0.5319979786872864 Training d_loss: 1.4137 Training g_loss: 0.6215 Training q_loss: 311.8094 Explore P: 0.6216\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 13.0 Average reward fake: 0.4666264057159424 Average reward real: 0.494805246591568 Training d_loss: 1.3481 Training g_loss: 0.7796 Training q_loss: 147.9433 Explore P: 0.6208\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 19.0 Average reward fake: 0.5550023913383484 Average reward real: 0.5319329500198364 Training d_loss: 1.4495 Training g_loss: 0.5924 Training q_loss: 4270.0234 Explore P: 0.6196\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 45.0 Average reward fake: 0.5354796648025513 Average reward real: 0.539753794670105 Training d_loss: 1.3957 Training g_loss: 0.6308 Training q_loss: 129.7104 Explore P: 0.6169\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 32.0 Average reward fake: 0.48614150285720825 Average reward real: 0.5666135549545288 Training d_loss: 1.2646 Training g_loss: 0.7666 Training q_loss: 278.4820 Explore P: 0.6149\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 72.0 Average reward fake: 0.5282858610153198 Average reward real: 0.5168203711509705 Training d_loss: 1.4158 Training g_loss: 0.6458 Training q_loss: 958.2808 Explore P: 0.6106\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 63.0 Average reward fake: 0.42344599962234497 Average reward real: 0.4876941740512848 Training d_loss: 1.2855 Training g_loss: 0.9155 Training q_loss: 107.7861 Explore P: 0.6068\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 61.0 Average reward fake: 0.48608484864234924 Average reward real: 0.4984307885169983 Training d_loss: 1.4111 Training g_loss: 0.7944 Training q_loss: 70.9301 Explore P: 0.6032\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 55.0 Average reward fake: 0.5102697610855103 Average reward real: 0.5120271444320679 Training d_loss: 1.3901 Training g_loss: 0.6775 Training q_loss: 87.9516 Explore P: 0.5999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 22.0 Average reward fake: 0.48119989037513733 Average reward real: 0.5127352476119995 Training d_loss: 1.3379 Training g_loss: 0.7480 Training q_loss: 36.1604 Explore P: 0.5986\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 90.0 Average reward fake: 0.5225791931152344 Average reward real: 0.49479493498802185 Training d_loss: 1.4599 Training g_loss: 0.6715 Training q_loss: 134.9679 Explore P: 0.5934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 25.0 Average reward fake: 0.5069558024406433 Average reward real: 0.5080951452255249 Training d_loss: 1.3889 Training g_loss: 0.6903 Training q_loss: 472.8653 Explore P: 0.5919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 13.0 Average reward fake: 0.48384609818458557 Average reward real: 0.5370556712150574 Training d_loss: 1.2858 Training g_loss: 0.7476 Training q_loss: 1270.1859 Explore P: 0.5911\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 42.0 Average reward fake: 0.4758886694908142 Average reward real: 0.5883839130401611 Training d_loss: 1.2186 Training g_loss: 0.7632 Training q_loss: 2033.8926 Explore P: 0.5887\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 22.0 Average reward fake: 0.4690534174442291 Average reward real: 0.5238016247749329 Training d_loss: 1.2939 Training g_loss: 0.7983 Training q_loss: 267.3661 Explore P: 0.5874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 16.0 Average reward fake: 0.3752637505531311 Average reward real: 0.5670112371444702 Training d_loss: 1.0606 Training g_loss: 1.0901 Training q_loss: 152.9693 Explore P: 0.5865\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 108.0 Average reward fake: 0.39408615231513977 Average reward real: 0.5791492462158203 Training d_loss: 1.0573 Training g_loss: 1.0049 Training q_loss: 67.6687 Explore P: 0.5803\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 67.0 Average reward fake: 0.5188681483268738 Average reward real: 0.5396884679794312 Training d_loss: 1.3606 Training g_loss: 0.6890 Training q_loss: 130.2181 Explore P: 0.5765\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 69.0 Average reward fake: 0.44355034828186035 Average reward real: 0.5003504753112793 Training d_loss: 1.3531 Training g_loss: 0.9151 Training q_loss: 1258.5781 Explore P: 0.5726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 21.0 Average reward fake: 0.4690150320529938 Average reward real: 0.4205477237701416 Training d_loss: 1.5789 Training g_loss: 0.8778 Training q_loss: 136.6572 Explore P: 0.5714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 102.0 Average reward fake: 0.510321855545044 Average reward real: 0.5566516518592834 Training d_loss: 1.3503 Training g_loss: 0.6897 Training q_loss: 1830.8881 Explore P: 0.5657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 25.0 Average reward fake: 0.4519920349121094 Average reward real: 0.57103431224823 Training d_loss: 1.1924 Training g_loss: 0.8231 Training q_loss: 306.8773 Explore P: 0.5644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 57.0 Average reward fake: 0.5311617851257324 Average reward real: 0.5065652132034302 Training d_loss: 1.4450 Training g_loss: 0.6405 Training q_loss: 2965.3635 Explore P: 0.5612\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 26.0 Average reward fake: 0.5388742685317993 Average reward real: 0.5354067087173462 Training d_loss: 1.4065 Training g_loss: 0.6414 Training q_loss: 2705.0710 Explore P: 0.5598\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 103.0 Average reward fake: 0.4354932904243469 Average reward real: 0.5199504494667053 Training d_loss: 1.2582 Training g_loss: 0.9546 Training q_loss: 42.4585 Explore P: 0.5541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 26.0 Average reward fake: 0.4763771891593933 Average reward real: 0.5393069386482239 Training d_loss: 1.3077 Training g_loss: 0.8257 Training q_loss: 87.8300 Explore P: 0.5527\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 43.0 Average reward fake: 0.4809732437133789 Average reward real: 0.48087430000305176 Training d_loss: 1.4255 Training g_loss: 0.8530 Training q_loss: 129.2370 Explore P: 0.5504\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 40.0 Average reward fake: 0.4737076759338379 Average reward real: 0.4876183569431305 Training d_loss: 1.3670 Training g_loss: 0.7607 Training q_loss: 677.6093 Explore P: 0.5482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 26.0 Average reward fake: 0.5634433031082153 Average reward real: 0.563328742980957 Training d_loss: 1.4153 Training g_loss: 0.5828 Training q_loss: 76.0040 Explore P: 0.5468\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 42.0 Average reward fake: 0.5282644033432007 Average reward real: 0.5222952961921692 Training d_loss: 1.4067 Training g_loss: 0.6503 Training q_loss: 540.0565 Explore P: 0.5446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 37.0 Average reward fake: 0.4998428225517273 Average reward real: 0.5337164998054504 Training d_loss: 1.3277 Training g_loss: 0.7250 Training q_loss: 2116.4255 Explore P: 0.5426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 49.0 Average reward fake: 0.4613123834133148 Average reward real: 0.5381189584732056 Training d_loss: 1.2590 Training g_loss: 0.8439 Training q_loss: 1274.8816 Explore P: 0.5400\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 29.0 Average reward fake: 0.45995235443115234 Average reward real: 0.4978862404823303 Training d_loss: 1.3459 Training g_loss: 0.8746 Training q_loss: 150.7794 Explore P: 0.5385\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 29.0 Average reward fake: 0.509513258934021 Average reward real: 0.49730032682418823 Training d_loss: 1.4541 Training g_loss: 0.8138 Training q_loss: 2577.2349 Explore P: 0.5369\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 75.0 Average reward fake: 0.415560245513916 Average reward real: 0.5099574327468872 Training d_loss: 1.2338 Training g_loss: 0.9780 Training q_loss: 22.0181 Explore P: 0.5330\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 84.0 Average reward fake: 0.37806904315948486 Average reward real: 0.5818079710006714 Training d_loss: 1.0848 Training g_loss: 1.3214 Training q_loss: 1225.2579 Explore P: 0.5286\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 53.0 Average reward fake: 0.5159441232681274 Average reward real: 0.5291526913642883 Training d_loss: 1.3757 Training g_loss: 0.6728 Training q_loss: 125.1348 Explore P: 0.5259\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 80.0 Average reward fake: 0.49675464630126953 Average reward real: 0.49313244223594666 Training d_loss: 1.3977 Training g_loss: 0.7033 Training q_loss: 78.2382 Explore P: 0.5218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 53.0 Average reward fake: 0.46325331926345825 Average reward real: 0.53147292137146 Training d_loss: 1.2828 Training g_loss: 0.8510 Training q_loss: 318.7446 Explore P: 0.5191\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 62.0 Average reward fake: 0.43486160039901733 Average reward real: 0.5484760403633118 Training d_loss: 1.2052 Training g_loss: 1.0116 Training q_loss: 2033.4489 Explore P: 0.5159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 21.0 Average reward fake: 0.4820828437805176 Average reward real: 0.45172953605651855 Training d_loss: 1.4853 Training g_loss: 0.9069 Training q_loss: 81.7492 Explore P: 0.5149\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 32.0 Average reward fake: 0.5008904337882996 Average reward real: 0.5007085204124451 Training d_loss: 1.3927 Training g_loss: 0.7017 Training q_loss: 98.8017 Explore P: 0.5133\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 48.0 Average reward fake: 0.42146554589271545 Average reward real: 0.5478017926216125 Training d_loss: 1.1576 Training g_loss: 0.9001 Training q_loss: 932.7516 Explore P: 0.5108\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 30.0 Average reward fake: 0.3774030804634094 Average reward real: 0.5174871683120728 Training d_loss: 1.1822 Training g_loss: 1.2053 Training q_loss: 161.6676 Explore P: 0.5093\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 63.0 Average reward fake: 0.4601052403450012 Average reward real: 0.5077998042106628 Training d_loss: 1.3701 Training g_loss: 0.9203 Training q_loss: 165.2660 Explore P: 0.5062\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 70.0 Average reward fake: 0.4420657753944397 Average reward real: 0.5908954739570618 Training d_loss: 1.1814 Training g_loss: 1.0888 Training q_loss: 554.2374 Explore P: 0.5028\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 14.0 Average reward fake: 0.46735087037086487 Average reward real: 0.4716559052467346 Training d_loss: 1.4186 Training g_loss: 0.8846 Training q_loss: 3193.1877 Explore P: 0.5021\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 68.0 Average reward fake: 0.49446073174476624 Average reward real: 0.5292445421218872 Training d_loss: 1.3244 Training g_loss: 0.7142 Training q_loss: 23.7570 Explore P: 0.4987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 24.0 Average reward fake: 0.6039156913757324 Average reward real: 0.5222240090370178 Training d_loss: 1.6179 Training g_loss: 0.5478 Training q_loss: 1482.8436 Explore P: 0.4976\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 63.0 Average reward fake: 0.4633130431175232 Average reward real: 0.5344065427780151 Training d_loss: 1.2746 Training g_loss: 0.9084 Training q_loss: 54.7590 Explore P: 0.4945\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 38.0 Average reward fake: 0.44990333914756775 Average reward real: 0.5101233720779419 Training d_loss: 1.2942 Training g_loss: 0.9889 Training q_loss: 1939.3494 Explore P: 0.4927\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 13.0 Average reward fake: 0.5161279439926147 Average reward real: 0.533299446105957 Training d_loss: 1.3664 Training g_loss: 0.6715 Training q_loss: 77.7516 Explore P: 0.4920\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 33.0 Average reward fake: 0.42002350091934204 Average reward real: 0.5142834186553955 Training d_loss: 1.2516 Training g_loss: 1.1239 Training q_loss: 1374.0295 Explore P: 0.4904\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 28.0 Average reward fake: 0.39204201102256775 Average reward real: 0.46898603439331055 Training d_loss: 1.3336 Training g_loss: 1.0823 Training q_loss: 860.7867 Explore P: 0.4891\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 53.0 Average reward fake: 0.3722448945045471 Average reward real: 0.5701927542686462 Training d_loss: 1.0997 Training g_loss: 1.4360 Training q_loss: 48.3938 Explore P: 0.4866\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 54.0 Average reward fake: 0.4867965579032898 Average reward real: 0.5414261221885681 Training d_loss: 1.3287 Training g_loss: 0.9031 Training q_loss: 3821.4839 Explore P: 0.4840\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 66.0 Average reward fake: 0.4269035756587982 Average reward real: 0.5628843903541565 Training d_loss: 1.2263 Training g_loss: 1.3221 Training q_loss: 60.8063 Explore P: 0.4809\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 89.0 Average reward fake: 0.449031263589859 Average reward real: 0.4991932511329651 Training d_loss: 1.4947 Training g_loss: 1.0818 Training q_loss: 54.1242 Explore P: 0.4767\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 44.0 Average reward fake: 0.38168734312057495 Average reward real: 0.4934817850589752 Training d_loss: 1.2521 Training g_loss: 1.3468 Training q_loss: 108.3203 Explore P: 0.4747\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 44.0 Average reward fake: 0.4920257031917572 Average reward real: 0.5283344984054565 Training d_loss: 1.3546 Training g_loss: 0.7388 Training q_loss: 88.7424 Explore P: 0.4726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 86.0 Average reward fake: 0.4529756009578705 Average reward real: 0.5363233685493469 Training d_loss: 1.2506 Training g_loss: 0.8019 Training q_loss: 41.7715 Explore P: 0.4687\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 45.0 Average reward fake: 0.3282853364944458 Average reward real: 0.48969578742980957 Training d_loss: 1.1661 Training g_loss: 1.4486 Training q_loss: 64.5776 Explore P: 0.4666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 114.0 Average reward fake: 0.38798433542251587 Average reward real: 0.5026767253875732 Training d_loss: 1.2269 Training g_loss: 1.1496 Training q_loss: 2943.6401 Explore P: 0.4614\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 84.0 Average reward fake: 0.467550665140152 Average reward real: 0.4602307677268982 Training d_loss: 1.4833 Training g_loss: 0.8114 Training q_loss: 231.8118 Explore P: 0.4576\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 40.0 Average reward fake: 0.5346808433532715 Average reward real: 0.5499526858329773 Training d_loss: 1.3756 Training g_loss: 0.6412 Training q_loss: 55.7641 Explore P: 0.4559\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 36.0 Average reward fake: 0.5174146890640259 Average reward real: 0.6188694834709167 Training d_loss: 1.2814 Training g_loss: 0.7178 Training q_loss: 128.0393 Explore P: 0.4543\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 130.0 Average reward fake: 0.5193462371826172 Average reward real: 0.5641603469848633 Training d_loss: 1.3455 Training g_loss: 0.7065 Training q_loss: 1274.5110 Explore P: 0.4485\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 51.0 Average reward fake: 0.5664507746696472 Average reward real: 0.5628076791763306 Training d_loss: 1.4156 Training g_loss: 0.5956 Training q_loss: 63.6143 Explore P: 0.4463\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 103.0 Average reward fake: 0.41841191053390503 Average reward real: 0.5052276253700256 Training d_loss: 1.2620 Training g_loss: 0.8771 Training q_loss: 29.3485 Explore P: 0.4418\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 14.0 Average reward fake: 0.4919634759426117 Average reward real: 0.5915565490722656 Training d_loss: 1.2459 Training g_loss: 0.8011 Training q_loss: 48.6464 Explore P: 0.4412\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 77.0 Average reward fake: 0.5004660487174988 Average reward real: 0.5066578984260559 Training d_loss: 1.3757 Training g_loss: 0.7056 Training q_loss: 30.4585 Explore P: 0.4379\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 36.0 Average reward fake: 0.5026819109916687 Average reward real: 0.5440407991409302 Training d_loss: 1.3186 Training g_loss: 0.6993 Training q_loss: 79.5891 Explore P: 0.4364\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 25.0 Average reward fake: 0.4660130441188812 Average reward real: 0.5282572507858276 Training d_loss: 1.3179 Training g_loss: 0.8249 Training q_loss: 167.4584 Explore P: 0.4353\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 48.0 Average reward fake: 0.5439616441726685 Average reward real: 0.5255358219146729 Training d_loss: 1.4322 Training g_loss: 0.6094 Training q_loss: 33.4311 Explore P: 0.4333\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 58.0 Average reward fake: 0.545762836933136 Average reward real: 0.5772985816001892 Training d_loss: 1.3533 Training g_loss: 0.6120 Training q_loss: 30.7723 Explore P: 0.4308\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 42.0 Average reward fake: 0.4362109303474426 Average reward real: 0.5120466947555542 Training d_loss: 1.2700 Training g_loss: 1.0317 Training q_loss: 108.1025 Explore P: 0.4291\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 26.0 Average reward fake: 0.46622323989868164 Average reward real: 0.4624517560005188 Training d_loss: 1.4495 Training g_loss: 0.9444 Training q_loss: 98.3774 Explore P: 0.4280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 41.0 Average reward fake: 0.5044244527816772 Average reward real: 0.5037802457809448 Training d_loss: 1.4412 Training g_loss: 0.7036 Training q_loss: 94.8034 Explore P: 0.4263\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 47.0 Average reward fake: 0.4934775233268738 Average reward real: 0.4832940995693207 Training d_loss: 1.4225 Training g_loss: 0.7214 Training q_loss: 311.3288 Explore P: 0.4243\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 25.0 Average reward fake: 0.5377262830734253 Average reward real: 0.6641713976860046 Training d_loss: 1.3596 Training g_loss: 0.7693 Training q_loss: 53.1891 Explore P: 0.4233\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 79.0 Average reward fake: 0.5191397666931152 Average reward real: 0.5486355423927307 Training d_loss: 1.3926 Training g_loss: 0.7054 Training q_loss: 524.8895 Explore P: 0.4200\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 60.0 Average reward fake: 0.523302435874939 Average reward real: 0.4953405261039734 Training d_loss: 1.4685 Training g_loss: 0.6539 Training q_loss: 167.3816 Explore P: 0.4176\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 39.0 Average reward fake: 0.5636488199234009 Average reward real: 0.567628026008606 Training d_loss: 1.4572 Training g_loss: 0.6130 Training q_loss: 124.2892 Explore P: 0.4160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 31.0 Average reward fake: 0.4782748222351074 Average reward real: 0.5170283317565918 Training d_loss: 1.3239 Training g_loss: 0.7605 Training q_loss: 88.4557 Explore P: 0.4147\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 81.0 Average reward fake: 0.5422784686088562 Average reward real: 0.5837637186050415 Training d_loss: 1.3497 Training g_loss: 0.6375 Training q_loss: 155.0158 Explore P: 0.4115\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 35.0 Average reward fake: 0.49829840660095215 Average reward real: 0.4965823292732239 Training d_loss: 1.4362 Training g_loss: 0.9300 Training q_loss: 103.0061 Explore P: 0.4101\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 37.0 Average reward fake: 0.5153353214263916 Average reward real: 0.4764803349971771 Training d_loss: 1.5213 Training g_loss: 0.7057 Training q_loss: 48.3020 Explore P: 0.4086\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 48.0 Average reward fake: 0.44886094331741333 Average reward real: 0.5219293236732483 Training d_loss: 1.2766 Training g_loss: 1.0348 Training q_loss: 165.1789 Explore P: 0.4067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 39.0 Average reward fake: 0.4318363070487976 Average reward real: 0.5772029757499695 Training d_loss: 1.1780 Training g_loss: 0.9890 Training q_loss: 29.0415 Explore P: 0.4051\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 50.0 Average reward fake: 0.4067467153072357 Average reward real: 0.5804445743560791 Training d_loss: 1.2183 Training g_loss: 0.9827 Training q_loss: 451.4853 Explore P: 0.4032\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 20.0 Average reward fake: 0.4397449493408203 Average reward real: 0.6580438613891602 Training d_loss: 1.1184 Training g_loss: 1.1816 Training q_loss: 99.6889 Explore P: 0.4024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 51.0 Average reward fake: 0.5603473782539368 Average reward real: 0.5248750448226929 Training d_loss: 1.5193 Training g_loss: 0.6123 Training q_loss: 115.0907 Explore P: 0.4004\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 30.0 Average reward fake: 0.4840254783630371 Average reward real: 0.4938201308250427 Training d_loss: 1.3776 Training g_loss: 0.7346 Training q_loss: 232.0405 Explore P: 0.3992\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 21.0 Average reward fake: 0.44774001836776733 Average reward real: 0.5602716207504272 Training d_loss: 1.2598 Training g_loss: 1.0121 Training q_loss: 940.7043 Explore P: 0.3984\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 26.0 Average reward fake: 0.45800209045410156 Average reward real: 0.49682754278182983 Training d_loss: 1.3625 Training g_loss: 0.9732 Training q_loss: 29.8407 Explore P: 0.3974\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 70.0 Average reward fake: 0.36150866746902466 Average reward real: 0.4877399802207947 Training d_loss: 1.2073 Training g_loss: 1.3207 Training q_loss: 88.4479 Explore P: 0.3947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 91.0 Average reward fake: 0.49147504568099976 Average reward real: 0.5397024750709534 Training d_loss: 1.3202 Training g_loss: 0.7414 Training q_loss: 4534.4346 Explore P: 0.3912\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 68.0 Average reward fake: 0.3961736559867859 Average reward real: 0.5311151742935181 Training d_loss: 1.1742 Training g_loss: 1.1221 Training q_loss: 234.1931 Explore P: 0.3886\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 63.0 Average reward fake: 0.3347374498844147 Average reward real: 0.5414596199989319 Training d_loss: 1.0695 Training g_loss: 1.4617 Training q_loss: 63.6905 Explore P: 0.3862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 35.0 Average reward fake: 0.4720606803894043 Average reward real: 0.5671070218086243 Training d_loss: 1.2764 Training g_loss: 0.9716 Training q_loss: 456.8264 Explore P: 0.3849\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 50.0 Average reward fake: 0.5820040106773376 Average reward real: 0.5153404474258423 Training d_loss: 1.5491 Training g_loss: 0.5496 Training q_loss: 347.9914 Explore P: 0.3830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 61.0 Average reward fake: 0.43212753534317017 Average reward real: 0.48331719636917114 Training d_loss: 1.3096 Training g_loss: 0.8784 Training q_loss: 142.1873 Explore P: 0.3808\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 54.0 Average reward fake: 0.4138912558555603 Average reward real: 0.5176466107368469 Training d_loss: 1.2336 Training g_loss: 0.9812 Training q_loss: 401.7723 Explore P: 0.3788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 46.0 Average reward fake: 0.41058793663978577 Average reward real: 0.6227267980575562 Training d_loss: 1.1311 Training g_loss: 0.9863 Training q_loss: 68.4914 Explore P: 0.3771\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 47.0 Average reward fake: 0.4596974849700928 Average reward real: 0.5271209478378296 Training d_loss: 1.2804 Training g_loss: 0.8236 Training q_loss: 23.7335 Explore P: 0.3754\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 32.0 Average reward fake: 0.490100622177124 Average reward real: 0.5157594084739685 Training d_loss: 1.3868 Training g_loss: 0.7339 Training q_loss: 198.3786 Explore P: 0.3742\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 59.0 Average reward fake: 0.503983199596405 Average reward real: 0.5101648569107056 Training d_loss: 1.3774 Training g_loss: 0.6844 Training q_loss: 95.7132 Explore P: 0.3721\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 35.0 Average reward fake: 0.4569510519504547 Average reward real: 0.5008076429367065 Training d_loss: 1.3706 Training g_loss: 0.8137 Training q_loss: 4127.0234 Explore P: 0.3708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 22.0 Average reward fake: 0.473447322845459 Average reward real: 0.5483694076538086 Training d_loss: 1.3411 Training g_loss: 0.7828 Training q_loss: 102.0734 Explore P: 0.3700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 33.0 Average reward fake: 0.48308515548706055 Average reward real: 0.46907156705856323 Training d_loss: 1.4302 Training g_loss: 0.7384 Training q_loss: 77.4222 Explore P: 0.3688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 90.0 Average reward fake: 0.4567205309867859 Average reward real: 0.4871769845485687 Training d_loss: 1.3451 Training g_loss: 0.8199 Training q_loss: 103.2963 Explore P: 0.3656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 43.0 Average reward fake: 0.4363003671169281 Average reward real: 0.4845685064792633 Training d_loss: 1.3741 Training g_loss: 1.0676 Training q_loss: 89.6333 Explore P: 0.3641\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 77.0 Average reward fake: 0.4944499433040619 Average reward real: 0.5174352526664734 Training d_loss: 1.3610 Training g_loss: 0.7103 Training q_loss: 69.1560 Explore P: 0.3614\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 130.0 Average reward fake: 0.5008987784385681 Average reward real: 0.5465412735939026 Training d_loss: 1.3179 Training g_loss: 0.7048 Training q_loss: 636.3845 Explore P: 0.3568\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 51.0 Average reward fake: 0.5081284046173096 Average reward real: 0.519713282585144 Training d_loss: 1.3835 Training g_loss: 0.6975 Training q_loss: 83.2026 Explore P: 0.3551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 87.0 Average reward fake: 0.4059487283229828 Average reward real: 0.5178050994873047 Training d_loss: 1.2247 Training g_loss: 1.2993 Training q_loss: 1173.4679 Explore P: 0.3521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 76.0 Average reward fake: 0.5164112448692322 Average reward real: 0.5162405967712402 Training d_loss: 1.3889 Training g_loss: 0.6697 Training q_loss: 18.8712 Explore P: 0.3495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 156.0 Average reward fake: 0.4370171129703522 Average reward real: 0.5258147716522217 Training d_loss: 1.2376 Training g_loss: 0.9086 Training q_loss: 83.2289 Explore P: 0.3442\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 67.0 Average reward fake: 0.45231738686561584 Average reward real: 0.5743881464004517 Training d_loss: 1.2152 Training g_loss: 0.8533 Training q_loss: 57.3866 Explore P: 0.3420\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 74.0 Average reward fake: 0.5229189395904541 Average reward real: 0.5319463610649109 Training d_loss: 1.4359 Training g_loss: 0.8282 Training q_loss: 156.1101 Explore P: 0.3395\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 187.0 Average reward fake: 0.4286040663719177 Average reward real: 0.5102323293685913 Training d_loss: 1.2838 Training g_loss: 1.1847 Training q_loss: 16.0781 Explore P: 0.3334\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 76.0 Average reward fake: 0.5359446406364441 Average reward real: 0.5328087210655212 Training d_loss: 1.4033 Training g_loss: 0.6342 Training q_loss: 42.3100 Explore P: 0.3310\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 323 Total reward: 104.0 Average reward fake: 0.5191971659660339 Average reward real: 0.517368495464325 Training d_loss: 1.3925 Training g_loss: 0.6557 Training q_loss: 104.9964 Explore P: 0.3277\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 324 Total reward: 112.0 Average reward fake: 0.4584777355194092 Average reward real: 0.5031973123550415 Training d_loss: 1.3338 Training g_loss: 0.9764 Training q_loss: 2411.8755 Explore P: 0.3241\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 325 Total reward: 166.0 Average reward fake: 0.47790542244911194 Average reward real: 0.51890629529953 Training d_loss: 1.3509 Training g_loss: 0.9315 Training q_loss: 45.2150 Explore P: 0.3190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 326 Total reward: 120.0 Average reward fake: 0.47016310691833496 Average reward real: 0.5285084247589111 Training d_loss: 1.3322 Training g_loss: 0.9623 Training q_loss: 49.9123 Explore P: 0.3153\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 327 Total reward: 177.0 Average reward fake: 0.5043722987174988 Average reward real: 0.5383512377738953 Training d_loss: 1.3396 Training g_loss: 0.6935 Training q_loss: 100.9293 Explore P: 0.3099\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 328 Total reward: 199.0 Average reward fake: 0.5307384729385376 Average reward real: 0.5558035373687744 Training d_loss: 1.3448 Training g_loss: 0.6414 Training q_loss: 98.5332 Explore P: 0.3040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 329 Total reward: 199.0 Average reward fake: 0.5082124471664429 Average reward real: 0.49805647134780884 Training d_loss: 1.4188 Training g_loss: 0.6917 Training q_loss: 71.6271 Explore P: 0.2982\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 330 Total reward: 124.0 Average reward fake: 0.5447831749916077 Average reward real: 0.5444446802139282 Training d_loss: 1.3986 Training g_loss: 0.6149 Training q_loss: 105.5812 Explore P: 0.2947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 331 Total reward: 127.0 Average reward fake: 0.42256349325180054 Average reward real: 0.5011138319969177 Training d_loss: 1.2517 Training g_loss: 0.8960 Training q_loss: 69.6391 Explore P: 0.2911\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 332 Total reward: 136.0 Average reward fake: 0.5661264061927795 Average reward real: 0.4794561266899109 Training d_loss: 1.5971 Training g_loss: 0.5646 Training q_loss: 88.0547 Explore P: 0.2873\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 333 Total reward: 91.0 Average reward fake: 0.41896700859069824 Average reward real: 0.58135986328125 Training d_loss: 1.1083 Training g_loss: 0.9204 Training q_loss: 5881.3716 Explore P: 0.2848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 334 Total reward: 182.0 Average reward fake: 0.5462523698806763 Average reward real: 0.5325630307197571 Training d_loss: 1.4336 Training g_loss: 0.6139 Training q_loss: 8681.5498 Explore P: 0.2798\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 335 Total reward: 199.0 Average reward fake: 0.4967775344848633 Average reward real: 0.48765212297439575 Training d_loss: 1.4188 Training g_loss: 0.7182 Training q_loss: 209.7814 Explore P: 0.2745\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 336 Total reward: 199.0 Average reward fake: 0.5127226710319519 Average reward real: 0.5238540768623352 Training d_loss: 1.3867 Training g_loss: 0.6829 Training q_loss: 241.2642 Explore P: 0.2693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 337 Total reward: 151.0 Average reward fake: 0.41899123787879944 Average reward real: 0.5213643312454224 Training d_loss: 1.2350 Training g_loss: 1.0105 Training q_loss: 68.6150 Explore P: 0.2654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 338 Total reward: 199.0 Average reward fake: 0.4669919013977051 Average reward real: 0.4913087785243988 Training d_loss: 1.3666 Training g_loss: 0.9523 Training q_loss: 132.2965 Explore P: 0.2604\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 339 Total reward: 199.0 Average reward fake: 0.5026251077651978 Average reward real: 0.49321380257606506 Training d_loss: 1.4079 Training g_loss: 0.6888 Training q_loss: 155.7205 Explore P: 0.2554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 340 Total reward: 158.0 Average reward fake: 0.4822574257850647 Average reward real: 0.5014914870262146 Training d_loss: 1.3593 Training g_loss: 0.7533 Training q_loss: 46.7991 Explore P: 0.2516\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 341 Total reward: 199.0 Average reward fake: 0.4913284182548523 Average reward real: 0.4931313991546631 Training d_loss: 1.3945 Training g_loss: 0.7278 Training q_loss: 105.9044 Explore P: 0.2468\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 342 Total reward: 199.0 Average reward fake: 0.4379276633262634 Average reward real: 0.492057740688324 Training d_loss: 1.3163 Training g_loss: 0.9784 Training q_loss: 99.5872 Explore P: 0.2422\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 343 Total reward: 199.0 Average reward fake: 0.41168704628944397 Average reward real: 0.620055079460144 Training d_loss: 1.0569 Training g_loss: 0.9307 Training q_loss: 8432.2441 Explore P: 0.2376\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 344 Total reward: 199.0 Average reward fake: 0.5181847810745239 Average reward real: 0.49226346611976624 Training d_loss: 1.4777 Training g_loss: 0.6808 Training q_loss: 169.3659 Explore P: 0.2331\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 345 Total reward: 199.0 Average reward fake: 0.5203058123588562 Average reward real: 0.5044618248939514 Training d_loss: 1.5018 Training g_loss: 0.6782 Training q_loss: 386.6452 Explore P: 0.2287\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 346 Total reward: 199.0 Average reward fake: 0.5125833749771118 Average reward real: 0.507994532585144 Training d_loss: 1.3962 Training g_loss: 0.6692 Training q_loss: 341.7980 Explore P: 0.2244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 347 Total reward: 199.0 Average reward fake: 0.41424861550331116 Average reward real: 0.5212172269821167 Training d_loss: 1.2276 Training g_loss: 1.1884 Training q_loss: 158.7876 Explore P: 0.2202\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 348 Total reward: 199.0 Average reward fake: 0.4863446354866028 Average reward real: 0.4913007616996765 Training d_loss: 1.3988 Training g_loss: 0.7671 Training q_loss: 99.1123 Explore P: 0.2160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 349 Total reward: 199.0 Average reward fake: 0.5100470781326294 Average reward real: 0.5525460243225098 Training d_loss: 1.3110 Training g_loss: 0.6851 Training q_loss: 128.2008 Explore P: 0.2120\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 350 Total reward: 199.0 Average reward fake: 0.5164750814437866 Average reward real: 0.4949769079685211 Training d_loss: 1.4319 Training g_loss: 0.6640 Training q_loss: 396.6813 Explore P: 0.2080\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 351 Total reward: 199.0 Average reward fake: 0.46992382407188416 Average reward real: 0.5110942721366882 Training d_loss: 1.3332 Training g_loss: 0.8473 Training q_loss: 137.5864 Explore P: 0.2041\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 352 Total reward: 199.0 Average reward fake: 0.5189716219902039 Average reward real: 0.5155197978019714 Training d_loss: 1.3991 Training g_loss: 0.6494 Training q_loss: 188.7464 Explore P: 0.2003\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 353 Total reward: 199.0 Average reward fake: 0.5056046843528748 Average reward real: 0.5183359384536743 Training d_loss: 1.3764 Training g_loss: 0.6875 Training q_loss: 423.9535 Explore P: 0.1965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 354 Total reward: 199.0 Average reward fake: 0.45187729597091675 Average reward real: 0.49185389280319214 Training d_loss: 1.3453 Training g_loss: 0.8109 Training q_loss: 737.7093 Explore P: 0.1928\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 355 Total reward: 199.0 Average reward fake: 0.397769033908844 Average reward real: 0.5063229203224182 Training d_loss: 1.2247 Training g_loss: 1.2268 Training q_loss: 154.4035 Explore P: 0.1892\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 356 Total reward: 199.0 Average reward fake: 0.515907883644104 Average reward real: 0.5133193731307983 Training d_loss: 1.4084 Training g_loss: 0.6723 Training q_loss: 68.1565 Explore P: 0.1857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 357 Total reward: 199.0 Average reward fake: 0.5044805407524109 Average reward real: 0.503828227519989 Training d_loss: 1.4506 Training g_loss: 0.7616 Training q_loss: 186.2362 Explore P: 0.1822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 358 Total reward: 199.0 Average reward fake: 0.39752811193466187 Average reward real: 0.5774516463279724 Training d_loss: 1.1651 Training g_loss: 1.1029 Training q_loss: 437.6902 Explore P: 0.1788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 359 Total reward: 127.0 Average reward fake: 0.36656931042671204 Average reward real: 0.6736600995063782 Training d_loss: 0.9440 Training g_loss: 1.0161 Training q_loss: 91.6415 Explore P: 0.1767\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 360 Total reward: 22.0 Average reward fake: 0.3594633638858795 Average reward real: 0.48845377564430237 Training d_loss: 1.2550 Training g_loss: 1.0204 Training q_loss: 1968.1127 Explore P: 0.1764\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 361 Total reward: 9.0 Average reward fake: 0.3696729242801666 Average reward real: 0.6131831407546997 Training d_loss: 1.0396 Training g_loss: 0.9972 Training q_loss: 1363.0374 Explore P: 0.1762\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 362 Total reward: 10.0 Average reward fake: 0.36511677503585815 Average reward real: 0.5837733745574951 Training d_loss: 1.1084 Training g_loss: 1.0064 Training q_loss: 2454.6636 Explore P: 0.1760\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 363 Total reward: 9.0 Average reward fake: 0.3691888451576233 Average reward real: 0.6503551602363586 Training d_loss: 0.9951 Training g_loss: 0.9967 Training q_loss: 1934.2695 Explore P: 0.1759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 364 Total reward: 8.0 Average reward fake: 0.36304378509521484 Average reward real: 0.6482378840446472 Training d_loss: 0.9963 Training g_loss: 1.0165 Training q_loss: 3543.6907 Explore P: 0.1758\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 365 Total reward: 10.0 Average reward fake: 0.33879101276397705 Average reward real: 0.6442315578460693 Training d_loss: 0.9794 Training g_loss: 1.0870 Training q_loss: 1867.0133 Explore P: 0.1756\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 366 Total reward: 10.0 Average reward fake: 0.3368895947933197 Average reward real: 0.7099443078041077 Training d_loss: 0.8608 Training g_loss: 1.0915 Training q_loss: 2582.7410 Explore P: 0.1754\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 367 Total reward: 8.0 Average reward fake: 0.3850948214530945 Average reward real: 0.7012931108474731 Training d_loss: 1.1246 Training g_loss: 1.0326 Training q_loss: 1616.0619 Explore P: 0.1753\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 368 Total reward: 9.0 Average reward fake: 0.323591411113739 Average reward real: 0.7074069380760193 Training d_loss: 0.8588 Training g_loss: 1.1270 Training q_loss: 1623.8845 Explore P: 0.1751\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 369 Total reward: 11.0 Average reward fake: 0.3628038465976715 Average reward real: 0.7183411121368408 Training d_loss: 0.8915 Training g_loss: 1.0098 Training q_loss: 2037.3035 Explore P: 0.1750\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 370 Total reward: 16.0 Average reward fake: 0.35095542669296265 Average reward real: 0.7178520560264587 Training d_loss: 0.8782 Training g_loss: 1.0549 Training q_loss: 3366.5601 Explore P: 0.1747\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 371 Total reward: 12.0 Average reward fake: 0.34300628304481506 Average reward real: 0.7112958431243896 Training d_loss: 0.8633 Training g_loss: 1.0795 Training q_loss: 3806.9458 Explore P: 0.1745\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 372 Total reward: 7.0 Average reward fake: 0.4003832936286926 Average reward real: 0.6981378793716431 Training d_loss: 1.1281 Training g_loss: 0.9831 Training q_loss: 3780.8130 Explore P: 0.1744\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 373 Total reward: 9.0 Average reward fake: 0.32979416847229004 Average reward real: 0.590420663356781 Training d_loss: 1.0546 Training g_loss: 1.1077 Training q_loss: 573809.4375 Explore P: 0.1742\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 374 Total reward: 8.0 Average reward fake: 0.37660449743270874 Average reward real: 0.6099917888641357 Training d_loss: 1.0773 Training g_loss: 0.9783 Training q_loss: 3899.4309 Explore P: 0.1741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 375 Total reward: 12.0 Average reward fake: 0.3414340615272522 Average reward real: 0.7262815833091736 Training d_loss: 0.8440 Training g_loss: 1.0867 Training q_loss: 554813.1875 Explore P: 0.1739\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 376 Total reward: 11.0 Average reward fake: 0.3045446276664734 Average reward real: 0.7102325558662415 Training d_loss: 0.8426 Training g_loss: 1.2371 Training q_loss: 5945.6899 Explore P: 0.1737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 377 Total reward: 9.0 Average reward fake: 0.4026743471622467 Average reward real: 0.5883909463882446 Training d_loss: 1.3924 Training g_loss: 0.9718 Training q_loss: 9421.7627 Explore P: 0.1736\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 378 Total reward: 11.0 Average reward fake: 0.324773371219635 Average reward real: 0.7857665419578552 Training d_loss: 0.7423 Training g_loss: 1.1299 Training q_loss: 8927.2910 Explore P: 0.1734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 379 Total reward: 9.0 Average reward fake: 0.3310917913913727 Average reward real: 0.7205120921134949 Training d_loss: 0.8541 Training g_loss: 1.1039 Training q_loss: 12020.9863 Explore P: 0.1733\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 380 Total reward: 10.0 Average reward fake: 0.324584037065506 Average reward real: 0.7206822633743286 Training d_loss: 0.8477 Training g_loss: 1.1257 Training q_loss: 9826.8916 Explore P: 0.1731\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 381 Total reward: 12.0 Average reward fake: 0.3324543237686157 Average reward real: 0.6603885889053345 Training d_loss: 0.9613 Training g_loss: 1.0991 Training q_loss: 7782.4868 Explore P: 0.1729\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 382 Total reward: 10.0 Average reward fake: 0.33850908279418945 Average reward real: 0.7264510989189148 Training d_loss: 0.8599 Training g_loss: 1.0839 Training q_loss: 8014.9287 Explore P: 0.1727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 383 Total reward: 8.0 Average reward fake: 0.3326795697212219 Average reward real: 0.5926652550697327 Training d_loss: 1.0784 Training g_loss: 1.1019 Training q_loss: 7148.2437 Explore P: 0.1726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 384 Total reward: 9.0 Average reward fake: 0.3266100287437439 Average reward real: 0.7907469868659973 Training d_loss: 0.7401 Training g_loss: 1.1274 Training q_loss: 5948.4194 Explore P: 0.1725\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 385 Total reward: 10.0 Average reward fake: 0.31612199544906616 Average reward real: 0.6528897285461426 Training d_loss: 0.9647 Training g_loss: 1.1455 Training q_loss: 2487.4092 Explore P: 0.1723\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 386 Total reward: 9.0 Average reward fake: 0.321930855512619 Average reward real: 0.4577992558479309 Training d_loss: 1.2915 Training g_loss: 1.1345 Training q_loss: 965930.0000 Explore P: 0.1721\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 387 Total reward: 17.0 Average reward fake: 0.33574551343917847 Average reward real: 0.5320112109184265 Training d_loss: 1.1777 Training g_loss: 1.0852 Training q_loss: 1851.1852 Explore P: 0.1719\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 388 Total reward: 10.0 Average reward fake: 0.33811089396476746 Average reward real: 0.7895721197128296 Training d_loss: 0.7496 Training g_loss: 1.0877 Training q_loss: 4635.4165 Explore P: 0.1717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 389 Total reward: 12.0 Average reward fake: 0.3426591753959656 Average reward real: 0.5304809808731079 Training d_loss: 1.1873 Training g_loss: 1.0682 Training q_loss: 2939.6875 Explore P: 0.1715\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 390 Total reward: 11.0 Average reward fake: 0.33437639474868774 Average reward real: 0.5929386019706726 Training d_loss: 1.0733 Training g_loss: 1.1018 Training q_loss: 6661.3369 Explore P: 0.1713\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 391 Total reward: 30.0 Average reward fake: 0.10348407924175262 Average reward real: 0.24329009652137756 Training d_loss: 1.8530 Training g_loss: 2.1776 Training q_loss: 2967.5894 Explore P: 0.1709\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 392 Total reward: 11.0 Average reward fake: 0.4303690493106842 Average reward real: 0.516538679599762 Training d_loss: 1.2557 Training g_loss: 0.8060 Training q_loss: 2347.0750 Explore P: 0.1707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 393 Total reward: 10.0 Average reward fake: 0.37423184514045715 Average reward real: 0.5928791761398315 Training d_loss: 1.0810 Training g_loss: 1.0561 Training q_loss: 2060.4131 Explore P: 0.1705\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 394 Total reward: 8.0 Average reward fake: 0.29985132813453674 Average reward real: 0.4012438654899597 Training d_loss: 1.4257 Training g_loss: 1.1656 Training q_loss: 3100.7454 Explore P: 0.1704\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 395 Total reward: 11.0 Average reward fake: 0.35619890689849854 Average reward real: 0.6854259371757507 Training d_loss: 0.9169 Training g_loss: 1.0360 Training q_loss: 3343.9680 Explore P: 0.1702\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 396 Total reward: 17.0 Average reward fake: 0.34797585010528564 Average reward real: 0.8018485903739929 Training d_loss: 0.7403 Training g_loss: 1.0859 Training q_loss: 3139.3694 Explore P: 0.1699\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 397 Total reward: 8.0 Average reward fake: 0.2687548100948334 Average reward real: 0.4840402603149414 Training d_loss: 1.2468 Training g_loss: 1.3258 Training q_loss: 2015.6582 Explore P: 0.1698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 398 Total reward: 11.0 Average reward fake: 0.31223931908607483 Average reward real: 0.7186778783798218 Training d_loss: 0.8575 Training g_loss: 1.1628 Training q_loss: 1263614.2500 Explore P: 0.1696\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 399 Total reward: 16.0 Average reward fake: 0.37304872274398804 Average reward real: 0.7366845011711121 Training d_loss: 0.8972 Training g_loss: 0.9725 Training q_loss: 2200.7964 Explore P: 0.1694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 400 Total reward: 8.0 Average reward fake: 0.3662473261356354 Average reward real: 0.5543463826179504 Training d_loss: 1.1642 Training g_loss: 1.0238 Training q_loss: 1475.7615 Explore P: 0.1693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 401 Total reward: 9.0 Average reward fake: 0.3172820210456848 Average reward real: 0.5169192552566528 Training d_loss: 1.2030 Training g_loss: 1.1420 Training q_loss: 2354.8364 Explore P: 0.1691\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 402 Total reward: 10.0 Average reward fake: 0.3509533405303955 Average reward real: 0.7357696294784546 Training d_loss: 0.8625 Training g_loss: 1.0576 Training q_loss: 2941.1926 Explore P: 0.1690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 403 Total reward: 12.0 Average reward fake: 0.33817368745803833 Average reward real: 0.7990557551383972 Training d_loss: 0.7430 Training g_loss: 1.1136 Training q_loss: 1183.6228 Explore P: 0.1688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 404 Total reward: 8.0 Average reward fake: 0.2943410277366638 Average reward real: 0.6503426432609558 Training d_loss: 0.9505 Training g_loss: 1.2231 Training q_loss: 235134.4531 Explore P: 0.1686\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 405 Total reward: 8.0 Average reward fake: 0.3300502300262451 Average reward real: 0.3814118802547455 Training d_loss: 1.4483 Training g_loss: 1.1036 Training q_loss: 1781.6617 Explore P: 0.1685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 406 Total reward: 10.0 Average reward fake: 0.33360815048217773 Average reward real: 0.7293310165405273 Training d_loss: 0.8559 Training g_loss: 1.0930 Training q_loss: 648742.6250 Explore P: 0.1683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 407 Total reward: 11.0 Average reward fake: 0.3572782874107361 Average reward real: 0.7441144585609436 Training d_loss: 0.8491 Training g_loss: 1.0332 Training q_loss: 561837.7500 Explore P: 0.1682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 408 Total reward: 7.0 Average reward fake: 0.36170703172683716 Average reward real: 0.7418791055679321 Training d_loss: 0.8622 Training g_loss: 1.0169 Training q_loss: 1206.4586 Explore P: 0.1681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 409 Total reward: 9.0 Average reward fake: 0.351645827293396 Average reward real: 0.8006182909011841 Training d_loss: 0.7611 Training g_loss: 1.0593 Training q_loss: 1836.2162 Explore P: 0.1679\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 410 Total reward: 8.0 Average reward fake: 0.3402920961380005 Average reward real: 0.7939173579216003 Training d_loss: 0.7657 Training g_loss: 1.0813 Training q_loss: 1629.4172 Explore P: 0.1678\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 411 Total reward: 12.0 Average reward fake: 0.3282352089881897 Average reward real: 0.6420014500617981 Training d_loss: 1.0273 Training g_loss: 1.1388 Training q_loss: 2506.4634 Explore P: 0.1676\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 412 Total reward: 8.0 Average reward fake: 0.32126036286354065 Average reward real: 0.7219643592834473 Training d_loss: 0.8602 Training g_loss: 1.1368 Training q_loss: 1072.7561 Explore P: 0.1675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 413 Total reward: 10.0 Average reward fake: 0.32095867395401 Average reward real: 0.662192702293396 Training d_loss: 0.9484 Training g_loss: 1.1395 Training q_loss: 1448.6216 Explore P: 0.1673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 414 Total reward: 12.0 Average reward fake: 0.3736158609390259 Average reward real: 0.7450717687606812 Training d_loss: 0.8719 Training g_loss: 0.9955 Training q_loss: 2997.7024 Explore P: 0.1671\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 415 Total reward: 9.0 Average reward fake: 0.34877878427505493 Average reward real: 0.6036416292190552 Training d_loss: 1.0776 Training g_loss: 1.0604 Training q_loss: 1591.4636 Explore P: 0.1670\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 416 Total reward: 14.0 Average reward fake: 0.34315234422683716 Average reward real: 0.6744076013565063 Training d_loss: 0.9461 Training g_loss: 1.0615 Training q_loss: 1876.6088 Explore P: 0.1668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 417 Total reward: 7.0 Average reward fake: 0.3596484065055847 Average reward real: 0.47103700041770935 Training d_loss: 1.3140 Training g_loss: 1.0188 Training q_loss: 3464.1750 Explore P: 0.1667\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 418 Total reward: 8.0 Average reward fake: 0.3383616805076599 Average reward real: 0.7951760292053223 Training d_loss: 0.7558 Training g_loss: 1.1206 Training q_loss: 1600.2649 Explore P: 0.1665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 419 Total reward: 14.0 Average reward fake: 0.27363866567611694 Average reward real: 0.5776880383491516 Training d_loss: 1.0491 Training g_loss: 1.2840 Training q_loss: 1506.9613 Explore P: 0.1663\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 420 Total reward: 12.0 Average reward fake: 0.3623908460140228 Average reward real: 0.6813673973083496 Training d_loss: 0.9570 Training g_loss: 1.0065 Training q_loss: 1197.1484 Explore P: 0.1661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 421 Total reward: 14.0 Average reward fake: 0.38657745718955994 Average reward real: 0.6992842555046082 Training d_loss: 0.9457 Training g_loss: 0.9789 Training q_loss: 386551.6875 Explore P: 0.1659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 422 Total reward: 10.0 Average reward fake: 0.3331872820854187 Average reward real: 0.6703461408615112 Training d_loss: 0.9390 Training g_loss: 1.0934 Training q_loss: 1249.6632 Explore P: 0.1658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 423 Total reward: 8.0 Average reward fake: 0.3641608953475952 Average reward real: 0.4782048165798187 Training d_loss: 1.2974 Training g_loss: 0.9955 Training q_loss: 3439.2534 Explore P: 0.1656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 424 Total reward: 10.0 Average reward fake: 0.4579307436943054 Average reward real: 0.7861040234565735 Training d_loss: 1.2962 Training g_loss: 0.8974 Training q_loss: 4793.0518 Explore P: 0.1655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 425 Total reward: 8.0 Average reward fake: 0.327923446893692 Average reward real: 0.7224725484848022 Training d_loss: 0.8446 Training g_loss: 1.1165 Training q_loss: 1751.1926 Explore P: 0.1654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 426 Total reward: 9.0 Average reward fake: 0.3510674834251404 Average reward real: 0.7736752033233643 Training d_loss: 0.7958 Training g_loss: 1.0576 Training q_loss: 1894.5508 Explore P: 0.1652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 427 Total reward: 13.0 Average reward fake: 0.29660239815711975 Average reward real: 0.45882853865623474 Training d_loss: 1.2458 Training g_loss: 1.2885 Training q_loss: 4781.7217 Explore P: 0.1650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 428 Total reward: 19.0 Average reward fake: 0.42069125175476074 Average reward real: 0.7180867791175842 Training d_loss: 1.1423 Training g_loss: 0.9500 Training q_loss: 2914.7012 Explore P: 0.1647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 429 Total reward: 19.0 Average reward fake: 0.4994557499885559 Average reward real: 0.6059128046035767 Training d_loss: 1.4397 Training g_loss: 0.7402 Training q_loss: 776.8835 Explore P: 0.1644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 430 Total reward: 16.0 Average reward fake: 0.35307741165161133 Average reward real: 0.5542029738426208 Training d_loss: 1.1535 Training g_loss: 1.0466 Training q_loss: 3952.4785 Explore P: 0.1642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 431 Total reward: 12.0 Average reward fake: 0.4116989076137543 Average reward real: 0.6307588219642639 Training d_loss: 1.2047 Training g_loss: 0.9335 Training q_loss: 6023.9307 Explore P: 0.1640\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 432 Total reward: 15.0 Average reward fake: 0.33774060010910034 Average reward real: 0.818791389465332 Training d_loss: 0.6772 Training g_loss: 1.0960 Training q_loss: 2473.7273 Explore P: 0.1638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 433 Total reward: 18.0 Average reward fake: 0.3702964186668396 Average reward real: 0.6747176647186279 Training d_loss: 0.9443 Training g_loss: 1.0111 Training q_loss: 4154.6182 Explore P: 0.1635\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 434 Total reward: 10.0 Average reward fake: 0.30222561955451965 Average reward real: 0.5544511079788208 Training d_loss: 1.1106 Training g_loss: 1.1994 Training q_loss: 6190.5835 Explore P: 0.1633\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 435 Total reward: 13.0 Average reward fake: 0.34137314558029175 Average reward real: 0.5855635404586792 Training d_loss: 1.0840 Training g_loss: 1.0663 Training q_loss: 3523.5020 Explore P: 0.1631\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 436 Total reward: 8.0 Average reward fake: 0.352061927318573 Average reward real: 0.6031762957572937 Training d_loss: 1.0658 Training g_loss: 1.0423 Training q_loss: 4591.0181 Explore P: 0.1630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 437 Total reward: 12.0 Average reward fake: 0.33246487379074097 Average reward real: 0.6572865843772888 Training d_loss: 0.9569 Training g_loss: 1.1202 Training q_loss: 8366.8262 Explore P: 0.1628\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 438 Total reward: 12.0 Average reward fake: 0.34870681166648865 Average reward real: 0.6269189715385437 Training d_loss: 1.1964 Training g_loss: 1.1790 Training q_loss: 3807.0457 Explore P: 0.1626\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 439 Total reward: 133.0 Average reward fake: 0.24305656552314758 Average reward real: 0.6720507740974426 Training d_loss: 0.7423 Training g_loss: 1.6109 Training q_loss: 2785.4031 Explore P: 0.1606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 440 Total reward: 135.0 Average reward fake: 0.030591070652008057 Average reward real: 0.9634857177734375 Training d_loss: 0.0685 Training g_loss: 3.8454 Training q_loss: 1791.1104 Explore P: 0.1586\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 441 Total reward: 149.0 Average reward fake: 0.44469889998435974 Average reward real: 0.41166409850120544 Training d_loss: 1.5519 Training g_loss: 0.8374 Training q_loss: 3485.4414 Explore P: 0.1564\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 442 Total reward: 199.0 Average reward fake: 0.4241184592247009 Average reward real: 0.49731525778770447 Training d_loss: 1.2949 Training g_loss: 0.9280 Training q_loss: 3032.6340 Explore P: 0.1535\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 443 Total reward: 198.0 Average reward fake: 0.44322142004966736 Average reward real: 0.4553805887699127 Training d_loss: 1.4374 Training g_loss: 0.8116 Training q_loss: 1174.1934 Explore P: 0.1507\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 444 Total reward: 199.0 Average reward fake: 0.5276548862457275 Average reward real: 0.5163973569869995 Training d_loss: 1.4126 Training g_loss: 0.6365 Training q_loss: 2210.3381 Explore P: 0.1479\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 445 Total reward: 199.0 Average reward fake: 0.42004066705703735 Average reward real: 0.4850720465183258 Training d_loss: 1.2953 Training g_loss: 0.9972 Training q_loss: 2314.1172 Explore P: 0.1452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 446 Total reward: 199.0 Average reward fake: 0.4891003668308258 Average reward real: 0.5127653479576111 Training d_loss: 1.3523 Training g_loss: 0.7514 Training q_loss: 822.4810 Explore P: 0.1426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 447 Total reward: 199.0 Average reward fake: 0.48984774947166443 Average reward real: 0.44472938776016235 Training d_loss: 1.4979 Training g_loss: 0.7207 Training q_loss: 641.3994 Explore P: 0.1399\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 448 Total reward: 199.0 Average reward fake: 0.5319657325744629 Average reward real: 0.5207523107528687 Training d_loss: 1.4236 Training g_loss: 0.6386 Training q_loss: 1075.0813 Explore P: 0.1374\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 449 Total reward: 199.0 Average reward fake: 0.4924081861972809 Average reward real: 0.5154063105583191 Training d_loss: 1.3458 Training g_loss: 0.7190 Training q_loss: 521.3448 Explore P: 0.1349\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 450 Total reward: 199.0 Average reward fake: 0.5197544097900391 Average reward real: 0.5608105659484863 Training d_loss: 1.3263 Training g_loss: 0.6659 Training q_loss: 395.1541 Explore P: 0.1324\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 451 Total reward: 199.0 Average reward fake: 0.5170179605484009 Average reward real: 0.5525627732276917 Training d_loss: 1.3285 Training g_loss: 0.6626 Training q_loss: 51520.5273 Explore P: 0.1300\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 452 Total reward: 199.0 Average reward fake: 0.493530809879303 Average reward real: 0.48980098962783813 Training d_loss: 1.4012 Training g_loss: 0.7052 Training q_loss: 444.2990 Explore P: 0.1276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 453 Total reward: 199.0 Average reward fake: 0.30200815200805664 Average reward real: 0.49638739228248596 Training d_loss: 1.0858 Training g_loss: 1.5524 Training q_loss: 1018.5409 Explore P: 0.1253\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 454 Total reward: 199.0 Average reward fake: 0.47110262513160706 Average reward real: 0.4781618118286133 Training d_loss: 1.4219 Training g_loss: 0.7635 Training q_loss: 956.7592 Explore P: 0.1230\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 455 Total reward: 199.0 Average reward fake: 0.48901647329330444 Average reward real: 0.5226976275444031 Training d_loss: 1.3317 Training g_loss: 0.7189 Training q_loss: 621.2533 Explore P: 0.1208\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 456 Total reward: 199.0 Average reward fake: 0.47834259271621704 Average reward real: 0.5075461268424988 Training d_loss: 1.3464 Training g_loss: 0.7958 Training q_loss: 194.6928 Explore P: 0.1186\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 457 Total reward: 199.0 Average reward fake: 0.5078034400939941 Average reward real: 0.5259147882461548 Training d_loss: 1.3744 Training g_loss: 0.7146 Training q_loss: 218.5490 Explore P: 0.1165\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 458 Total reward: 199.0 Average reward fake: 0.5307983160018921 Average reward real: 0.5555554628372192 Training d_loss: 1.3515 Training g_loss: 0.6358 Training q_loss: 231.8265 Explore P: 0.1144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 459 Total reward: 199.0 Average reward fake: 0.39702993631362915 Average reward real: 0.5639356374740601 Training d_loss: 1.0902 Training g_loss: 0.9689 Training q_loss: 111.7937 Explore P: 0.1123\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 460 Total reward: 199.0 Average reward fake: 0.5040863156318665 Average reward real: 0.5560997128486633 Training d_loss: 1.3106 Training g_loss: 0.7118 Training q_loss: 280.2070 Explore P: 0.1103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 461 Total reward: 188.0 Average reward fake: 0.4003883898258209 Average reward real: 0.5897240042686462 Training d_loss: 1.1037 Training g_loss: 1.0649 Training q_loss: 168.9243 Explore P: 0.1085\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 462 Total reward: 199.0 Average reward fake: 0.4757792353630066 Average reward real: 0.49551767110824585 Training d_loss: 1.3554 Training g_loss: 0.7614 Training q_loss: 253.3008 Explore P: 0.1065\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 463 Total reward: 199.0 Average reward fake: 0.5138950943946838 Average reward real: 0.5451699495315552 Training d_loss: 1.3370 Training g_loss: 0.6875 Training q_loss: 56.3705 Explore P: 0.1046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 464 Total reward: 199.0 Average reward fake: 0.528123676776886 Average reward real: 0.5344921350479126 Training d_loss: 1.4094 Training g_loss: 0.6554 Training q_loss: 88.9191 Explore P: 0.1027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 465 Total reward: 199.0 Average reward fake: 0.4157685339450836 Average reward real: 0.6026521921157837 Training d_loss: 1.1583 Training g_loss: 1.3439 Training q_loss: 245.8946 Explore P: 0.1009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 466 Total reward: 199.0 Average reward fake: 0.585162878036499 Average reward real: 0.5576964616775513 Training d_loss: 1.5281 Training g_loss: 0.6450 Training q_loss: 48.8037 Explore P: 0.0991\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 467 Total reward: 199.0 Average reward fake: 0.4002808630466461 Average reward real: 0.48792320489883423 Training d_loss: 1.2565 Training g_loss: 0.9943 Training q_loss: 179.6677 Explore P: 0.0974\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 468 Total reward: 199.0 Average reward fake: 0.5359758734703064 Average reward real: 0.537519097328186 Training d_loss: 1.3934 Training g_loss: 0.6292 Training q_loss: 192.2653 Explore P: 0.0957\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 469 Total reward: 199.0 Average reward fake: 0.5285336375236511 Average reward real: 0.5286270380020142 Training d_loss: 1.3948 Training g_loss: 0.6534 Training q_loss: 161.5685 Explore P: 0.0940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 470 Total reward: 199.0 Average reward fake: 0.4779207110404968 Average reward real: 0.5173741579055786 Training d_loss: 1.3522 Training g_loss: 0.9592 Training q_loss: 3716.2983 Explore P: 0.0923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 471 Total reward: 199.0 Average reward fake: 0.5131185054779053 Average reward real: 0.6036832332611084 Training d_loss: 1.2969 Training g_loss: 0.7090 Training q_loss: 119.0882 Explore P: 0.0907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 472 Total reward: 199.0 Average reward fake: 0.39511924982070923 Average reward real: 0.5911031365394592 Training d_loss: 1.1386 Training g_loss: 1.2692 Training q_loss: 251.1900 Explore P: 0.0891\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 473 Total reward: 199.0 Average reward fake: 0.42308592796325684 Average reward real: 0.6116344332695007 Training d_loss: 1.0893 Training g_loss: 1.3112 Training q_loss: 150.9901 Explore P: 0.0875\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 474 Total reward: 199.0 Average reward fake: 0.4429999887943268 Average reward real: 0.5502668619155884 Training d_loss: 1.2355 Training g_loss: 1.1149 Training q_loss: 79.0118 Explore P: 0.0860\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 475 Total reward: 199.0 Average reward fake: 0.44048595428466797 Average reward real: 0.48780521750450134 Training d_loss: 1.3373 Training g_loss: 0.8737 Training q_loss: 235.0802 Explore P: 0.0845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 476 Total reward: 199.0 Average reward fake: 0.4502106308937073 Average reward real: 0.6255866885185242 Training d_loss: 1.1831 Training g_loss: 1.0692 Training q_loss: 140.0461 Explore P: 0.0830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 477 Total reward: 199.0 Average reward fake: 0.3378998637199402 Average reward real: 0.5781374573707581 Training d_loss: 1.0247 Training g_loss: 1.7693 Training q_loss: 86.1309 Explore P: 0.0816\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 478 Total reward: 199.0 Average reward fake: 0.32246536016464233 Average reward real: 0.7442651987075806 Training d_loss: 0.7228 Training g_loss: 1.6862 Training q_loss: 232.5255 Explore P: 0.0802\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 479 Total reward: 199.0 Average reward fake: 0.42148470878601074 Average reward real: 0.5707656741142273 Training d_loss: 1.2196 Training g_loss: 1.5544 Training q_loss: 84.2660 Explore P: 0.0788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 480 Total reward: 199.0 Average reward fake: 0.5096520185470581 Average reward real: 0.5866261124610901 Training d_loss: 1.2896 Training g_loss: 1.0552 Training q_loss: 81.4766 Explore P: 0.0775\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 481 Total reward: 199.0 Average reward fake: 0.3535972535610199 Average reward real: 0.5541642904281616 Training d_loss: 1.1186 Training g_loss: 1.8519 Training q_loss: 103.5405 Explore P: 0.0761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 482 Total reward: 199.0 Average reward fake: 0.4573970437049866 Average reward real: 0.5519401431083679 Training d_loss: 1.2464 Training g_loss: 1.1810 Training q_loss: 26.6512 Explore P: 0.0748\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 483 Total reward: 199.0 Average reward fake: 0.4315904676914215 Average reward real: 0.62239009141922 Training d_loss: 1.1099 Training g_loss: 1.3445 Training q_loss: 219.6903 Explore P: 0.0735\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 484 Total reward: 199.0 Average reward fake: 0.36542201042175293 Average reward real: 0.534344494342804 Training d_loss: 1.1465 Training g_loss: 1.3034 Training q_loss: 159.9034 Explore P: 0.0723\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 485 Total reward: 199.0 Average reward fake: 0.36423593759536743 Average reward real: 0.7539551854133606 Training d_loss: 0.8582 Training g_loss: 1.8836 Training q_loss: 155.8489 Explore P: 0.0711\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 486 Total reward: 107.0 Average reward fake: 0.36974698305130005 Average reward real: 0.6632050275802612 Training d_loss: 0.9573 Training g_loss: 1.1120 Training q_loss: 184.9412 Explore P: 0.0704\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 487 Total reward: 15.0 Average reward fake: 0.3709791302680969 Average reward real: 0.6468536257743835 Training d_loss: 0.9693 Training g_loss: 1.5932 Training q_loss: 377.4391 Explore P: 0.0703\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 488 Total reward: 9.0 Average reward fake: 0.349048376083374 Average reward real: 0.7352081537246704 Training d_loss: 0.8210 Training g_loss: 1.3567 Training q_loss: 9188.6758 Explore P: 0.0703\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 489 Total reward: 7.0 Average reward fake: 0.356822669506073 Average reward real: 0.5411214828491211 Training d_loss: 1.1521 Training g_loss: 1.0628 Training q_loss: 1082.6881 Explore P: 0.0702\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 490 Total reward: 14.0 Average reward fake: 0.3704230487346649 Average reward real: 0.5887969136238098 Training d_loss: 1.1797 Training g_loss: 1.1161 Training q_loss: 1787.0188 Explore P: 0.0701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 491 Total reward: 10.0 Average reward fake: 0.2356911152601242 Average reward real: 0.5741956830024719 Training d_loss: 0.9805 Training g_loss: 1.4779 Training q_loss: 3767.4175 Explore P: 0.0701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 492 Total reward: 13.0 Average reward fake: 0.1687922477722168 Average reward real: 0.6520134210586548 Training d_loss: 0.7587 Training g_loss: 1.8664 Training q_loss: 6865.6548 Explore P: 0.0700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 493 Total reward: 8.0 Average reward fake: 0.16653653979301453 Average reward real: 0.7185686826705933 Training d_loss: 0.5888 Training g_loss: 1.9513 Training q_loss: 9669.1279 Explore P: 0.0700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 494 Total reward: 10.0 Average reward fake: 0.25335901975631714 Average reward real: 0.7826783657073975 Training d_loss: 0.5967 Training g_loss: 1.5126 Training q_loss: 7778.5752 Explore P: 0.0699\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 495 Total reward: 9.0 Average reward fake: 0.3668431043624878 Average reward real: 0.7497724294662476 Training d_loss: 0.8027 Training g_loss: 1.0432 Training q_loss: 3472.5522 Explore P: 0.0698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 496 Total reward: 12.0 Average reward fake: 0.30041879415512085 Average reward real: 0.8632631301879883 Training d_loss: 0.5696 Training g_loss: 1.2913 Training q_loss: 5954.7197 Explore P: 0.0698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 497 Total reward: 10.0 Average reward fake: 0.3552497327327728 Average reward real: 0.597303032875061 Training d_loss: 1.1753 Training g_loss: 1.5834 Training q_loss: 14101.1797 Explore P: 0.0697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 498 Total reward: 11.0 Average reward fake: 0.22622892260551453 Average reward real: 0.7456666231155396 Training d_loss: 0.5899 Training g_loss: 1.6333 Training q_loss: 28030.5625 Explore P: 0.0697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 499 Total reward: 8.0 Average reward fake: 0.27725958824157715 Average reward real: 0.8110090494155884 Training d_loss: 0.5868 Training g_loss: 1.4820 Training q_loss: 13475.4951 Explore P: 0.0696\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 500 Total reward: 7.0 Average reward fake: 0.2508346438407898 Average reward real: 0.8124769330024719 Training d_loss: 0.5744 Training g_loss: 1.5356 Training q_loss: 38723.7070 Explore P: 0.0696\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 501 Total reward: 13.0 Average reward fake: 0.29826945066452026 Average reward real: 0.69379061460495 Training d_loss: 0.9885 Training g_loss: 1.5914 Training q_loss: 7358.4658 Explore P: 0.0695\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 502 Total reward: 9.0 Average reward fake: 0.2364506721496582 Average reward real: 0.7219234704971313 Training d_loss: 0.6800 Training g_loss: 1.5216 Training q_loss: 8300.6113 Explore P: 0.0694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 503 Total reward: 9.0 Average reward fake: 0.34446996450424194 Average reward real: 0.7641934752464294 Training d_loss: 0.7538 Training g_loss: 1.1512 Training q_loss: 11614.5645 Explore P: 0.0694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 504 Total reward: 8.0 Average reward fake: 0.36783328652381897 Average reward real: 0.7288046479225159 Training d_loss: 0.9077 Training g_loss: 1.1287 Training q_loss: 5373.8472 Explore P: 0.0693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 505 Total reward: 12.0 Average reward fake: 0.31171146035194397 Average reward real: 0.6187615990638733 Training d_loss: 0.9528 Training g_loss: 1.2500 Training q_loss: 10993.8457 Explore P: 0.0693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 506 Total reward: 15.0 Average reward fake: 0.1955920159816742 Average reward real: 0.4212263226509094 Training d_loss: 1.3764 Training g_loss: 2.0815 Training q_loss: 11058.5498 Explore P: 0.0692\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 507 Total reward: 11.0 Average reward fake: 0.41327381134033203 Average reward real: 0.7123348712921143 Training d_loss: 1.0152 Training g_loss: 1.0192 Training q_loss: 6344.6460 Explore P: 0.0691\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 508 Total reward: 7.0 Average reward fake: 0.14433881640434265 Average reward real: 0.9158690571784973 Training d_loss: 0.2941 Training g_loss: 2.7717 Training q_loss: 1854.3226 Explore P: 0.0691\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 509 Total reward: 12.0 Average reward fake: 0.289995014667511 Average reward real: 0.7051320672035217 Training d_loss: 0.7661 Training g_loss: 1.2736 Training q_loss: 6694.2212 Explore P: 0.0690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 510 Total reward: 9.0 Average reward fake: 0.3137693405151367 Average reward real: 0.6600072383880615 Training d_loss: 0.8823 Training g_loss: 1.1697 Training q_loss: 10129.3145 Explore P: 0.0689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 511 Total reward: 9.0 Average reward fake: 0.28779760003089905 Average reward real: 0.7002600431442261 Training d_loss: 0.8463 Training g_loss: 1.4770 Training q_loss: 492899.0000 Explore P: 0.0689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 512 Total reward: 9.0 Average reward fake: 0.2687458395957947 Average reward real: 0.7166378498077393 Training d_loss: 0.8661 Training g_loss: 1.5537 Training q_loss: 4775.5684 Explore P: 0.0688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 513 Total reward: 8.0 Average reward fake: 0.3386876881122589 Average reward real: 0.696208655834198 Training d_loss: 0.9298 Training g_loss: 1.2534 Training q_loss: 6895.9180 Explore P: 0.0688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 514 Total reward: 8.0 Average reward fake: 0.2605193257331848 Average reward real: 0.6336833238601685 Training d_loss: 0.8712 Training g_loss: 1.3876 Training q_loss: 9991.3926 Explore P: 0.0687\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 515 Total reward: 12.0 Average reward fake: 0.3217780590057373 Average reward real: 0.740902841091156 Training d_loss: 0.8145 Training g_loss: 1.2329 Training q_loss: 5985.2881 Explore P: 0.0687\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 516 Total reward: 11.0 Average reward fake: 0.2923271059989929 Average reward real: 0.6253443956375122 Training d_loss: 0.9354 Training g_loss: 1.2265 Training q_loss: 7316.2617 Explore P: 0.0686\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 517 Total reward: 13.0 Average reward fake: 0.2650073170661926 Average reward real: 0.7676247358322144 Training d_loss: 0.6607 Training g_loss: 1.2576 Training q_loss: 2976.0454 Explore P: 0.0685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 518 Total reward: 9.0 Average reward fake: 0.35127195715904236 Average reward real: 0.6807781457901001 Training d_loss: 0.9084 Training g_loss: 1.1874 Training q_loss: 3043070.5000 Explore P: 0.0685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 519 Total reward: 8.0 Average reward fake: 0.19917947053909302 Average reward real: 0.6226641535758972 Training d_loss: 0.8821 Training g_loss: 2.1604 Training q_loss: 2073155.2500 Explore P: 0.0684\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 520 Total reward: 7.0 Average reward fake: 0.35937264561653137 Average reward real: 0.6944904923439026 Training d_loss: 0.8919 Training g_loss: 1.0677 Training q_loss: 2615.6008 Explore P: 0.0684\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 521 Total reward: 9.0 Average reward fake: 0.28861528635025024 Average reward real: 0.4985058307647705 Training d_loss: 1.1421 Training g_loss: 1.3944 Training q_loss: 3735.8914 Explore P: 0.0683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 522 Total reward: 8.0 Average reward fake: 0.3355422616004944 Average reward real: 0.7592726349830627 Training d_loss: 0.7908 Training g_loss: 1.6994 Training q_loss: 1001.6504 Explore P: 0.0683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 523 Total reward: 8.0 Average reward fake: 0.2786741554737091 Average reward real: 0.6447798013687134 Training d_loss: 1.0099 Training g_loss: 1.5033 Training q_loss: 1127.8206 Explore P: 0.0682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 524 Total reward: 8.0 Average reward fake: 0.3138643801212311 Average reward real: 0.8825209736824036 Training d_loss: 0.5595 Training g_loss: 1.2849 Training q_loss: 2312.7202 Explore P: 0.0682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 525 Total reward: 9.0 Average reward fake: 0.3330267667770386 Average reward real: 0.5965010523796082 Training d_loss: 1.3734 Training g_loss: 1.2607 Training q_loss: 631.1621 Explore P: 0.0681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 526 Total reward: 10.0 Average reward fake: 0.3301331102848053 Average reward real: 0.5635846853256226 Training d_loss: 1.0972 Training g_loss: 1.1541 Training q_loss: 674.0951 Explore P: 0.0681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 527 Total reward: 7.0 Average reward fake: 0.274293452501297 Average reward real: 0.599854052066803 Training d_loss: 0.9875 Training g_loss: 1.3288 Training q_loss: 1725.9426 Explore P: 0.0680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 528 Total reward: 8.0 Average reward fake: 0.28144556283950806 Average reward real: 0.7122429609298706 Training d_loss: 0.8557 Training g_loss: 1.3168 Training q_loss: 1314.5847 Explore P: 0.0680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 529 Total reward: 8.0 Average reward fake: 0.34563836455345154 Average reward real: 0.4457016587257385 Training d_loss: 1.3317 Training g_loss: 1.4870 Training q_loss: 2599.7617 Explore P: 0.0680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 530 Total reward: 11.0 Average reward fake: 0.22783789038658142 Average reward real: 0.6489602327346802 Training d_loss: 0.8699 Training g_loss: 2.1616 Training q_loss: 285454.6562 Explore P: 0.0679\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 531 Total reward: 10.0 Average reward fake: 0.3089780807495117 Average reward real: 0.5229489207267761 Training d_loss: 1.2614 Training g_loss: 1.1303 Training q_loss: 1932.1428 Explore P: 0.0678\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 532 Total reward: 8.0 Average reward fake: 0.337416410446167 Average reward real: 0.771574079990387 Training d_loss: 0.7651 Training g_loss: 1.6070 Training q_loss: 4587.4932 Explore P: 0.0678\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 533 Total reward: 9.0 Average reward fake: 0.2659350037574768 Average reward real: 0.5913032293319702 Training d_loss: 0.9905 Training g_loss: 1.3331 Training q_loss: 1176.7893 Explore P: 0.0677\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 534 Total reward: 10.0 Average reward fake: 0.3544313311576843 Average reward real: 0.7131808996200562 Training d_loss: 0.8665 Training g_loss: 1.1515 Training q_loss: 1279434.3750 Explore P: 0.0677\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 535 Total reward: 14.0 Average reward fake: 0.2970564365386963 Average reward real: 0.6256230473518372 Training d_loss: 0.9194 Training g_loss: 1.1761 Training q_loss: 1783.4583 Explore P: 0.0676\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 536 Total reward: 11.0 Average reward fake: 0.3788243234157562 Average reward real: 0.6203616261482239 Training d_loss: 1.0208 Training g_loss: 1.1097 Training q_loss: 1762.1315 Explore P: 0.0675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 537 Total reward: 75.0 Average reward fake: 0.2903904616832733 Average reward real: 0.7044504880905151 Training d_loss: 0.7932 Training g_loss: 1.9251 Training q_loss: 4172.0942 Explore P: 0.0671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 538 Total reward: 12.0 Average reward fake: 0.31223630905151367 Average reward real: 0.7243971228599548 Training d_loss: 0.7945 Training g_loss: 1.4194 Training q_loss: 177217.1250 Explore P: 0.0670\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 539 Total reward: 9.0 Average reward fake: 0.3464083671569824 Average reward real: 0.6566751599311829 Training d_loss: 0.9781 Training g_loss: 1.3678 Training q_loss: 4774.0874 Explore P: 0.0670\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 540 Total reward: 10.0 Average reward fake: 0.18230010569095612 Average reward real: 0.605035662651062 Training d_loss: 0.9433 Training g_loss: 1.7162 Training q_loss: 3308.8347 Explore P: 0.0669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 541 Total reward: 9.0 Average reward fake: 0.29442131519317627 Average reward real: 0.7629644870758057 Training d_loss: 0.7952 Training g_loss: 1.3887 Training q_loss: 4007.8218 Explore P: 0.0669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 542 Total reward: 9.0 Average reward fake: 0.3651981055736542 Average reward real: 0.7319357991218567 Training d_loss: 1.0074 Training g_loss: 1.0817 Training q_loss: 1707.5426 Explore P: 0.0668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 543 Total reward: 10.0 Average reward fake: 0.28192511200904846 Average reward real: 0.7167505025863647 Training d_loss: 0.8044 Training g_loss: 2.2141 Training q_loss: 1816.5289 Explore P: 0.0668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 544 Total reward: 10.0 Average reward fake: 0.25510790944099426 Average reward real: 0.5819321870803833 Training d_loss: 1.0164 Training g_loss: 1.5172 Training q_loss: 2713.7896 Explore P: 0.0667\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 545 Total reward: 11.0 Average reward fake: 0.31845587491989136 Average reward real: 0.6534560918807983 Training d_loss: 0.9687 Training g_loss: 1.1808 Training q_loss: 986.8742 Explore P: 0.0666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 546 Total reward: 8.0 Average reward fake: 0.3125503957271576 Average reward real: 0.8437435030937195 Training d_loss: 0.6426 Training g_loss: 1.2364 Training q_loss: 7715.0054 Explore P: 0.0666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 547 Total reward: 11.0 Average reward fake: 0.25294750928878784 Average reward real: 0.4875515103340149 Training d_loss: 1.2585 Training g_loss: 2.0301 Training q_loss: 5497.8945 Explore P: 0.0665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 548 Total reward: 12.0 Average reward fake: 0.3633575439453125 Average reward real: 0.6336060762405396 Training d_loss: 1.0269 Training g_loss: 1.2092 Training q_loss: 2856.6123 Explore P: 0.0665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 549 Total reward: 13.0 Average reward fake: 0.28154703974723816 Average reward real: 0.7711899280548096 Training d_loss: 0.7285 Training g_loss: 1.2808 Training q_loss: 1587.7706 Explore P: 0.0664\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 550 Total reward: 14.0 Average reward fake: 0.3968889117240906 Average reward real: 0.683221697807312 Training d_loss: 1.2008 Training g_loss: 1.0065 Training q_loss: 1196.3271 Explore P: 0.0663\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 551 Total reward: 13.0 Average reward fake: 0.40650486946105957 Average reward real: 0.6022636890411377 Training d_loss: 1.1071 Training g_loss: 0.9086 Training q_loss: 1288.1672 Explore P: 0.0662\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 552 Total reward: 14.0 Average reward fake: 0.2980714440345764 Average reward real: 0.5144275426864624 Training d_loss: 1.1757 Training g_loss: 1.1231 Training q_loss: 1542.0767 Explore P: 0.0662\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 553 Total reward: 16.0 Average reward fake: 0.3332681953907013 Average reward real: 0.6288207173347473 Training d_loss: 1.2915 Training g_loss: 1.1911 Training q_loss: 669.5902 Explore P: 0.0661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 554 Total reward: 12.0 Average reward fake: 0.380171000957489 Average reward real: 0.6717392802238464 Training d_loss: 0.9935 Training g_loss: 0.9404 Training q_loss: 2133.8594 Explore P: 0.0660\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 555 Total reward: 14.0 Average reward fake: 0.2126304805278778 Average reward real: 0.8329983949661255 Training d_loss: 0.5456 Training g_loss: 1.6462 Training q_loss: 3020.6274 Explore P: 0.0659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 556 Total reward: 10.0 Average reward fake: 0.43542036414146423 Average reward real: 0.6664476990699768 Training d_loss: 1.0830 Training g_loss: 2.0257 Training q_loss: 1604.2598 Explore P: 0.0659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 557 Total reward: 11.0 Average reward fake: 0.3505515158176422 Average reward real: 0.5896192789077759 Training d_loss: 1.2994 Training g_loss: 1.1341 Training q_loss: 4238.7778 Explore P: 0.0658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 558 Total reward: 9.0 Average reward fake: 0.3146823048591614 Average reward real: 0.709291398525238 Training d_loss: 0.8761 Training g_loss: 1.2447 Training q_loss: 2893.3215 Explore P: 0.0658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 559 Total reward: 13.0 Average reward fake: 0.3204052448272705 Average reward real: 0.7803590297698975 Training d_loss: 0.7670 Training g_loss: 1.5021 Training q_loss: 389341.8750 Explore P: 0.0657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 560 Total reward: 8.0 Average reward fake: 0.3452391028404236 Average reward real: 0.6259369850158691 Training d_loss: 1.0121 Training g_loss: 1.0830 Training q_loss: 4966.7339 Explore P: 0.0656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 561 Total reward: 11.0 Average reward fake: 0.3674987554550171 Average reward real: 0.704345166683197 Training d_loss: 0.9075 Training g_loss: 1.1061 Training q_loss: 1409.3977 Explore P: 0.0656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 562 Total reward: 13.0 Average reward fake: 0.23989275097846985 Average reward real: 0.8347896337509155 Training d_loss: 0.6083 Training g_loss: 1.5653 Training q_loss: 6001.0947 Explore P: 0.0655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 563 Total reward: 31.0 Average reward fake: 0.35565465688705444 Average reward real: 0.5607091188430786 Training d_loss: 1.4502 Training g_loss: 1.6574 Training q_loss: 5603.5205 Explore P: 0.0653\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 564 Total reward: 30.0 Average reward fake: 0.3465600609779358 Average reward real: 0.7286660075187683 Training d_loss: 0.8454 Training g_loss: 1.1376 Training q_loss: 1032.8501 Explore P: 0.0652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 565 Total reward: 22.0 Average reward fake: 0.44165629148483276 Average reward real: 0.8415705561637878 Training d_loss: 0.8902 Training g_loss: 0.9385 Training q_loss: 5094.0068 Explore P: 0.0651\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 566 Total reward: 14.0 Average reward fake: 0.34363943338394165 Average reward real: 0.6119047403335571 Training d_loss: 1.0331 Training g_loss: 1.0127 Training q_loss: 784.1720 Explore P: 0.0650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 567 Total reward: 18.0 Average reward fake: 0.4299715459346771 Average reward real: 0.7417305111885071 Training d_loss: 0.9563 Training g_loss: 0.8832 Training q_loss: 184.0254 Explore P: 0.0649\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 568 Total reward: 19.0 Average reward fake: 0.3876977860927582 Average reward real: 0.7029191255569458 Training d_loss: 0.9190 Training g_loss: 1.0433 Training q_loss: 3839.4653 Explore P: 0.0648\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 569 Total reward: 22.0 Average reward fake: 0.3500392436981201 Average reward real: 0.6503557562828064 Training d_loss: 1.1217 Training g_loss: 1.6762 Training q_loss: 6496.9243 Explore P: 0.0647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 570 Total reward: 21.0 Average reward fake: 0.404165118932724 Average reward real: 0.7083703279495239 Training d_loss: 0.9142 Training g_loss: 0.8845 Training q_loss: 1065.3961 Explore P: 0.0645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 571 Total reward: 36.0 Average reward fake: 0.32708460092544556 Average reward real: 0.7418731451034546 Training d_loss: 0.7965 Training g_loss: 1.1554 Training q_loss: 1251.5114 Explore P: 0.0643\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 572 Total reward: 20.0 Average reward fake: 0.4010382294654846 Average reward real: 0.7456793189048767 Training d_loss: 0.9023 Training g_loss: 0.9873 Training q_loss: 1272.4103 Explore P: 0.0642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 573 Total reward: 77.0 Average reward fake: 0.3445824086666107 Average reward real: 0.6717311143875122 Training d_loss: 0.9513 Training g_loss: 1.0500 Training q_loss: 1260.5042 Explore P: 0.0638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 574 Total reward: 199.0 Average reward fake: 0.5731327533721924 Average reward real: 0.6258524656295776 Training d_loss: 1.3365 Training g_loss: 0.5923 Training q_loss: 1955.5697 Explore P: 0.0628\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 575 Total reward: 51.0 Average reward fake: 0.28379544615745544 Average reward real: 0.8632556200027466 Training d_loss: 0.5386 Training g_loss: 1.3932 Training q_loss: 1444.4973 Explore P: 0.0625\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 576 Total reward: 22.0 Average reward fake: 0.2657470405101776 Average reward real: 0.6677289009094238 Training d_loss: 0.8587 Training g_loss: 1.4577 Training q_loss: 4369.5649 Explore P: 0.0624\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 577 Total reward: 77.0 Average reward fake: 0.35308584570884705 Average reward real: 0.5814849138259888 Training d_loss: 1.1049 Training g_loss: 1.0706 Training q_loss: 3745.1243 Explore P: 0.0620\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 578 Total reward: 9.0 Average reward fake: 0.29479390382766724 Average reward real: 0.564072847366333 Training d_loss: 1.0761 Training g_loss: 1.2399 Training q_loss: 2936.8174 Explore P: 0.0619\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 579 Total reward: 15.0 Average reward fake: 0.37311241030693054 Average reward real: 0.7430019974708557 Training d_loss: 0.8592 Training g_loss: 0.9852 Training q_loss: 1078926.8750 Explore P: 0.0618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 580 Total reward: 13.0 Average reward fake: 0.3121339976787567 Average reward real: 0.7437196373939514 Training d_loss: 0.7754 Training g_loss: 1.5761 Training q_loss: 6156.2031 Explore P: 0.0618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 581 Total reward: 23.0 Average reward fake: 0.33561354875564575 Average reward real: 0.6585756540298462 Training d_loss: 0.9574 Training g_loss: 1.1071 Training q_loss: 3057.5767 Explore P: 0.0617\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 582 Total reward: 18.0 Average reward fake: 0.29821839928627014 Average reward real: 0.7057098150253296 Training d_loss: 0.8386 Training g_loss: 1.2879 Training q_loss: 4027.4602 Explore P: 0.0616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 583 Total reward: 118.0 Average reward fake: 0.3596538305282593 Average reward real: 0.541667103767395 Training d_loss: 1.2906 Training g_loss: 1.0772 Training q_loss: 4504.3535 Explore P: 0.0610\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 584 Total reward: 199.0 Average reward fake: 0.3744106888771057 Average reward real: 0.6514735817909241 Training d_loss: 0.9155 Training g_loss: 1.1047 Training q_loss: 1154.8713 Explore P: 0.0600\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 585 Total reward: 199.0 Average reward fake: 0.4038316309452057 Average reward real: 0.5385515689849854 Training d_loss: 1.2640 Training g_loss: 1.1786 Training q_loss: 3315.8789 Explore P: 0.0590\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 586 Total reward: 199.0 Average reward fake: 0.5217293500900269 Average reward real: 0.5212990045547485 Training d_loss: 1.4364 Training g_loss: 0.6849 Training q_loss: 3005.0574 Explore P: 0.0580\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 587 Total reward: 199.0 Average reward fake: 0.4997076392173767 Average reward real: 0.6109373569488525 Training d_loss: 1.2754 Training g_loss: 0.8078 Training q_loss: 714.4473 Explore P: 0.0571\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 588 Total reward: 199.0 Average reward fake: 0.36041516065597534 Average reward real: 0.5701428055763245 Training d_loss: 1.0791 Training g_loss: 1.3719 Training q_loss: 911.8397 Explore P: 0.0561\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 589 Total reward: 164.0 Average reward fake: 0.4276348054409027 Average reward real: 0.5132163763046265 Training d_loss: 1.3872 Training g_loss: 0.9199 Training q_loss: 3801.7983 Explore P: 0.0554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 590 Total reward: 160.0 Average reward fake: 0.4202830195426941 Average reward real: 0.5294274091720581 Training d_loss: 1.3100 Training g_loss: 1.3773 Training q_loss: 1024.9312 Explore P: 0.0547\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 591 Total reward: 140.0 Average reward fake: 0.4219101071357727 Average reward real: 0.5432361960411072 Training d_loss: 1.2153 Training g_loss: 0.9387 Training q_loss: 542.2664 Explore P: 0.0540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 592 Total reward: 149.0 Average reward fake: 0.5590775609016418 Average reward real: 0.5979188680648804 Training d_loss: 1.5170 Training g_loss: 0.6738 Training q_loss: 2238.7480 Explore P: 0.0534\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 593 Total reward: 180.0 Average reward fake: 0.4205661714076996 Average reward real: 0.6023208498954773 Training d_loss: 1.1217 Training g_loss: 0.9011 Training q_loss: 798.0773 Explore P: 0.0526\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 594 Total reward: 197.0 Average reward fake: 0.531939685344696 Average reward real: 0.532954752445221 Training d_loss: 1.4617 Training g_loss: 0.6682 Training q_loss: 724758.3750 Explore P: 0.0518\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 595 Total reward: 199.0 Average reward fake: 0.428469181060791 Average reward real: 0.5273066759109497 Training d_loss: 1.2314 Training g_loss: 0.8961 Training q_loss: 426.6039 Explore P: 0.0510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 596 Total reward: 199.0 Average reward fake: 0.42606958746910095 Average reward real: 0.500203549861908 Training d_loss: 1.2784 Training g_loss: 0.9035 Training q_loss: 513.5580 Explore P: 0.0502\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "d_loss_list, g_loss_list, q_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions}\n",
    "            rewards_fake, rewards_real = sess.run([model.rewards_fake, model.rewards_real], feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions, model.targetQs: targetQs}\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/DQAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/DQAN-cartpole.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e50c306690e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtest_max_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Get action from DQAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 1000\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    # Save the trained model \n",
    "    saver.restore(sess, 'checkpoints/DQAN-cartpole.ckpt')\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
