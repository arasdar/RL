{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-GAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[-0.03074888 -0.19904344  0.0167938   0.24935339] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03472975 -0.00416529  0.02178086 -0.03798547] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03481306  0.19063766  0.02102116 -0.32371752] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.0310003  -0.00477721  0.0145468  -0.02448021] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03109585  0.19013314  0.0140572  -0.31253819] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.02729319  0.38505203  0.00780644 -0.60075497] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.01959214  0.58006391 -0.00420866 -0.8909688 ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.00799087  0.77524271 -0.02202804 -1.18497175] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00751399  0.97064336 -0.04572747 -1.48447734] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02692686  1.16629201 -0.07541702 -1.79108273] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.1662920088672648 -1.791082728620816\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Next states given\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Current actions given\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, action_size, hidden_size, actions, targetQs, state_size, next_states, alpha=0.1):\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_logits = generator(actions=actions_fake, state_size=state_size, hidden_size=hidden_size)\n",
    "    \n",
    "    # GAN: Discriminate between fake and real\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(states=next_states_fake, hidden_size=hidden_size, reuse=False)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(states=next_states_real, hidden_size=hidden_size, reuse=True)\n",
    "\n",
    "    # GAN: Adverserial training - D-learning\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # GAN: Adverserial training - G-learning\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, d_loss, g_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.d_loss, self.g_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, next_states=self.next_states, actions=self.actions, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, q_loss=self.q_loss, \n",
    "                                                       learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 2000               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = DQAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 17.0 Average reward fake: 0.4733968675136566 Average reward real: 0.5066425204277039 Training q_loss: 0.4849 Training g_loss: 0.7479 Training d_loss: 1.3244 Explore P: 0.9983\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 30.0 Average reward fake: 0.5069016814231873 Average reward real: 0.48017579317092896 Training q_loss: 0.5890 Training g_loss: 0.6794 Training d_loss: 1.4512 Explore P: 0.9954\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 41.0 Average reward fake: 0.4231354296207428 Average reward real: 0.548786997795105 Training q_loss: 3.1925 Training g_loss: 0.8602 Training d_loss: 1.1780 Explore P: 0.9913\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 22.0 Average reward fake: 0.4505396783351898 Average reward real: 0.48093971610069275 Training q_loss: 1.1158 Training g_loss: 0.7971 Training d_loss: 1.3738 Explore P: 0.9892\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 16.0 Average reward fake: 0.49842357635498047 Average reward real: 0.44388899207115173 Training q_loss: 1.4153 Training g_loss: 0.6992 Training d_loss: 1.5648 Explore P: 0.9876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 29.0 Average reward fake: 0.5334846377372742 Average reward real: 0.4975953698158264 Training q_loss: 2.6177 Training g_loss: 0.6265 Training d_loss: 1.4960 Explore P: 0.9848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 31.0 Average reward fake: 0.34968289732933044 Average reward real: 0.5703398585319519 Training q_loss: 6.1598 Training g_loss: 1.0523 Training d_loss: 1.0184 Explore P: 0.9818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 14.0 Average reward fake: 0.44756847620010376 Average reward real: 0.7186121940612793 Training q_loss: 30.7037 Training g_loss: 0.8040 Training d_loss: 0.9962 Explore P: 0.9804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 48.0 Average reward fake: 0.2785690426826477 Average reward real: 0.545834481716156 Training q_loss: 31.4207 Training g_loss: 1.2781 Training d_loss: 0.9580 Explore P: 0.9757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 39.0 Average reward fake: 0.2164224088191986 Average reward real: 0.8139468431472778 Training q_loss: 437.7214 Training g_loss: 1.5305 Training d_loss: 0.5215 Explore P: 0.9720\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 14.0 Average reward fake: 0.3854054808616638 Average reward real: 0.6461480855941772 Training q_loss: 672.5925 Training g_loss: 0.9534 Training d_loss: 1.2121 Explore P: 0.9706\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 9.0 Average reward fake: 0.5606706738471985 Average reward real: 0.3454211354255676 Training q_loss: 15.4454 Training g_loss: 0.5786 Training d_loss: 2.3338 Explore P: 0.9698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 17.0 Average reward fake: 0.5542069673538208 Average reward real: 0.23185229301452637 Training q_loss: 13.1543 Training g_loss: 0.5902 Training d_loss: 2.3153 Explore P: 0.9682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 14.0 Average reward fake: 0.37635520100593567 Average reward real: 0.4913002550601959 Training q_loss: 3.5206 Training g_loss: 0.9764 Training d_loss: 1.1888 Explore P: 0.9668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 27.0 Average reward fake: 0.3717930018901825 Average reward real: 0.655476987361908 Training q_loss: 667.4810 Training g_loss: 0.9896 Training d_loss: 0.9288 Explore P: 0.9642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 17.0 Average reward fake: 0.5970247387886047 Average reward real: 0.5780445337295532 Training q_loss: 4.6480 Training g_loss: 0.5159 Training d_loss: 1.5188 Explore P: 0.9626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 23.0 Average reward fake: 0.4838082790374756 Average reward real: 0.6022272109985352 Training q_loss: 5.3014 Training g_loss: 0.7257 Training d_loss: 1.2004 Explore P: 0.9604\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 17.0 Average reward fake: 0.3720339834690094 Average reward real: 0.6104952692985535 Training q_loss: 508.7154 Training g_loss: 0.9886 Training d_loss: 0.9775 Explore P: 0.9588\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 22.0 Average reward fake: 0.3925846219062805 Average reward real: 0.6547876000404358 Training q_loss: 4.1622 Training g_loss: 0.9331 Training d_loss: 0.9658 Explore P: 0.9567\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 39.0 Average reward fake: 0.5918242931365967 Average reward real: 0.5030062794685364 Training q_loss: 2.6529 Training g_loss: 0.5240 Training d_loss: 1.6108 Explore P: 0.9530\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 42.0 Average reward fake: 0.5038512349128723 Average reward real: 0.5320714712142944 Training q_loss: 17.0842 Training g_loss: 0.6855 Training d_loss: 1.4037 Explore P: 0.9491\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 22.0 Average reward fake: 0.3400997221469879 Average reward real: 0.36750105023384094 Training q_loss: 3.7766 Training g_loss: 1.0785 Training d_loss: 1.4318 Explore P: 0.9470\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 12.0 Average reward fake: 0.3200823962688446 Average reward real: 0.5324198007583618 Training q_loss: 2.4663 Training g_loss: 1.1391 Training d_loss: 1.0401 Explore P: 0.9459\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 10.0 Average reward fake: 0.2908213138580322 Average reward real: 0.5288034677505493 Training q_loss: 452.6029 Training g_loss: 1.2353 Training d_loss: 1.0281 Explore P: 0.9450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 25.0 Average reward fake: 0.4070822596549988 Average reward real: 0.4201062321662903 Training q_loss: 271.5314 Training g_loss: 0.8982 Training d_loss: 1.4704 Explore P: 0.9426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 8.0 Average reward fake: 0.45462483167648315 Average reward real: 0.43087559938430786 Training q_loss: 1.0712 Training g_loss: 0.7884 Training d_loss: 1.5327 Explore P: 0.9419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 20.0 Average reward fake: 0.41345223784446716 Average reward real: 0.5285361409187317 Training q_loss: 324.1792 Training g_loss: 0.8823 Training d_loss: 1.1977 Explore P: 0.9400\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 37.0 Average reward fake: 0.51775062084198 Average reward real: 0.6117554903030396 Training q_loss: 2.0671 Training g_loss: 0.6584 Training d_loss: 1.2816 Explore P: 0.9366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 21.0 Average reward fake: 0.6322425603866577 Average reward real: 0.5626811385154724 Training q_loss: 2.9727 Training g_loss: 0.4585 Training d_loss: 1.6172 Explore P: 0.9346\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 41.0 Average reward fake: 0.4316011965274811 Average reward real: 0.552795946598053 Training q_loss: 163.2846 Training g_loss: 0.8410 Training d_loss: 1.1719 Explore P: 0.9309\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 10.0 Average reward fake: 0.44069018959999084 Average reward real: 0.5913708806037903 Training q_loss: 205.0819 Training g_loss: 0.8216 Training d_loss: 1.1291 Explore P: 0.9299\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 19.0 Average reward fake: 0.48533740639686584 Average reward real: 0.5906649827957153 Training q_loss: 118.2134 Training g_loss: 0.7226 Training d_loss: 1.2207 Explore P: 0.9282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 104.0 Average reward fake: 0.38074591755867004 Average reward real: 0.567433774471283 Training q_loss: 39.7625 Training g_loss: 0.9789 Training d_loss: 1.1121 Explore P: 0.9187\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 9.0 Average reward fake: 0.44014567136764526 Average reward real: 0.5865759253501892 Training q_loss: 0.6074 Training g_loss: 0.8301 Training d_loss: 1.1744 Explore P: 0.9179\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 43.0 Average reward fake: 0.5142651796340942 Average reward real: 0.4469117522239685 Training q_loss: 1.1666 Training g_loss: 0.6659 Training d_loss: 1.5865 Explore P: 0.9140\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 17.0 Average reward fake: 0.5372225046157837 Average reward real: 0.4667786955833435 Training q_loss: 0.6550 Training g_loss: 0.6219 Training d_loss: 1.5587 Explore P: 0.9124\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 24.0 Average reward fake: 0.5622814297676086 Average reward real: 0.4349035322666168 Training q_loss: 1.6642 Training g_loss: 0.5758 Training d_loss: 1.6759 Explore P: 0.9103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 21.0 Average reward fake: 0.5403053164482117 Average reward real: 0.5117075443267822 Training q_loss: 2.7327 Training g_loss: 0.6156 Training d_loss: 1.4551 Explore P: 0.9084\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 22.0 Average reward fake: 0.5260841846466064 Average reward real: 0.5507424473762512 Training q_loss: 0.9809 Training g_loss: 0.6423 Training d_loss: 1.3464 Explore P: 0.9064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 10.0 Average reward fake: 0.5040445923805237 Average reward real: 0.561866044998169 Training q_loss: 61.8592 Training g_loss: 0.6851 Training d_loss: 1.2809 Explore P: 0.9055\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 22.0 Average reward fake: 0.4769037663936615 Average reward real: 0.5844792127609253 Training q_loss: 0.6495 Training g_loss: 0.7406 Training d_loss: 1.1922 Explore P: 0.9036\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 25.0 Average reward fake: 0.47810062766075134 Average reward real: 0.5939972996711731 Training q_loss: 1.4231 Training g_loss: 0.7392 Training d_loss: 1.1877 Explore P: 0.9013\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 23.0 Average reward fake: 0.4876529574394226 Average reward real: 0.548098623752594 Training q_loss: 21.5071 Training g_loss: 0.7201 Training d_loss: 1.2877 Explore P: 0.8993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 20.0 Average reward fake: 0.5073140859603882 Average reward real: 0.5011984705924988 Training q_loss: 1.5644 Training g_loss: 0.6812 Training d_loss: 1.4180 Explore P: 0.8975\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 44.0 Average reward fake: 0.4677094519138336 Average reward real: 0.5068175792694092 Training q_loss: 4.0074 Training g_loss: 0.7601 Training d_loss: 1.3220 Explore P: 0.8936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 10.0 Average reward fake: 0.4552474915981293 Average reward real: 0.48854461312294006 Training q_loss: 0.7151 Training g_loss: 0.7880 Training d_loss: 1.3286 Explore P: 0.8927\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 34.0 Average reward fake: 0.49558648467063904 Average reward real: 0.5365912318229675 Training q_loss: 20.4550 Training g_loss: 0.7088 Training d_loss: 1.3271 Explore P: 0.8897\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 91.0 Average reward fake: 0.5145807266235352 Average reward real: 0.5224341154098511 Training q_loss: 3.0706 Training g_loss: 0.6651 Training d_loss: 1.3774 Explore P: 0.8818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 17.0 Average reward fake: 0.5011904239654541 Average reward real: 0.4957202076911926 Training q_loss: 2.2532 Training g_loss: 0.6915 Training d_loss: 1.4063 Explore P: 0.8803\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 26.0 Average reward fake: 0.5143030881881714 Average reward real: 0.5256907939910889 Training q_loss: 1.1183 Training g_loss: 0.6658 Training d_loss: 1.3697 Explore P: 0.8780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 14.0 Average reward fake: 0.4950564503669739 Average reward real: 0.5292736887931824 Training q_loss: 21.3510 Training g_loss: 0.7039 Training d_loss: 1.3214 Explore P: 0.8768\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 23.0 Average reward fake: 0.48563772439956665 Average reward real: 0.5726386904716492 Training q_loss: 6.8087 Training g_loss: 0.7236 Training d_loss: 1.2276 Explore P: 0.8748\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 31.0 Average reward fake: 0.4728388786315918 Average reward real: 0.5338847637176514 Training q_loss: 2.8188 Training g_loss: 0.7495 Training d_loss: 1.2747 Explore P: 0.8721\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 9.0 Average reward fake: 0.4766255021095276 Average reward real: 0.5645107626914978 Training q_loss: 1.3927 Training g_loss: 0.7413 Training d_loss: 1.2270 Explore P: 0.8714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 36.0 Average reward fake: 0.4737366735935211 Average reward real: 0.4589076638221741 Training q_loss: 30.8064 Training g_loss: 0.7508 Training d_loss: 1.4383 Explore P: 0.8683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 12.0 Average reward fake: 0.4757866859436035 Average reward real: 0.4377937316894531 Training q_loss: 0.4193 Training g_loss: 0.7434 Training d_loss: 1.4881 Explore P: 0.8672\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 80.0 Average reward fake: 0.49812760949134827 Average reward real: 0.5050333142280579 Training q_loss: 3.9061 Training g_loss: 0.7015 Training d_loss: 1.3915 Explore P: 0.8604\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 22.0 Average reward fake: 0.4984172284603119 Average reward real: 0.4892917573451996 Training q_loss: 1.3978 Training g_loss: 0.7024 Training d_loss: 1.4167 Explore P: 0.8585\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 34.0 Average reward fake: 0.4649030268192291 Average reward real: 0.48325562477111816 Training q_loss: 2.2610 Training g_loss: 0.7685 Training d_loss: 1.3698 Explore P: 0.8556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 18.0 Average reward fake: 0.4583263397216797 Average reward real: 0.49395161867141724 Training q_loss: 1.1234 Training g_loss: 0.7820 Training d_loss: 1.3278 Explore P: 0.8541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 14.0 Average reward fake: 0.4741194248199463 Average reward real: 0.49213919043540955 Training q_loss: 1.0813 Training g_loss: 0.7480 Training d_loss: 1.3611 Explore P: 0.8529\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 13.0 Average reward fake: 0.47870343923568726 Average reward real: 0.4875510334968567 Training q_loss: 0.7410 Training g_loss: 0.7387 Training d_loss: 1.3777 Explore P: 0.8519\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 23.0 Average reward fake: 0.521318793296814 Average reward real: 0.5255423784255981 Training q_loss: 3.5371 Training g_loss: 0.6536 Training d_loss: 1.3937 Explore P: 0.8499\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 42.0 Average reward fake: 0.47954970598220825 Average reward real: 0.5403599143028259 Training q_loss: 40.4648 Training g_loss: 0.7347 Training d_loss: 1.2797 Explore P: 0.8464\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 14.0 Average reward fake: 0.46393951773643494 Average reward real: 0.5326390862464905 Training q_loss: 1.1291 Training g_loss: 0.7682 Training d_loss: 1.2618 Explore P: 0.8452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 33.0 Average reward fake: 0.4821268916130066 Average reward real: 0.5212443470954895 Training q_loss: 22.7058 Training g_loss: 0.7296 Training d_loss: 1.3220 Explore P: 0.8425\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 68.0 Average reward fake: 0.3309726119041443 Average reward real: 0.6225009560585022 Training q_loss: 25.9702 Training g_loss: 1.1141 Training d_loss: 0.9256 Explore P: 0.8368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 20.0 Average reward fake: 0.48154744505882263 Average reward real: 0.39066481590270996 Training q_loss: 1.6175 Training g_loss: 0.7389 Training d_loss: 1.7696 Explore P: 0.8352\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 28.0 Average reward fake: 0.46209973096847534 Average reward real: 0.5774620175361633 Training q_loss: 1.0733 Training g_loss: 0.7735 Training d_loss: 1.2483 Explore P: 0.8329\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 34.0 Average reward fake: 0.5276035666465759 Average reward real: 0.5504467487335205 Training q_loss: 42.5767 Training g_loss: 0.6461 Training d_loss: 1.4004 Explore P: 0.8301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 14.0 Average reward fake: 0.4459071755409241 Average reward real: 0.47738438844680786 Training q_loss: 4.4286 Training g_loss: 0.8206 Training d_loss: 1.4387 Explore P: 0.8289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 49.0 Average reward fake: 0.3550422787666321 Average reward real: 0.4648404121398926 Training q_loss: 1.6035 Training g_loss: 1.0406 Training d_loss: 1.4796 Explore P: 0.8249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 34.0 Average reward fake: 0.4356992840766907 Average reward real: 0.3464757800102234 Training q_loss: 2.4547 Training g_loss: 0.8322 Training d_loss: 1.6804 Explore P: 0.8222\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 14.0 Average reward fake: 0.36488625407218933 Average reward real: 0.5445603132247925 Training q_loss: 1.8966 Training g_loss: 1.0084 Training d_loss: 1.0952 Explore P: 0.8210\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 34.0 Average reward fake: 0.41157978773117065 Average reward real: 0.7259323596954346 Training q_loss: 2.6691 Training g_loss: 0.8877 Training d_loss: 0.9209 Explore P: 0.8183\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 17.0 Average reward fake: 0.4683923125267029 Average reward real: 0.5758675336837769 Training q_loss: 1.9874 Training g_loss: 0.7587 Training d_loss: 1.3514 Explore P: 0.8169\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 14.0 Average reward fake: 0.45041823387145996 Average reward real: 0.5059017539024353 Training q_loss: 1.2537 Training g_loss: 0.7978 Training d_loss: 1.3942 Explore P: 0.8158\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 26.0 Average reward fake: 0.5137926340103149 Average reward real: 0.47189727425575256 Training q_loss: 3.3127 Training g_loss: 0.6659 Training d_loss: 1.5308 Explore P: 0.8137\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 14.0 Average reward fake: 0.5334557890892029 Average reward real: 0.5160156488418579 Training q_loss: 2.1682 Training g_loss: 0.6289 Training d_loss: 1.4724 Explore P: 0.8126\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 20.0 Average reward fake: 0.5641013383865356 Average reward real: 0.48768025636672974 Training q_loss: 2.6990 Training g_loss: 0.5727 Training d_loss: 1.5663 Explore P: 0.8110\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 14.0 Average reward fake: 0.5537261366844177 Average reward real: 0.4407682418823242 Training q_loss: 145.9627 Training g_loss: 0.5919 Training d_loss: 1.6364 Explore P: 0.8098\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 32.0 Average reward fake: 0.4477495551109314 Average reward real: 0.5156829953193665 Training q_loss: 1.3156 Training g_loss: 0.8038 Training d_loss: 1.2597 Explore P: 0.8073\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 20.0 Average reward fake: 0.4049050807952881 Average reward real: 0.5907891988754272 Training q_loss: 3.6768 Training g_loss: 0.9043 Training d_loss: 1.0585 Explore P: 0.8057\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 54.0 Average reward fake: 0.4463551640510559 Average reward real: 0.5027942657470703 Training q_loss: 3.0842 Training g_loss: 0.8066 Training d_loss: 1.2945 Explore P: 0.8014\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 25.0 Average reward fake: 0.46904587745666504 Average reward real: 0.5251954197883606 Training q_loss: 2.0015 Training g_loss: 0.7571 Training d_loss: 1.2921 Explore P: 0.7994\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 31.0 Average reward fake: 0.4940316677093506 Average reward real: 0.5224030613899231 Training q_loss: 2.1891 Training g_loss: 0.7051 Training d_loss: 1.3456 Explore P: 0.7970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 52.0 Average reward fake: 0.44014468789100647 Average reward real: 0.5311852097511292 Training q_loss: 1.4218 Training g_loss: 0.8207 Training d_loss: 1.2250 Explore P: 0.7929\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 71.0 Average reward fake: 0.4781338572502136 Average reward real: 0.5485180616378784 Training q_loss: 1.2204 Training g_loss: 0.7380 Training d_loss: 1.2695 Explore P: 0.7874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 56.0 Average reward fake: 0.5432122945785522 Average reward real: 0.6004701852798462 Training q_loss: 4.6208 Training g_loss: 0.6156 Training d_loss: 1.3316 Explore P: 0.7830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 21.0 Average reward fake: 0.5140421986579895 Average reward real: 0.589018702507019 Training q_loss: 2.7097 Training g_loss: 0.6781 Training d_loss: 1.2894 Explore P: 0.7814\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 22.0 Average reward fake: 0.4888528287410736 Average reward real: 0.47135287523269653 Training q_loss: 119.6109 Training g_loss: 0.7259 Training d_loss: 1.4684 Explore P: 0.7797\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 46.0 Average reward fake: 0.47958046197891235 Average reward real: 0.5059897899627686 Training q_loss: 1.5706 Training g_loss: 0.7585 Training d_loss: 1.4329 Explore P: 0.7762\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 31.0 Average reward fake: 0.4221034646034241 Average reward real: 0.5363606214523315 Training q_loss: 1.9677 Training g_loss: 0.8639 Training d_loss: 1.1912 Explore P: 0.7738\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 48.0 Average reward fake: 0.4003715515136719 Average reward real: 0.46312838792800903 Training q_loss: 64.7181 Training g_loss: 0.9172 Training d_loss: 1.3117 Explore P: 0.7701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 47.0 Average reward fake: 0.5002356767654419 Average reward real: 0.5416938066482544 Training q_loss: 2.7216 Training g_loss: 0.8781 Training d_loss: 1.5429 Explore P: 0.7666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 67.0 Average reward fake: 0.4981081485748291 Average reward real: 0.5226852893829346 Training q_loss: 1.7303 Training g_loss: 0.6985 Training d_loss: 1.3487 Explore P: 0.7615\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 31.0 Average reward fake: 0.5144556760787964 Average reward real: 0.5200132727622986 Training q_loss: 2.2477 Training g_loss: 0.6646 Training d_loss: 1.3803 Explore P: 0.7592\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 21.0 Average reward fake: 0.4820868968963623 Average reward real: 0.5592914819717407 Training q_loss: 75.2532 Training g_loss: 0.7334 Training d_loss: 1.2454 Explore P: 0.7576\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 12.0 Average reward fake: 0.47187677025794983 Average reward real: 0.5513986349105835 Training q_loss: 60.8062 Training g_loss: 0.7556 Training d_loss: 1.2436 Explore P: 0.7567\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 24.0 Average reward fake: 0.4878348708152771 Average reward real: 0.5514531135559082 Training q_loss: 93.4869 Training g_loss: 0.7240 Training d_loss: 1.2894 Explore P: 0.7549\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 54.0 Average reward fake: 0.5252276659011841 Average reward real: 0.48335814476013184 Training q_loss: 3.4408 Training g_loss: 0.6442 Training d_loss: 1.4804 Explore P: 0.7509\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 30.0 Average reward fake: 0.5147062540054321 Average reward real: 0.4606053829193115 Training q_loss: 128.5137 Training g_loss: 0.6685 Training d_loss: 1.5153 Explore P: 0.7487\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 42.0 Average reward fake: 0.489196240901947 Average reward real: 0.5015711784362793 Training q_loss: 4.3976 Training g_loss: 0.7159 Training d_loss: 1.3670 Explore P: 0.7456\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 15.0 Average reward fake: 0.496767520904541 Average reward real: 0.47683948278427124 Training q_loss: 2.2745 Training g_loss: 0.7004 Training d_loss: 1.4322 Explore P: 0.7445\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 29.0 Average reward fake: 0.471097469329834 Average reward real: 0.4920080304145813 Training q_loss: 4.9828 Training g_loss: 0.7527 Training d_loss: 1.3480 Explore P: 0.7424\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 55.0 Average reward fake: 0.4717167019844055 Average reward real: 0.4800797402858734 Training q_loss: 1.2652 Training g_loss: 0.7518 Training d_loss: 1.3784 Explore P: 0.7384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 25.0 Average reward fake: 0.503696620464325 Average reward real: 0.4794275760650635 Training q_loss: 4.9449 Training g_loss: 0.6869 Training d_loss: 1.4438 Explore P: 0.7365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 28.0 Average reward fake: 0.5006760954856873 Average reward real: 0.503774106502533 Training q_loss: 11.5983 Training g_loss: 0.6924 Training d_loss: 1.3877 Explore P: 0.7345\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 30.0 Average reward fake: 0.48398107290267944 Average reward real: 0.5181438326835632 Training q_loss: 5.5628 Training g_loss: 0.7266 Training d_loss: 1.3256 Explore P: 0.7323\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 65.0 Average reward fake: 0.5041435956954956 Average reward real: 0.5597221255302429 Training q_loss: 4.2680 Training g_loss: 0.6853 Training d_loss: 1.2937 Explore P: 0.7277\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 11.0 Average reward fake: 0.4922965466976166 Average reward real: 0.5472244024276733 Training q_loss: 49.9925 Training g_loss: 0.7088 Training d_loss: 1.2975 Explore P: 0.7269\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 24.0 Average reward fake: 0.4540237784385681 Average reward real: 0.5621142387390137 Training q_loss: 4.9649 Training g_loss: 0.7896 Training d_loss: 1.1936 Explore P: 0.7252\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 54.0 Average reward fake: 0.5337578058242798 Average reward real: 0.5159876942634583 Training q_loss: 4.2766 Training g_loss: 0.6285 Training d_loss: 1.4384 Explore P: 0.7213\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 39.0 Average reward fake: 0.4876807630062103 Average reward real: 0.46503907442092896 Training q_loss: 1.7034 Training g_loss: 0.7190 Training d_loss: 1.4406 Explore P: 0.7185\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 41.0 Average reward fake: 0.44137105345726013 Average reward real: 0.44479966163635254 Training q_loss: 112.3415 Training g_loss: 0.8196 Training d_loss: 1.3951 Explore P: 0.7156\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 27.0 Average reward fake: 0.4213469922542572 Average reward real: 0.5308651924133301 Training q_loss: 23.1246 Training g_loss: 0.8656 Training d_loss: 1.2019 Explore P: 0.7137\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 21.0 Average reward fake: 0.5101144909858704 Average reward real: 0.5173164010047913 Training q_loss: 1.9128 Training g_loss: 0.6739 Training d_loss: 1.3902 Explore P: 0.7123\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 25.0 Average reward fake: 0.48099440336227417 Average reward real: 0.5474568605422974 Training q_loss: 3.8323 Training g_loss: 0.7484 Training d_loss: 1.3656 Explore P: 0.7105\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 40.0 Average reward fake: 0.5369477272033691 Average reward real: 0.5040999054908752 Training q_loss: 85.4616 Training g_loss: 0.6221 Training d_loss: 1.4612 Explore P: 0.7077\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 110.0 Average reward fake: 0.5254062414169312 Average reward real: 0.4529341757297516 Training q_loss: 1.6219 Training g_loss: 0.6436 Training d_loss: 1.5468 Explore P: 0.7001\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 60.0 Average reward fake: 0.5129382014274597 Average reward real: 0.4538099765777588 Training q_loss: 7.4103 Training g_loss: 0.6676 Training d_loss: 1.5531 Explore P: 0.6960\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 125.0 Average reward fake: 0.3977918028831482 Average reward real: 0.5780754089355469 Training q_loss: 4.6441 Training g_loss: 0.9221 Training d_loss: 1.0713 Explore P: 0.6874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 28.0 Average reward fake: 0.4214909076690674 Average reward real: 0.6503992676734924 Training q_loss: 3.3350 Training g_loss: 0.8637 Training d_loss: 1.0034 Explore P: 0.6855\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 32.0 Average reward fake: 0.5922120809555054 Average reward real: 0.5594085454940796 Training q_loss: 3.2701 Training g_loss: 0.5445 Training d_loss: 1.6631 Explore P: 0.6834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 91.0 Average reward fake: 0.5240521430969238 Average reward real: 0.5018901824951172 Training q_loss: 4.6240 Training g_loss: 0.6504 Training d_loss: 1.4562 Explore P: 0.6773\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 58.0 Average reward fake: 0.4869323670864105 Average reward real: 0.48363909125328064 Training q_loss: 5.1838 Training g_loss: 0.7198 Training d_loss: 1.4385 Explore P: 0.6734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 56.0 Average reward fake: 0.32695433497428894 Average reward real: 0.5398023724555969 Training q_loss: 1.8636 Training g_loss: 1.1178 Training d_loss: 1.0480 Explore P: 0.6697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 36.0 Average reward fake: 0.3815603256225586 Average reward real: 0.5466140508651733 Training q_loss: 4.2604 Training g_loss: 0.9633 Training d_loss: 1.0999 Explore P: 0.6673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 85.0 Average reward fake: 0.5037238597869873 Average reward real: 0.581647515296936 Training q_loss: 4.2513 Training g_loss: 0.6891 Training d_loss: 1.3121 Explore P: 0.6618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 30.0 Average reward fake: 0.568339467048645 Average reward real: 0.4820834994316101 Training q_loss: 184.8676 Training g_loss: 0.5841 Training d_loss: 1.6465 Explore P: 0.6598\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 41.0 Average reward fake: 0.6269657015800476 Average reward real: 0.4697180688381195 Training q_loss: 274.3485 Training g_loss: 0.4835 Training d_loss: 1.8247 Explore P: 0.6572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 122.0 Average reward fake: 0.47851625084877014 Average reward real: 0.49782609939575195 Training q_loss: 5.0262 Training g_loss: 0.7374 Training d_loss: 1.3702 Explore P: 0.6493\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 119.0 Average reward fake: 0.4594019949436188 Average reward real: 0.5353674292564392 Training q_loss: 5.3615 Training g_loss: 0.7780 Training d_loss: 1.2998 Explore P: 0.6418\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 85.0 Average reward fake: 0.4863045811653137 Average reward real: 0.5811032056808472 Training q_loss: 8.3985 Training g_loss: 0.7236 Training d_loss: 1.2244 Explore P: 0.6364\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 87.0 Average reward fake: 0.4757173955440521 Average reward real: 0.5544084310531616 Training q_loss: 1.7547 Training g_loss: 0.7429 Training d_loss: 1.2424 Explore P: 0.6310\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 58.0 Average reward fake: 0.4765566289424896 Average reward real: 0.4751199185848236 Training q_loss: 9.9034 Training g_loss: 0.7412 Training d_loss: 1.3994 Explore P: 0.6274\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 21.0 Average reward fake: 0.4781547486782074 Average reward real: 0.4526579976081848 Training q_loss: 5.8935 Training g_loss: 0.7378 Training d_loss: 1.4739 Explore P: 0.6261\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 108.0 Average reward fake: 0.45204591751098633 Average reward real: 0.43775829672813416 Training q_loss: 5.3134 Training g_loss: 0.7937 Training d_loss: 1.4927 Explore P: 0.6195\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 118.0 Average reward fake: 0.4850695729255676 Average reward real: 0.45123010873794556 Training q_loss: 8.3753 Training g_loss: 0.7255 Training d_loss: 1.5117 Explore P: 0.6123\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 95.0 Average reward fake: 0.4394541382789612 Average reward real: 0.5440611839294434 Training q_loss: 8.6425 Training g_loss: 0.8236 Training d_loss: 1.1983 Explore P: 0.6066\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 85.0 Average reward fake: 0.496587336063385 Average reward real: 0.5667728185653687 Training q_loss: 14.6526 Training g_loss: 0.7002 Training d_loss: 1.2843 Explore P: 0.6016\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 111.0 Average reward fake: 0.5170462727546692 Average reward real: 0.530196487903595 Training q_loss: 9.5213 Training g_loss: 0.6596 Training d_loss: 1.3704 Explore P: 0.5951\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 176.0 Average reward fake: 0.4812239110469818 Average reward real: 0.5202260613441467 Training q_loss: 28.8442 Training g_loss: 0.7316 Training d_loss: 1.3237 Explore P: 0.5848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 146.0 Average reward fake: 0.35241180658340454 Average reward real: 0.57285076379776 Training q_loss: 14.5825 Training g_loss: 1.1260 Training d_loss: 1.0334 Explore P: 0.5765\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 156.0 Average reward fake: 0.5186900496482849 Average reward real: 0.49538150429725647 Training q_loss: 7.7217 Training g_loss: 0.6564 Training d_loss: 1.4424 Explore P: 0.5677\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 40.0 Average reward fake: 0.5201542377471924 Average reward real: 0.505262553691864 Training q_loss: 23.0053 Training g_loss: 0.6534 Training d_loss: 1.4218 Explore P: 0.5655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 33.0 Average reward fake: 0.5228703022003174 Average reward real: 0.5157651305198669 Training q_loss: 10.6968 Training g_loss: 0.6486 Training d_loss: 1.4041 Explore P: 0.5637\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 21.0 Average reward fake: 0.5093927383422852 Average reward real: 0.5422519445419312 Training q_loss: 10.0143 Training g_loss: 0.6746 Training d_loss: 1.3261 Explore P: 0.5625\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 126.0 Average reward fake: 0.46749943494796753 Average reward real: 0.5013691782951355 Training q_loss: 502.4823 Training g_loss: 0.7604 Training d_loss: 1.3257 Explore P: 0.5556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 129.0 Average reward fake: 0.49357929825782776 Average reward real: 0.5064276456832886 Training q_loss: 285.5262 Training g_loss: 0.7076 Training d_loss: 1.3654 Explore P: 0.5486\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "d_loss_list, g_loss_list, q_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions}\n",
    "            rewards_fake, rewards_real = sess.run([model.rewards_fake, model.rewards_real], feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions, \n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/Q-GAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/Q-GAN-cartpole.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-24514cd51c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtest_max_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Get action from DQAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "test_episodes = 5\n",
    "test_max_steps = 200\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    saver.restore(sess, 'checkpoints/Q-GAN-cartpole.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
