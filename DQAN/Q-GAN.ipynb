{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-GAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[ 0.01932118 -0.20522046  0.02098161  0.32420111] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01521677 -0.01040345  0.02746563  0.03820805] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.0150087   0.18431409  0.02822979 -0.24568423] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.01869498 -0.01119944  0.02331611  0.05576768] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.018471    0.18358057  0.02443146 -0.2294686 ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02214261  0.37834503  0.01984209 -0.51434597] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02970951  0.573182    0.00955517 -0.8007107 ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.04117315  0.76817159 -0.00645904 -1.09037257] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.05653658  0.96337809 -0.0282665  -1.38507515] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.07580414  1.15884088 -0.055968   -1.68646146] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.1588408840355289 -1.6864614638868696\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Next states given\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Current actions given\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits_actions)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, action_size, hidden_size, actions, targetQs, state_size, next_states, alpha=0.1):\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_logits = generator(actions=actions_fake, state_size=state_size, hidden_size=hidden_size)\n",
    "    \n",
    "    # GAN: Discriminate between fake and real\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(states=next_states_fake, hidden_size=hidden_size, reuse=False)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(states=next_states_real, hidden_size=hidden_size, reuse=True)\n",
    "\n",
    "    # GAN: Adverserial training - D-learning\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # GAN: Adverserial training - G-learning\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, d_loss, g_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, q_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.d_loss, self.g_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, actions=self.actions, states=self.states, \n",
    "            next_states=self.next_states, hidden_size=hidden_size, targetQs=self.targetQs)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(d_loss=self.d_loss, g_loss=self.g_loss, \n",
    "                                                       q_loss=self.q_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 500          # max number of episodes to learn from\n",
    "max_steps = 2000               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64              # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                # number of units for the input state/observation -- simulation\n",
    "action_size = 2               # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 10                # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = DQAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 4.0 Average reward fake: 0.4996602535247803 Average reward real: 0.5107257962226868 Training d_loss: 1.3654 Training g_loss: 0.6938 Training q_loss: 0.3732 Explore P: 0.9996\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 23.0 Average reward fake: 0.5017067193984985 Average reward real: 0.5169975757598877 Training d_loss: 1.3612 Training g_loss: 0.6897 Training q_loss: 0.6118 Explore P: 0.9973\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 8.0 Average reward fake: 0.5013338923454285 Average reward real: 0.4972912669181824 Training d_loss: 1.3991 Training g_loss: 0.6905 Training q_loss: 0.4966 Explore P: 0.9965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 20.0 Average reward fake: 0.4978434443473816 Average reward real: 0.5005236864089966 Training d_loss: 1.3860 Training g_loss: 0.6974 Training q_loss: 2.5713 Explore P: 0.9946\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 10.0 Average reward fake: 0.48997268080711365 Average reward real: 0.5233646035194397 Training d_loss: 1.3296 Training g_loss: 0.7132 Training q_loss: 0.9001 Explore P: 0.9936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 13.0 Average reward fake: 0.47282689809799194 Average reward real: 0.4985678791999817 Training d_loss: 1.3547 Training g_loss: 0.7484 Training q_loss: 4.3751 Explore P: 0.9923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 21.0 Average reward fake: 0.4772868752479553 Average reward real: 0.44721174240112305 Training d_loss: 1.4722 Training g_loss: 0.7396 Training q_loss: 1.0912 Explore P: 0.9902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 10.0 Average reward fake: 0.4742652475833893 Average reward real: 0.46800798177719116 Training d_loss: 1.4185 Training g_loss: 0.7447 Training q_loss: 12.6659 Explore P: 0.9893\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 15.0 Average reward fake: 0.49881237745285034 Average reward real: 0.47019076347351074 Training d_loss: 1.4546 Training g_loss: 0.6954 Training q_loss: 2.5950 Explore P: 0.9878\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 13.0 Average reward fake: 0.5090036392211914 Average reward real: 0.4882713258266449 Training d_loss: 1.4362 Training g_loss: 0.6757 Training q_loss: 2.5039 Explore P: 0.9865\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 13.0 Average reward fake: 0.5057275295257568 Average reward real: 0.5020065307617188 Training d_loss: 1.4049 Training g_loss: 0.6818 Training q_loss: 7.4334 Explore P: 0.9853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 31.0 Average reward fake: 0.4725203514099121 Average reward real: 0.5256892442703247 Training d_loss: 1.2875 Training g_loss: 0.7497 Training q_loss: 13.5344 Explore P: 0.9822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 17.0 Average reward fake: 0.3848220705986023 Average reward real: 0.5913076996803284 Training d_loss: 1.0137 Training g_loss: 0.9554 Training q_loss: 10.5859 Explore P: 0.9806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 25.0 Average reward fake: 0.3590358793735504 Average reward real: 0.7521286010742188 Training d_loss: 0.7954 Training g_loss: 1.0362 Training q_loss: 98.5874 Explore P: 0.9782\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 25.0 Average reward fake: 0.5700867772102356 Average reward real: 0.44230180978775024 Training d_loss: 1.7999 Training g_loss: 0.5618 Training q_loss: 106.7927 Explore P: 0.9757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 17.0 Average reward fake: 0.6879696249961853 Average reward real: 0.2706969678401947 Training d_loss: 2.5541 Training g_loss: 0.3740 Training q_loss: 30.1771 Explore P: 0.9741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 13.0 Average reward fake: 0.5786105394363403 Average reward real: 0.32547706365585327 Training d_loss: 1.9990 Training g_loss: 0.5471 Training q_loss: 56.0962 Explore P: 0.9729\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 12.0 Average reward fake: 0.49411648511886597 Average reward real: 0.4237836003303528 Training d_loss: 1.5420 Training g_loss: 0.7050 Training q_loss: 147.9174 Explore P: 0.9717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 17.0 Average reward fake: 0.4290241301059723 Average reward real: 0.5172603726387024 Training d_loss: 1.2304 Training g_loss: 0.8465 Training q_loss: 5.8897 Explore P: 0.9701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 18.0 Average reward fake: 0.3558324873447418 Average reward real: 0.5324622392654419 Training d_loss: 1.0826 Training g_loss: 1.0336 Training q_loss: 13.7163 Explore P: 0.9683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 10.0 Average reward fake: 0.3144629895687103 Average reward real: 0.5675348043441772 Training d_loss: 0.9706 Training g_loss: 1.1570 Training q_loss: 3.2958 Explore P: 0.9674\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 20.0 Average reward fake: 0.3033713698387146 Average reward real: 0.6256643533706665 Training d_loss: 0.8511 Training g_loss: 1.1924 Training q_loss: 2.0649 Explore P: 0.9655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 51.0 Average reward fake: 0.36060288548469543 Average reward real: 0.7086614370346069 Training d_loss: 0.8905 Training g_loss: 1.0203 Training q_loss: 130.0705 Explore P: 0.9606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 13.0 Average reward fake: 0.528826892375946 Average reward real: 0.4767417013645172 Training d_loss: 1.6796 Training g_loss: 0.6365 Training q_loss: 83.8347 Explore P: 0.9594\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 16.0 Average reward fake: 0.45345640182495117 Average reward real: 0.4273253083229065 Training d_loss: 1.5843 Training g_loss: 0.7897 Training q_loss: 70.6124 Explore P: 0.9579\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 13.0 Average reward fake: 0.4661548137664795 Average reward real: 0.4876793920993805 Training d_loss: 1.4446 Training g_loss: 0.7602 Training q_loss: 1.6991 Explore P: 0.9566\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 9.0 Average reward fake: 0.48290354013442993 Average reward real: 0.46079978346824646 Training d_loss: 1.5263 Training g_loss: 0.7361 Training q_loss: 1.4248 Explore P: 0.9558\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 21.0 Average reward fake: 0.3821052610874176 Average reward real: 0.5128808617591858 Training d_loss: 1.1695 Training g_loss: 0.9644 Training q_loss: 0.7648 Explore P: 0.9538\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 26.0 Average reward fake: 0.35130858421325684 Average reward real: 0.5957189202308655 Training d_loss: 1.0703 Training g_loss: 1.0571 Training q_loss: 1.1101 Explore P: 0.9513\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 27.0 Average reward fake: 0.4476455748081207 Average reward real: 0.48668843507766724 Training d_loss: 1.3712 Training g_loss: 0.8096 Training q_loss: 0.6229 Explore P: 0.9488\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 26.0 Average reward fake: 0.5112501978874207 Average reward real: 0.47434863448143005 Training d_loss: 1.5660 Training g_loss: 0.6768 Training q_loss: 31.8509 Explore P: 0.9464\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 60.0 Average reward fake: 0.5497351884841919 Average reward real: 0.5214313864707947 Training d_loss: 1.4724 Training g_loss: 0.5983 Training q_loss: 17.6637 Explore P: 0.9408\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 13.0 Average reward fake: 0.5723288655281067 Average reward real: 0.478348970413208 Training d_loss: 1.6043 Training g_loss: 0.5577 Training q_loss: 0.6944 Explore P: 0.9396\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 22.0 Average reward fake: 0.5509771108627319 Average reward real: 0.4723638594150543 Training d_loss: 1.5518 Training g_loss: 0.5962 Training q_loss: 46.8292 Explore P: 0.9375\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 10.0 Average reward fake: 0.48762258887290955 Average reward real: 0.5253255367279053 Training d_loss: 1.3165 Training g_loss: 0.7183 Training q_loss: 16.0302 Explore P: 0.9366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 15.0 Average reward fake: 0.4577775001525879 Average reward real: 0.5511488914489746 Training d_loss: 1.2210 Training g_loss: 0.7819 Training q_loss: 1.3029 Explore P: 0.9352\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 23.0 Average reward fake: 0.4467921257019043 Average reward real: 0.5482815504074097 Training d_loss: 1.2089 Training g_loss: 0.8063 Training q_loss: 24.5733 Explore P: 0.9331\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 14.0 Average reward fake: 0.47223129868507385 Average reward real: 0.512559711933136 Training d_loss: 1.3332 Training g_loss: 0.7502 Training q_loss: 48.2857 Explore P: 0.9318\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 16.0 Average reward fake: 0.5040770173072815 Average reward real: 0.5114450454711914 Training d_loss: 1.3918 Training g_loss: 0.6850 Training q_loss: 0.5959 Explore P: 0.9303\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 11.0 Average reward fake: 0.5174638032913208 Average reward real: 0.47070497274398804 Training d_loss: 1.5095 Training g_loss: 0.6594 Training q_loss: 33.7497 Explore P: 0.9293\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 12.0 Average reward fake: 0.5378214716911316 Average reward real: 0.4202795922756195 Training d_loss: 1.6573 Training g_loss: 0.6212 Training q_loss: 15.1248 Explore P: 0.9282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 21.0 Average reward fake: 0.5117475390434265 Average reward real: 0.45162826776504517 Training d_loss: 1.5222 Training g_loss: 0.6694 Training q_loss: 2.1312 Explore P: 0.9263\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 34.0 Average reward fake: 0.4588659405708313 Average reward real: 0.5086764693260193 Training d_loss: 1.3002 Training g_loss: 0.7789 Training q_loss: 34.3737 Explore P: 0.9232\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 14.0 Average reward fake: 0.4621663987636566 Average reward real: 0.5082548260688782 Training d_loss: 1.3113 Training g_loss: 0.7722 Training q_loss: 1.6499 Explore P: 0.9219\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 15.0 Average reward fake: 0.4290776252746582 Average reward real: 0.5178982615470886 Training d_loss: 1.2268 Training g_loss: 0.8462 Training q_loss: 1.9217 Explore P: 0.9205\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 15.0 Average reward fake: 0.4190579950809479 Average reward real: 0.5330228209495544 Training d_loss: 1.1840 Training g_loss: 0.8697 Training q_loss: 1.8034 Explore P: 0.9191\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 18.0 Average reward fake: 0.42557471990585327 Average reward real: 0.5850846171379089 Training d_loss: 1.1088 Training g_loss: 0.8543 Training q_loss: 27.8047 Explore P: 0.9175\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 22.0 Average reward fake: 0.49719128012657166 Average reward real: 0.43327468633651733 Training d_loss: 1.5581 Training g_loss: 0.6988 Training q_loss: 0.6318 Explore P: 0.9155\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 12.0 Average reward fake: 0.4983108639717102 Average reward real: 0.4866876006126404 Training d_loss: 1.4327 Training g_loss: 0.6971 Training q_loss: 2.2604 Explore P: 0.9144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 29.0 Average reward fake: 0.5579544901847839 Average reward real: 0.5139913558959961 Training d_loss: 1.5145 Training g_loss: 0.5831 Training q_loss: 58.3690 Explore P: 0.9118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 11.0 Average reward fake: 0.5539010763168335 Average reward real: 0.49475350975990295 Training d_loss: 1.5412 Training g_loss: 0.5911 Training q_loss: 1.3248 Explore P: 0.9108\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 15.0 Average reward fake: 0.5811492204666138 Average reward real: 0.5008746385574341 Training d_loss: 1.5863 Training g_loss: 0.5429 Training q_loss: 3.2114 Explore P: 0.9095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 20.0 Average reward fake: 0.5476690530776978 Average reward real: 0.5011233687400818 Training d_loss: 1.4914 Training g_loss: 0.6022 Training q_loss: 1.3106 Explore P: 0.9077\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 66.0 Average reward fake: 0.47302526235580444 Average reward real: 0.590764582157135 Training d_loss: 1.2134 Training g_loss: 0.7487 Training q_loss: 26.7946 Explore P: 0.9018\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 34.0 Average reward fake: 0.5019945502281189 Average reward real: 0.4707034230232239 Training d_loss: 1.4751 Training g_loss: 0.6896 Training q_loss: 3.3380 Explore P: 0.8987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 20.0 Average reward fake: 0.4991037845611572 Average reward real: 0.4884864389896393 Training d_loss: 1.4221 Training g_loss: 0.6952 Training q_loss: 21.6797 Explore P: 0.8970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 15.0 Average reward fake: 0.49897733330726624 Average reward real: 0.4853590428829193 Training d_loss: 1.4225 Training g_loss: 0.6952 Training q_loss: 1.1669 Explore P: 0.8956\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 32.0 Average reward fake: 0.49342623353004456 Average reward real: 0.47624140977859497 Training d_loss: 1.4355 Training g_loss: 0.7065 Training q_loss: 28.0603 Explore P: 0.8928\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 68.0 Average reward fake: 0.46498551964759827 Average reward real: 0.4688801169395447 Training d_loss: 1.3968 Training g_loss: 0.7662 Training q_loss: 71.7124 Explore P: 0.8868\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 39.0 Average reward fake: 0.4964748024940491 Average reward real: 0.48539265990257263 Training d_loss: 1.4172 Training g_loss: 0.7003 Training q_loss: 1.2019 Explore P: 0.8834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 37.0 Average reward fake: 0.48435935378074646 Average reward real: 0.4469391703605652 Training d_loss: 1.4725 Training g_loss: 0.7249 Training q_loss: 1.9789 Explore P: 0.8802\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 15.0 Average reward fake: 0.45487862825393677 Average reward real: 0.4728444218635559 Training d_loss: 1.3570 Training g_loss: 0.7877 Training q_loss: 1.0351 Explore P: 0.8789\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 10.0 Average reward fake: 0.44109010696411133 Average reward real: 0.5207235217094421 Training d_loss: 1.2381 Training g_loss: 0.8188 Training q_loss: 1.3877 Explore P: 0.8780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 16.0 Average reward fake: 0.425114244222641 Average reward real: 0.5264788866043091 Training d_loss: 1.2049 Training g_loss: 0.8558 Training q_loss: 2.8965 Explore P: 0.8766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 89.0 Average reward fake: 0.5250399112701416 Average reward real: 0.5129412412643433 Training d_loss: 1.4236 Training g_loss: 0.6446 Training q_loss: 4.5387 Explore P: 0.8689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 23.0 Average reward fake: 0.5370593070983887 Average reward real: 0.504940390586853 Training d_loss: 1.4611 Training g_loss: 0.6221 Training q_loss: 2.5413 Explore P: 0.8670\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 8.0 Average reward fake: 0.5182740092277527 Average reward real: 0.49242424964904785 Training d_loss: 1.4450 Training g_loss: 0.6582 Training q_loss: 64.2558 Explore P: 0.8663\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 26.0 Average reward fake: 0.553573727607727 Average reward real: 0.5265594720840454 Training d_loss: 1.4594 Training g_loss: 0.5910 Training q_loss: 71.6271 Explore P: 0.8641\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 18.0 Average reward fake: 0.5341500639915466 Average reward real: 0.49605512619018555 Training d_loss: 1.4718 Training g_loss: 0.6274 Training q_loss: 32.0070 Explore P: 0.8625\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 28.0 Average reward fake: 0.5098902583122253 Average reward real: 0.5670927166938782 Training d_loss: 1.2845 Training g_loss: 0.6737 Training q_loss: 0.5878 Explore P: 0.8601\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 16.0 Average reward fake: 0.46226805448532104 Average reward real: 0.6319223642349243 Training d_loss: 1.0864 Training g_loss: 0.7719 Training q_loss: 9.4975 Explore P: 0.8588\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 13.0 Average reward fake: 0.44895458221435547 Average reward real: 0.6330411434173584 Training d_loss: 1.0706 Training g_loss: 0.8052 Training q_loss: 74.6742 Explore P: 0.8577\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 47.0 Average reward fake: 0.5769604444503784 Average reward real: 0.4857664704322815 Training d_loss: 1.6189 Training g_loss: 0.5498 Training q_loss: 0.9320 Explore P: 0.8537\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 27.0 Average reward fake: 0.4848254323005676 Average reward real: 0.49169501662254333 Training d_loss: 1.3807 Training g_loss: 0.7248 Training q_loss: 23.8517 Explore P: 0.8514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 53.0 Average reward fake: 0.48233556747436523 Average reward real: 0.4679953455924988 Training d_loss: 1.4548 Training g_loss: 0.7294 Training q_loss: 3.2120 Explore P: 0.8470\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 13.0 Average reward fake: 0.4505029320716858 Average reward real: 0.40119680762290955 Training d_loss: 1.5433 Training g_loss: 0.7977 Training q_loss: 79.4984 Explore P: 0.8459\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 42.0 Average reward fake: 0.33490636944770813 Average reward real: 0.46473565697669983 Training d_loss: 1.1767 Training g_loss: 1.0939 Training q_loss: 42.3056 Explore P: 0.8424\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 47.0 Average reward fake: 0.3467292785644531 Average reward real: 0.6398218274116516 Training d_loss: 0.9097 Training g_loss: 1.0634 Training q_loss: 35.3791 Explore P: 0.8385\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 24.0 Average reward fake: 0.47616061568260193 Average reward real: 0.4433491826057434 Training d_loss: 1.5492 Training g_loss: 0.7421 Training q_loss: 3.7546 Explore P: 0.8365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 56.0 Average reward fake: 0.5192966461181641 Average reward real: 0.5291343331336975 Training d_loss: 1.3879 Training g_loss: 0.6560 Training q_loss: 126.9386 Explore P: 0.8319\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 16.0 Average reward fake: 0.5696004033088684 Average reward real: 0.5663400292396545 Training d_loss: 1.4410 Training g_loss: 0.5632 Training q_loss: 2.2047 Explore P: 0.8306\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 66.0 Average reward fake: 0.4831223487854004 Average reward real: 0.5797877311706543 Training d_loss: 1.2781 Training g_loss: 0.7280 Training q_loss: 2.5555 Explore P: 0.8252\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 27.0 Average reward fake: 0.46611881256103516 Average reward real: 0.5272071361541748 Training d_loss: 1.3146 Training g_loss: 0.7634 Training q_loss: 2.0220 Explore P: 0.8230\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 10.0 Average reward fake: 0.435553640127182 Average reward real: 0.5306944251060486 Training d_loss: 1.2450 Training g_loss: 0.8323 Training q_loss: 30.7099 Explore P: 0.8222\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 42.0 Average reward fake: 0.43351054191589355 Average reward real: 0.3598038852214813 Training d_loss: 1.7034 Training g_loss: 0.8384 Training q_loss: 45.9499 Explore P: 0.8188\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 25.0 Average reward fake: 0.4975220263004303 Average reward real: 0.32766515016555786 Training d_loss: 1.8443 Training g_loss: 0.6983 Training q_loss: 4.4551 Explore P: 0.8167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 48.0 Average reward fake: 0.3719314634799957 Average reward real: 0.6375300884246826 Training d_loss: 0.9386 Training g_loss: 0.9901 Training q_loss: 1.9860 Explore P: 0.8129\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 19.0 Average reward fake: 0.4494243264198303 Average reward real: 0.5841799974441528 Training d_loss: 1.2060 Training g_loss: 0.8036 Training q_loss: 89.9288 Explore P: 0.8114\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 24.0 Average reward fake: 0.5354925394058228 Average reward real: 0.48185300827026367 Training d_loss: 1.5367 Training g_loss: 0.6245 Training q_loss: 54.4594 Explore P: 0.8094\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 13.0 Average reward fake: 0.5122162699699402 Average reward real: 0.48031458258628845 Training d_loss: 1.4950 Training g_loss: 0.6681 Training q_loss: 2.7539 Explore P: 0.8084\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 33.0 Average reward fake: 0.5798909068107605 Average reward real: 0.5047488808631897 Training d_loss: 1.5603 Training g_loss: 0.5450 Training q_loss: 4.8674 Explore P: 0.8058\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 15.0 Average reward fake: 0.5369076728820801 Average reward real: 0.5605852007865906 Training d_loss: 1.3539 Training g_loss: 0.6216 Training q_loss: 1.6494 Explore P: 0.8046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 18.0 Average reward fake: 0.4660569131374359 Average reward real: 0.6051689386367798 Training d_loss: 1.1373 Training g_loss: 0.7644 Training q_loss: 61.7566 Explore P: 0.8031\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 24.0 Average reward fake: 0.39320698380470276 Average reward real: 0.5696187615394592 Training d_loss: 1.1400 Training g_loss: 0.9391 Training q_loss: 10.8302 Explore P: 0.8012\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 17.0 Average reward fake: 0.43830862641334534 Average reward real: 0.49156028032302856 Training d_loss: 1.3563 Training g_loss: 0.8248 Training q_loss: 8.9079 Explore P: 0.7999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 12.0 Average reward fake: 0.44955530762672424 Average reward real: 0.38335293531417847 Training d_loss: 1.6158 Training g_loss: 0.8008 Training q_loss: 71.9798 Explore P: 0.7990\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 17.0 Average reward fake: 0.5035148859024048 Average reward real: 0.4302602708339691 Training d_loss: 1.5559 Training g_loss: 0.6865 Training q_loss: 1.8025 Explore P: 0.7976\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 16.0 Average reward fake: 0.4860394597053528 Average reward real: 0.4961718022823334 Training d_loss: 1.3734 Training g_loss: 0.7216 Training q_loss: 65.0346 Explore P: 0.7964\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 15.0 Average reward fake: 0.41691452264785767 Average reward real: 0.5675745606422424 Training d_loss: 1.1282 Training g_loss: 0.8753 Training q_loss: 36.5426 Explore P: 0.7952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 38.0 Average reward fake: 0.431433767080307 Average reward real: 0.5943601727485657 Training d_loss: 1.1500 Training g_loss: 0.8453 Training q_loss: 1.2835 Explore P: 0.7922\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 8.0 Average reward fake: 0.4405662417411804 Average reward real: 0.5908398032188416 Training d_loss: 1.1577 Training g_loss: 0.8224 Training q_loss: 6.8373 Explore P: 0.7916\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 30.0 Average reward fake: 0.5521664023399353 Average reward real: 0.47201603651046753 Training d_loss: 1.5735 Training g_loss: 0.5944 Training q_loss: 5.6368 Explore P: 0.7892\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 9.0 Average reward fake: 0.53272545337677 Average reward real: 0.463190495967865 Training d_loss: 1.5438 Training g_loss: 0.6311 Training q_loss: 35.4659 Explore P: 0.7885\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 22.0 Average reward fake: 0.4642702639102936 Average reward real: 0.48292428255081177 Training d_loss: 1.3600 Training g_loss: 0.7674 Training q_loss: 39.0383 Explore P: 0.7868\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 53.0 Average reward fake: 0.45724591612815857 Average reward real: 0.5524178743362427 Training d_loss: 1.2333 Training g_loss: 0.7853 Training q_loss: 7.0397 Explore P: 0.7827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 31.0 Average reward fake: 0.4674568176269531 Average reward real: 0.451896607875824 Training d_loss: 1.4461 Training g_loss: 0.7601 Training q_loss: 210.4222 Explore P: 0.7803\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 36.0 Average reward fake: 0.44616371393203735 Average reward real: 0.4991481900215149 Training d_loss: 1.2928 Training g_loss: 0.8080 Training q_loss: 3.7153 Explore P: 0.7776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 12.0 Average reward fake: 0.46711039543151855 Average reward real: 0.5492838621139526 Training d_loss: 1.2358 Training g_loss: 0.7616 Training q_loss: 4.1900 Explore P: 0.7766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 35.0 Average reward fake: 0.44393453001976013 Average reward real: 0.5795724987983704 Training d_loss: 1.1468 Training g_loss: 0.8123 Training q_loss: 4.3017 Explore P: 0.7740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 108.0 Average reward fake: 0.5841895937919617 Average reward real: 0.5466802716255188 Training d_loss: 1.6698 Training g_loss: 0.5836 Training q_loss: 3.6058 Explore P: 0.7657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 14.0 Average reward fake: 0.47766929864883423 Average reward real: 0.6513541340827942 Training d_loss: 1.2849 Training g_loss: 0.9049 Training q_loss: 1.4729 Explore P: 0.7647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 41.0 Average reward fake: 0.4356080889701843 Average reward real: 0.4437568187713623 Training d_loss: 1.4035 Training g_loss: 0.8313 Training q_loss: 2.4890 Explore P: 0.7616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 29.0 Average reward fake: 0.48320603370666504 Average reward real: 0.5883100628852844 Training d_loss: 1.2424 Training g_loss: 0.7506 Training q_loss: 3.6318 Explore P: 0.7594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 52.0 Average reward fake: 0.5081766843795776 Average reward real: 0.5035048127174377 Training d_loss: 1.3989 Training g_loss: 0.6853 Training q_loss: 1.5316 Explore P: 0.7555\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 42.0 Average reward fake: 0.4939200282096863 Average reward real: 0.4880916476249695 Training d_loss: 1.4229 Training g_loss: 0.7058 Training q_loss: 5.3528 Explore P: 0.7524\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 25.0 Average reward fake: 0.497200071811676 Average reward real: 0.5119742155075073 Training d_loss: 1.3612 Training g_loss: 0.6995 Training q_loss: 2.7650 Explore P: 0.7506\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 22.0 Average reward fake: 0.5115573406219482 Average reward real: 0.5001780390739441 Training d_loss: 1.4230 Training g_loss: 0.6717 Training q_loss: 2.0632 Explore P: 0.7489\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 20.0 Average reward fake: 0.512348473072052 Average reward real: 0.4688819944858551 Training d_loss: 1.4830 Training g_loss: 0.6692 Training q_loss: 2.7082 Explore P: 0.7475\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 13.0 Average reward fake: 0.5502066016197205 Average reward real: 0.5034774541854858 Training d_loss: 1.4893 Training g_loss: 0.5974 Training q_loss: 2.4071 Explore P: 0.7465\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 26.0 Average reward fake: 0.5103080868721008 Average reward real: 0.5178337693214417 Training d_loss: 1.3764 Training g_loss: 0.6734 Training q_loss: 49.1195 Explore P: 0.7446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 58.0 Average reward fake: 0.4891045093536377 Average reward real: 0.5093219876289368 Training d_loss: 1.3541 Training g_loss: 0.7152 Training q_loss: 2.9515 Explore P: 0.7403\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 25.0 Average reward fake: 0.522598147392273 Average reward real: 0.4656919836997986 Training d_loss: 1.5116 Training g_loss: 0.6491 Training q_loss: 3.8739 Explore P: 0.7385\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 72.0 Average reward fake: 0.4792719781398773 Average reward real: 0.5107067823410034 Training d_loss: 1.3270 Training g_loss: 0.7356 Training q_loss: 2.1484 Explore P: 0.7333\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 26.0 Average reward fake: 0.48023462295532227 Average reward real: 0.5258031487464905 Training d_loss: 1.3011 Training g_loss: 0.7336 Training q_loss: 4.3667 Explore P: 0.7314\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 27.0 Average reward fake: 0.4866122603416443 Average reward real: 0.5103556513786316 Training d_loss: 1.3448 Training g_loss: 0.7202 Training q_loss: 5.4455 Explore P: 0.7295\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 78.0 Average reward fake: 0.45589679479599 Average reward real: 0.5214316844940186 Training d_loss: 1.2682 Training g_loss: 0.7884 Training q_loss: 5.0775 Explore P: 0.7239\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 71.0 Average reward fake: 0.4771153926849365 Average reward real: 0.4720524251461029 Training d_loss: 1.4059 Training g_loss: 0.7453 Training q_loss: 47.3040 Explore P: 0.7188\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 32.0 Average reward fake: 0.48483985662460327 Average reward real: 0.5325549840927124 Training d_loss: 1.3115 Training g_loss: 0.7279 Training q_loss: 15.0159 Explore P: 0.7166\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 81.0 Average reward fake: 0.48272591829299927 Average reward real: 0.4944733679294586 Training d_loss: 1.3839 Training g_loss: 0.7291 Training q_loss: 5.6609 Explore P: 0.7109\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 42.0 Average reward fake: 0.507536768913269 Average reward real: 0.4906488358974457 Training d_loss: 1.4525 Training g_loss: 0.6811 Training q_loss: 105.8384 Explore P: 0.7079\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 43.0 Average reward fake: 0.49426835775375366 Average reward real: 0.47906923294067383 Training d_loss: 1.4349 Training g_loss: 0.7079 Training q_loss: 7.4974 Explore P: 0.7049\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 36.0 Average reward fake: 0.4148470461368561 Average reward real: 0.5356243252754211 Training d_loss: 1.1736 Training g_loss: 0.8819 Training q_loss: 8.1919 Explore P: 0.7024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 47.0 Average reward fake: 0.5334467887878418 Average reward real: 0.4889729619026184 Training d_loss: 1.5084 Training g_loss: 0.6345 Training q_loss: 140.7968 Explore P: 0.6992\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 39.0 Average reward fake: 0.46944373846054077 Average reward real: 0.5252028703689575 Training d_loss: 1.2886 Training g_loss: 0.7586 Training q_loss: 4.3599 Explore P: 0.6965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 50.0 Average reward fake: 0.5047990679740906 Average reward real: 0.4438880383968353 Training d_loss: 1.5317 Training g_loss: 0.6851 Training q_loss: 4.3362 Explore P: 0.6931\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 45.0 Average reward fake: 0.5004502534866333 Average reward real: 0.5526870489120483 Training d_loss: 1.2966 Training g_loss: 0.6933 Training q_loss: 5.4434 Explore P: 0.6900\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 41.0 Average reward fake: 0.48246678709983826 Average reward real: 0.5297679901123047 Training d_loss: 1.3100 Training g_loss: 0.7309 Training q_loss: 506.6120 Explore P: 0.6872\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 22.0 Average reward fake: 0.4881995618343353 Average reward real: 0.5199181437492371 Training d_loss: 1.3342 Training g_loss: 0.7178 Training q_loss: 3.5536 Explore P: 0.6857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 35.0 Average reward fake: 0.46494752168655396 Average reward real: 0.47772878408432007 Training d_loss: 1.3827 Training g_loss: 0.7640 Training q_loss: 199.9521 Explore P: 0.6834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 21.0 Average reward fake: 0.5232125520706177 Average reward real: 0.4396258294582367 Training d_loss: 1.5851 Training g_loss: 0.6497 Training q_loss: 183.2461 Explore P: 0.6820\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 61.0 Average reward fake: 0.38744935393333435 Average reward real: 0.5065024495124817 Training d_loss: 1.2017 Training g_loss: 0.9760 Training q_loss: 7.6899 Explore P: 0.6779\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 18.0 Average reward fake: 0.44183477759361267 Average reward real: 0.48185890913009644 Training d_loss: 1.3348 Training g_loss: 0.8196 Training q_loss: 20.3160 Explore P: 0.6767\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 57.0 Average reward fake: 0.6570660471916199 Average reward real: 0.5095509886741638 Training d_loss: 1.8758 Training g_loss: 0.4388 Training q_loss: 59.5012 Explore P: 0.6729\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 22.0 Average reward fake: 0.6338825225830078 Average reward real: 0.47021859884262085 Training d_loss: 1.7695 Training g_loss: 0.4554 Training q_loss: 4.6976 Explore P: 0.6714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 65.0 Average reward fake: 0.4753468930721283 Average reward real: 0.5308297276496887 Training d_loss: 1.3020 Training g_loss: 0.7454 Training q_loss: 8.0092 Explore P: 0.6671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 10.0 Average reward fake: 0.4502216875553131 Average reward real: 0.515333890914917 Training d_loss: 1.2741 Training g_loss: 0.7982 Training q_loss: 3.0857 Explore P: 0.6665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 128.0 Average reward fake: 0.5053759813308716 Average reward real: 0.4531148374080658 Training d_loss: 1.5524 Training g_loss: 0.6925 Training q_loss: 4.0651 Explore P: 0.6581\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 28.0 Average reward fake: 0.43321317434310913 Average reward real: 0.4007095694541931 Training d_loss: 1.4910 Training g_loss: 0.8429 Training q_loss: 2.5781 Explore P: 0.6563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 36.0 Average reward fake: 0.4063851833343506 Average reward real: 0.5218998789787292 Training d_loss: 1.2146 Training g_loss: 0.9089 Training q_loss: 7.8403 Explore P: 0.6540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 40.0 Average reward fake: 0.4323139190673828 Average reward real: 0.4886096119880676 Training d_loss: 1.2989 Training g_loss: 0.8421 Training q_loss: 10.5864 Explore P: 0.6514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 45.0 Average reward fake: 0.39777567982673645 Average reward real: 0.5236603021621704 Training d_loss: 1.2015 Training g_loss: 0.9651 Training q_loss: 46.2939 Explore P: 0.6486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 90.0 Average reward fake: 0.596526026725769 Average reward real: 0.5605939626693726 Training d_loss: 1.5848 Training g_loss: 0.5473 Training q_loss: 585.3061 Explore P: 0.6428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 56.0 Average reward fake: 0.3958578407764435 Average reward real: 0.5956171751022339 Training d_loss: 1.0650 Training g_loss: 0.9562 Training q_loss: 4.5599 Explore P: 0.6393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 30.0 Average reward fake: 0.5229454040527344 Average reward real: 0.5321453809738159 Training d_loss: 1.4813 Training g_loss: 0.6726 Training q_loss: 11.5276 Explore P: 0.6374\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 35.0 Average reward fake: 0.4999691843986511 Average reward real: 0.45256662368774414 Training d_loss: 1.4935 Training g_loss: 0.6928 Training q_loss: 4.8524 Explore P: 0.6352\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 55.0 Average reward fake: 0.3820490837097168 Average reward real: 0.48517388105392456 Training d_loss: 1.2664 Training g_loss: 0.9728 Training q_loss: 5.4377 Explore P: 0.6318\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 76.0 Average reward fake: 0.3800409436225891 Average reward real: 0.584040641784668 Training d_loss: 1.0520 Training g_loss: 0.9719 Training q_loss: 12.4338 Explore P: 0.6271\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 118.0 Average reward fake: 0.5276260375976562 Average reward real: 0.523840606212616 Training d_loss: 1.5160 Training g_loss: 0.6409 Training q_loss: 15.1710 Explore P: 0.6198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 109.0 Average reward fake: 0.479830265045166 Average reward real: 0.44918546080589294 Training d_loss: 1.5499 Training g_loss: 0.7361 Training q_loss: 10.3130 Explore P: 0.6132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 100.0 Average reward fake: 0.40110987424850464 Average reward real: 0.5122677683830261 Training d_loss: 1.1873 Training g_loss: 0.9134 Training q_loss: 291.6717 Explore P: 0.6072\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 34.0 Average reward fake: 0.34063291549682617 Average reward real: 0.7837051153182983 Training d_loss: 0.6869 Training g_loss: 1.0773 Training q_loss: 18.7085 Explore P: 0.6052\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 45.0 Average reward fake: 0.5796505808830261 Average reward real: 0.39793717861175537 Training d_loss: 1.8488 Training g_loss: 0.5479 Training q_loss: 17.8421 Explore P: 0.6025\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 142.0 Average reward fake: 0.48729491233825684 Average reward real: 0.4822656214237213 Training d_loss: 1.4347 Training g_loss: 0.7193 Training q_loss: 14.4068 Explore P: 0.5942\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 10.0 Average reward fake: 0.5024865865707397 Average reward real: 0.4716780185699463 Training d_loss: 1.4881 Training g_loss: 0.6889 Training q_loss: 9.6570 Explore P: 0.5936\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 54.0 Average reward fake: 0.5045982003211975 Average reward real: 0.45322394371032715 Training d_loss: 1.5274 Training g_loss: 0.6846 Training q_loss: 574.8917 Explore P: 0.5905\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 28.0 Average reward fake: 0.4031727910041809 Average reward real: 0.5146746039390564 Training d_loss: 1.2066 Training g_loss: 0.9085 Training q_loss: 881.7775 Explore P: 0.5888\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 81.0 Average reward fake: 0.45656663179397583 Average reward real: 0.5301281809806824 Training d_loss: 1.2690 Training g_loss: 0.7824 Training q_loss: 55.9591 Explore P: 0.5842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 31.0 Average reward fake: 0.5461193323135376 Average reward real: 0.4480660557746887 Training d_loss: 1.6279 Training g_loss: 0.6059 Training q_loss: 13.2994 Explore P: 0.5824\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 173.0 Average reward fake: 0.484448105096817 Average reward real: 0.5711666345596313 Training d_loss: 1.2480 Training g_loss: 0.7258 Training q_loss: 26.2032 Explore P: 0.5726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 47.0 Average reward fake: 0.4183713495731354 Average reward real: 0.5137882828712463 Training d_loss: 1.2813 Training g_loss: 0.8716 Training q_loss: 11.4539 Explore P: 0.5699\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 96.0 Average reward fake: 0.47379645705223083 Average reward real: 0.5116134881973267 Training d_loss: 1.3749 Training g_loss: 0.7503 Training q_loss: 59.6594 Explore P: 0.5646\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 13.0 Average reward fake: 0.46721845865249634 Average reward real: 0.5404264330863953 Training d_loss: 1.3338 Training g_loss: 0.7626 Training q_loss: 10.3398 Explore P: 0.5639\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 78.0 Average reward fake: 0.47948843240737915 Average reward real: 0.5734601020812988 Training d_loss: 1.2233 Training g_loss: 0.7350 Training q_loss: 33.8147 Explore P: 0.5596\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 76.0 Average reward fake: 0.423738956451416 Average reward real: 0.47088584303855896 Training d_loss: 1.2993 Training g_loss: 0.8586 Training q_loss: 38.0909 Explore P: 0.5554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 177.0 Average reward fake: 0.5015460252761841 Average reward real: 0.56107497215271 Training d_loss: 1.2792 Training g_loss: 0.6903 Training q_loss: 8.6403 Explore P: 0.5458\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 118.0 Average reward fake: 0.446766197681427 Average reward real: 0.5458778142929077 Training d_loss: 1.2185 Training g_loss: 0.8061 Training q_loss: 287.0792 Explore P: 0.5395\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 113.0 Average reward fake: 0.4562538266181946 Average reward real: 0.5443472266197205 Training d_loss: 1.2365 Training g_loss: 0.7866 Training q_loss: 91.5103 Explore P: 0.5336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 134.0 Average reward fake: 0.5799983143806458 Average reward real: 0.4963953495025635 Training d_loss: 1.5843 Training g_loss: 0.5468 Training q_loss: 511.7480 Explore P: 0.5266\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 74.0 Average reward fake: 0.46599847078323364 Average reward real: 0.5494377613067627 Training d_loss: 1.2388 Training g_loss: 0.7668 Training q_loss: 99.2214 Explore P: 0.5228\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 58.0 Average reward fake: 0.5251671671867371 Average reward real: 0.4986805319786072 Training d_loss: 1.4647 Training g_loss: 0.6474 Training q_loss: 93.9150 Explore P: 0.5198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 133.0 Average reward fake: 0.475419282913208 Average reward real: 0.49197134375572205 Training d_loss: 1.3734 Training g_loss: 0.7444 Training q_loss: 32.6604 Explore P: 0.5131\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 78.0 Average reward fake: 0.4656763970851898 Average reward real: 0.5282794833183289 Training d_loss: 1.2731 Training g_loss: 0.7643 Training q_loss: 34.0517 Explore P: 0.5092\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "d_loss_list, g_loss_list, q_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        d_loss, g_loss, q_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions}\n",
    "            rewards_fake, rewards_real = sess.run([model.rewards_fake, model.rewards_real], feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions, \n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/Q-GAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 5\n",
    "test_max_steps = 2000\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/Q-GAN-cartpole.ckpt')    \n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
