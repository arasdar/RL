{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QGAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    #     print('state, action, reward, done, info')\n",
    "    #     print(state, action, reward, done, info)\n",
    "    if done:\n",
    "    #         print('state, action, reward, done, info')\n",
    "    #         print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Next states given\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Current actions given\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder/Classifier\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits: Sqeezed/compressed/represented states into actions size\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(states, actions, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Fuse compressed states (actions fake) with actions (actions real)\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions]) # NxD: axis1=N, and axis2=D\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, actions, next_states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=next_states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=action_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Fused compressed states, actions, and compressed next_states (all three in action size)\n",
    "        #h3 = tf.layers.dense(inputs=nl2, units=action_size)\n",
    "        h3_fused = tf.concat(axis=1, values=[states, actions, nl2])\n",
    "        bn3 = tf.layers.batch_normalization(h3_fused, training=training)        \n",
    "        nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl3, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, next_states, targetQs, # model_input\n",
    "               state_size, action_size, hidden_size): # model_init\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(indices=actions, depth=action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_logits = generator(states=actions_fake, actions=actions_real, \n",
    "                                   state_size=state_size, hidden_size=hidden_size)\n",
    "    \n",
    "    # GAN: Discriminate between fake and real\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(states=actions_fake, actions=actions_real, action_size=action_size,\n",
    "                                  next_states=next_states_fake, hidden_size=hidden_size, reuse=False)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(states=actions_fake, actions=actions_real, action_size=action_size,\n",
    "                                  next_states=next_states_real, hidden_size=hidden_size, reuse=True)    \n",
    "\n",
    "    # GAN: Adverserial training - G-learning -  Relavistic GAN\n",
    "    g_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "    g_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.zeros_like(d_logits_real)))\n",
    "    g_loss = g_loss_real + g_loss_fake\n",
    "    \n",
    "    # VAE: Variational AE reconstruction/prediction loss\n",
    "    loss_reconst = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, labels=next_states_real))\n",
    "    q_loss += g_loss + loss_reconst \n",
    "    g_loss += loss_reconst\n",
    "    \n",
    "    # GAN: Adverserial training - D-learning-  Standard GAN\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, g_loss, d_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.g_loss, self.d_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, next_states=self.next_states, actions=self.actions, targetQs=self.targetQs) # model input data\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss, g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                                       learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = QGAN(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 33.0 Average reward fake: 0.19600506126880646 Average reward real: 0.1960034817457199 Training q_loss: 2.6797 Training g_loss: 2.5373 Training d_loss: 1.8479 Explore P: 0.9967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 41.0 Average reward fake: 0.24042381346225739 Average reward real: 0.24041762948036194 Training q_loss: 4.5685 Training g_loss: 2.3783 Training d_loss: 1.7004 Explore P: 0.9927\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 14.0 Average reward fake: 0.25487691164016724 Average reward real: 0.25488054752349854 Training q_loss: 6.5446 Training g_loss: 2.3374 Training d_loss: 1.6611 Explore P: 0.9913\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 27.0 Average reward fake: 0.2840881645679474 Average reward real: 0.28413480520248413 Training q_loss: 9.3247 Training g_loss: 2.2767 Training d_loss: 1.5926 Explore P: 0.9887\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 22.0 Average reward fake: 0.3076390027999878 Average reward real: 0.3076241612434387 Training q_loss: 4.7606 Training g_loss: 2.2276 Training d_loss: 1.5465 Explore P: 0.9865\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 26.0 Average reward fake: 0.3352756202220917 Average reward real: 0.3351287543773651 Training q_loss: 5.2320 Training g_loss: 2.1802 Training d_loss: 1.5017 Explore P: 0.9840\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 13.0 Average reward fake: 0.34919998049736023 Average reward real: 0.3489571511745453 Training q_loss: 6.9617 Training g_loss: 2.1596 Training d_loss: 1.4824 Explore P: 0.9827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 13.0 Average reward fake: 0.36367806792259216 Average reward real: 0.3631771206855774 Training q_loss: 7.5959 Training g_loss: 2.1462 Training d_loss: 1.4649 Explore P: 0.9815\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 28.0 Average reward fake: 0.394459992647171 Average reward real: 0.39346274733543396 Training q_loss: 6.5974 Training g_loss: 2.1130 Training d_loss: 1.4350 Explore P: 0.9787\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 45.0 Average reward fake: 0.44512131810188293 Average reward real: 0.4417616128921509 Training q_loss: 10.9911 Training g_loss: 2.0823 Training d_loss: 1.4060 Explore P: 0.9744\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 30.0 Average reward fake: 0.4735363721847534 Average reward real: 0.4697447121143341 Training q_loss: 13.5838 Training g_loss: 2.0700 Training d_loss: 1.3977 Explore P: 0.9715\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 23.0 Average reward fake: 0.48642870783805847 Average reward real: 0.48078155517578125 Training q_loss: 2.5871 Training g_loss: 2.0713 Training d_loss: 1.3987 Explore P: 0.9693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 16.0 Average reward fake: 0.4889974296092987 Average reward real: 0.484275221824646 Training q_loss: 8.4431 Training g_loss: 2.0678 Training d_loss: 1.3973 Explore P: 0.9678\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 47.0 Average reward fake: 0.49063530564308167 Average reward real: 0.4868054986000061 Training q_loss: 3.4191 Training g_loss: 2.0716 Training d_loss: 1.3946 Explore P: 0.9633\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 19.0 Average reward fake: 0.4907713830471039 Average reward real: 0.4882092773914337 Training q_loss: 8.8934 Training g_loss: 2.0820 Training d_loss: 1.3908 Explore P: 0.9615\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 12.0 Average reward fake: 0.4924851357936859 Average reward real: 0.48842671513557434 Training q_loss: 6.2844 Training g_loss: 2.0728 Training d_loss: 1.3947 Explore P: 0.9603\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 15.0 Average reward fake: 0.4938274323940277 Average reward real: 0.4897274672985077 Training q_loss: 15.8140 Training g_loss: 2.0744 Training d_loss: 1.3946 Explore P: 0.9589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 16.0 Average reward fake: 0.4940636157989502 Average reward real: 0.4898220896720886 Training q_loss: 3.2570 Training g_loss: 2.0731 Training d_loss: 1.3950 Explore P: 0.9574\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 21.0 Average reward fake: 0.4932197630405426 Average reward real: 0.4900200366973877 Training q_loss: 5.6823 Training g_loss: 2.0741 Training d_loss: 1.3930 Explore P: 0.9554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 19.0 Average reward fake: 0.49408936500549316 Average reward real: 0.49090680480003357 Training q_loss: 6.3894 Training g_loss: 2.0761 Training d_loss: 1.3932 Explore P: 0.9536\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 39.0 Average reward fake: 0.49399352073669434 Average reward real: 0.49157726764678955 Training q_loss: 7.3672 Training g_loss: 2.0749 Training d_loss: 1.3914 Explore P: 0.9499\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 11.0 Average reward fake: 0.49395951628685 Average reward real: 0.4920523762702942 Training q_loss: 16.3409 Training g_loss: 2.0756 Training d_loss: 1.3904 Explore P: 0.9489\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 10.0 Average reward fake: 0.49436190724372864 Average reward real: 0.4915718138217926 Training q_loss: 9.5808 Training g_loss: 2.0729 Training d_loss: 1.3922 Explore P: 0.9480\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 22.0 Average reward fake: 0.4949415624141693 Average reward real: 0.49277472496032715 Training q_loss: 9.4793 Training g_loss: 2.0744 Training d_loss: 1.3908 Explore P: 0.9459\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 14.0 Average reward fake: 0.4950315058231354 Average reward real: 0.49310654401779175 Training q_loss: 7.3082 Training g_loss: 2.0752 Training d_loss: 1.3903 Explore P: 0.9446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 16.0 Average reward fake: 0.49539700150489807 Average reward real: 0.4934948682785034 Training q_loss: 7.3157 Training g_loss: 2.0744 Training d_loss: 1.3902 Explore P: 0.9431\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 18.0 Average reward fake: 0.4956896901130676 Average reward real: 0.49392417073249817 Training q_loss: 8.2933 Training g_loss: 2.0718 Training d_loss: 1.3900 Explore P: 0.9414\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 18.0 Average reward fake: 0.4956466555595398 Average reward real: 0.4939665198326111 Training q_loss: 4.2470 Training g_loss: 2.0731 Training d_loss: 1.3897 Explore P: 0.9397\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 20.0 Average reward fake: 0.49574506282806396 Average reward real: 0.4942895472049713 Training q_loss: 8.7132 Training g_loss: 2.0714 Training d_loss: 1.3892 Explore P: 0.9379\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 16.0 Average reward fake: 0.4960569143295288 Average reward real: 0.4943714439868927 Training q_loss: 8.7140 Training g_loss: 2.0715 Training d_loss: 1.3897 Explore P: 0.9364\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 16.0 Average reward fake: 0.4964415729045868 Average reward real: 0.494974821805954 Training q_loss: 3.1066 Training g_loss: 2.0741 Training d_loss: 1.3893 Explore P: 0.9349\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 21.0 Average reward fake: 0.49633026123046875 Average reward real: 0.49526557326316833 Training q_loss: 4.0705 Training g_loss: 2.0705 Training d_loss: 1.3886 Explore P: 0.9330\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 17.0 Average reward fake: 0.49644336104393005 Average reward real: 0.49559229612350464 Training q_loss: 8.0338 Training g_loss: 2.0779 Training d_loss: 1.3879 Explore P: 0.9314\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 17.0 Average reward fake: 0.49705561995506287 Average reward real: 0.49608710408210754 Training q_loss: 8.2243 Training g_loss: 2.0740 Training d_loss: 1.3881 Explore P: 0.9298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 18.0 Average reward fake: 0.49701789021492004 Average reward real: 0.49615341424942017 Training q_loss: 6.1003 Training g_loss: 2.0766 Training d_loss: 1.3880 Explore P: 0.9282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 15.0 Average reward fake: 0.497030109167099 Average reward real: 0.4964621663093567 Training q_loss: 5.1660 Training g_loss: 2.0755 Training d_loss: 1.3875 Explore P: 0.9268\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 34.0 Average reward fake: 0.4975719451904297 Average reward real: 0.49670931696891785 Training q_loss: 3.4359 Training g_loss: 2.0799 Training d_loss: 1.3879 Explore P: 0.9237\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 22.0 Average reward fake: 0.49752968549728394 Average reward real: 0.49707698822021484 Training q_loss: 3.1785 Training g_loss: 2.0728 Training d_loss: 1.3873 Explore P: 0.9217\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 23.0 Average reward fake: 0.4978182315826416 Average reward real: 0.49735862016677856 Training q_loss: 4.2631 Training g_loss: 2.0720 Training d_loss: 1.3872 Explore P: 0.9196\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 17.0 Average reward fake: 0.49765339493751526 Average reward real: 0.49735748767852783 Training q_loss: 5.2735 Training g_loss: 2.0753 Training d_loss: 1.3869 Explore P: 0.9181\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 17.0 Average reward fake: 0.49774226546287537 Average reward real: 0.49769484996795654 Training q_loss: 4.6492 Training g_loss: 2.0742 Training d_loss: 1.3864 Explore P: 0.9165\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 16.0 Average reward fake: 0.49848517775535583 Average reward real: 0.4981187582015991 Training q_loss: 3.7322 Training g_loss: 2.0765 Training d_loss: 1.3870 Explore P: 0.9151\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 20.0 Average reward fake: 0.4985503852367401 Average reward real: 0.49857980012893677 Training q_loss: 3.8423 Training g_loss: 2.0770 Training d_loss: 1.3863 Explore P: 0.9133\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 13.0 Average reward fake: 0.4988298714160919 Average reward real: 0.4985438585281372 Training q_loss: 3.4110 Training g_loss: 2.0707 Training d_loss: 1.3869 Explore P: 0.9121\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 26.0 Average reward fake: 0.49818989634513855 Average reward real: 0.4981478750705719 Training q_loss: 4.4008 Training g_loss: 2.0749 Training d_loss: 1.3864 Explore P: 0.9097\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 12.0 Average reward fake: 0.498309850692749 Average reward real: 0.49865829944610596 Training q_loss: 4.5941 Training g_loss: 2.0732 Training d_loss: 1.3858 Explore P: 0.9087\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 25.0 Average reward fake: 0.4987887442111969 Average reward real: 0.4988235831260681 Training q_loss: 3.0473 Training g_loss: 2.0717 Training d_loss: 1.3863 Explore P: 0.9064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 12.0 Average reward fake: 0.4989415109157562 Average reward real: 0.49893224239349365 Training q_loss: 2.9987 Training g_loss: 2.0690 Training d_loss: 1.3864 Explore P: 0.9053\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 9.0 Average reward fake: 0.4985668957233429 Average reward real: 0.49882981181144714 Training q_loss: 3.9884 Training g_loss: 2.0752 Training d_loss: 1.3857 Explore P: 0.9045\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 20.0 Average reward fake: 0.49908608198165894 Average reward real: 0.4997040629386902 Training q_loss: 5.3445 Training g_loss: 2.0734 Training d_loss: 1.3851 Explore P: 0.9027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 24.0 Average reward fake: 0.49942007660865784 Average reward real: 0.4999349117279053 Training q_loss: 4.1626 Training g_loss: 2.0784 Training d_loss: 1.3854 Explore P: 0.9006\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 15.0 Average reward fake: 0.4993339478969574 Average reward real: 0.5001152753829956 Training q_loss: 2.6650 Training g_loss: 2.0765 Training d_loss: 1.3848 Explore P: 0.8993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 11.0 Average reward fake: 0.49974656105041504 Average reward real: 0.5002255439758301 Training q_loss: 3.7451 Training g_loss: 2.0783 Training d_loss: 1.3852 Explore P: 0.8983\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 12.0 Average reward fake: 0.5002912282943726 Average reward real: 0.5006381869316101 Training q_loss: 4.4230 Training g_loss: 2.0762 Training d_loss: 1.3856 Explore P: 0.8972\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 18.0 Average reward fake: 0.49961864948272705 Average reward real: 0.5000739097595215 Training q_loss: 3.2002 Training g_loss: 2.0698 Training d_loss: 1.3856 Explore P: 0.8956\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 23.0 Average reward fake: 0.49852049350738525 Average reward real: 0.49938011169433594 Training q_loss: 3.4996 Training g_loss: 2.0834 Training d_loss: 1.3845 Explore P: 0.8936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 31.0 Average reward fake: 0.500118613243103 Average reward real: 0.5008803009986877 Training q_loss: 4.3249 Training g_loss: 2.0823 Training d_loss: 1.3848 Explore P: 0.8909\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 23.0 Average reward fake: 0.4998432993888855 Average reward real: 0.5009036064147949 Training q_loss: 2.9567 Training g_loss: 2.0786 Training d_loss: 1.3842 Explore P: 0.8888\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 22.0 Average reward fake: 0.4992714822292328 Average reward real: 0.5008264780044556 Training q_loss: 2.9208 Training g_loss: 2.0783 Training d_loss: 1.3833 Explore P: 0.8869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 39.0 Average reward fake: 0.5005773901939392 Average reward real: 0.5020171999931335 Training q_loss: 3.4078 Training g_loss: 2.0798 Training d_loss: 1.3836 Explore P: 0.8835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 24.0 Average reward fake: 0.4992341697216034 Average reward real: 0.5011430978775024 Training q_loss: 3.2321 Training g_loss: 2.0803 Training d_loss: 1.3824 Explore P: 0.8814\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 14.0 Average reward fake: 0.4997430443763733 Average reward real: 0.5009739995002747 Training q_loss: 2.5062 Training g_loss: 2.0813 Training d_loss: 1.3839 Explore P: 0.8802\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 16.0 Average reward fake: 0.5005978345870972 Average reward real: 0.502236545085907 Training q_loss: 3.7075 Training g_loss: 2.0794 Training d_loss: 1.3832 Explore P: 0.8788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 15.0 Average reward fake: 0.5003342032432556 Average reward real: 0.5015580654144287 Training q_loss: 3.3910 Training g_loss: 2.0816 Training d_loss: 1.3839 Explore P: 0.8775\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 13.0 Average reward fake: 0.5003393292427063 Average reward real: 0.5024174451828003 Training q_loss: 2.6886 Training g_loss: 2.0803 Training d_loss: 1.3824 Explore P: 0.8764\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 14.0 Average reward fake: 0.5004714727401733 Average reward real: 0.5027637481689453 Training q_loss: 3.5571 Training g_loss: 2.0816 Training d_loss: 1.3819 Explore P: 0.8752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 11.0 Average reward fake: 0.5016863942146301 Average reward real: 0.5035369992256165 Training q_loss: 3.1673 Training g_loss: 2.0804 Training d_loss: 1.3829 Explore P: 0.8742\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 22.0 Average reward fake: 0.5019176006317139 Average reward real: 0.503365695476532 Training q_loss: 3.7027 Training g_loss: 2.0837 Training d_loss: 1.3834 Explore P: 0.8723\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 17.0 Average reward fake: 0.4991650879383087 Average reward real: 0.5012366771697998 Training q_loss: 2.7946 Training g_loss: 2.0806 Training d_loss: 1.3822 Explore P: 0.8708\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 13.0 Average reward fake: 0.4988137185573578 Average reward real: 0.49972784519195557 Training q_loss: 3.8863 Training g_loss: 2.0763 Training d_loss: 1.3849 Explore P: 0.8697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 9.0 Average reward fake: 0.5002654790878296 Average reward real: 0.5005820393562317 Training q_loss: 4.3321 Training g_loss: 2.0734 Training d_loss: 1.3859 Explore P: 0.8689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 21.0 Average reward fake: 0.49883323907852173 Average reward real: 0.500308096408844 Training q_loss: 2.8112 Training g_loss: 2.0740 Training d_loss: 1.3830 Explore P: 0.8671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 14.0 Average reward fake: 0.49890005588531494 Average reward real: 0.4994364082813263 Training q_loss: 3.9117 Training g_loss: 2.0704 Training d_loss: 1.3849 Explore P: 0.8659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 12.0 Average reward fake: 0.4993114769458771 Average reward real: 0.5001617670059204 Training q_loss: 4.0138 Training g_loss: 2.0841 Training d_loss: 1.3847 Explore P: 0.8649\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 14.0 Average reward fake: 0.49966201186180115 Average reward real: 0.500548243522644 Training q_loss: 2.6956 Training g_loss: 2.0709 Training d_loss: 1.3845 Explore P: 0.8637\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 85.0 Average reward fake: 0.4995020925998688 Average reward real: 0.5006340146064758 Training q_loss: 3.4800 Training g_loss: 2.0674 Training d_loss: 1.3841 Explore P: 0.8565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 25.0 Average reward fake: 0.49997222423553467 Average reward real: 0.5003560781478882 Training q_loss: 2.5370 Training g_loss: 2.0717 Training d_loss: 1.3855 Explore P: 0.8544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 16.0 Average reward fake: 0.5006899237632751 Average reward real: 0.5008353590965271 Training q_loss: 3.0603 Training g_loss: 2.0695 Training d_loss: 1.3863 Explore P: 0.8530\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 18.0 Average reward fake: 0.4993274211883545 Average reward real: 0.4990188479423523 Training q_loss: 3.8539 Training g_loss: 2.0671 Training d_loss: 1.3875 Explore P: 0.8515\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 10.0 Average reward fake: 0.4991152584552765 Average reward real: 0.4987637400627136 Training q_loss: 3.6819 Training g_loss: 2.0713 Training d_loss: 1.3872 Explore P: 0.8507\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 39.0 Average reward fake: 0.5003494024276733 Average reward real: 0.4999288320541382 Training q_loss: 4.0923 Training g_loss: 2.0673 Training d_loss: 1.3871 Explore P: 0.8474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 29.0 Average reward fake: 0.4994867742061615 Average reward real: 0.4995828866958618 Training q_loss: 3.2399 Training g_loss: 2.0709 Training d_loss: 1.3862 Explore P: 0.8450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 13.0 Average reward fake: 0.5004252791404724 Average reward real: 0.5000788569450378 Training q_loss: 3.5394 Training g_loss: 2.0662 Training d_loss: 1.3873 Explore P: 0.8439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 25.0 Average reward fake: 0.49892494082450867 Average reward real: 0.4987453818321228 Training q_loss: 2.5647 Training g_loss: 2.0700 Training d_loss: 1.3871 Explore P: 0.8418\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 19.0 Average reward fake: 0.500186026096344 Average reward real: 0.5005537867546082 Training q_loss: 3.7531 Training g_loss: 2.0628 Training d_loss: 1.3856 Explore P: 0.8402\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 24.0 Average reward fake: 0.4996268153190613 Average reward real: 0.5000849366188049 Training q_loss: 3.1565 Training g_loss: 2.0774 Training d_loss: 1.3849 Explore P: 0.8382\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 25.0 Average reward fake: 0.5012443661689758 Average reward real: 0.499898761510849 Training q_loss: 3.6387 Training g_loss: 2.0678 Training d_loss: 1.3882 Explore P: 0.8362\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 45.0 Average reward fake: 0.5002928376197815 Average reward real: 0.5005179047584534 Training q_loss: 2.9470 Training g_loss: 2.0621 Training d_loss: 1.3860 Explore P: 0.8325\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 24.0 Average reward fake: 0.49915534257888794 Average reward real: 0.49867674708366394 Training q_loss: 2.8419 Training g_loss: 2.0620 Training d_loss: 1.3871 Explore P: 0.8305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 10.0 Average reward fake: 0.4990953803062439 Average reward real: 0.49870413541793823 Training q_loss: 3.4857 Training g_loss: 2.0686 Training d_loss: 1.3872 Explore P: 0.8297\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 58.0 Average reward fake: 0.4999520480632782 Average reward real: 0.500231146812439 Training q_loss: 2.6979 Training g_loss: 2.0719 Training d_loss: 1.3854 Explore P: 0.8249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 17.0 Average reward fake: 0.5004006028175354 Average reward real: 0.5009170174598694 Training q_loss: 2.6928 Training g_loss: 2.0675 Training d_loss: 1.3853 Explore P: 0.8235\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 22.0 Average reward fake: 0.4997202157974243 Average reward real: 0.4991941750049591 Training q_loss: 4.6467 Training g_loss: 2.0631 Training d_loss: 1.3874 Explore P: 0.8218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 20.0 Average reward fake: 0.4994674026966095 Average reward real: 0.4996006488800049 Training q_loss: 2.9229 Training g_loss: 2.0643 Training d_loss: 1.3861 Explore P: 0.8201\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 56.0 Average reward fake: 0.49896126985549927 Average reward real: 0.4993942379951477 Training q_loss: 4.6402 Training g_loss: 2.0659 Training d_loss: 1.3854 Explore P: 0.8156\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 13.0 Average reward fake: 0.49993765354156494 Average reward real: 0.5001478791236877 Training q_loss: 3.6454 Training g_loss: 2.0687 Training d_loss: 1.3859 Explore P: 0.8146\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 78.0 Average reward fake: 0.500977098941803 Average reward real: 0.5014190077781677 Training q_loss: 3.0719 Training g_loss: 2.0663 Training d_loss: 1.3859 Explore P: 0.8083\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 20.0 Average reward fake: 0.49941346049308777 Average reward real: 0.4990832507610321 Training q_loss: 3.6089 Training g_loss: 2.0666 Training d_loss: 1.3869 Explore P: 0.8067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 55.0 Average reward fake: 0.5002642869949341 Average reward real: 0.5012457370758057 Training q_loss: 4.5042 Training g_loss: 2.0694 Training d_loss: 1.3844 Explore P: 0.8024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 63.0 Average reward fake: 0.5004953145980835 Average reward real: 0.5004808902740479 Training q_loss: 2.9060 Training g_loss: 2.0684 Training d_loss: 1.3865 Explore P: 0.7974\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 67.0 Average reward fake: 0.4997349679470062 Average reward real: 0.5002719163894653 Training q_loss: 2.4481 Training g_loss: 2.0817 Training d_loss: 1.3853 Explore P: 0.7921\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 11.0 Average reward fake: 0.5008094906806946 Average reward real: 0.5008547902107239 Training q_loss: 6.0016 Training g_loss: 2.0650 Training d_loss: 1.3862 Explore P: 0.7913\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 20.0 Average reward fake: 0.49863457679748535 Average reward real: 0.4988045394420624 Training q_loss: 3.0639 Training g_loss: 2.0688 Training d_loss: 1.3858 Explore P: 0.7897\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 93.0 Average reward fake: 0.5003818273544312 Average reward real: 0.5009841322898865 Training q_loss: 3.3420 Training g_loss: 2.0658 Training d_loss: 1.3851 Explore P: 0.7825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 17.0 Average reward fake: 0.5002573132514954 Average reward real: 0.501407265663147 Training q_loss: 3.3075 Training g_loss: 2.0674 Training d_loss: 1.3841 Explore P: 0.7812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 30.0 Average reward fake: 0.5020244717597961 Average reward real: 0.5007359981536865 Training q_loss: 3.0467 Training g_loss: 2.0635 Training d_loss: 1.3892 Explore P: 0.7789\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 48.0 Average reward fake: 0.5005859136581421 Average reward real: 0.5014011263847351 Training q_loss: 3.0315 Training g_loss: 2.0690 Training d_loss: 1.3850 Explore P: 0.7752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 54.0 Average reward fake: 0.4996299743652344 Average reward real: 0.4997760057449341 Training q_loss: 4.9213 Training g_loss: 2.0710 Training d_loss: 1.3859 Explore P: 0.7711\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 45.0 Average reward fake: 0.49984070658683777 Average reward real: 0.500505805015564 Training q_loss: 2.8126 Training g_loss: 2.0674 Training d_loss: 1.3851 Explore P: 0.7676\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 38.0 Average reward fake: 0.5011844635009766 Average reward real: 0.5002656579017639 Training q_loss: 3.4376 Training g_loss: 2.0686 Training d_loss: 1.3879 Explore P: 0.7648\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 29.0 Average reward fake: 0.5003601312637329 Average reward real: 0.5002802014350891 Training q_loss: 2.1581 Training g_loss: 2.0655 Training d_loss: 1.3866 Explore P: 0.7626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 26.0 Average reward fake: 0.4993584454059601 Average reward real: 0.4993157684803009 Training q_loss: 4.2934 Training g_loss: 2.0714 Training d_loss: 1.3869 Explore P: 0.7606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 38.0 Average reward fake: 0.49998748302459717 Average reward real: 0.5002425909042358 Training q_loss: 3.7783 Training g_loss: 2.0665 Training d_loss: 1.3859 Explore P: 0.7578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 43.0 Average reward fake: 0.5010541677474976 Average reward real: 0.5016839504241943 Training q_loss: 3.4996 Training g_loss: 2.0718 Training d_loss: 1.3851 Explore P: 0.7546\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 118.0 Average reward fake: 0.5009661316871643 Average reward real: 0.500367283821106 Training q_loss: 3.0356 Training g_loss: 2.0661 Training d_loss: 1.3876 Explore P: 0.7458\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 11.0 Average reward fake: 0.49916571378707886 Average reward real: 0.49931061267852783 Training q_loss: 2.8025 Training g_loss: 2.0700 Training d_loss: 1.3858 Explore P: 0.7450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 17.0 Average reward fake: 0.5005367398262024 Average reward real: 0.5004507303237915 Training q_loss: 32.2110 Training g_loss: 2.0987 Training d_loss: 1.3895 Explore P: 0.7438\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 31.0 Average reward fake: 0.5000736117362976 Average reward real: 0.5032782554626465 Training q_loss: 3.1685 Training g_loss: 2.0854 Training d_loss: 1.3800 Explore P: 0.7415\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 29.0 Average reward fake: 0.49971243739128113 Average reward real: 0.5048757791519165 Training q_loss: 2.7239 Training g_loss: 2.0869 Training d_loss: 1.3775 Explore P: 0.7394\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 22.0 Average reward fake: 0.49442845582962036 Average reward real: 0.49706023931503296 Training q_loss: 2.9352 Training g_loss: 2.0910 Training d_loss: 1.3809 Explore P: 0.7378\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 24.0 Average reward fake: 0.5051302313804626 Average reward real: 0.4952329993247986 Training q_loss: 2.3501 Training g_loss: 2.0777 Training d_loss: 1.4061 Explore P: 0.7360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 111.0 Average reward fake: 0.4915795922279358 Average reward real: 0.4920404553413391 Training q_loss: 4.5253 Training g_loss: 2.0813 Training d_loss: 1.3857 Explore P: 0.7280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 25.0 Average reward fake: 0.5068159103393555 Average reward real: 0.5009469389915466 Training q_loss: 2.6969 Training g_loss: 2.0758 Training d_loss: 1.3975 Explore P: 0.7262\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 20.0 Average reward fake: 0.49676713347435 Average reward real: 0.49758780002593994 Training q_loss: 2.6607 Training g_loss: 2.0752 Training d_loss: 1.3860 Explore P: 0.7248\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 23.0 Average reward fake: 0.5005117654800415 Average reward real: 0.498540461063385 Training q_loss: 8.2064 Training g_loss: 2.0718 Training d_loss: 1.3900 Explore P: 0.7232\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 12.0 Average reward fake: 0.49939072132110596 Average reward real: 0.4980923533439636 Training q_loss: 3.0412 Training g_loss: 2.0648 Training d_loss: 1.3889 Explore P: 0.7223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 49.0 Average reward fake: 0.5007169842720032 Average reward real: 0.49972695112228394 Training q_loss: 3.2358 Training g_loss: 2.0662 Training d_loss: 1.3885 Explore P: 0.7188\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 40.0 Average reward fake: 0.5002354383468628 Average reward real: 0.5003368258476257 Training q_loss: 2.7586 Training g_loss: 2.0714 Training d_loss: 1.3861 Explore P: 0.7160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 23.0 Average reward fake: 0.4984844923019409 Average reward real: 0.5000820755958557 Training q_loss: 6.2130 Training g_loss: 2.0781 Training d_loss: 1.3831 Explore P: 0.7144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 17.0 Average reward fake: 0.4996595084667206 Average reward real: 0.5014057755470276 Training q_loss: 3.0895 Training g_loss: 2.0712 Training d_loss: 1.3836 Explore P: 0.7132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 38.0 Average reward fake: 0.4998948276042938 Average reward real: 0.49945586919784546 Training q_loss: 2.1322 Training g_loss: 2.0686 Training d_loss: 1.3870 Explore P: 0.7105\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 25.0 Average reward fake: 0.5003202557563782 Average reward real: 0.5003702044487 Training q_loss: 2.9024 Training g_loss: 2.0738 Training d_loss: 1.3864 Explore P: 0.7088\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 18.0 Average reward fake: 0.5005142092704773 Average reward real: 0.49950242042541504 Training q_loss: 2.5987 Training g_loss: 2.0693 Training d_loss: 1.3883 Explore P: 0.7075\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 25.0 Average reward fake: 0.49919453263282776 Average reward real: 0.5000010132789612 Training q_loss: 2.4799 Training g_loss: 2.0791 Training d_loss: 1.3843 Explore P: 0.7058\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 27.0 Average reward fake: 0.49943941831588745 Average reward real: 0.5006727576255798 Training q_loss: 2.9203 Training g_loss: 2.0719 Training d_loss: 1.3841 Explore P: 0.7039\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 33.0 Average reward fake: 0.5003920197486877 Average reward real: 0.5001635551452637 Training q_loss: 2.1346 Training g_loss: 2.0658 Training d_loss: 1.3871 Explore P: 0.7016\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 9.0 Average reward fake: 0.4999168813228607 Average reward real: 0.4999014139175415 Training q_loss: 3.8005 Training g_loss: 2.0663 Training d_loss: 1.3863 Explore P: 0.7010\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 84.0 Average reward fake: 0.4998325705528259 Average reward real: 0.5000383853912354 Training q_loss: 2.7054 Training g_loss: 2.0721 Training d_loss: 1.3861 Explore P: 0.6952\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 62.0 Average reward fake: 0.49945706129074097 Average reward real: 0.500033438205719 Training q_loss: 2.5994 Training g_loss: 2.0739 Training d_loss: 1.3851 Explore P: 0.6910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 19.0 Average reward fake: 0.4996790289878845 Average reward real: 0.5000208616256714 Training q_loss: 2.4610 Training g_loss: 2.0752 Training d_loss: 1.3857 Explore P: 0.6897\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 26.0 Average reward fake: 0.5002864003181458 Average reward real: 0.5006552338600159 Training q_loss: 2.8991 Training g_loss: 2.0664 Training d_loss: 1.3856 Explore P: 0.6879\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 56.0 Average reward fake: 0.49977314472198486 Average reward real: 0.49930164217948914 Training q_loss: 2.8012 Training g_loss: 2.0665 Training d_loss: 1.3869 Explore P: 0.6841\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 32.0 Average reward fake: 0.5003142356872559 Average reward real: 0.5001880526542664 Training q_loss: 2.4131 Training g_loss: 2.0699 Training d_loss: 1.3862 Explore P: 0.6820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 21.0 Average reward fake: 0.4994245767593384 Average reward real: 0.5000096559524536 Training q_loss: 2.7427 Training g_loss: 2.0733 Training d_loss: 1.3851 Explore P: 0.6806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 45.0 Average reward fake: 0.4999772012233734 Average reward real: 0.49999818205833435 Training q_loss: 3.7705 Training g_loss: 2.0687 Training d_loss: 1.3863 Explore P: 0.6775\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 65.0 Average reward fake: 0.49979567527770996 Average reward real: 0.5000259280204773 Training q_loss: 3.3523 Training g_loss: 2.0709 Training d_loss: 1.3860 Explore P: 0.6732\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 19.0 Average reward fake: 0.4995763301849365 Average reward real: 0.49991142749786377 Training q_loss: 2.7634 Training g_loss: 2.0726 Training d_loss: 1.3855 Explore P: 0.6720\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 27.0 Average reward fake: 0.49995726346969604 Average reward real: 0.5006876587867737 Training q_loss: 2.1865 Training g_loss: 2.0696 Training d_loss: 1.3848 Explore P: 0.6702\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 67.0 Average reward fake: 0.500904381275177 Average reward real: 0.5002879500389099 Training q_loss: 2.5417 Training g_loss: 2.0664 Training d_loss: 1.3875 Explore P: 0.6658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 24.0 Average reward fake: 0.49954330921173096 Average reward real: 0.49985426664352417 Training q_loss: 2.6366 Training g_loss: 2.0692 Training d_loss: 1.3856 Explore P: 0.6642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 61.0 Average reward fake: 0.5001686215400696 Average reward real: 0.5004653930664062 Training q_loss: 2.7465 Training g_loss: 2.0698 Training d_loss: 1.3857 Explore P: 0.6602\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 22.0 Average reward fake: 0.4993062913417816 Average reward real: 0.4999021589756012 Training q_loss: 3.1986 Training g_loss: 2.0767 Training d_loss: 1.3853 Explore P: 0.6588\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 35.0 Average reward fake: 0.4995249807834625 Average reward real: 0.49992796778678894 Training q_loss: 2.4069 Training g_loss: 2.0671 Training d_loss: 1.3854 Explore P: 0.6565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 84.0 Average reward fake: 0.49877429008483887 Average reward real: 0.49958565831184387 Training q_loss: 2.6198 Training g_loss: 2.0733 Training d_loss: 1.3847 Explore P: 0.6511\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 21.0 Average reward fake: 0.5013852715492249 Average reward real: 0.5008108019828796 Training q_loss: 3.7379 Training g_loss: 2.0645 Training d_loss: 1.3875 Explore P: 0.6498\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 9.0 Average reward fake: 0.5001065135002136 Average reward real: 0.5005975961685181 Training q_loss: 3.6341 Training g_loss: 2.0673 Training d_loss: 1.3850 Explore P: 0.6492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 49.0 Average reward fake: 0.500210702419281 Average reward real: 0.5002026557922363 Training q_loss: 3.2367 Training g_loss: 2.0708 Training d_loss: 1.3865 Explore P: 0.6461\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 25.0 Average reward fake: 0.4995642900466919 Average reward real: 0.5000413656234741 Training q_loss: 5.8362 Training g_loss: 2.0703 Training d_loss: 1.3856 Explore P: 0.6445\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 27.0 Average reward fake: 0.5003978610038757 Average reward real: 0.5000922679901123 Training q_loss: 2.6620 Training g_loss: 2.0651 Training d_loss: 1.3869 Explore P: 0.6428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 19.0 Average reward fake: 0.4995391070842743 Average reward real: 0.49996814131736755 Training q_loss: 2.0996 Training g_loss: 2.0743 Training d_loss: 1.3854 Explore P: 0.6416\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 26.0 Average reward fake: 0.49997657537460327 Average reward real: 0.500030517578125 Training q_loss: 2.1573 Training g_loss: 2.0695 Training d_loss: 1.3860 Explore P: 0.6399\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 32.0 Average reward fake: 0.5002392530441284 Average reward real: 0.5001225471496582 Training q_loss: 2.7488 Training g_loss: 2.0655 Training d_loss: 1.3866 Explore P: 0.6379\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 38.0 Average reward fake: 0.5000689625740051 Average reward real: 0.5002164244651794 Training q_loss: 2.3616 Training g_loss: 2.0685 Training d_loss: 1.3858 Explore P: 0.6355\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 15.0 Average reward fake: 0.4993981122970581 Average reward real: 0.4996839463710785 Training q_loss: 2.4063 Training g_loss: 2.0745 Training d_loss: 1.3857 Explore P: 0.6346\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 38.0 Average reward fake: 0.4992261528968811 Average reward real: 0.49957534670829773 Training q_loss: 2.6285 Training g_loss: 2.0657 Training d_loss: 1.3859 Explore P: 0.6322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 53.0 Average reward fake: 0.4996293783187866 Average reward real: 0.49985983967781067 Training q_loss: 2.2676 Training g_loss: 2.0697 Training d_loss: 1.3859 Explore P: 0.6289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 17.0 Average reward fake: 0.49993225932121277 Average reward real: 0.5002530813217163 Training q_loss: 3.0575 Training g_loss: 2.0679 Training d_loss: 1.3856 Explore P: 0.6279\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 67.0 Average reward fake: 0.49997496604919434 Average reward real: 0.5003606677055359 Training q_loss: 2.5334 Training g_loss: 2.0654 Training d_loss: 1.3856 Explore P: 0.6238\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 25.0 Average reward fake: 0.5000167489051819 Average reward real: 0.5001690983772278 Training q_loss: 2.5888 Training g_loss: 2.0646 Training d_loss: 1.3863 Explore P: 0.6222\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 99.0 Average reward fake: 0.5008943676948547 Average reward real: 0.5008940696716309 Training q_loss: 2.7309 Training g_loss: 2.0656 Training d_loss: 1.3863 Explore P: 0.6162\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 36.0 Average reward fake: 0.5005906224250793 Average reward real: 0.5006272792816162 Training q_loss: 2.3441 Training g_loss: 2.0703 Training d_loss: 1.3861 Explore P: 0.6140\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 24.0 Average reward fake: 0.5001130104064941 Average reward real: 0.5003331899642944 Training q_loss: 4.4700 Training g_loss: 2.0662 Training d_loss: 1.3858 Explore P: 0.6126\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 70.0 Average reward fake: 0.499428391456604 Average reward real: 0.4998559057712555 Training q_loss: 2.7214 Training g_loss: 2.0701 Training d_loss: 1.3858 Explore P: 0.6084\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 33.0 Average reward fake: 0.49963846802711487 Average reward real: 0.5001111030578613 Training q_loss: 2.4788 Training g_loss: 2.0747 Training d_loss: 1.3853 Explore P: 0.6064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 15.0 Average reward fake: 0.5002071857452393 Average reward real: 0.5003319382667542 Training q_loss: 2.6474 Training g_loss: 2.0710 Training d_loss: 1.3858 Explore P: 0.6055\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 25.0 Average reward fake: 0.49986398220062256 Average reward real: 0.5003755688667297 Training q_loss: 2.4826 Training g_loss: 2.0682 Training d_loss: 1.3848 Explore P: 0.6040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 60.0 Average reward fake: 0.5000132918357849 Average reward real: 0.5001237988471985 Training q_loss: 2.7313 Training g_loss: 2.0688 Training d_loss: 1.3860 Explore P: 0.6005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 41.0 Average reward fake: 0.5015407204627991 Average reward real: 0.5010619163513184 Training q_loss: 3.1460 Training g_loss: 2.0704 Training d_loss: 1.3861 Explore P: 0.5980\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 69.0 Average reward fake: 0.49984437227249146 Average reward real: 0.5001938939094543 Training q_loss: 2.2313 Training g_loss: 2.0706 Training d_loss: 1.3863 Explore P: 0.5940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 94.0 Average reward fake: 0.5004856586456299 Average reward real: 0.5004656314849854 Training q_loss: 2.2952 Training g_loss: 2.0746 Training d_loss: 1.3865 Explore P: 0.5885\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 29.0 Average reward fake: 0.49905452132225037 Average reward real: 0.4996771216392517 Training q_loss: 3.2576 Training g_loss: 2.0700 Training d_loss: 1.3850 Explore P: 0.5869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 63.0 Average reward fake: 0.49993079900741577 Average reward real: 0.5004268884658813 Training q_loss: 2.4576 Training g_loss: 2.0703 Training d_loss: 1.3854 Explore P: 0.5832\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 19.0 Average reward fake: 0.4998767375946045 Average reward real: 0.5001113414764404 Training q_loss: 2.6328 Training g_loss: 2.0717 Training d_loss: 1.3863 Explore P: 0.5822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 82.0 Average reward fake: 0.49973922967910767 Average reward real: 0.5000253915786743 Training q_loss: 2.3126 Training g_loss: 2.0683 Training d_loss: 1.3858 Explore P: 0.5775\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 27.0 Average reward fake: 0.5003901720046997 Average reward real: 0.4995577335357666 Training q_loss: 2.1440 Training g_loss: 2.0654 Training d_loss: 1.3878 Explore P: 0.5759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 40.0 Average reward fake: 0.4999585449695587 Average reward real: 0.5003758072853088 Training q_loss: 2.5240 Training g_loss: 2.0654 Training d_loss: 1.3852 Explore P: 0.5737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 84.0 Average reward fake: 0.49960067868232727 Average reward real: 0.49992233514785767 Training q_loss: 2.4007 Training g_loss: 2.0707 Training d_loss: 1.3859 Explore P: 0.5690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 27.0 Average reward fake: 0.49977052211761475 Average reward real: 0.4998348355293274 Training q_loss: 3.3353 Training g_loss: 2.0736 Training d_loss: 1.3862 Explore P: 0.5675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 20.0 Average reward fake: 0.4995855391025543 Average reward real: 0.5000132918357849 Training q_loss: 2.2001 Training g_loss: 2.0783 Training d_loss: 1.3854 Explore P: 0.5664\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 17.0 Average reward fake: 0.5005627274513245 Average reward real: 0.5005170702934265 Training q_loss: 2.4082 Training g_loss: 2.0685 Training d_loss: 1.3864 Explore P: 0.5654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 37.0 Average reward fake: 0.5000229477882385 Average reward real: 0.5004432201385498 Training q_loss: 2.1851 Training g_loss: 2.0693 Training d_loss: 1.3855 Explore P: 0.5634\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 111.0 Average reward fake: 0.4990224540233612 Average reward real: 0.4995037913322449 Training q_loss: 2.7583 Training g_loss: 2.0709 Training d_loss: 1.3856 Explore P: 0.5572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 71.0 Average reward fake: 0.49953514337539673 Average reward real: 0.5000253915786743 Training q_loss: 2.5614 Training g_loss: 2.0757 Training d_loss: 1.3851 Explore P: 0.5534\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 27.0 Average reward fake: 0.4998175799846649 Average reward real: 0.5005064606666565 Training q_loss: 2.6203 Training g_loss: 2.0746 Training d_loss: 1.3857 Explore P: 0.5519\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 31.0 Average reward fake: 0.4988800287246704 Average reward real: 0.4994302988052368 Training q_loss: 2.7648 Training g_loss: 2.0764 Training d_loss: 1.3852 Explore P: 0.5502\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 18.0 Average reward fake: 0.49998390674591064 Average reward real: 0.5009081363677979 Training q_loss: 2.2396 Training g_loss: 2.0674 Training d_loss: 1.3846 Explore P: 0.5493\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 13.0 Average reward fake: 0.4998563826084137 Average reward real: 0.4998912811279297 Training q_loss: 2.5877 Training g_loss: 2.0689 Training d_loss: 1.3861 Explore P: 0.5486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 83.0 Average reward fake: 0.5001484155654907 Average reward real: 0.49998557567596436 Training q_loss: 2.3797 Training g_loss: 2.0689 Training d_loss: 1.3865 Explore P: 0.5441\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 25.0 Average reward fake: 0.49984559416770935 Average reward real: 0.500148355960846 Training q_loss: 2.4211 Training g_loss: 2.0776 Training d_loss: 1.3858 Explore P: 0.5428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 16.0 Average reward fake: 0.5016286969184875 Average reward real: 0.5006728768348694 Training q_loss: 2.9879 Training g_loss: 2.0652 Training d_loss: 1.3885 Explore P: 0.5419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 82.0 Average reward fake: 0.4999806582927704 Average reward real: 0.49980881810188293 Training q_loss: 2.1462 Training g_loss: 2.0758 Training d_loss: 1.3867 Explore P: 0.5376\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 34.0 Average reward fake: 0.4990684390068054 Average reward real: 0.4992956519126892 Training q_loss: 2.3045 Training g_loss: 2.0731 Training d_loss: 1.3858 Explore P: 0.5358\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 119.0 Average reward fake: 0.5001376867294312 Average reward real: 0.500340461730957 Training q_loss: 2.7987 Training g_loss: 2.0658 Training d_loss: 1.3860 Explore P: 0.5296\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 36.0 Average reward fake: 0.5009267330169678 Average reward real: 0.5006518363952637 Training q_loss: 2.4820 Training g_loss: 2.0706 Training d_loss: 1.3870 Explore P: 0.5277\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 24.0 Average reward fake: 0.4999762773513794 Average reward real: 0.5000319480895996 Training q_loss: 2.3291 Training g_loss: 2.0779 Training d_loss: 1.3859 Explore P: 0.5265\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 16.0 Average reward fake: 0.4991080164909363 Average reward real: 0.49940812587738037 Training q_loss: 2.2447 Training g_loss: 2.0737 Training d_loss: 1.3856 Explore P: 0.5256\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 46.0 Average reward fake: 0.5005841255187988 Average reward real: 0.5006567239761353 Training q_loss: 3.3355 Training g_loss: 2.0715 Training d_loss: 1.3863 Explore P: 0.5233\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 13.0 Average reward fake: 0.5009104013442993 Average reward real: 0.5003104209899902 Training q_loss: 2.1682 Training g_loss: 2.0776 Training d_loss: 1.3873 Explore P: 0.5226\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 30.0 Average reward fake: 0.5002678036689758 Average reward real: 0.5015771389007568 Training q_loss: 2.4005 Training g_loss: 2.0723 Training d_loss: 1.3840 Explore P: 0.5211\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 44.0 Average reward fake: 0.5007959604263306 Average reward real: 0.5013903379440308 Training q_loss: 2.4386 Training g_loss: 2.0739 Training d_loss: 1.3851 Explore P: 0.5188\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 51.0 Average reward fake: 0.499370276927948 Average reward real: 0.5002878904342651 Training q_loss: 2.2912 Training g_loss: 2.0693 Training d_loss: 1.3846 Explore P: 0.5162\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 65.0 Average reward fake: 0.4995749592781067 Average reward real: 0.4996279180049896 Training q_loss: 3.8118 Training g_loss: 2.0753 Training d_loss: 1.3861 Explore P: 0.5130\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 53.0 Average reward fake: 0.5007564425468445 Average reward real: 0.5002335906028748 Training q_loss: 2.1982 Training g_loss: 2.0674 Training d_loss: 1.3885 Explore P: 0.5103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 47.0 Average reward fake: 0.4995720684528351 Average reward real: 0.49991241097450256 Training q_loss: 2.5332 Training g_loss: 2.0718 Training d_loss: 1.3857 Explore P: 0.5080\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 20.0 Average reward fake: 0.4991725981235504 Average reward real: 0.4996625483036041 Training q_loss: 2.3121 Training g_loss: 2.0685 Training d_loss: 1.3854 Explore P: 0.5070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 17.0 Average reward fake: 0.4997364282608032 Average reward real: 0.5004411935806274 Training q_loss: 2.4471 Training g_loss: 2.0703 Training d_loss: 1.3851 Explore P: 0.5061\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 26.0 Average reward fake: 0.5001437664031982 Average reward real: 0.4999019503593445 Training q_loss: 2.3841 Training g_loss: 2.0756 Training d_loss: 1.3866 Explore P: 0.5048\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 27.0 Average reward fake: 0.49978867173194885 Average reward real: 0.5008056163787842 Training q_loss: 2.2778 Training g_loss: 2.0722 Training d_loss: 1.3846 Explore P: 0.5035\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 17.0 Average reward fake: 0.4992998242378235 Average reward real: 0.49957385659217834 Training q_loss: 2.3176 Training g_loss: 2.0703 Training d_loss: 1.3860 Explore P: 0.5027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 53.0 Average reward fake: 0.4995235204696655 Average reward real: 0.49908748269081116 Training q_loss: 2.4432 Training g_loss: 2.0724 Training d_loss: 1.3871 Explore P: 0.5000\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 27.0 Average reward fake: 0.5005916357040405 Average reward real: 0.5005130171775818 Training q_loss: 2.6037 Training g_loss: 2.0698 Training d_loss: 1.3866 Explore P: 0.4987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 42.0 Average reward fake: 0.4996046721935272 Average reward real: 0.5016207098960876 Training q_loss: 2.2123 Training g_loss: 2.0784 Training d_loss: 1.3824 Explore P: 0.4967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 81.0 Average reward fake: 0.49983206391334534 Average reward real: 0.5010141134262085 Training q_loss: 2.2392 Training g_loss: 2.0714 Training d_loss: 1.3857 Explore P: 0.4928\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 104.0 Average reward fake: 0.4997166395187378 Average reward real: 0.5001599788665771 Training q_loss: 2.2569 Training g_loss: 2.0727 Training d_loss: 1.3854 Explore P: 0.4878\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 18.0 Average reward fake: 0.49939224123954773 Average reward real: 0.5005190372467041 Training q_loss: 2.4872 Training g_loss: 2.0716 Training d_loss: 1.3842 Explore P: 0.4869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 50.0 Average reward fake: 0.49967440962791443 Average reward real: 0.49968352913856506 Training q_loss: 2.3606 Training g_loss: 2.0698 Training d_loss: 1.3861 Explore P: 0.4845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 20.0 Average reward fake: 0.4998621344566345 Average reward real: 0.5004688501358032 Training q_loss: 2.3163 Training g_loss: 2.0751 Training d_loss: 1.3851 Explore P: 0.4836\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 23.0 Average reward fake: 0.4992634654045105 Average reward real: 0.5000036954879761 Training q_loss: 2.2184 Training g_loss: 2.0758 Training d_loss: 1.3850 Explore P: 0.4825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 81.0 Average reward fake: 0.5018157958984375 Average reward real: 0.49963676929473877 Training q_loss: 2.1217 Training g_loss: 2.0726 Training d_loss: 1.3913 Explore P: 0.4787\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 24.0 Average reward fake: 0.49910610914230347 Average reward real: 0.4986521005630493 Training q_loss: 2.1615 Training g_loss: 2.0731 Training d_loss: 1.3859 Explore P: 0.4775\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 142.0 Average reward fake: 0.5000513195991516 Average reward real: 0.5010879039764404 Training q_loss: 2.7476 Training g_loss: 2.0733 Training d_loss: 1.3845 Explore P: 0.4710\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 9.0 Average reward fake: 0.5002407431602478 Average reward real: 0.5005209445953369 Training q_loss: 3.4696 Training g_loss: 2.0706 Training d_loss: 1.3860 Explore P: 0.4705\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 28.0 Average reward fake: 0.49997633695602417 Average reward real: 0.5006880164146423 Training q_loss: 2.2351 Training g_loss: 2.0749 Training d_loss: 1.3855 Explore P: 0.4693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 20.0 Average reward fake: 0.5010603070259094 Average reward real: 0.49879664182662964 Training q_loss: 2.7436 Training g_loss: 2.0752 Training d_loss: 1.3908 Explore P: 0.4683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 47.0 Average reward fake: 0.4995681047439575 Average reward real: 0.5003975033760071 Training q_loss: 2.4706 Training g_loss: 2.0716 Training d_loss: 1.3848 Explore P: 0.4662\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 75.0 Average reward fake: 0.4996340870857239 Average reward real: 0.5015184283256531 Training q_loss: 2.6406 Training g_loss: 2.0765 Training d_loss: 1.3826 Explore P: 0.4628\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 14.0 Average reward fake: 0.49966827034950256 Average reward real: 0.5011757016181946 Training q_loss: 2.5401 Training g_loss: 2.0734 Training d_loss: 1.3833 Explore P: 0.4621\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 47.0 Average reward fake: 0.5015558004379272 Average reward real: 0.5014041662216187 Training q_loss: 2.6903 Training g_loss: 2.0834 Training d_loss: 1.3873 Explore P: 0.4600\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 25.0 Average reward fake: 0.49920254945755005 Average reward real: 0.4997312128543854 Training q_loss: 2.6724 Training g_loss: 2.0740 Training d_loss: 1.3855 Explore P: 0.4589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 15.0 Average reward fake: 0.4994295835494995 Average reward real: 0.4999527633190155 Training q_loss: 2.4110 Training g_loss: 2.0767 Training d_loss: 1.3853 Explore P: 0.4582\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 82.0 Average reward fake: 0.49981072545051575 Average reward real: 0.5007098913192749 Training q_loss: 2.5109 Training g_loss: 2.0754 Training d_loss: 1.3845 Explore P: 0.4546\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 51.0 Average reward fake: 0.500479519367218 Average reward real: 0.5002803206443787 Training q_loss: 2.3335 Training g_loss: 2.0672 Training d_loss: 1.3866 Explore P: 0.4523\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 25.0 Average reward fake: 0.4998162090778351 Average reward real: 0.5007034540176392 Training q_loss: 2.4587 Training g_loss: 2.0761 Training d_loss: 1.3848 Explore P: 0.4512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 32.0 Average reward fake: 0.5001571178436279 Average reward real: 0.49952566623687744 Training q_loss: 2.1209 Training g_loss: 2.0739 Training d_loss: 1.3876 Explore P: 0.4498\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 72.0 Average reward fake: 0.499327689409256 Average reward real: 0.5004236698150635 Training q_loss: 2.7627 Training g_loss: 2.0729 Training d_loss: 1.3843 Explore P: 0.4466\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 38.0 Average reward fake: 0.49953845143318176 Average reward real: 0.5006900429725647 Training q_loss: 2.6343 Training g_loss: 2.0835 Training d_loss: 1.3841 Explore P: 0.4450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 15.0 Average reward fake: 0.500478208065033 Average reward real: 0.5008987188339233 Training q_loss: 2.6140 Training g_loss: 2.0716 Training d_loss: 1.3855 Explore P: 0.4443\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 17.0 Average reward fake: 0.49929144978523254 Average reward real: 0.5001531839370728 Training q_loss: 2.4195 Training g_loss: 2.0734 Training d_loss: 1.3847 Explore P: 0.4436\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 64.0 Average reward fake: 0.4993728995323181 Average reward real: 0.49962103366851807 Training q_loss: 2.4045 Training g_loss: 2.0776 Training d_loss: 1.3860 Explore P: 0.4408\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 55.0 Average reward fake: 0.49993008375167847 Average reward real: 0.4997026026248932 Training q_loss: 2.5595 Training g_loss: 2.0690 Training d_loss: 1.3866 Explore P: 0.4385\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 43.0 Average reward fake: 0.4997401535511017 Average reward real: 0.49991554021835327 Training q_loss: 2.7977 Training g_loss: 2.0725 Training d_loss: 1.3860 Explore P: 0.4366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 30.0 Average reward fake: 0.5000575184822083 Average reward real: 0.4999508261680603 Training q_loss: 2.4476 Training g_loss: 2.0699 Training d_loss: 1.3865 Explore P: 0.4353\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 35.0 Average reward fake: 0.499418169260025 Average reward real: 0.5005521178245544 Training q_loss: 2.9287 Training g_loss: 2.0716 Training d_loss: 1.3843 Explore P: 0.4339\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 59.0 Average reward fake: 0.4996771216392517 Average reward real: 0.4993983507156372 Training q_loss: 2.9747 Training g_loss: 2.0798 Training d_loss: 1.3869 Explore P: 0.4314\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 35.0 Average reward fake: 0.5012040734291077 Average reward real: 0.5007290840148926 Training q_loss: 2.4865 Training g_loss: 2.0736 Training d_loss: 1.3869 Explore P: 0.4299\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 81.0 Average reward fake: 0.5006187558174133 Average reward real: 0.500103771686554 Training q_loss: 2.3693 Training g_loss: 2.0716 Training d_loss: 1.3873 Explore P: 0.4265\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 32.0 Average reward fake: 0.5011336803436279 Average reward real: 0.5005390048027039 Training q_loss: 2.4836 Training g_loss: 2.0705 Training d_loss: 1.3875 Explore P: 0.4252\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 35.0 Average reward fake: 0.4996976852416992 Average reward real: 0.4998358190059662 Training q_loss: 2.3693 Training g_loss: 2.0826 Training d_loss: 1.3863 Explore P: 0.4237\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 36.0 Average reward fake: 0.5005586743354797 Average reward real: 0.5010452270507812 Training q_loss: 2.4359 Training g_loss: 2.0669 Training d_loss: 1.3857 Explore P: 0.4222\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 70.0 Average reward fake: 0.5002074241638184 Average reward real: 0.5003253221511841 Training q_loss: 2.4002 Training g_loss: 2.0715 Training d_loss: 1.3863 Explore P: 0.4194\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 17.0 Average reward fake: 0.49947836995124817 Average reward real: 0.4994101822376251 Training q_loss: 2.3701 Training g_loss: 2.0798 Training d_loss: 1.3865 Explore P: 0.4187\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 52.0 Average reward fake: 0.5000243186950684 Average reward real: 0.5002808570861816 Training q_loss: 2.3472 Training g_loss: 2.0779 Training d_loss: 1.3859 Explore P: 0.4165\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 40.0 Average reward fake: 0.49949783086776733 Average reward real: 0.499144971370697 Training q_loss: 2.5511 Training g_loss: 2.0718 Training d_loss: 1.3872 Explore P: 0.4149\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 22.0 Average reward fake: 0.4998619556427002 Average reward real: 0.5001556873321533 Training q_loss: 2.7271 Training g_loss: 2.0692 Training d_loss: 1.3857 Explore P: 0.4140\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 38.0 Average reward fake: 0.5008611083030701 Average reward real: 0.5016490817070007 Training q_loss: 2.7268 Training g_loss: 2.0725 Training d_loss: 1.3849 Explore P: 0.4125\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 14.0 Average reward fake: 0.5003381967544556 Average reward real: 0.5009585022926331 Training q_loss: 2.2056 Training g_loss: 2.0703 Training d_loss: 1.3856 Explore P: 0.4119\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 74.0 Average reward fake: 0.5003417730331421 Average reward real: 0.4998839497566223 Training q_loss: 4.5241 Training g_loss: 2.0728 Training d_loss: 1.3870 Explore P: 0.4090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 47.0 Average reward fake: 0.4999200403690338 Average reward real: 0.49968841671943665 Training q_loss: 2.6612 Training g_loss: 2.0711 Training d_loss: 1.3866 Explore P: 0.4071\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 15.0 Average reward fake: 0.5000876784324646 Average reward real: 0.5002564191818237 Training q_loss: 2.7409 Training g_loss: 2.0754 Training d_loss: 1.3864 Explore P: 0.4065\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 43.0 Average reward fake: 0.5003917813301086 Average reward real: 0.5000944137573242 Training q_loss: 2.5330 Training g_loss: 2.0750 Training d_loss: 1.3870 Explore P: 0.4048\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 62.0 Average reward fake: 0.5000565052032471 Average reward real: 0.5001171231269836 Training q_loss: 2.7290 Training g_loss: 2.0704 Training d_loss: 1.3865 Explore P: 0.4024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 33.0 Average reward fake: 0.49977028369903564 Average reward real: 0.500592827796936 Training q_loss: 2.5680 Training g_loss: 2.0712 Training d_loss: 1.3847 Explore P: 0.4011\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 35.0 Average reward fake: 0.49923065304756165 Average reward real: 0.499923437833786 Training q_loss: 2.1671 Training g_loss: 2.0743 Training d_loss: 1.3848 Explore P: 0.3997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 47.0 Average reward fake: 0.4999690353870392 Average reward real: 0.5001264214515686 Training q_loss: 2.8515 Training g_loss: 2.0732 Training d_loss: 1.3861 Explore P: 0.3979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 25.0 Average reward fake: 0.5001780986785889 Average reward real: 0.5006970763206482 Training q_loss: 2.3781 Training g_loss: 2.0684 Training d_loss: 1.3854 Explore P: 0.3969\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 87.0 Average reward fake: 0.4991665780544281 Average reward real: 0.4994798004627228 Training q_loss: 2.6271 Training g_loss: 2.0709 Training d_loss: 1.3858 Explore P: 0.3936\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 44.0 Average reward fake: 0.4991869330406189 Average reward real: 0.4998537302017212 Training q_loss: 2.1741 Training g_loss: 2.0759 Training d_loss: 1.3850 Explore P: 0.3919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 67.0 Average reward fake: 0.5009555816650391 Average reward real: 0.5006891489028931 Training q_loss: 2.1097 Training g_loss: 2.0697 Training d_loss: 1.3859 Explore P: 0.3893\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 35.0 Average reward fake: 0.5001368522644043 Average reward real: 0.5004146695137024 Training q_loss: 2.4805 Training g_loss: 2.0778 Training d_loss: 1.3856 Explore P: 0.3880\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 52.0 Average reward fake: 0.5003251433372498 Average reward real: 0.5014774203300476 Training q_loss: 2.3656 Training g_loss: 2.0688 Training d_loss: 1.3841 Explore P: 0.3860\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 37.0 Average reward fake: 0.49953651428222656 Average reward real: 0.4999455213546753 Training q_loss: 2.3363 Training g_loss: 2.0751 Training d_loss: 1.3854 Explore P: 0.3847\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 42.0 Average reward fake: 0.49943068623542786 Average reward real: 0.5005871653556824 Training q_loss: 2.2068 Training g_loss: 2.0772 Training d_loss: 1.3841 Explore P: 0.3831\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 39.0 Average reward fake: 0.4994269609451294 Average reward real: 0.5004704594612122 Training q_loss: 2.5845 Training g_loss: 2.0732 Training d_loss: 1.3842 Explore P: 0.3816\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 75.0 Average reward fake: 0.5010700821876526 Average reward real: 0.49995627999305725 Training q_loss: 3.0405 Training g_loss: 2.0688 Training d_loss: 1.3884 Explore P: 0.3789\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 25.0 Average reward fake: 0.5002539753913879 Average reward real: 0.500465452671051 Training q_loss: 2.1872 Training g_loss: 2.0759 Training d_loss: 1.3857 Explore P: 0.3779\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 92.0 Average reward fake: 0.5000251531600952 Average reward real: 0.4993082284927368 Training q_loss: 3.7069 Training g_loss: 2.0749 Training d_loss: 1.3874 Explore P: 0.3746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 79.0 Average reward fake: 0.5005672574043274 Average reward real: 0.5002075433731079 Training q_loss: 2.1225 Training g_loss: 2.0731 Training d_loss: 1.3869 Explore P: 0.3717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 45.0 Average reward fake: 0.5005282163619995 Average reward real: 0.5000454187393188 Training q_loss: 2.3281 Training g_loss: 2.0730 Training d_loss: 1.3876 Explore P: 0.3701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 35.0 Average reward fake: 0.5000792145729065 Average reward real: 0.500617504119873 Training q_loss: 2.1535 Training g_loss: 2.0696 Training d_loss: 1.3855 Explore P: 0.3688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 46.0 Average reward fake: 0.5016435384750366 Average reward real: 0.49987098574638367 Training q_loss: 2.4137 Training g_loss: 2.0741 Training d_loss: 1.3897 Explore P: 0.3672\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 38.0 Average reward fake: 0.49967536330223083 Average reward real: 0.49977073073387146 Training q_loss: 2.1630 Training g_loss: 2.0734 Training d_loss: 1.3860 Explore P: 0.3658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 28.0 Average reward fake: 0.4987632632255554 Average reward real: 0.499394029378891 Training q_loss: 4.2146 Training g_loss: 2.0756 Training d_loss: 1.3850 Explore P: 0.3648\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "q_loss_list, g_loss_list, d_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     # Restore/load the trained model \n",
    "    #     #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        q_loss, g_loss, d_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                rewards_real_list.append((ep, rewards_real_mean))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Calculating real current reward and next action\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states}\n",
    "            next_actions_logits, rewards_fake, rewards_real = sess.run([model.actions_logits, \n",
    "                                                                        model.rewards_fake, model.rewards_real], \n",
    "                                                                       feed_dict)\n",
    "            #             feed_dict={model.states: next_states}\n",
    "            #             next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0) # NOTE: action size\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            #targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "            targetQs = rewards_real.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating/training/optimizing the model\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states,\n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_fake_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Fake rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_real_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Real rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/QGAN-cartpole.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the env\n",
    "# WARNING: If you close, you can NOT restart again!!!!!!\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
