{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QGAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, reward, done, info\n",
      "[-0.03819029 -0.17341777 -0.02608009  0.29950888] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.04165865  0.02206603 -0.02008991 -0.00128378] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.04121733  0.21747025 -0.02011559 -0.300237  ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03686792  0.02264071 -0.02612033 -0.0139654 ] 0 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03641511  0.21812733 -0.02639963 -0.31477381] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.03205256  0.4136152  -0.03269511 -0.61566402] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.02378026  0.60917833 -0.04500839 -0.9184626 ] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[-0.01159669  0.80487888 -0.06337764 -1.22494414] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.00450089  1.00075712 -0.08787653 -1.53679216] 1 1.0 False {}\n",
      "state, action, reward, done, info\n",
      "[ 0.02451603  1.19682015 -0.11861237 -1.85555436] 1 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    print('state, action, reward, done, info')\n",
    "    print(state, action, reward, done, info)\n",
    "    if done:\n",
    "        print('state, action, reward, done, info')\n",
    "        print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "1.1968201487563734 -1.8555543566278778\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Next states given\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Current actions given\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, next_states, actions, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder/Classifier\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1, training=True):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        #bn1 = tf.layers.batch_normalization(inputs=h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        #bn2 = tf.layers.batch_normalization(inputs=h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(actions, state_size, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=actions, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, hidden_size, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, action_size, hidden_size, actions, targetQs, state_size, next_states, alpha=0.1):\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(actions, action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    next_states_logits = generator(actions=actions_fake, state_size=state_size, hidden_size=hidden_size)\n",
    "    \n",
    "    # GAN: Discriminate between fake and real\n",
    "    next_states_fake = tf.sigmoid(x=next_states_logits)\n",
    "    d_logits_fake = discriminator(states=next_states_fake, hidden_size=hidden_size, reuse=False)\n",
    "    next_states_real = tf.sigmoid(x=next_states) \n",
    "    d_logits_real = discriminator(states=next_states_real, hidden_size=hidden_size, reuse=True)\n",
    "\n",
    "    # GAN: Adverserial training - G-learning\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "    \n",
    "    # GAN: Adverserial training - D-learning\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, g_loss, d_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.actions, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.g_loss, self.d_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, next_states=self.next_states, actions=self.actions, targetQs=self.targetQs) # model input data\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss, g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                                       learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 10                 # experience mini-batch size\n",
    "learning_rate = 0.001           # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = QGAN(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 5.0 Average reward fake: 0.4911949634552002 Average reward real: 0.49546584486961365 Training q_loss: 0.3094 Training g_loss: 0.7109 Training d_loss: 1.3780 Explore P: 0.9995\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 17.0 Average reward fake: 0.4959811568260193 Average reward real: 0.512540340423584 Training q_loss: 0.3046 Training g_loss: 0.7012 Training d_loss: 1.3566 Explore P: 0.9978\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 18.0 Average reward fake: 0.50539231300354 Average reward real: 0.48796653747558594 Training q_loss: 0.5050 Training g_loss: 0.6824 Training d_loss: 1.4304 Explore P: 0.9960\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 20.0 Average reward fake: 0.49933376908302307 Average reward real: 0.5064293146133423 Training q_loss: 0.8394 Training g_loss: 0.6945 Training d_loss: 1.3737 Explore P: 0.9941\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 16.0 Average reward fake: 0.4466969072818756 Average reward real: 0.5633696913719177 Training q_loss: 1.5439 Training g_loss: 0.8060 Training d_loss: 1.1695 Explore P: 0.9925\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 63.0 Average reward fake: 0.49794721603393555 Average reward real: 0.4031735956668854 Training q_loss: 2.1872 Training g_loss: 0.6977 Training d_loss: 1.6048 Explore P: 0.9863\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 15.0 Average reward fake: 0.44544029235839844 Average reward real: 0.4307142198085785 Training q_loss: 15.9004 Training g_loss: 0.8087 Training d_loss: 1.4326 Explore P: 0.9849\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 12.0 Average reward fake: 0.42236781120300293 Average reward real: 0.46288853883743286 Training q_loss: 1.9716 Training g_loss: 0.8614 Training d_loss: 1.3207 Explore P: 0.9837\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 20.0 Average reward fake: 0.406524658203125 Average reward real: 0.5937716364860535 Training q_loss: 0.8118 Training g_loss: 0.8945 Training d_loss: 1.0597 Explore P: 0.9818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 22.0 Average reward fake: 0.5236692428588867 Average reward real: 0.5583752989768982 Training q_loss: 19.3487 Training g_loss: 0.6471 Training d_loss: 1.3683 Explore P: 0.9796\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 18.0 Average reward fake: 0.5604289770126343 Average reward real: 0.38918012380599976 Training q_loss: 0.6015 Training g_loss: 0.5877 Training d_loss: 1.8118 Explore P: 0.9779\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 11.0 Average reward fake: 0.5666511654853821 Average reward real: 0.4719860553741455 Training q_loss: 0.8791 Training g_loss: 0.5707 Training d_loss: 1.6021 Explore P: 0.9768\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 23.0 Average reward fake: 0.49077311158180237 Average reward real: 0.5582249760627747 Training q_loss: 0.7095 Training g_loss: 0.7117 Training d_loss: 1.2589 Explore P: 0.9746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 16.0 Average reward fake: 0.4705725610256195 Average reward real: 0.6278768181800842 Training q_loss: 0.7055 Training g_loss: 0.7543 Training d_loss: 1.1130 Explore P: 0.9730\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 12.0 Average reward fake: 0.4596903920173645 Average reward real: 0.6090792417526245 Training q_loss: 0.7596 Training g_loss: 0.7763 Training d_loss: 1.1320 Explore P: 0.9719\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 11.0 Average reward fake: 0.401938259601593 Average reward real: 0.5558621287345886 Training q_loss: 53.4735 Training g_loss: 0.9186 Training d_loss: 1.1351 Explore P: 0.9708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 41.0 Average reward fake: 0.4295993447303772 Average reward real: 0.4099106192588806 Training q_loss: 10.9133 Training g_loss: 0.8448 Training d_loss: 1.4846 Explore P: 0.9669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 21.0 Average reward fake: 0.4202856123447418 Average reward real: 0.4473641514778137 Training q_loss: 0.2797 Training g_loss: 0.8663 Training d_loss: 1.3611 Explore P: 0.9649\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 13.0 Average reward fake: 0.4146840572357178 Average reward real: 0.46075353026390076 Training q_loss: 0.2481 Training g_loss: 0.8797 Training d_loss: 1.3290 Explore P: 0.9637\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 19.0 Average reward fake: 0.46640413999557495 Average reward real: 0.47239941358566284 Training q_loss: 0.5359 Training g_loss: 0.7630 Training d_loss: 1.3870 Explore P: 0.9618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 18.0 Average reward fake: 0.4865078032016754 Average reward real: 0.5789720416069031 Training q_loss: 6.4689 Training g_loss: 0.7227 Training d_loss: 1.2664 Explore P: 0.9601\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 10.0 Average reward fake: 0.6228958964347839 Average reward real: 0.677575945854187 Training q_loss: 25.5923 Training g_loss: 0.4737 Training d_loss: 1.4193 Explore P: 0.9592\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 14.0 Average reward fake: 0.6489761471748352 Average reward real: 0.5042853951454163 Training q_loss: 2.5740 Training g_loss: 0.4324 Training d_loss: 1.7884 Explore P: 0.9579\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 14.0 Average reward fake: 0.5327214002609253 Average reward real: 0.5390410423278809 Training q_loss: 0.7106 Training g_loss: 0.6298 Training d_loss: 1.3885 Explore P: 0.9565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 22.0 Average reward fake: 0.41923660039901733 Average reward real: 0.6555162668228149 Training q_loss: 8.1867 Training g_loss: 0.8695 Training d_loss: 0.9729 Explore P: 0.9545\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 23.0 Average reward fake: 0.4466458261013031 Average reward real: 0.49799561500549316 Training q_loss: 0.2586 Training g_loss: 0.8065 Training d_loss: 1.3946 Explore P: 0.9523\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 19.0 Average reward fake: 0.5246685743331909 Average reward real: 0.3767950236797333 Training q_loss: 0.6365 Training g_loss: 0.6452 Training d_loss: 1.8592 Explore P: 0.9505\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 16.0 Average reward fake: 0.5740712285041809 Average reward real: 0.36057668924331665 Training q_loss: 0.8642 Training g_loss: 0.5549 Training d_loss: 1.9571 Explore P: 0.9490\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 68.0 Average reward fake: 0.33375197649002075 Average reward real: 0.5128251910209656 Training q_loss: 9.0975 Training g_loss: 1.0973 Training d_loss: 1.0975 Explore P: 0.9426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 14.0 Average reward fake: 0.4114914536476135 Average reward real: 0.5792509317398071 Training q_loss: 0.7097 Training g_loss: 0.8885 Training d_loss: 1.1268 Explore P: 0.9413\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 23.0 Average reward fake: 0.3028329014778137 Average reward real: 0.5981783270835876 Training q_loss: 5.0268 Training g_loss: 1.1946 Training d_loss: 0.8959 Explore P: 0.9392\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 17.0 Average reward fake: 0.33345943689346313 Average reward real: 0.6214909553527832 Training q_loss: 0.2746 Training g_loss: 1.0988 Training d_loss: 0.9463 Explore P: 0.9376\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 36.0 Average reward fake: 0.5870163440704346 Average reward real: 0.4263584017753601 Training q_loss: 0.6198 Training g_loss: 0.5329 Training d_loss: 1.7514 Explore P: 0.9343\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 18.0 Average reward fake: 0.4100767970085144 Average reward real: 0.49118152260780334 Training q_loss: 0.4599 Training g_loss: 0.8917 Training d_loss: 1.2448 Explore P: 0.9326\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 23.0 Average reward fake: 0.4204707741737366 Average reward real: 0.5418586134910583 Training q_loss: 0.5419 Training g_loss: 0.8665 Training d_loss: 1.1735 Explore P: 0.9305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 22.0 Average reward fake: 0.33923274278640747 Average reward real: 0.6336262226104736 Training q_loss: 13.2070 Training g_loss: 1.0905 Training d_loss: 0.9119 Explore P: 0.9285\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 29.0 Average reward fake: 0.5197663307189941 Average reward real: 0.5667385458946228 Training q_loss: 4.4676 Training g_loss: 0.6557 Training d_loss: 1.4433 Explore P: 0.9258\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 18.0 Average reward fake: 0.6201655268669128 Average reward real: 0.5537453889846802 Training q_loss: 1.1241 Training g_loss: 0.4777 Training d_loss: 1.6245 Explore P: 0.9242\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 30.0 Average reward fake: 0.4449235498905182 Average reward real: 0.49500417709350586 Training q_loss: 0.9555 Training g_loss: 0.8409 Training d_loss: 1.3215 Explore P: 0.9214\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 14.0 Average reward fake: 0.5135790109634399 Average reward real: 0.5358947515487671 Training q_loss: 35.0821 Training g_loss: 0.6686 Training d_loss: 1.3628 Explore P: 0.9201\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 29.0 Average reward fake: 0.4425303041934967 Average reward real: 0.5012381672859192 Training q_loss: 1.7320 Training g_loss: 0.8213 Training d_loss: 1.3238 Explore P: 0.9175\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 13.0 Average reward fake: 0.4713592529296875 Average reward real: 0.5304224491119385 Training q_loss: 0.4747 Training g_loss: 0.7550 Training d_loss: 1.3106 Explore P: 0.9163\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 11.0 Average reward fake: 0.5168883204460144 Average reward real: 0.4251998960971832 Training q_loss: 1.9498 Training g_loss: 0.6595 Training d_loss: 1.6291 Explore P: 0.9153\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 40.0 Average reward fake: 0.5156403183937073 Average reward real: 0.4742439389228821 Training q_loss: 3.5158 Training g_loss: 0.6623 Training d_loss: 1.4751 Explore P: 0.9117\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 21.0 Average reward fake: 0.4872218668460846 Average reward real: 0.5038909912109375 Training q_loss: 1.3104 Training g_loss: 0.7190 Training d_loss: 1.3553 Explore P: 0.9098\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 12.0 Average reward fake: 0.48608213663101196 Average reward real: 0.5272933840751648 Training q_loss: 3.0508 Training g_loss: 0.7214 Training d_loss: 1.3087 Explore P: 0.9088\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 42.0 Average reward fake: 0.4756486415863037 Average reward real: 0.5056494474411011 Training q_loss: 0.8020 Training g_loss: 0.7428 Training d_loss: 1.3457 Explore P: 0.9050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 11.0 Average reward fake: 0.48656001687049866 Average reward real: 0.5017200112342834 Training q_loss: 1.0874 Training g_loss: 0.7201 Training d_loss: 1.3863 Explore P: 0.9040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 66.0 Average reward fake: 0.5041626691818237 Average reward real: 0.49893754720687866 Training q_loss: 1.8344 Training g_loss: 0.6860 Training d_loss: 1.4070 Explore P: 0.8981\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 12.0 Average reward fake: 0.4839301109313965 Average reward real: 0.5033736824989319 Training q_loss: 1.2808 Training g_loss: 0.7264 Training d_loss: 1.3530 Explore P: 0.8971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 41.0 Average reward fake: 0.4823026657104492 Average reward real: 0.4915785789489746 Training q_loss: 3.5580 Training g_loss: 0.7288 Training d_loss: 1.3866 Explore P: 0.8934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 41.0 Average reward fake: 0.4922401010990143 Average reward real: 0.5183646082878113 Training q_loss: 1.0614 Training g_loss: 0.7111 Training d_loss: 1.3514 Explore P: 0.8898\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 13.0 Average reward fake: 0.5112255811691284 Average reward real: 0.5024264454841614 Training q_loss: 2.0022 Training g_loss: 0.6716 Training d_loss: 1.4185 Explore P: 0.8887\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 55.0 Average reward fake: 0.5200510025024414 Average reward real: 0.4854075014591217 Training q_loss: 2.6264 Training g_loss: 0.6539 Training d_loss: 1.4608 Explore P: 0.8838\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 41.0 Average reward fake: 0.47047728300094604 Average reward real: 0.5517774820327759 Training q_loss: 2.2830 Training g_loss: 0.7565 Training d_loss: 1.2348 Explore P: 0.8803\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 22.0 Average reward fake: 0.4687311053276062 Average reward real: 0.5240963697433472 Training q_loss: 83.6209 Training g_loss: 0.7588 Training d_loss: 1.2862 Explore P: 0.8784\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 23.0 Average reward fake: 0.4782220721244812 Average reward real: 0.49058929085731506 Training q_loss: 2.2733 Training g_loss: 0.7371 Training d_loss: 1.3717 Explore P: 0.8764\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 16.0 Average reward fake: 0.5163997411727905 Average reward real: 0.4814806878566742 Training q_loss: 7.0855 Training g_loss: 0.6612 Training d_loss: 1.4693 Explore P: 0.8750\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 32.0 Average reward fake: 0.4663164019584656 Average reward real: 0.5196850299835205 Training q_loss: 29.5142 Training g_loss: 0.7628 Training d_loss: 1.2918 Explore P: 0.8722\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 54.0 Average reward fake: 0.46904826164245605 Average reward real: 0.5099896192550659 Training q_loss: 5.7865 Training g_loss: 0.7568 Training d_loss: 1.3148 Explore P: 0.8676\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 17.0 Average reward fake: 0.4868357181549072 Average reward real: 0.5097790360450745 Training q_loss: 4.1524 Training g_loss: 0.7198 Training d_loss: 1.3505 Explore P: 0.8661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 11.0 Average reward fake: 0.5040264129638672 Average reward real: 0.4909523129463196 Training q_loss: 4.3196 Training g_loss: 0.6855 Training d_loss: 1.4218 Explore P: 0.8652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 67.0 Average reward fake: 0.44879335165023804 Average reward real: 0.487179696559906 Training q_loss: 3.5822 Training g_loss: 0.8010 Training d_loss: 1.3214 Explore P: 0.8595\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 12.0 Average reward fake: 0.47866350412368774 Average reward real: 0.5564621090888977 Training q_loss: 46.9002 Training g_loss: 0.7370 Training d_loss: 1.2546 Explore P: 0.8584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 12.0 Average reward fake: 0.46632108092308044 Average reward real: 0.4941749572753906 Training q_loss: 2.5678 Training g_loss: 0.7638 Training d_loss: 1.3407 Explore P: 0.8574\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 26.0 Average reward fake: 0.49713563919067383 Average reward real: 0.5183811187744141 Training q_loss: 10.4183 Training g_loss: 0.6991 Training d_loss: 1.3609 Explore P: 0.8552\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 33.0 Average reward fake: 0.41833072900772095 Average reward real: 0.5450245141983032 Training q_loss: 52.0192 Training g_loss: 0.8722 Training d_loss: 1.1587 Explore P: 0.8524\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 23.0 Average reward fake: 0.44539475440979004 Average reward real: 0.5315810441970825 Training q_loss: 5.6266 Training g_loss: 0.8155 Training d_loss: 1.2638 Explore P: 0.8505\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 35.0 Average reward fake: 0.5474161505699158 Average reward real: 0.544734537601471 Training q_loss: 52.1272 Training g_loss: 0.6880 Training d_loss: 1.5455 Explore P: 0.8476\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 64.0 Average reward fake: 0.43267518281936646 Average reward real: 0.5471369624137878 Training q_loss: 61.1912 Training g_loss: 0.9208 Training d_loss: 1.2282 Explore P: 0.8422\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 27.0 Average reward fake: 0.43584972620010376 Average reward real: 0.46399807929992676 Training q_loss: 6.1177 Training g_loss: 0.8459 Training d_loss: 1.3793 Explore P: 0.8400\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 15.0 Average reward fake: 0.5244380235671997 Average reward real: 0.4906739294528961 Training q_loss: 10.7452 Training g_loss: 0.6479 Training d_loss: 1.4854 Explore P: 0.8387\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 105.0 Average reward fake: 0.5232644081115723 Average reward real: 0.4140723645687103 Training q_loss: 97.6218 Training g_loss: 0.6478 Training d_loss: 1.6500 Explore P: 0.8301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 18.0 Average reward fake: 0.5414900779724121 Average reward real: 0.4500257074832916 Training q_loss: 3.5275 Training g_loss: 0.6136 Training d_loss: 1.5890 Explore P: 0.8286\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 11.0 Average reward fake: 0.5439926385879517 Average reward real: 0.4732224941253662 Training q_loss: 2.0300 Training g_loss: 0.6088 Training d_loss: 1.5383 Explore P: 0.8277\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 10.0 Average reward fake: 0.5348724126815796 Average reward real: 0.4960847496986389 Training q_loss: 17.6167 Training g_loss: 0.6260 Training d_loss: 1.4695 Explore P: 0.8269\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 18.0 Average reward fake: 0.5091518759727478 Average reward real: 0.5112931132316589 Training q_loss: 4.7054 Training g_loss: 0.6750 Training d_loss: 1.3834 Explore P: 0.8254\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 31.0 Average reward fake: 0.45985180139541626 Average reward real: 0.5586299896240234 Training q_loss: 7.7153 Training g_loss: 0.7776 Training d_loss: 1.2102 Explore P: 0.8229\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 37.0 Average reward fake: 0.47222739458084106 Average reward real: 0.5229369401931763 Training q_loss: 1.6834 Training g_loss: 0.7508 Training d_loss: 1.3203 Explore P: 0.8199\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 36.0 Average reward fake: 0.5020920038223267 Average reward real: 0.5123673677444458 Training q_loss: 4.9303 Training g_loss: 0.6896 Training d_loss: 1.3960 Explore P: 0.8170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 35.0 Average reward fake: 0.43997764587402344 Average reward real: 0.5205199122428894 Training q_loss: 157.6073 Training g_loss: 0.8225 Training d_loss: 1.2656 Explore P: 0.8142\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 38.0 Average reward fake: 0.4150005877017975 Average reward real: 0.5820567011833191 Training q_loss: 8.7414 Training g_loss: 0.8795 Training d_loss: 1.1014 Explore P: 0.8111\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 27.0 Average reward fake: 0.4487605094909668 Average reward real: 0.5373793840408325 Training q_loss: 5.1943 Training g_loss: 0.8032 Training d_loss: 1.2402 Explore P: 0.8090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 40.0 Average reward fake: 0.36348313093185425 Average reward real: 0.5532218217849731 Training q_loss: 1.7516 Training g_loss: 1.0153 Training d_loss: 1.0717 Explore P: 0.8058\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 85.0 Average reward fake: 0.46576324105262756 Average reward real: 0.5097317695617676 Training q_loss: 2.2520 Training g_loss: 0.7632 Training d_loss: 1.3611 Explore P: 0.7990\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 68.0 Average reward fake: 0.5112733840942383 Average reward real: 0.3612794280052185 Training q_loss: 5.5810 Training g_loss: 0.6718 Training d_loss: 1.8024 Explore P: 0.7937\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 17.0 Average reward fake: 0.42026057839393616 Average reward real: 0.5232344269752502 Training q_loss: 7.3488 Training g_loss: 0.8685 Training d_loss: 1.2260 Explore P: 0.7924\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 19.0 Average reward fake: 0.45248574018478394 Average reward real: 0.5450869798660278 Training q_loss: 6.7557 Training g_loss: 0.7970 Training d_loss: 1.2212 Explore P: 0.7909\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 24.0 Average reward fake: 0.5556181073188782 Average reward real: 0.5145726799964905 Training q_loss: 5.0164 Training g_loss: 0.5877 Training d_loss: 1.5085 Explore P: 0.7890\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 37.0 Average reward fake: 0.4402640759944916 Average reward real: 0.44858425855636597 Training q_loss: 101.9739 Training g_loss: 0.8261 Training d_loss: 1.4282 Explore P: 0.7861\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 44.0 Average reward fake: 0.4585399031639099 Average reward real: 0.5523644685745239 Training q_loss: 4.1736 Training g_loss: 0.7802 Training d_loss: 1.2316 Explore P: 0.7827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 21.0 Average reward fake: 0.4499737620353699 Average reward real: 0.4545285701751709 Training q_loss: 5.0970 Training g_loss: 0.7993 Training d_loss: 1.4395 Explore P: 0.7811\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 19.0 Average reward fake: 0.5019701719284058 Average reward real: 0.4528627395629883 Training q_loss: 6.4686 Training g_loss: 0.6899 Training d_loss: 1.5464 Explore P: 0.7796\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 20.0 Average reward fake: 0.5348923802375793 Average reward real: 0.5937634706497192 Training q_loss: 4.2219 Training g_loss: 0.6263 Training d_loss: 1.3213 Explore P: 0.7781\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 56.0 Average reward fake: 0.3999108672142029 Average reward real: 0.6247643232345581 Training q_loss: 4.5255 Training g_loss: 0.9170 Training d_loss: 1.0271 Explore P: 0.7738\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 59.0 Average reward fake: 0.40885719656944275 Average reward real: 0.5179274678230286 Training q_loss: 6.4070 Training g_loss: 0.8944 Training d_loss: 1.1998 Explore P: 0.7693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 60.0 Average reward fake: 0.5483776330947876 Average reward real: 0.4127754271030426 Training q_loss: 40.3711 Training g_loss: 0.6013 Training d_loss: 1.8452 Explore P: 0.7648\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 10.0 Average reward fake: 0.5894306898117065 Average reward real: 0.45406198501586914 Training q_loss: 6.7419 Training g_loss: 0.5296 Training d_loss: 1.8022 Explore P: 0.7640\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 68.0 Average reward fake: 0.44465574622154236 Average reward real: 0.5972718596458435 Training q_loss: 158.3142 Training g_loss: 0.8108 Training d_loss: 1.1111 Explore P: 0.7589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 25.0 Average reward fake: 0.4281657338142395 Average reward real: 0.6130732297897339 Training q_loss: 4.7920 Training g_loss: 0.8481 Training d_loss: 1.0883 Explore P: 0.7570\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 26.0 Average reward fake: 0.4783051013946533 Average reward real: 0.5766245126724243 Training q_loss: 153.7845 Training g_loss: 0.7376 Training d_loss: 1.2891 Explore P: 0.7551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 48.0 Average reward fake: 0.4986036717891693 Average reward real: 0.5582637190818787 Training q_loss: 121.1344 Training g_loss: 0.6959 Training d_loss: 1.2933 Explore P: 0.7515\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 50.0 Average reward fake: 0.5097198486328125 Average reward real: 0.5589531064033508 Training q_loss: 8.7246 Training g_loss: 0.6737 Training d_loss: 1.3068 Explore P: 0.7478\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 57.0 Average reward fake: 0.4874456524848938 Average reward real: 0.49844756722450256 Training q_loss: 4.9159 Training g_loss: 0.7186 Training d_loss: 1.3766 Explore P: 0.7436\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 90.0 Average reward fake: 0.47543102502822876 Average reward real: 0.4253396987915039 Training q_loss: 8.5849 Training g_loss: 0.7425 Training d_loss: 1.5199 Explore P: 0.7371\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 69.0 Average reward fake: 0.4671894907951355 Average reward real: 0.45887112617492676 Training q_loss: 30.9362 Training g_loss: 0.7579 Training d_loss: 1.4220 Explore P: 0.7321\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 22.0 Average reward fake: 0.40372520685195923 Average reward real: 0.500163197517395 Training q_loss: 216.8941 Training g_loss: 0.9171 Training d_loss: 1.2534 Explore P: 0.7305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 31.0 Average reward fake: 0.4660366177558899 Average reward real: 0.48549309372901917 Training q_loss: 7.4365 Training g_loss: 0.7635 Training d_loss: 1.3694 Explore P: 0.7282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 70.0 Average reward fake: 0.4572792053222656 Average reward real: 0.5457742214202881 Training q_loss: 648.8378 Training g_loss: 0.7854 Training d_loss: 1.2724 Explore P: 0.7232\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 55.0 Average reward fake: 0.3530174493789673 Average reward real: 0.7390755414962769 Training q_loss: 5.3913 Training g_loss: 1.1247 Training d_loss: 0.8176 Explore P: 0.7193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 19.0 Average reward fake: 0.3416709303855896 Average reward real: 0.511227548122406 Training q_loss: 30.5938 Training g_loss: 1.2735 Training d_loss: 1.2466 Explore P: 0.7180\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 62.0 Average reward fake: 0.4114597737789154 Average reward real: 0.4960193634033203 Training q_loss: 404.4691 Training g_loss: 0.8973 Training d_loss: 1.2652 Explore P: 0.7136\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 66.0 Average reward fake: 0.37081634998321533 Average reward real: 0.41209182143211365 Training q_loss: 6.2617 Training g_loss: 0.9981 Training d_loss: 1.3978 Explore P: 0.7090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 14.0 Average reward fake: 0.3553047776222229 Average reward real: 0.5166593194007874 Training q_loss: 17.4782 Training g_loss: 1.0537 Training d_loss: 1.1268 Explore P: 0.7080\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 53.0 Average reward fake: 0.527906060218811 Average reward real: 0.5164926648139954 Training q_loss: 40.9084 Training g_loss: 0.6389 Training d_loss: 1.6845 Explore P: 0.7043\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 52.0 Average reward fake: 0.5434076189994812 Average reward real: 0.480551540851593 Training q_loss: 34.3842 Training g_loss: 0.6104 Training d_loss: 1.5568 Explore P: 0.7007\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 89.0 Average reward fake: 0.5227254629135132 Average reward real: 0.554332435131073 Training q_loss: 70.7340 Training g_loss: 0.6480 Training d_loss: 1.3547 Explore P: 0.6946\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 43.0 Average reward fake: 0.4839090406894684 Average reward real: 0.5834170579910278 Training q_loss: 18.7388 Training g_loss: 0.7271 Training d_loss: 1.2171 Explore P: 0.6916\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 98.0 Average reward fake: 0.5019086599349976 Average reward real: 0.46038565039634705 Training q_loss: 99.8586 Training g_loss: 0.6898 Training d_loss: 1.4844 Explore P: 0.6850\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 13.0 Average reward fake: 0.42800673842430115 Average reward real: 0.42703142762184143 Training q_loss: 470.1314 Training g_loss: 0.8493 Training d_loss: 1.4302 Explore P: 0.6841\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 17.0 Average reward fake: 0.43610548973083496 Average reward real: 0.43109211325645447 Training q_loss: 947.1180 Training g_loss: 0.8299 Training d_loss: 1.4229 Explore P: 0.6830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 19.0 Average reward fake: 0.37281402945518494 Average reward real: 0.4752922058105469 Training q_loss: 48.5666 Training g_loss: 0.9867 Training d_loss: 1.2206 Explore P: 0.6817\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 34.0 Average reward fake: 0.37504902482032776 Average reward real: 0.5384075045585632 Training q_loss: 32.3731 Training g_loss: 0.9866 Training d_loss: 1.1129 Explore P: 0.6794\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 63.0 Average reward fake: 0.30742955207824707 Average reward real: 0.6484450101852417 Training q_loss: 472.1427 Training g_loss: 1.4661 Training d_loss: 0.9417 Explore P: 0.6752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 58.0 Average reward fake: 0.506374180316925 Average reward real: 0.4241340756416321 Training q_loss: 45.3460 Training g_loss: 0.6806 Training d_loss: 1.5834 Explore P: 0.6714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 56.0 Average reward fake: 0.4954710006713867 Average reward real: 0.502149224281311 Training q_loss: 52.4667 Training g_loss: 0.7024 Training d_loss: 1.3773 Explore P: 0.6677\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 44.0 Average reward fake: 0.4790407121181488 Average reward real: 0.5106187462806702 Training q_loss: 568.8334 Training g_loss: 0.7361 Training d_loss: 1.3399 Explore P: 0.6648\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 9.0 Average reward fake: 0.49165043234825134 Average reward real: 0.501181423664093 Training q_loss: 34.8601 Training g_loss: 0.7107 Training d_loss: 1.3868 Explore P: 0.6642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 169.0 Average reward fake: 0.5303869843482971 Average reward real: 0.4342009127140045 Training q_loss: 99.0227 Training g_loss: 0.6342 Training d_loss: 1.6332 Explore P: 0.6532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 69.0 Average reward fake: 0.5036571621894836 Average reward real: 0.49097996950149536 Training q_loss: 208.9958 Training g_loss: 0.6859 Training d_loss: 1.4122 Explore P: 0.6488\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 14.0 Average reward fake: 0.48704394698143005 Average reward real: 0.5149742364883423 Training q_loss: 66.1333 Training g_loss: 0.7194 Training d_loss: 1.3321 Explore P: 0.6479\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 50.0 Average reward fake: 0.46393394470214844 Average reward real: 0.47692275047302246 Training q_loss: 63.2309 Training g_loss: 0.7681 Training d_loss: 1.3730 Explore P: 0.6447\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 122.0 Average reward fake: 0.505510687828064 Average reward real: 0.5509161353111267 Training q_loss: 80.3917 Training g_loss: 0.6823 Training d_loss: 1.3206 Explore P: 0.6370\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 34.0 Average reward fake: 0.5433067083358765 Average reward real: 0.4915573000907898 Training q_loss: 1759.5879 Training g_loss: 0.6104 Training d_loss: 1.5082 Explore P: 0.6349\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 76.0 Average reward fake: 0.44306057691574097 Average reward real: 0.5321980714797974 Training q_loss: 108.7410 Training g_loss: 0.8146 Training d_loss: 1.2788 Explore P: 0.6302\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 12.0 Average reward fake: 0.47829675674438477 Average reward real: 0.4845758080482483 Training q_loss: 97.1186 Training g_loss: 0.7378 Training d_loss: 1.4133 Explore P: 0.6294\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 29.0 Average reward fake: 0.5438891649246216 Average reward real: 0.5215384364128113 Training q_loss: 1638.6074 Training g_loss: 0.6093 Training d_loss: 1.4603 Explore P: 0.6276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 117.0 Average reward fake: 0.5042369365692139 Average reward real: 0.4885319173336029 Training q_loss: 863.0994 Training g_loss: 0.6847 Training d_loss: 1.4192 Explore P: 0.6205\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 56.0 Average reward fake: 0.44906502962112427 Average reward real: 0.5033008456230164 Training q_loss: 78.3934 Training g_loss: 0.8006 Training d_loss: 1.2910 Explore P: 0.6170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 64.0 Average reward fake: 0.42845600843429565 Average reward real: 0.5014148950576782 Training q_loss: 2270.6467 Training g_loss: 0.8477 Training d_loss: 1.2640 Explore P: 0.6132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 78.0 Average reward fake: 0.49683746695518494 Average reward real: 0.4908638894557953 Training q_loss: 74.3347 Training g_loss: 0.7006 Training d_loss: 1.4041 Explore P: 0.6085\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 40.0 Average reward fake: 0.5300401449203491 Average reward real: 0.49729499220848083 Training q_loss: 188.4491 Training g_loss: 0.6364 Training d_loss: 1.4632 Explore P: 0.6061\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 32.0 Average reward fake: 0.5298946499824524 Average reward real: 0.529443085193634 Training q_loss: 155.1219 Training g_loss: 0.6351 Training d_loss: 1.3931 Explore P: 0.6042\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 19.0 Average reward fake: 0.4887773394584656 Average reward real: 0.5163935422897339 Training q_loss: 19.4033 Training g_loss: 0.7158 Training d_loss: 1.3353 Explore P: 0.6031\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 57.0 Average reward fake: 0.49625954031944275 Average reward real: 0.5266695022583008 Training q_loss: 81.1766 Training g_loss: 0.7016 Training d_loss: 1.3318 Explore P: 0.5997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 61.0 Average reward fake: 0.5051642656326294 Average reward real: 0.5520389080047607 Training q_loss: 91.5993 Training g_loss: 0.6826 Training d_loss: 1.3038 Explore P: 0.5961\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 38.0 Average reward fake: 0.5050441026687622 Average reward real: 0.5649943351745605 Training q_loss: 7178.2983 Training g_loss: 0.6851 Training d_loss: 1.2891 Explore P: 0.5939\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 57.0 Average reward fake: 0.4949916899204254 Average reward real: 0.5255316495895386 Training q_loss: 83.9286 Training g_loss: 0.7073 Training d_loss: 1.3472 Explore P: 0.5906\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 64.0 Average reward fake: 0.4917601943016052 Average reward real: 0.475058376789093 Training q_loss: 374.7153 Training g_loss: 0.7107 Training d_loss: 1.4268 Explore P: 0.5869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 83.0 Average reward fake: 0.4685094952583313 Average reward real: 0.4692573547363281 Training q_loss: 738.1765 Training g_loss: 0.7585 Training d_loss: 1.3989 Explore P: 0.5821\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 59.0 Average reward fake: 0.46477431058883667 Average reward real: 0.5275906324386597 Training q_loss: 504.6580 Training g_loss: 0.7694 Training d_loss: 1.2747 Explore P: 0.5787\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 130.0 Average reward fake: 0.5194383263587952 Average reward real: 0.4961238503456116 Training q_loss: 179.2764 Training g_loss: 0.7323 Training d_loss: 1.5915 Explore P: 0.5714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 56.0 Average reward fake: 0.5171611309051514 Average reward real: 0.47079944610595703 Training q_loss: 76.3957 Training g_loss: 0.6601 Training d_loss: 1.5064 Explore P: 0.5682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 29.0 Average reward fake: 0.49858418107032776 Average reward real: 0.5857210755348206 Training q_loss: 136.5250 Training g_loss: 0.6980 Training d_loss: 1.2575 Explore P: 0.5666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 47.0 Average reward fake: 0.5383108854293823 Average reward real: 0.5372358560562134 Training q_loss: 318.9319 Training g_loss: 0.6383 Training d_loss: 1.4570 Explore P: 0.5640\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 140.0 Average reward fake: 0.4989206790924072 Average reward real: 0.4405214786529541 Training q_loss: 213.0731 Training g_loss: 0.6953 Training d_loss: 1.5208 Explore P: 0.5563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 88.0 Average reward fake: 0.5595206022262573 Average reward real: 0.5096694231033325 Training q_loss: 239.7375 Training g_loss: 0.5878 Training d_loss: 1.5473 Explore P: 0.5515\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 131.0 Average reward fake: 0.4901483654975891 Average reward real: 0.4648555815219879 Training q_loss: 173.2674 Training g_loss: 0.7130 Training d_loss: 1.4408 Explore P: 0.5445\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 40.0 Average reward fake: 0.47024983167648315 Average reward real: 0.4972550868988037 Training q_loss: 43.7764 Training g_loss: 0.7548 Training d_loss: 1.3373 Explore P: 0.5424\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 35.0 Average reward fake: 0.44950565695762634 Average reward real: 0.5346137881278992 Training q_loss: 112.6940 Training g_loss: 0.8001 Training d_loss: 1.2342 Explore P: 0.5405\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 113.0 Average reward fake: 0.4998781085014343 Average reward real: 0.4710633158683777 Training q_loss: 206.5994 Training g_loss: 0.7081 Training d_loss: 1.4945 Explore P: 0.5345\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 126.0 Average reward fake: 0.4141518175601959 Average reward real: 0.5758973360061646 Training q_loss: 221.0649 Training g_loss: 0.8876 Training d_loss: 1.1331 Explore P: 0.5280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 84.0 Average reward fake: 0.41683053970336914 Average reward real: 0.6590596437454224 Training q_loss: 2340.9507 Training g_loss: 0.9465 Training d_loss: 1.0654 Explore P: 0.5236\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 141.0 Average reward fake: 0.38079506158828735 Average reward real: 0.47781530022621155 Training q_loss: 658.2645 Training g_loss: 0.9670 Training d_loss: 1.3079 Explore P: 0.5164\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 186.0 Average reward fake: 0.6472949385643005 Average reward real: 0.4223349690437317 Training q_loss: 50.8579 Training g_loss: 0.4349 Training d_loss: 1.9339 Explore P: 0.5071\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 148.0 Average reward fake: 0.44159308075904846 Average reward real: 0.4400893747806549 Training q_loss: 158.6344 Training g_loss: 0.8174 Training d_loss: 1.4272 Explore P: 0.4998\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 155.0 Average reward fake: 0.4453219473361969 Average reward real: 0.5159353017807007 Training q_loss: 147.7258 Training g_loss: 0.8090 Training d_loss: 1.2680 Explore P: 0.4923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 32.0 Average reward fake: 0.4521175026893616 Average reward real: 0.5805344581604004 Training q_loss: 186.6505 Training g_loss: 0.7938 Training d_loss: 1.1654 Explore P: 0.4907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 98.0 Average reward fake: 0.5421864986419678 Average reward real: 0.45211154222488403 Training q_loss: 422.1931 Training g_loss: 0.6123 Training d_loss: 1.5874 Explore P: 0.4860\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 127.0 Average reward fake: 0.3925638198852539 Average reward real: 0.5193981528282166 Training q_loss: 4880.4072 Training g_loss: 0.9356 Training d_loss: 1.1694 Explore P: 0.4800\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 199.0 Average reward fake: 0.4574979841709137 Average reward real: 0.436168372631073 Training q_loss: 133.3243 Training g_loss: 0.7821 Training d_loss: 1.4887 Explore P: 0.4708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 139.0 Average reward fake: 0.5410236716270447 Average reward real: 0.5366552472114563 Training q_loss: 139.7220 Training g_loss: 0.6143 Training d_loss: 1.4086 Explore P: 0.4644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 196.0 Average reward fake: 0.4605835974216461 Average reward real: 0.4681849479675293 Training q_loss: 281.9554 Training g_loss: 0.7753 Training d_loss: 1.3774 Explore P: 0.4556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 169.0 Average reward fake: 0.5209104418754578 Average reward real: 0.4883880615234375 Training q_loss: 663.5290 Training g_loss: 0.6522 Training d_loss: 1.4576 Explore P: 0.4481\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 199.0 Average reward fake: 0.5202329754829407 Average reward real: 0.48801231384277344 Training q_loss: 790.0453 Training g_loss: 0.6535 Training d_loss: 1.4547 Explore P: 0.4395\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 11.0 Average reward fake: 0.5116917490959167 Average reward real: 0.4770774841308594 Training q_loss: 464.7415 Training g_loss: 0.6700 Training d_loss: 1.4624 Explore P: 0.4390\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 132.0 Average reward fake: 0.5124067068099976 Average reward real: 0.36084043979644775 Training q_loss: 624.3539 Training g_loss: 0.6700 Training d_loss: 1.7919 Explore P: 0.4334\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 119.0 Average reward fake: 0.4768773913383484 Average reward real: 0.46232113242149353 Training q_loss: 734.1613 Training g_loss: 0.7405 Training d_loss: 1.4747 Explore P: 0.4284\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 135.0 Average reward fake: 0.5161125659942627 Average reward real: 0.5055878758430481 Training q_loss: 68.2228 Training g_loss: 0.6614 Training d_loss: 1.4201 Explore P: 0.4228\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 173.0 Average reward fake: 0.4716709554195404 Average reward real: 0.555012583732605 Training q_loss: 36631.5703 Training g_loss: 0.7518 Training d_loss: 1.2358 Explore P: 0.4157\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 199.0 Average reward fake: 0.53581303358078 Average reward real: 0.4812636375427246 Training q_loss: 57.5918 Training g_loss: 0.6274 Training d_loss: 1.5404 Explore P: 0.4077\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 198.0 Average reward fake: 0.4997938275337219 Average reward real: 0.5102552175521851 Training q_loss: 3330.1304 Training g_loss: 0.6956 Training d_loss: 1.3814 Explore P: 0.3999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 199.0 Average reward fake: 0.507828414440155 Average reward real: 0.5200442671775818 Training q_loss: 1766.7152 Training g_loss: 0.6776 Training d_loss: 1.3650 Explore P: 0.3922\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 190.0 Average reward fake: 0.45800405740737915 Average reward real: 0.481947124004364 Training q_loss: 303.3430 Training g_loss: 0.7812 Training d_loss: 1.3477 Explore P: 0.3850\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 199.0 Average reward fake: 0.49566346406936646 Average reward real: 0.4876480996608734 Training q_loss: 953.2365 Training g_loss: 0.7056 Training d_loss: 1.4636 Explore P: 0.3776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 199.0 Average reward fake: 0.46586543321609497 Average reward real: 0.5174630880355835 Training q_loss: 106.4250 Training g_loss: 0.7747 Training d_loss: 1.3058 Explore P: 0.3704\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 199.0 Average reward fake: 0.4870850145816803 Average reward real: 0.49893802404403687 Training q_loss: 616.8458 Training g_loss: 0.7195 Training d_loss: 1.3841 Explore P: 0.3633\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 181.0 Average reward fake: 0.33034661412239075 Average reward real: 0.4579358696937561 Training q_loss: 231.7555 Training g_loss: 1.5183 Training d_loss: 1.4355 Explore P: 0.3570\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 199.0 Average reward fake: 0.47992008924484253 Average reward real: 0.5462251305580139 Training q_loss: 317.7551 Training g_loss: 0.7344 Training d_loss: 1.2662 Explore P: 0.3501\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 199.0 Average reward fake: 0.4507991671562195 Average reward real: 0.5540921092033386 Training q_loss: 33.4379 Training g_loss: 0.7964 Training d_loss: 1.2231 Explore P: 0.3434\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 199.0 Average reward fake: 0.5814265012741089 Average reward real: 0.5086344480514526 Training q_loss: 410.7355 Training g_loss: 0.5424 Training d_loss: 1.5662 Explore P: 0.3369\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 79.0 Average reward fake: 0.39848166704177856 Average reward real: 0.5012222528457642 Training q_loss: 502.7266 Training g_loss: 0.9202 Training d_loss: 1.2895 Explore P: 0.3343\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 199.0 Average reward fake: 0.5183472633361816 Average reward real: 0.5004042387008667 Training q_loss: 389.1462 Training g_loss: 0.6587 Training d_loss: 1.4457 Explore P: 0.3279\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 103.0 Average reward fake: 0.49108511209487915 Average reward real: 0.524325966835022 Training q_loss: 22123.6191 Training g_loss: 0.7111 Training d_loss: 1.3294 Explore P: 0.3246\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 199.0 Average reward fake: 0.5165842175483704 Average reward real: 0.5327369570732117 Training q_loss: 85394.5938 Training g_loss: 0.6691 Training d_loss: 1.3880 Explore P: 0.3184\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 199.0 Average reward fake: 0.5097761154174805 Average reward real: 0.5232886075973511 Training q_loss: 492.1709 Training g_loss: 0.6740 Training d_loss: 1.3872 Explore P: 0.3124\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 199.0 Average reward fake: 0.5146786570549011 Average reward real: 0.5524115562438965 Training q_loss: 671.7770 Training g_loss: 0.6648 Training d_loss: 1.3269 Explore P: 0.3064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 199.0 Average reward fake: 0.44177693128585815 Average reward real: 0.535175621509552 Training q_loss: 436.7573 Training g_loss: 0.8166 Training d_loss: 1.2214 Explore P: 0.3006\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 199.0 Average reward fake: 0.4878126084804535 Average reward real: 0.5114348530769348 Training q_loss: 271.6246 Training g_loss: 0.7180 Training d_loss: 1.3450 Explore P: 0.2948\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 175.0 Average reward fake: 0.4031294286251068 Average reward real: 0.5719311237335205 Training q_loss: 467.8135 Training g_loss: 0.9085 Training d_loss: 1.0804 Explore P: 0.2899\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 199.0 Average reward fake: 0.45656833052635193 Average reward real: 0.5155442953109741 Training q_loss: 267.6376 Training g_loss: 0.7840 Training d_loss: 1.2858 Explore P: 0.2844\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 199.0 Average reward fake: 0.45904022455215454 Average reward real: 0.5107582807540894 Training q_loss: 247.8462 Training g_loss: 0.7790 Training d_loss: 1.3277 Explore P: 0.2790\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 199.0 Average reward fake: 0.5409650802612305 Average reward real: 0.49656540155410767 Training q_loss: 253.7642 Training g_loss: 0.6144 Training d_loss: 1.4910 Explore P: 0.2737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 199.0 Average reward fake: 0.4886224865913391 Average reward real: 0.5149381756782532 Training q_loss: 250.0011 Training g_loss: 0.7162 Training d_loss: 1.3389 Explore P: 0.2685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 199.0 Average reward fake: 0.534142792224884 Average reward real: 0.49727073311805725 Training q_loss: 262.5607 Training g_loss: 0.6271 Training d_loss: 1.4827 Explore P: 0.2634\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 199.0 Average reward fake: 0.48699039220809937 Average reward real: 0.517302393913269 Training q_loss: 930.5610 Training g_loss: 0.7199 Training d_loss: 1.3454 Explore P: 0.2584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 199.0 Average reward fake: 0.514384925365448 Average reward real: 0.429166316986084 Training q_loss: 571.0003 Training g_loss: 0.6648 Training d_loss: 1.5729 Explore P: 0.2535\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 144.0 Average reward fake: 0.48876771330833435 Average reward real: 0.5569807291030884 Training q_loss: 420.5563 Training g_loss: 0.7159 Training d_loss: 1.2603 Explore P: 0.2500\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 199.0 Average reward fake: 0.4859894812107086 Average reward real: 0.4985516667366028 Training q_loss: 186.4117 Training g_loss: 0.7216 Training d_loss: 1.3870 Explore P: 0.2453\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 199.0 Average reward fake: 0.5588909387588501 Average reward real: 0.4963862895965576 Training q_loss: 149.8018 Training g_loss: 0.5818 Training d_loss: 1.5263 Explore P: 0.2407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 199.0 Average reward fake: 0.5094761848449707 Average reward real: 0.43375682830810547 Training q_loss: 276.5426 Training g_loss: 0.6744 Training d_loss: 1.5534 Explore P: 0.2361\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 183.0 Average reward fake: 0.4339829385280609 Average reward real: 0.5859285593032837 Training q_loss: 437.6127 Training g_loss: 0.8349 Training d_loss: 1.1208 Explore P: 0.2320\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 199.0 Average reward fake: 0.5666765570640564 Average reward real: 0.44752416014671326 Training q_loss: 315.1105 Training g_loss: 0.5681 Training d_loss: 1.6595 Explore P: 0.2276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 189.0 Average reward fake: 0.4856216013431549 Average reward real: 0.5432888865470886 Training q_loss: 391.7365 Training g_loss: 0.7223 Training d_loss: 1.2816 Explore P: 0.2236\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 199.0 Average reward fake: 0.504989504814148 Average reward real: 0.39940744638442993 Training q_loss: 110.8356 Training g_loss: 0.6832 Training d_loss: 1.6314 Explore P: 0.2193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 199.0 Average reward fake: 0.46629905700683594 Average reward real: 0.412409245967865 Training q_loss: 410.4308 Training g_loss: 0.7627 Training d_loss: 1.6324 Explore P: 0.2152\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 199.0 Average reward fake: 0.4816541075706482 Average reward real: 0.5619375705718994 Training q_loss: 75.4010 Training g_loss: 0.7308 Training d_loss: 1.2456 Explore P: 0.2112\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 199.0 Average reward fake: 0.5115373730659485 Average reward real: 0.5117425918579102 Training q_loss: 169.4821 Training g_loss: 0.6704 Training d_loss: 1.3994 Explore P: 0.2072\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 199.0 Average reward fake: 0.501194417476654 Average reward real: 0.48301205039024353 Training q_loss: 27.2933 Training g_loss: 0.6908 Training d_loss: 1.4289 Explore P: 0.2033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 199.0 Average reward fake: 0.4940611720085144 Average reward real: 0.46866464614868164 Training q_loss: 209.6622 Training g_loss: 0.7052 Training d_loss: 1.4407 Explore P: 0.1995\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 199.0 Average reward fake: 0.49966055154800415 Average reward real: 0.5566681623458862 Training q_loss: 95.5971 Training g_loss: 0.6939 Training d_loss: 1.2842 Explore P: 0.1958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 194.0 Average reward fake: 0.4780117869377136 Average reward real: 0.48161202669143677 Training q_loss: 102.1758 Training g_loss: 0.7382 Training d_loss: 1.3852 Explore P: 0.1922\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 180.0 Average reward fake: 0.5081629157066345 Average reward real: 0.5165607929229736 Training q_loss: 49.1604 Training g_loss: 0.6770 Training d_loss: 1.3757 Explore P: 0.1890\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 199.0 Average reward fake: 0.5647186040878296 Average reward real: 0.5522885918617249 Training q_loss: 258.8908 Training g_loss: 0.5714 Training d_loss: 1.4281 Explore P: 0.1854\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 199.0 Average reward fake: 0.4946984350681305 Average reward real: 0.47702282667160034 Training q_loss: 16.6576 Training g_loss: 0.7038 Training d_loss: 1.4259 Explore P: 0.1820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 172.0 Average reward fake: 0.44431957602500916 Average reward real: 0.5168825387954712 Training q_loss: 67.9082 Training g_loss: 0.8118 Training d_loss: 1.2655 Explore P: 0.1791\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 199.0 Average reward fake: 0.44730091094970703 Average reward real: 0.49894848465919495 Training q_loss: 43.9878 Training g_loss: 0.8089 Training d_loss: 1.3066 Explore P: 0.1757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 187.0 Average reward fake: 0.5191689133644104 Average reward real: 0.5182669758796692 Training q_loss: 95.4332 Training g_loss: 0.6568 Training d_loss: 1.4010 Explore P: 0.1727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 199.0 Average reward fake: 0.43730276823043823 Average reward real: 0.5867905020713806 Training q_loss: 85.8269 Training g_loss: 0.8324 Training d_loss: 1.1448 Explore P: 0.1694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 199.0 Average reward fake: 0.6012914776802063 Average reward real: 0.43245968222618103 Training q_loss: 111.4790 Training g_loss: 0.5142 Training d_loss: 1.8323 Explore P: 0.1663\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 199.0 Average reward fake: 0.5854860544204712 Average reward real: 0.5297554731369019 Training q_loss: 61.8710 Training g_loss: 0.5360 Training d_loss: 1.5440 Explore P: 0.1632\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 148.0 Average reward fake: 0.5089521408081055 Average reward real: 0.4572969973087311 Training q_loss: 28.0182 Training g_loss: 0.6754 Training d_loss: 1.5266 Explore P: 0.1610\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 199.0 Average reward fake: 0.5661741495132446 Average reward real: 0.49799007177352905 Training q_loss: 96.4039 Training g_loss: 0.5689 Training d_loss: 1.5505 Explore P: 0.1580\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 199.0 Average reward fake: 0.49856677651405334 Average reward real: 0.522497832775116 Training q_loss: 113.2595 Training g_loss: 0.6958 Training d_loss: 1.3434 Explore P: 0.1551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 187.0 Average reward fake: 0.480150043964386 Average reward real: 0.5071184635162354 Training q_loss: 90.8954 Training g_loss: 0.7343 Training d_loss: 1.3378 Explore P: 0.1524\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 187.0 Average reward fake: 0.5029178857803345 Average reward real: 0.5060189962387085 Training q_loss: 58.8248 Training g_loss: 0.6876 Training d_loss: 1.3842 Explore P: 0.1498\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 182.0 Average reward fake: 0.3825356364250183 Average reward real: 0.516903281211853 Training q_loss: 103.0312 Training g_loss: 1.0330 Training d_loss: 1.1950 Explore P: 0.1472\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 199.0 Average reward fake: 0.46701255440711975 Average reward real: 0.5884678959846497 Training q_loss: 29.2038 Training g_loss: 0.7687 Training d_loss: 1.1757 Explore P: 0.1445\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 199.0 Average reward fake: 0.5529208779335022 Average reward real: 0.5107907652854919 Training q_loss: 136.2614 Training g_loss: 0.5964 Training d_loss: 1.5461 Explore P: 0.1419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 199.0 Average reward fake: 0.5565469861030579 Average reward real: 0.5280650854110718 Training q_loss: 23.8615 Training g_loss: 0.5860 Training d_loss: 1.4558 Explore P: 0.1393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 197.0 Average reward fake: 0.49704068899154663 Average reward real: 0.4904642105102539 Training q_loss: 77.9251 Training g_loss: 0.6993 Training d_loss: 1.4109 Explore P: 0.1368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 199.0 Average reward fake: 0.48414698243141174 Average reward real: 0.49611884355545044 Training q_loss: 81.6164 Training g_loss: 0.7256 Training d_loss: 1.3661 Explore P: 0.1343\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 161.0 Average reward fake: 0.4800966680049896 Average reward real: 0.5878220796585083 Training q_loss: 96.3074 Training g_loss: 0.7425 Training d_loss: 1.2076 Explore P: 0.1323\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 199.0 Average reward fake: 0.5589156746864319 Average reward real: 0.5068162679672241 Training q_loss: 50.9932 Training g_loss: 0.5824 Training d_loss: 1.5048 Explore P: 0.1299\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 199.0 Average reward fake: 0.484234094619751 Average reward real: 0.5012645125389099 Training q_loss: 18.0187 Training g_loss: 0.7255 Training d_loss: 1.3582 Explore P: 0.1275\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 199.0 Average reward fake: 0.3024459779262543 Average reward real: 0.490723192691803 Training q_loss: 91.6940 Training g_loss: 1.2050 Training d_loss: 1.1320 Explore P: 0.1252\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 180.0 Average reward fake: 0.5172728896141052 Average reward real: 0.4574604630470276 Training q_loss: 33.1358 Training g_loss: 0.6592 Training d_loss: 1.5329 Explore P: 0.1231\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 199.0 Average reward fake: 0.46968239545822144 Average reward real: 0.6074769496917725 Training q_loss: 50.7624 Training g_loss: 0.7557 Training d_loss: 1.1670 Explore P: 0.1209\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 199.0 Average reward fake: 0.4548780918121338 Average reward real: 0.5514456033706665 Training q_loss: 21.7912 Training g_loss: 0.7877 Training d_loss: 1.2130 Explore P: 0.1187\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 199.0 Average reward fake: 0.4531472623348236 Average reward real: 0.5021964311599731 Training q_loss: 21.6635 Training g_loss: 0.8033 Training d_loss: 1.4011 Explore P: 0.1166\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 199.0 Average reward fake: 0.4698292315006256 Average reward real: 0.5022903680801392 Training q_loss: 153.7513 Training g_loss: 0.7555 Training d_loss: 1.3929 Explore P: 0.1145\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 199.0 Average reward fake: 0.48116976022720337 Average reward real: 0.5283249020576477 Training q_loss: 66.3569 Training g_loss: 0.7317 Training d_loss: 1.4380 Explore P: 0.1124\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 199.0 Average reward fake: 0.4704705774784088 Average reward real: 0.5355385541915894 Training q_loss: 24.1826 Training g_loss: 0.7540 Training d_loss: 1.3481 Explore P: 0.1104\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 199.0 Average reward fake: 0.4890671670436859 Average reward real: 0.5060819983482361 Training q_loss: 57.7129 Training g_loss: 0.7140 Training d_loss: 1.3596 Explore P: 0.1084\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 199.0 Average reward fake: 0.4888913035392761 Average reward real: 0.5452092289924622 Training q_loss: 21.4943 Training g_loss: 0.7158 Training d_loss: 1.2942 Explore P: 0.1065\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 199.0 Average reward fake: 0.5191982984542847 Average reward real: 0.5659986734390259 Training q_loss: 26.9074 Training g_loss: 0.6555 Training d_loss: 1.3114 Explore P: 0.1046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 199.0 Average reward fake: 0.5097454786300659 Average reward real: 0.5321705937385559 Training q_loss: 388.9910 Training g_loss: 0.6752 Training d_loss: 1.3580 Explore P: 0.1027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 199.0 Average reward fake: 0.5157966613769531 Average reward real: 0.56791090965271 Training q_loss: 34.5330 Training g_loss: 0.6647 Training d_loss: 1.3111 Explore P: 0.1009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 198.0 Average reward fake: 0.4930526316165924 Average reward real: 0.5767733454704285 Training q_loss: 24.0774 Training g_loss: 0.7102 Training d_loss: 1.2579 Explore P: 0.0991\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 192.0 Average reward fake: 0.46585506200790405 Average reward real: 0.49350705742836 Training q_loss: 202.2864 Training g_loss: 0.7639 Training d_loss: 1.3486 Explore P: 0.0974\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 199.0 Average reward fake: 0.4987124800682068 Average reward real: 0.5489636659622192 Training q_loss: 36.5036 Training g_loss: 0.7027 Training d_loss: 1.3160 Explore P: 0.0957\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 166.0 Average reward fake: 0.4055485725402832 Average reward real: 0.4951857030391693 Training q_loss: 4.4747 Training g_loss: 0.9933 Training d_loss: 1.2929 Explore P: 0.0943\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 198.0 Average reward fake: 0.45967230200767517 Average reward real: 0.5089196562767029 Training q_loss: 25.8022 Training g_loss: 0.8196 Training d_loss: 1.3473 Explore P: 0.0926\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 199.0 Average reward fake: 0.2894707918167114 Average reward real: 0.6554783582687378 Training q_loss: 18.3453 Training g_loss: 1.2714 Training d_loss: 0.8135 Explore P: 0.0910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 194.0 Average reward fake: 0.47869211435317993 Average reward real: 0.5598186254501343 Training q_loss: 1.8926 Training g_loss: 0.7368 Training d_loss: 1.2482 Explore P: 0.0894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 199.0 Average reward fake: 0.48013538122177124 Average reward real: 0.4195879101753235 Training q_loss: 11.2221 Training g_loss: 0.7331 Training d_loss: 1.5373 Explore P: 0.0879\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 199.0 Average reward fake: 0.5177144408226013 Average reward real: 0.49206072092056274 Training q_loss: 7.3112 Training g_loss: 0.6585 Training d_loss: 1.4665 Explore P: 0.0863\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 198.0 Average reward fake: 0.4865531027317047 Average reward real: 0.4836735725402832 Training q_loss: 8.2393 Training g_loss: 0.7206 Training d_loss: 1.4017 Explore P: 0.0849\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 199.0 Average reward fake: 0.4814818799495697 Average reward real: 0.5078468918800354 Training q_loss: 3.5878 Training g_loss: 0.7310 Training d_loss: 1.3644 Explore P: 0.0834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 199.0 Average reward fake: 0.46128207445144653 Average reward real: 0.5121666193008423 Training q_loss: 16.3464 Training g_loss: 0.7744 Training d_loss: 1.3030 Explore P: 0.0819\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 199.0 Average reward fake: 0.46868008375167847 Average reward real: 0.5035303235054016 Training q_loss: 6.7893 Training g_loss: 0.7578 Training d_loss: 1.3236 Explore P: 0.0805\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 199.0 Average reward fake: 0.47467559576034546 Average reward real: 0.5027773380279541 Training q_loss: 4.7748 Training g_loss: 0.7450 Training d_loss: 1.3575 Explore P: 0.0791\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 199.0 Average reward fake: 0.45519915223121643 Average reward real: 0.6555019021034241 Training q_loss: 4.0446 Training g_loss: 0.7884 Training d_loss: 1.0445 Explore P: 0.0778\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 199.0 Average reward fake: 0.3869650363922119 Average reward real: 0.6499120593070984 Training q_loss: 14.6636 Training g_loss: 1.0177 Training d_loss: 1.0396 Explore P: 0.0764\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 199.0 Average reward fake: 0.34564581513404846 Average reward real: 0.4949219226837158 Training q_loss: 6.2884 Training g_loss: 1.0632 Training d_loss: 1.1744 Explore P: 0.0751\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 199.0 Average reward fake: 0.5306380987167358 Average reward real: 0.4978438913822174 Training q_loss: 21.5367 Training g_loss: 0.6351 Training d_loss: 1.5058 Explore P: 0.0738\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 199.0 Average reward fake: 0.44933590292930603 Average reward real: 0.5067093372344971 Training q_loss: 13.3746 Training g_loss: 0.8000 Training d_loss: 1.3305 Explore P: 0.0726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 199.0 Average reward fake: 0.5085106492042542 Average reward real: 0.5192660689353943 Training q_loss: 4.2413 Training g_loss: 0.6765 Training d_loss: 1.3655 Explore P: 0.0713\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 199.0 Average reward fake: 0.28434696793556213 Average reward real: 0.6319642663002014 Training q_loss: 7.4885 Training g_loss: 1.2570 Training d_loss: 0.7989 Explore P: 0.0701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 199.0 Average reward fake: 0.5059998035430908 Average reward real: 0.5206971168518066 Training q_loss: 8.7384 Training g_loss: 0.6812 Training d_loss: 1.3580 Explore P: 0.0690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 179.0 Average reward fake: 0.48871392011642456 Average reward real: 0.3909173011779785 Training q_loss: 23.1873 Training g_loss: 0.7163 Training d_loss: 1.7546 Explore P: 0.0679\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 186.0 Average reward fake: 0.6260808706283569 Average reward real: 0.4833090901374817 Training q_loss: 6.4609 Training g_loss: 0.4683 Training d_loss: 1.7165 Explore P: 0.0668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 199.0 Average reward fake: 0.5184529423713684 Average reward real: 0.49864715337753296 Training q_loss: 5.1626 Training g_loss: 0.6569 Training d_loss: 1.4281 Explore P: 0.0657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 199.0 Average reward fake: 0.5058918595314026 Average reward real: 0.5273059606552124 Training q_loss: 8.9801 Training g_loss: 0.6814 Training d_loss: 1.3609 Explore P: 0.0646\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 199.0 Average reward fake: 0.4996231496334076 Average reward real: 0.5229876637458801 Training q_loss: 5.4566 Training g_loss: 0.6950 Training d_loss: 1.3607 Explore P: 0.0635\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 177.0 Average reward fake: 0.4575583040714264 Average reward real: 0.4621240496635437 Training q_loss: 6.6661 Training g_loss: 0.7821 Training d_loss: 1.3916 Explore P: 0.0626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 190.0 Average reward fake: 0.5329429507255554 Average reward real: 0.20190277695655823 Training q_loss: 3.4568 Training g_loss: 0.6306 Training d_loss: 2.5569 Explore P: 0.0616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 199.0 Average reward fake: 0.4484504163265228 Average reward real: 0.5176176428794861 Training q_loss: 23.3344 Training g_loss: 0.8036 Training d_loss: 1.2861 Explore P: 0.0606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 199.0 Average reward fake: 0.47798123955726624 Average reward real: 0.5261695981025696 Training q_loss: 3.1664 Training g_loss: 0.7409 Training d_loss: 1.3104 Explore P: 0.0596\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 179.0 Average reward fake: 0.5358613729476929 Average reward real: 0.4446510374546051 Training q_loss: 11.2228 Training g_loss: 0.6312 Training d_loss: 1.6005 Explore P: 0.0587\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 191.0 Average reward fake: 0.4519895613193512 Average reward real: 0.5678175091743469 Training q_loss: 12.6707 Training g_loss: 0.7979 Training d_loss: 1.1838 Explore P: 0.0578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 199.0 Average reward fake: 0.5316022634506226 Average reward real: 0.63850337266922 Training q_loss: 6.3457 Training g_loss: 0.6291 Training d_loss: 1.2500 Explore P: 0.0569\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 199.0 Average reward fake: 0.6461517214775085 Average reward real: 0.39497604966163635 Training q_loss: 458.1904 Training g_loss: 0.4392 Training d_loss: 1.9915 Explore P: 0.0559\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 199.0 Average reward fake: 0.5714613199234009 Average reward real: 0.42532381415367126 Training q_loss: 18.3442 Training g_loss: 0.5608 Training d_loss: 1.7550 Explore P: 0.0550\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 199.0 Average reward fake: 0.4126002788543701 Average reward real: 0.6898372173309326 Training q_loss: 8.4506 Training g_loss: 0.8872 Training d_loss: 0.9414 Explore P: 0.0541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 199.0 Average reward fake: 0.579510509967804 Average reward real: 0.47587960958480835 Training q_loss: 31.0346 Training g_loss: 0.5457 Training d_loss: 1.6200 Explore P: 0.0533\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 180.0 Average reward fake: 0.4383619725704193 Average reward real: 0.5367682576179504 Training q_loss: 15.5167 Training g_loss: 0.8252 Training d_loss: 1.2139 Explore P: 0.0525\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 183.0 Average reward fake: 0.46315282583236694 Average reward real: 0.49602827429771423 Training q_loss: 3.0113 Training g_loss: 0.7697 Training d_loss: 1.3250 Explore P: 0.0517\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 164.0 Average reward fake: 0.4798150658607483 Average reward real: 0.5302442312240601 Training q_loss: 27.3058 Training g_loss: 0.7455 Training d_loss: 1.3221 Explore P: 0.0511\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 186.0 Average reward fake: 0.4809587895870209 Average reward real: 0.4497775137424469 Training q_loss: 5.9666 Training g_loss: 0.7318 Training d_loss: 1.4778 Explore P: 0.0503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 194.0 Average reward fake: 0.4948246479034424 Average reward real: 0.6127904057502747 Training q_loss: 4.3792 Training g_loss: 0.7035 Training d_loss: 1.2162 Explore P: 0.0495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 199.0 Average reward fake: 0.5337352752685547 Average reward real: 0.3196534812450409 Training q_loss: 6.4804 Training g_loss: 0.6430 Training d_loss: 1.9848 Explore P: 0.0487\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "q_loss_list, g_loss_list, d_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     # Restore/load the trained model \n",
    "    #     saver.restore(sess, 'checkpoints/Q-GAN-cartpole.ckpt')    \n",
    "    #     #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        q_loss, g_loss, d_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions}\n",
    "            rewards_fake, rewards_real = sess.run([model.rewards_fake, model.rewards_real], feed_dict)\n",
    "            feed_dict={model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0)\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating the model\n",
    "            feed_dict = {model.states: states, model.next_states: next_states, model.actions: actions, \n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model \n",
    "    saver.save(sess, 'checkpoints/QGAN-cartpole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 5\n",
    "test_max_steps = 2000\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    saver.restore(sess, 'checkpoints/QGAN-cartpole.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "# Closing the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
