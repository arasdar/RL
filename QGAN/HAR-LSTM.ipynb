{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAR classification \n",
    "# Author: Burak Himmetoglu\n",
    "# 8/15/2017\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_data(data_path, split = \"train\"):\n",
    "\t\"\"\" Read data \"\"\"\n",
    "\n",
    "\t# Fixed params\n",
    "\tn_class = 6\n",
    "\tn_steps = 128\n",
    "\n",
    "\t# Paths\n",
    "\tpath_ = os.path.join(data_path, split)\n",
    "\tpath_signals = os.path.join(path_, \"Inertial_Signals\")\n",
    "\n",
    "\t# Read labels and one-hot encode\n",
    "\tlabel_path = os.path.join(path_, \"y_\" + split + \".txt\")\n",
    "\tlabels = pd.read_csv(label_path, header = None)\n",
    "\n",
    "\t# Read time-series data\n",
    "\tchannel_files = os.listdir(path_signals)\n",
    "\tchannel_files.sort()\n",
    "\tn_channels = len(channel_files)\n",
    "\tposix = len(split) + 5\n",
    "\n",
    "\t# Initiate array\n",
    "\tlist_of_channels = []\n",
    "\tX = np.zeros((len(labels), n_steps, n_channels))\n",
    "\ti_ch = 0\n",
    "\tfor fil_ch in channel_files:\n",
    "\t\tchannel_name = fil_ch[:-posix]\n",
    "\t\tdat_ = pd.read_csv(os.path.join(path_signals,fil_ch), delim_whitespace = True, header = None)\n",
    "\t\tX[:,:,i_ch] = dat_.as_matrix()\n",
    "\n",
    "\t\t# Record names\n",
    "\t\tlist_of_channels.append(channel_name)\n",
    "\n",
    "\t\t# iterate\n",
    "\t\ti_ch += 1\n",
    "\n",
    "\t# Return \n",
    "\treturn X, labels[0].values, list_of_channels\n",
    "\n",
    "# How to normalize the input data correctly\n",
    "def standardize(train, test):\n",
    "\t\"\"\" Standardize data \"\"\"\n",
    "\t# Standardize train and test\n",
    "\tX_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "\tX_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "\treturn X_train, X_test\n",
    "\n",
    "# How to onehot-encoding\n",
    "# def one_hot(labels, n_class = 6):\n",
    "# \t\"\"\" One-hot encoding \"\"\"\n",
    "# \texpansion = np.eye(n_class)\n",
    "# \ty = expansion[:, labels-1].T\n",
    "# \tassert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "# \treturn y\n",
    "\n",
    "def get_batches(X, y, batch_size = 100):\n",
    "\t\"\"\" Return a generator for batches \"\"\"\n",
    "\tn_batches = len(X) // batch_size\n",
    "\tX, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "\t# Loop over batches and yield\n",
    "\tfor b in range(0, len(X), batch_size):\n",
    "\t\tyield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data: Traing/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, list_ch_train = read_data(data_path=\"/home/arasdar/datasets/har-data/\", split=\"train\") # train\n",
    "Xtest, Ytest, list_ch_test = read_data(data_path=\"/home/arasdar/datasets/har-data/\", split=\"test\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest = standardize(test=Xtest, train=Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, stratify = Ytrain, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5514, 128, 9) float64\n",
      "(1838, 128, 9) float64\n",
      "(2947, 128, 9) float64\n",
      "(5514,) int64 6 1\n",
      "(1838,) int64 6 1\n",
      "(2947,) int64 6 1\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Xtrain.dtype)\n",
    "print(Xvalid.shape, Xvalid.dtype)\n",
    "print(Xtest.shape, Xtest.dtype)\n",
    "print(Ytrain.shape, Ytrain.dtype, Ytrain.max(), Ytrain.min())\n",
    "print(Yvalid.shape, Yvalid.dtype, Yvalid.max(), Yvalid.min())\n",
    "print(Ytest.shape, Ytest.dtype, Ytest.max(), Ytest.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_size = 27         # 3 times the amount of channels\n",
    "lstm_layers = 1        # Number of layers\n",
    "batch_size = 600       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.001  # Learning rate (default is 0.001)\n",
    "epochs = 1000\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "# with graph.as_default():\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "indices_ = tf.placeholder(tf.int32, [None], name = 'indices')\n",
    "# keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "# learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct inputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 9)\n",
      "(128, ?, 9)\n",
      "(?, 9)\n",
      "128 (?, 9)\n"
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "# Construct the LSTM inputs and LSTM cells\n",
    "print(inputs_.shape)\n",
    "lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "print(lstm_in.shape)\n",
    "lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "print(lstm_in.shape)\n",
    "\n",
    "# Open up the tensor into a list of seq_len pieces\n",
    "lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "print(len(lstm_in), lstm_in[0].shape)\n",
    "\n",
    "# Add LSTM layers\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "#     drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward pass, cost function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with graph.as_default():\n",
    "# with tf.variable_scope('RNN', reuse=True):\n",
    "outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state = initial_state)\n",
    "\n",
    "# We only need the last output tensor to pass into a classifier\n",
    "logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "labels = tf.one_hot(depth=n_classes, indices=indices_)\n",
    "\n",
    "# Loss/cost using labels and logits\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy using logits and labels\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "# Optimize using loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) # No grad clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 (600, 27)\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs), outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"logits/BiasAdd:0\", shape=(600, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/1000 loss:1.4380 acc:0.2546\n",
      "Validation Epoch: 1/1000 loss:1.3998 acc:0.2939\n",
      "Training Epoch: 2/1000 loss:1.3664 acc:0.3263\n",
      "Validation Epoch: 2/1000 loss:1.3278 acc:0.3772\n",
      "Training Epoch: 3/1000 loss:1.2912 acc:0.4050\n",
      "Validation Epoch: 3/1000 loss:1.2518 acc:0.4150\n",
      "Training Epoch: 4/1000 loss:1.2130 acc:0.4217\n",
      "Validation Epoch: 4/1000 loss:1.1717 acc:0.4306\n",
      "Training Epoch: 5/1000 loss:1.1282 acc:0.4517\n",
      "Validation Epoch: 5/1000 loss:1.0818 acc:0.4806\n",
      "Training Epoch: 6/1000 loss:1.0427 acc:0.4996\n",
      "Validation Epoch: 6/1000 loss:1.0068 acc:0.5056\n",
      "Training Epoch: 7/1000 loss:0.9608 acc:0.5311\n",
      "Validation Epoch: 7/1000 loss:0.9202 acc:0.5350\n",
      "Training Epoch: 8/1000 loss:0.8825 acc:0.5604\n",
      "Validation Epoch: 8/1000 loss:0.8374 acc:0.5744\n",
      "Training Epoch: 9/1000 loss:0.8036 acc:0.6083\n",
      "Validation Epoch: 9/1000 loss:0.7581 acc:0.6256\n",
      "Training Epoch: 10/1000 loss:0.7307 acc:0.6298\n",
      "Validation Epoch: 10/1000 loss:0.6919 acc:0.6100\n",
      "Training Epoch: 11/1000 loss:0.6651 acc:0.6191\n",
      "Validation Epoch: 11/1000 loss:0.6332 acc:0.6156\n",
      "Training Epoch: 12/1000 loss:0.6097 acc:0.6257\n",
      "Validation Epoch: 12/1000 loss:0.5841 acc:0.6261\n",
      "Training Epoch: 13/1000 loss:0.5593 acc:0.6350\n",
      "Validation Epoch: 13/1000 loss:0.5348 acc:0.6317\n",
      "Training Epoch: 14/1000 loss:0.5172 acc:0.6489\n",
      "Validation Epoch: 14/1000 loss:0.4989 acc:0.6511\n",
      "Training Epoch: 15/1000 loss:0.4809 acc:0.6776\n",
      "Validation Epoch: 15/1000 loss:0.4711 acc:0.6756\n",
      "Training Epoch: 16/1000 loss:0.4578 acc:0.6948\n",
      "Validation Epoch: 16/1000 loss:0.4549 acc:0.6872\n",
      "Training Epoch: 17/1000 loss:0.4667 acc:0.6933\n",
      "Validation Epoch: 17/1000 loss:0.4623 acc:0.6906\n",
      "Training Epoch: 18/1000 loss:0.4347 acc:0.7028\n",
      "Validation Epoch: 18/1000 loss:0.4207 acc:0.7050\n",
      "Training Epoch: 19/1000 loss:0.4118 acc:0.7144\n",
      "Validation Epoch: 19/1000 loss:0.4071 acc:0.6933\n",
      "Training Epoch: 20/1000 loss:0.3910 acc:0.7163\n",
      "Validation Epoch: 20/1000 loss:0.3848 acc:0.7067\n",
      "Training Epoch: 21/1000 loss:0.3710 acc:0.7224\n",
      "Validation Epoch: 21/1000 loss:0.3708 acc:0.7133\n",
      "Training Epoch: 22/1000 loss:0.3568 acc:0.7281\n",
      "Validation Epoch: 22/1000 loss:0.3582 acc:0.7161\n",
      "Training Epoch: 23/1000 loss:0.3489 acc:0.7309\n",
      "Validation Epoch: 23/1000 loss:0.3637 acc:0.7111\n",
      "Training Epoch: 24/1000 loss:0.3424 acc:0.7311\n",
      "Validation Epoch: 24/1000 loss:0.3590 acc:0.7217\n",
      "Training Epoch: 25/1000 loss:0.3480 acc:0.7300\n",
      "Validation Epoch: 25/1000 loss:0.3654 acc:0.7150\n",
      "Training Epoch: 26/1000 loss:0.3424 acc:0.7313\n",
      "Validation Epoch: 26/1000 loss:0.3487 acc:0.7194\n",
      "Training Epoch: 27/1000 loss:0.3294 acc:0.7354\n",
      "Validation Epoch: 27/1000 loss:0.3305 acc:0.7233\n",
      "Training Epoch: 28/1000 loss:0.3158 acc:0.7344\n",
      "Validation Epoch: 28/1000 loss:0.3262 acc:0.7211\n",
      "Training Epoch: 29/1000 loss:0.3057 acc:0.7406\n",
      "Validation Epoch: 29/1000 loss:0.3103 acc:0.7228\n",
      "Training Epoch: 30/1000 loss:0.2965 acc:0.7389\n",
      "Validation Epoch: 30/1000 loss:0.3072 acc:0.7294\n",
      "Training Epoch: 31/1000 loss:0.2876 acc:0.7411\n",
      "Validation Epoch: 31/1000 loss:0.2894 acc:0.7350\n",
      "Training Epoch: 32/1000 loss:0.2774 acc:0.7474\n",
      "Validation Epoch: 32/1000 loss:0.2875 acc:0.7328\n",
      "Training Epoch: 33/1000 loss:0.2713 acc:0.7469\n",
      "Validation Epoch: 33/1000 loss:0.2800 acc:0.7356\n",
      "Training Epoch: 34/1000 loss:0.2662 acc:0.7481\n",
      "Validation Epoch: 34/1000 loss:0.2780 acc:0.7389\n",
      "Training Epoch: 35/1000 loss:0.2623 acc:0.7459\n",
      "Validation Epoch: 35/1000 loss:0.2706 acc:0.7361\n",
      "Training Epoch: 36/1000 loss:0.2509 acc:0.7515\n",
      "Validation Epoch: 36/1000 loss:0.2714 acc:0.7344\n",
      "Training Epoch: 37/1000 loss:0.2458 acc:0.7539\n",
      "Validation Epoch: 37/1000 loss:0.2632 acc:0.7344\n",
      "Training Epoch: 38/1000 loss:0.2411 acc:0.7481\n",
      "Validation Epoch: 38/1000 loss:0.2634 acc:0.7328\n",
      "Training Epoch: 39/1000 loss:0.2450 acc:0.7500\n",
      "Validation Epoch: 39/1000 loss:0.2499 acc:0.7428\n",
      "Training Epoch: 40/1000 loss:0.2333 acc:0.7548\n",
      "Validation Epoch: 40/1000 loss:0.2526 acc:0.7406\n",
      "Training Epoch: 41/1000 loss:0.2310 acc:0.7546\n",
      "Validation Epoch: 41/1000 loss:0.2407 acc:0.7439\n",
      "Training Epoch: 42/1000 loss:0.2270 acc:0.7522\n",
      "Validation Epoch: 42/1000 loss:0.2359 acc:0.7478\n",
      "Training Epoch: 43/1000 loss:0.2204 acc:0.7554\n",
      "Validation Epoch: 43/1000 loss:0.2279 acc:0.7472\n",
      "Training Epoch: 44/1000 loss:0.2131 acc:0.7576\n",
      "Validation Epoch: 44/1000 loss:0.2258 acc:0.7450\n",
      "Training Epoch: 45/1000 loss:0.2136 acc:0.7531\n",
      "Validation Epoch: 45/1000 loss:0.2233 acc:0.7456\n",
      "Training Epoch: 46/1000 loss:0.2071 acc:0.7569\n",
      "Validation Epoch: 46/1000 loss:0.2173 acc:0.7522\n",
      "Training Epoch: 47/1000 loss:0.1999 acc:0.7631\n",
      "Validation Epoch: 47/1000 loss:0.2098 acc:0.7533\n",
      "Training Epoch: 48/1000 loss:0.1974 acc:0.7602\n",
      "Validation Epoch: 48/1000 loss:0.2065 acc:0.7550\n",
      "Training Epoch: 49/1000 loss:0.1921 acc:0.7619\n",
      "Validation Epoch: 49/1000 loss:0.2036 acc:0.7583\n",
      "Training Epoch: 50/1000 loss:0.1901 acc:0.7635\n",
      "Validation Epoch: 50/1000 loss:0.2121 acc:0.7522\n",
      "Training Epoch: 51/1000 loss:0.1939 acc:0.7620\n",
      "Validation Epoch: 51/1000 loss:0.2139 acc:0.7478\n",
      "Training Epoch: 52/1000 loss:0.1928 acc:0.7633\n",
      "Validation Epoch: 52/1000 loss:0.2070 acc:0.7533\n",
      "Training Epoch: 53/1000 loss:0.1862 acc:0.7639\n",
      "Validation Epoch: 53/1000 loss:0.1973 acc:0.7544\n",
      "Training Epoch: 54/1000 loss:0.1801 acc:0.7646\n",
      "Validation Epoch: 54/1000 loss:0.1947 acc:0.7539\n",
      "Training Epoch: 55/1000 loss:0.1771 acc:0.7646\n",
      "Validation Epoch: 55/1000 loss:0.1939 acc:0.7533\n",
      "Training Epoch: 56/1000 loss:0.1767 acc:0.7644\n",
      "Validation Epoch: 56/1000 loss:0.1895 acc:0.7556\n",
      "Training Epoch: 57/1000 loss:0.1727 acc:0.7650\n",
      "Validation Epoch: 57/1000 loss:0.1864 acc:0.7561\n",
      "Training Epoch: 58/1000 loss:0.1748 acc:0.7643\n",
      "Validation Epoch: 58/1000 loss:0.1864 acc:0.7550\n",
      "Training Epoch: 59/1000 loss:0.1710 acc:0.7644\n",
      "Validation Epoch: 59/1000 loss:0.1824 acc:0.7567\n",
      "Training Epoch: 60/1000 loss:0.1708 acc:0.7656\n",
      "Validation Epoch: 60/1000 loss:0.1862 acc:0.7528\n",
      "Training Epoch: 61/1000 loss:0.1679 acc:0.7639\n",
      "Validation Epoch: 61/1000 loss:0.1822 acc:0.7561\n",
      "Training Epoch: 62/1000 loss:0.1650 acc:0.7670\n",
      "Validation Epoch: 62/1000 loss:0.1777 acc:0.7594\n",
      "Training Epoch: 63/1000 loss:0.1627 acc:0.7665\n",
      "Validation Epoch: 63/1000 loss:0.1804 acc:0.7550\n",
      "Training Epoch: 64/1000 loss:0.1917 acc:0.7581\n",
      "Validation Epoch: 64/1000 loss:0.2283 acc:0.7428\n",
      "Training Epoch: 65/1000 loss:0.2060 acc:0.7517\n",
      "Validation Epoch: 65/1000 loss:0.1973 acc:0.7522\n",
      "Training Epoch: 66/1000 loss:0.1871 acc:0.7580\n",
      "Validation Epoch: 66/1000 loss:0.1833 acc:0.7567\n",
      "Training Epoch: 67/1000 loss:0.1754 acc:0.7617\n",
      "Validation Epoch: 67/1000 loss:0.1838 acc:0.7539\n",
      "Training Epoch: 68/1000 loss:0.1937 acc:0.7487\n",
      "Validation Epoch: 68/1000 loss:0.2025 acc:0.7422\n",
      "Training Epoch: 69/1000 loss:0.1933 acc:0.7513\n",
      "Validation Epoch: 69/1000 loss:0.1976 acc:0.7489\n",
      "Training Epoch: 70/1000 loss:0.1889 acc:0.7530\n",
      "Validation Epoch: 70/1000 loss:0.1934 acc:0.7489\n",
      "Training Epoch: 71/1000 loss:0.1808 acc:0.7572\n",
      "Validation Epoch: 71/1000 loss:0.1812 acc:0.7539\n",
      "Training Epoch: 72/1000 loss:0.1728 acc:0.7619\n",
      "Validation Epoch: 72/1000 loss:0.1745 acc:0.7572\n",
      "Training Epoch: 73/1000 loss:0.1718 acc:0.7639\n",
      "Validation Epoch: 73/1000 loss:0.1801 acc:0.7561\n",
      "Training Epoch: 74/1000 loss:0.1756 acc:0.7626\n",
      "Validation Epoch: 74/1000 loss:0.1989 acc:0.7506\n",
      "Training Epoch: 75/1000 loss:0.1899 acc:0.7598\n",
      "Validation Epoch: 75/1000 loss:0.2020 acc:0.7506\n",
      "Training Epoch: 76/1000 loss:0.1828 acc:0.7598\n",
      "Validation Epoch: 76/1000 loss:0.1939 acc:0.7517\n",
      "Training Epoch: 77/1000 loss:0.1770 acc:0.7617\n",
      "Validation Epoch: 77/1000 loss:0.1860 acc:0.7561\n",
      "Training Epoch: 78/1000 loss:0.1660 acc:0.7644\n",
      "Validation Epoch: 78/1000 loss:0.1746 acc:0.7550\n",
      "Training Epoch: 79/1000 loss:0.1636 acc:0.7639\n",
      "Validation Epoch: 79/1000 loss:0.1761 acc:0.7561\n",
      "Training Epoch: 80/1000 loss:0.1561 acc:0.7669\n",
      "Validation Epoch: 80/1000 loss:0.1692 acc:0.7600\n",
      "Training Epoch: 81/1000 loss:0.1632 acc:0.7643\n",
      "Validation Epoch: 81/1000 loss:0.1812 acc:0.7506\n",
      "Training Epoch: 82/1000 loss:0.1690 acc:0.7591\n",
      "Validation Epoch: 82/1000 loss:0.1859 acc:0.7517\n",
      "Training Epoch: 83/1000 loss:0.1673 acc:0.7591\n",
      "Validation Epoch: 83/1000 loss:0.1843 acc:0.7550\n",
      "Training Epoch: 84/1000 loss:0.1664 acc:0.7641\n",
      "Validation Epoch: 84/1000 loss:0.1774 acc:0.7561\n",
      "Training Epoch: 85/1000 loss:0.1640 acc:0.7644\n",
      "Validation Epoch: 85/1000 loss:0.1685 acc:0.7594\n",
      "Training Epoch: 86/1000 loss:0.1586 acc:0.7641\n",
      "Validation Epoch: 86/1000 loss:0.1672 acc:0.7578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 87/1000 loss:0.1619 acc:0.7630\n",
      "Validation Epoch: 87/1000 loss:0.1597 acc:0.7594\n",
      "Training Epoch: 88/1000 loss:0.1528 acc:0.7646\n",
      "Validation Epoch: 88/1000 loss:0.1604 acc:0.7561\n",
      "Training Epoch: 89/1000 loss:0.1489 acc:0.7670\n",
      "Validation Epoch: 89/1000 loss:0.1572 acc:0.7606\n",
      "Training Epoch: 90/1000 loss:0.1481 acc:0.7672\n",
      "Validation Epoch: 90/1000 loss:0.1570 acc:0.7606\n",
      "Training Epoch: 91/1000 loss:0.1448 acc:0.7678\n",
      "Validation Epoch: 91/1000 loss:0.1553 acc:0.7583\n",
      "Training Epoch: 92/1000 loss:0.1465 acc:0.7665\n",
      "Validation Epoch: 92/1000 loss:0.1547 acc:0.7567\n",
      "Training Epoch: 93/1000 loss:0.1449 acc:0.7661\n",
      "Validation Epoch: 93/1000 loss:0.1550 acc:0.7572\n",
      "Training Epoch: 94/1000 loss:0.1452 acc:0.7663\n",
      "Validation Epoch: 94/1000 loss:0.1534 acc:0.7611\n",
      "Training Epoch: 95/1000 loss:0.1428 acc:0.7683\n",
      "Validation Epoch: 95/1000 loss:0.1524 acc:0.7606\n",
      "Training Epoch: 96/1000 loss:0.1428 acc:0.7680\n",
      "Validation Epoch: 96/1000 loss:0.1539 acc:0.7600\n",
      "Training Epoch: 97/1000 loss:0.1454 acc:0.7665\n",
      "Validation Epoch: 97/1000 loss:0.1556 acc:0.7617\n",
      "Training Epoch: 98/1000 loss:0.1492 acc:0.7633\n",
      "Validation Epoch: 98/1000 loss:0.1572 acc:0.7611\n",
      "Training Epoch: 99/1000 loss:0.1419 acc:0.7663\n",
      "Validation Epoch: 99/1000 loss:0.1551 acc:0.7594\n",
      "Training Epoch: 100/1000 loss:0.1437 acc:0.7669\n",
      "Validation Epoch: 100/1000 loss:0.1598 acc:0.7544\n",
      "Training Epoch: 101/1000 loss:0.1468 acc:0.7639\n",
      "Validation Epoch: 101/1000 loss:0.1517 acc:0.7589\n",
      "Training Epoch: 102/1000 loss:0.1481 acc:0.7637\n",
      "Validation Epoch: 102/1000 loss:0.1521 acc:0.7589\n",
      "Training Epoch: 103/1000 loss:0.1474 acc:0.7654\n",
      "Validation Epoch: 103/1000 loss:0.1599 acc:0.7550\n",
      "Training Epoch: 104/1000 loss:0.1436 acc:0.7654\n",
      "Validation Epoch: 104/1000 loss:0.1649 acc:0.7506\n",
      "Training Epoch: 105/1000 loss:0.2088 acc:0.7361\n",
      "Validation Epoch: 105/1000 loss:0.2260 acc:0.7239\n",
      "Training Epoch: 106/1000 loss:0.1904 acc:0.7474\n",
      "Validation Epoch: 106/1000 loss:0.1989 acc:0.7428\n",
      "Training Epoch: 107/1000 loss:0.1788 acc:0.7513\n",
      "Validation Epoch: 107/1000 loss:0.1796 acc:0.7522\n",
      "Training Epoch: 108/1000 loss:0.1612 acc:0.7643\n",
      "Validation Epoch: 108/1000 loss:0.1637 acc:0.7578\n",
      "Training Epoch: 109/1000 loss:0.1572 acc:0.7639\n",
      "Validation Epoch: 109/1000 loss:0.1582 acc:0.7594\n",
      "Training Epoch: 110/1000 loss:0.1486 acc:0.7672\n",
      "Validation Epoch: 110/1000 loss:0.1559 acc:0.7556\n",
      "Training Epoch: 111/1000 loss:0.1449 acc:0.7663\n",
      "Validation Epoch: 111/1000 loss:0.1528 acc:0.7572\n",
      "Training Epoch: 112/1000 loss:0.1474 acc:0.7630\n",
      "Validation Epoch: 112/1000 loss:0.1670 acc:0.7528\n",
      "Training Epoch: 113/1000 loss:0.1447 acc:0.7657\n",
      "Validation Epoch: 113/1000 loss:0.1537 acc:0.7578\n",
      "Training Epoch: 114/1000 loss:0.1421 acc:0.7676\n",
      "Validation Epoch: 114/1000 loss:0.1478 acc:0.7606\n",
      "Training Epoch: 115/1000 loss:0.1438 acc:0.7654\n",
      "Validation Epoch: 115/1000 loss:0.1582 acc:0.7583\n",
      "Training Epoch: 116/1000 loss:0.1679 acc:0.7572\n",
      "Validation Epoch: 116/1000 loss:0.2041 acc:0.7383\n",
      "Training Epoch: 117/1000 loss:0.2368 acc:0.7206\n",
      "Validation Epoch: 117/1000 loss:0.2111 acc:0.7289\n",
      "Training Epoch: 118/1000 loss:0.2177 acc:0.7513\n",
      "Validation Epoch: 118/1000 loss:0.2097 acc:0.7528\n",
      "Training Epoch: 119/1000 loss:0.1976 acc:0.7576\n",
      "Validation Epoch: 119/1000 loss:0.1964 acc:0.7528\n",
      "Training Epoch: 120/1000 loss:0.1802 acc:0.7583\n",
      "Validation Epoch: 120/1000 loss:0.1746 acc:0.7550\n",
      "Training Epoch: 121/1000 loss:0.1732 acc:0.7604\n",
      "Validation Epoch: 121/1000 loss:0.1908 acc:0.7494\n",
      "Training Epoch: 122/1000 loss:0.2516 acc:0.7313\n",
      "Validation Epoch: 122/1000 loss:0.2306 acc:0.7322\n",
      "Training Epoch: 123/1000 loss:0.2571 acc:0.7146\n",
      "Validation Epoch: 123/1000 loss:0.2248 acc:0.7206\n",
      "Training Epoch: 124/1000 loss:0.2242 acc:0.7424\n",
      "Validation Epoch: 124/1000 loss:0.1997 acc:0.7456\n",
      "Training Epoch: 125/1000 loss:0.2097 acc:0.7454\n",
      "Validation Epoch: 125/1000 loss:0.1815 acc:0.7561\n",
      "Training Epoch: 126/1000 loss:0.1962 acc:0.7507\n",
      "Validation Epoch: 126/1000 loss:0.1801 acc:0.7533\n",
      "Training Epoch: 127/1000 loss:0.1850 acc:0.7574\n",
      "Validation Epoch: 127/1000 loss:0.1770 acc:0.7528\n",
      "Training Epoch: 128/1000 loss:0.1796 acc:0.7581\n",
      "Validation Epoch: 128/1000 loss:0.1701 acc:0.7572\n",
      "Training Epoch: 129/1000 loss:0.1696 acc:0.7604\n",
      "Validation Epoch: 129/1000 loss:0.1660 acc:0.7572\n",
      "Training Epoch: 130/1000 loss:0.1652 acc:0.7615\n",
      "Validation Epoch: 130/1000 loss:0.1619 acc:0.7600\n",
      "Training Epoch: 131/1000 loss:0.1641 acc:0.7606\n",
      "Validation Epoch: 131/1000 loss:0.1593 acc:0.7600\n",
      "Training Epoch: 132/1000 loss:0.1649 acc:0.7607\n",
      "Validation Epoch: 132/1000 loss:0.1597 acc:0.7583\n",
      "Training Epoch: 133/1000 loss:0.1638 acc:0.7604\n",
      "Validation Epoch: 133/1000 loss:0.1596 acc:0.7583\n",
      "Training Epoch: 134/1000 loss:0.1620 acc:0.7609\n",
      "Validation Epoch: 134/1000 loss:0.1570 acc:0.7594\n",
      "Training Epoch: 135/1000 loss:0.1603 acc:0.7611\n",
      "Validation Epoch: 135/1000 loss:0.1570 acc:0.7600\n",
      "Training Epoch: 136/1000 loss:0.1610 acc:0.7620\n",
      "Validation Epoch: 136/1000 loss:0.1604 acc:0.7606\n",
      "Training Epoch: 137/1000 loss:0.1633 acc:0.7596\n",
      "Validation Epoch: 137/1000 loss:0.1615 acc:0.7567\n",
      "Training Epoch: 138/1000 loss:0.1586 acc:0.7626\n",
      "Validation Epoch: 138/1000 loss:0.1555 acc:0.7594\n",
      "Training Epoch: 139/1000 loss:0.1569 acc:0.7626\n",
      "Validation Epoch: 139/1000 loss:0.1544 acc:0.7600\n",
      "Training Epoch: 140/1000 loss:0.1551 acc:0.7613\n",
      "Validation Epoch: 140/1000 loss:0.1536 acc:0.7606\n",
      "Training Epoch: 141/1000 loss:0.1533 acc:0.7633\n",
      "Validation Epoch: 141/1000 loss:0.1509 acc:0.7622\n",
      "Training Epoch: 142/1000 loss:0.1524 acc:0.7628\n",
      "Validation Epoch: 142/1000 loss:0.1718 acc:0.7533\n",
      "Training Epoch: 143/1000 loss:0.1678 acc:0.7563\n",
      "Validation Epoch: 143/1000 loss:0.1923 acc:0.7472\n",
      "Training Epoch: 144/1000 loss:0.1656 acc:0.7589\n",
      "Validation Epoch: 144/1000 loss:0.1826 acc:0.7489\n",
      "Training Epoch: 145/1000 loss:0.1632 acc:0.7591\n",
      "Validation Epoch: 145/1000 loss:0.1754 acc:0.7561\n",
      "Training Epoch: 146/1000 loss:0.1641 acc:0.7578\n",
      "Validation Epoch: 146/1000 loss:0.1691 acc:0.7517\n",
      "Training Epoch: 147/1000 loss:0.1578 acc:0.7620\n",
      "Validation Epoch: 147/1000 loss:0.1686 acc:0.7506\n",
      "Training Epoch: 148/1000 loss:0.1543 acc:0.7613\n",
      "Validation Epoch: 148/1000 loss:0.1937 acc:0.7578\n",
      "Training Epoch: 149/1000 loss:0.2043 acc:0.7578\n",
      "Validation Epoch: 149/1000 loss:0.2099 acc:0.7439\n",
      "Training Epoch: 150/1000 loss:0.1828 acc:0.7476\n",
      "Validation Epoch: 150/1000 loss:0.1850 acc:0.7422\n",
      "Training Epoch: 151/1000 loss:0.1690 acc:0.7506\n",
      "Validation Epoch: 151/1000 loss:0.1798 acc:0.7433\n",
      "Training Epoch: 152/1000 loss:0.1641 acc:0.7544\n",
      "Validation Epoch: 152/1000 loss:0.1738 acc:0.7489\n",
      "Training Epoch: 153/1000 loss:0.1598 acc:0.7604\n",
      "Validation Epoch: 153/1000 loss:0.1699 acc:0.7561\n",
      "Training Epoch: 154/1000 loss:0.1574 acc:0.7622\n",
      "Validation Epoch: 154/1000 loss:0.1677 acc:0.7572\n",
      "Training Epoch: 155/1000 loss:0.1563 acc:0.7628\n",
      "Validation Epoch: 155/1000 loss:0.1663 acc:0.7589\n",
      "Training Epoch: 156/1000 loss:0.1540 acc:0.7633\n",
      "Validation Epoch: 156/1000 loss:0.1612 acc:0.7578\n",
      "Training Epoch: 157/1000 loss:0.1523 acc:0.7637\n",
      "Validation Epoch: 157/1000 loss:0.1578 acc:0.7594\n",
      "Training Epoch: 158/1000 loss:0.1508 acc:0.7643\n",
      "Validation Epoch: 158/1000 loss:0.1557 acc:0.7594\n",
      "Training Epoch: 159/1000 loss:0.1488 acc:0.7650\n",
      "Validation Epoch: 159/1000 loss:0.1543 acc:0.7589\n",
      "Training Epoch: 160/1000 loss:0.1478 acc:0.7644\n",
      "Validation Epoch: 160/1000 loss:0.1523 acc:0.7589\n",
      "Training Epoch: 161/1000 loss:0.1464 acc:0.7648\n",
      "Validation Epoch: 161/1000 loss:0.1512 acc:0.7600\n",
      "Training Epoch: 162/1000 loss:0.1456 acc:0.7650\n",
      "Validation Epoch: 162/1000 loss:0.1498 acc:0.7606\n",
      "Training Epoch: 163/1000 loss:0.1476 acc:0.7633\n",
      "Validation Epoch: 163/1000 loss:0.1526 acc:0.7628\n",
      "Training Epoch: 164/1000 loss:0.1470 acc:0.7639\n",
      "Validation Epoch: 164/1000 loss:0.1519 acc:0.7528\n",
      "Training Epoch: 165/1000 loss:0.1462 acc:0.7596\n",
      "Validation Epoch: 165/1000 loss:0.1528 acc:0.7611\n",
      "Training Epoch: 166/1000 loss:0.1476 acc:0.7643\n",
      "Validation Epoch: 166/1000 loss:0.1529 acc:0.7494\n",
      "Training Epoch: 167/1000 loss:0.1480 acc:0.7591\n",
      "Validation Epoch: 167/1000 loss:0.1511 acc:0.7628\n",
      "Training Epoch: 168/1000 loss:0.1440 acc:0.7652\n",
      "Validation Epoch: 168/1000 loss:0.1602 acc:0.7483\n",
      "Training Epoch: 169/1000 loss:0.1518 acc:0.7576\n",
      "Validation Epoch: 169/1000 loss:0.1546 acc:0.7594\n",
      "Training Epoch: 170/1000 loss:0.1479 acc:0.7626\n",
      "Validation Epoch: 170/1000 loss:0.1548 acc:0.7589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 171/1000 loss:0.1466 acc:0.7650\n",
      "Validation Epoch: 171/1000 loss:0.1528 acc:0.7594\n",
      "Training Epoch: 172/1000 loss:0.1472 acc:0.7644\n",
      "Validation Epoch: 172/1000 loss:0.1456 acc:0.7622\n",
      "Training Epoch: 173/1000 loss:0.1460 acc:0.7646\n",
      "Validation Epoch: 173/1000 loss:0.1534 acc:0.7611\n",
      "Training Epoch: 174/1000 loss:0.1432 acc:0.7676\n",
      "Validation Epoch: 174/1000 loss:0.1529 acc:0.7628\n",
      "Training Epoch: 175/1000 loss:0.1450 acc:0.7663\n",
      "Validation Epoch: 175/1000 loss:0.1562 acc:0.7611\n",
      "Training Epoch: 176/1000 loss:0.1500 acc:0.7648\n",
      "Validation Epoch: 176/1000 loss:0.1568 acc:0.7617\n",
      "Training Epoch: 177/1000 loss:0.1520 acc:0.7644\n",
      "Validation Epoch: 177/1000 loss:0.1580 acc:0.7622\n",
      "Training Epoch: 178/1000 loss:0.1513 acc:0.7656\n",
      "Validation Epoch: 178/1000 loss:0.1569 acc:0.7628\n",
      "Training Epoch: 179/1000 loss:0.1493 acc:0.7644\n",
      "Validation Epoch: 179/1000 loss:0.1525 acc:0.7622\n",
      "Training Epoch: 180/1000 loss:0.1471 acc:0.7656\n",
      "Validation Epoch: 180/1000 loss:0.1514 acc:0.7622\n",
      "Training Epoch: 181/1000 loss:0.1460 acc:0.7643\n",
      "Validation Epoch: 181/1000 loss:0.1558 acc:0.7611\n",
      "Training Epoch: 182/1000 loss:0.1529 acc:0.7639\n",
      "Validation Epoch: 182/1000 loss:0.1619 acc:0.7583\n",
      "Training Epoch: 183/1000 loss:0.1548 acc:0.7626\n",
      "Validation Epoch: 183/1000 loss:0.1533 acc:0.7639\n",
      "Training Epoch: 184/1000 loss:0.1473 acc:0.7652\n",
      "Validation Epoch: 184/1000 loss:0.1602 acc:0.7578\n",
      "Training Epoch: 185/1000 loss:0.1538 acc:0.7598\n",
      "Validation Epoch: 185/1000 loss:0.1486 acc:0.7633\n",
      "Training Epoch: 186/1000 loss:0.1445 acc:0.7659\n",
      "Validation Epoch: 186/1000 loss:0.1417 acc:0.7594\n",
      "Training Epoch: 187/1000 loss:0.1453 acc:0.7628\n",
      "Validation Epoch: 187/1000 loss:0.1457 acc:0.7628\n",
      "Training Epoch: 188/1000 loss:0.1468 acc:0.7650\n",
      "Validation Epoch: 188/1000 loss:0.1435 acc:0.7622\n",
      "Training Epoch: 189/1000 loss:0.1565 acc:0.7598\n",
      "Validation Epoch: 189/1000 loss:0.1432 acc:0.7617\n",
      "Training Epoch: 190/1000 loss:0.1473 acc:0.7644\n",
      "Validation Epoch: 190/1000 loss:0.1448 acc:0.7594\n",
      "Training Epoch: 191/1000 loss:0.1561 acc:0.7594\n",
      "Validation Epoch: 191/1000 loss:0.1581 acc:0.7611\n",
      "Training Epoch: 192/1000 loss:0.1598 acc:0.7617\n",
      "Validation Epoch: 192/1000 loss:0.1769 acc:0.7533\n",
      "Training Epoch: 193/1000 loss:0.1589 acc:0.7631\n",
      "Validation Epoch: 193/1000 loss:0.1540 acc:0.7622\n",
      "Training Epoch: 194/1000 loss:0.1437 acc:0.7678\n",
      "Validation Epoch: 194/1000 loss:0.1413 acc:0.7633\n",
      "Training Epoch: 195/1000 loss:0.1416 acc:0.7681\n",
      "Validation Epoch: 195/1000 loss:0.1435 acc:0.7639\n",
      "Training Epoch: 196/1000 loss:0.1425 acc:0.7681\n",
      "Validation Epoch: 196/1000 loss:0.1390 acc:0.7656\n",
      "Training Epoch: 197/1000 loss:0.1377 acc:0.7689\n",
      "Validation Epoch: 197/1000 loss:0.1385 acc:0.7656\n",
      "Training Epoch: 198/1000 loss:0.1375 acc:0.7683\n",
      "Validation Epoch: 198/1000 loss:0.1370 acc:0.7650\n",
      "Training Epoch: 199/1000 loss:0.1354 acc:0.7680\n",
      "Validation Epoch: 199/1000 loss:0.1365 acc:0.7633\n",
      "Training Epoch: 200/1000 loss:0.1346 acc:0.7678\n",
      "Validation Epoch: 200/1000 loss:0.1364 acc:0.7644\n",
      "Training Epoch: 201/1000 loss:0.1342 acc:0.7681\n",
      "Validation Epoch: 201/1000 loss:0.1344 acc:0.7656\n",
      "Training Epoch: 202/1000 loss:0.1342 acc:0.7683\n",
      "Validation Epoch: 202/1000 loss:0.1331 acc:0.7656\n",
      "Training Epoch: 203/1000 loss:0.1329 acc:0.7689\n",
      "Validation Epoch: 203/1000 loss:0.1351 acc:0.7656\n",
      "Training Epoch: 204/1000 loss:0.1322 acc:0.7683\n",
      "Validation Epoch: 204/1000 loss:0.1341 acc:0.7650\n",
      "Training Epoch: 205/1000 loss:0.1320 acc:0.7689\n",
      "Validation Epoch: 205/1000 loss:0.1329 acc:0.7650\n",
      "Training Epoch: 206/1000 loss:0.1317 acc:0.7683\n",
      "Validation Epoch: 206/1000 loss:0.1333 acc:0.7644\n",
      "Training Epoch: 207/1000 loss:0.1316 acc:0.7685\n",
      "Validation Epoch: 207/1000 loss:0.1354 acc:0.7644\n",
      "Training Epoch: 208/1000 loss:0.1308 acc:0.7689\n",
      "Validation Epoch: 208/1000 loss:0.1352 acc:0.7650\n",
      "Training Epoch: 209/1000 loss:0.1318 acc:0.7683\n",
      "Validation Epoch: 209/1000 loss:0.1354 acc:0.7628\n",
      "Training Epoch: 210/1000 loss:0.1314 acc:0.7681\n",
      "Validation Epoch: 210/1000 loss:0.1389 acc:0.7656\n",
      "Training Epoch: 211/1000 loss:0.1302 acc:0.7696\n",
      "Validation Epoch: 211/1000 loss:0.1377 acc:0.7622\n",
      "Training Epoch: 212/1000 loss:0.1315 acc:0.7687\n",
      "Validation Epoch: 212/1000 loss:0.1380 acc:0.7628\n",
      "Training Epoch: 213/1000 loss:0.1292 acc:0.7694\n",
      "Validation Epoch: 213/1000 loss:0.1362 acc:0.7644\n",
      "Training Epoch: 214/1000 loss:0.1321 acc:0.7674\n",
      "Validation Epoch: 214/1000 loss:0.1348 acc:0.7650\n",
      "Training Epoch: 215/1000 loss:0.1327 acc:0.7676\n",
      "Validation Epoch: 215/1000 loss:0.1347 acc:0.7656\n",
      "Training Epoch: 216/1000 loss:0.1281 acc:0.7687\n",
      "Validation Epoch: 216/1000 loss:0.1353 acc:0.7628\n",
      "Training Epoch: 217/1000 loss:0.1296 acc:0.7674\n",
      "Validation Epoch: 217/1000 loss:0.1329 acc:0.7650\n",
      "Training Epoch: 218/1000 loss:0.1278 acc:0.7683\n",
      "Validation Epoch: 218/1000 loss:0.1353 acc:0.7611\n",
      "Training Epoch: 219/1000 loss:0.1305 acc:0.7676\n",
      "Validation Epoch: 219/1000 loss:0.1397 acc:0.7633\n",
      "Training Epoch: 220/1000 loss:0.1568 acc:0.7472\n",
      "Validation Epoch: 220/1000 loss:0.2374 acc:0.6956\n",
      "Training Epoch: 221/1000 loss:0.1916 acc:0.7352\n",
      "Validation Epoch: 221/1000 loss:0.2153 acc:0.7272\n",
      "Training Epoch: 222/1000 loss:0.2314 acc:0.7154\n",
      "Validation Epoch: 222/1000 loss:0.2339 acc:0.7022\n",
      "Training Epoch: 223/1000 loss:0.2014 acc:0.7304\n",
      "Validation Epoch: 223/1000 loss:0.2071 acc:0.7389\n",
      "Training Epoch: 224/1000 loss:0.1937 acc:0.7509\n",
      "Validation Epoch: 224/1000 loss:0.2018 acc:0.7478\n",
      "Training Epoch: 225/1000 loss:0.1820 acc:0.7543\n",
      "Validation Epoch: 225/1000 loss:0.1866 acc:0.7389\n",
      "Training Epoch: 226/1000 loss:0.1712 acc:0.7506\n",
      "Validation Epoch: 226/1000 loss:0.1787 acc:0.7406\n",
      "Training Epoch: 227/1000 loss:0.1645 acc:0.7528\n",
      "Validation Epoch: 227/1000 loss:0.1721 acc:0.7528\n",
      "Training Epoch: 228/1000 loss:0.1567 acc:0.7624\n",
      "Validation Epoch: 228/1000 loss:0.1636 acc:0.7561\n",
      "Training Epoch: 229/1000 loss:0.1507 acc:0.7630\n",
      "Validation Epoch: 229/1000 loss:0.1589 acc:0.7550\n",
      "Training Epoch: 230/1000 loss:0.1499 acc:0.7622\n",
      "Validation Epoch: 230/1000 loss:0.1671 acc:0.7539\n",
      "Training Epoch: 231/1000 loss:0.1582 acc:0.7587\n",
      "Validation Epoch: 231/1000 loss:0.1718 acc:0.7478\n",
      "Training Epoch: 232/1000 loss:0.1603 acc:0.7581\n",
      "Validation Epoch: 232/1000 loss:0.1660 acc:0.7550\n",
      "Training Epoch: 233/1000 loss:0.1527 acc:0.7615\n",
      "Validation Epoch: 233/1000 loss:0.1572 acc:0.7550\n",
      "Training Epoch: 234/1000 loss:0.1463 acc:0.7613\n",
      "Validation Epoch: 234/1000 loss:0.1496 acc:0.7617\n",
      "Training Epoch: 235/1000 loss:0.1438 acc:0.7657\n",
      "Validation Epoch: 235/1000 loss:0.1480 acc:0.7611\n",
      "Training Epoch: 236/1000 loss:0.1412 acc:0.7652\n",
      "Validation Epoch: 236/1000 loss:0.1444 acc:0.7600\n",
      "Training Epoch: 237/1000 loss:0.1407 acc:0.7654\n",
      "Validation Epoch: 237/1000 loss:0.1429 acc:0.7611\n",
      "Training Epoch: 238/1000 loss:0.1390 acc:0.7652\n",
      "Validation Epoch: 238/1000 loss:0.1416 acc:0.7611\n",
      "Training Epoch: 239/1000 loss:0.1362 acc:0.7665\n",
      "Validation Epoch: 239/1000 loss:0.1398 acc:0.7622\n",
      "Training Epoch: 240/1000 loss:0.1346 acc:0.7654\n",
      "Validation Epoch: 240/1000 loss:0.1396 acc:0.7617\n",
      "Training Epoch: 241/1000 loss:0.1338 acc:0.7657\n",
      "Validation Epoch: 241/1000 loss:0.1389 acc:0.7622\n",
      "Training Epoch: 242/1000 loss:0.1326 acc:0.7667\n",
      "Validation Epoch: 242/1000 loss:0.1387 acc:0.7611\n",
      "Training Epoch: 243/1000 loss:0.1325 acc:0.7661\n",
      "Validation Epoch: 243/1000 loss:0.1391 acc:0.7611\n",
      "Training Epoch: 244/1000 loss:0.1313 acc:0.7665\n",
      "Validation Epoch: 244/1000 loss:0.1359 acc:0.7622\n",
      "Training Epoch: 245/1000 loss:0.1328 acc:0.7657\n",
      "Validation Epoch: 245/1000 loss:0.1408 acc:0.7567\n",
      "Training Epoch: 246/1000 loss:0.1306 acc:0.7663\n",
      "Validation Epoch: 246/1000 loss:0.1364 acc:0.7594\n",
      "Training Epoch: 247/1000 loss:0.1326 acc:0.7646\n",
      "Validation Epoch: 247/1000 loss:0.1415 acc:0.7572\n",
      "Training Epoch: 248/1000 loss:0.1345 acc:0.7652\n",
      "Validation Epoch: 248/1000 loss:0.1450 acc:0.7561\n",
      "Training Epoch: 249/1000 loss:0.1311 acc:0.7676\n",
      "Validation Epoch: 249/1000 loss:0.1368 acc:0.7622\n",
      "Training Epoch: 250/1000 loss:0.1300 acc:0.7657\n",
      "Validation Epoch: 250/1000 loss:0.1379 acc:0.7617\n",
      "Training Epoch: 251/1000 loss:0.1352 acc:0.7663\n",
      "Validation Epoch: 251/1000 loss:0.1508 acc:0.7522\n",
      "Training Epoch: 252/1000 loss:0.1450 acc:0.7631\n",
      "Validation Epoch: 252/1000 loss:0.1542 acc:0.7600\n",
      "Training Epoch: 253/1000 loss:0.1444 acc:0.7654\n",
      "Validation Epoch: 253/1000 loss:0.1494 acc:0.7622\n",
      "Training Epoch: 254/1000 loss:0.1401 acc:0.7672\n",
      "Validation Epoch: 254/1000 loss:0.1433 acc:0.7639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 255/1000 loss:0.1388 acc:0.7674\n",
      "Validation Epoch: 255/1000 loss:0.1442 acc:0.7644\n",
      "Training Epoch: 256/1000 loss:0.1424 acc:0.7669\n",
      "Validation Epoch: 256/1000 loss:0.1403 acc:0.7656\n",
      "Training Epoch: 257/1000 loss:0.1404 acc:0.7672\n",
      "Validation Epoch: 257/1000 loss:0.1421 acc:0.7650\n",
      "Training Epoch: 258/1000 loss:0.1390 acc:0.7667\n",
      "Validation Epoch: 258/1000 loss:0.1413 acc:0.7633\n",
      "Training Epoch: 259/1000 loss:0.1363 acc:0.7681\n",
      "Validation Epoch: 259/1000 loss:0.1387 acc:0.7639\n",
      "Training Epoch: 260/1000 loss:0.1352 acc:0.7681\n",
      "Validation Epoch: 260/1000 loss:0.1359 acc:0.7656\n",
      "Training Epoch: 261/1000 loss:0.1345 acc:0.7680\n",
      "Validation Epoch: 261/1000 loss:0.1350 acc:0.7656\n",
      "Training Epoch: 262/1000 loss:0.1326 acc:0.7685\n",
      "Validation Epoch: 262/1000 loss:0.1336 acc:0.7650\n",
      "Training Epoch: 263/1000 loss:0.1322 acc:0.7670\n",
      "Validation Epoch: 263/1000 loss:0.1327 acc:0.7644\n",
      "Training Epoch: 264/1000 loss:0.1341 acc:0.7633\n",
      "Validation Epoch: 264/1000 loss:0.1423 acc:0.7561\n",
      "Training Epoch: 265/1000 loss:0.1306 acc:0.7654\n",
      "Validation Epoch: 265/1000 loss:0.1378 acc:0.7606\n",
      "Training Epoch: 266/1000 loss:0.1320 acc:0.7674\n",
      "Validation Epoch: 266/1000 loss:0.1362 acc:0.7617\n",
      "Training Epoch: 267/1000 loss:0.1299 acc:0.7659\n",
      "Validation Epoch: 267/1000 loss:0.1318 acc:0.7644\n",
      "Training Epoch: 268/1000 loss:0.1300 acc:0.7676\n",
      "Validation Epoch: 268/1000 loss:0.1391 acc:0.7633\n",
      "Training Epoch: 269/1000 loss:0.1332 acc:0.7659\n",
      "Validation Epoch: 269/1000 loss:0.1354 acc:0.7600\n",
      "Training Epoch: 270/1000 loss:0.1312 acc:0.7663\n",
      "Validation Epoch: 270/1000 loss:0.1317 acc:0.7639\n",
      "Training Epoch: 271/1000 loss:0.1274 acc:0.7683\n",
      "Validation Epoch: 271/1000 loss:0.1294 acc:0.7628\n",
      "Training Epoch: 272/1000 loss:0.1253 acc:0.7680\n",
      "Validation Epoch: 272/1000 loss:0.1284 acc:0.7633\n",
      "Training Epoch: 273/1000 loss:0.1269 acc:0.7676\n",
      "Validation Epoch: 273/1000 loss:0.1341 acc:0.7617\n",
      "Training Epoch: 274/1000 loss:0.1236 acc:0.7689\n",
      "Validation Epoch: 274/1000 loss:0.1273 acc:0.7644\n",
      "Training Epoch: 275/1000 loss:0.1240 acc:0.7681\n",
      "Validation Epoch: 275/1000 loss:0.1270 acc:0.7644\n",
      "Training Epoch: 276/1000 loss:0.1232 acc:0.7687\n",
      "Validation Epoch: 276/1000 loss:0.1262 acc:0.7650\n",
      "Training Epoch: 277/1000 loss:0.1225 acc:0.7685\n",
      "Validation Epoch: 277/1000 loss:0.1294 acc:0.7622\n",
      "Training Epoch: 278/1000 loss:0.1222 acc:0.7687\n",
      "Validation Epoch: 278/1000 loss:0.1277 acc:0.7628\n",
      "Training Epoch: 279/1000 loss:0.1225 acc:0.7680\n",
      "Validation Epoch: 279/1000 loss:0.1274 acc:0.7611\n",
      "Training Epoch: 280/1000 loss:0.1227 acc:0.7672\n",
      "Validation Epoch: 280/1000 loss:0.1263 acc:0.7628\n",
      "Training Epoch: 281/1000 loss:0.1217 acc:0.7696\n",
      "Validation Epoch: 281/1000 loss:0.1257 acc:0.7622\n",
      "Training Epoch: 282/1000 loss:0.1202 acc:0.7696\n",
      "Validation Epoch: 282/1000 loss:0.1241 acc:0.7644\n",
      "Training Epoch: 283/1000 loss:0.1207 acc:0.7689\n",
      "Validation Epoch: 283/1000 loss:0.1259 acc:0.7650\n",
      "Training Epoch: 284/1000 loss:0.1222 acc:0.7693\n",
      "Validation Epoch: 284/1000 loss:0.1284 acc:0.7611\n",
      "Training Epoch: 285/1000 loss:0.1610 acc:0.7563\n",
      "Validation Epoch: 285/1000 loss:0.1994 acc:0.7433\n",
      "Training Epoch: 286/1000 loss:0.1611 acc:0.7630\n",
      "Validation Epoch: 286/1000 loss:0.1636 acc:0.7478\n",
      "Training Epoch: 287/1000 loss:0.1364 acc:0.7626\n",
      "Validation Epoch: 287/1000 loss:0.1344 acc:0.7639\n"
     ]
    }
   ],
   "source": [
    "valid_acc, valid_loss = [], []\n",
    "train_acc, train_loss = [], []\n",
    "\n",
    "# with graph.as_default():\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the global variable instead of loading them or if there is nothing to load.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #     # Restore\n",
    "    #     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    #     saver.restore(sess,\"checkpoints/har-lstm.ckpt\")\n",
    "    \n",
    "    # Epochs/episodes of training/updating the model/network\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Training: Loop over batches\n",
    "        state = sess.run(initial_state)\n",
    "        acc_batch, loss_batch = [], []\n",
    "        for Xbatch, Ybatch in get_batches(Xtrain, Ytrain, batch_size):\n",
    "\n",
    "            # Feed dictionary\n",
    "            feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "            loss, _, state, acc = sess.run([cost, optimizer, final_state, accuracy], feed_dict)\n",
    "            acc_batch.append(acc)\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Print at each epoch/iteration\n",
    "        print(\"Training Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"loss:{:.4f}\".format(np.mean(loss_batch)),\n",
    "              \"acc:{:.4f}\".format(np.mean(acc_batch)))\n",
    "\n",
    "        # Store at each epoch/iteration\n",
    "        train_loss.append(np.mean(loss_batch))\n",
    "        train_acc.append(np.mean(acc_batch))\n",
    "        \n",
    "        # Validation: Loop over batches\n",
    "        state = sess.run(initial_state)\n",
    "        acc_batch, loss_batch = [], []\n",
    "        for Xbatch, Ybatch in get_batches(Xvalid, Yvalid, batch_size):\n",
    "\n",
    "            # Feed dictionary\n",
    "            feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "            loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict)\n",
    "            acc_batch.append(acc)\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Print at each epoch/iteration\n",
    "        print(\"Validation Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"loss:{:.4f}\".format(np.mean(loss_batch)),\n",
    "              \"acc:{:.4f}\".format(np.mean(acc_batch)))\n",
    "\n",
    "        # Store at each epoch/iteration\n",
    "        valid_loss.append(np.mean(loss_batch))\n",
    "        valid_acc.append(np.mean(acc_batch))\n",
    "            \n",
    "    saver.save(sess,\"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(np.array(train_loss), 'r-', np.array(valid_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracies\n",
    "plt.plot(np.array(train_acc), 'r-', valid_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "\n",
    "    # Testing: Loop over batches\n",
    "    state = sess.run(initial_state)\n",
    "    acc_batch, loss_batch = [], []\n",
    "    for Xbatch, Ybatch in get_batches(Xtest, Ytest, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "        loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Print at each epoch/iteration\n",
    "    print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "          \"Test loss: {}\".format(np.mean(loss_batch)),\n",
    "          \"Test acc: {}\".format(np.mean(acc_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
