{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAR classification \n",
    "# Author: Burak Himmetoglu\n",
    "# 8/15/2017\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_data(data_path, split = \"train\"):\n",
    "\t\"\"\" Read data \"\"\"\n",
    "\n",
    "\t# Fixed params\n",
    "\tn_class = 6\n",
    "\tn_steps = 128\n",
    "\n",
    "\t# Paths\n",
    "\tpath_ = os.path.join(data_path, split)\n",
    "\tpath_signals = os.path.join(path_, \"Inertial_Signals\")\n",
    "\n",
    "\t# Read labels and one-hot encode\n",
    "\tlabel_path = os.path.join(path_, \"y_\" + split + \".txt\")\n",
    "\tlabels = pd.read_csv(label_path, header = None)\n",
    "\n",
    "\t# Read time-series data\n",
    "\tchannel_files = os.listdir(path_signals)\n",
    "\tchannel_files.sort()\n",
    "\tn_channels = len(channel_files)\n",
    "\tposix = len(split) + 5\n",
    "\n",
    "\t# Initiate array\n",
    "\tlist_of_channels = []\n",
    "\tX = np.zeros((len(labels), n_steps, n_channels))\n",
    "\ti_ch = 0\n",
    "\tfor fil_ch in channel_files:\n",
    "\t\tchannel_name = fil_ch[:-posix]\n",
    "\t\tdat_ = pd.read_csv(os.path.join(path_signals,fil_ch), delim_whitespace = True, header = None)\n",
    "\t\tX[:,:,i_ch] = dat_.as_matrix()\n",
    "\n",
    "\t\t# Record names\n",
    "\t\tlist_of_channels.append(channel_name)\n",
    "\n",
    "\t\t# iterate\n",
    "\t\ti_ch += 1\n",
    "\n",
    "\t# Return \n",
    "\treturn X, labels[0].values, list_of_channels\n",
    "\n",
    "# How to normalize the input data correctly\n",
    "def standardize(train, test):\n",
    "\t\"\"\" Standardize data \"\"\"\n",
    "\t# Standardize train and test\n",
    "\tX_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "\tX_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "\treturn X_train, X_test\n",
    "\n",
    "# How to onehot-encoding\n",
    "# def one_hot(labels, n_class = 6):\n",
    "# \t\"\"\" One-hot encoding \"\"\"\n",
    "# \texpansion = np.eye(n_class)\n",
    "# \ty = expansion[:, labels-1].T\n",
    "# \tassert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "# \treturn y\n",
    "\n",
    "def get_batches(X, y, batch_size = 100):\n",
    "\t\"\"\" Return a generator for batches \"\"\"\n",
    "\tn_batches = len(X) // batch_size\n",
    "\tX, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "\t# Loop over batches and yield\n",
    "\tfor b in range(0, len(X), batch_size):\n",
    "\t\tyield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data: Traing/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, list_ch_train = read_data(data_path=\"/home/arasdar/datasets/har-data/\", split=\"train\") # train\n",
    "Xtest, Ytest, list_ch_test = read_data(data_path=\"/home/arasdar/datasets/har-data/\", split=\"test\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest = standardize(test=Xtest, train=Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, stratify = Ytrain, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5514, 128, 9) float64\n",
      "(1838, 128, 9) float64\n",
      "(2947, 128, 9) float64\n",
      "(5514,) int64 6 1\n",
      "(1838,) int64 6 1\n",
      "(2947,) int64 6 1\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Xtrain.dtype)\n",
    "print(Xvalid.shape, Xvalid.dtype)\n",
    "print(Xtest.shape, Xtest.dtype)\n",
    "print(Ytrain.shape, Ytrain.dtype, Ytrain.max(), Ytrain.min())\n",
    "print(Yvalid.shape, Yvalid.dtype, Yvalid.max(), Yvalid.min())\n",
    "print(Ytest.shape, Ytest.dtype, Ytest.max(), Ytest.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_size = 27         # 3 times the amount of channels\n",
    "lstm_layers = 1        # Number of layers\n",
    "batch_size = 256       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.001  # Learning rate (default is 0.001)\n",
    "epochs = 1000\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "# with graph.as_default():\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "indices_ = tf.placeholder(tf.int32, [None], name = 'indices')\n",
    "# keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "# learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct inputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 9)\n",
      "(128, ?, 9)\n",
      "(?, 9)\n",
      "128 (?, 9)\n"
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "# Construct the LSTM inputs and LSTM cells\n",
    "print(inputs_.shape)\n",
    "lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "print(lstm_in.shape)\n",
    "lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "print(lstm_in.shape)\n",
    "\n",
    "# Open up the tensor into a list of seq_len pieces\n",
    "lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "print(len(lstm_in), lstm_in[0].shape)\n",
    "\n",
    "# Add LSTM layers\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "#     drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward pass, cost function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-10-09723a368595>\", line 3, in <module>\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state = initial_state)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-09723a368595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with graph.as_default():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# with tf.variable_scope('RNN', reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We only need the last output tensor to pass into a classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1290\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1291\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 339\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    586\u001b[0m     self._kernel = self.add_variable(\n\u001b[1;32m    587\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         shape=[input_depth + h_depth, 4 * self._num_units])\n\u001b[0m\u001b[1;32m    589\u001b[0m     self._bias = self.add_variable(\n\u001b[1;32m    590\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    434\u001b[0m     new_variable = getter(\n\u001b[1;32m    435\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"constraint\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[0;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-10-09723a368595>\", line 3, in <module>\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state = initial_state)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "# with tf.variable_scope('RNN', reuse=True):\n",
    "outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state = initial_state)\n",
    "\n",
    "# We only need the last output tensor to pass into a classifier\n",
    "logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "labels = tf.one_hot(depth=n_classes, indices=indices_)\n",
    "\n",
    "# Loss/cost using labels and logits\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy using logits and labels\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "# Optimize using loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) # No grad clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 (2947, 27)\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs), outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"logits/BiasAdd:0\", shape=(2947, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/1000 loss:1.4527 acc:0.2046\n",
      "Validation Epoch: 1/1000 loss:nan acc:nan\n",
      "Training Epoch: 2/1000 loss:1.4461 acc:0.2348\n",
      "Validation Epoch: 2/1000 loss:nan acc:nan\n",
      "Training Epoch: 3/1000 loss:1.4395 acc:0.2647\n",
      "Validation Epoch: 3/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/1000 loss:1.4330 acc:0.2891\n",
      "Validation Epoch: 4/1000 loss:nan acc:nan\n",
      "Training Epoch: 5/1000 loss:1.4264 acc:0.3088\n",
      "Validation Epoch: 5/1000 loss:nan acc:nan\n",
      "Training Epoch: 6/1000 loss:1.4199 acc:0.3234\n",
      "Validation Epoch: 6/1000 loss:nan acc:nan\n",
      "Training Epoch: 7/1000 loss:1.4134 acc:0.3349\n",
      "Validation Epoch: 7/1000 loss:nan acc:nan\n",
      "Training Epoch: 8/1000 loss:1.4070 acc:0.3441\n",
      "Validation Epoch: 8/1000 loss:nan acc:nan\n",
      "Training Epoch: 9/1000 loss:1.4004 acc:0.3492\n",
      "Validation Epoch: 9/1000 loss:nan acc:nan\n",
      "Training Epoch: 10/1000 loss:1.3939 acc:0.3553\n",
      "Validation Epoch: 10/1000 loss:nan acc:nan\n",
      "Training Epoch: 11/1000 loss:1.3873 acc:0.3590\n",
      "Validation Epoch: 11/1000 loss:nan acc:nan\n",
      "Training Epoch: 12/1000 loss:1.3807 acc:0.3631\n",
      "Validation Epoch: 12/1000 loss:nan acc:nan\n",
      "Training Epoch: 13/1000 loss:1.3740 acc:0.3655\n",
      "Validation Epoch: 13/1000 loss:nan acc:nan\n",
      "Training Epoch: 14/1000 loss:1.3672 acc:0.3685\n",
      "Validation Epoch: 14/1000 loss:nan acc:nan\n",
      "Training Epoch: 15/1000 loss:1.3603 acc:0.3699\n",
      "Validation Epoch: 15/1000 loss:nan acc:nan\n",
      "Training Epoch: 16/1000 loss:1.3533 acc:0.3722\n",
      "Validation Epoch: 16/1000 loss:nan acc:nan\n",
      "Training Epoch: 17/1000 loss:1.3461 acc:0.3736\n",
      "Validation Epoch: 17/1000 loss:nan acc:nan\n",
      "Training Epoch: 18/1000 loss:1.3388 acc:0.3760\n",
      "Validation Epoch: 18/1000 loss:nan acc:nan\n",
      "Training Epoch: 19/1000 loss:1.3314 acc:0.3767\n",
      "Validation Epoch: 19/1000 loss:nan acc:nan\n",
      "Training Epoch: 20/1000 loss:1.3237 acc:0.3797\n",
      "Validation Epoch: 20/1000 loss:nan acc:nan\n",
      "Training Epoch: 21/1000 loss:1.3158 acc:0.3811\n",
      "Validation Epoch: 21/1000 loss:nan acc:nan\n",
      "Training Epoch: 22/1000 loss:1.3078 acc:0.3821\n",
      "Validation Epoch: 22/1000 loss:nan acc:nan\n",
      "Training Epoch: 23/1000 loss:1.2994 acc:0.3814\n",
      "Validation Epoch: 23/1000 loss:nan acc:nan\n",
      "Training Epoch: 24/1000 loss:1.2908 acc:0.3804\n",
      "Validation Epoch: 24/1000 loss:nan acc:nan\n",
      "Training Epoch: 25/1000 loss:1.2820 acc:0.3807\n",
      "Validation Epoch: 25/1000 loss:nan acc:nan\n",
      "Training Epoch: 26/1000 loss:1.2728 acc:0.3821\n",
      "Validation Epoch: 26/1000 loss:nan acc:nan\n",
      "Training Epoch: 27/1000 loss:1.2633 acc:0.3841\n",
      "Validation Epoch: 27/1000 loss:nan acc:nan\n",
      "Training Epoch: 28/1000 loss:1.2535 acc:0.3841\n",
      "Validation Epoch: 28/1000 loss:nan acc:nan\n",
      "Training Epoch: 29/1000 loss:1.2434 acc:0.3838\n",
      "Validation Epoch: 29/1000 loss:nan acc:nan\n",
      "Training Epoch: 30/1000 loss:1.2329 acc:0.3845\n",
      "Validation Epoch: 30/1000 loss:nan acc:nan\n",
      "Training Epoch: 31/1000 loss:1.2221 acc:0.3851\n",
      "Validation Epoch: 31/1000 loss:nan acc:nan\n",
      "Training Epoch: 32/1000 loss:1.2109 acc:0.3851\n",
      "Validation Epoch: 32/1000 loss:nan acc:nan\n",
      "Training Epoch: 33/1000 loss:1.1994 acc:0.3872\n",
      "Validation Epoch: 33/1000 loss:nan acc:nan\n",
      "Training Epoch: 34/1000 loss:1.1876 acc:0.3889\n",
      "Validation Epoch: 34/1000 loss:nan acc:nan\n",
      "Training Epoch: 35/1000 loss:1.1755 acc:0.3909\n",
      "Validation Epoch: 35/1000 loss:nan acc:nan\n",
      "Training Epoch: 36/1000 loss:1.1631 acc:0.3919\n",
      "Validation Epoch: 36/1000 loss:nan acc:nan\n",
      "Training Epoch: 37/1000 loss:1.1506 acc:0.3919\n",
      "Validation Epoch: 37/1000 loss:nan acc:nan\n",
      "Training Epoch: 38/1000 loss:1.1379 acc:0.3953\n",
      "Validation Epoch: 38/1000 loss:nan acc:nan\n",
      "Training Epoch: 39/1000 loss:1.1253 acc:0.3984\n",
      "Validation Epoch: 39/1000 loss:nan acc:nan\n",
      "Training Epoch: 40/1000 loss:1.1127 acc:0.4024\n",
      "Validation Epoch: 40/1000 loss:nan acc:nan\n",
      "Training Epoch: 41/1000 loss:1.1002 acc:0.4069\n",
      "Validation Epoch: 41/1000 loss:nan acc:nan\n",
      "Training Epoch: 42/1000 loss:1.0878 acc:0.4065\n",
      "Validation Epoch: 42/1000 loss:nan acc:nan\n",
      "Training Epoch: 43/1000 loss:1.0756 acc:0.4116\n",
      "Validation Epoch: 43/1000 loss:nan acc:nan\n",
      "Training Epoch: 44/1000 loss:1.0636 acc:0.4157\n",
      "Validation Epoch: 44/1000 loss:nan acc:nan\n",
      "Training Epoch: 45/1000 loss:1.0516 acc:0.4184\n",
      "Validation Epoch: 45/1000 loss:nan acc:nan\n",
      "Training Epoch: 46/1000 loss:1.0398 acc:0.4235\n",
      "Validation Epoch: 46/1000 loss:nan acc:nan\n",
      "Training Epoch: 47/1000 loss:1.0280 acc:0.4330\n",
      "Validation Epoch: 47/1000 loss:nan acc:nan\n",
      "Training Epoch: 48/1000 loss:1.0163 acc:0.4415\n",
      "Validation Epoch: 48/1000 loss:nan acc:nan\n",
      "Training Epoch: 49/1000 loss:1.0045 acc:0.4479\n",
      "Validation Epoch: 49/1000 loss:nan acc:nan\n",
      "Training Epoch: 50/1000 loss:0.9929 acc:0.4530\n",
      "Validation Epoch: 50/1000 loss:nan acc:nan\n",
      "Training Epoch: 51/1000 loss:0.9812 acc:0.4611\n",
      "Validation Epoch: 51/1000 loss:nan acc:nan\n",
      "Training Epoch: 52/1000 loss:0.9690 acc:0.4706\n",
      "Validation Epoch: 52/1000 loss:nan acc:nan\n",
      "Training Epoch: 53/1000 loss:0.9576 acc:0.4788\n",
      "Validation Epoch: 53/1000 loss:nan acc:nan\n",
      "Training Epoch: 54/1000 loss:0.9467 acc:0.4849\n",
      "Validation Epoch: 54/1000 loss:nan acc:nan\n",
      "Training Epoch: 55/1000 loss:0.9360 acc:0.4897\n",
      "Validation Epoch: 55/1000 loss:nan acc:nan\n",
      "Training Epoch: 56/1000 loss:0.9256 acc:0.4937\n",
      "Validation Epoch: 56/1000 loss:nan acc:nan\n",
      "Training Epoch: 57/1000 loss:0.9159 acc:0.4964\n",
      "Validation Epoch: 57/1000 loss:nan acc:nan\n",
      "Training Epoch: 58/1000 loss:0.9067 acc:0.4985\n",
      "Validation Epoch: 58/1000 loss:nan acc:nan\n",
      "Training Epoch: 59/1000 loss:0.8978 acc:0.5036\n",
      "Validation Epoch: 59/1000 loss:nan acc:nan\n",
      "Training Epoch: 60/1000 loss:0.8891 acc:0.5097\n",
      "Validation Epoch: 60/1000 loss:nan acc:nan\n",
      "Training Epoch: 61/1000 loss:0.8806 acc:0.5107\n",
      "Validation Epoch: 61/1000 loss:nan acc:nan\n",
      "Training Epoch: 62/1000 loss:0.8728 acc:0.5124\n",
      "Validation Epoch: 62/1000 loss:nan acc:nan\n",
      "Training Epoch: 63/1000 loss:0.8651 acc:0.5158\n",
      "Validation Epoch: 63/1000 loss:nan acc:nan\n",
      "Training Epoch: 64/1000 loss:0.8574 acc:0.5161\n",
      "Validation Epoch: 64/1000 loss:nan acc:nan\n",
      "Training Epoch: 65/1000 loss:0.8509 acc:0.5168\n",
      "Validation Epoch: 65/1000 loss:nan acc:nan\n",
      "Training Epoch: 66/1000 loss:0.8451 acc:0.5154\n",
      "Validation Epoch: 66/1000 loss:nan acc:nan\n",
      "Training Epoch: 67/1000 loss:0.8392 acc:0.5165\n",
      "Validation Epoch: 67/1000 loss:nan acc:nan\n",
      "Training Epoch: 68/1000 loss:0.8328 acc:0.5175\n",
      "Validation Epoch: 68/1000 loss:nan acc:nan\n",
      "Training Epoch: 69/1000 loss:0.8283 acc:0.5175\n",
      "Validation Epoch: 69/1000 loss:nan acc:nan\n",
      "Training Epoch: 70/1000 loss:0.8242 acc:0.5185\n",
      "Validation Epoch: 70/1000 loss:nan acc:nan\n",
      "Training Epoch: 71/1000 loss:0.8200 acc:0.5229\n",
      "Validation Epoch: 71/1000 loss:nan acc:nan\n",
      "Training Epoch: 72/1000 loss:0.8160 acc:0.5249\n",
      "Validation Epoch: 72/1000 loss:nan acc:nan\n",
      "Training Epoch: 73/1000 loss:0.8118 acc:0.5246\n",
      "Validation Epoch: 73/1000 loss:nan acc:nan\n",
      "Training Epoch: 74/1000 loss:0.8071 acc:0.5266\n",
      "Validation Epoch: 74/1000 loss:nan acc:nan\n",
      "Training Epoch: 75/1000 loss:0.8031 acc:0.5280\n",
      "Validation Epoch: 75/1000 loss:nan acc:nan\n",
      "Training Epoch: 76/1000 loss:0.7990 acc:0.5297\n",
      "Validation Epoch: 76/1000 loss:nan acc:nan\n",
      "Training Epoch: 77/1000 loss:0.7950 acc:0.5324\n",
      "Validation Epoch: 77/1000 loss:nan acc:nan\n",
      "Training Epoch: 78/1000 loss:0.7905 acc:0.5344\n",
      "Validation Epoch: 78/1000 loss:nan acc:nan\n",
      "Training Epoch: 79/1000 loss:0.7864 acc:0.5378\n",
      "Validation Epoch: 79/1000 loss:nan acc:nan\n",
      "Training Epoch: 80/1000 loss:0.7835 acc:0.5372\n",
      "Validation Epoch: 80/1000 loss:nan acc:nan\n",
      "Training Epoch: 81/1000 loss:0.7810 acc:0.5338\n",
      "Validation Epoch: 81/1000 loss:nan acc:nan\n",
      "Training Epoch: 82/1000 loss:0.7775 acc:0.5355\n",
      "Validation Epoch: 82/1000 loss:nan acc:nan\n",
      "Training Epoch: 83/1000 loss:0.7737 acc:0.5368\n",
      "Validation Epoch: 83/1000 loss:nan acc:nan\n",
      "Training Epoch: 84/1000 loss:0.7699 acc:0.5436\n",
      "Validation Epoch: 84/1000 loss:nan acc:nan\n",
      "Training Epoch: 85/1000 loss:0.7657 acc:0.5456\n",
      "Validation Epoch: 85/1000 loss:nan acc:nan\n",
      "Training Epoch: 86/1000 loss:0.7620 acc:0.5463\n",
      "Validation Epoch: 86/1000 loss:nan acc:nan\n",
      "Training Epoch: 87/1000 loss:0.7584 acc:0.5480\n",
      "Validation Epoch: 87/1000 loss:nan acc:nan\n",
      "Training Epoch: 88/1000 loss:0.7550 acc:0.5484\n",
      "Validation Epoch: 88/1000 loss:nan acc:nan\n",
      "Training Epoch: 89/1000 loss:0.7524 acc:0.5490\n",
      "Validation Epoch: 89/1000 loss:nan acc:nan\n",
      "Training Epoch: 90/1000 loss:0.7491 acc:0.5511\n",
      "Validation Epoch: 90/1000 loss:nan acc:nan\n",
      "Training Epoch: 91/1000 loss:0.7461 acc:0.5521\n",
      "Validation Epoch: 91/1000 loss:nan acc:nan\n",
      "Training Epoch: 92/1000 loss:0.7429 acc:0.5534\n",
      "Validation Epoch: 92/1000 loss:nan acc:nan\n",
      "Training Epoch: 93/1000 loss:0.7397 acc:0.5534\n",
      "Validation Epoch: 93/1000 loss:nan acc:nan\n",
      "Training Epoch: 94/1000 loss:0.7367 acc:0.5517\n",
      "Validation Epoch: 94/1000 loss:nan acc:nan\n",
      "Training Epoch: 95/1000 loss:0.7343 acc:0.5514\n",
      "Validation Epoch: 95/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 96/1000 loss:0.7313 acc:0.5531\n",
      "Validation Epoch: 96/1000 loss:nan acc:nan\n",
      "Training Epoch: 97/1000 loss:0.7289 acc:0.5548\n",
      "Validation Epoch: 97/1000 loss:nan acc:nan\n",
      "Training Epoch: 98/1000 loss:0.7262 acc:0.5568\n",
      "Validation Epoch: 98/1000 loss:nan acc:nan\n",
      "Training Epoch: 99/1000 loss:0.7235 acc:0.5575\n",
      "Validation Epoch: 99/1000 loss:nan acc:nan\n",
      "Training Epoch: 100/1000 loss:0.7208 acc:0.5582\n",
      "Validation Epoch: 100/1000 loss:nan acc:nan\n",
      "Training Epoch: 101/1000 loss:0.7181 acc:0.5596\n",
      "Validation Epoch: 101/1000 loss:nan acc:nan\n",
      "Training Epoch: 102/1000 loss:0.7150 acc:0.5612\n",
      "Validation Epoch: 102/1000 loss:nan acc:nan\n",
      "Training Epoch: 103/1000 loss:0.7121 acc:0.5619\n",
      "Validation Epoch: 103/1000 loss:nan acc:nan\n",
      "Training Epoch: 104/1000 loss:0.7094 acc:0.5633\n",
      "Validation Epoch: 104/1000 loss:nan acc:nan\n",
      "Training Epoch: 105/1000 loss:0.7066 acc:0.5633\n",
      "Validation Epoch: 105/1000 loss:nan acc:nan\n",
      "Training Epoch: 106/1000 loss:0.7039 acc:0.5636\n",
      "Validation Epoch: 106/1000 loss:nan acc:nan\n",
      "Training Epoch: 107/1000 loss:0.7005 acc:0.5646\n",
      "Validation Epoch: 107/1000 loss:nan acc:nan\n",
      "Training Epoch: 108/1000 loss:0.6977 acc:0.5657\n",
      "Validation Epoch: 108/1000 loss:nan acc:nan\n",
      "Training Epoch: 109/1000 loss:0.6950 acc:0.5657\n",
      "Validation Epoch: 109/1000 loss:nan acc:nan\n",
      "Training Epoch: 110/1000 loss:0.6919 acc:0.5663\n",
      "Validation Epoch: 110/1000 loss:nan acc:nan\n",
      "Training Epoch: 111/1000 loss:0.6893 acc:0.5663\n",
      "Validation Epoch: 111/1000 loss:nan acc:nan\n",
      "Training Epoch: 112/1000 loss:0.6868 acc:0.5670\n",
      "Validation Epoch: 112/1000 loss:nan acc:nan\n",
      "Training Epoch: 113/1000 loss:0.6838 acc:0.5687\n",
      "Validation Epoch: 113/1000 loss:nan acc:nan\n",
      "Training Epoch: 114/1000 loss:0.6809 acc:0.5701\n",
      "Validation Epoch: 114/1000 loss:nan acc:nan\n",
      "Training Epoch: 115/1000 loss:0.6785 acc:0.5704\n",
      "Validation Epoch: 115/1000 loss:nan acc:nan\n",
      "Training Epoch: 116/1000 loss:0.6758 acc:0.5714\n",
      "Validation Epoch: 116/1000 loss:nan acc:nan\n",
      "Training Epoch: 117/1000 loss:0.6734 acc:0.5721\n",
      "Validation Epoch: 117/1000 loss:nan acc:nan\n",
      "Training Epoch: 118/1000 loss:0.6711 acc:0.5738\n",
      "Validation Epoch: 118/1000 loss:nan acc:nan\n",
      "Training Epoch: 119/1000 loss:0.6687 acc:0.5745\n",
      "Validation Epoch: 119/1000 loss:nan acc:nan\n",
      "Training Epoch: 120/1000 loss:0.6663 acc:0.5755\n",
      "Validation Epoch: 120/1000 loss:nan acc:nan\n",
      "Training Epoch: 121/1000 loss:0.6640 acc:0.5755\n",
      "Validation Epoch: 121/1000 loss:nan acc:nan\n",
      "Training Epoch: 122/1000 loss:0.6615 acc:0.5765\n",
      "Validation Epoch: 122/1000 loss:nan acc:nan\n",
      "Training Epoch: 123/1000 loss:0.6591 acc:0.5789\n",
      "Validation Epoch: 123/1000 loss:nan acc:nan\n",
      "Training Epoch: 124/1000 loss:0.6570 acc:0.5789\n",
      "Validation Epoch: 124/1000 loss:nan acc:nan\n",
      "Training Epoch: 125/1000 loss:0.6545 acc:0.5789\n",
      "Validation Epoch: 125/1000 loss:nan acc:nan\n",
      "Training Epoch: 126/1000 loss:0.6520 acc:0.5806\n",
      "Validation Epoch: 126/1000 loss:nan acc:nan\n",
      "Training Epoch: 127/1000 loss:0.6494 acc:0.5840\n",
      "Validation Epoch: 127/1000 loss:nan acc:nan\n",
      "Training Epoch: 128/1000 loss:0.6467 acc:0.5860\n",
      "Validation Epoch: 128/1000 loss:nan acc:nan\n",
      "Training Epoch: 129/1000 loss:0.6445 acc:0.5881\n",
      "Validation Epoch: 129/1000 loss:nan acc:nan\n",
      "Training Epoch: 130/1000 loss:0.6415 acc:0.5884\n",
      "Validation Epoch: 130/1000 loss:nan acc:nan\n",
      "Training Epoch: 131/1000 loss:0.6389 acc:0.5891\n",
      "Validation Epoch: 131/1000 loss:nan acc:nan\n",
      "Training Epoch: 132/1000 loss:0.6364 acc:0.5894\n",
      "Validation Epoch: 132/1000 loss:nan acc:nan\n",
      "Training Epoch: 133/1000 loss:0.6339 acc:0.5894\n",
      "Validation Epoch: 133/1000 loss:nan acc:nan\n",
      "Training Epoch: 134/1000 loss:0.6314 acc:0.5914\n",
      "Validation Epoch: 134/1000 loss:nan acc:nan\n",
      "Training Epoch: 135/1000 loss:0.6288 acc:0.5914\n",
      "Validation Epoch: 135/1000 loss:nan acc:nan\n",
      "Training Epoch: 136/1000 loss:0.6260 acc:0.5931\n",
      "Validation Epoch: 136/1000 loss:nan acc:nan\n",
      "Training Epoch: 137/1000 loss:0.6232 acc:0.5935\n",
      "Validation Epoch: 137/1000 loss:nan acc:nan\n",
      "Training Epoch: 138/1000 loss:0.6205 acc:0.5928\n",
      "Validation Epoch: 138/1000 loss:nan acc:nan\n",
      "Training Epoch: 139/1000 loss:0.6177 acc:0.5935\n",
      "Validation Epoch: 139/1000 loss:nan acc:nan\n",
      "Training Epoch: 140/1000 loss:0.6149 acc:0.5955\n",
      "Validation Epoch: 140/1000 loss:nan acc:nan\n",
      "Training Epoch: 141/1000 loss:0.6121 acc:0.5982\n",
      "Validation Epoch: 141/1000 loss:nan acc:nan\n",
      "Training Epoch: 142/1000 loss:0.6091 acc:0.5999\n",
      "Validation Epoch: 142/1000 loss:nan acc:nan\n",
      "Training Epoch: 143/1000 loss:0.6063 acc:0.6023\n",
      "Validation Epoch: 143/1000 loss:nan acc:nan\n",
      "Training Epoch: 144/1000 loss:0.6045 acc:0.6023\n",
      "Validation Epoch: 144/1000 loss:nan acc:nan\n",
      "Training Epoch: 145/1000 loss:0.6017 acc:0.6033\n",
      "Validation Epoch: 145/1000 loss:nan acc:nan\n",
      "Training Epoch: 146/1000 loss:0.5988 acc:0.6050\n",
      "Validation Epoch: 146/1000 loss:nan acc:nan\n",
      "Training Epoch: 147/1000 loss:0.5959 acc:0.6060\n",
      "Validation Epoch: 147/1000 loss:nan acc:nan\n",
      "Training Epoch: 148/1000 loss:0.5929 acc:0.6067\n",
      "Validation Epoch: 148/1000 loss:nan acc:nan\n",
      "Training Epoch: 149/1000 loss:0.5898 acc:0.6088\n",
      "Validation Epoch: 149/1000 loss:nan acc:nan\n",
      "Training Epoch: 150/1000 loss:0.5866 acc:0.6105\n",
      "Validation Epoch: 150/1000 loss:nan acc:nan\n",
      "Training Epoch: 151/1000 loss:0.5835 acc:0.6108\n",
      "Validation Epoch: 151/1000 loss:nan acc:nan\n",
      "Training Epoch: 152/1000 loss:0.5802 acc:0.6132\n",
      "Validation Epoch: 152/1000 loss:nan acc:nan\n",
      "Training Epoch: 153/1000 loss:0.5768 acc:0.6145\n",
      "Validation Epoch: 153/1000 loss:nan acc:nan\n",
      "Training Epoch: 154/1000 loss:0.5733 acc:0.6159\n",
      "Validation Epoch: 154/1000 loss:nan acc:nan\n",
      "Training Epoch: 155/1000 loss:0.5698 acc:0.6176\n",
      "Validation Epoch: 155/1000 loss:nan acc:nan\n",
      "Training Epoch: 156/1000 loss:0.5661 acc:0.6200\n",
      "Validation Epoch: 156/1000 loss:nan acc:nan\n",
      "Training Epoch: 157/1000 loss:0.5627 acc:0.6230\n",
      "Validation Epoch: 157/1000 loss:nan acc:nan\n",
      "Training Epoch: 158/1000 loss:0.5591 acc:0.6247\n",
      "Validation Epoch: 158/1000 loss:nan acc:nan\n",
      "Training Epoch: 159/1000 loss:0.5552 acc:0.6284\n",
      "Validation Epoch: 159/1000 loss:nan acc:nan\n",
      "Training Epoch: 160/1000 loss:0.5511 acc:0.6312\n",
      "Validation Epoch: 160/1000 loss:nan acc:nan\n",
      "Training Epoch: 161/1000 loss:0.5468 acc:0.6342\n",
      "Validation Epoch: 161/1000 loss:nan acc:nan\n",
      "Training Epoch: 162/1000 loss:0.5424 acc:0.6386\n",
      "Validation Epoch: 162/1000 loss:nan acc:nan\n",
      "Training Epoch: 163/1000 loss:0.5382 acc:0.6430\n",
      "Validation Epoch: 163/1000 loss:nan acc:nan\n",
      "Training Epoch: 164/1000 loss:0.5349 acc:0.6440\n",
      "Validation Epoch: 164/1000 loss:nan acc:nan\n",
      "Training Epoch: 165/1000 loss:0.5307 acc:0.6468\n",
      "Validation Epoch: 165/1000 loss:nan acc:nan\n",
      "Training Epoch: 166/1000 loss:0.5263 acc:0.6512\n",
      "Validation Epoch: 166/1000 loss:nan acc:nan\n",
      "Training Epoch: 167/1000 loss:0.5224 acc:0.6529\n",
      "Validation Epoch: 167/1000 loss:nan acc:nan\n",
      "Training Epoch: 168/1000 loss:0.5182 acc:0.6573\n",
      "Validation Epoch: 168/1000 loss:nan acc:nan\n",
      "Training Epoch: 169/1000 loss:0.5142 acc:0.6603\n",
      "Validation Epoch: 169/1000 loss:nan acc:nan\n",
      "Training Epoch: 170/1000 loss:0.5097 acc:0.6641\n",
      "Validation Epoch: 170/1000 loss:nan acc:nan\n",
      "Training Epoch: 171/1000 loss:0.5052 acc:0.6668\n",
      "Validation Epoch: 171/1000 loss:nan acc:nan\n",
      "Training Epoch: 172/1000 loss:0.5008 acc:0.6732\n",
      "Validation Epoch: 172/1000 loss:nan acc:nan\n",
      "Training Epoch: 173/1000 loss:0.4961 acc:0.6766\n",
      "Validation Epoch: 173/1000 loss:nan acc:nan\n",
      "Training Epoch: 174/1000 loss:0.4919 acc:0.6807\n",
      "Validation Epoch: 174/1000 loss:nan acc:nan\n",
      "Training Epoch: 175/1000 loss:0.4883 acc:0.6787\n",
      "Validation Epoch: 175/1000 loss:nan acc:nan\n",
      "Training Epoch: 176/1000 loss:0.4844 acc:0.6790\n",
      "Validation Epoch: 176/1000 loss:nan acc:nan\n",
      "Training Epoch: 177/1000 loss:0.4792 acc:0.6837\n",
      "Validation Epoch: 177/1000 loss:nan acc:nan\n",
      "Training Epoch: 178/1000 loss:0.4740 acc:0.6885\n",
      "Validation Epoch: 178/1000 loss:nan acc:nan\n",
      "Training Epoch: 179/1000 loss:0.4694 acc:0.6922\n",
      "Validation Epoch: 179/1000 loss:nan acc:nan\n",
      "Training Epoch: 180/1000 loss:0.4649 acc:0.6963\n",
      "Validation Epoch: 180/1000 loss:nan acc:nan\n",
      "Training Epoch: 181/1000 loss:0.4592 acc:0.7011\n",
      "Validation Epoch: 181/1000 loss:nan acc:nan\n",
      "Training Epoch: 182/1000 loss:0.4536 acc:0.7055\n",
      "Validation Epoch: 182/1000 loss:nan acc:nan\n",
      "Training Epoch: 183/1000 loss:0.4490 acc:0.7065\n",
      "Validation Epoch: 183/1000 loss:nan acc:nan\n",
      "Training Epoch: 184/1000 loss:0.4437 acc:0.7082\n",
      "Validation Epoch: 184/1000 loss:nan acc:nan\n",
      "Training Epoch: 185/1000 loss:0.4397 acc:0.7089\n",
      "Validation Epoch: 185/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 186/1000 loss:0.4376 acc:0.7109\n",
      "Validation Epoch: 186/1000 loss:nan acc:nan\n",
      "Training Epoch: 187/1000 loss:0.4330 acc:0.7156\n",
      "Validation Epoch: 187/1000 loss:nan acc:nan\n",
      "Training Epoch: 188/1000 loss:0.4279 acc:0.7184\n",
      "Validation Epoch: 188/1000 loss:nan acc:nan\n",
      "Training Epoch: 189/1000 loss:0.4224 acc:0.7194\n",
      "Validation Epoch: 189/1000 loss:nan acc:nan\n",
      "Training Epoch: 190/1000 loss:0.4182 acc:0.7211\n",
      "Validation Epoch: 190/1000 loss:nan acc:nan\n",
      "Training Epoch: 191/1000 loss:0.4151 acc:0.7218\n",
      "Validation Epoch: 191/1000 loss:nan acc:nan\n",
      "Training Epoch: 192/1000 loss:0.4119 acc:0.7251\n",
      "Validation Epoch: 192/1000 loss:nan acc:nan\n",
      "Training Epoch: 193/1000 loss:0.4091 acc:0.7268\n",
      "Validation Epoch: 193/1000 loss:nan acc:nan\n",
      "Training Epoch: 194/1000 loss:0.4043 acc:0.7272\n",
      "Validation Epoch: 194/1000 loss:nan acc:nan\n",
      "Training Epoch: 195/1000 loss:0.3994 acc:0.7299\n",
      "Validation Epoch: 195/1000 loss:nan acc:nan\n",
      "Training Epoch: 196/1000 loss:0.3947 acc:0.7323\n",
      "Validation Epoch: 196/1000 loss:nan acc:nan\n",
      "Training Epoch: 197/1000 loss:0.3901 acc:0.7340\n",
      "Validation Epoch: 197/1000 loss:nan acc:nan\n",
      "Training Epoch: 198/1000 loss:0.3862 acc:0.7340\n",
      "Validation Epoch: 198/1000 loss:nan acc:nan\n",
      "Training Epoch: 199/1000 loss:0.3822 acc:0.7336\n",
      "Validation Epoch: 199/1000 loss:nan acc:nan\n",
      "Training Epoch: 200/1000 loss:0.3803 acc:0.7326\n",
      "Validation Epoch: 200/1000 loss:nan acc:nan\n",
      "Training Epoch: 201/1000 loss:0.3772 acc:0.7336\n",
      "Validation Epoch: 201/1000 loss:nan acc:nan\n",
      "Training Epoch: 202/1000 loss:0.3735 acc:0.7340\n",
      "Validation Epoch: 202/1000 loss:nan acc:nan\n",
      "Training Epoch: 203/1000 loss:0.3698 acc:0.7360\n",
      "Validation Epoch: 203/1000 loss:nan acc:nan\n",
      "Training Epoch: 204/1000 loss:0.3667 acc:0.7353\n",
      "Validation Epoch: 204/1000 loss:nan acc:nan\n",
      "Training Epoch: 205/1000 loss:0.3644 acc:0.7387\n",
      "Validation Epoch: 205/1000 loss:nan acc:nan\n",
      "Training Epoch: 206/1000 loss:0.3613 acc:0.7404\n",
      "Validation Epoch: 206/1000 loss:nan acc:nan\n",
      "Training Epoch: 207/1000 loss:0.3593 acc:0.7411\n",
      "Validation Epoch: 207/1000 loss:nan acc:nan\n",
      "Training Epoch: 208/1000 loss:0.3565 acc:0.7404\n",
      "Validation Epoch: 208/1000 loss:nan acc:nan\n",
      "Training Epoch: 209/1000 loss:0.3566 acc:0.7401\n",
      "Validation Epoch: 209/1000 loss:nan acc:nan\n",
      "Training Epoch: 210/1000 loss:0.3552 acc:0.7394\n",
      "Validation Epoch: 210/1000 loss:nan acc:nan\n",
      "Training Epoch: 211/1000 loss:0.3530 acc:0.7401\n",
      "Validation Epoch: 211/1000 loss:nan acc:nan\n",
      "Training Epoch: 212/1000 loss:0.3517 acc:0.7418\n",
      "Validation Epoch: 212/1000 loss:nan acc:nan\n",
      "Training Epoch: 213/1000 loss:0.3487 acc:0.7411\n",
      "Validation Epoch: 213/1000 loss:nan acc:nan\n",
      "Training Epoch: 214/1000 loss:0.3456 acc:0.7431\n",
      "Validation Epoch: 214/1000 loss:nan acc:nan\n",
      "Training Epoch: 215/1000 loss:0.3441 acc:0.7424\n",
      "Validation Epoch: 215/1000 loss:nan acc:nan\n",
      "Training Epoch: 216/1000 loss:0.3441 acc:0.7404\n",
      "Validation Epoch: 216/1000 loss:nan acc:nan\n",
      "Training Epoch: 217/1000 loss:0.3423 acc:0.7431\n",
      "Validation Epoch: 217/1000 loss:nan acc:nan\n",
      "Training Epoch: 218/1000 loss:0.3403 acc:0.7428\n",
      "Validation Epoch: 218/1000 loss:nan acc:nan\n",
      "Training Epoch: 219/1000 loss:0.3384 acc:0.7421\n",
      "Validation Epoch: 219/1000 loss:nan acc:nan\n",
      "Training Epoch: 220/1000 loss:0.3390 acc:0.7414\n",
      "Validation Epoch: 220/1000 loss:nan acc:nan\n",
      "Training Epoch: 221/1000 loss:0.3365 acc:0.7421\n",
      "Validation Epoch: 221/1000 loss:nan acc:nan\n",
      "Training Epoch: 222/1000 loss:0.3351 acc:0.7414\n",
      "Validation Epoch: 222/1000 loss:nan acc:nan\n",
      "Training Epoch: 223/1000 loss:0.3330 acc:0.7428\n",
      "Validation Epoch: 223/1000 loss:nan acc:nan\n",
      "Training Epoch: 224/1000 loss:0.3304 acc:0.7418\n",
      "Validation Epoch: 224/1000 loss:nan acc:nan\n",
      "Training Epoch: 225/1000 loss:0.3281 acc:0.7435\n",
      "Validation Epoch: 225/1000 loss:nan acc:nan\n",
      "Training Epoch: 226/1000 loss:0.3413 acc:0.7360\n",
      "Validation Epoch: 226/1000 loss:nan acc:nan\n",
      "Training Epoch: 227/1000 loss:0.3634 acc:0.7190\n",
      "Validation Epoch: 227/1000 loss:nan acc:nan\n",
      "Training Epoch: 228/1000 loss:0.3820 acc:0.7099\n",
      "Validation Epoch: 228/1000 loss:nan acc:nan\n",
      "Training Epoch: 229/1000 loss:0.3768 acc:0.7133\n",
      "Validation Epoch: 229/1000 loss:nan acc:nan\n",
      "Training Epoch: 230/1000 loss:0.3536 acc:0.7251\n",
      "Validation Epoch: 230/1000 loss:nan acc:nan\n",
      "Training Epoch: 231/1000 loss:0.3351 acc:0.7350\n",
      "Validation Epoch: 231/1000 loss:nan acc:nan\n",
      "Training Epoch: 232/1000 loss:0.3214 acc:0.7424\n",
      "Validation Epoch: 232/1000 loss:nan acc:nan\n",
      "Training Epoch: 233/1000 loss:0.3265 acc:0.7367\n",
      "Validation Epoch: 233/1000 loss:nan acc:nan\n",
      "Training Epoch: 234/1000 loss:0.3322 acc:0.7323\n",
      "Validation Epoch: 234/1000 loss:nan acc:nan\n",
      "Training Epoch: 235/1000 loss:0.3286 acc:0.7309\n",
      "Validation Epoch: 235/1000 loss:nan acc:nan\n",
      "Training Epoch: 236/1000 loss:0.3214 acc:0.7374\n",
      "Validation Epoch: 236/1000 loss:nan acc:nan\n",
      "Training Epoch: 237/1000 loss:0.3166 acc:0.7394\n",
      "Validation Epoch: 237/1000 loss:nan acc:nan\n",
      "Training Epoch: 238/1000 loss:0.3134 acc:0.7424\n",
      "Validation Epoch: 238/1000 loss:nan acc:nan\n",
      "Training Epoch: 239/1000 loss:0.3195 acc:0.7394\n",
      "Validation Epoch: 239/1000 loss:nan acc:nan\n",
      "Training Epoch: 240/1000 loss:0.3154 acc:0.7414\n",
      "Validation Epoch: 240/1000 loss:nan acc:nan\n",
      "Training Epoch: 241/1000 loss:0.3129 acc:0.7435\n",
      "Validation Epoch: 241/1000 loss:nan acc:nan\n",
      "Training Epoch: 242/1000 loss:0.3121 acc:0.7441\n",
      "Validation Epoch: 242/1000 loss:nan acc:nan\n",
      "Training Epoch: 243/1000 loss:0.3175 acc:0.7384\n",
      "Validation Epoch: 243/1000 loss:nan acc:nan\n",
      "Training Epoch: 244/1000 loss:0.3201 acc:0.7360\n",
      "Validation Epoch: 244/1000 loss:nan acc:nan\n",
      "Training Epoch: 245/1000 loss:0.3186 acc:0.7346\n",
      "Validation Epoch: 245/1000 loss:nan acc:nan\n",
      "Training Epoch: 246/1000 loss:0.3176 acc:0.7353\n",
      "Validation Epoch: 246/1000 loss:nan acc:nan\n",
      "Training Epoch: 247/1000 loss:0.3162 acc:0.7370\n",
      "Validation Epoch: 247/1000 loss:nan acc:nan\n",
      "Training Epoch: 248/1000 loss:0.3152 acc:0.7357\n",
      "Validation Epoch: 248/1000 loss:nan acc:nan\n",
      "Training Epoch: 249/1000 loss:0.3122 acc:0.7401\n",
      "Validation Epoch: 249/1000 loss:nan acc:nan\n",
      "Training Epoch: 250/1000 loss:0.3086 acc:0.7414\n",
      "Validation Epoch: 250/1000 loss:nan acc:nan\n",
      "Training Epoch: 251/1000 loss:0.3060 acc:0.7421\n",
      "Validation Epoch: 251/1000 loss:nan acc:nan\n",
      "Training Epoch: 252/1000 loss:0.3036 acc:0.7424\n",
      "Validation Epoch: 252/1000 loss:nan acc:nan\n",
      "Training Epoch: 253/1000 loss:0.3043 acc:0.7411\n",
      "Validation Epoch: 253/1000 loss:nan acc:nan\n",
      "Training Epoch: 254/1000 loss:0.3034 acc:0.7435\n",
      "Validation Epoch: 254/1000 loss:nan acc:nan\n",
      "Training Epoch: 255/1000 loss:0.3098 acc:0.7411\n",
      "Validation Epoch: 255/1000 loss:nan acc:nan\n",
      "Training Epoch: 256/1000 loss:0.3161 acc:0.7380\n",
      "Validation Epoch: 256/1000 loss:nan acc:nan\n",
      "Training Epoch: 257/1000 loss:0.3223 acc:0.7346\n",
      "Validation Epoch: 257/1000 loss:nan acc:nan\n",
      "Training Epoch: 258/1000 loss:0.3242 acc:0.7329\n",
      "Validation Epoch: 258/1000 loss:nan acc:nan\n",
      "Training Epoch: 259/1000 loss:0.3235 acc:0.7329\n",
      "Validation Epoch: 259/1000 loss:nan acc:nan\n",
      "Training Epoch: 260/1000 loss:0.3201 acc:0.7346\n",
      "Validation Epoch: 260/1000 loss:nan acc:nan\n",
      "Training Epoch: 261/1000 loss:0.3136 acc:0.7367\n",
      "Validation Epoch: 261/1000 loss:nan acc:nan\n",
      "Training Epoch: 262/1000 loss:0.3067 acc:0.7404\n",
      "Validation Epoch: 262/1000 loss:nan acc:nan\n",
      "Training Epoch: 263/1000 loss:0.3037 acc:0.7421\n",
      "Validation Epoch: 263/1000 loss:nan acc:nan\n",
      "Training Epoch: 264/1000 loss:0.3001 acc:0.7428\n",
      "Validation Epoch: 264/1000 loss:nan acc:nan\n",
      "Training Epoch: 265/1000 loss:0.2957 acc:0.7458\n",
      "Validation Epoch: 265/1000 loss:nan acc:nan\n",
      "Training Epoch: 266/1000 loss:0.2932 acc:0.7455\n",
      "Validation Epoch: 266/1000 loss:nan acc:nan\n",
      "Training Epoch: 267/1000 loss:0.2944 acc:0.7428\n",
      "Validation Epoch: 267/1000 loss:nan acc:nan\n",
      "Training Epoch: 268/1000 loss:0.3002 acc:0.7394\n",
      "Validation Epoch: 268/1000 loss:nan acc:nan\n",
      "Training Epoch: 269/1000 loss:0.3012 acc:0.7384\n",
      "Validation Epoch: 269/1000 loss:nan acc:nan\n",
      "Training Epoch: 270/1000 loss:0.2990 acc:0.7387\n",
      "Validation Epoch: 270/1000 loss:nan acc:nan\n",
      "Training Epoch: 271/1000 loss:0.2925 acc:0.7401\n",
      "Validation Epoch: 271/1000 loss:nan acc:nan\n",
      "Training Epoch: 272/1000 loss:0.2873 acc:0.7431\n",
      "Validation Epoch: 272/1000 loss:nan acc:nan\n",
      "Training Epoch: 273/1000 loss:0.2842 acc:0.7452\n",
      "Validation Epoch: 273/1000 loss:nan acc:nan\n",
      "Training Epoch: 274/1000 loss:0.2823 acc:0.7469\n",
      "Validation Epoch: 274/1000 loss:nan acc:nan\n",
      "Training Epoch: 275/1000 loss:0.2829 acc:0.7475\n",
      "Validation Epoch: 275/1000 loss:nan acc:nan\n",
      "Training Epoch: 276/1000 loss:0.2803 acc:0.7472\n",
      "Validation Epoch: 276/1000 loss:nan acc:nan\n",
      "Training Epoch: 277/1000 loss:0.2798 acc:0.7472\n",
      "Validation Epoch: 277/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 278/1000 loss:0.2793 acc:0.7462\n",
      "Validation Epoch: 278/1000 loss:nan acc:nan\n",
      "Training Epoch: 279/1000 loss:0.2792 acc:0.7472\n",
      "Validation Epoch: 279/1000 loss:nan acc:nan\n",
      "Training Epoch: 280/1000 loss:0.2776 acc:0.7479\n",
      "Validation Epoch: 280/1000 loss:nan acc:nan\n",
      "Training Epoch: 281/1000 loss:0.2759 acc:0.7479\n",
      "Validation Epoch: 281/1000 loss:nan acc:nan\n",
      "Training Epoch: 282/1000 loss:0.2737 acc:0.7479\n",
      "Validation Epoch: 282/1000 loss:nan acc:nan\n",
      "Training Epoch: 283/1000 loss:0.2718 acc:0.7486\n",
      "Validation Epoch: 283/1000 loss:nan acc:nan\n",
      "Training Epoch: 284/1000 loss:0.2702 acc:0.7496\n",
      "Validation Epoch: 284/1000 loss:nan acc:nan\n",
      "Training Epoch: 285/1000 loss:0.2663 acc:0.7513\n",
      "Validation Epoch: 285/1000 loss:nan acc:nan\n",
      "Training Epoch: 286/1000 loss:0.2652 acc:0.7509\n",
      "Validation Epoch: 286/1000 loss:nan acc:nan\n",
      "Training Epoch: 287/1000 loss:0.2634 acc:0.7503\n",
      "Validation Epoch: 287/1000 loss:nan acc:nan\n",
      "Training Epoch: 288/1000 loss:0.2634 acc:0.7499\n",
      "Validation Epoch: 288/1000 loss:nan acc:nan\n",
      "Training Epoch: 289/1000 loss:0.2626 acc:0.7503\n",
      "Validation Epoch: 289/1000 loss:nan acc:nan\n",
      "Training Epoch: 290/1000 loss:0.2619 acc:0.7499\n",
      "Validation Epoch: 290/1000 loss:nan acc:nan\n",
      "Training Epoch: 291/1000 loss:0.2609 acc:0.7496\n",
      "Validation Epoch: 291/1000 loss:nan acc:nan\n",
      "Training Epoch: 292/1000 loss:0.2594 acc:0.7503\n",
      "Validation Epoch: 292/1000 loss:nan acc:nan\n",
      "Training Epoch: 293/1000 loss:0.2579 acc:0.7506\n",
      "Validation Epoch: 293/1000 loss:nan acc:nan\n",
      "Training Epoch: 294/1000 loss:0.2543 acc:0.7513\n",
      "Validation Epoch: 294/1000 loss:nan acc:nan\n",
      "Training Epoch: 295/1000 loss:0.2527 acc:0.7516\n",
      "Validation Epoch: 295/1000 loss:nan acc:nan\n",
      "Training Epoch: 296/1000 loss:0.2520 acc:0.7513\n",
      "Validation Epoch: 296/1000 loss:nan acc:nan\n",
      "Training Epoch: 297/1000 loss:0.2513 acc:0.7513\n",
      "Validation Epoch: 297/1000 loss:nan acc:nan\n",
      "Training Epoch: 298/1000 loss:0.2549 acc:0.7513\n",
      "Validation Epoch: 298/1000 loss:nan acc:nan\n",
      "Training Epoch: 299/1000 loss:0.2478 acc:0.7523\n",
      "Validation Epoch: 299/1000 loss:nan acc:nan\n",
      "Training Epoch: 300/1000 loss:0.2458 acc:0.7526\n",
      "Validation Epoch: 300/1000 loss:nan acc:nan\n",
      "Training Epoch: 301/1000 loss:0.2444 acc:0.7530\n",
      "Validation Epoch: 301/1000 loss:nan acc:nan\n",
      "Training Epoch: 302/1000 loss:0.2431 acc:0.7536\n",
      "Validation Epoch: 302/1000 loss:nan acc:nan\n",
      "Training Epoch: 303/1000 loss:0.2424 acc:0.7540\n",
      "Validation Epoch: 303/1000 loss:nan acc:nan\n",
      "Training Epoch: 304/1000 loss:0.2424 acc:0.7536\n",
      "Validation Epoch: 304/1000 loss:nan acc:nan\n",
      "Training Epoch: 305/1000 loss:0.2409 acc:0.7540\n",
      "Validation Epoch: 305/1000 loss:nan acc:nan\n",
      "Training Epoch: 306/1000 loss:0.2415 acc:0.7547\n",
      "Validation Epoch: 306/1000 loss:nan acc:nan\n",
      "Training Epoch: 307/1000 loss:0.2387 acc:0.7553\n",
      "Validation Epoch: 307/1000 loss:nan acc:nan\n",
      "Training Epoch: 308/1000 loss:0.2366 acc:0.7553\n",
      "Validation Epoch: 308/1000 loss:nan acc:nan\n",
      "Training Epoch: 309/1000 loss:0.2354 acc:0.7557\n",
      "Validation Epoch: 309/1000 loss:nan acc:nan\n",
      "Training Epoch: 310/1000 loss:0.2339 acc:0.7560\n",
      "Validation Epoch: 310/1000 loss:nan acc:nan\n",
      "Training Epoch: 311/1000 loss:0.2351 acc:0.7557\n",
      "Validation Epoch: 311/1000 loss:nan acc:nan\n",
      "Training Epoch: 312/1000 loss:0.2345 acc:0.7557\n",
      "Validation Epoch: 312/1000 loss:nan acc:nan\n",
      "Training Epoch: 313/1000 loss:0.2350 acc:0.7553\n",
      "Validation Epoch: 313/1000 loss:nan acc:nan\n",
      "Training Epoch: 314/1000 loss:0.2443 acc:0.7506\n",
      "Validation Epoch: 314/1000 loss:nan acc:nan\n",
      "Training Epoch: 315/1000 loss:0.2423 acc:0.7520\n",
      "Validation Epoch: 315/1000 loss:nan acc:nan\n",
      "Training Epoch: 316/1000 loss:0.2370 acc:0.7543\n",
      "Validation Epoch: 316/1000 loss:nan acc:nan\n",
      "Training Epoch: 317/1000 loss:0.2687 acc:0.7329\n",
      "Validation Epoch: 317/1000 loss:nan acc:nan\n",
      "Training Epoch: 318/1000 loss:0.3313 acc:0.7000\n",
      "Validation Epoch: 318/1000 loss:nan acc:nan\n",
      "Training Epoch: 319/1000 loss:0.3860 acc:0.6736\n",
      "Validation Epoch: 319/1000 loss:nan acc:nan\n",
      "Training Epoch: 320/1000 loss:0.4307 acc:0.6532\n",
      "Validation Epoch: 320/1000 loss:nan acc:nan\n",
      "Training Epoch: 321/1000 loss:0.4680 acc:0.6362\n",
      "Validation Epoch: 321/1000 loss:nan acc:nan\n",
      "Training Epoch: 322/1000 loss:0.4873 acc:0.6250\n",
      "Validation Epoch: 322/1000 loss:nan acc:nan\n",
      "Training Epoch: 323/1000 loss:0.4932 acc:0.6271\n",
      "Validation Epoch: 323/1000 loss:nan acc:nan\n",
      "Training Epoch: 324/1000 loss:0.4910 acc:0.6284\n",
      "Validation Epoch: 324/1000 loss:nan acc:nan\n",
      "Training Epoch: 325/1000 loss:0.4822 acc:0.6312\n",
      "Validation Epoch: 325/1000 loss:nan acc:nan\n",
      "Training Epoch: 326/1000 loss:0.4692 acc:0.6369\n",
      "Validation Epoch: 326/1000 loss:nan acc:nan\n",
      "Training Epoch: 327/1000 loss:0.4504 acc:0.6464\n",
      "Validation Epoch: 327/1000 loss:nan acc:nan\n",
      "Training Epoch: 328/1000 loss:0.4249 acc:0.6586\n",
      "Validation Epoch: 328/1000 loss:nan acc:nan\n",
      "Training Epoch: 329/1000 loss:0.3971 acc:0.6715\n",
      "Validation Epoch: 329/1000 loss:nan acc:nan\n",
      "Training Epoch: 330/1000 loss:0.3715 acc:0.6827\n",
      "Validation Epoch: 330/1000 loss:nan acc:nan\n",
      "Training Epoch: 331/1000 loss:0.3487 acc:0.6922\n",
      "Validation Epoch: 331/1000 loss:nan acc:nan\n",
      "Training Epoch: 332/1000 loss:0.3322 acc:0.7014\n",
      "Validation Epoch: 332/1000 loss:nan acc:nan\n",
      "Training Epoch: 333/1000 loss:0.3229 acc:0.7048\n",
      "Validation Epoch: 333/1000 loss:nan acc:nan\n",
      "Training Epoch: 334/1000 loss:0.3165 acc:0.7078\n",
      "Validation Epoch: 334/1000 loss:nan acc:nan\n",
      "Training Epoch: 335/1000 loss:0.3135 acc:0.7099\n",
      "Validation Epoch: 335/1000 loss:nan acc:nan\n",
      "Training Epoch: 336/1000 loss:0.3096 acc:0.7136\n",
      "Validation Epoch: 336/1000 loss:nan acc:nan\n",
      "Training Epoch: 337/1000 loss:0.3078 acc:0.7136\n",
      "Validation Epoch: 337/1000 loss:nan acc:nan\n",
      "Training Epoch: 338/1000 loss:0.3049 acc:0.7167\n",
      "Validation Epoch: 338/1000 loss:nan acc:nan\n",
      "Training Epoch: 339/1000 loss:0.3033 acc:0.7197\n",
      "Validation Epoch: 339/1000 loss:nan acc:nan\n",
      "Training Epoch: 340/1000 loss:0.3016 acc:0.7211\n",
      "Validation Epoch: 340/1000 loss:nan acc:nan\n",
      "Training Epoch: 341/1000 loss:0.3038 acc:0.7197\n",
      "Validation Epoch: 341/1000 loss:nan acc:nan\n",
      "Training Epoch: 342/1000 loss:0.3059 acc:0.7173\n",
      "Validation Epoch: 342/1000 loss:nan acc:nan\n",
      "Training Epoch: 343/1000 loss:0.3050 acc:0.7167\n",
      "Validation Epoch: 343/1000 loss:nan acc:nan\n",
      "Training Epoch: 344/1000 loss:0.3034 acc:0.7184\n",
      "Validation Epoch: 344/1000 loss:nan acc:nan\n",
      "Training Epoch: 345/1000 loss:0.3007 acc:0.7207\n",
      "Validation Epoch: 345/1000 loss:nan acc:nan\n",
      "Training Epoch: 346/1000 loss:0.2991 acc:0.7221\n",
      "Validation Epoch: 346/1000 loss:nan acc:nan\n",
      "Training Epoch: 347/1000 loss:0.2984 acc:0.7218\n",
      "Validation Epoch: 347/1000 loss:nan acc:nan\n",
      "Training Epoch: 348/1000 loss:0.2971 acc:0.7231\n",
      "Validation Epoch: 348/1000 loss:nan acc:nan\n",
      "Training Epoch: 349/1000 loss:0.2958 acc:0.7234\n",
      "Validation Epoch: 349/1000 loss:nan acc:nan\n",
      "Training Epoch: 350/1000 loss:0.2942 acc:0.7234\n",
      "Validation Epoch: 350/1000 loss:nan acc:nan\n",
      "Training Epoch: 351/1000 loss:0.2913 acc:0.7248\n",
      "Validation Epoch: 351/1000 loss:nan acc:nan\n",
      "Training Epoch: 352/1000 loss:0.2889 acc:0.7262\n",
      "Validation Epoch: 352/1000 loss:nan acc:nan\n",
      "Training Epoch: 353/1000 loss:0.2871 acc:0.7279\n",
      "Validation Epoch: 353/1000 loss:nan acc:nan\n",
      "Training Epoch: 354/1000 loss:0.2841 acc:0.7292\n",
      "Validation Epoch: 354/1000 loss:nan acc:nan\n",
      "Training Epoch: 355/1000 loss:0.2815 acc:0.7302\n",
      "Validation Epoch: 355/1000 loss:nan acc:nan\n",
      "Training Epoch: 356/1000 loss:0.2783 acc:0.7313\n",
      "Validation Epoch: 356/1000 loss:nan acc:nan\n",
      "Training Epoch: 357/1000 loss:0.2767 acc:0.7323\n",
      "Validation Epoch: 357/1000 loss:nan acc:nan\n",
      "Training Epoch: 358/1000 loss:0.2751 acc:0.7340\n",
      "Validation Epoch: 358/1000 loss:nan acc:nan\n",
      "Training Epoch: 359/1000 loss:0.2734 acc:0.7346\n",
      "Validation Epoch: 359/1000 loss:nan acc:nan\n",
      "Training Epoch: 360/1000 loss:0.2731 acc:0.7343\n",
      "Validation Epoch: 360/1000 loss:nan acc:nan\n",
      "Training Epoch: 361/1000 loss:0.2730 acc:0.7343\n",
      "Validation Epoch: 361/1000 loss:nan acc:nan\n",
      "Training Epoch: 362/1000 loss:0.2727 acc:0.7353\n",
      "Validation Epoch: 362/1000 loss:nan acc:nan\n",
      "Training Epoch: 363/1000 loss:0.2726 acc:0.7360\n",
      "Validation Epoch: 363/1000 loss:nan acc:nan\n",
      "Training Epoch: 364/1000 loss:0.2725 acc:0.7360\n",
      "Validation Epoch: 364/1000 loss:nan acc:nan\n",
      "Training Epoch: 365/1000 loss:0.2725 acc:0.7357\n",
      "Validation Epoch: 365/1000 loss:nan acc:nan\n",
      "Training Epoch: 366/1000 loss:0.2728 acc:0.7353\n",
      "Validation Epoch: 366/1000 loss:nan acc:nan\n",
      "Training Epoch: 367/1000 loss:0.2733 acc:0.7343\n",
      "Validation Epoch: 367/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 368/1000 loss:0.2834 acc:0.7296\n",
      "Validation Epoch: 368/1000 loss:nan acc:nan\n",
      "Training Epoch: 369/1000 loss:0.2951 acc:0.7228\n",
      "Validation Epoch: 369/1000 loss:nan acc:nan\n",
      "Training Epoch: 370/1000 loss:0.3052 acc:0.7190\n",
      "Validation Epoch: 370/1000 loss:nan acc:nan\n",
      "Training Epoch: 371/1000 loss:0.3207 acc:0.7126\n",
      "Validation Epoch: 371/1000 loss:nan acc:nan\n",
      "Training Epoch: 372/1000 loss:0.3340 acc:0.7078\n",
      "Validation Epoch: 372/1000 loss:nan acc:nan\n",
      "Training Epoch: 373/1000 loss:0.3423 acc:0.7048\n",
      "Validation Epoch: 373/1000 loss:nan acc:nan\n",
      "Training Epoch: 374/1000 loss:0.3468 acc:0.7017\n",
      "Validation Epoch: 374/1000 loss:nan acc:nan\n",
      "Training Epoch: 375/1000 loss:0.3494 acc:0.7021\n",
      "Validation Epoch: 375/1000 loss:nan acc:nan\n",
      "Training Epoch: 376/1000 loss:0.3486 acc:0.7038\n",
      "Validation Epoch: 376/1000 loss:nan acc:nan\n",
      "Training Epoch: 377/1000 loss:0.3436 acc:0.7078\n",
      "Validation Epoch: 377/1000 loss:nan acc:nan\n",
      "Training Epoch: 378/1000 loss:0.3287 acc:0.7133\n",
      "Validation Epoch: 378/1000 loss:nan acc:nan\n",
      "Training Epoch: 379/1000 loss:0.3154 acc:0.7207\n",
      "Validation Epoch: 379/1000 loss:nan acc:nan\n",
      "Training Epoch: 380/1000 loss:0.3061 acc:0.7255\n",
      "Validation Epoch: 380/1000 loss:nan acc:nan\n",
      "Training Epoch: 381/1000 loss:0.2981 acc:0.7282\n",
      "Validation Epoch: 381/1000 loss:nan acc:nan\n",
      "Training Epoch: 382/1000 loss:0.2902 acc:0.7306\n",
      "Validation Epoch: 382/1000 loss:nan acc:nan\n",
      "Training Epoch: 383/1000 loss:0.2816 acc:0.7357\n",
      "Validation Epoch: 383/1000 loss:nan acc:nan\n",
      "Training Epoch: 384/1000 loss:0.2768 acc:0.7363\n",
      "Validation Epoch: 384/1000 loss:nan acc:nan\n",
      "Training Epoch: 385/1000 loss:0.2726 acc:0.7384\n",
      "Validation Epoch: 385/1000 loss:nan acc:nan\n",
      "Training Epoch: 386/1000 loss:0.2705 acc:0.7404\n",
      "Validation Epoch: 386/1000 loss:nan acc:nan\n",
      "Training Epoch: 387/1000 loss:0.2663 acc:0.7418\n",
      "Validation Epoch: 387/1000 loss:nan acc:nan\n",
      "Training Epoch: 388/1000 loss:0.2629 acc:0.7431\n",
      "Validation Epoch: 388/1000 loss:nan acc:nan\n",
      "Training Epoch: 389/1000 loss:0.2600 acc:0.7435\n",
      "Validation Epoch: 389/1000 loss:nan acc:nan\n",
      "Training Epoch: 390/1000 loss:0.2574 acc:0.7452\n",
      "Validation Epoch: 390/1000 loss:nan acc:nan\n",
      "Training Epoch: 391/1000 loss:0.2560 acc:0.7452\n",
      "Validation Epoch: 391/1000 loss:nan acc:nan\n",
      "Training Epoch: 392/1000 loss:0.2566 acc:0.7435\n",
      "Validation Epoch: 392/1000 loss:nan acc:nan\n",
      "Training Epoch: 393/1000 loss:0.2564 acc:0.7435\n",
      "Validation Epoch: 393/1000 loss:nan acc:nan\n",
      "Training Epoch: 394/1000 loss:0.2566 acc:0.7445\n",
      "Validation Epoch: 394/1000 loss:nan acc:nan\n",
      "Training Epoch: 395/1000 loss:0.2577 acc:0.7455\n",
      "Validation Epoch: 395/1000 loss:nan acc:nan\n",
      "Training Epoch: 396/1000 loss:0.2588 acc:0.7448\n",
      "Validation Epoch: 396/1000 loss:nan acc:nan\n",
      "Training Epoch: 397/1000 loss:0.2601 acc:0.7441\n",
      "Validation Epoch: 397/1000 loss:nan acc:nan\n",
      "Training Epoch: 398/1000 loss:0.2593 acc:0.7445\n",
      "Validation Epoch: 398/1000 loss:nan acc:nan\n",
      "Training Epoch: 399/1000 loss:0.2578 acc:0.7458\n",
      "Validation Epoch: 399/1000 loss:nan acc:nan\n",
      "Training Epoch: 400/1000 loss:0.2541 acc:0.7469\n",
      "Validation Epoch: 400/1000 loss:nan acc:nan\n",
      "Training Epoch: 401/1000 loss:0.2532 acc:0.7469\n",
      "Validation Epoch: 401/1000 loss:nan acc:nan\n",
      "Training Epoch: 402/1000 loss:0.2526 acc:0.7469\n",
      "Validation Epoch: 402/1000 loss:nan acc:nan\n",
      "Training Epoch: 403/1000 loss:0.2519 acc:0.7462\n",
      "Validation Epoch: 403/1000 loss:nan acc:nan\n",
      "Training Epoch: 404/1000 loss:0.2513 acc:0.7469\n",
      "Validation Epoch: 404/1000 loss:nan acc:nan\n",
      "Training Epoch: 405/1000 loss:0.2507 acc:0.7465\n",
      "Validation Epoch: 405/1000 loss:nan acc:nan\n",
      "Training Epoch: 406/1000 loss:0.2497 acc:0.7469\n",
      "Validation Epoch: 406/1000 loss:nan acc:nan\n",
      "Training Epoch: 407/1000 loss:0.2481 acc:0.7472\n",
      "Validation Epoch: 407/1000 loss:nan acc:nan\n",
      "Training Epoch: 408/1000 loss:0.2476 acc:0.7472\n",
      "Validation Epoch: 408/1000 loss:nan acc:nan\n",
      "Training Epoch: 409/1000 loss:0.2465 acc:0.7479\n",
      "Validation Epoch: 409/1000 loss:nan acc:nan\n",
      "Training Epoch: 410/1000 loss:0.2458 acc:0.7486\n",
      "Validation Epoch: 410/1000 loss:nan acc:nan\n",
      "Training Epoch: 411/1000 loss:0.2453 acc:0.7486\n",
      "Validation Epoch: 411/1000 loss:nan acc:nan\n",
      "Training Epoch: 412/1000 loss:0.2448 acc:0.7486\n",
      "Validation Epoch: 412/1000 loss:nan acc:nan\n",
      "Training Epoch: 413/1000 loss:0.2445 acc:0.7486\n",
      "Validation Epoch: 413/1000 loss:nan acc:nan\n",
      "Training Epoch: 414/1000 loss:0.2441 acc:0.7482\n",
      "Validation Epoch: 414/1000 loss:nan acc:nan\n",
      "Training Epoch: 415/1000 loss:0.2435 acc:0.7492\n",
      "Validation Epoch: 415/1000 loss:nan acc:nan\n",
      "Training Epoch: 416/1000 loss:0.2431 acc:0.7492\n",
      "Validation Epoch: 416/1000 loss:nan acc:nan\n",
      "Training Epoch: 417/1000 loss:0.2427 acc:0.7492\n",
      "Validation Epoch: 417/1000 loss:nan acc:nan\n",
      "Training Epoch: 418/1000 loss:0.2423 acc:0.7492\n",
      "Validation Epoch: 418/1000 loss:nan acc:nan\n",
      "Training Epoch: 419/1000 loss:0.2420 acc:0.7492\n",
      "Validation Epoch: 419/1000 loss:nan acc:nan\n",
      "Training Epoch: 420/1000 loss:0.2416 acc:0.7492\n",
      "Validation Epoch: 420/1000 loss:nan acc:nan\n",
      "Training Epoch: 421/1000 loss:0.2412 acc:0.7496\n",
      "Validation Epoch: 421/1000 loss:nan acc:nan\n",
      "Training Epoch: 422/1000 loss:0.2408 acc:0.7499\n",
      "Validation Epoch: 422/1000 loss:nan acc:nan\n",
      "Training Epoch: 423/1000 loss:0.2404 acc:0.7499\n",
      "Validation Epoch: 423/1000 loss:nan acc:nan\n",
      "Training Epoch: 424/1000 loss:0.2400 acc:0.7496\n",
      "Validation Epoch: 424/1000 loss:nan acc:nan\n",
      "Training Epoch: 425/1000 loss:0.2396 acc:0.7492\n",
      "Validation Epoch: 425/1000 loss:nan acc:nan\n",
      "Training Epoch: 426/1000 loss:0.2392 acc:0.7492\n",
      "Validation Epoch: 426/1000 loss:nan acc:nan\n",
      "Training Epoch: 427/1000 loss:0.2388 acc:0.7492\n",
      "Validation Epoch: 427/1000 loss:nan acc:nan\n",
      "Training Epoch: 428/1000 loss:0.2385 acc:0.7489\n",
      "Validation Epoch: 428/1000 loss:nan acc:nan\n",
      "Training Epoch: 429/1000 loss:0.2381 acc:0.7489\n",
      "Validation Epoch: 429/1000 loss:nan acc:nan\n",
      "Training Epoch: 430/1000 loss:0.2378 acc:0.7489\n",
      "Validation Epoch: 430/1000 loss:nan acc:nan\n",
      "Training Epoch: 431/1000 loss:0.2374 acc:0.7489\n",
      "Validation Epoch: 431/1000 loss:nan acc:nan\n",
      "Training Epoch: 432/1000 loss:0.2371 acc:0.7489\n",
      "Validation Epoch: 432/1000 loss:nan acc:nan\n",
      "Training Epoch: 433/1000 loss:0.2367 acc:0.7489\n",
      "Validation Epoch: 433/1000 loss:nan acc:nan\n",
      "Training Epoch: 434/1000 loss:0.2363 acc:0.7492\n",
      "Validation Epoch: 434/1000 loss:nan acc:nan\n",
      "Training Epoch: 435/1000 loss:0.2360 acc:0.7492\n",
      "Validation Epoch: 435/1000 loss:nan acc:nan\n",
      "Training Epoch: 436/1000 loss:0.2357 acc:0.7492\n",
      "Validation Epoch: 436/1000 loss:nan acc:nan\n",
      "Training Epoch: 437/1000 loss:0.2353 acc:0.7496\n",
      "Validation Epoch: 437/1000 loss:nan acc:nan\n",
      "Training Epoch: 438/1000 loss:0.2350 acc:0.7499\n",
      "Validation Epoch: 438/1000 loss:nan acc:nan\n",
      "Training Epoch: 439/1000 loss:0.2347 acc:0.7499\n",
      "Validation Epoch: 439/1000 loss:nan acc:nan\n",
      "Training Epoch: 440/1000 loss:0.2343 acc:0.7499\n",
      "Validation Epoch: 440/1000 loss:nan acc:nan\n",
      "Training Epoch: 441/1000 loss:0.2340 acc:0.7499\n",
      "Validation Epoch: 441/1000 loss:nan acc:nan\n",
      "Training Epoch: 442/1000 loss:0.2338 acc:0.7499\n",
      "Validation Epoch: 442/1000 loss:nan acc:nan\n",
      "Training Epoch: 443/1000 loss:0.2335 acc:0.7499\n",
      "Validation Epoch: 443/1000 loss:nan acc:nan\n",
      "Training Epoch: 444/1000 loss:0.2332 acc:0.7503\n",
      "Validation Epoch: 444/1000 loss:nan acc:nan\n",
      "Training Epoch: 445/1000 loss:0.2329 acc:0.7503\n",
      "Validation Epoch: 445/1000 loss:nan acc:nan\n",
      "Training Epoch: 446/1000 loss:0.2326 acc:0.7506\n",
      "Validation Epoch: 446/1000 loss:nan acc:nan\n",
      "Training Epoch: 447/1000 loss:0.2324 acc:0.7506\n",
      "Validation Epoch: 447/1000 loss:nan acc:nan\n",
      "Training Epoch: 448/1000 loss:0.2320 acc:0.7509\n",
      "Validation Epoch: 448/1000 loss:nan acc:nan\n",
      "Training Epoch: 449/1000 loss:0.2318 acc:0.7509\n",
      "Validation Epoch: 449/1000 loss:nan acc:nan\n",
      "Training Epoch: 450/1000 loss:0.2315 acc:0.7509\n",
      "Validation Epoch: 450/1000 loss:nan acc:nan\n",
      "Training Epoch: 451/1000 loss:0.2312 acc:0.7509\n",
      "Validation Epoch: 451/1000 loss:nan acc:nan\n",
      "Training Epoch: 452/1000 loss:0.2310 acc:0.7509\n",
      "Validation Epoch: 452/1000 loss:nan acc:nan\n",
      "Training Epoch: 453/1000 loss:0.2307 acc:0.7513\n",
      "Validation Epoch: 453/1000 loss:nan acc:nan\n",
      "Training Epoch: 454/1000 loss:0.2304 acc:0.7509\n",
      "Validation Epoch: 454/1000 loss:nan acc:nan\n",
      "Training Epoch: 455/1000 loss:0.2301 acc:0.7509\n",
      "Validation Epoch: 455/1000 loss:nan acc:nan\n",
      "Training Epoch: 456/1000 loss:0.2297 acc:0.7509\n",
      "Validation Epoch: 456/1000 loss:nan acc:nan\n",
      "Training Epoch: 457/1000 loss:0.2295 acc:0.7509\n",
      "Validation Epoch: 457/1000 loss:nan acc:nan\n",
      "Training Epoch: 458/1000 loss:0.2292 acc:0.7509\n",
      "Validation Epoch: 458/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 459/1000 loss:0.2290 acc:0.7509\n",
      "Validation Epoch: 459/1000 loss:nan acc:nan\n",
      "Training Epoch: 460/1000 loss:0.2287 acc:0.7516\n",
      "Validation Epoch: 460/1000 loss:nan acc:nan\n",
      "Training Epoch: 461/1000 loss:0.2285 acc:0.7516\n",
      "Validation Epoch: 461/1000 loss:nan acc:nan\n",
      "Training Epoch: 462/1000 loss:0.2282 acc:0.7516\n",
      "Validation Epoch: 462/1000 loss:nan acc:nan\n",
      "Training Epoch: 463/1000 loss:0.2279 acc:0.7516\n",
      "Validation Epoch: 463/1000 loss:nan acc:nan\n",
      "Training Epoch: 464/1000 loss:0.2276 acc:0.7513\n",
      "Validation Epoch: 464/1000 loss:nan acc:nan\n",
      "Training Epoch: 465/1000 loss:0.2270 acc:0.7513\n",
      "Validation Epoch: 465/1000 loss:nan acc:nan\n",
      "Training Epoch: 466/1000 loss:0.2263 acc:0.7516\n",
      "Validation Epoch: 466/1000 loss:nan acc:nan\n",
      "Training Epoch: 467/1000 loss:0.2261 acc:0.7516\n",
      "Validation Epoch: 467/1000 loss:nan acc:nan\n",
      "Training Epoch: 468/1000 loss:0.2258 acc:0.7516\n",
      "Validation Epoch: 468/1000 loss:nan acc:nan\n",
      "Training Epoch: 469/1000 loss:0.2259 acc:0.7513\n",
      "Validation Epoch: 469/1000 loss:nan acc:nan\n",
      "Training Epoch: 470/1000 loss:0.2254 acc:0.7520\n",
      "Validation Epoch: 470/1000 loss:nan acc:nan\n",
      "Training Epoch: 471/1000 loss:0.2252 acc:0.7523\n",
      "Validation Epoch: 471/1000 loss:nan acc:nan\n",
      "Training Epoch: 472/1000 loss:0.2249 acc:0.7523\n",
      "Validation Epoch: 472/1000 loss:nan acc:nan\n",
      "Training Epoch: 473/1000 loss:0.2247 acc:0.7520\n",
      "Validation Epoch: 473/1000 loss:nan acc:nan\n",
      "Training Epoch: 474/1000 loss:0.2245 acc:0.7516\n",
      "Validation Epoch: 474/1000 loss:nan acc:nan\n",
      "Training Epoch: 475/1000 loss:0.2242 acc:0.7516\n",
      "Validation Epoch: 475/1000 loss:nan acc:nan\n",
      "Training Epoch: 476/1000 loss:0.2240 acc:0.7516\n",
      "Validation Epoch: 476/1000 loss:nan acc:nan\n",
      "Training Epoch: 477/1000 loss:0.2238 acc:0.7516\n",
      "Validation Epoch: 477/1000 loss:nan acc:nan\n",
      "Training Epoch: 478/1000 loss:0.2236 acc:0.7520\n",
      "Validation Epoch: 478/1000 loss:nan acc:nan\n",
      "Training Epoch: 479/1000 loss:0.2234 acc:0.7520\n",
      "Validation Epoch: 479/1000 loss:nan acc:nan\n",
      "Training Epoch: 480/1000 loss:0.2232 acc:0.7523\n",
      "Validation Epoch: 480/1000 loss:nan acc:nan\n",
      "Training Epoch: 481/1000 loss:0.2230 acc:0.7523\n",
      "Validation Epoch: 481/1000 loss:nan acc:nan\n",
      "Training Epoch: 482/1000 loss:0.2227 acc:0.7520\n",
      "Validation Epoch: 482/1000 loss:nan acc:nan\n",
      "Training Epoch: 483/1000 loss:0.2225 acc:0.7520\n",
      "Validation Epoch: 483/1000 loss:nan acc:nan\n",
      "Training Epoch: 484/1000 loss:0.2223 acc:0.7520\n",
      "Validation Epoch: 484/1000 loss:nan acc:nan\n",
      "Training Epoch: 485/1000 loss:0.2220 acc:0.7520\n",
      "Validation Epoch: 485/1000 loss:nan acc:nan\n",
      "Training Epoch: 486/1000 loss:0.2218 acc:0.7520\n",
      "Validation Epoch: 486/1000 loss:nan acc:nan\n",
      "Training Epoch: 487/1000 loss:0.2215 acc:0.7520\n",
      "Validation Epoch: 487/1000 loss:nan acc:nan\n",
      "Training Epoch: 488/1000 loss:0.2211 acc:0.7523\n",
      "Validation Epoch: 488/1000 loss:nan acc:nan\n",
      "Training Epoch: 489/1000 loss:0.2190 acc:0.7533\n",
      "Validation Epoch: 489/1000 loss:nan acc:nan\n",
      "Training Epoch: 490/1000 loss:0.2190 acc:0.7530\n",
      "Validation Epoch: 490/1000 loss:nan acc:nan\n",
      "Training Epoch: 491/1000 loss:0.2188 acc:0.7530\n",
      "Validation Epoch: 491/1000 loss:nan acc:nan\n",
      "Training Epoch: 492/1000 loss:0.2185 acc:0.7526\n",
      "Validation Epoch: 492/1000 loss:nan acc:nan\n",
      "Training Epoch: 493/1000 loss:0.2182 acc:0.7523\n",
      "Validation Epoch: 493/1000 loss:nan acc:nan\n",
      "Training Epoch: 494/1000 loss:0.2178 acc:0.7536\n",
      "Validation Epoch: 494/1000 loss:nan acc:nan\n",
      "Training Epoch: 495/1000 loss:0.2175 acc:0.7536\n",
      "Validation Epoch: 495/1000 loss:nan acc:nan\n",
      "Training Epoch: 496/1000 loss:0.2172 acc:0.7536\n",
      "Validation Epoch: 496/1000 loss:nan acc:nan\n",
      "Training Epoch: 497/1000 loss:0.2170 acc:0.7536\n",
      "Validation Epoch: 497/1000 loss:nan acc:nan\n",
      "Training Epoch: 498/1000 loss:0.2168 acc:0.7533\n",
      "Validation Epoch: 498/1000 loss:nan acc:nan\n",
      "Training Epoch: 499/1000 loss:0.2166 acc:0.7533\n",
      "Validation Epoch: 499/1000 loss:nan acc:nan\n",
      "Training Epoch: 500/1000 loss:0.2163 acc:0.7533\n",
      "Validation Epoch: 500/1000 loss:nan acc:nan\n",
      "Training Epoch: 501/1000 loss:0.2161 acc:0.7536\n",
      "Validation Epoch: 501/1000 loss:nan acc:nan\n",
      "Training Epoch: 502/1000 loss:0.2159 acc:0.7536\n",
      "Validation Epoch: 502/1000 loss:nan acc:nan\n",
      "Training Epoch: 503/1000 loss:0.2157 acc:0.7536\n",
      "Validation Epoch: 503/1000 loss:nan acc:nan\n",
      "Training Epoch: 504/1000 loss:0.2166 acc:0.7533\n",
      "Validation Epoch: 504/1000 loss:nan acc:nan\n",
      "Training Epoch: 505/1000 loss:0.2194 acc:0.7509\n",
      "Validation Epoch: 505/1000 loss:nan acc:nan\n",
      "Training Epoch: 506/1000 loss:0.2333 acc:0.7438\n",
      "Validation Epoch: 506/1000 loss:nan acc:nan\n",
      "Training Epoch: 507/1000 loss:0.2509 acc:0.7340\n",
      "Validation Epoch: 507/1000 loss:nan acc:nan\n",
      "Training Epoch: 508/1000 loss:0.2696 acc:0.7258\n",
      "Validation Epoch: 508/1000 loss:nan acc:nan\n",
      "Training Epoch: 509/1000 loss:0.2871 acc:0.7201\n",
      "Validation Epoch: 509/1000 loss:nan acc:nan\n",
      "Training Epoch: 510/1000 loss:0.2906 acc:0.7201\n",
      "Validation Epoch: 510/1000 loss:nan acc:nan\n",
      "Training Epoch: 511/1000 loss:0.2942 acc:0.7194\n",
      "Validation Epoch: 511/1000 loss:nan acc:nan\n",
      "Training Epoch: 512/1000 loss:0.2948 acc:0.7173\n",
      "Validation Epoch: 512/1000 loss:nan acc:nan\n",
      "Training Epoch: 513/1000 loss:0.2941 acc:0.7173\n",
      "Validation Epoch: 513/1000 loss:nan acc:nan\n",
      "Training Epoch: 514/1000 loss:0.2885 acc:0.7245\n",
      "Validation Epoch: 514/1000 loss:nan acc:nan\n",
      "Training Epoch: 515/1000 loss:0.2808 acc:0.7282\n",
      "Validation Epoch: 515/1000 loss:nan acc:nan\n",
      "Training Epoch: 516/1000 loss:0.2770 acc:0.7302\n",
      "Validation Epoch: 516/1000 loss:nan acc:nan\n",
      "Training Epoch: 517/1000 loss:0.2738 acc:0.7299\n",
      "Validation Epoch: 517/1000 loss:nan acc:nan\n",
      "Training Epoch: 518/1000 loss:0.2698 acc:0.7313\n",
      "Validation Epoch: 518/1000 loss:nan acc:nan\n",
      "Training Epoch: 519/1000 loss:0.2664 acc:0.7374\n",
      "Validation Epoch: 519/1000 loss:nan acc:nan\n",
      "Training Epoch: 520/1000 loss:0.2639 acc:0.7363\n",
      "Validation Epoch: 520/1000 loss:nan acc:nan\n",
      "Training Epoch: 521/1000 loss:0.2581 acc:0.7370\n",
      "Validation Epoch: 521/1000 loss:nan acc:nan\n",
      "Training Epoch: 522/1000 loss:0.2542 acc:0.7394\n",
      "Validation Epoch: 522/1000 loss:nan acc:nan\n",
      "Training Epoch: 523/1000 loss:0.2522 acc:0.7397\n",
      "Validation Epoch: 523/1000 loss:nan acc:nan\n",
      "Training Epoch: 524/1000 loss:0.2486 acc:0.7421\n",
      "Validation Epoch: 524/1000 loss:nan acc:nan\n",
      "Training Epoch: 525/1000 loss:0.2445 acc:0.7438\n",
      "Validation Epoch: 525/1000 loss:nan acc:nan\n",
      "Training Epoch: 526/1000 loss:0.2422 acc:0.7458\n",
      "Validation Epoch: 526/1000 loss:nan acc:nan\n",
      "Training Epoch: 527/1000 loss:0.2401 acc:0.7479\n",
      "Validation Epoch: 527/1000 loss:nan acc:nan\n",
      "Training Epoch: 528/1000 loss:0.2381 acc:0.7479\n",
      "Validation Epoch: 528/1000 loss:nan acc:nan\n",
      "Training Epoch: 529/1000 loss:0.2352 acc:0.7492\n",
      "Validation Epoch: 529/1000 loss:nan acc:nan\n",
      "Training Epoch: 530/1000 loss:0.2337 acc:0.7503\n",
      "Validation Epoch: 530/1000 loss:nan acc:nan\n",
      "Training Epoch: 531/1000 loss:0.2325 acc:0.7503\n",
      "Validation Epoch: 531/1000 loss:nan acc:nan\n",
      "Training Epoch: 532/1000 loss:0.2347 acc:0.7489\n",
      "Validation Epoch: 532/1000 loss:nan acc:nan\n",
      "Training Epoch: 533/1000 loss:0.2349 acc:0.7479\n",
      "Validation Epoch: 533/1000 loss:nan acc:nan\n",
      "Training Epoch: 534/1000 loss:0.2346 acc:0.7482\n",
      "Validation Epoch: 534/1000 loss:nan acc:nan\n",
      "Training Epoch: 535/1000 loss:0.2340 acc:0.7486\n",
      "Validation Epoch: 535/1000 loss:nan acc:nan\n",
      "Training Epoch: 536/1000 loss:0.2328 acc:0.7489\n",
      "Validation Epoch: 536/1000 loss:nan acc:nan\n",
      "Training Epoch: 537/1000 loss:0.2306 acc:0.7496\n",
      "Validation Epoch: 537/1000 loss:nan acc:nan\n",
      "Training Epoch: 538/1000 loss:0.2295 acc:0.7506\n",
      "Validation Epoch: 538/1000 loss:nan acc:nan\n",
      "Training Epoch: 539/1000 loss:0.2289 acc:0.7520\n",
      "Validation Epoch: 539/1000 loss:nan acc:nan\n",
      "Training Epoch: 540/1000 loss:0.2287 acc:0.7513\n",
      "Validation Epoch: 540/1000 loss:nan acc:nan\n",
      "Training Epoch: 541/1000 loss:0.2282 acc:0.7509\n",
      "Validation Epoch: 541/1000 loss:nan acc:nan\n",
      "Training Epoch: 542/1000 loss:0.2260 acc:0.7513\n",
      "Validation Epoch: 542/1000 loss:nan acc:nan\n",
      "Training Epoch: 543/1000 loss:0.2247 acc:0.7516\n",
      "Validation Epoch: 543/1000 loss:nan acc:nan\n",
      "Training Epoch: 544/1000 loss:0.2241 acc:0.7523\n",
      "Validation Epoch: 544/1000 loss:nan acc:nan\n",
      "Training Epoch: 545/1000 loss:0.2238 acc:0.7530\n",
      "Validation Epoch: 545/1000 loss:nan acc:nan\n",
      "Training Epoch: 546/1000 loss:0.2234 acc:0.7536\n",
      "Validation Epoch: 546/1000 loss:nan acc:nan\n",
      "Training Epoch: 547/1000 loss:0.2227 acc:0.7533\n",
      "Validation Epoch: 547/1000 loss:nan acc:nan\n",
      "Training Epoch: 548/1000 loss:0.2217 acc:0.7540\n",
      "Validation Epoch: 548/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 549/1000 loss:0.2208 acc:0.7540\n",
      "Validation Epoch: 549/1000 loss:nan acc:nan\n",
      "Training Epoch: 550/1000 loss:0.2204 acc:0.7540\n",
      "Validation Epoch: 550/1000 loss:nan acc:nan\n",
      "Training Epoch: 551/1000 loss:0.2198 acc:0.7543\n",
      "Validation Epoch: 551/1000 loss:nan acc:nan\n",
      "Training Epoch: 552/1000 loss:0.2196 acc:0.7543\n",
      "Validation Epoch: 552/1000 loss:nan acc:nan\n",
      "Training Epoch: 553/1000 loss:0.2193 acc:0.7553\n",
      "Validation Epoch: 553/1000 loss:nan acc:nan\n",
      "Training Epoch: 554/1000 loss:0.2188 acc:0.7550\n",
      "Validation Epoch: 554/1000 loss:nan acc:nan\n",
      "Training Epoch: 555/1000 loss:0.2181 acc:0.7553\n",
      "Validation Epoch: 555/1000 loss:nan acc:nan\n",
      "Training Epoch: 556/1000 loss:0.2180 acc:0.7533\n",
      "Validation Epoch: 556/1000 loss:nan acc:nan\n",
      "Training Epoch: 557/1000 loss:0.2179 acc:0.7536\n",
      "Validation Epoch: 557/1000 loss:nan acc:nan\n",
      "Training Epoch: 558/1000 loss:0.2178 acc:0.7533\n",
      "Validation Epoch: 558/1000 loss:nan acc:nan\n",
      "Training Epoch: 559/1000 loss:0.2174 acc:0.7533\n",
      "Validation Epoch: 559/1000 loss:nan acc:nan\n",
      "Training Epoch: 560/1000 loss:0.2170 acc:0.7540\n",
      "Validation Epoch: 560/1000 loss:nan acc:nan\n",
      "Training Epoch: 561/1000 loss:0.2164 acc:0.7540\n",
      "Validation Epoch: 561/1000 loss:nan acc:nan\n",
      "Training Epoch: 562/1000 loss:0.2158 acc:0.7536\n",
      "Validation Epoch: 562/1000 loss:nan acc:nan\n",
      "Training Epoch: 563/1000 loss:0.2153 acc:0.7533\n",
      "Validation Epoch: 563/1000 loss:nan acc:nan\n",
      "Training Epoch: 564/1000 loss:0.2147 acc:0.7530\n",
      "Validation Epoch: 564/1000 loss:nan acc:nan\n",
      "Training Epoch: 565/1000 loss:0.2143 acc:0.7530\n",
      "Validation Epoch: 565/1000 loss:nan acc:nan\n",
      "Training Epoch: 566/1000 loss:0.2140 acc:0.7530\n",
      "Validation Epoch: 566/1000 loss:nan acc:nan\n",
      "Training Epoch: 567/1000 loss:0.2137 acc:0.7530\n",
      "Validation Epoch: 567/1000 loss:nan acc:nan\n",
      "Training Epoch: 568/1000 loss:0.2134 acc:0.7533\n",
      "Validation Epoch: 568/1000 loss:nan acc:nan\n",
      "Training Epoch: 569/1000 loss:0.2132 acc:0.7533\n",
      "Validation Epoch: 569/1000 loss:nan acc:nan\n",
      "Training Epoch: 570/1000 loss:0.2130 acc:0.7533\n",
      "Validation Epoch: 570/1000 loss:nan acc:nan\n",
      "Training Epoch: 571/1000 loss:0.2127 acc:0.7530\n",
      "Validation Epoch: 571/1000 loss:nan acc:nan\n",
      "Training Epoch: 572/1000 loss:0.2124 acc:0.7533\n",
      "Validation Epoch: 572/1000 loss:nan acc:nan\n",
      "Training Epoch: 573/1000 loss:0.2120 acc:0.7540\n",
      "Validation Epoch: 573/1000 loss:nan acc:nan\n",
      "Training Epoch: 574/1000 loss:0.2117 acc:0.7543\n",
      "Validation Epoch: 574/1000 loss:nan acc:nan\n",
      "Training Epoch: 575/1000 loss:0.2114 acc:0.7547\n",
      "Validation Epoch: 575/1000 loss:nan acc:nan\n",
      "Training Epoch: 576/1000 loss:0.2112 acc:0.7547\n",
      "Validation Epoch: 576/1000 loss:nan acc:nan\n",
      "Training Epoch: 577/1000 loss:0.2109 acc:0.7553\n",
      "Validation Epoch: 577/1000 loss:nan acc:nan\n",
      "Training Epoch: 578/1000 loss:0.2107 acc:0.7550\n",
      "Validation Epoch: 578/1000 loss:nan acc:nan\n",
      "Training Epoch: 579/1000 loss:0.2105 acc:0.7547\n",
      "Validation Epoch: 579/1000 loss:nan acc:nan\n",
      "Training Epoch: 580/1000 loss:0.2102 acc:0.7557\n",
      "Validation Epoch: 580/1000 loss:nan acc:nan\n",
      "Training Epoch: 581/1000 loss:0.2100 acc:0.7557\n",
      "Validation Epoch: 581/1000 loss:nan acc:nan\n",
      "Training Epoch: 582/1000 loss:0.2097 acc:0.7553\n",
      "Validation Epoch: 582/1000 loss:nan acc:nan\n",
      "Training Epoch: 583/1000 loss:0.2095 acc:0.7557\n",
      "Validation Epoch: 583/1000 loss:nan acc:nan\n",
      "Training Epoch: 584/1000 loss:0.2093 acc:0.7560\n",
      "Validation Epoch: 584/1000 loss:nan acc:nan\n",
      "Training Epoch: 585/1000 loss:0.2090 acc:0.7560\n",
      "Validation Epoch: 585/1000 loss:nan acc:nan\n",
      "Training Epoch: 586/1000 loss:0.2088 acc:0.7560\n",
      "Validation Epoch: 586/1000 loss:nan acc:nan\n",
      "Training Epoch: 587/1000 loss:0.2085 acc:0.7557\n",
      "Validation Epoch: 587/1000 loss:nan acc:nan\n",
      "Training Epoch: 588/1000 loss:0.2083 acc:0.7553\n",
      "Validation Epoch: 588/1000 loss:nan acc:nan\n",
      "Training Epoch: 589/1000 loss:0.2081 acc:0.7547\n",
      "Validation Epoch: 589/1000 loss:nan acc:nan\n",
      "Training Epoch: 590/1000 loss:0.2078 acc:0.7550\n",
      "Validation Epoch: 590/1000 loss:nan acc:nan\n",
      "Training Epoch: 591/1000 loss:0.2076 acc:0.7550\n",
      "Validation Epoch: 591/1000 loss:nan acc:nan\n",
      "Training Epoch: 592/1000 loss:0.2074 acc:0.7547\n",
      "Validation Epoch: 592/1000 loss:nan acc:nan\n",
      "Training Epoch: 593/1000 loss:0.2072 acc:0.7543\n",
      "Validation Epoch: 593/1000 loss:nan acc:nan\n",
      "Training Epoch: 594/1000 loss:0.2070 acc:0.7543\n",
      "Validation Epoch: 594/1000 loss:nan acc:nan\n",
      "Training Epoch: 595/1000 loss:0.2068 acc:0.7547\n",
      "Validation Epoch: 595/1000 loss:nan acc:nan\n",
      "Training Epoch: 596/1000 loss:0.2066 acc:0.7547\n",
      "Validation Epoch: 596/1000 loss:nan acc:nan\n",
      "Training Epoch: 597/1000 loss:0.2064 acc:0.7547\n",
      "Validation Epoch: 597/1000 loss:nan acc:nan\n",
      "Training Epoch: 598/1000 loss:0.2062 acc:0.7547\n",
      "Validation Epoch: 598/1000 loss:nan acc:nan\n",
      "Training Epoch: 599/1000 loss:0.2060 acc:0.7547\n",
      "Validation Epoch: 599/1000 loss:nan acc:nan\n",
      "Training Epoch: 600/1000 loss:0.2058 acc:0.7547\n",
      "Validation Epoch: 600/1000 loss:nan acc:nan\n",
      "Training Epoch: 601/1000 loss:0.2057 acc:0.7547\n",
      "Validation Epoch: 601/1000 loss:nan acc:nan\n",
      "Training Epoch: 602/1000 loss:0.2055 acc:0.7547\n",
      "Validation Epoch: 602/1000 loss:nan acc:nan\n",
      "Training Epoch: 603/1000 loss:0.2053 acc:0.7547\n",
      "Validation Epoch: 603/1000 loss:nan acc:nan\n",
      "Training Epoch: 604/1000 loss:0.2051 acc:0.7543\n",
      "Validation Epoch: 604/1000 loss:nan acc:nan\n",
      "Training Epoch: 605/1000 loss:0.2049 acc:0.7543\n",
      "Validation Epoch: 605/1000 loss:nan acc:nan\n",
      "Training Epoch: 606/1000 loss:0.2047 acc:0.7543\n",
      "Validation Epoch: 606/1000 loss:nan acc:nan\n",
      "Training Epoch: 607/1000 loss:0.2046 acc:0.7543\n",
      "Validation Epoch: 607/1000 loss:nan acc:nan\n",
      "Training Epoch: 608/1000 loss:0.2044 acc:0.7543\n",
      "Validation Epoch: 608/1000 loss:nan acc:nan\n",
      "Training Epoch: 609/1000 loss:0.2042 acc:0.7543\n",
      "Validation Epoch: 609/1000 loss:nan acc:nan\n",
      "Training Epoch: 610/1000 loss:0.2040 acc:0.7543\n",
      "Validation Epoch: 610/1000 loss:nan acc:nan\n",
      "Training Epoch: 611/1000 loss:0.2038 acc:0.7543\n",
      "Validation Epoch: 611/1000 loss:nan acc:nan\n",
      "Training Epoch: 612/1000 loss:0.2037 acc:0.7547\n",
      "Validation Epoch: 612/1000 loss:nan acc:nan\n",
      "Training Epoch: 613/1000 loss:0.2035 acc:0.7547\n",
      "Validation Epoch: 613/1000 loss:nan acc:nan\n",
      "Training Epoch: 614/1000 loss:0.2033 acc:0.7547\n",
      "Validation Epoch: 614/1000 loss:nan acc:nan\n",
      "Training Epoch: 615/1000 loss:0.2032 acc:0.7547\n",
      "Validation Epoch: 615/1000 loss:nan acc:nan\n",
      "Training Epoch: 616/1000 loss:0.2030 acc:0.7547\n",
      "Validation Epoch: 616/1000 loss:nan acc:nan\n",
      "Training Epoch: 617/1000 loss:0.2028 acc:0.7547\n",
      "Validation Epoch: 617/1000 loss:nan acc:nan\n",
      "Training Epoch: 618/1000 loss:0.2030 acc:0.7543\n",
      "Validation Epoch: 618/1000 loss:nan acc:nan\n",
      "Training Epoch: 619/1000 loss:0.2031 acc:0.7547\n",
      "Validation Epoch: 619/1000 loss:nan acc:nan\n",
      "Training Epoch: 620/1000 loss:0.2021 acc:0.7540\n",
      "Validation Epoch: 620/1000 loss:nan acc:nan\n",
      "Training Epoch: 621/1000 loss:0.2019 acc:0.7543\n",
      "Validation Epoch: 621/1000 loss:nan acc:nan\n",
      "Training Epoch: 622/1000 loss:0.2016 acc:0.7550\n",
      "Validation Epoch: 622/1000 loss:nan acc:nan\n",
      "Training Epoch: 623/1000 loss:0.2015 acc:0.7553\n",
      "Validation Epoch: 623/1000 loss:nan acc:nan\n",
      "Training Epoch: 624/1000 loss:0.2013 acc:0.7553\n",
      "Validation Epoch: 624/1000 loss:nan acc:nan\n",
      "Training Epoch: 625/1000 loss:0.2011 acc:0.7553\n",
      "Validation Epoch: 625/1000 loss:nan acc:nan\n",
      "Training Epoch: 626/1000 loss:0.2011 acc:0.7553\n",
      "Validation Epoch: 626/1000 loss:nan acc:nan\n",
      "Training Epoch: 627/1000 loss:0.2012 acc:0.7550\n",
      "Validation Epoch: 627/1000 loss:nan acc:nan\n",
      "Training Epoch: 628/1000 loss:0.2007 acc:0.7560\n",
      "Validation Epoch: 628/1000 loss:nan acc:nan\n",
      "Training Epoch: 629/1000 loss:0.2004 acc:0.7564\n",
      "Validation Epoch: 629/1000 loss:nan acc:nan\n",
      "Training Epoch: 630/1000 loss:0.2004 acc:0.7564\n",
      "Validation Epoch: 630/1000 loss:nan acc:nan\n",
      "Training Epoch: 631/1000 loss:0.2001 acc:0.7564\n",
      "Validation Epoch: 631/1000 loss:nan acc:nan\n",
      "Training Epoch: 632/1000 loss:0.2000 acc:0.7564\n",
      "Validation Epoch: 632/1000 loss:nan acc:nan\n",
      "Training Epoch: 633/1000 loss:0.1998 acc:0.7564\n",
      "Validation Epoch: 633/1000 loss:nan acc:nan\n",
      "Training Epoch: 634/1000 loss:0.1998 acc:0.7560\n",
      "Validation Epoch: 634/1000 loss:nan acc:nan\n",
      "Training Epoch: 635/1000 loss:0.1997 acc:0.7557\n",
      "Validation Epoch: 635/1000 loss:nan acc:nan\n",
      "Training Epoch: 636/1000 loss:0.1997 acc:0.7560\n",
      "Validation Epoch: 636/1000 loss:nan acc:nan\n",
      "Training Epoch: 637/1000 loss:0.1995 acc:0.7557\n",
      "Validation Epoch: 637/1000 loss:nan acc:nan\n",
      "Training Epoch: 638/1000 loss:0.1991 acc:0.7560\n",
      "Validation Epoch: 638/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 639/1000 loss:0.2006 acc:0.7553\n",
      "Validation Epoch: 639/1000 loss:nan acc:nan\n",
      "Training Epoch: 640/1000 loss:0.2008 acc:0.7547\n",
      "Validation Epoch: 640/1000 loss:nan acc:nan\n",
      "Training Epoch: 641/1000 loss:0.2012 acc:0.7547\n",
      "Validation Epoch: 641/1000 loss:nan acc:nan\n",
      "Training Epoch: 642/1000 loss:0.2030 acc:0.7536\n",
      "Validation Epoch: 642/1000 loss:nan acc:nan\n",
      "Training Epoch: 643/1000 loss:0.2028 acc:0.7536\n",
      "Validation Epoch: 643/1000 loss:nan acc:nan\n",
      "Training Epoch: 644/1000 loss:0.2028 acc:0.7533\n",
      "Validation Epoch: 644/1000 loss:nan acc:nan\n",
      "Training Epoch: 645/1000 loss:0.2026 acc:0.7543\n",
      "Validation Epoch: 645/1000 loss:nan acc:nan\n",
      "Training Epoch: 646/1000 loss:0.2024 acc:0.7547\n",
      "Validation Epoch: 646/1000 loss:nan acc:nan\n",
      "Training Epoch: 647/1000 loss:0.2021 acc:0.7550\n",
      "Validation Epoch: 647/1000 loss:nan acc:nan\n",
      "Training Epoch: 648/1000 loss:0.2017 acc:0.7557\n",
      "Validation Epoch: 648/1000 loss:nan acc:nan\n",
      "Training Epoch: 649/1000 loss:0.2013 acc:0.7553\n",
      "Validation Epoch: 649/1000 loss:nan acc:nan\n",
      "Training Epoch: 650/1000 loss:0.2011 acc:0.7557\n",
      "Validation Epoch: 650/1000 loss:nan acc:nan\n",
      "Training Epoch: 651/1000 loss:0.2008 acc:0.7560\n",
      "Validation Epoch: 651/1000 loss:nan acc:nan\n",
      "Training Epoch: 652/1000 loss:0.2005 acc:0.7560\n",
      "Validation Epoch: 652/1000 loss:nan acc:nan\n",
      "Training Epoch: 653/1000 loss:0.2002 acc:0.7564\n",
      "Validation Epoch: 653/1000 loss:nan acc:nan\n",
      "Training Epoch: 654/1000 loss:0.1999 acc:0.7560\n",
      "Validation Epoch: 654/1000 loss:nan acc:nan\n",
      "Training Epoch: 655/1000 loss:0.1995 acc:0.7567\n",
      "Validation Epoch: 655/1000 loss:nan acc:nan\n",
      "Training Epoch: 656/1000 loss:0.1991 acc:0.7567\n",
      "Validation Epoch: 656/1000 loss:nan acc:nan\n",
      "Training Epoch: 657/1000 loss:0.1988 acc:0.7567\n",
      "Validation Epoch: 657/1000 loss:nan acc:nan\n",
      "Training Epoch: 658/1000 loss:0.1987 acc:0.7560\n",
      "Validation Epoch: 658/1000 loss:nan acc:nan\n",
      "Training Epoch: 659/1000 loss:0.1985 acc:0.7564\n",
      "Validation Epoch: 659/1000 loss:nan acc:nan\n",
      "Training Epoch: 660/1000 loss:0.1983 acc:0.7564\n",
      "Validation Epoch: 660/1000 loss:nan acc:nan\n",
      "Training Epoch: 661/1000 loss:0.1985 acc:0.7564\n",
      "Validation Epoch: 661/1000 loss:nan acc:nan\n",
      "Training Epoch: 662/1000 loss:0.1988 acc:0.7567\n",
      "Validation Epoch: 662/1000 loss:nan acc:nan\n",
      "Training Epoch: 663/1000 loss:0.1987 acc:0.7567\n",
      "Validation Epoch: 663/1000 loss:nan acc:nan\n",
      "Training Epoch: 664/1000 loss:0.1985 acc:0.7567\n",
      "Validation Epoch: 664/1000 loss:nan acc:nan\n",
      "Training Epoch: 665/1000 loss:0.1985 acc:0.7564\n",
      "Validation Epoch: 665/1000 loss:nan acc:nan\n",
      "Training Epoch: 666/1000 loss:0.1984 acc:0.7560\n",
      "Validation Epoch: 666/1000 loss:nan acc:nan\n",
      "Training Epoch: 667/1000 loss:0.1984 acc:0.7560\n",
      "Validation Epoch: 667/1000 loss:nan acc:nan\n",
      "Training Epoch: 668/1000 loss:0.1983 acc:0.7564\n",
      "Validation Epoch: 668/1000 loss:nan acc:nan\n",
      "Training Epoch: 669/1000 loss:0.1982 acc:0.7564\n",
      "Validation Epoch: 669/1000 loss:nan acc:nan\n",
      "Training Epoch: 670/1000 loss:0.1980 acc:0.7564\n",
      "Validation Epoch: 670/1000 loss:nan acc:nan\n",
      "Training Epoch: 671/1000 loss:0.1978 acc:0.7564\n",
      "Validation Epoch: 671/1000 loss:nan acc:nan\n",
      "Training Epoch: 672/1000 loss:0.1977 acc:0.7560\n",
      "Validation Epoch: 672/1000 loss:nan acc:nan\n",
      "Training Epoch: 673/1000 loss:0.1976 acc:0.7564\n",
      "Validation Epoch: 673/1000 loss:nan acc:nan\n",
      "Training Epoch: 674/1000 loss:0.1974 acc:0.7564\n",
      "Validation Epoch: 674/1000 loss:nan acc:nan\n",
      "Training Epoch: 675/1000 loss:0.1973 acc:0.7560\n",
      "Validation Epoch: 675/1000 loss:nan acc:nan\n",
      "Training Epoch: 676/1000 loss:0.1971 acc:0.7560\n",
      "Validation Epoch: 676/1000 loss:nan acc:nan\n",
      "Training Epoch: 677/1000 loss:0.1970 acc:0.7560\n",
      "Validation Epoch: 677/1000 loss:nan acc:nan\n",
      "Training Epoch: 678/1000 loss:0.1968 acc:0.7564\n",
      "Validation Epoch: 678/1000 loss:nan acc:nan\n",
      "Training Epoch: 679/1000 loss:0.1967 acc:0.7574\n",
      "Validation Epoch: 679/1000 loss:nan acc:nan\n",
      "Training Epoch: 680/1000 loss:0.1965 acc:0.7574\n",
      "Validation Epoch: 680/1000 loss:nan acc:nan\n",
      "Training Epoch: 681/1000 loss:0.1963 acc:0.7574\n",
      "Validation Epoch: 681/1000 loss:nan acc:nan\n",
      "Training Epoch: 682/1000 loss:0.1961 acc:0.7574\n",
      "Validation Epoch: 682/1000 loss:nan acc:nan\n",
      "Training Epoch: 683/1000 loss:0.1959 acc:0.7567\n",
      "Validation Epoch: 683/1000 loss:nan acc:nan\n",
      "Training Epoch: 684/1000 loss:0.1954 acc:0.7574\n",
      "Validation Epoch: 684/1000 loss:nan acc:nan\n",
      "Training Epoch: 685/1000 loss:0.1952 acc:0.7570\n",
      "Validation Epoch: 685/1000 loss:nan acc:nan\n",
      "Training Epoch: 686/1000 loss:0.1954 acc:0.7567\n",
      "Validation Epoch: 686/1000 loss:nan acc:nan\n",
      "Training Epoch: 687/1000 loss:0.1950 acc:0.7567\n",
      "Validation Epoch: 687/1000 loss:nan acc:nan\n",
      "Training Epoch: 688/1000 loss:0.1947 acc:0.7567\n",
      "Validation Epoch: 688/1000 loss:nan acc:nan\n",
      "Training Epoch: 689/1000 loss:0.1937 acc:0.7574\n",
      "Validation Epoch: 689/1000 loss:nan acc:nan\n",
      "Training Epoch: 690/1000 loss:0.1935 acc:0.7574\n",
      "Validation Epoch: 690/1000 loss:nan acc:nan\n",
      "Training Epoch: 691/1000 loss:0.1933 acc:0.7577\n",
      "Validation Epoch: 691/1000 loss:nan acc:nan\n",
      "Training Epoch: 692/1000 loss:0.1932 acc:0.7577\n",
      "Validation Epoch: 692/1000 loss:nan acc:nan\n",
      "Training Epoch: 693/1000 loss:0.1930 acc:0.7577\n",
      "Validation Epoch: 693/1000 loss:nan acc:nan\n",
      "Training Epoch: 694/1000 loss:0.1927 acc:0.7574\n",
      "Validation Epoch: 694/1000 loss:nan acc:nan\n",
      "Training Epoch: 695/1000 loss:0.1925 acc:0.7574\n",
      "Validation Epoch: 695/1000 loss:nan acc:nan\n",
      "Training Epoch: 696/1000 loss:0.1923 acc:0.7574\n",
      "Validation Epoch: 696/1000 loss:nan acc:nan\n",
      "Training Epoch: 697/1000 loss:0.1921 acc:0.7574\n",
      "Validation Epoch: 697/1000 loss:nan acc:nan\n",
      "Training Epoch: 698/1000 loss:0.1919 acc:0.7577\n",
      "Validation Epoch: 698/1000 loss:nan acc:nan\n",
      "Training Epoch: 699/1000 loss:0.1917 acc:0.7577\n",
      "Validation Epoch: 699/1000 loss:nan acc:nan\n",
      "Training Epoch: 700/1000 loss:0.1914 acc:0.7577\n",
      "Validation Epoch: 700/1000 loss:nan acc:nan\n",
      "Training Epoch: 701/1000 loss:0.1908 acc:0.7577\n",
      "Validation Epoch: 701/1000 loss:nan acc:nan\n",
      "Training Epoch: 702/1000 loss:0.1904 acc:0.7584\n",
      "Validation Epoch: 702/1000 loss:nan acc:nan\n",
      "Training Epoch: 703/1000 loss:0.1900 acc:0.7587\n",
      "Validation Epoch: 703/1000 loss:nan acc:nan\n",
      "Training Epoch: 704/1000 loss:0.1898 acc:0.7587\n",
      "Validation Epoch: 704/1000 loss:nan acc:nan\n",
      "Training Epoch: 705/1000 loss:0.1894 acc:0.7591\n",
      "Validation Epoch: 705/1000 loss:nan acc:nan\n",
      "Training Epoch: 706/1000 loss:0.1892 acc:0.7594\n",
      "Validation Epoch: 706/1000 loss:nan acc:nan\n",
      "Training Epoch: 707/1000 loss:0.1890 acc:0.7594\n",
      "Validation Epoch: 707/1000 loss:nan acc:nan\n",
      "Training Epoch: 708/1000 loss:0.1888 acc:0.7591\n",
      "Validation Epoch: 708/1000 loss:nan acc:nan\n",
      "Training Epoch: 709/1000 loss:0.1887 acc:0.7591\n",
      "Validation Epoch: 709/1000 loss:nan acc:nan\n",
      "Training Epoch: 710/1000 loss:0.1885 acc:0.7587\n",
      "Validation Epoch: 710/1000 loss:nan acc:nan\n",
      "Training Epoch: 711/1000 loss:0.1884 acc:0.7587\n",
      "Validation Epoch: 711/1000 loss:nan acc:nan\n",
      "Training Epoch: 712/1000 loss:0.1882 acc:0.7584\n",
      "Validation Epoch: 712/1000 loss:nan acc:nan\n",
      "Training Epoch: 713/1000 loss:0.1879 acc:0.7587\n",
      "Validation Epoch: 713/1000 loss:nan acc:nan\n",
      "Training Epoch: 714/1000 loss:0.1877 acc:0.7587\n",
      "Validation Epoch: 714/1000 loss:nan acc:nan\n",
      "Training Epoch: 715/1000 loss:0.1878 acc:0.7591\n",
      "Validation Epoch: 715/1000 loss:nan acc:nan\n",
      "Training Epoch: 716/1000 loss:0.1875 acc:0.7591\n",
      "Validation Epoch: 716/1000 loss:nan acc:nan\n",
      "Training Epoch: 717/1000 loss:0.1874 acc:0.7591\n",
      "Validation Epoch: 717/1000 loss:nan acc:nan\n",
      "Training Epoch: 718/1000 loss:0.1874 acc:0.7591\n",
      "Validation Epoch: 718/1000 loss:nan acc:nan\n",
      "Training Epoch: 719/1000 loss:0.1873 acc:0.7591\n",
      "Validation Epoch: 719/1000 loss:nan acc:nan\n",
      "Training Epoch: 720/1000 loss:0.1871 acc:0.7591\n",
      "Validation Epoch: 720/1000 loss:nan acc:nan\n",
      "Training Epoch: 721/1000 loss:0.1869 acc:0.7584\n",
      "Validation Epoch: 721/1000 loss:nan acc:nan\n",
      "Training Epoch: 722/1000 loss:0.1868 acc:0.7584\n",
      "Validation Epoch: 722/1000 loss:nan acc:nan\n",
      "Training Epoch: 723/1000 loss:0.1865 acc:0.7584\n",
      "Validation Epoch: 723/1000 loss:nan acc:nan\n",
      "Training Epoch: 724/1000 loss:0.1864 acc:0.7584\n",
      "Validation Epoch: 724/1000 loss:nan acc:nan\n",
      "Training Epoch: 725/1000 loss:0.1862 acc:0.7587\n",
      "Validation Epoch: 725/1000 loss:nan acc:nan\n",
      "Training Epoch: 726/1000 loss:0.1861 acc:0.7584\n",
      "Validation Epoch: 726/1000 loss:nan acc:nan\n",
      "Training Epoch: 727/1000 loss:0.1870 acc:0.7581\n",
      "Validation Epoch: 727/1000 loss:nan acc:nan\n",
      "Training Epoch: 728/1000 loss:0.1860 acc:0.7581\n",
      "Validation Epoch: 728/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 729/1000 loss:0.1867 acc:0.7581\n",
      "Validation Epoch: 729/1000 loss:nan acc:nan\n",
      "Training Epoch: 730/1000 loss:0.1871 acc:0.7581\n",
      "Validation Epoch: 730/1000 loss:nan acc:nan\n",
      "Training Epoch: 731/1000 loss:0.1876 acc:0.7577\n",
      "Validation Epoch: 731/1000 loss:nan acc:nan\n",
      "Training Epoch: 732/1000 loss:0.1869 acc:0.7577\n",
      "Validation Epoch: 732/1000 loss:nan acc:nan\n",
      "Training Epoch: 733/1000 loss:0.1866 acc:0.7577\n",
      "Validation Epoch: 733/1000 loss:nan acc:nan\n",
      "Training Epoch: 734/1000 loss:0.1861 acc:0.7577\n",
      "Validation Epoch: 734/1000 loss:nan acc:nan\n",
      "Training Epoch: 735/1000 loss:0.1854 acc:0.7584\n",
      "Validation Epoch: 735/1000 loss:nan acc:nan\n",
      "Training Epoch: 736/1000 loss:0.1852 acc:0.7584\n",
      "Validation Epoch: 736/1000 loss:nan acc:nan\n",
      "Training Epoch: 737/1000 loss:0.1851 acc:0.7587\n",
      "Validation Epoch: 737/1000 loss:nan acc:nan\n",
      "Training Epoch: 738/1000 loss:0.1851 acc:0.7581\n",
      "Validation Epoch: 738/1000 loss:nan acc:nan\n",
      "Training Epoch: 739/1000 loss:0.1851 acc:0.7584\n",
      "Validation Epoch: 739/1000 loss:nan acc:nan\n",
      "Training Epoch: 740/1000 loss:0.1850 acc:0.7587\n",
      "Validation Epoch: 740/1000 loss:nan acc:nan\n",
      "Training Epoch: 741/1000 loss:0.1850 acc:0.7587\n",
      "Validation Epoch: 741/1000 loss:nan acc:nan\n",
      "Training Epoch: 742/1000 loss:0.1849 acc:0.7587\n",
      "Validation Epoch: 742/1000 loss:nan acc:nan\n",
      "Training Epoch: 743/1000 loss:0.1847 acc:0.7587\n",
      "Validation Epoch: 743/1000 loss:nan acc:nan\n",
      "Training Epoch: 744/1000 loss:0.1846 acc:0.7591\n",
      "Validation Epoch: 744/1000 loss:nan acc:nan\n",
      "Training Epoch: 745/1000 loss:0.1846 acc:0.7584\n",
      "Validation Epoch: 745/1000 loss:nan acc:nan\n",
      "Training Epoch: 746/1000 loss:0.1845 acc:0.7587\n",
      "Validation Epoch: 746/1000 loss:nan acc:nan\n",
      "Training Epoch: 747/1000 loss:0.1845 acc:0.7594\n",
      "Validation Epoch: 747/1000 loss:nan acc:nan\n",
      "Training Epoch: 748/1000 loss:0.1845 acc:0.7598\n",
      "Validation Epoch: 748/1000 loss:nan acc:nan\n",
      "Training Epoch: 749/1000 loss:0.1844 acc:0.7587\n",
      "Validation Epoch: 749/1000 loss:nan acc:nan\n",
      "Training Epoch: 750/1000 loss:0.1844 acc:0.7584\n",
      "Validation Epoch: 750/1000 loss:nan acc:nan\n",
      "Training Epoch: 751/1000 loss:0.1845 acc:0.7587\n",
      "Validation Epoch: 751/1000 loss:nan acc:nan\n",
      "Training Epoch: 752/1000 loss:0.1845 acc:0.7591\n",
      "Validation Epoch: 752/1000 loss:nan acc:nan\n",
      "Training Epoch: 753/1000 loss:0.1846 acc:0.7591\n",
      "Validation Epoch: 753/1000 loss:nan acc:nan\n",
      "Training Epoch: 754/1000 loss:0.1847 acc:0.7587\n",
      "Validation Epoch: 754/1000 loss:nan acc:nan\n",
      "Training Epoch: 755/1000 loss:0.1847 acc:0.7584\n",
      "Validation Epoch: 755/1000 loss:nan acc:nan\n",
      "Training Epoch: 756/1000 loss:0.1847 acc:0.7581\n",
      "Validation Epoch: 756/1000 loss:nan acc:nan\n",
      "Training Epoch: 757/1000 loss:0.1846 acc:0.7587\n",
      "Validation Epoch: 757/1000 loss:nan acc:nan\n",
      "Training Epoch: 758/1000 loss:0.1844 acc:0.7591\n",
      "Validation Epoch: 758/1000 loss:nan acc:nan\n",
      "Training Epoch: 759/1000 loss:0.1841 acc:0.7594\n",
      "Validation Epoch: 759/1000 loss:nan acc:nan\n",
      "Training Epoch: 760/1000 loss:0.1838 acc:0.7594\n",
      "Validation Epoch: 760/1000 loss:nan acc:nan\n",
      "Training Epoch: 761/1000 loss:0.1836 acc:0.7594\n",
      "Validation Epoch: 761/1000 loss:nan acc:nan\n",
      "Training Epoch: 762/1000 loss:0.1834 acc:0.7591\n",
      "Validation Epoch: 762/1000 loss:nan acc:nan\n",
      "Training Epoch: 763/1000 loss:0.1832 acc:0.7591\n",
      "Validation Epoch: 763/1000 loss:nan acc:nan\n",
      "Training Epoch: 764/1000 loss:0.1831 acc:0.7598\n",
      "Validation Epoch: 764/1000 loss:nan acc:nan\n",
      "Training Epoch: 765/1000 loss:0.1829 acc:0.7598\n",
      "Validation Epoch: 765/1000 loss:nan acc:nan\n",
      "Training Epoch: 766/1000 loss:0.1828 acc:0.7598\n",
      "Validation Epoch: 766/1000 loss:nan acc:nan\n",
      "Training Epoch: 767/1000 loss:0.1826 acc:0.7591\n",
      "Validation Epoch: 767/1000 loss:nan acc:nan\n",
      "Training Epoch: 768/1000 loss:0.1825 acc:0.7591\n",
      "Validation Epoch: 768/1000 loss:nan acc:nan\n",
      "Training Epoch: 769/1000 loss:0.1823 acc:0.7591\n",
      "Validation Epoch: 769/1000 loss:nan acc:nan\n",
      "Training Epoch: 770/1000 loss:0.1822 acc:0.7591\n",
      "Validation Epoch: 770/1000 loss:nan acc:nan\n",
      "Training Epoch: 771/1000 loss:0.1821 acc:0.7591\n",
      "Validation Epoch: 771/1000 loss:nan acc:nan\n",
      "Training Epoch: 772/1000 loss:0.1819 acc:0.7587\n",
      "Validation Epoch: 772/1000 loss:nan acc:nan\n",
      "Training Epoch: 773/1000 loss:0.1817 acc:0.7584\n",
      "Validation Epoch: 773/1000 loss:nan acc:nan\n",
      "Training Epoch: 774/1000 loss:0.1816 acc:0.7587\n",
      "Validation Epoch: 774/1000 loss:nan acc:nan\n",
      "Training Epoch: 775/1000 loss:0.1814 acc:0.7594\n",
      "Validation Epoch: 775/1000 loss:nan acc:nan\n",
      "Training Epoch: 776/1000 loss:0.1812 acc:0.7591\n",
      "Validation Epoch: 776/1000 loss:nan acc:nan\n",
      "Training Epoch: 777/1000 loss:0.1810 acc:0.7584\n",
      "Validation Epoch: 777/1000 loss:nan acc:nan\n",
      "Training Epoch: 778/1000 loss:0.1809 acc:0.7584\n",
      "Validation Epoch: 778/1000 loss:nan acc:nan\n",
      "Training Epoch: 779/1000 loss:0.1807 acc:0.7587\n",
      "Validation Epoch: 779/1000 loss:nan acc:nan\n",
      "Training Epoch: 780/1000 loss:0.1805 acc:0.7591\n",
      "Validation Epoch: 780/1000 loss:nan acc:nan\n",
      "Training Epoch: 781/1000 loss:0.1804 acc:0.7587\n",
      "Validation Epoch: 781/1000 loss:nan acc:nan\n",
      "Training Epoch: 782/1000 loss:0.1802 acc:0.7587\n",
      "Validation Epoch: 782/1000 loss:nan acc:nan\n",
      "Training Epoch: 783/1000 loss:0.1800 acc:0.7587\n",
      "Validation Epoch: 783/1000 loss:nan acc:nan\n",
      "Training Epoch: 784/1000 loss:0.1798 acc:0.7584\n",
      "Validation Epoch: 784/1000 loss:nan acc:nan\n",
      "Training Epoch: 785/1000 loss:0.1795 acc:0.7591\n",
      "Validation Epoch: 785/1000 loss:nan acc:nan\n",
      "Training Epoch: 786/1000 loss:0.1790 acc:0.7587\n",
      "Validation Epoch: 786/1000 loss:nan acc:nan\n",
      "Training Epoch: 787/1000 loss:0.1789 acc:0.7591\n",
      "Validation Epoch: 787/1000 loss:nan acc:nan\n",
      "Training Epoch: 788/1000 loss:0.1787 acc:0.7587\n",
      "Validation Epoch: 788/1000 loss:nan acc:nan\n",
      "Training Epoch: 789/1000 loss:0.1787 acc:0.7587\n",
      "Validation Epoch: 789/1000 loss:nan acc:nan\n",
      "Training Epoch: 790/1000 loss:0.1785 acc:0.7594\n",
      "Validation Epoch: 790/1000 loss:nan acc:nan\n",
      "Training Epoch: 791/1000 loss:0.1783 acc:0.7598\n",
      "Validation Epoch: 791/1000 loss:nan acc:nan\n",
      "Training Epoch: 792/1000 loss:0.1781 acc:0.7601\n",
      "Validation Epoch: 792/1000 loss:nan acc:nan\n",
      "Training Epoch: 793/1000 loss:0.1779 acc:0.7598\n",
      "Validation Epoch: 793/1000 loss:nan acc:nan\n",
      "Training Epoch: 794/1000 loss:0.1777 acc:0.7601\n",
      "Validation Epoch: 794/1000 loss:nan acc:nan\n",
      "Training Epoch: 795/1000 loss:0.1776 acc:0.7598\n",
      "Validation Epoch: 795/1000 loss:nan acc:nan\n",
      "Training Epoch: 796/1000 loss:0.1776 acc:0.7591\n",
      "Validation Epoch: 796/1000 loss:nan acc:nan\n",
      "Training Epoch: 797/1000 loss:0.1775 acc:0.7594\n",
      "Validation Epoch: 797/1000 loss:nan acc:nan\n",
      "Training Epoch: 798/1000 loss:0.1773 acc:0.7601\n",
      "Validation Epoch: 798/1000 loss:nan acc:nan\n",
      "Training Epoch: 799/1000 loss:0.1771 acc:0.7601\n",
      "Validation Epoch: 799/1000 loss:nan acc:nan\n",
      "Training Epoch: 800/1000 loss:0.1768 acc:0.7601\n",
      "Validation Epoch: 800/1000 loss:nan acc:nan\n",
      "Training Epoch: 801/1000 loss:0.1767 acc:0.7594\n",
      "Validation Epoch: 801/1000 loss:nan acc:nan\n",
      "Training Epoch: 802/1000 loss:0.1766 acc:0.7598\n",
      "Validation Epoch: 802/1000 loss:nan acc:nan\n",
      "Training Epoch: 803/1000 loss:0.1764 acc:0.7608\n",
      "Validation Epoch: 803/1000 loss:nan acc:nan\n",
      "Training Epoch: 804/1000 loss:0.1763 acc:0.7615\n",
      "Validation Epoch: 804/1000 loss:nan acc:nan\n",
      "Training Epoch: 805/1000 loss:0.1762 acc:0.7611\n",
      "Validation Epoch: 805/1000 loss:nan acc:nan\n",
      "Training Epoch: 806/1000 loss:0.1761 acc:0.7604\n",
      "Validation Epoch: 806/1000 loss:nan acc:nan\n",
      "Training Epoch: 807/1000 loss:0.1759 acc:0.7608\n",
      "Validation Epoch: 807/1000 loss:nan acc:nan\n",
      "Training Epoch: 808/1000 loss:0.1758 acc:0.7618\n",
      "Validation Epoch: 808/1000 loss:nan acc:nan\n",
      "Training Epoch: 809/1000 loss:0.1757 acc:0.7618\n",
      "Validation Epoch: 809/1000 loss:nan acc:nan\n",
      "Training Epoch: 810/1000 loss:0.1756 acc:0.7618\n",
      "Validation Epoch: 810/1000 loss:nan acc:nan\n",
      "Training Epoch: 811/1000 loss:0.1755 acc:0.7608\n",
      "Validation Epoch: 811/1000 loss:nan acc:nan\n",
      "Training Epoch: 812/1000 loss:0.1753 acc:0.7608\n",
      "Validation Epoch: 812/1000 loss:nan acc:nan\n",
      "Training Epoch: 813/1000 loss:0.1752 acc:0.7618\n",
      "Validation Epoch: 813/1000 loss:nan acc:nan\n",
      "Training Epoch: 814/1000 loss:0.1751 acc:0.7618\n",
      "Validation Epoch: 814/1000 loss:nan acc:nan\n",
      "Training Epoch: 815/1000 loss:0.1750 acc:0.7615\n",
      "Validation Epoch: 815/1000 loss:nan acc:nan\n",
      "Training Epoch: 816/1000 loss:0.1749 acc:0.7611\n",
      "Validation Epoch: 816/1000 loss:nan acc:nan\n",
      "Training Epoch: 817/1000 loss:0.1747 acc:0.7611\n",
      "Validation Epoch: 817/1000 loss:nan acc:nan\n",
      "Training Epoch: 818/1000 loss:0.1746 acc:0.7618\n",
      "Validation Epoch: 818/1000 loss:nan acc:nan\n",
      "Training Epoch: 819/1000 loss:0.1745 acc:0.7618\n",
      "Validation Epoch: 819/1000 loss:nan acc:nan\n",
      "Training Epoch: 820/1000 loss:0.1744 acc:0.7615\n",
      "Validation Epoch: 820/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 821/1000 loss:0.1742 acc:0.7611\n",
      "Validation Epoch: 821/1000 loss:nan acc:nan\n",
      "Training Epoch: 822/1000 loss:0.1739 acc:0.7615\n",
      "Validation Epoch: 822/1000 loss:nan acc:nan\n",
      "Training Epoch: 823/1000 loss:0.1738 acc:0.7618\n",
      "Validation Epoch: 823/1000 loss:nan acc:nan\n",
      "Training Epoch: 824/1000 loss:0.1738 acc:0.7618\n",
      "Validation Epoch: 824/1000 loss:nan acc:nan\n",
      "Training Epoch: 825/1000 loss:0.1734 acc:0.7611\n",
      "Validation Epoch: 825/1000 loss:nan acc:nan\n",
      "Training Epoch: 826/1000 loss:0.1732 acc:0.7611\n",
      "Validation Epoch: 826/1000 loss:nan acc:nan\n",
      "Training Epoch: 827/1000 loss:0.1718 acc:0.7618\n",
      "Validation Epoch: 827/1000 loss:nan acc:nan\n",
      "Training Epoch: 828/1000 loss:0.1717 acc:0.7621\n",
      "Validation Epoch: 828/1000 loss:nan acc:nan\n",
      "Training Epoch: 829/1000 loss:0.1710 acc:0.7621\n",
      "Validation Epoch: 829/1000 loss:nan acc:nan\n",
      "Training Epoch: 830/1000 loss:0.1708 acc:0.7611\n",
      "Validation Epoch: 830/1000 loss:nan acc:nan\n",
      "Training Epoch: 831/1000 loss:0.1707 acc:0.7611\n",
      "Validation Epoch: 831/1000 loss:nan acc:nan\n",
      "Training Epoch: 832/1000 loss:0.1704 acc:0.7615\n",
      "Validation Epoch: 832/1000 loss:nan acc:nan\n",
      "Training Epoch: 833/1000 loss:0.1700 acc:0.7615\n",
      "Validation Epoch: 833/1000 loss:nan acc:nan\n",
      "Training Epoch: 834/1000 loss:0.1697 acc:0.7615\n",
      "Validation Epoch: 834/1000 loss:nan acc:nan\n",
      "Training Epoch: 835/1000 loss:0.1694 acc:0.7618\n",
      "Validation Epoch: 835/1000 loss:nan acc:nan\n",
      "Training Epoch: 836/1000 loss:0.1692 acc:0.7615\n",
      "Validation Epoch: 836/1000 loss:nan acc:nan\n",
      "Training Epoch: 837/1000 loss:0.1690 acc:0.7618\n",
      "Validation Epoch: 837/1000 loss:nan acc:nan\n",
      "Training Epoch: 838/1000 loss:0.1689 acc:0.7618\n",
      "Validation Epoch: 838/1000 loss:nan acc:nan\n",
      "Training Epoch: 839/1000 loss:0.1688 acc:0.7611\n",
      "Validation Epoch: 839/1000 loss:nan acc:nan\n",
      "Training Epoch: 840/1000 loss:0.1687 acc:0.7615\n",
      "Validation Epoch: 840/1000 loss:nan acc:nan\n",
      "Training Epoch: 841/1000 loss:0.1688 acc:0.7625\n",
      "Validation Epoch: 841/1000 loss:nan acc:nan\n",
      "Training Epoch: 842/1000 loss:0.1706 acc:0.7615\n",
      "Validation Epoch: 842/1000 loss:nan acc:nan\n",
      "Training Epoch: 843/1000 loss:0.1707 acc:0.7615\n",
      "Validation Epoch: 843/1000 loss:nan acc:nan\n",
      "Training Epoch: 844/1000 loss:0.1707 acc:0.7618\n",
      "Validation Epoch: 844/1000 loss:nan acc:nan\n",
      "Training Epoch: 845/1000 loss:0.1709 acc:0.7618\n",
      "Validation Epoch: 845/1000 loss:nan acc:nan\n",
      "Training Epoch: 846/1000 loss:0.1707 acc:0.7615\n",
      "Validation Epoch: 846/1000 loss:nan acc:nan\n",
      "Training Epoch: 847/1000 loss:0.1702 acc:0.7611\n",
      "Validation Epoch: 847/1000 loss:nan acc:nan\n",
      "Training Epoch: 848/1000 loss:0.1699 acc:0.7621\n",
      "Validation Epoch: 848/1000 loss:nan acc:nan\n",
      "Training Epoch: 849/1000 loss:0.1698 acc:0.7621\n",
      "Validation Epoch: 849/1000 loss:nan acc:nan\n",
      "Training Epoch: 850/1000 loss:0.1697 acc:0.7615\n",
      "Validation Epoch: 850/1000 loss:nan acc:nan\n",
      "Training Epoch: 851/1000 loss:0.1697 acc:0.7618\n",
      "Validation Epoch: 851/1000 loss:nan acc:nan\n",
      "Training Epoch: 852/1000 loss:0.1698 acc:0.7618\n",
      "Validation Epoch: 852/1000 loss:nan acc:nan\n",
      "Training Epoch: 853/1000 loss:0.1693 acc:0.7621\n",
      "Validation Epoch: 853/1000 loss:nan acc:nan\n",
      "Training Epoch: 854/1000 loss:0.1695 acc:0.7628\n",
      "Validation Epoch: 854/1000 loss:nan acc:nan\n",
      "Training Epoch: 855/1000 loss:0.1699 acc:0.7615\n",
      "Validation Epoch: 855/1000 loss:nan acc:nan\n",
      "Training Epoch: 856/1000 loss:0.1706 acc:0.7611\n",
      "Validation Epoch: 856/1000 loss:nan acc:nan\n",
      "Training Epoch: 857/1000 loss:0.1705 acc:0.7615\n",
      "Validation Epoch: 857/1000 loss:nan acc:nan\n",
      "Training Epoch: 858/1000 loss:0.1704 acc:0.7621\n",
      "Validation Epoch: 858/1000 loss:nan acc:nan\n",
      "Training Epoch: 859/1000 loss:0.1696 acc:0.7628\n",
      "Validation Epoch: 859/1000 loss:nan acc:nan\n",
      "Training Epoch: 860/1000 loss:0.1697 acc:0.7618\n",
      "Validation Epoch: 860/1000 loss:nan acc:nan\n",
      "Training Epoch: 861/1000 loss:0.1743 acc:0.7577\n",
      "Validation Epoch: 861/1000 loss:nan acc:nan\n",
      "Training Epoch: 862/1000 loss:0.1778 acc:0.7540\n",
      "Validation Epoch: 862/1000 loss:nan acc:nan\n",
      "Training Epoch: 863/1000 loss:0.1789 acc:0.7520\n",
      "Validation Epoch: 863/1000 loss:nan acc:nan\n",
      "Training Epoch: 864/1000 loss:0.1793 acc:0.7540\n",
      "Validation Epoch: 864/1000 loss:nan acc:nan\n",
      "Training Epoch: 865/1000 loss:0.1792 acc:0.7530\n",
      "Validation Epoch: 865/1000 loss:nan acc:nan\n",
      "Training Epoch: 866/1000 loss:0.1777 acc:0.7513\n",
      "Validation Epoch: 866/1000 loss:nan acc:nan\n",
      "Training Epoch: 867/1000 loss:0.1761 acc:0.7553\n",
      "Validation Epoch: 867/1000 loss:nan acc:nan\n",
      "Training Epoch: 868/1000 loss:0.1725 acc:0.7581\n",
      "Validation Epoch: 868/1000 loss:nan acc:nan\n",
      "Training Epoch: 869/1000 loss:0.1721 acc:0.7598\n",
      "Validation Epoch: 869/1000 loss:nan acc:nan\n",
      "Training Epoch: 870/1000 loss:0.1708 acc:0.7615\n",
      "Validation Epoch: 870/1000 loss:nan acc:nan\n",
      "Training Epoch: 871/1000 loss:0.1727 acc:0.7608\n",
      "Validation Epoch: 871/1000 loss:nan acc:nan\n",
      "Training Epoch: 872/1000 loss:0.1727 acc:0.7587\n",
      "Validation Epoch: 872/1000 loss:nan acc:nan\n",
      "Training Epoch: 873/1000 loss:0.1789 acc:0.7530\n",
      "Validation Epoch: 873/1000 loss:nan acc:nan\n",
      "Training Epoch: 874/1000 loss:0.1813 acc:0.7492\n",
      "Validation Epoch: 874/1000 loss:nan acc:nan\n",
      "Training Epoch: 875/1000 loss:0.1817 acc:0.7492\n",
      "Validation Epoch: 875/1000 loss:nan acc:nan\n",
      "Training Epoch: 876/1000 loss:0.1804 acc:0.7489\n",
      "Validation Epoch: 876/1000 loss:nan acc:nan\n",
      "Training Epoch: 877/1000 loss:0.1769 acc:0.7486\n",
      "Validation Epoch: 877/1000 loss:nan acc:nan\n",
      "Training Epoch: 878/1000 loss:0.1745 acc:0.7523\n",
      "Validation Epoch: 878/1000 loss:nan acc:nan\n",
      "Training Epoch: 879/1000 loss:0.1751 acc:0.7536\n",
      "Validation Epoch: 879/1000 loss:nan acc:nan\n",
      "Training Epoch: 880/1000 loss:0.1768 acc:0.7543\n",
      "Validation Epoch: 880/1000 loss:nan acc:nan\n",
      "Training Epoch: 881/1000 loss:0.1768 acc:0.7564\n",
      "Validation Epoch: 881/1000 loss:nan acc:nan\n",
      "Training Epoch: 882/1000 loss:0.1755 acc:0.7570\n",
      "Validation Epoch: 882/1000 loss:nan acc:nan\n",
      "Training Epoch: 883/1000 loss:0.1739 acc:0.7587\n",
      "Validation Epoch: 883/1000 loss:nan acc:nan\n",
      "Training Epoch: 884/1000 loss:0.1723 acc:0.7601\n",
      "Validation Epoch: 884/1000 loss:nan acc:nan\n",
      "Training Epoch: 885/1000 loss:0.1702 acc:0.7608\n",
      "Validation Epoch: 885/1000 loss:nan acc:nan\n",
      "Training Epoch: 886/1000 loss:0.1689 acc:0.7601\n",
      "Validation Epoch: 886/1000 loss:nan acc:nan\n",
      "Training Epoch: 887/1000 loss:0.1682 acc:0.7611\n",
      "Validation Epoch: 887/1000 loss:nan acc:nan\n",
      "Training Epoch: 888/1000 loss:0.1699 acc:0.7604\n",
      "Validation Epoch: 888/1000 loss:nan acc:nan\n",
      "Training Epoch: 889/1000 loss:0.1708 acc:0.7584\n",
      "Validation Epoch: 889/1000 loss:nan acc:nan\n",
      "Training Epoch: 890/1000 loss:0.1714 acc:0.7587\n",
      "Validation Epoch: 890/1000 loss:nan acc:nan\n",
      "Training Epoch: 891/1000 loss:0.1720 acc:0.7598\n",
      "Validation Epoch: 891/1000 loss:nan acc:nan\n",
      "Training Epoch: 892/1000 loss:0.1714 acc:0.7598\n",
      "Validation Epoch: 892/1000 loss:nan acc:nan\n",
      "Training Epoch: 893/1000 loss:0.1709 acc:0.7594\n",
      "Validation Epoch: 893/1000 loss:nan acc:nan\n",
      "Training Epoch: 894/1000 loss:0.1708 acc:0.7594\n",
      "Validation Epoch: 894/1000 loss:nan acc:nan\n",
      "Training Epoch: 895/1000 loss:0.1696 acc:0.7594\n",
      "Validation Epoch: 895/1000 loss:nan acc:nan\n",
      "Training Epoch: 896/1000 loss:0.1695 acc:0.7604\n",
      "Validation Epoch: 896/1000 loss:nan acc:nan\n",
      "Training Epoch: 897/1000 loss:0.1681 acc:0.7618\n",
      "Validation Epoch: 897/1000 loss:nan acc:nan\n",
      "Training Epoch: 898/1000 loss:0.1691 acc:0.7611\n",
      "Validation Epoch: 898/1000 loss:nan acc:nan\n",
      "Training Epoch: 899/1000 loss:0.1687 acc:0.7618\n",
      "Validation Epoch: 899/1000 loss:nan acc:nan\n",
      "Training Epoch: 900/1000 loss:0.1684 acc:0.7615\n",
      "Validation Epoch: 900/1000 loss:nan acc:nan\n",
      "Training Epoch: 901/1000 loss:0.1686 acc:0.7615\n",
      "Validation Epoch: 901/1000 loss:nan acc:nan\n",
      "Training Epoch: 902/1000 loss:0.1686 acc:0.7604\n",
      "Validation Epoch: 902/1000 loss:nan acc:nan\n",
      "Training Epoch: 903/1000 loss:0.1682 acc:0.7611\n",
      "Validation Epoch: 903/1000 loss:nan acc:nan\n",
      "Training Epoch: 904/1000 loss:0.1667 acc:0.7611\n",
      "Validation Epoch: 904/1000 loss:nan acc:nan\n",
      "Training Epoch: 905/1000 loss:0.1670 acc:0.7604\n",
      "Validation Epoch: 905/1000 loss:nan acc:nan\n",
      "Training Epoch: 906/1000 loss:0.1669 acc:0.7618\n",
      "Validation Epoch: 906/1000 loss:nan acc:nan\n",
      "Training Epoch: 907/1000 loss:0.1676 acc:0.7621\n",
      "Validation Epoch: 907/1000 loss:nan acc:nan\n",
      "Training Epoch: 908/1000 loss:0.1673 acc:0.7615\n",
      "Validation Epoch: 908/1000 loss:nan acc:nan\n",
      "Training Epoch: 909/1000 loss:0.1663 acc:0.7615\n",
      "Validation Epoch: 909/1000 loss:nan acc:nan\n",
      "Training Epoch: 910/1000 loss:0.1670 acc:0.7621\n",
      "Validation Epoch: 910/1000 loss:nan acc:nan\n",
      "Training Epoch: 911/1000 loss:0.1667 acc:0.7621\n",
      "Validation Epoch: 911/1000 loss:nan acc:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 912/1000 loss:0.1663 acc:0.7615\n",
      "Validation Epoch: 912/1000 loss:nan acc:nan\n",
      "Training Epoch: 913/1000 loss:0.1674 acc:0.7625\n",
      "Validation Epoch: 913/1000 loss:nan acc:nan\n",
      "Training Epoch: 914/1000 loss:0.1679 acc:0.7615\n",
      "Validation Epoch: 914/1000 loss:nan acc:nan\n",
      "Training Epoch: 915/1000 loss:0.1679 acc:0.7615\n",
      "Validation Epoch: 915/1000 loss:nan acc:nan\n",
      "Training Epoch: 916/1000 loss:0.1666 acc:0.7615\n",
      "Validation Epoch: 916/1000 loss:nan acc:nan\n",
      "Training Epoch: 917/1000 loss:0.1661 acc:0.7611\n",
      "Validation Epoch: 917/1000 loss:nan acc:nan\n",
      "Training Epoch: 918/1000 loss:0.1654 acc:0.7611\n",
      "Validation Epoch: 918/1000 loss:nan acc:nan\n",
      "Training Epoch: 919/1000 loss:0.1652 acc:0.7621\n",
      "Validation Epoch: 919/1000 loss:nan acc:nan\n",
      "Training Epoch: 920/1000 loss:0.1657 acc:0.7621\n",
      "Validation Epoch: 920/1000 loss:nan acc:nan\n",
      "Training Epoch: 921/1000 loss:0.1668 acc:0.7615\n",
      "Validation Epoch: 921/1000 loss:nan acc:nan\n",
      "Training Epoch: 922/1000 loss:0.1673 acc:0.7611\n",
      "Validation Epoch: 922/1000 loss:nan acc:nan\n",
      "Training Epoch: 923/1000 loss:0.1700 acc:0.7604\n",
      "Validation Epoch: 923/1000 loss:nan acc:nan\n",
      "Training Epoch: 924/1000 loss:0.1710 acc:0.7601\n",
      "Validation Epoch: 924/1000 loss:nan acc:nan\n",
      "Training Epoch: 925/1000 loss:0.1708 acc:0.7611\n",
      "Validation Epoch: 925/1000 loss:nan acc:nan\n",
      "Training Epoch: 926/1000 loss:0.1695 acc:0.7611\n",
      "Validation Epoch: 926/1000 loss:nan acc:nan\n",
      "Training Epoch: 927/1000 loss:0.1681 acc:0.7611\n",
      "Validation Epoch: 927/1000 loss:nan acc:nan\n",
      "Training Epoch: 928/1000 loss:0.1648 acc:0.7611\n",
      "Validation Epoch: 928/1000 loss:nan acc:nan\n",
      "Training Epoch: 929/1000 loss:0.1677 acc:0.7618\n",
      "Validation Epoch: 929/1000 loss:nan acc:nan\n",
      "Training Epoch: 930/1000 loss:0.1686 acc:0.7601\n",
      "Validation Epoch: 930/1000 loss:nan acc:nan\n",
      "Training Epoch: 931/1000 loss:0.1677 acc:0.7594\n",
      "Validation Epoch: 931/1000 loss:nan acc:nan\n",
      "Training Epoch: 932/1000 loss:0.1655 acc:0.7625\n",
      "Validation Epoch: 932/1000 loss:nan acc:nan\n",
      "Training Epoch: 933/1000 loss:0.1637 acc:0.7608\n",
      "Validation Epoch: 933/1000 loss:nan acc:nan\n",
      "Training Epoch: 934/1000 loss:0.1646 acc:0.7615\n",
      "Validation Epoch: 934/1000 loss:nan acc:nan\n",
      "Training Epoch: 935/1000 loss:0.1653 acc:0.7611\n",
      "Validation Epoch: 935/1000 loss:nan acc:nan\n",
      "Training Epoch: 936/1000 loss:0.1652 acc:0.7611\n",
      "Validation Epoch: 936/1000 loss:nan acc:nan\n",
      "Training Epoch: 937/1000 loss:0.1652 acc:0.7604\n",
      "Validation Epoch: 937/1000 loss:nan acc:nan\n",
      "Training Epoch: 938/1000 loss:0.1644 acc:0.7625\n",
      "Validation Epoch: 938/1000 loss:nan acc:nan\n",
      "Training Epoch: 939/1000 loss:0.1635 acc:0.7625\n",
      "Validation Epoch: 939/1000 loss:nan acc:nan\n",
      "Training Epoch: 940/1000 loss:0.1631 acc:0.7618\n",
      "Validation Epoch: 940/1000 loss:nan acc:nan\n",
      "Training Epoch: 941/1000 loss:0.1682 acc:0.7594\n",
      "Validation Epoch: 941/1000 loss:nan acc:nan\n",
      "Training Epoch: 942/1000 loss:0.1731 acc:0.7553\n",
      "Validation Epoch: 942/1000 loss:nan acc:nan\n",
      "Training Epoch: 943/1000 loss:0.1758 acc:0.7506\n",
      "Validation Epoch: 943/1000 loss:nan acc:nan\n",
      "Training Epoch: 944/1000 loss:0.1772 acc:0.7499\n",
      "Validation Epoch: 944/1000 loss:nan acc:nan\n",
      "Training Epoch: 945/1000 loss:0.1774 acc:0.7509\n",
      "Validation Epoch: 945/1000 loss:nan acc:nan\n",
      "Training Epoch: 946/1000 loss:0.1761 acc:0.7533\n",
      "Validation Epoch: 946/1000 loss:nan acc:nan\n",
      "Training Epoch: 947/1000 loss:0.1737 acc:0.7550\n",
      "Validation Epoch: 947/1000 loss:nan acc:nan\n",
      "Training Epoch: 948/1000 loss:0.1705 acc:0.7543\n",
      "Validation Epoch: 948/1000 loss:nan acc:nan\n",
      "Training Epoch: 949/1000 loss:0.1681 acc:0.7567\n",
      "Validation Epoch: 949/1000 loss:nan acc:nan\n",
      "Training Epoch: 950/1000 loss:0.1662 acc:0.7591\n",
      "Validation Epoch: 950/1000 loss:nan acc:nan\n",
      "Training Epoch: 951/1000 loss:0.1654 acc:0.7601\n",
      "Validation Epoch: 951/1000 loss:nan acc:nan\n",
      "Training Epoch: 952/1000 loss:0.1650 acc:0.7608\n",
      "Validation Epoch: 952/1000 loss:nan acc:nan\n",
      "Training Epoch: 953/1000 loss:0.1640 acc:0.7615\n",
      "Validation Epoch: 953/1000 loss:nan acc:nan\n",
      "Training Epoch: 954/1000 loss:0.1640 acc:0.7621\n",
      "Validation Epoch: 954/1000 loss:nan acc:nan\n",
      "Training Epoch: 955/1000 loss:0.1639 acc:0.7615\n",
      "Validation Epoch: 955/1000 loss:nan acc:nan\n",
      "Training Epoch: 956/1000 loss:0.1635 acc:0.7615\n",
      "Validation Epoch: 956/1000 loss:nan acc:nan\n",
      "Training Epoch: 957/1000 loss:0.1650 acc:0.7615\n",
      "Validation Epoch: 957/1000 loss:nan acc:nan\n",
      "Training Epoch: 958/1000 loss:0.1661 acc:0.7594\n",
      "Validation Epoch: 958/1000 loss:nan acc:nan\n",
      "Training Epoch: 959/1000 loss:0.1650 acc:0.7604\n",
      "Validation Epoch: 959/1000 loss:nan acc:nan\n",
      "Training Epoch: 960/1000 loss:0.1634 acc:0.7621\n",
      "Validation Epoch: 960/1000 loss:nan acc:nan\n",
      "Training Epoch: 961/1000 loss:0.1625 acc:0.7621\n",
      "Validation Epoch: 961/1000 loss:nan acc:nan\n",
      "Training Epoch: 962/1000 loss:0.1619 acc:0.7625\n",
      "Validation Epoch: 962/1000 loss:nan acc:nan\n",
      "Training Epoch: 963/1000 loss:0.1620 acc:0.7628\n",
      "Validation Epoch: 963/1000 loss:nan acc:nan\n",
      "Training Epoch: 964/1000 loss:0.1638 acc:0.7608\n",
      "Validation Epoch: 964/1000 loss:nan acc:nan\n",
      "Training Epoch: 965/1000 loss:0.1682 acc:0.7570\n",
      "Validation Epoch: 965/1000 loss:nan acc:nan\n",
      "Training Epoch: 966/1000 loss:0.1712 acc:0.7547\n",
      "Validation Epoch: 966/1000 loss:nan acc:nan\n",
      "Training Epoch: 967/1000 loss:0.1728 acc:0.7523\n",
      "Validation Epoch: 967/1000 loss:nan acc:nan\n",
      "Training Epoch: 968/1000 loss:0.1725 acc:0.7513\n",
      "Validation Epoch: 968/1000 loss:nan acc:nan\n",
      "Training Epoch: 969/1000 loss:0.1722 acc:0.7520\n",
      "Validation Epoch: 969/1000 loss:nan acc:nan\n",
      "Training Epoch: 970/1000 loss:0.1700 acc:0.7530\n",
      "Validation Epoch: 970/1000 loss:nan acc:nan\n",
      "Training Epoch: 971/1000 loss:0.1704 acc:0.7520\n",
      "Validation Epoch: 971/1000 loss:nan acc:nan\n",
      "Training Epoch: 972/1000 loss:0.1708 acc:0.7543\n",
      "Validation Epoch: 972/1000 loss:nan acc:nan\n",
      "Training Epoch: 973/1000 loss:0.1705 acc:0.7564\n",
      "Validation Epoch: 973/1000 loss:nan acc:nan\n",
      "Training Epoch: 974/1000 loss:0.1706 acc:0.7557\n",
      "Validation Epoch: 974/1000 loss:nan acc:nan\n",
      "Training Epoch: 975/1000 loss:0.1701 acc:0.7567\n",
      "Validation Epoch: 975/1000 loss:nan acc:nan\n",
      "Training Epoch: 976/1000 loss:0.1693 acc:0.7560\n",
      "Validation Epoch: 976/1000 loss:nan acc:nan\n",
      "Training Epoch: 977/1000 loss:0.1684 acc:0.7550\n",
      "Validation Epoch: 977/1000 loss:nan acc:nan\n",
      "Training Epoch: 978/1000 loss:0.1671 acc:0.7570\n",
      "Validation Epoch: 978/1000 loss:nan acc:nan\n",
      "Training Epoch: 979/1000 loss:0.1662 acc:0.7570\n",
      "Validation Epoch: 979/1000 loss:nan acc:nan\n",
      "Training Epoch: 980/1000 loss:0.1653 acc:0.7574\n",
      "Validation Epoch: 980/1000 loss:nan acc:nan\n",
      "Training Epoch: 981/1000 loss:0.1650 acc:0.7581\n",
      "Validation Epoch: 981/1000 loss:nan acc:nan\n",
      "Training Epoch: 982/1000 loss:0.1641 acc:0.7591\n",
      "Validation Epoch: 982/1000 loss:nan acc:nan\n",
      "Training Epoch: 983/1000 loss:0.1621 acc:0.7615\n",
      "Validation Epoch: 983/1000 loss:nan acc:nan\n",
      "Training Epoch: 984/1000 loss:0.1615 acc:0.7628\n",
      "Validation Epoch: 984/1000 loss:nan acc:nan\n",
      "Training Epoch: 985/1000 loss:0.1619 acc:0.7635\n",
      "Validation Epoch: 985/1000 loss:nan acc:nan\n",
      "Training Epoch: 986/1000 loss:0.1618 acc:0.7621\n",
      "Validation Epoch: 986/1000 loss:nan acc:nan\n",
      "Training Epoch: 987/1000 loss:0.1616 acc:0.7618\n",
      "Validation Epoch: 987/1000 loss:nan acc:nan\n",
      "Training Epoch: 988/1000 loss:0.1605 acc:0.7625\n",
      "Validation Epoch: 988/1000 loss:nan acc:nan\n",
      "Training Epoch: 989/1000 loss:0.1610 acc:0.7621\n",
      "Validation Epoch: 989/1000 loss:nan acc:nan\n",
      "Training Epoch: 990/1000 loss:0.1608 acc:0.7621\n",
      "Validation Epoch: 990/1000 loss:nan acc:nan\n",
      "Training Epoch: 991/1000 loss:0.1593 acc:0.7621\n",
      "Validation Epoch: 991/1000 loss:nan acc:nan\n",
      "Training Epoch: 992/1000 loss:0.1593 acc:0.7611\n",
      "Validation Epoch: 992/1000 loss:nan acc:nan\n",
      "Training Epoch: 993/1000 loss:0.1592 acc:0.7608\n",
      "Validation Epoch: 993/1000 loss:nan acc:nan\n",
      "Training Epoch: 994/1000 loss:0.1595 acc:0.7611\n",
      "Validation Epoch: 994/1000 loss:nan acc:nan\n",
      "Training Epoch: 995/1000 loss:0.1596 acc:0.7601\n",
      "Validation Epoch: 995/1000 loss:nan acc:nan\n",
      "Training Epoch: 996/1000 loss:0.1595 acc:0.7598\n",
      "Validation Epoch: 996/1000 loss:nan acc:nan\n",
      "Training Epoch: 997/1000 loss:0.1591 acc:0.7604\n",
      "Validation Epoch: 997/1000 loss:nan acc:nan\n",
      "Training Epoch: 998/1000 loss:0.1589 acc:0.7608\n",
      "Validation Epoch: 998/1000 loss:nan acc:nan\n",
      "Training Epoch: 999/1000 loss:0.1584 acc:0.7611\n",
      "Validation Epoch: 999/1000 loss:nan acc:nan\n",
      "Training Epoch: 1000/1000 loss:0.1589 acc:0.7615\n",
      "Validation Epoch: 1000/1000 loss:nan acc:nan\n"
     ]
    }
   ],
   "source": [
    "valid_acc, valid_loss = [], []\n",
    "train_acc, train_loss = [], []\n",
    "\n",
    "# with graph.as_default():\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the global variable instead of loading them or if there is nothing to load.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #     # Restore\n",
    "    #     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    #     saver.restore(sess,\"checkpoints/har-lstm.ckpt\")\n",
    "    \n",
    "    # Epochs/episodes of training/updating the model/network\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Training: Loop over batches\n",
    "        state = sess.run(initial_state)\n",
    "        acc_batch, loss_batch = [], []\n",
    "        for Xbatch, Ybatch in get_batches(Xtrain, Ytrain, batch_size):\n",
    "\n",
    "            # Feed dictionary\n",
    "            feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "            loss, _, state, acc = sess.run([cost, optimizer, final_state, accuracy], feed_dict)\n",
    "            acc_batch.append(acc)\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Print at each epoch/iteration\n",
    "        print(\"Training Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"loss:{:.4f}\".format(np.mean(loss_batch)),\n",
    "              \"acc:{:.4f}\".format(np.mean(acc_batch)))\n",
    "\n",
    "        # Store at each epoch/iteration\n",
    "        train_loss.append(np.mean(loss_batch))\n",
    "        train_acc.append(np.mean(acc_batch))\n",
    "        \n",
    "        # Validation: Loop over batches\n",
    "        state = sess.run(initial_state)\n",
    "        acc_batch, loss_batch = [], []\n",
    "        for Xbatch, Ybatch in get_batches(Xvalid, Yvalid, batch_size):\n",
    "\n",
    "            # Feed dictionary\n",
    "            feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "            loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict)\n",
    "            acc_batch.append(acc)\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Print at each epoch/iteration\n",
    "        print(\"Validation Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"loss:{:.4f}\".format(np.mean(loss_batch)),\n",
    "              \"acc:{:.4f}\".format(np.mean(acc_batch)))\n",
    "\n",
    "        # Store at each epoch/iteration\n",
    "        valid_loss.append(np.mean(loss_batch))\n",
    "        valid_acc.append(np.mean(acc_batch))\n",
    "            \n",
    "    saver.save(sess,\"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(np.array(train_loss), 'r-', np.array(valid_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.plot(np.array(train_acc), 'r-', valid_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "Epoch: 1000/1000 Test loss: 0.39789098501205444 Test acc: 0.6996946334838867\n"
     ]
    }
   ],
   "source": [
    "loss, acc = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "\n",
    "    # Testing: Loop over batches\n",
    "    state = sess.run(initial_state)\n",
    "    acc_batch, loss_batch = [], []\n",
    "    for Xbatch, Ybatch in get_batches(Xtest, Ytest, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed_dict = {inputs_: Xbatch, indices_: Ybatch, initial_state: state}\n",
    "        loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Print at each epoch/iteration\n",
    "    print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "          \"Test loss: {}\".format(np.mean(loss_batch)),\n",
    "          \"Test acc: {}\".format(np.mean(acc_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
