{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q-GAN\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    if done:\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current and next states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Previous and current actions given\n",
    "    prev_actions = tf.placeholder(tf.int32, [None], name='prev_actions')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    \n",
    "    # Qs = qs+ (gamma * nextQs)\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs') # masked\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return prev_actions, states, actions, next_states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/predicting action and next states\n",
    "def generator(prev_actions, states, action_size, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=(action_size + state_size))\n",
    "        actions_logits, next_states_logits = tf.split(axis=1, num_or_size_splits=[action_size, state_size], \n",
    "                                                      value=logits)\n",
    "        #predictions = tf.nn.softmax(actions_logits)\n",
    "        #predictions = tf.sigmoid(next_states_logits)\n",
    "\n",
    "        # return actions and states logits\n",
    "        return actions_logits, next_states_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(prev_actions, states, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        #print(x_fused.shape)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        #print(h1.shape)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        #print(h2.shape)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return reward logits/Qs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of generator based on the generated/predicted states and actions\n",
    "def model_output(actions, next_states, \n",
    "                 state_size, action_size, hidden_size):\n",
    "    # Generator for nextQs as targetQs\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size)\n",
    "    next_actions_logits, _ = generator(prev_actions=actions_onehot, states=next_states,\n",
    "                                                   hidden_size=hidden_size, state_size=state_size, \n",
    "                                                   action_size=action_size)\n",
    "    nextQs_unmasked = next_actions_logits\n",
    "    return nextQs_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of generator based on the generated/predicted states and actions\n",
    "def model_loss(prev_actions, states, actions, next_states, targetQs, \n",
    "               state_size, action_size, hidden_size):\n",
    "    # Generator actions and next states\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    actions_logits, next_states_logits = generator(prev_actions=prev_actions_onehot, states=states,\n",
    "                                                   hidden_size=hidden_size, state_size=state_size, \n",
    "                                                   action_size=action_size, reuse=True)\n",
    "    \n",
    "    # Discriminator for nextQs_fake\n",
    "    #actions_predictions = tf.nn.softmax(actions_logits)\n",
    "    nextQs_fake = discriminator(prev_actions=actions_logits, states=next_states_logits,\n",
    "                                hidden_size=hidden_size)\n",
    "    \n",
    "    # Discriminator for nextQs_real\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size)\n",
    "    nextQs_real = discriminator(prev_actions=actions_onehot, states=next_states,\n",
    "                                hidden_size=hidden_size, reuse=True)\n",
    "    \n",
    "    # Reshape targetQs from (?, 1) to (?,)\n",
    "    #targetQs = tf.reshape(targetQs, [-1])\n",
    "    nextQs_fake = tf.reshape(nextQs_fake, [-1])\n",
    "    nextQs_real = tf.reshape(nextQs_real, [-1])\n",
    "    \n",
    "    # Adverserial learning/training\n",
    "    g_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=nextQs_real, \n",
    "                                                                         labels=tf.zeros_like(targetQs)))\n",
    "    g_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=nextQs_fake,\n",
    "                                                                         labels=tf.sigmoid(targetQs)))\n",
    "    # #g_loss_real = tf.reduce_mean(tf.square(nextQs_real - tf.zeros_like(targetQs)))\n",
    "    # g_loss_real = tf.reduce_mean(tf.square(nextQs_real))\n",
    "    # g_loss_fake = tf.reduce_mean(tf.square(nextQs_fake - targetQs))\n",
    "    g_loss = g_loss_real + g_loss_fake\n",
    "\n",
    "    # Adverserial learning/training\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=nextQs_real, \n",
    "                                                                         labels=tf.sigmoid(targetQs)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=nextQs_fake,\n",
    "                                                                         labels=tf.zeros_like(targetQs)))\n",
    "    # d_loss_real = tf.reduce_mean(tf.square(nextQs_real - targetQs))\n",
    "    # #d_loss_fake = tf.reduce_mean(tf.square(nextQs_fake - tf.zeros_like(targetQs)))\n",
    "    # d_loss_fake = tf.reduce_mean(tf.square(nextQs_fake))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "    # Returning the D loss\n",
    "    return actions_logits, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param g_loss: Generator loss for next state and action prediction\n",
    "    :param d_loss: Discriminator loss for reward prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Used for BN (batchnorm params)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars) # action prediction\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars) # reward prediction\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        ####################################### Model data inputs/outputs #######################################\n",
    "        # Input of the Model: make the data available inside the framework\n",
    "        self.prev_actions, self.states, self.actions, self.next_states, self.targetQs = model_input(\n",
    "            state_size=state_size)\n",
    "        \n",
    "        ######################################## Model losses #####################################################\n",
    "        # Loss of the Model: action prediction/generation\n",
    "        self.nextQs = model_output(actions=self.actions, next_states=self.next_states, \n",
    "                                   state_size=state_size, action_size=action_size, hidden_size=hidden_size)\n",
    "        \n",
    "        # NOTE: Qs will be outputed and targetQs are the Qs used as labels\n",
    "        self.actions_logits, self.g_loss, self.d_loss = model_loss(targetQs=self.targetQs, \n",
    "                                                                   prev_actions=self.prev_actions, \n",
    "                                                                   states=self.states, actions=self.actions,\n",
    "                                                                   next_states=self.next_states,\n",
    "                                                                   state_size=state_size, action_size=action_size, \n",
    "                                                                   hidden_size=hidden_size)\n",
    "\n",
    "        ######################################## Model updates #####################################################\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(g_loss=self.g_loss, \n",
    "                                           d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "prev_action = env.action_space.sample() # At-1\n",
    "state, _, done, _ = env.step(prev_action) # St, Rt/Et (Epiosde)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()# At\n",
    "    next_state, _, done, _ = env.step(action) #St+1\n",
    "\n",
    "    # End of the episodes which defines the goal of the episode/mission\n",
    "    if done is True:\n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        prev_action = env.action_space.sample()\n",
    "        state, _, done, _ = env.step(prev_action)\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Prepare for the next round\n",
    "        prev_action = action\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 1 Training g_loss: 0.0000 Training d_loss: 0.0000 Explore P: 0.9999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 12 Training g_loss: 1.2951 Training d_loss: 1.3078 Explore P: 0.9987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 20 Training g_loss: 1.3713 Training d_loss: 1.2427 Explore P: 0.9967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 21 Training g_loss: 1.6487 Training d_loss: 1.1979 Explore P: 0.9947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 21 Training g_loss: 1.7783 Training d_loss: 1.3814 Explore P: 0.9926\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 16 Training g_loss: 1.4950 Training d_loss: 1.4530 Explore P: 0.9910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 10 Training g_loss: 1.4440 Training d_loss: 1.5721 Explore P: 0.9901\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 10 Training g_loss: 1.2864 Training d_loss: 1.2436 Explore P: 0.9891\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 21 Training g_loss: 0.8085 Training d_loss: 0.6480 Explore P: 0.9870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 33 Training g_loss: 1.4391 Training d_loss: 1.2685 Explore P: 0.9838\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 10 Training g_loss: 1.2313 Training d_loss: 1.3980 Explore P: 0.9828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 9 Training g_loss: 1.3128 Training d_loss: 1.4490 Explore P: 0.9820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 28 Training g_loss: 1.3889 Training d_loss: 1.3803 Explore P: 0.9792\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 21 Training g_loss: 1.3173 Training d_loss: 1.2094 Explore P: 0.9772\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 27 Training g_loss: 1.3739 Training d_loss: 1.3611 Explore P: 0.9746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 40 Training g_loss: 1.2553 Training d_loss: 1.2745 Explore P: 0.9707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 12 Training g_loss: 1.2371 Training d_loss: 1.2600 Explore P: 0.9696\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 11 Training g_loss: 1.2408 Training d_loss: 1.2222 Explore P: 0.9685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 15 Training g_loss: 1.3034 Training d_loss: 1.1194 Explore P: 0.9671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 68 Training g_loss: 1.3958 Training d_loss: 1.3373 Explore P: 0.9606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 30 Training g_loss: 1.3737 Training d_loss: 1.2943 Explore P: 0.9578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 11 Training g_loss: 1.4062 Training d_loss: 1.3674 Explore P: 0.9567\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 11 Training g_loss: 1.4707 Training d_loss: 1.2717 Explore P: 0.9557\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 20 Training g_loss: 1.4552 Training d_loss: 1.1102 Explore P: 0.9538\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 12 Training g_loss: 1.5063 Training d_loss: 1.2299 Explore P: 0.9527\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 34 Training g_loss: 1.6859 Training d_loss: 1.1080 Explore P: 0.9495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 10 Training g_loss: 1.6629 Training d_loss: 0.9973 Explore P: 0.9485\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 20 Training g_loss: 1.5374 Training d_loss: 1.1911 Explore P: 0.9466\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 13 Training g_loss: 2.0312 Training d_loss: 1.1980 Explore P: 0.9454\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 24 Training g_loss: 2.4133 Training d_loss: 1.1017 Explore P: 0.9432\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 10 Training g_loss: 2.0130 Training d_loss: 1.2222 Explore P: 0.9423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 17 Training g_loss: 1.7753 Training d_loss: 1.1666 Explore P: 0.9407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 14 Training g_loss: 1.5630 Training d_loss: 1.2128 Explore P: 0.9394\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 30 Training g_loss: 2.0727 Training d_loss: 1.1519 Explore P: 0.9366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 20 Training g_loss: 2.0821 Training d_loss: 1.1131 Explore P: 0.9347\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 19 Training g_loss: 1.7351 Training d_loss: 1.3982 Explore P: 0.9330\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 14 Training g_loss: 1.4779 Training d_loss: 1.5615 Explore P: 0.9317\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 19 Training g_loss: 1.3782 Training d_loss: 1.3390 Explore P: 0.9299\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 23 Training g_loss: 1.4569 Training d_loss: 1.2474 Explore P: 0.9278\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 10 Training g_loss: 1.3816 Training d_loss: 1.3251 Explore P: 0.9269\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 25 Training g_loss: 1.4471 Training d_loss: 1.2583 Explore P: 0.9246\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 12 Training g_loss: 1.4371 Training d_loss: 1.2412 Explore P: 0.9235\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 17 Training g_loss: 1.5654 Training d_loss: 1.2948 Explore P: 0.9220\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 24 Training g_loss: 1.6211 Training d_loss: 1.1959 Explore P: 0.9198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 13 Training g_loss: 1.6292 Training d_loss: 1.1320 Explore P: 0.9186\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 10 Training g_loss: 1.6462 Training d_loss: 1.1063 Explore P: 0.9177\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 41 Training g_loss: 1.7121 Training d_loss: 1.0676 Explore P: 0.9140\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 25 Training g_loss: 1.4684 Training d_loss: 1.1541 Explore P: 0.9117\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 39 Training g_loss: 1.7681 Training d_loss: 1.2430 Explore P: 0.9082\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 20 Training g_loss: 1.6191 Training d_loss: 1.1472 Explore P: 0.9064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 20 Training g_loss: 1.7470 Training d_loss: 1.1594 Explore P: 0.9046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 47 Training g_loss: 1.8785 Training d_loss: 1.1061 Explore P: 0.9004\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 17 Training g_loss: 1.8521 Training d_loss: 1.2026 Explore P: 0.8989\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 21 Training g_loss: 1.8461 Training d_loss: 1.1328 Explore P: 0.8971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 14 Training g_loss: 1.6607 Training d_loss: 1.2239 Explore P: 0.8958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 31 Training g_loss: 1.5720 Training d_loss: 1.3221 Explore P: 0.8931\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 26 Training g_loss: 1.6543 Training d_loss: 1.2564 Explore P: 0.8908\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 14 Training g_loss: 1.8199 Training d_loss: 1.2460 Explore P: 0.8895\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 34 Training g_loss: 1.3146 Training d_loss: 1.4518 Explore P: 0.8866\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 23 Training g_loss: 1.2809 Training d_loss: 1.3216 Explore P: 0.8845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 13 Training g_loss: 1.4785 Training d_loss: 1.1635 Explore P: 0.8834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 14 Training g_loss: 1.3848 Training d_loss: 1.2100 Explore P: 0.8822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 15 Training g_loss: 1.3774 Training d_loss: 1.2516 Explore P: 0.8809\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 13 Training g_loss: 1.4180 Training d_loss: 1.3027 Explore P: 0.8797\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 66 Training g_loss: 1.4134 Training d_loss: 1.2375 Explore P: 0.8740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 37 Training g_loss: 1.8825 Training d_loss: 1.0094 Explore P: 0.8708\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 18 Training g_loss: 1.7341 Training d_loss: 1.0658 Explore P: 0.8693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 11 Training g_loss: 1.8415 Training d_loss: 1.0446 Explore P: 0.8683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 39 Training g_loss: 1.7427 Training d_loss: 1.1546 Explore P: 0.8650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 25 Training g_loss: 1.5990 Training d_loss: 1.2553 Explore P: 0.8629\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 23 Training g_loss: 2.0100 Training d_loss: 1.1858 Explore P: 0.8609\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 20 Training g_loss: 2.2606 Training d_loss: 1.1846 Explore P: 0.8592\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 12 Training g_loss: 1.8898 Training d_loss: 1.2470 Explore P: 0.8582\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 16 Training g_loss: 1.6389 Training d_loss: 1.1988 Explore P: 0.8568\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 16 Training g_loss: 1.5956 Training d_loss: 1.2011 Explore P: 0.8555\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 13 Training g_loss: 1.5423 Training d_loss: 1.2290 Explore P: 0.8544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 9 Training g_loss: 1.5714 Training d_loss: 1.2231 Explore P: 0.8536\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 12 Training g_loss: 1.5548 Training d_loss: 1.2095 Explore P: 0.8526\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 37 Training g_loss: 1.5587 Training d_loss: 1.0851 Explore P: 0.8495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 11 Training g_loss: 1.5979 Training d_loss: 1.1234 Explore P: 0.8486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 12 Training g_loss: 1.5887 Training d_loss: 1.1961 Explore P: 0.8476\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 15 Training g_loss: 1.7119 Training d_loss: 1.1136 Explore P: 0.8463\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 10 Training g_loss: 1.5957 Training d_loss: 1.1051 Explore P: 0.8455\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 9 Training g_loss: 1.6324 Training d_loss: 1.1395 Explore P: 0.8447\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 23 Training g_loss: 1.5671 Training d_loss: 1.2490 Explore P: 0.8428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 7 Training g_loss: 1.5412 Training d_loss: 1.1962 Explore P: 0.8422\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 14 Training g_loss: 1.6694 Training d_loss: 1.1359 Explore P: 0.8411\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 16 Training g_loss: 1.6803 Training d_loss: 1.1000 Explore P: 0.8397\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 16 Training g_loss: 1.5404 Training d_loss: 1.1966 Explore P: 0.8384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 13 Training g_loss: 1.6125 Training d_loss: 1.1253 Explore P: 0.8373\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 20 Training g_loss: 1.6731 Training d_loss: 1.1608 Explore P: 0.8357\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 57 Training g_loss: 1.6808 Training d_loss: 1.1430 Explore P: 0.8310\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 9 Training g_loss: 1.9021 Training d_loss: 1.0605 Explore P: 0.8302\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 12 Training g_loss: 1.8417 Training d_loss: 1.0679 Explore P: 0.8293\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 31 Training g_loss: 1.6073 Training d_loss: 1.1983 Explore P: 0.8267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 28 Training g_loss: 1.6948 Training d_loss: 1.0975 Explore P: 0.8244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 17 Training g_loss: 2.0845 Training d_loss: 1.0614 Explore P: 0.8231\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 13 Training g_loss: 1.8114 Training d_loss: 1.2230 Explore P: 0.8220\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 15 Training g_loss: 1.6096 Training d_loss: 1.2894 Explore P: 0.8208\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 15 Training g_loss: 1.6015 Training d_loss: 1.0963 Explore P: 0.8196\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 21 Training g_loss: 1.6163 Training d_loss: 1.2465 Explore P: 0.8179\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 15 Training g_loss: 1.6170 Training d_loss: 1.2352 Explore P: 0.8167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 12 Training g_loss: 1.4658 Training d_loss: 1.3005 Explore P: 0.8157\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 13 Training g_loss: 1.7445 Training d_loss: 1.1093 Explore P: 0.8146\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 20 Training g_loss: 1.7618 Training d_loss: 1.1721 Explore P: 0.8130\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 34 Training g_loss: 1.5557 Training d_loss: 1.2768 Explore P: 0.8103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 16 Training g_loss: 1.6136 Training d_loss: 1.2003 Explore P: 0.8090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 20 Training g_loss: 1.5395 Training d_loss: 1.1972 Explore P: 0.8074\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 11 Training g_loss: 1.7168 Training d_loss: 1.1797 Explore P: 0.8066\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 15 Training g_loss: 1.4571 Training d_loss: 1.3233 Explore P: 0.8054\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 12 Training g_loss: 1.5807 Training d_loss: 1.2251 Explore P: 0.8044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 10 Training g_loss: 1.5672 Training d_loss: 1.2324 Explore P: 0.8036\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 22 Training g_loss: 1.7045 Training d_loss: 1.1509 Explore P: 0.8019\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 17 Training g_loss: 1.5102 Training d_loss: 1.2128 Explore P: 0.8005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 10 Training g_loss: 1.5987 Training d_loss: 1.2304 Explore P: 0.7997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 12 Training g_loss: 1.5962 Training d_loss: 1.3170 Explore P: 0.7988\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 16 Training g_loss: 1.6505 Training d_loss: 1.2166 Explore P: 0.7975\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 23 Training g_loss: 1.6823 Training d_loss: 1.1197 Explore P: 0.7957\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 14 Training g_loss: 1.5044 Training d_loss: 1.3510 Explore P: 0.7946\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 17 Training g_loss: 1.4664 Training d_loss: 1.3101 Explore P: 0.7933\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 18 Training g_loss: 1.8532 Training d_loss: 1.0504 Explore P: 0.7919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 17 Training g_loss: 1.7067 Training d_loss: 1.1811 Explore P: 0.7906\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 23 Training g_loss: 1.4052 Training d_loss: 1.3843 Explore P: 0.7888\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 14 Training g_loss: 1.4894 Training d_loss: 1.3593 Explore P: 0.7877\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 16 Training g_loss: 1.5196 Training d_loss: 1.2090 Explore P: 0.7864\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 10 Training g_loss: 1.5851 Training d_loss: 1.1793 Explore P: 0.7857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 11 Training g_loss: 1.5676 Training d_loss: 1.1887 Explore P: 0.7848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 9 Training g_loss: 1.6451 Training d_loss: 1.1616 Explore P: 0.7841\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 21 Training g_loss: 1.6293 Training d_loss: 1.2096 Explore P: 0.7825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 20 Training g_loss: 1.4911 Training d_loss: 1.3399 Explore P: 0.7809\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 25 Training g_loss: 1.4938 Training d_loss: 1.3024 Explore P: 0.7790\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 14 Training g_loss: 1.6061 Training d_loss: 1.1893 Explore P: 0.7779\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 12 Training g_loss: 1.6286 Training d_loss: 1.1716 Explore P: 0.7770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 12 Training g_loss: 1.5278 Training d_loss: 1.2279 Explore P: 0.7761\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 17 Training g_loss: 1.5079 Training d_loss: 1.2234 Explore P: 0.7748\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 12 Training g_loss: 1.5368 Training d_loss: 1.1988 Explore P: 0.7739\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 15 Training g_loss: 1.5534 Training d_loss: 1.2494 Explore P: 0.7727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 19 Training g_loss: 1.5383 Training d_loss: 1.2755 Explore P: 0.7713\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 8 Training g_loss: 1.4254 Training d_loss: 1.3538 Explore P: 0.7707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 13 Training g_loss: 1.4185 Training d_loss: 1.3349 Explore P: 0.7697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 18 Training g_loss: 1.5020 Training d_loss: 1.3007 Explore P: 0.7683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 14 Training g_loss: 1.6334 Training d_loss: 1.1824 Explore P: 0.7673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 30 Training g_loss: 1.5155 Training d_loss: 1.2450 Explore P: 0.7650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 20 Training g_loss: 1.5443 Training d_loss: 1.2249 Explore P: 0.7635\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 21 Training g_loss: 1.4842 Training d_loss: 1.3232 Explore P: 0.7619\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 10 Training g_loss: 1.5925 Training d_loss: 1.2048 Explore P: 0.7611\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 9 Training g_loss: 1.5878 Training d_loss: 1.1733 Explore P: 0.7605\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 10 Training g_loss: 1.5383 Training d_loss: 1.2130 Explore P: 0.7597\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 63 Training g_loss: 1.5711 Training d_loss: 1.2282 Explore P: 0.7550\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 14 Training g_loss: 1.5596 Training d_loss: 1.1780 Explore P: 0.7540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 15 Training g_loss: 1.4674 Training d_loss: 1.2990 Explore P: 0.7529\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 12 Training g_loss: 1.5099 Training d_loss: 1.3122 Explore P: 0.7520\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 12 Training g_loss: 1.4828 Training d_loss: 1.2206 Explore P: 0.7511\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 18 Training g_loss: 1.4878 Training d_loss: 1.3022 Explore P: 0.7497\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 10 Training g_loss: 1.5142 Training d_loss: 1.2609 Explore P: 0.7490\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 11 Training g_loss: 1.6649 Training d_loss: 1.1569 Explore P: 0.7482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 14 Training g_loss: 1.4593 Training d_loss: 1.2797 Explore P: 0.7472\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 16 Training g_loss: 1.4410 Training d_loss: 1.2456 Explore P: 0.7460\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 10 Training g_loss: 1.5030 Training d_loss: 1.1603 Explore P: 0.7452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 17 Training g_loss: 1.4610 Training d_loss: 1.2971 Explore P: 0.7440\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 15 Training g_loss: 1.6718 Training d_loss: 1.1677 Explore P: 0.7429\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 11 Training g_loss: 1.4893 Training d_loss: 1.2576 Explore P: 0.7421\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 10 Training g_loss: 1.4132 Training d_loss: 1.2571 Explore P: 0.7414\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 13 Training g_loss: 1.4926 Training d_loss: 1.1875 Explore P: 0.7404\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 16 Training g_loss: 1.5128 Training d_loss: 1.2626 Explore P: 0.7392\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 11 Training g_loss: 1.4961 Training d_loss: 1.2669 Explore P: 0.7384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 9 Training g_loss: 1.5865 Training d_loss: 1.2532 Explore P: 0.7378\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 29 Training g_loss: 1.3705 Training d_loss: 1.2922 Explore P: 0.7357\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 9 Training g_loss: 1.4544 Training d_loss: 1.2235 Explore P: 0.7350\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 16 Training g_loss: 1.5689 Training d_loss: 1.1819 Explore P: 0.7339\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 23 Training g_loss: 1.4273 Training d_loss: 1.3681 Explore P: 0.7322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 10 Training g_loss: 1.6679 Training d_loss: 1.1925 Explore P: 0.7315\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 15 Training g_loss: 1.5970 Training d_loss: 1.2074 Explore P: 0.7304\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 21 Training g_loss: 1.4057 Training d_loss: 1.2580 Explore P: 0.7289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 10 Training g_loss: 1.5605 Training d_loss: 1.1649 Explore P: 0.7282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 15 Training g_loss: 1.5718 Training d_loss: 1.1634 Explore P: 0.7271\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 17 Training g_loss: 1.4808 Training d_loss: 1.3290 Explore P: 0.7259\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 13 Training g_loss: 1.6674 Training d_loss: 1.1795 Explore P: 0.7249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 9 Training g_loss: 1.5833 Training d_loss: 1.2110 Explore P: 0.7243\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 11 Training g_loss: 1.4079 Training d_loss: 1.3246 Explore P: 0.7235\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 17 Training g_loss: 1.5359 Training d_loss: 1.1665 Explore P: 0.7223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 15 Training g_loss: 1.5537 Training d_loss: 1.2678 Explore P: 0.7212\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 14 Training g_loss: 1.4912 Training d_loss: 1.2988 Explore P: 0.7202\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 14 Training g_loss: 1.6320 Training d_loss: 1.1848 Explore P: 0.7192\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 22 Training g_loss: 1.4314 Training d_loss: 1.2555 Explore P: 0.7177\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 16 Training g_loss: 1.5566 Training d_loss: 1.1537 Explore P: 0.7166\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 10 Training g_loss: 1.5144 Training d_loss: 1.2409 Explore P: 0.7159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 21 Training g_loss: 1.5014 Training d_loss: 1.2716 Explore P: 0.7144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 18 Training g_loss: 1.5935 Training d_loss: 1.1935 Explore P: 0.7131\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 8 Training g_loss: 1.5395 Training d_loss: 1.2010 Explore P: 0.7125\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 7 Training g_loss: 1.4420 Training d_loss: 1.2473 Explore P: 0.7121\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 8 Training g_loss: 1.4521 Training d_loss: 1.2445 Explore P: 0.7115\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 9 Training g_loss: 1.5677 Training d_loss: 1.1772 Explore P: 0.7109\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 10 Training g_loss: 1.4883 Training d_loss: 1.2224 Explore P: 0.7102\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 9 Training g_loss: 1.4517 Training d_loss: 1.2967 Explore P: 0.7095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 11 Training g_loss: 1.5235 Training d_loss: 1.2852 Explore P: 0.7088\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 10 Training g_loss: 1.5198 Training d_loss: 1.2851 Explore P: 0.7081\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 12 Training g_loss: 1.6767 Training d_loss: 1.1421 Explore P: 0.7072\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 23 Training g_loss: 1.4677 Training d_loss: 1.2710 Explore P: 0.7056\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 17 Training g_loss: 1.5911 Training d_loss: 1.1678 Explore P: 0.7044\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 17 Training g_loss: 1.5182 Training d_loss: 1.2538 Explore P: 0.7033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 17 Training g_loss: 1.6627 Training d_loss: 1.1733 Explore P: 0.7021\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 12 Training g_loss: 1.5643 Training d_loss: 1.1933 Explore P: 0.7013\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 11 Training g_loss: 1.4639 Training d_loss: 1.2704 Explore P: 0.7005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 9 Training g_loss: 1.4569 Training d_loss: 1.2356 Explore P: 0.6999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 11 Training g_loss: 1.6200 Training d_loss: 1.1968 Explore P: 0.6991\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 30 Training g_loss: 1.5393 Training d_loss: 1.2893 Explore P: 0.6970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 8 Training g_loss: 1.5005 Training d_loss: 1.2969 Explore P: 0.6965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 36 Training g_loss: 1.4795 Training d_loss: 1.2748 Explore P: 0.6940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 10 Training g_loss: 1.4211 Training d_loss: 1.4044 Explore P: 0.6933\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 15 Training g_loss: 1.5518 Training d_loss: 1.2127 Explore P: 0.6923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 12 Training g_loss: 1.7290 Training d_loss: 1.1294 Explore P: 0.6915\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 7 Training g_loss: 1.8372 Training d_loss: 1.0883 Explore P: 0.6910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 17 Training g_loss: 2.0542 Training d_loss: 1.0374 Explore P: 0.6899\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 19 Training g_loss: 1.4673 Training d_loss: 1.5216 Explore P: 0.6886\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 28 Training g_loss: 1.6036 Training d_loss: 1.2674 Explore P: 0.6867\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 17 Training g_loss: 1.9381 Training d_loss: 1.0427 Explore P: 0.6855\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 9 Training g_loss: 1.8769 Training d_loss: 1.0926 Explore P: 0.6849\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 11 Training g_loss: 1.6214 Training d_loss: 1.1965 Explore P: 0.6842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 8 Training g_loss: 1.4348 Training d_loss: 1.4128 Explore P: 0.6836\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 9 Training g_loss: 1.3423 Training d_loss: 1.5210 Explore P: 0.6830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 8 Training g_loss: 1.4300 Training d_loss: 1.4837 Explore P: 0.6825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 8 Training g_loss: 1.6049 Training d_loss: 1.3003 Explore P: 0.6820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 12 Training g_loss: 2.2789 Training d_loss: 1.0217 Explore P: 0.6812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 12 Training g_loss: 1.7825 Training d_loss: 1.1522 Explore P: 0.6804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 12 Training g_loss: 1.4141 Training d_loss: 1.4063 Explore P: 0.6795\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 7 Training g_loss: 1.4121 Training d_loss: 1.5455 Explore P: 0.6791\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 10 Training g_loss: 1.4590 Training d_loss: 1.3409 Explore P: 0.6784\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 25 Training g_loss: 1.5372 Training d_loss: 1.2179 Explore P: 0.6767\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 13 Training g_loss: 1.5215 Training d_loss: 1.2382 Explore P: 0.6759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 20 Training g_loss: 1.5385 Training d_loss: 1.2487 Explore P: 0.6745\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 14 Training g_loss: 1.4686 Training d_loss: 1.2709 Explore P: 0.6736\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 15 Training g_loss: 1.4844 Training d_loss: 1.2542 Explore P: 0.6726\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 19 Training g_loss: 1.4806 Training d_loss: 1.2562 Explore P: 0.6714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 11 Training g_loss: 1.4912 Training d_loss: 1.2578 Explore P: 0.6706\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 15 Training g_loss: 1.4842 Training d_loss: 1.2430 Explore P: 0.6696\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 10 Training g_loss: 1.4631 Training d_loss: 1.2585 Explore P: 0.6690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 18 Training g_loss: 1.4850 Training d_loss: 1.2640 Explore P: 0.6678\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 13 Training g_loss: 1.4543 Training d_loss: 1.2621 Explore P: 0.6669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 14 Training g_loss: 1.4512 Training d_loss: 1.2783 Explore P: 0.6660\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 10 Training g_loss: 1.4195 Training d_loss: 1.2950 Explore P: 0.6654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 11 Training g_loss: 1.4441 Training d_loss: 1.2582 Explore P: 0.6647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 23 Training g_loss: 1.4649 Training d_loss: 1.2629 Explore P: 0.6631\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 12 Training g_loss: 1.4798 Training d_loss: 1.2578 Explore P: 0.6624\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 17 Training g_loss: 1.4734 Training d_loss: 1.2549 Explore P: 0.6613\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 12 Training g_loss: 1.4725 Training d_loss: 1.2541 Explore P: 0.6605\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 20 Training g_loss: 1.4066 Training d_loss: 1.3034 Explore P: 0.6592\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 10 Training g_loss: 1.4047 Training d_loss: 1.2807 Explore P: 0.6585\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 12 Training g_loss: 1.4610 Training d_loss: 1.2424 Explore P: 0.6578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 14 Training g_loss: 1.4777 Training d_loss: 1.2464 Explore P: 0.6568\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 8 Training g_loss: 1.4876 Training d_loss: 1.2481 Explore P: 0.6563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 14 Training g_loss: 1.4666 Training d_loss: 1.2836 Explore P: 0.6554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 21 Training g_loss: 1.4784 Training d_loss: 1.2711 Explore P: 0.6541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 11 Training g_loss: 1.4953 Training d_loss: 1.2621 Explore P: 0.6534\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 12 Training g_loss: 1.4934 Training d_loss: 1.2270 Explore P: 0.6526\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 13 Training g_loss: 1.4445 Training d_loss: 1.2401 Explore P: 0.6518\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 17 Training g_loss: 1.4548 Training d_loss: 1.2361 Explore P: 0.6507\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 18 Training g_loss: 1.4118 Training d_loss: 1.2632 Explore P: 0.6495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 10 Training g_loss: 1.4138 Training d_loss: 1.2925 Explore P: 0.6489\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 17 Training g_loss: 1.4378 Training d_loss: 1.2666 Explore P: 0.6478\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 13 Training g_loss: 1.5157 Training d_loss: 1.2153 Explore P: 0.6470\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 25 Training g_loss: 1.4977 Training d_loss: 1.2067 Explore P: 0.6454\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 11 Training g_loss: 1.4674 Training d_loss: 1.2525 Explore P: 0.6447\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 8 Training g_loss: 1.4585 Training d_loss: 1.2614 Explore P: 0.6442\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 11 Training g_loss: 1.3951 Training d_loss: 1.3029 Explore P: 0.6435\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 13 Training g_loss: 1.4331 Training d_loss: 1.2516 Explore P: 0.6426\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 18 Training g_loss: 1.4623 Training d_loss: 1.2680 Explore P: 0.6415\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 18 Training g_loss: 1.4634 Training d_loss: 1.2239 Explore P: 0.6404\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 8 Training g_loss: 1.4887 Training d_loss: 1.2278 Explore P: 0.6399\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 9 Training g_loss: 1.4684 Training d_loss: 1.2511 Explore P: 0.6393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 20 Training g_loss: 1.4557 Training d_loss: 1.2597 Explore P: 0.6380\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 19 Training g_loss: 1.4373 Training d_loss: 1.2432 Explore P: 0.6368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 17 Training g_loss: 1.4714 Training d_loss: 1.2438 Explore P: 0.6358\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 12 Training g_loss: 1.4441 Training d_loss: 1.2541 Explore P: 0.6350\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 14 Training g_loss: 1.4661 Training d_loss: 1.2476 Explore P: 0.6342\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 14 Training g_loss: 1.4360 Training d_loss: 1.2761 Explore P: 0.6333\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 18 Training g_loss: 1.4813 Training d_loss: 1.2513 Explore P: 0.6322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 12 Training g_loss: 1.4630 Training d_loss: 1.2782 Explore P: 0.6314\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 30 Training g_loss: 1.4634 Training d_loss: 1.2327 Explore P: 0.6296\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 10 Training g_loss: 1.4699 Training d_loss: 1.2305 Explore P: 0.6289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 14 Training g_loss: 1.4613 Training d_loss: 1.2376 Explore P: 0.6281\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 10 Training g_loss: 1.4588 Training d_loss: 1.2685 Explore P: 0.6275\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 16 Training g_loss: 1.4731 Training d_loss: 1.2460 Explore P: 0.6265\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 13 Training g_loss: 1.5165 Training d_loss: 1.2334 Explore P: 0.6257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 13 Training g_loss: 1.5121 Training d_loss: 1.2400 Explore P: 0.6249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 18 Training g_loss: 1.4271 Training d_loss: 1.2471 Explore P: 0.6238\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 8 Training g_loss: 1.4653 Training d_loss: 1.2163 Explore P: 0.6233\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 14 Training g_loss: 1.4686 Training d_loss: 1.2540 Explore P: 0.6224\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 11 Training g_loss: 1.4419 Training d_loss: 1.2550 Explore P: 0.6217\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 9 Training g_loss: 1.4426 Training d_loss: 1.2676 Explore P: 0.6212\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 22 Training g_loss: 1.4761 Training d_loss: 1.2634 Explore P: 0.6198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 9 Training g_loss: 1.4670 Training d_loss: 1.2643 Explore P: 0.6193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 14 Training g_loss: 1.5036 Training d_loss: 1.2363 Explore P: 0.6184\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 9 Training g_loss: 1.4810 Training d_loss: 1.2455 Explore P: 0.6179\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 9 Training g_loss: 1.4537 Training d_loss: 1.2482 Explore P: 0.6173\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 10 Training g_loss: 1.4403 Training d_loss: 1.2600 Explore P: 0.6167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 14 Training g_loss: 1.4651 Training d_loss: 1.2402 Explore P: 0.6159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 8 Training g_loss: 1.4780 Training d_loss: 1.2274 Explore P: 0.6154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 22 Training g_loss: 1.4547 Training d_loss: 1.2610 Explore P: 0.6141\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 10 Training g_loss: 1.4911 Training d_loss: 1.2354 Explore P: 0.6135\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 7 Training g_loss: 1.4959 Training d_loss: 1.2331 Explore P: 0.6131\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 14 Training g_loss: 1.4583 Training d_loss: 1.2531 Explore P: 0.6122\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 9 Training g_loss: 1.4805 Training d_loss: 1.2388 Explore P: 0.6117\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 11 Training g_loss: 1.4130 Training d_loss: 1.2820 Explore P: 0.6110\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 11 Training g_loss: 1.4660 Training d_loss: 1.2482 Explore P: 0.6103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 11 Training g_loss: 1.4498 Training d_loss: 1.2430 Explore P: 0.6097\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 12 Training g_loss: 1.5122 Training d_loss: 1.1972 Explore P: 0.6090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 14 Training g_loss: 1.4298 Training d_loss: 1.3051 Explore P: 0.6081\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 12 Training g_loss: 1.4916 Training d_loss: 1.2590 Explore P: 0.6074\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 9 Training g_loss: 1.4628 Training d_loss: 1.2842 Explore P: 0.6069\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 13 Training g_loss: 1.4837 Training d_loss: 1.2454 Explore P: 0.6061\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 15 Training g_loss: 1.4249 Training d_loss: 1.2710 Explore P: 0.6052\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 14 Training g_loss: 1.4225 Training d_loss: 1.2646 Explore P: 0.6044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 18 Training g_loss: 1.4954 Training d_loss: 1.1917 Explore P: 0.6033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 14 Training g_loss: 1.4680 Training d_loss: 1.2407 Explore P: 0.6025\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 12 Training g_loss: 1.4227 Training d_loss: 1.3175 Explore P: 0.6018\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 43 Training g_loss: 1.4189 Training d_loss: 1.2867 Explore P: 0.5992\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 20 Training g_loss: 1.5352 Training d_loss: 1.2490 Explore P: 0.5980\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 16 Training g_loss: 1.5170 Training d_loss: 1.1871 Explore P: 0.5971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 10 Training g_loss: 1.4678 Training d_loss: 1.2887 Explore P: 0.5965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 13 Training g_loss: 1.4281 Training d_loss: 1.3363 Explore P: 0.5958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 11 Training g_loss: 1.6135 Training d_loss: 1.1724 Explore P: 0.5951\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 11 Training g_loss: 1.6434 Training d_loss: 1.1653 Explore P: 0.5945\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 323 Total reward: 24 Training g_loss: 1.4017 Training d_loss: 1.3114 Explore P: 0.5931\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 324 Total reward: 11 Training g_loss: 1.5347 Training d_loss: 1.2410 Explore P: 0.5924\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 325 Total reward: 12 Training g_loss: 1.5197 Training d_loss: 1.1406 Explore P: 0.5917\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 326 Total reward: 12 Training g_loss: 1.5228 Training d_loss: 1.2163 Explore P: 0.5910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 327 Total reward: 12 Training g_loss: 1.3988 Training d_loss: 1.3969 Explore P: 0.5903\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 328 Total reward: 18 Training g_loss: 1.6575 Training d_loss: 1.1271 Explore P: 0.5893\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 329 Total reward: 15 Training g_loss: 1.4234 Training d_loss: 1.2718 Explore P: 0.5884\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 330 Total reward: 14 Training g_loss: 1.3756 Training d_loss: 1.3153 Explore P: 0.5876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 331 Total reward: 15 Training g_loss: 1.5059 Training d_loss: 1.1778 Explore P: 0.5867\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 332 Total reward: 13 Training g_loss: 1.5654 Training d_loss: 1.1688 Explore P: 0.5860\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 333 Total reward: 10 Training g_loss: 1.4315 Training d_loss: 1.2623 Explore P: 0.5854\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 334 Total reward: 19 Training g_loss: 1.5466 Training d_loss: 1.2619 Explore P: 0.5843\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 335 Total reward: 15 Training g_loss: 1.6882 Training d_loss: 1.1478 Explore P: 0.5835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 336 Total reward: 12 Training g_loss: 1.4275 Training d_loss: 1.2811 Explore P: 0.5828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 337 Total reward: 10 Training g_loss: 1.3716 Training d_loss: 1.2899 Explore P: 0.5822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 338 Total reward: 18 Training g_loss: 1.5885 Training d_loss: 1.2403 Explore P: 0.5812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 339 Total reward: 14 Training g_loss: 1.4653 Training d_loss: 1.3118 Explore P: 0.5804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 340 Total reward: 12 Training g_loss: 1.5258 Training d_loss: 1.2608 Explore P: 0.5797\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 341 Total reward: 11 Training g_loss: 1.6846 Training d_loss: 1.1466 Explore P: 0.5791\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 342 Total reward: 14 Training g_loss: 1.4745 Training d_loss: 1.2520 Explore P: 0.5783\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 343 Total reward: 10 Training g_loss: 1.4497 Training d_loss: 1.2810 Explore P: 0.5777\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 344 Total reward: 16 Training g_loss: 1.5123 Training d_loss: 1.1999 Explore P: 0.5768\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 345 Total reward: 11 Training g_loss: 1.5301 Training d_loss: 1.2276 Explore P: 0.5762\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 346 Total reward: 12 Training g_loss: 1.3677 Training d_loss: 1.3613 Explore P: 0.5755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 347 Total reward: 13 Training g_loss: 1.4661 Training d_loss: 1.3096 Explore P: 0.5748\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 348 Total reward: 27 Training g_loss: 1.4588 Training d_loss: 1.2526 Explore P: 0.5732\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 349 Total reward: 19 Training g_loss: 1.4592 Training d_loss: 1.2084 Explore P: 0.5722\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 350 Total reward: 13 Training g_loss: 1.5054 Training d_loss: 1.2104 Explore P: 0.5714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 351 Total reward: 15 Training g_loss: 1.4219 Training d_loss: 1.3099 Explore P: 0.5706\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 352 Total reward: 12 Training g_loss: 1.5509 Training d_loss: 1.2456 Explore P: 0.5699\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 353 Total reward: 10 Training g_loss: 1.5947 Training d_loss: 1.2045 Explore P: 0.5694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 354 Total reward: 10 Training g_loss: 1.5578 Training d_loss: 1.2082 Explore P: 0.5688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 355 Total reward: 13 Training g_loss: 1.4379 Training d_loss: 1.3024 Explore P: 0.5681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 356 Total reward: 10 Training g_loss: 1.5731 Training d_loss: 1.2133 Explore P: 0.5675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 357 Total reward: 18 Training g_loss: 1.4544 Training d_loss: 1.2627 Explore P: 0.5665\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 358 Total reward: 17 Training g_loss: 1.4556 Training d_loss: 1.3093 Explore P: 0.5656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 359 Total reward: 11 Training g_loss: 1.5483 Training d_loss: 1.2225 Explore P: 0.5650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 360 Total reward: 9 Training g_loss: 1.4942 Training d_loss: 1.2025 Explore P: 0.5645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 361 Total reward: 9 Training g_loss: 1.5323 Training d_loss: 1.2118 Explore P: 0.5640\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 362 Total reward: 13 Training g_loss: 1.4970 Training d_loss: 1.2727 Explore P: 0.5632\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 363 Total reward: 13 Training g_loss: 1.4212 Training d_loss: 1.2580 Explore P: 0.5625\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 364 Total reward: 24 Training g_loss: 1.4935 Training d_loss: 1.2689 Explore P: 0.5612\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 365 Total reward: 14 Training g_loss: 1.4834 Training d_loss: 1.2629 Explore P: 0.5604\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 366 Total reward: 18 Training g_loss: 1.4302 Training d_loss: 1.2893 Explore P: 0.5594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 367 Total reward: 10 Training g_loss: 1.4219 Training d_loss: 1.2850 Explore P: 0.5589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 368 Total reward: 9 Training g_loss: 1.4359 Training d_loss: 1.2511 Explore P: 0.5584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 369 Total reward: 15 Training g_loss: 1.5041 Training d_loss: 1.1999 Explore P: 0.5576\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 370 Total reward: 10 Training g_loss: 1.5320 Training d_loss: 1.2022 Explore P: 0.5570\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 371 Total reward: 19 Training g_loss: 1.3895 Training d_loss: 1.2630 Explore P: 0.5560\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 372 Total reward: 11 Training g_loss: 1.5172 Training d_loss: 1.2566 Explore P: 0.5554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 373 Total reward: 22 Training g_loss: 1.5501 Training d_loss: 1.2513 Explore P: 0.5542\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 374 Total reward: 9 Training g_loss: 1.4561 Training d_loss: 1.2495 Explore P: 0.5537\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 375 Total reward: 10 Training g_loss: 1.4524 Training d_loss: 1.2486 Explore P: 0.5532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 376 Total reward: 19 Training g_loss: 1.4825 Training d_loss: 1.2260 Explore P: 0.5521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 377 Total reward: 13 Training g_loss: 1.5044 Training d_loss: 1.2258 Explore P: 0.5514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 378 Total reward: 19 Training g_loss: 1.4467 Training d_loss: 1.3062 Explore P: 0.5504\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 379 Total reward: 20 Training g_loss: 1.5243 Training d_loss: 1.2544 Explore P: 0.5493\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 380 Total reward: 11 Training g_loss: 1.4312 Training d_loss: 1.3060 Explore P: 0.5487\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 381 Total reward: 12 Training g_loss: 1.5305 Training d_loss: 1.2178 Explore P: 0.5481\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 382 Total reward: 10 Training g_loss: 1.5109 Training d_loss: 1.1828 Explore P: 0.5475\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 383 Total reward: 12 Training g_loss: 1.5115 Training d_loss: 1.2592 Explore P: 0.5469\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 384 Total reward: 14 Training g_loss: 1.4408 Training d_loss: 1.3248 Explore P: 0.5461\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 385 Total reward: 12 Training g_loss: 1.4597 Training d_loss: 1.2945 Explore P: 0.5455\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 386 Total reward: 10 Training g_loss: 1.4997 Training d_loss: 1.2462 Explore P: 0.5450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 387 Total reward: 13 Training g_loss: 1.3859 Training d_loss: 1.3113 Explore P: 0.5443\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 388 Total reward: 10 Training g_loss: 1.5094 Training d_loss: 1.2307 Explore P: 0.5437\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 389 Total reward: 8 Training g_loss: 1.5191 Training d_loss: 1.2328 Explore P: 0.5433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 390 Total reward: 8 Training g_loss: 1.5270 Training d_loss: 1.2313 Explore P: 0.5429\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 391 Total reward: 25 Training g_loss: 1.4240 Training d_loss: 1.2521 Explore P: 0.5416\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 392 Total reward: 11 Training g_loss: 1.4609 Training d_loss: 1.3124 Explore P: 0.5410\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 393 Total reward: 12 Training g_loss: 1.4446 Training d_loss: 1.2958 Explore P: 0.5403\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 394 Total reward: 8 Training g_loss: 1.4085 Training d_loss: 1.2998 Explore P: 0.5399\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 395 Total reward: 16 Training g_loss: 1.4457 Training d_loss: 1.2364 Explore P: 0.5391\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 396 Total reward: 12 Training g_loss: 1.4243 Training d_loss: 1.2349 Explore P: 0.5384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 397 Total reward: 13 Training g_loss: 1.4938 Training d_loss: 1.2305 Explore P: 0.5377\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 398 Total reward: 8 Training g_loss: 1.4365 Training d_loss: 1.2665 Explore P: 0.5373\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 399 Total reward: 15 Training g_loss: 1.4325 Training d_loss: 1.3048 Explore P: 0.5365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 400 Total reward: 19 Training g_loss: 1.5205 Training d_loss: 1.2419 Explore P: 0.5355\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 401 Total reward: 13 Training g_loss: 1.4756 Training d_loss: 1.2575 Explore P: 0.5348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 402 Total reward: 12 Training g_loss: 1.4266 Training d_loss: 1.2902 Explore P: 0.5342\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 403 Total reward: 12 Training g_loss: 1.5507 Training d_loss: 1.2263 Explore P: 0.5336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 404 Total reward: 39 Training g_loss: 1.4579 Training d_loss: 1.2670 Explore P: 0.5315\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 405 Total reward: 7 Training g_loss: 1.4364 Training d_loss: 1.2650 Explore P: 0.5312\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 406 Total reward: 10 Training g_loss: 1.4599 Training d_loss: 1.2582 Explore P: 0.5307\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 407 Total reward: 11 Training g_loss: 1.3958 Training d_loss: 1.2971 Explore P: 0.5301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 408 Total reward: 12 Training g_loss: 1.4838 Training d_loss: 1.2374 Explore P: 0.5295\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 409 Total reward: 9 Training g_loss: 1.4772 Training d_loss: 1.2174 Explore P: 0.5290\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 410 Total reward: 16 Training g_loss: 1.4566 Training d_loss: 1.2324 Explore P: 0.5282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 411 Total reward: 11 Training g_loss: 1.4714 Training d_loss: 1.2333 Explore P: 0.5276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 412 Total reward: 12 Training g_loss: 1.5014 Training d_loss: 1.1959 Explore P: 0.5270\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 413 Total reward: 17 Training g_loss: 1.4712 Training d_loss: 1.2286 Explore P: 0.5261\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 414 Total reward: 20 Training g_loss: 1.4684 Training d_loss: 1.2802 Explore P: 0.5251\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 415 Total reward: 16 Training g_loss: 1.4506 Training d_loss: 1.2758 Explore P: 0.5242\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 416 Total reward: 14 Training g_loss: 1.4439 Training d_loss: 1.2783 Explore P: 0.5235\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 417 Total reward: 12 Training g_loss: 1.5131 Training d_loss: 1.1931 Explore P: 0.5229\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 418 Total reward: 12 Training g_loss: 1.7343 Training d_loss: 1.0912 Explore P: 0.5223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 419 Total reward: 10 Training g_loss: 1.6771 Training d_loss: 1.1488 Explore P: 0.5218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 420 Total reward: 9 Training g_loss: 1.4785 Training d_loss: 1.3151 Explore P: 0.5213\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 421 Total reward: 16 Training g_loss: 1.3513 Training d_loss: 1.4325 Explore P: 0.5205\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 422 Total reward: 25 Training g_loss: 1.6275 Training d_loss: 1.1483 Explore P: 0.5192\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 423 Total reward: 12 Training g_loss: 1.7778 Training d_loss: 1.0801 Explore P: 0.5186\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 424 Total reward: 13 Training g_loss: 1.6329 Training d_loss: 1.1582 Explore P: 0.5180\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 425 Total reward: 14 Training g_loss: 1.4235 Training d_loss: 1.3499 Explore P: 0.5173\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 426 Total reward: 7 Training g_loss: 1.3282 Training d_loss: 1.4505 Explore P: 0.5169\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 427 Total reward: 10 Training g_loss: 1.2607 Training d_loss: 1.5939 Explore P: 0.5164\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 428 Total reward: 21 Training g_loss: 1.8374 Training d_loss: 1.0805 Explore P: 0.5153\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 429 Total reward: 15 Training g_loss: 1.6552 Training d_loss: 1.1370 Explore P: 0.5146\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 430 Total reward: 13 Training g_loss: 1.5264 Training d_loss: 1.2294 Explore P: 0.5139\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 431 Total reward: 11 Training g_loss: 1.2601 Training d_loss: 1.5366 Explore P: 0.5134\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 432 Total reward: 11 Training g_loss: 1.5653 Training d_loss: 1.2572 Explore P: 0.5128\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 433 Total reward: 17 Training g_loss: 1.4636 Training d_loss: 1.2191 Explore P: 0.5120\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 434 Total reward: 11 Training g_loss: 1.3633 Training d_loss: 1.3209 Explore P: 0.5114\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 435 Total reward: 13 Training g_loss: 1.5206 Training d_loss: 1.2401 Explore P: 0.5107\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 436 Total reward: 14 Training g_loss: 1.4056 Training d_loss: 1.3066 Explore P: 0.5100\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list = []\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward = 0\n",
    "        g_loss = 0\n",
    "        d_loss = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                             model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits) # arg with max value/Q is the class of action\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            #total_reward += reward\n",
    "            total_reward += 1 # done=False\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done is True:\n",
    "                # the episode ends so no next state\n",
    "                #next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, _, done, _ = env.step(prev_action)\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # One step forward: At-1=At and St=St+1\n",
    "                prev_action = action\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            prev_actions = np.array([each[0] for each in batch])\n",
    "            states = np.array([each[1] for each in batch])\n",
    "            actions = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            \n",
    "            # Calculating nextQs and setting them to 0 for states where episode ends/fails\n",
    "            feed_dict={model.actions: actions, model.next_states: next_states}\n",
    "            nextQs = sess.run(model.nextQs, feed_dict)\n",
    "\n",
    "            # Masking for the end of episodes/ goals\n",
    "            dones_mask = (1 - dones.astype(nextQs[0].dtype)).reshape(-1, 1)\n",
    "            nextQs_masked = np.multiply(nextQs[0], dones_mask)\n",
    "            nextQs = np.max(nextQs_masked, axis=1)\n",
    "            #print(len(Qs), Qs.shape, np.reshape(Qs, [-1]).shape)\n",
    "            targetQs = nextQs\n",
    "\n",
    "            # Calculating nextQs for Discriminator using D(At-1, St)= Qt: NOT this one\n",
    "            feed_dict = {model.prev_actions: prev_actions, \n",
    "                         model.states: states, \n",
    "                         model.actions: actions, \n",
    "                         model.next_states: next_states, \n",
    "                         model.targetQs: targetQs}\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "                        \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "# env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from the model\n",
    "            feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                         model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, reward, done, _ = env.step(prev_action)\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the env\n",
    "# WARNING: If you close, you can NOT restart again!!!!!!\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
