{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q-GAN\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    if done:\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current and next states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Previous and current actions given\n",
    "    prev_actions = tf.placeholder(tf.int32, [None], name='prev_actions')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    \n",
    "    # End of episodes/goal/task where nextQs = 0 and Qs=rs\n",
    "    dones = tf.placeholder(tf.bool, [None], name='dones') # masked\n",
    "\n",
    "    # Qs = qs+ (gamma * nextQs)\n",
    "    nextQs = tf.placeholder(tf.float32, [None], name='nextQs') # masked\n",
    "    nextQs_D = tf.placeholder(tf.float32, [None], name='nextQs_D') # masked\n",
    "    nextnextQs_G = tf.placeholder(tf.float32, [None], name='nextQs_D') # masked\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return prev_actions, states, actions, next_states, dones, nextQs, nextQs_D, nextnextQs_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/predicting action and next states\n",
    "def generator(prev_actions, states, action_size, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=(action_size + state_size))\n",
    "        actions_logits, next_states_logits = tf.split(axis=1, num_or_size_splits=[action_size, state_size], \n",
    "                                                      value=logits)\n",
    "        #predictions = tf.nn.softmax(actions_logits)\n",
    "        #predictions = tf.sigmoid(next_states_logits)\n",
    "\n",
    "        # return actions and states logits\n",
    "        return actions_logits, next_states_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(prev_actions, states, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        #print(x_fused.shape)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        #print(h1.shape)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        #print(h2.shape)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return reward logits/Qs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model loss for predicted/generated actions\n",
    "def model_loss(prev_actions, states, actions, nextQs, # model input data for targetQs\n",
    "               state_size, action_size, hidden_size): # model init for Qs\n",
    "    # Calculating Qs total rewards\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    actions_logits, _ = generator(prev_actions=prev_actions_onehot, states=states, \n",
    "                                  hidden_size=hidden_size, state_size=state_size, action_size=action_size)\n",
    "    \n",
    "    # Masking actions_logits unmasked\n",
    "    actions_mask = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs_masked = tf.multiply(actions_logits, actions_mask)\n",
    "    Qs = tf.reduce_max(Qs_masked, axis=1)\n",
    "    \n",
    "    # # Bellman equaion for calculating total rewards using current reward + total future rewards/nextQs\n",
    "    # qs = tf.sigmoid(Qs) # qt\n",
    "    # targetQs = qs + (gamma * nextQs)\n",
    "    # # Calculating the loss: logits/predictions vs labels\n",
    "    # q_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    q_loss = tf.reduce_mean(tf.square(Qs - nextQs))\n",
    "\n",
    "    return actions_logits, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputing the unmasked nextQs for D to be used as the target/label\n",
    "def model_output(actions, next_states,\n",
    "                 action_size, hidden_size):\n",
    "    # Discriminator for nextQs_D\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size)\n",
    "    nextQs_D_unmasked = discriminator(prev_actions=actions_onehot, states=next_states, hidden_size=hidden_size)\n",
    "    \n",
    "    # Returning unmasked nextQs_D to masked using dones/ends of episodes\n",
    "    return nextQs_D_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model loss for the NEW idea G & D\n",
    "def model_loss2(nextQs_D, prev_actions, states, \n",
    "                action_size, hidden_size):\n",
    "    # Calculating Qs total rewards using Discriminator\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    Qs = discriminator(prev_actions=prev_actions_onehot, states=states, hidden_size=hidden_size, reuse=True)\n",
    "        \n",
    "    # # Bellman equaion: Qs = rt/qt + nextQs_G/D\n",
    "    # qs = tf.sigmoid(Qs) # qt\n",
    "    # targetQs_D = qs + (gamma * nextQs_D)\n",
    "    # Calculating the loss: logits/predictions vs labels\n",
    "    #d_loss = tf.reduce_mean(tf.square(Qs - targetQs_D))\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - nextQs_D))\n",
    "    \n",
    "    # Returning the D loss\n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of generator based on the generated/predicted states and actions\n",
    "def model_output2(actions, next_states, \n",
    "                  state_size, action_size, hidden_size):\n",
    "    # Generator for nextnextQs_G\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size)\n",
    "    next_actions_logits, nextnext_states_logits = generator(prev_actions=actions_onehot, states=next_states,\n",
    "                                                            hidden_size=hidden_size, state_size=state_size, \n",
    "                                                            action_size=action_size, reuse=True)\n",
    "    \n",
    "    # Discriminator for nextnextQs_G\n",
    "    nextnextQs_G_unmasked = discriminator(prev_actions=next_actions_logits, states=nextnext_states_logits, \n",
    "                                          hidden_size=hidden_size, reuse=True)\n",
    "        \n",
    "    # Returning nextnextQs_G_unmasked\n",
    "    return nextnextQs_G_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of generator based on the generated/predicted states and actions\n",
    "def model_loss3(nextnextQs_G, prev_actions, states, dones, \n",
    "                state_size, action_size, hidden_size):\n",
    "    # Generator for nextQs_G\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    actions_logits, next_states_logits = generator(prev_actions=prev_actions_onehot, states=states,\n",
    "                                                   hidden_size=hidden_size, state_size=state_size, \n",
    "                                                   action_size=action_size, reuse=True)\n",
    "    \n",
    "    # Discriminator for nextQs_G\n",
    "    nextQs_G_unmasked = discriminator(prev_actions=actions_logits, states=next_states_logits, \n",
    "                                      hidden_size=hidden_size, reuse=True)\n",
    "    \n",
    "    # Masking the unmasked nextQs_G using dones/end of episodes/goal\n",
    "    dones_mask = tf.reshape(tensor=(1 - tf.cast(dtype=nextQs_G_unmasked.dtype, x=dones)), shape=[-1, 1])\n",
    "    nextQs_G_masked = tf.multiply(nextQs_G_unmasked, dones_mask)\n",
    "    nextQs_G = tf.reduce_max(axis=1, input_tensor=nextQs_G_masked)\n",
    "\n",
    "    # Below is the idea behind this loss\n",
    "    # # Bellman equaion: Qs = rt/qt + nextQs_G/D\n",
    "    # qs = tf.sigmoid(Qs) # qt\n",
    "    # targetQs_G = qs + (gamma * nextQs_G)\n",
    "    g_loss = tf.reduce_mean(tf.square(nextQs_G - nextnextQs_G))\n",
    "    \n",
    "    # Returning g_loss which should impact Generator\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Generator loss for action prediction\n",
    "    :param g_loss: Generator loss for state prediction\n",
    "    :param d_loss: Discriminator loss for reward prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Used for BN (batchnorm params)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize((q_loss + g_loss), var_list=g_vars) # action prediction\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize((d_loss + g_loss), var_list=d_vars) # reward prediction\n",
    "\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        ####################################### Model data inputs/outputs #######################################\n",
    "        # Input of the Model: make the data available inside the framework\n",
    "        self.prev_actions, self.states, self.actions, self.next_states, self.dones, self.nextQs, self.nextQs_D, self.nextnextQs_G = model_input(\n",
    "            state_size=state_size)\n",
    "        \n",
    "        ######################################## Model losses #####################################################\n",
    "        # Loss of the Model: action prediction/generation\n",
    "        self.actions_logits, self.q_loss = model_loss(nextQs=self.nextQs, prev_actions=self.prev_actions, \n",
    "                                                      states=self.states, actions=self.actions,\n",
    "                                                      state_size=state_size, action_size=action_size, \n",
    "                                                      hidden_size=hidden_size)\n",
    "\n",
    "        # Loss of the model: states prediction/generation\n",
    "        self.nextQs_D_unmasked = model_output(actions=self.actions, next_states=self.next_states,\n",
    "                                              action_size=action_size, hidden_size=hidden_size)\n",
    "        self.d_loss = model_loss2(nextQs_D=self.nextQs_D, prev_actions=self.prev_actions, states=self.states,\n",
    "                                  action_size=action_size, hidden_size=hidden_size)\n",
    "        \n",
    "        # Loss of the model: states prediction/generation\n",
    "        self.nextnextQs_G_unmasked = model_output2(actions=self.actions, next_states=self.next_states,\n",
    "                                                   state_size=state_size, action_size=action_size, \n",
    "                                                   hidden_size=hidden_size)\n",
    "        self.g_loss = model_loss3(nextnextQs_G=self.nextnextQs_G, dones=self.dones,\n",
    "                                  prev_actions=self.prev_actions, states=self.states,\n",
    "                                  state_size=state_size, action_size=action_size, hidden_size=hidden_size)\n",
    "        \n",
    "        ######################################## Model updates #####################################################\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss,\n",
    "                                           g_loss=self.g_loss, \n",
    "                                           d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "train_episodes = 2000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "prev_action = env.action_space.sample() # At-1\n",
    "state, _, done, _ = env.step(prev_action) # St, Rt/Et (Epiosde)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()# At\n",
    "    next_state, _, done, _ = env.step(action) #St+1\n",
    "\n",
    "    # End of the episodes which defines the goal of the episode/mission\n",
    "    if done is True:\n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        prev_action = env.action_space.sample()\n",
    "        state, _, done, _ = env.step(prev_action)\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Prepare for the next round\n",
    "        prev_action = action\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 18 Training q_loss: 0.0450 Training g_loss: 0.0062 Training d_loss: 0.0085 Explore P: 0.9982\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 8 Training q_loss: 0.0845 Training g_loss: 0.0011 Training d_loss: 0.0049 Explore P: 0.9974\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 23 Training q_loss: 0.2735 Training g_loss: 0.0003 Training d_loss: 0.0021 Explore P: 0.9952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 16 Training q_loss: 0.3611 Training g_loss: 0.0003 Training d_loss: 0.0008 Explore P: 0.9936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 11 Training q_loss: 0.4697 Training g_loss: 0.0001 Training d_loss: 0.0001 Explore P: 0.9925\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 12 Training q_loss: 0.5314 Training g_loss: 0.0011 Training d_loss: 0.0002 Explore P: 0.9913\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 24 Training q_loss: 1.1213 Training g_loss: 0.0015 Training d_loss: 0.0002 Explore P: 0.9890\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 21 Training q_loss: 0.8745 Training g_loss: 0.0001 Training d_loss: 0.0004 Explore P: 0.9869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 17 Training q_loss: 0.9074 Training g_loss: 0.0002 Training d_loss: 0.0004 Explore P: 0.9853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 18 Training q_loss: 1.3216 Training g_loss: 0.0004 Training d_loss: 0.0005 Explore P: 0.9835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 10 Training q_loss: 1.4539 Training g_loss: 0.0004 Training d_loss: 0.0004 Explore P: 0.9825\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 25 Training q_loss: 2.1207 Training g_loss: 0.0010 Training d_loss: 0.0003 Explore P: 0.9801\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 49 Training q_loss: 1.2174 Training g_loss: 0.0002 Training d_loss: 0.0003 Explore P: 0.9754\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 11 Training q_loss: 1.1687 Training g_loss: 0.0001 Training d_loss: 0.0003 Explore P: 0.9743\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 55 Training q_loss: 1.3665 Training g_loss: 0.0003 Training d_loss: 0.0002 Explore P: 0.9690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 18 Training q_loss: 2.3594 Training g_loss: 0.0003 Training d_loss: 0.0003 Explore P: 0.9673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 12 Training q_loss: 3.1344 Training g_loss: 0.0038 Training d_loss: 0.0004 Explore P: 0.9661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 18 Training q_loss: 2.3190 Training g_loss: 0.0025 Training d_loss: 0.0006 Explore P: 0.9644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 16 Training q_loss: 4.0081 Training g_loss: 0.0124 Training d_loss: 0.0026 Explore P: 0.9629\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 24 Training q_loss: 10.0504 Training g_loss: 0.0093 Training d_loss: 0.0020 Explore P: 0.9606\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 11 Training q_loss: 8.0821 Training g_loss: 0.0069 Training d_loss: 0.0028 Explore P: 0.9596\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 22 Training q_loss: 8.4684 Training g_loss: 0.0014 Training d_loss: 0.0012 Explore P: 0.9575\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 12 Training q_loss: 7.7774 Training g_loss: 0.0008 Training d_loss: 0.0046 Explore P: 0.9563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 21 Training q_loss: 7.7509 Training g_loss: 0.0013 Training d_loss: 0.0014 Explore P: 0.9544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 24 Training q_loss: 6.9000 Training g_loss: 0.0142 Training d_loss: 0.0006 Explore P: 0.9521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 9 Training q_loss: 7.7367 Training g_loss: 0.0029 Training d_loss: 0.0009 Explore P: 0.9512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 22 Training q_loss: 8.8506 Training g_loss: 0.0014 Training d_loss: 0.0023 Explore P: 0.9492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 20 Training q_loss: 29.4039 Training g_loss: 1.8334 Training d_loss: 0.0065 Explore P: 0.9473\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 15 Training q_loss: 118.5364 Training g_loss: 1.9747 Training d_loss: 0.0623 Explore P: 0.9459\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 21 Training q_loss: 325.6700 Training g_loss: 6.5427 Training d_loss: 0.2345 Explore P: 0.9439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 9 Training q_loss: 555.4346 Training g_loss: 4.4716 Training d_loss: 0.1974 Explore P: 0.9431\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 8 Training q_loss: 383.4308 Training g_loss: 2.4127 Training d_loss: 0.2112 Explore P: 0.9423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 16 Training q_loss: 406.2784 Training g_loss: 1.0138 Training d_loss: 0.1343 Explore P: 0.9409\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 23 Training q_loss: 311.3911 Training g_loss: 0.4633 Training d_loss: 0.2019 Explore P: 0.9387\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 45 Training q_loss: 309.5111 Training g_loss: 0.2546 Training d_loss: 0.0596 Explore P: 0.9345\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 10 Training q_loss: 331.0026 Training g_loss: 1.4606 Training d_loss: 0.1233 Explore P: 0.9336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 75 Training q_loss: 402.7650 Training g_loss: 0.4467 Training d_loss: 0.2571 Explore P: 0.9267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 15 Training q_loss: 346.0919 Training g_loss: 4.3815 Training d_loss: 0.0959 Explore P: 0.9253\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 36 Training q_loss: 395.5236 Training g_loss: 0.3941 Training d_loss: 0.1450 Explore P: 0.9221\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 34 Training q_loss: 456.0977 Training g_loss: 0.6759 Training d_loss: 0.2213 Explore P: 0.9190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 50 Training q_loss: 597.9174 Training g_loss: 4.4371 Training d_loss: 0.2309 Explore P: 0.9144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 16 Training q_loss: 447.0485 Training g_loss: 1.0179 Training d_loss: 2.2462 Explore P: 0.9130\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 23 Training q_loss: 439.0218 Training g_loss: 0.4566 Training d_loss: 0.1734 Explore P: 0.9109\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 15 Training q_loss: 446.6595 Training g_loss: 0.2752 Training d_loss: 0.0825 Explore P: 0.9096\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 15 Training q_loss: 461.7303 Training g_loss: 0.3329 Training d_loss: 0.1948 Explore P: 0.9082\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 24 Training q_loss: 672.7032 Training g_loss: 0.5182 Training d_loss: 0.1488 Explore P: 0.9061\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 22 Training q_loss: 638.3484 Training g_loss: 4.4061 Training d_loss: 0.2273 Explore P: 0.9041\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 14 Training q_loss: 497.2620 Training g_loss: 0.4709 Training d_loss: 0.0979 Explore P: 0.9028\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 15 Training q_loss: 428.0874 Training g_loss: 0.9939 Training d_loss: 0.0826 Explore P: 0.9015\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 8 Training q_loss: 401.8920 Training g_loss: 0.6006 Training d_loss: 0.1623 Explore P: 0.9008\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 70 Training q_loss: 489.7043 Training g_loss: 3.0372 Training d_loss: 0.2535 Explore P: 0.8946\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 17 Training q_loss: 593.3200 Training g_loss: 5.6419 Training d_loss: 0.0953 Explore P: 0.8931\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 25 Training q_loss: 752.1691 Training g_loss: 2.7850 Training d_loss: 0.2148 Explore P: 0.8909\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 8 Training q_loss: 619.0903 Training g_loss: 2.7482 Training d_loss: 0.1140 Explore P: 0.8902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 22 Training q_loss: 438.1251 Training g_loss: 1.6216 Training d_loss: 0.0674 Explore P: 0.8882\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 9 Training q_loss: 369.3661 Training g_loss: 0.6922 Training d_loss: 0.0532 Explore P: 0.8874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 14 Training q_loss: 329.3730 Training g_loss: 0.1771 Training d_loss: 0.0553 Explore P: 0.8862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 8 Training q_loss: 326.9122 Training g_loss: 2.3723 Training d_loss: 0.1416 Explore P: 0.8855\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 15 Training q_loss: 323.7786 Training g_loss: 0.2315 Training d_loss: 0.0578 Explore P: 0.8842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 33 Training q_loss: 305.4877 Training g_loss: 1.2886 Training d_loss: 0.0379 Explore P: 0.8813\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 84 Training q_loss: 308.4148 Training g_loss: 0.8494 Training d_loss: 0.0406 Explore P: 0.8740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 14 Training q_loss: 297.0661 Training g_loss: 0.6588 Training d_loss: 0.0232 Explore P: 0.8728\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 10 Training q_loss: 288.1628 Training g_loss: 1.0138 Training d_loss: 0.0312 Explore P: 0.8720\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 15 Training q_loss: 279.8689 Training g_loss: 0.2245 Training d_loss: 0.0239 Explore P: 0.8707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 10 Training q_loss: 268.9035 Training g_loss: 0.2657 Training d_loss: 0.0239 Explore P: 0.8698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 22 Training q_loss: 258.9249 Training g_loss: 0.2036 Training d_loss: 0.0250 Explore P: 0.8679\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 9 Training q_loss: 249.5797 Training g_loss: 0.1904 Training d_loss: 0.0418 Explore P: 0.8671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 20 Training q_loss: 255.5525 Training g_loss: 0.2356 Training d_loss: 0.0691 Explore P: 0.8654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 20 Training q_loss: 289.0427 Training g_loss: 0.2380 Training d_loss: 0.0229 Explore P: 0.8637\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 62 Training q_loss: 345.9219 Training g_loss: 0.3524 Training d_loss: 0.0311 Explore P: 0.8584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 41 Training q_loss: 344.8174 Training g_loss: 0.1281 Training d_loss: 0.0533 Explore P: 0.8550\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 16 Training q_loss: 374.8117 Training g_loss: 0.5125 Training d_loss: 0.0520 Explore P: 0.8536\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 9 Training q_loss: 358.4891 Training g_loss: 0.2002 Training d_loss: 0.0322 Explore P: 0.8529\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 12 Training q_loss: 338.6527 Training g_loss: 0.3011 Training d_loss: 0.0433 Explore P: 0.8519\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 10 Training q_loss: 330.4125 Training g_loss: 0.1712 Training d_loss: 0.0225 Explore P: 0.8510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 12 Training q_loss: 346.7558 Training g_loss: 4.4105 Training d_loss: 0.1548 Explore P: 0.8500\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 16 Training q_loss: 363.9543 Training g_loss: 0.4468 Training d_loss: 0.0483 Explore P: 0.8487\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 28 Training q_loss: 352.8749 Training g_loss: 0.2371 Training d_loss: 0.0241 Explore P: 0.8463\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 13 Training q_loss: 430.9402 Training g_loss: 4.0020 Training d_loss: 0.0548 Explore P: 0.8452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 11 Training q_loss: 504.5129 Training g_loss: 1.5061 Training d_loss: 0.1307 Explore P: 0.8443\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 11 Training q_loss: 630.5031 Training g_loss: 1.2086 Training d_loss: 0.0828 Explore P: 0.8434\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 20 Training q_loss: 638.4446 Training g_loss: 1.6466 Training d_loss: 0.0838 Explore P: 0.8417\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 36 Training q_loss: 1019.1249 Training g_loss: 21.5003 Training d_loss: 0.1951 Explore P: 0.8387\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 30 Training q_loss: 612.3142 Training g_loss: 0.7658 Training d_loss: 0.1093 Explore P: 0.8363\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 30 Training q_loss: 581.5228 Training g_loss: 0.2254 Training d_loss: 0.0782 Explore P: 0.8338\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 12 Training q_loss: 610.7184 Training g_loss: 0.7172 Training d_loss: 0.2079 Explore P: 0.8328\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 37 Training q_loss: 712.3722 Training g_loss: 0.3403 Training d_loss: 0.0985 Explore P: 0.8298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 19 Training q_loss: 746.4512 Training g_loss: 0.7884 Training d_loss: 0.0957 Explore P: 0.8282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 47 Training q_loss: 649.4041 Training g_loss: 0.3182 Training d_loss: 0.0941 Explore P: 0.8244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 19 Training q_loss: 832.7873 Training g_loss: 0.7876 Training d_loss: 0.1093 Explore P: 0.8228\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 22 Training q_loss: 1191.1609 Training g_loss: 34.3036 Training d_loss: 0.2262 Explore P: 0.8210\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 10 Training q_loss: 1042.2466 Training g_loss: 143.4651 Training d_loss: 0.4695 Explore P: 0.8202\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 14 Training q_loss: 1484.9404 Training g_loss: 25.7706 Training d_loss: 0.4179 Explore P: 0.8191\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 73 Training q_loss: 2570.0066 Training g_loss: 12.9092 Training d_loss: 0.6394 Explore P: 0.8132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 17 Training q_loss: 3012.7644 Training g_loss: 3.2330 Training d_loss: 0.8495 Explore P: 0.8118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 19 Training q_loss: 3155.6594 Training g_loss: 4.3810 Training d_loss: 1.1134 Explore P: 0.8103\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 10 Training q_loss: 3222.1853 Training g_loss: 3.2352 Training d_loss: 0.9985 Explore P: 0.8095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 10 Training q_loss: 3107.5466 Training g_loss: 9.2971 Training d_loss: 2.1818 Explore P: 0.8087\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 12 Training q_loss: 2941.2593 Training g_loss: 12.8532 Training d_loss: 1.4791 Explore P: 0.8078\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 13 Training q_loss: 2538.9807 Training g_loss: 11.9959 Training d_loss: 4.9451 Explore P: 0.8067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 12 Training q_loss: 2324.1013 Training g_loss: 12.1638 Training d_loss: 0.3661 Explore P: 0.8058\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 20 Training q_loss: 2066.3875 Training g_loss: 0.9468 Training d_loss: 0.3369 Explore P: 0.8042\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 17 Training q_loss: 2090.0103 Training g_loss: 7.4637 Training d_loss: 0.3404 Explore P: 0.8028\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 12 Training q_loss: 2343.0571 Training g_loss: 7.8296 Training d_loss: 0.3683 Explore P: 0.8019\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 15 Training q_loss: 2075.4099 Training g_loss: 7.4260 Training d_loss: 0.3468 Explore P: 0.8007\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 13 Training q_loss: 1816.5693 Training g_loss: 0.8483 Training d_loss: 0.3731 Explore P: 0.7997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 10 Training q_loss: 1619.2798 Training g_loss: 0.7757 Training d_loss: 0.1993 Explore P: 0.7989\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 15 Training q_loss: 1525.9418 Training g_loss: 1.8054 Training d_loss: 0.2015 Explore P: 0.7977\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 12 Training q_loss: 1479.6765 Training g_loss: 0.5931 Training d_loss: 0.1692 Explore P: 0.7967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 17 Training q_loss: 1558.4303 Training g_loss: 0.4583 Training d_loss: 0.1879 Explore P: 0.7954\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 7 Training q_loss: 1603.6436 Training g_loss: 1.2130 Training d_loss: 0.1359 Explore P: 0.7949\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 15 Training q_loss: 1535.0830 Training g_loss: 0.7294 Training d_loss: 0.2037 Explore P: 0.7937\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 14 Training q_loss: 1516.3835 Training g_loss: 0.2776 Training d_loss: 0.2499 Explore P: 0.7926\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 24 Training q_loss: 1488.7220 Training g_loss: 0.2974 Training d_loss: 0.1347 Explore P: 0.7907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 27 Training q_loss: 1382.1646 Training g_loss: 0.5102 Training d_loss: 0.1792 Explore P: 0.7886\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 26 Training q_loss: 1335.5469 Training g_loss: 1.2973 Training d_loss: 1.1649 Explore P: 0.7866\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 19 Training q_loss: 1417.7889 Training g_loss: 0.8499 Training d_loss: 0.1123 Explore P: 0.7851\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 17 Training q_loss: 1442.9528 Training g_loss: 1.0634 Training d_loss: 0.7118 Explore P: 0.7838\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 12 Training q_loss: 1381.9604 Training g_loss: 0.8036 Training d_loss: 0.2085 Explore P: 0.7829\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 13 Training q_loss: 1360.2098 Training g_loss: 0.6961 Training d_loss: 0.1636 Explore P: 0.7819\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 20 Training q_loss: 1329.4042 Training g_loss: 0.5917 Training d_loss: 0.0984 Explore P: 0.7803\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 23 Training q_loss: 1347.2589 Training g_loss: 13.1362 Training d_loss: 0.1087 Explore P: 0.7785\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 17 Training q_loss: 1395.5420 Training g_loss: 0.7724 Training d_loss: 0.7210 Explore P: 0.7772\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 42 Training q_loss: 1450.7036 Training g_loss: 11.8236 Training d_loss: 0.1621 Explore P: 0.7740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 14 Training q_loss: 1530.4181 Training g_loss: 0.8927 Training d_loss: 0.1213 Explore P: 0.7730\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 13 Training q_loss: 1492.5358 Training g_loss: 1.7005 Training d_loss: 0.2428 Explore P: 0.7720\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 18 Training q_loss: 1481.4558 Training g_loss: 1.8011 Training d_loss: 0.1492 Explore P: 0.7706\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 12 Training q_loss: 1418.5328 Training g_loss: 0.3714 Training d_loss: 0.1323 Explore P: 0.7697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 8 Training q_loss: 1417.1748 Training g_loss: 0.4087 Training d_loss: 0.0937 Explore P: 0.7691\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 12 Training q_loss: 1459.1246 Training g_loss: 0.2595 Training d_loss: 0.1905 Explore P: 0.7682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 10 Training q_loss: 1425.2195 Training g_loss: 0.2213 Training d_loss: 0.3117 Explore P: 0.7674\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 38 Training q_loss: 1386.0106 Training g_loss: 0.8858 Training d_loss: 0.7610 Explore P: 0.7645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 20 Training q_loss: 1354.9264 Training g_loss: 48.7513 Training d_loss: 3.4365 Explore P: 0.7630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 20 Training q_loss: 1436.1996 Training g_loss: 0.5855 Training d_loss: 0.2929 Explore P: 0.7615\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 18 Training q_loss: 1540.2048 Training g_loss: 1.7297 Training d_loss: 0.1379 Explore P: 0.7602\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 25 Training q_loss: 1538.4337 Training g_loss: 1.7199 Training d_loss: 0.1896 Explore P: 0.7583\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 21 Training q_loss: 1532.3441 Training g_loss: 0.2436 Training d_loss: 0.2414 Explore P: 0.7567\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 38 Training q_loss: 1603.8497 Training g_loss: 0.9969 Training d_loss: 0.1429 Explore P: 0.7539\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 12 Training q_loss: 1554.0814 Training g_loss: 0.3014 Training d_loss: 0.1665 Explore P: 0.7530\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 16 Training q_loss: 1520.7482 Training g_loss: 0.3345 Training d_loss: 0.1904 Explore P: 0.7518\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 10 Training q_loss: 1518.4628 Training g_loss: 0.7374 Training d_loss: 0.1957 Explore P: 0.7511\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 13 Training q_loss: 1556.8799 Training g_loss: 0.4458 Training d_loss: 0.1112 Explore P: 0.7501\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 12 Training q_loss: 1556.0741 Training g_loss: 0.3022 Training d_loss: 0.6055 Explore P: 0.7492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 17 Training q_loss: 1629.4240 Training g_loss: 0.7489 Training d_loss: 0.2302 Explore P: 0.7480\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 11 Training q_loss: 1669.0703 Training g_loss: 3.2402 Training d_loss: 0.3761 Explore P: 0.7472\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 16 Training q_loss: 1685.6462 Training g_loss: 1.0944 Training d_loss: 0.1487 Explore P: 0.7460\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 17 Training q_loss: 1660.9526 Training g_loss: 7.0570 Training d_loss: 0.1525 Explore P: 0.7447\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 11 Training q_loss: 1630.7128 Training g_loss: 0.5340 Training d_loss: 0.1712 Explore P: 0.7439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 13 Training q_loss: 1631.5027 Training g_loss: 0.3180 Training d_loss: 0.3039 Explore P: 0.7430\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 18 Training q_loss: 1699.0385 Training g_loss: 1.1075 Training d_loss: 0.1318 Explore P: 0.7417\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 7 Training q_loss: 1799.6897 Training g_loss: 0.4495 Training d_loss: 0.5729 Explore P: 0.7411\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 12 Training q_loss: 2008.3726 Training g_loss: 1.3340 Training d_loss: 0.2712 Explore P: 0.7403\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 31 Training q_loss: 2174.6675 Training g_loss: 0.5953 Training d_loss: 0.2064 Explore P: 0.7380\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 9 Training q_loss: 2127.1621 Training g_loss: 0.4537 Training d_loss: 0.1914 Explore P: 0.7373\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 12 Training q_loss: 2155.3647 Training g_loss: 18.1107 Training d_loss: 0.2052 Explore P: 0.7365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 15 Training q_loss: 2317.1777 Training g_loss: 1.3235 Training d_loss: 0.5142 Explore P: 0.7354\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 18 Training q_loss: 2474.7212 Training g_loss: 0.7698 Training d_loss: 0.5039 Explore P: 0.7341\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 7 Training q_loss: 2496.0906 Training g_loss: 0.6792 Training d_loss: 0.5441 Explore P: 0.7336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 21 Training q_loss: 2418.7119 Training g_loss: 2.5584 Training d_loss: 0.3197 Explore P: 0.7321\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 15 Training q_loss: 2159.7068 Training g_loss: 1.6700 Training d_loss: 0.4244 Explore P: 0.7310\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 15 Training q_loss: 2031.6184 Training g_loss: 4.3114 Training d_loss: 0.1750 Explore P: 0.7299\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 16 Training q_loss: 1530.4559 Training g_loss: 1.4693 Training d_loss: 0.1617 Explore P: 0.7287\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 20 Training q_loss: 1407.7784 Training g_loss: 0.4393 Training d_loss: 0.1077 Explore P: 0.7273\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 15 Training q_loss: 1370.2418 Training g_loss: 1.0768 Training d_loss: 0.1707 Explore P: 0.7262\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 8 Training q_loss: 1366.2501 Training g_loss: 0.7072 Training d_loss: 0.0950 Explore P: 0.7257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 21 Training q_loss: 1434.6548 Training g_loss: 0.2033 Training d_loss: 0.0864 Explore P: 0.7242\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 10 Training q_loss: 1499.2432 Training g_loss: 0.6913 Training d_loss: 0.1323 Explore P: 0.7234\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 10 Training q_loss: 1493.7467 Training g_loss: 1.0659 Training d_loss: 0.1087 Explore P: 0.7227\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 12 Training q_loss: 1461.8694 Training g_loss: 0.9497 Training d_loss: 0.1106 Explore P: 0.7219\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 12 Training q_loss: 1428.1609 Training g_loss: 0.3861 Training d_loss: 0.0971 Explore P: 0.7210\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 17 Training q_loss: 1336.5162 Training g_loss: 0.2745 Training d_loss: 0.1050 Explore P: 0.7198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 12 Training q_loss: 1328.0396 Training g_loss: 0.5165 Training d_loss: 0.1566 Explore P: 0.7190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 13 Training q_loss: 1348.2844 Training g_loss: 0.3231 Training d_loss: 0.2878 Explore P: 0.7180\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 27 Training q_loss: 1291.1877 Training g_loss: 2.8868 Training d_loss: 0.0696 Explore P: 0.7161\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 54 Training q_loss: 1420.8114 Training g_loss: 0.9327 Training d_loss: 0.1021 Explore P: 0.7123\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 10 Training q_loss: 1412.8481 Training g_loss: 1.3607 Training d_loss: 0.1942 Explore P: 0.7116\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 17 Training q_loss: 1645.4156 Training g_loss: 1.7626 Training d_loss: 0.1928 Explore P: 0.7104\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 13 Training q_loss: 1701.3901 Training g_loss: 1.0163 Training d_loss: 0.3284 Explore P: 0.7095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 35 Training q_loss: 1679.7228 Training g_loss: 2.7361 Training d_loss: 0.1393 Explore P: 0.7071\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 15 Training q_loss: 1759.9426 Training g_loss: 84.0184 Training d_loss: 3.7354 Explore P: 0.7060\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 55 Training q_loss: 1271.0411 Training g_loss: 0.5472 Training d_loss: 0.2755 Explore P: 0.7022\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 12 Training q_loss: 1253.7820 Training g_loss: 0.2525 Training d_loss: 0.0718 Explore P: 0.7014\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 13 Training q_loss: 1271.0970 Training g_loss: 0.4177 Training d_loss: 0.1020 Explore P: 0.7005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 9 Training q_loss: 1309.6031 Training g_loss: 8.3886 Training d_loss: 0.1283 Explore P: 0.6999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 21 Training q_loss: 1388.1801 Training g_loss: 1.5662 Training d_loss: 0.0699 Explore P: 0.6984\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 9 Training q_loss: 1415.6249 Training g_loss: 0.2603 Training d_loss: 0.3234 Explore P: 0.6978\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 10 Training q_loss: 1529.2222 Training g_loss: 0.3891 Training d_loss: 0.1166 Explore P: 0.6971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 17 Training q_loss: 1676.8096 Training g_loss: 1.0633 Training d_loss: 0.1780 Explore P: 0.6960\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 15 Training q_loss: 1871.6434 Training g_loss: 7.9805 Training d_loss: 0.1346 Explore P: 0.6949\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 42 Training q_loss: 2261.2432 Training g_loss: 52.4791 Training d_loss: 0.2798 Explore P: 0.6921\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 8 Training q_loss: 2448.2371 Training g_loss: 187.6138 Training d_loss: 0.3333 Explore P: 0.6915\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 12 Training q_loss: 3270.1760 Training g_loss: 5.8442 Training d_loss: 1.0221 Explore P: 0.6907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 35 Training q_loss: 1160.3378 Training g_loss: 39.7495 Training d_loss: 0.0809 Explore P: 0.6883\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 24 Training q_loss: 1052.8997 Training g_loss: 1.6057 Training d_loss: 0.1193 Explore P: 0.6867\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 12 Training q_loss: 1104.9396 Training g_loss: 11.3134 Training d_loss: 0.0639 Explore P: 0.6859\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 40 Training q_loss: 1126.5342 Training g_loss: 0.7661 Training d_loss: 0.3697 Explore P: 0.6832\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 18 Training q_loss: 1136.4886 Training g_loss: 1.1794 Training d_loss: 0.1203 Explore P: 0.6820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 10 Training q_loss: 1179.1504 Training g_loss: 0.3271 Training d_loss: 0.1606 Explore P: 0.6813\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 11 Training q_loss: 1168.5311 Training g_loss: 0.5441 Training d_loss: 0.0810 Explore P: 0.6806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 19 Training q_loss: 1163.9059 Training g_loss: 0.4578 Training d_loss: 0.1646 Explore P: 0.6793\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 22 Training q_loss: 1104.7006 Training g_loss: 0.2719 Training d_loss: 0.0730 Explore P: 0.6778\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 12 Training q_loss: 1171.6139 Training g_loss: 0.6053 Training d_loss: 0.0889 Explore P: 0.6770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 19 Training q_loss: 1164.2532 Training g_loss: 0.2747 Training d_loss: 0.1036 Explore P: 0.6757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 9 Training q_loss: 1145.5312 Training g_loss: 0.2133 Training d_loss: 0.1095 Explore P: 0.6751\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 15 Training q_loss: 1181.0330 Training g_loss: 2.1235 Training d_loss: 0.1456 Explore P: 0.6741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 12 Training q_loss: 1182.5123 Training g_loss: 0.2275 Training d_loss: 0.0672 Explore P: 0.6734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 27 Training q_loss: 1207.2375 Training g_loss: 1.6014 Training d_loss: 0.1082 Explore P: 0.6716\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 12 Training q_loss: 1218.3168 Training g_loss: 0.8762 Training d_loss: 0.0738 Explore P: 0.6708\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 11 Training q_loss: 1196.1769 Training g_loss: 21.7942 Training d_loss: 0.2785 Explore P: 0.6700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 12 Training q_loss: 1227.7449 Training g_loss: 2.1735 Training d_loss: 0.1039 Explore P: 0.6693\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 38 Training q_loss: 1277.0068 Training g_loss: 2.3697 Training d_loss: 0.1058 Explore P: 0.6668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 17 Training q_loss: 1313.0242 Training g_loss: 0.1525 Training d_loss: 0.0653 Explore P: 0.6656\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 8 Training q_loss: 1333.1829 Training g_loss: 0.2588 Training d_loss: 0.1012 Explore P: 0.6651\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 10 Training q_loss: 1433.8545 Training g_loss: 0.5337 Training d_loss: 0.0962 Explore P: 0.6645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 18 Training q_loss: 1501.0078 Training g_loss: 3.2641 Training d_loss: 0.3334 Explore P: 0.6633\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 14 Training q_loss: 1550.9519 Training g_loss: 0.4184 Training d_loss: 0.1200 Explore P: 0.6624\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 46 Training q_loss: 1683.9569 Training g_loss: 4.7105 Training d_loss: 0.1746 Explore P: 0.6594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 9 Training q_loss: 1689.1025 Training g_loss: 1.3076 Training d_loss: 0.1034 Explore P: 0.6588\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 9 Training q_loss: 1702.4906 Training g_loss: 2.4079 Training d_loss: 0.2408 Explore P: 0.6582\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 26 Training q_loss: 1711.6915 Training g_loss: 3.1057 Training d_loss: 0.2805 Explore P: 0.6565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 15 Training q_loss: 1833.2562 Training g_loss: 1.0665 Training d_loss: 0.2618 Explore P: 0.6556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 23 Training q_loss: 1842.0381 Training g_loss: 1.0451 Training d_loss: 0.2139 Explore P: 0.6541\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 25 Training q_loss: 1935.3610 Training g_loss: 17.3880 Training d_loss: 0.1270 Explore P: 0.6525\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 19 Training q_loss: 1666.6464 Training g_loss: 0.6723 Training d_loss: 0.1127 Explore P: 0.6512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 15 Training q_loss: 1667.7443 Training g_loss: 0.5171 Training d_loss: 0.1110 Explore P: 0.6503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 17 Training q_loss: 1569.6215 Training g_loss: 0.2201 Training d_loss: 0.1345 Explore P: 0.6492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 9 Training q_loss: 1587.9670 Training g_loss: 0.2261 Training d_loss: 0.1384 Explore P: 0.6486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 13 Training q_loss: 1622.7672 Training g_loss: 1.4144 Training d_loss: 0.0901 Explore P: 0.6478\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 15 Training q_loss: 1753.6722 Training g_loss: 2.8587 Training d_loss: 0.1418 Explore P: 0.6468\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 8 Training q_loss: 1849.3196 Training g_loss: 1.7433 Training d_loss: 0.1201 Explore P: 0.6463\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 8 Training q_loss: 1883.2050 Training g_loss: 0.9897 Training d_loss: 0.2981 Explore P: 0.6458\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 11 Training q_loss: 1955.5090 Training g_loss: 4.4669 Training d_loss: 0.2572 Explore P: 0.6451\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 13 Training q_loss: 2218.3250 Training g_loss: 3.6494 Training d_loss: 0.3595 Explore P: 0.6443\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 11 Training q_loss: 2299.4109 Training g_loss: 3.4347 Training d_loss: 0.1898 Explore P: 0.6436\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 13 Training q_loss: 2586.1416 Training g_loss: 23.8102 Training d_loss: 0.6678 Explore P: 0.6428\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 11 Training q_loss: 2512.3545 Training g_loss: 8.3748 Training d_loss: 1.6896 Explore P: 0.6421\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 19 Training q_loss: 2404.5493 Training g_loss: 2.7103 Training d_loss: 0.1941 Explore P: 0.6409\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 8 Training q_loss: 2593.4426 Training g_loss: 4.1837 Training d_loss: 0.3559 Explore P: 0.6404\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 20 Training q_loss: 2625.3323 Training g_loss: 18.5155 Training d_loss: 4.2447 Explore P: 0.6391\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 11 Training q_loss: 2519.6160 Training g_loss: 7.6438 Training d_loss: 0.3937 Explore P: 0.6384\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 38 Training q_loss: 2805.5242 Training g_loss: 0.6431 Training d_loss: 0.1585 Explore P: 0.6360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 19 Training q_loss: 2373.9741 Training g_loss: 1.4421 Training d_loss: 0.1525 Explore P: 0.6348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 16 Training q_loss: 2162.2800 Training g_loss: 3.7951 Training d_loss: 1.2561 Explore P: 0.6338\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 28 Training q_loss: 1853.7355 Training g_loss: 3.1698 Training d_loss: 0.0888 Explore P: 0.6321\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 22 Training q_loss: 1567.1083 Training g_loss: 1.5244 Training d_loss: 0.0732 Explore P: 0.6307\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 15 Training q_loss: 1457.7255 Training g_loss: 3.4452 Training d_loss: 0.0664 Explore P: 0.6298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 10 Training q_loss: 1423.1001 Training g_loss: 3.0548 Training d_loss: 0.1418 Explore P: 0.6292\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 15 Training q_loss: 1417.8497 Training g_loss: 0.5375 Training d_loss: 0.1060 Explore P: 0.6283\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 8 Training q_loss: 1371.4045 Training g_loss: 0.5433 Training d_loss: 0.0463 Explore P: 0.6278\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 21 Training q_loss: 1451.4619 Training g_loss: 1.2147 Training d_loss: 0.1011 Explore P: 0.6265\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 18 Training q_loss: 1503.7405 Training g_loss: 1.8031 Training d_loss: 0.0589 Explore P: 0.6254\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 13 Training q_loss: 1579.5483 Training g_loss: 0.7172 Training d_loss: 0.0892 Explore P: 0.6246\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 9 Training q_loss: 1574.5870 Training g_loss: 0.2980 Training d_loss: 0.0671 Explore P: 0.6240\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 11 Training q_loss: 1627.2035 Training g_loss: 0.3554 Training d_loss: 0.0922 Explore P: 0.6233\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 15 Training q_loss: 1699.4655 Training g_loss: 0.3657 Training d_loss: 0.1040 Explore P: 0.6224\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 10 Training q_loss: 1693.5757 Training g_loss: 0.6516 Training d_loss: 0.1196 Explore P: 0.6218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 15 Training q_loss: 1638.3628 Training g_loss: 2.4457 Training d_loss: 0.0727 Explore P: 0.6209\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 24 Training q_loss: 1700.4329 Training g_loss: 0.9200 Training d_loss: 0.0837 Explore P: 0.6194\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 8 Training q_loss: 1775.4169 Training g_loss: 0.3060 Training d_loss: 0.1614 Explore P: 0.6189\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 9 Training q_loss: 1792.5122 Training g_loss: 0.4290 Training d_loss: 0.0821 Explore P: 0.6184\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 14 Training q_loss: 1841.8577 Training g_loss: 0.1677 Training d_loss: 0.0965 Explore P: 0.6175\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 18 Training q_loss: 1950.2191 Training g_loss: 1.2536 Training d_loss: 0.3370 Explore P: 0.6164\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 10 Training q_loss: 2141.7917 Training g_loss: 4.0277 Training d_loss: 0.5551 Explore P: 0.6158\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 11 Training q_loss: 2347.2646 Training g_loss: 3.4152 Training d_loss: 0.1193 Explore P: 0.6152\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 15 Training q_loss: 2196.2507 Training g_loss: 15.6121 Training d_loss: 0.0860 Explore P: 0.6143\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 10 Training q_loss: 1912.2384 Training g_loss: 5.2929 Training d_loss: 0.5906 Explore P: 0.6137\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 10 Training q_loss: 1650.2649 Training g_loss: 1.4916 Training d_loss: 0.0499 Explore P: 0.6131\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 14 Training q_loss: 1600.1683 Training g_loss: 2.8889 Training d_loss: 0.0563 Explore P: 0.6122\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 12 Training q_loss: 1504.7410 Training g_loss: 4.0777 Training d_loss: 0.0646 Explore P: 0.6115\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 8 Training q_loss: 1477.7266 Training g_loss: 0.5010 Training d_loss: 0.0815 Explore P: 0.6110\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 32 Training q_loss: 1604.4034 Training g_loss: 41.4955 Training d_loss: 0.0452 Explore P: 0.6091\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 19 Training q_loss: 1701.7251 Training g_loss: 19.9297 Training d_loss: 0.1393 Explore P: 0.6079\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 12 Training q_loss: 1945.7694 Training g_loss: 17.4938 Training d_loss: 0.0826 Explore P: 0.6072\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 8 Training q_loss: 2363.8506 Training g_loss: 102.7325 Training d_loss: 0.1640 Explore P: 0.6068\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 15 Training q_loss: 2837.7866 Training g_loss: 49.4524 Training d_loss: 0.2075 Explore P: 0.6059\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 12 Training q_loss: 3171.2720 Training g_loss: 11.2764 Training d_loss: 0.4153 Explore P: 0.6051\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 10 Training q_loss: 3516.2156 Training g_loss: 6.7232 Training d_loss: 0.3820 Explore P: 0.6046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 17 Training q_loss: 3505.6387 Training g_loss: 25.0909 Training d_loss: 0.4240 Explore P: 0.6035\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 20 Training q_loss: 3465.3430 Training g_loss: 63.7389 Training d_loss: 0.3618 Explore P: 0.6024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 17 Training q_loss: 2143.0933 Training g_loss: 8.2959 Training d_loss: 0.6326 Explore P: 0.6013\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 14 Training q_loss: 2017.7028 Training g_loss: 0.7160 Training d_loss: 0.1299 Explore P: 0.6005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 16 Training q_loss: 2202.0476 Training g_loss: 0.5631 Training d_loss: 0.1310 Explore P: 0.5996\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 29 Training q_loss: 2462.3228 Training g_loss: 37.6541 Training d_loss: 0.1622 Explore P: 0.5979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 13 Training q_loss: 2818.8362 Training g_loss: 16.7389 Training d_loss: 0.2323 Explore P: 0.5971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 32 Training q_loss: 3043.1328 Training g_loss: 3.1207 Training d_loss: 0.3902 Explore P: 0.5952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 44 Training q_loss: 2868.8413 Training g_loss: 11.5272 Training d_loss: 0.1853 Explore P: 0.5927\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 13 Training q_loss: 2431.4060 Training g_loss: 7.7118 Training d_loss: 0.1973 Explore P: 0.5919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 9 Training q_loss: 2202.8665 Training g_loss: 31.5500 Training d_loss: 0.1589 Explore P: 0.5914\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 19 Training q_loss: 1595.4778 Training g_loss: 5.1293 Training d_loss: 0.2744 Explore P: 0.5903\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 15 Training q_loss: 1591.1484 Training g_loss: 0.5819 Training d_loss: 0.1593 Explore P: 0.5894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 9 Training q_loss: 1607.0051 Training g_loss: 1.5167 Training d_loss: 0.0742 Explore P: 0.5889\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 23 Training q_loss: 1790.5817 Training g_loss: 4.0746 Training d_loss: 1.5050 Explore P: 0.5876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 11 Training q_loss: 1840.0703 Training g_loss: 1.8890 Training d_loss: 0.0918 Explore P: 0.5869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 12 Training q_loss: 1855.2936 Training g_loss: 0.6217 Training d_loss: 0.1266 Explore P: 0.5862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 16 Training q_loss: 1916.2506 Training g_loss: 0.1952 Training d_loss: 0.0891 Explore P: 0.5853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 9 Training q_loss: 1846.9734 Training g_loss: 0.2667 Training d_loss: 0.1612 Explore P: 0.5848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 22 Training q_loss: 1958.5227 Training g_loss: 0.2043 Training d_loss: 0.1027 Explore P: 0.5835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 12 Training q_loss: 1916.0919 Training g_loss: 0.2637 Training d_loss: 0.1046 Explore P: 0.5828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 12 Training q_loss: 1968.1622 Training g_loss: 0.2889 Training d_loss: 0.1319 Explore P: 0.5822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 10 Training q_loss: 2012.7517 Training g_loss: 2.4517 Training d_loss: 0.1419 Explore P: 0.5816\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 14 Training q_loss: 2171.5576 Training g_loss: 0.3664 Training d_loss: 0.1044 Explore P: 0.5808\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 17 Training q_loss: 2394.1096 Training g_loss: 19.7367 Training d_loss: 0.1215 Explore P: 0.5798\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 31 Training q_loss: 2770.7246 Training g_loss: 9.0232 Training d_loss: 0.5177 Explore P: 0.5780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 19 Training q_loss: 3033.8530 Training g_loss: 431.7423 Training d_loss: 0.3932 Explore P: 0.5770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 9 Training q_loss: 4870.3721 Training g_loss: 264.1621 Training d_loss: 0.8852 Explore P: 0.5765\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 10 Training q_loss: 5875.4302 Training g_loss: 910.4385 Training d_loss: 0.9169 Explore P: 0.5759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 10 Training q_loss: 2475.3372 Training g_loss: 51.8542 Training d_loss: 0.1309 Explore P: 0.5753\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 17 Training q_loss: 1537.8464 Training g_loss: 45.6740 Training d_loss: 0.1377 Explore P: 0.5744\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 11 Training q_loss: 1446.7361 Training g_loss: 145.2981 Training d_loss: 0.1329 Explore P: 0.5737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 21 Training q_loss: 1402.4471 Training g_loss: 2.0240 Training d_loss: 0.2884 Explore P: 0.5726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 21 Training q_loss: 1406.6735 Training g_loss: 3.8971 Training d_loss: 0.0737 Explore P: 0.5714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 12 Training q_loss: 1473.6545 Training g_loss: 2.9140 Training d_loss: 0.0703 Explore P: 0.5707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 21 Training q_loss: 1480.3710 Training g_loss: 1.4092 Training d_loss: 0.0780 Explore P: 0.5695\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 12 Training q_loss: 1494.7295 Training g_loss: 2.0555 Training d_loss: 0.0769 Explore P: 0.5689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 9 Training q_loss: 1518.6364 Training g_loss: 1.2990 Training d_loss: 0.1089 Explore P: 0.5684\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 24 Training q_loss: 1527.4709 Training g_loss: 0.7134 Training d_loss: 0.0611 Explore P: 0.5670\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 11 Training q_loss: 1545.7983 Training g_loss: 2.8454 Training d_loss: 0.0557 Explore P: 0.5664\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 13 Training q_loss: 1512.2150 Training g_loss: 0.9287 Training d_loss: 0.0942 Explore P: 0.5657\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 8 Training q_loss: 1526.6971 Training g_loss: 0.5829 Training d_loss: 0.1713 Explore P: 0.5652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 15 Training q_loss: 1512.4972 Training g_loss: 7.8298 Training d_loss: 0.1262 Explore P: 0.5644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 14 Training q_loss: 1534.4797 Training g_loss: 0.5704 Training d_loss: 0.0603 Explore P: 0.5636\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 7 Training q_loss: 1562.5454 Training g_loss: 2.2841 Training d_loss: 0.0679 Explore P: 0.5632\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 13 Training q_loss: 1539.9817 Training g_loss: 1.9698 Training d_loss: 0.1192 Explore P: 0.5625\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list = []\n",
    "q_loss_list = []\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward = 0\n",
    "        q_loss = 0\n",
    "        g_loss = 0\n",
    "        d_loss = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                             model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits) # arg with max value/Q is the class of action\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            #total_reward += reward\n",
    "            total_reward += 1 # done=False\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done is True:\n",
    "                # the episode ends so no next state\n",
    "                #next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, _, done, _ = env.step(prev_action)\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # One step forward: At-1=At and St=St+1\n",
    "                prev_action = action\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            prev_actions = np.array([each[0] for each in batch])\n",
    "            states = np.array([each[1] for each in batch])\n",
    "            actions = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            \n",
    "            # Calculating nextQs and setting them to 0 for states where episode ends/fails\n",
    "            feed_dict={model.prev_actions: actions, \n",
    "                       model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            \n",
    "            # Masking for the end of episodes/ goals\n",
    "            next_actions_mask = (1 - dones.astype(next_actions_logits.dtype)).reshape(-1, 1) \n",
    "            nextQs_masked = np.multiply(next_actions_logits, next_actions_mask)\n",
    "            nextQs = np.max(nextQs_masked, axis=1)\n",
    "            \n",
    "            # Calculating nextQs for Discriminator using D(At, St+1)= Qt+1/nextQs_D/nextQs\n",
    "            # Calculating nextQs for Discriminator using D(~At, ~St+1)= ~Qt+1/nextQs_G/nextQs2\n",
    "            feed_dict={model.prev_actions: prev_actions, model.states: states,\n",
    "                       model.actions: actions, model.next_states: next_states}\n",
    "            nextQs_D_unmasked, nextnextQs_G_unmasked = sess.run([model.nextQs_D_unmasked,\n",
    "                                                                 model.nextnextQs_G_unmasked], feed_dict)\n",
    "            \n",
    "            # Masking for the end of episodes/ goals\n",
    "            dones_mask = (1 - dones.astype(nextQs_D_unmasked[0].dtype)).reshape(-1, 1)\n",
    "            nextQs_D_masked = np.multiply(nextQs_D_unmasked[0], dones_mask)\n",
    "            nextnextQs_G_masked = np.multiply(nextnextQs_G_unmasked[0], dones_mask)\n",
    "            nextQs_D = np.max(nextQs_D_masked, axis=1)\n",
    "            nextnextQs_G = np.max(nextnextQs_G_masked, axis=1)\n",
    "            \n",
    "            # Calculating nextQs for Discriminator using D(At-1, St)= Qt: NOT this one\n",
    "            # NextQs/Qt+1 are given both:\n",
    "            feed_dict = {model.prev_actions: prev_actions, \n",
    "                         model.states: states, \n",
    "                         model.actions: actions, \n",
    "                         model.next_states: next_states, \n",
    "                         model.dones: dones,\n",
    "                         model.nextQs: nextQs,\n",
    "                         model.nextQs_D: nextQs_D,\n",
    "                         model.nextnextQs_G: nextnextQs_G}\n",
    "            q_loss, g_loss, d_loss, _, _ = sess.run([model.q_loss,\n",
    "                                                     model.g_loss,\n",
    "                                                     model.d_loss,\n",
    "                                                     model.g_opt,\n",
    "                                                     model.d_opt], feed_dict)\n",
    "                        \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "# env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from the model\n",
    "            feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                         model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, reward, done, _ = env.step(prev_action)\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the env\n",
    "# WARNING: If you close, you can NOT restart again!!!!!!\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
