{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QGAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    #     print('state, action, reward, done, info')\n",
    "    #     print(state, action, reward, done, info)\n",
    "    if done:\n",
    "    #         print('state, action, reward, done, info')\n",
    "    #         print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given: input data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Current actions given: indices\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    \n",
    "    # Next states given: next input data\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, actions, next_states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder/Classifier\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits: Sqeezed/compressed/represented states into actions size\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(states, actions, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Fuse compressed states (actions fake) with actions (actions real)\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions]) # NxD: axis1=N, and axis2=D\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, actions, next_states, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fuse states, actions, and next states (St, at, St+1) and (St, ~at, ~St+1)\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions, next_states]) # NxD: axis1=N, and axis2=D\n",
    "\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        #         # Fused compressed states, actions, and compressed next_states (all three in action size)\n",
    "        #         #h3 = tf.layers.dense(inputs=nl2, units=action_size)\n",
    "        #         h3_fused = tf.concat(axis=1, values=[states, actions, nl2])\n",
    "        #         bn3 = tf.layers.batch_normalization(h3_fused, training=training)        \n",
    "        #         nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, next_states, targetQs, # model_input\n",
    "               state_size, action_size, hidden_size): # model_init\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(indices=actions, depth=action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    next_states_logits = generator(states=states, actions=actions_real, \n",
    "                                   state_size=state_size, hidden_size=hidden_size)\n",
    "        \n",
    "    # GAN: Discriminate between fake and real\n",
    "    d_logits_fake = discriminator(states=states, actions=actions_real, next_states=next_states_logits, \n",
    "                                  hidden_size=hidden_size, reuse=False)\n",
    "    d_logits_real = discriminator(states=states, actions=actions_real, next_states=next_states, \n",
    "                                  hidden_size=hidden_size, reuse=True)    \n",
    "\n",
    "    # GAN: Adverserial training - G-learning -  Variational AE\n",
    "    g_loss_reconst = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, labels=tf.sigmoid(x=next_states)))\n",
    "    \n",
    "    # GAN: Adverserial training - G-learning -  Relavistic GAN\n",
    "    g_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "    g_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.zeros_like(d_logits_real)))\n",
    "    g_loss = g_loss_real + g_loss_fake + g_loss_reconst\n",
    "    \n",
    "    # GAN: Adverserial training - D-learning-  Standard GAN\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, g_loss, d_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.g_loss, self.d_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, next_states=self.next_states, actions=self.actions, targetQs=self.targetQs) # model input data\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss, g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                                       learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 2000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = QGAN(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 24.0 Average reward fake: 0.5200744867324829 Average reward real: 0.5155307650566101 Training q_loss: 0.3971 Training g_loss: 2.0785 Training d_loss: 1.4308 Explore P: 0.9976\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 19.0 Average reward fake: 0.46569985151290894 Average reward real: 0.4442448318004608 Training q_loss: 1.0494 Training g_loss: 2.0789 Training d_loss: 1.4651 Explore P: 0.9958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 13.0 Average reward fake: 0.40071800351142883 Average reward real: 0.43023544549942017 Training q_loss: 1.4876 Training g_loss: 2.2417 Training d_loss: 1.3910 Explore P: 0.9945\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 12.0 Average reward fake: 0.42454537749290466 Average reward real: 0.46994301676750183 Training q_loss: 2.3990 Training g_loss: 2.2198 Training d_loss: 1.3345 Explore P: 0.9933\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 28.0 Average reward fake: 0.5347908735275269 Average reward real: 0.49373161792755127 Training q_loss: 7.3891 Training g_loss: 1.9946 Training d_loss: 1.4962 Explore P: 0.9905\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 19.0 Average reward fake: 0.5046955347061157 Average reward real: 0.509968101978302 Training q_loss: 11.1245 Training g_loss: 2.1050 Training d_loss: 1.3832 Explore P: 0.9887\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 17.0 Average reward fake: 0.4740077257156372 Average reward real: 0.5385931134223938 Training q_loss: 11.3999 Training g_loss: 2.2183 Training d_loss: 1.2771 Explore P: 0.9870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 13.0 Average reward fake: 0.47984999418258667 Average reward real: 0.5529011487960815 Training q_loss: 16.3008 Training g_loss: 2.2363 Training d_loss: 1.2687 Explore P: 0.9857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 14.0 Average reward fake: 0.544230043888092 Average reward real: 0.5359748601913452 Training q_loss: 7.0877 Training g_loss: 2.0616 Training d_loss: 1.4485 Explore P: 0.9844\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 10.0 Average reward fake: 0.5378240942955017 Average reward real: 0.47306159138679504 Training q_loss: 10.5129 Training g_loss: 1.9611 Training d_loss: 1.5513 Explore P: 0.9834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 16.0 Average reward fake: 0.37490350008010864 Average reward real: 0.4467732906341553 Training q_loss: 9.0811 Training g_loss: 2.2976 Training d_loss: 1.2964 Explore P: 0.9819\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 21.0 Average reward fake: 0.5234778523445129 Average reward real: 0.4870014190673828 Training q_loss: 6.4236 Training g_loss: 2.0024 Training d_loss: 1.4870 Explore P: 0.9798\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 27.0 Average reward fake: 0.4713599681854248 Average reward real: 0.5305532813072205 Training q_loss: 6.4334 Training g_loss: 2.1902 Training d_loss: 1.2819 Explore P: 0.9772\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 15.0 Average reward fake: 0.486974835395813 Average reward real: 0.5269845724105835 Training q_loss: 5.5266 Training g_loss: 2.1437 Training d_loss: 1.3234 Explore P: 0.9757\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 27.0 Average reward fake: 0.5320205092430115 Average reward real: 0.5141772627830505 Training q_loss: 5.0098 Training g_loss: 2.0470 Training d_loss: 1.4450 Explore P: 0.9731\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 13.0 Average reward fake: 0.5125731825828552 Average reward real: 0.514020562171936 Training q_loss: 8.3673 Training g_loss: 2.0811 Training d_loss: 1.4046 Explore P: 0.9719\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 12.0 Average reward fake: 0.4234464168548584 Average reward real: 0.4826413094997406 Training q_loss: 7.7756 Training g_loss: 2.2204 Training d_loss: 1.2982 Explore P: 0.9707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 15.0 Average reward fake: 0.4542635381221771 Average reward real: 0.5096009373664856 Training q_loss: 2.4360 Training g_loss: 2.2040 Training d_loss: 1.3251 Explore P: 0.9693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 28.0 Average reward fake: 0.483134925365448 Average reward real: 0.506005585193634 Training q_loss: 2.4594 Training g_loss: 2.1292 Training d_loss: 1.3750 Explore P: 0.9666\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 16.0 Average reward fake: 0.49660953879356384 Average reward real: 0.5392435193061829 Training q_loss: 6.1762 Training g_loss: 2.1656 Training d_loss: 1.3242 Explore P: 0.9651\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 15.0 Average reward fake: 0.4584135413169861 Average reward real: 0.5118642449378967 Training q_loss: 7.4171 Training g_loss: 2.1870 Training d_loss: 1.2986 Explore P: 0.9637\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 35.0 Average reward fake: 0.5538661479949951 Average reward real: 0.48203033208847046 Training q_loss: 7.0786 Training g_loss: 1.9369 Training d_loss: 1.5868 Explore P: 0.9603\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 13.0 Average reward fake: 0.4600745737552643 Average reward real: 0.4857592284679413 Training q_loss: 2.8739 Training g_loss: 2.1752 Training d_loss: 1.3805 Explore P: 0.9591\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 22.0 Average reward fake: 0.489157497882843 Average reward real: 0.49268093705177307 Training q_loss: 9.1418 Training g_loss: 2.0796 Training d_loss: 1.4199 Explore P: 0.9570\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 33.0 Average reward fake: 0.503702700138092 Average reward real: 0.5316231846809387 Training q_loss: 1.7019 Training g_loss: 2.1306 Training d_loss: 1.3649 Explore P: 0.9539\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 36.0 Average reward fake: 0.48223984241485596 Average reward real: 0.5281516909599304 Training q_loss: 1.8065 Training g_loss: 2.1812 Training d_loss: 1.3374 Explore P: 0.9505\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 12.0 Average reward fake: 0.4767362177371979 Average reward real: 0.4958835542201996 Training q_loss: 9.5504 Training g_loss: 2.1108 Training d_loss: 1.3659 Explore P: 0.9494\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 21.0 Average reward fake: 0.4884396493434906 Average reward real: 0.526884913444519 Training q_loss: 2.9828 Training g_loss: 2.1451 Training d_loss: 1.3417 Explore P: 0.9474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 13.0 Average reward fake: 0.4948626458644867 Average reward real: 0.522544801235199 Training q_loss: 2.5671 Training g_loss: 2.1273 Training d_loss: 1.3606 Explore P: 0.9462\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 27.0 Average reward fake: 0.481627881526947 Average reward real: 0.47575005888938904 Training q_loss: 9.8610 Training g_loss: 2.0521 Training d_loss: 1.4070 Explore P: 0.9437\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 14.0 Average reward fake: 0.4413661062717438 Average reward real: 0.501208484172821 Training q_loss: 3.9369 Training g_loss: 2.1967 Training d_loss: 1.2891 Explore P: 0.9423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 23.0 Average reward fake: 0.5018650889396667 Average reward real: 0.5191513299942017 Training q_loss: 10.2032 Training g_loss: 2.1050 Training d_loss: 1.3723 Explore P: 0.9402\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 21.0 Average reward fake: 0.5055490136146545 Average reward real: 0.48598000407218933 Training q_loss: 7.3683 Training g_loss: 2.0326 Training d_loss: 1.4404 Explore P: 0.9383\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 24.0 Average reward fake: 0.48655518889427185 Average reward real: 0.5175743699073792 Training q_loss: 15.0375 Training g_loss: 2.1297 Training d_loss: 1.3331 Explore P: 0.9360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 18.0 Average reward fake: 0.48527899384498596 Average reward real: 0.523881733417511 Training q_loss: 1.1268 Training g_loss: 2.1464 Training d_loss: 1.3223 Explore P: 0.9344\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 23.0 Average reward fake: 0.4965628683567047 Average reward real: 0.5289430022239685 Training q_loss: 9.2599 Training g_loss: 2.1328 Training d_loss: 1.3342 Explore P: 0.9322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 11.0 Average reward fake: 0.5065081119537354 Average reward real: 0.5202746987342834 Training q_loss: 0.8977 Training g_loss: 2.0935 Training d_loss: 1.3830 Explore P: 0.9312\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 13.0 Average reward fake: 0.4370381236076355 Average reward real: 0.478149950504303 Training q_loss: 6.0287 Training g_loss: 2.1698 Training d_loss: 1.3290 Explore P: 0.9300\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 18.0 Average reward fake: 0.5070946216583252 Average reward real: 0.515261173248291 Training q_loss: 3.9011 Training g_loss: 2.0856 Training d_loss: 1.3835 Explore P: 0.9284\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 16.0 Average reward fake: 0.48957955837249756 Average reward real: 0.513335645198822 Training q_loss: 0.8924 Training g_loss: 2.1133 Training d_loss: 1.3631 Explore P: 0.9269\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 15.0 Average reward fake: 0.43822920322418213 Average reward real: 0.4822956323623657 Training q_loss: 0.5676 Training g_loss: 2.1732 Training d_loss: 1.3301 Explore P: 0.9255\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 52.0 Average reward fake: 0.4496556520462036 Average reward real: 0.47832778096199036 Training q_loss: 0.7644 Training g_loss: 2.1366 Training d_loss: 1.3541 Explore P: 0.9208\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 9.0 Average reward fake: 0.47542449831962585 Average reward real: 0.5371997952461243 Training q_loss: 2.4967 Training g_loss: 2.2149 Training d_loss: 1.2979 Explore P: 0.9200\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 55.0 Average reward fake: 0.45379021763801575 Average reward real: 0.5445911288261414 Training q_loss: 1.6417 Training g_loss: 2.2730 Training d_loss: 1.2395 Explore P: 0.9150\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 16.0 Average reward fake: 0.46761536598205566 Average reward real: 0.5215738415718079 Training q_loss: 2.7669 Training g_loss: 2.1985 Training d_loss: 1.3244 Explore P: 0.9135\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 24.0 Average reward fake: 0.46057745814323425 Average reward real: 0.5457853078842163 Training q_loss: 1.4432 Training g_loss: 2.2647 Training d_loss: 1.2421 Explore P: 0.9114\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 48.0 Average reward fake: 0.4491208791732788 Average reward real: 0.5697070956230164 Training q_loss: 0.7011 Training g_loss: 2.3521 Training d_loss: 1.1893 Explore P: 0.9070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 13.0 Average reward fake: 0.43326106667518616 Average reward real: 0.5530596971511841 Training q_loss: 0.9087 Training g_loss: 2.3443 Training d_loss: 1.2015 Explore P: 0.9059\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 42.0 Average reward fake: 0.43280670046806335 Average reward real: 0.5641851425170898 Training q_loss: 2.2203 Training g_loss: 2.3937 Training d_loss: 1.1676 Explore P: 0.9021\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 26.0 Average reward fake: 0.47775718569755554 Average reward real: 0.48542526364326477 Training q_loss: 0.3850 Training g_loss: 2.1230 Training d_loss: 1.4034 Explore P: 0.8998\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 22.0 Average reward fake: 0.45640021562576294 Average reward real: 0.5101336240768433 Training q_loss: 1.1001 Training g_loss: 2.2173 Training d_loss: 1.3248 Explore P: 0.8979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 16.0 Average reward fake: 0.5243866443634033 Average reward real: 0.471365362405777 Training q_loss: 0.5676 Training g_loss: 2.0424 Training d_loss: 1.5337 Explore P: 0.8964\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 39.0 Average reward fake: 0.4547293782234192 Average reward real: 0.5494869351387024 Training q_loss: 0.2559 Training g_loss: 2.3311 Training d_loss: 1.2436 Explore P: 0.8930\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 12.0 Average reward fake: 0.4980222284793854 Average reward real: 0.5050764679908752 Training q_loss: 0.8333 Training g_loss: 2.1686 Training d_loss: 1.4453 Explore P: 0.8919\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 24.0 Average reward fake: 0.45390549302101135 Average reward real: 0.533775806427002 Training q_loss: 1.4714 Training g_loss: 2.3158 Training d_loss: 1.2609 Explore P: 0.8898\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 7.0 Average reward fake: 0.463998407125473 Average reward real: 0.5380253791809082 Training q_loss: 0.8748 Training g_loss: 2.3105 Training d_loss: 1.2831 Explore P: 0.8892\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 36.0 Average reward fake: 0.4296087920665741 Average reward real: 0.5531471371650696 Training q_loss: 0.6558 Training g_loss: 2.4254 Training d_loss: 1.1752 Explore P: 0.8860\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 37.0 Average reward fake: 0.48174354434013367 Average reward real: 0.4637610614299774 Training q_loss: 0.2721 Training g_loss: 2.1341 Training d_loss: 1.4618 Explore P: 0.8828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 15.0 Average reward fake: 0.41922253370285034 Average reward real: 0.5304874181747437 Training q_loss: 0.8399 Training g_loss: 2.3995 Training d_loss: 1.2084 Explore P: 0.8815\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 19.0 Average reward fake: 0.4882711172103882 Average reward real: 0.5052107572555542 Training q_loss: 0.3338 Training g_loss: 2.1776 Training d_loss: 1.4064 Explore P: 0.8798\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 23.0 Average reward fake: 0.43149977922439575 Average reward real: 0.5282362103462219 Training q_loss: 0.3891 Training g_loss: 2.4556 Training d_loss: 1.2688 Explore P: 0.8778\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 24.0 Average reward fake: 0.469196617603302 Average reward real: 0.5384967923164368 Training q_loss: 1.2126 Training g_loss: 2.3441 Training d_loss: 1.3040 Explore P: 0.8758\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 30.0 Average reward fake: 0.4687986373901367 Average reward real: 0.48058807849884033 Training q_loss: 0.4994 Training g_loss: 2.3488 Training d_loss: 1.4347 Explore P: 0.8732\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 34.0 Average reward fake: 0.40593722462654114 Average reward real: 0.55034339427948 Training q_loss: 1.0472 Training g_loss: 2.5318 Training d_loss: 1.1787 Explore P: 0.8702\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 25.0 Average reward fake: 0.4263581931591034 Average reward real: 0.5872806310653687 Training q_loss: 0.7999 Training g_loss: 2.5557 Training d_loss: 1.1384 Explore P: 0.8681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 31.0 Average reward fake: 0.45408594608306885 Average reward real: 0.5377202033996582 Training q_loss: 1.3631 Training g_loss: 2.4663 Training d_loss: 1.2803 Explore P: 0.8654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 42.0 Average reward fake: 0.4566604495048523 Average reward real: 0.5496441721916199 Training q_loss: 0.2125 Training g_loss: 2.2860 Training d_loss: 1.2567 Explore P: 0.8618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 25.0 Average reward fake: 0.47471198439598083 Average reward real: 0.5250693559646606 Training q_loss: 0.2098 Training g_loss: 2.2767 Training d_loss: 1.3196 Explore P: 0.8597\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 13.0 Average reward fake: 0.4406543970108032 Average reward real: 0.5577551126480103 Training q_loss: 0.5817 Training g_loss: 2.5491 Training d_loss: 1.2045 Explore P: 0.8586\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 17.0 Average reward fake: 0.5072810649871826 Average reward real: 0.5231478214263916 Training q_loss: 0.2091 Training g_loss: 2.2585 Training d_loss: 1.3817 Explore P: 0.8572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 23.0 Average reward fake: 0.44214165210723877 Average reward real: 0.5665960907936096 Training q_loss: 1.2407 Training g_loss: 2.6482 Training d_loss: 1.2034 Explore P: 0.8552\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 10.0 Average reward fake: 0.45780280232429504 Average reward real: 0.5247656106948853 Training q_loss: 0.3758 Training g_loss: 2.4026 Training d_loss: 1.2986 Explore P: 0.8544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 16.0 Average reward fake: 0.4511703848838806 Average reward real: 0.5136951208114624 Training q_loss: 0.8563 Training g_loss: 2.4644 Training d_loss: 1.3146 Explore P: 0.8530\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 26.0 Average reward fake: 0.47177329659461975 Average reward real: 0.5156996250152588 Training q_loss: 1.4707 Training g_loss: 2.3462 Training d_loss: 1.3564 Explore P: 0.8508\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 17.0 Average reward fake: 0.4543392062187195 Average reward real: 0.549964189529419 Training q_loss: 0.2701 Training g_loss: 2.4157 Training d_loss: 1.2416 Explore P: 0.8494\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 9.0 Average reward fake: 0.42290228605270386 Average reward real: 0.5189347863197327 Training q_loss: 0.1442 Training g_loss: 2.3484 Training d_loss: 1.2421 Explore P: 0.8487\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 9.0 Average reward fake: 0.45394256711006165 Average reward real: 0.5149178504943848 Training q_loss: 0.9275 Training g_loss: 2.4756 Training d_loss: 1.3259 Explore P: 0.8479\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 10.0 Average reward fake: 0.49765241146087646 Average reward real: 0.4809591770172119 Training q_loss: 0.3318 Training g_loss: 2.2848 Training d_loss: 1.4865 Explore P: 0.8471\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 14.0 Average reward fake: 0.4229624271392822 Average reward real: 0.5392028093338013 Training q_loss: 0.8337 Training g_loss: 2.5741 Training d_loss: 1.2248 Explore P: 0.8459\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 12.0 Average reward fake: 0.4465400278568268 Average reward real: 0.5422605276107788 Training q_loss: 0.9483 Training g_loss: 2.5167 Training d_loss: 1.2751 Explore P: 0.8449\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 15.0 Average reward fake: 0.4911158084869385 Average reward real: 0.5187854766845703 Training q_loss: 0.4728 Training g_loss: 2.4322 Training d_loss: 1.4164 Explore P: 0.8436\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 24.0 Average reward fake: 0.4420822560787201 Average reward real: 0.5247328281402588 Training q_loss: 0.4772 Training g_loss: 2.5932 Training d_loss: 1.2924 Explore P: 0.8416\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 10.0 Average reward fake: 0.4872647821903229 Average reward real: 0.503876805305481 Training q_loss: 0.5889 Training g_loss: 2.3137 Training d_loss: 1.4110 Explore P: 0.8408\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 27.0 Average reward fake: 0.46854886412620544 Average reward real: 0.5035962462425232 Training q_loss: 0.5253 Training g_loss: 2.2491 Training d_loss: 1.3640 Explore P: 0.8386\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 9.0 Average reward fake: 0.4943842589855194 Average reward real: 0.5083208084106445 Training q_loss: 0.2089 Training g_loss: 2.2516 Training d_loss: 1.4109 Explore P: 0.8378\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 17.0 Average reward fake: 0.43190237879753113 Average reward real: 0.5532472133636475 Training q_loss: 0.8509 Training g_loss: 2.6023 Training d_loss: 1.1917 Explore P: 0.8364\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 43.0 Average reward fake: 0.40127846598625183 Average reward real: 0.5844572186470032 Training q_loss: 1.4163 Training g_loss: 2.7410 Training d_loss: 1.0960 Explore P: 0.8329\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 26.0 Average reward fake: 0.5164336562156677 Average reward real: 0.5332896113395691 Training q_loss: 0.5909 Training g_loss: 2.4951 Training d_loss: 1.4025 Explore P: 0.8307\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 15.0 Average reward fake: 0.42087143659591675 Average reward real: 0.5665709972381592 Training q_loss: 0.7755 Training g_loss: 2.7336 Training d_loss: 1.1639 Explore P: 0.8295\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 19.0 Average reward fake: 0.49241089820861816 Average reward real: 0.49275246262550354 Training q_loss: 1.8624 Training g_loss: 2.3495 Training d_loss: 1.4467 Explore P: 0.8280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 24.0 Average reward fake: 0.4246869683265686 Average reward real: 0.5373129844665527 Training q_loss: 0.3055 Training g_loss: 2.4722 Training d_loss: 1.2143 Explore P: 0.8260\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 21.0 Average reward fake: 0.5144482254981995 Average reward real: 0.47300660610198975 Training q_loss: 0.1920 Training g_loss: 2.1317 Training d_loss: 1.5151 Explore P: 0.8243\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 14.0 Average reward fake: 0.4931409955024719 Average reward real: 0.5119316577911377 Training q_loss: 0.3087 Training g_loss: 2.2799 Training d_loss: 1.3839 Explore P: 0.8231\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 19.0 Average reward fake: 0.4379398822784424 Average reward real: 0.5523155927658081 Training q_loss: 0.1325 Training g_loss: 2.6973 Training d_loss: 1.2215 Explore P: 0.8216\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 11.0 Average reward fake: 0.486075222492218 Average reward real: 0.4613844156265259 Training q_loss: 0.1940 Training g_loss: 2.1639 Training d_loss: 1.5194 Explore P: 0.8207\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 46.0 Average reward fake: 0.47599655389785767 Average reward real: 0.5048449039459229 Training q_loss: 0.4612 Training g_loss: 2.3060 Training d_loss: 1.3669 Explore P: 0.8170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 12.0 Average reward fake: 0.5353546142578125 Average reward real: 0.4877893030643463 Training q_loss: 0.4460 Training g_loss: 2.2474 Training d_loss: 1.5140 Explore P: 0.8160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 16.0 Average reward fake: 0.453205406665802 Average reward real: 0.5591505169868469 Training q_loss: 1.5727 Training g_loss: 2.6137 Training d_loss: 1.2499 Explore P: 0.8147\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 16.0 Average reward fake: 0.4355427920818329 Average reward real: 0.5250793099403381 Training q_loss: 0.2735 Training g_loss: 2.4194 Training d_loss: 1.2652 Explore P: 0.8134\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 22.0 Average reward fake: 0.5105282068252563 Average reward real: 0.4932546317577362 Training q_loss: 0.8059 Training g_loss: 2.3577 Training d_loss: 1.4778 Explore P: 0.8117\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 33.0 Average reward fake: 0.4933009743690491 Average reward real: 0.5002899765968323 Training q_loss: 0.2668 Training g_loss: 2.3345 Training d_loss: 1.4470 Explore P: 0.8090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 50.0 Average reward fake: 0.5021165609359741 Average reward real: 0.5356541275978088 Training q_loss: 0.4910 Training g_loss: 2.5626 Training d_loss: 1.3730 Explore P: 0.8050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 12.0 Average reward fake: 0.5043413043022156 Average reward real: 0.5064895749092102 Training q_loss: 0.2939 Training g_loss: 2.2767 Training d_loss: 1.4394 Explore P: 0.8041\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 13.0 Average reward fake: 0.4257601499557495 Average reward real: 0.5660344362258911 Training q_loss: 1.1175 Training g_loss: 2.6201 Training d_loss: 1.1710 Explore P: 0.8031\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 9.0 Average reward fake: 0.44285285472869873 Average reward real: 0.5704009532928467 Training q_loss: 0.6161 Training g_loss: 2.6243 Training d_loss: 1.1949 Explore P: 0.8024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 25.0 Average reward fake: 0.4862240254878998 Average reward real: 0.5285516977310181 Training q_loss: 0.0958 Training g_loss: 2.2896 Training d_loss: 1.3688 Explore P: 0.8004\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 12.0 Average reward fake: 0.4366154372692108 Average reward real: 0.5641709566116333 Training q_loss: 0.1460 Training g_loss: 2.5980 Training d_loss: 1.1945 Explore P: 0.7994\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 14.0 Average reward fake: 0.4965359568595886 Average reward real: 0.5006274580955505 Training q_loss: 0.3671 Training g_loss: 2.3867 Training d_loss: 1.4347 Explore P: 0.7983\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 20.0 Average reward fake: 0.45673632621765137 Average reward real: 0.5316943526268005 Training q_loss: 0.3187 Training g_loss: 2.4020 Training d_loss: 1.2896 Explore P: 0.7967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 26.0 Average reward fake: 0.47055894136428833 Average reward real: 0.5243493914604187 Training q_loss: 2.1917 Training g_loss: 2.5171 Training d_loss: 1.3178 Explore P: 0.7947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 29.0 Average reward fake: 0.4444727301597595 Average reward real: 0.558824360370636 Training q_loss: 0.5020 Training g_loss: 2.7940 Training d_loss: 1.2357 Explore P: 0.7924\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 34.0 Average reward fake: 0.46660637855529785 Average reward real: 0.4890628755092621 Training q_loss: 0.5276 Training g_loss: 2.4287 Training d_loss: 1.3877 Explore P: 0.7898\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 35.0 Average reward fake: 0.43594521284103394 Average reward real: 0.5594529509544373 Training q_loss: 0.1856 Training g_loss: 2.5706 Training d_loss: 1.2026 Explore P: 0.7871\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 17.0 Average reward fake: 0.4854469299316406 Average reward real: 0.5021051168441772 Training q_loss: 1.3959 Training g_loss: 2.4478 Training d_loss: 1.4214 Explore P: 0.7857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 29.0 Average reward fake: 0.4195106625556946 Average reward real: 0.5571998357772827 Training q_loss: 0.4556 Training g_loss: 2.8897 Training d_loss: 1.1927 Explore P: 0.7835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 8.0 Average reward fake: 0.46171805262565613 Average reward real: 0.5668939352035522 Training q_loss: 1.3823 Training g_loss: 3.0199 Training d_loss: 1.2512 Explore P: 0.7829\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 39.0 Average reward fake: 0.421866774559021 Average reward real: 0.5698539614677429 Training q_loss: 1.3660 Training g_loss: 3.1337 Training d_loss: 1.1711 Explore P: 0.7799\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 42.0 Average reward fake: 0.4375053346157074 Average reward real: 0.5145783424377441 Training q_loss: 0.5039 Training g_loss: 2.6944 Training d_loss: 1.3009 Explore P: 0.7766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 25.0 Average reward fake: 0.46482476592063904 Average reward real: 0.530358076095581 Training q_loss: 0.5512 Training g_loss: 2.5165 Training d_loss: 1.2988 Explore P: 0.7747\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 18.0 Average reward fake: 0.4980250597000122 Average reward real: 0.4664604067802429 Training q_loss: 0.9337 Training g_loss: 2.5343 Training d_loss: 1.5036 Explore P: 0.7733\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 32.0 Average reward fake: 0.4118622839450836 Average reward real: 0.5504452586174011 Training q_loss: 0.9494 Training g_loss: 2.6676 Training d_loss: 1.1787 Explore P: 0.7709\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 13.0 Average reward fake: 0.49620234966278076 Average reward real: 0.4869767129421234 Training q_loss: 0.8648 Training g_loss: 2.6109 Training d_loss: 1.4802 Explore P: 0.7699\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 19.0 Average reward fake: 0.41083401441574097 Average reward real: 0.5545594096183777 Training q_loss: 1.0457 Training g_loss: 2.6566 Training d_loss: 1.1594 Explore P: 0.7685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 31.0 Average reward fake: 0.4960794150829315 Average reward real: 0.4446731209754944 Training q_loss: 0.3505 Training g_loss: 2.3409 Training d_loss: 1.5833 Explore P: 0.7661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 15.0 Average reward fake: 0.457783043384552 Average reward real: 0.5922538042068481 Training q_loss: 0.1516 Training g_loss: 2.5051 Training d_loss: 1.1485 Explore P: 0.7650\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 21.0 Average reward fake: 0.48084479570388794 Average reward real: 0.5179446935653687 Training q_loss: 1.1085 Training g_loss: 2.7428 Training d_loss: 1.3784 Explore P: 0.7634\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 27.0 Average reward fake: 0.4504476487636566 Average reward real: 0.5371497273445129 Training q_loss: 0.2650 Training g_loss: 2.6252 Training d_loss: 1.2875 Explore P: 0.7614\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 49.0 Average reward fake: 0.4806743562221527 Average reward real: 0.4982454776763916 Training q_loss: 0.4064 Training g_loss: 2.2825 Training d_loss: 1.3991 Explore P: 0.7577\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 16.0 Average reward fake: 0.4583266079425812 Average reward real: 0.5457600355148315 Training q_loss: 0.3370 Training g_loss: 2.5899 Training d_loss: 1.2608 Explore P: 0.7565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 17.0 Average reward fake: 0.4769038259983063 Average reward real: 0.4926314949989319 Training q_loss: 0.1425 Training g_loss: 2.2319 Training d_loss: 1.3953 Explore P: 0.7552\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 46.0 Average reward fake: 0.4626717269420624 Average reward real: 0.5170350670814514 Training q_loss: 0.3847 Training g_loss: 2.4813 Training d_loss: 1.3499 Explore P: 0.7518\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 14.0 Average reward fake: 0.4599956274032593 Average reward real: 0.49409303069114685 Training q_loss: 0.9895 Training g_loss: 2.4774 Training d_loss: 1.3507 Explore P: 0.7508\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 17.0 Average reward fake: 0.45847469568252563 Average reward real: 0.555398166179657 Training q_loss: 0.9251 Training g_loss: 2.7204 Training d_loss: 1.2469 Explore P: 0.7495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 18.0 Average reward fake: 0.4869421124458313 Average reward real: 0.4757993221282959 Training q_loss: 0.5833 Training g_loss: 2.6008 Training d_loss: 1.4898 Explore P: 0.7482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 26.0 Average reward fake: 0.419411838054657 Average reward real: 0.5762543678283691 Training q_loss: 1.3298 Training g_loss: 2.7751 Training d_loss: 1.1301 Explore P: 0.7463\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 32.0 Average reward fake: 0.431408166885376 Average reward real: 0.48437264561653137 Training q_loss: 0.5666 Training g_loss: 2.7837 Training d_loss: 1.3198 Explore P: 0.7439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 13.0 Average reward fake: 0.44008922576904297 Average reward real: 0.5964531898498535 Training q_loss: 0.2112 Training g_loss: 2.6961 Training d_loss: 1.1183 Explore P: 0.7430\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 13.0 Average reward fake: 0.4417385756969452 Average reward real: 0.5187563896179199 Training q_loss: 0.2454 Training g_loss: 2.6921 Training d_loss: 1.2812 Explore P: 0.7420\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 30.0 Average reward fake: 0.3485381305217743 Average reward real: 0.5803329944610596 Training q_loss: 0.2450 Training g_loss: 3.1985 Training d_loss: 1.0520 Explore P: 0.7398\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 33.0 Average reward fake: 0.4383338987827301 Average reward real: 0.6287384033203125 Training q_loss: 0.2421 Training g_loss: 2.9405 Training d_loss: 1.1209 Explore P: 0.7374\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 12.0 Average reward fake: 0.4460321068763733 Average reward real: 0.5467967391014099 Training q_loss: 0.3585 Training g_loss: 2.6301 Training d_loss: 1.2815 Explore P: 0.7365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 10.0 Average reward fake: 0.4899924397468567 Average reward real: 0.5239007472991943 Training q_loss: 0.6069 Training g_loss: 2.6568 Training d_loss: 1.3719 Explore P: 0.7358\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 14.0 Average reward fake: 0.40264594554901123 Average reward real: 0.494620680809021 Training q_loss: 0.7939 Training g_loss: 2.6918 Training d_loss: 1.2779 Explore P: 0.7348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 65.0 Average reward fake: 0.4611605703830719 Average reward real: 0.5382378697395325 Training q_loss: 1.3395 Training g_loss: 2.5953 Training d_loss: 1.3328 Explore P: 0.7301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 49.0 Average reward fake: 0.46783578395843506 Average reward real: 0.5502386689186096 Training q_loss: 0.6796 Training g_loss: 2.7864 Training d_loss: 1.3259 Explore P: 0.7266\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 20.0 Average reward fake: 0.4324398338794708 Average reward real: 0.5526463389396667 Training q_loss: 0.6075 Training g_loss: 2.6583 Training d_loss: 1.2196 Explore P: 0.7252\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 11.0 Average reward fake: 0.48349258303642273 Average reward real: 0.49707281589508057 Training q_loss: 0.5668 Training g_loss: 2.5490 Training d_loss: 1.4991 Explore P: 0.7244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 18.0 Average reward fake: 0.45748549699783325 Average reward real: 0.5412343144416809 Training q_loss: 0.2343 Training g_loss: 2.7652 Training d_loss: 1.2667 Explore P: 0.7231\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 38.0 Average reward fake: 0.4852331578731537 Average reward real: 0.5311226844787598 Training q_loss: 0.3970 Training g_loss: 2.6183 Training d_loss: 1.4136 Explore P: 0.7204\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 15.0 Average reward fake: 0.35976460576057434 Average reward real: 0.5416024923324585 Training q_loss: 0.8692 Training g_loss: 3.2713 Training d_loss: 1.1474 Explore P: 0.7193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 11.0 Average reward fake: 0.4186723232269287 Average reward real: 0.5596824884414673 Training q_loss: 1.9876 Training g_loss: 2.9961 Training d_loss: 1.1954 Explore P: 0.7185\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 12.0 Average reward fake: 0.4869838058948517 Average reward real: 0.5258369445800781 Training q_loss: 0.5236 Training g_loss: 2.4825 Training d_loss: 1.3372 Explore P: 0.7177\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 12.0 Average reward fake: 0.4809877872467041 Average reward real: 0.5874828100204468 Training q_loss: 0.9919 Training g_loss: 3.3114 Training d_loss: 1.1897 Explore P: 0.7168\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 20.0 Average reward fake: 0.4663536846637726 Average reward real: 0.5592523217201233 Training q_loss: 0.8131 Training g_loss: 2.4336 Training d_loss: 1.2990 Explore P: 0.7154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 16.0 Average reward fake: 0.4748919606208801 Average reward real: 0.5384747385978699 Training q_loss: 1.0531 Training g_loss: 2.9124 Training d_loss: 1.3036 Explore P: 0.7143\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 38.0 Average reward fake: 0.5167251825332642 Average reward real: 0.5083457231521606 Training q_loss: 1.4127 Training g_loss: 2.6138 Training d_loss: 1.5277 Explore P: 0.7116\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 12.0 Average reward fake: 0.328919917345047 Average reward real: 0.5962437987327576 Training q_loss: 1.0904 Training g_loss: 3.1545 Training d_loss: 0.9870 Explore P: 0.7108\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 15.0 Average reward fake: 0.46400177478790283 Average reward real: 0.5149528980255127 Training q_loss: 0.6907 Training g_loss: 2.4710 Training d_loss: 1.3919 Explore P: 0.7097\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 21.0 Average reward fake: 0.4089610278606415 Average reward real: 0.560935378074646 Training q_loss: 0.1805 Training g_loss: 2.7986 Training d_loss: 1.1985 Explore P: 0.7083\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 16.0 Average reward fake: 0.45888352394104004 Average reward real: 0.5195882320404053 Training q_loss: 0.3717 Training g_loss: 2.6509 Training d_loss: 1.3725 Explore P: 0.7072\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 13.0 Average reward fake: 0.4224879741668701 Average reward real: 0.5209288001060486 Training q_loss: 1.0615 Training g_loss: 2.8613 Training d_loss: 1.2892 Explore P: 0.7062\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 18.0 Average reward fake: 0.4472801983356476 Average reward real: 0.5658201575279236 Training q_loss: 0.9018 Training g_loss: 2.8803 Training d_loss: 1.2210 Explore P: 0.7050\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 14.0 Average reward fake: 0.43842360377311707 Average reward real: 0.5960349440574646 Training q_loss: 0.3886 Training g_loss: 3.2362 Training d_loss: 1.1899 Explore P: 0.7040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 12.0 Average reward fake: 0.441396027803421 Average reward real: 0.4977227747440338 Training q_loss: 0.1602 Training g_loss: 2.5612 Training d_loss: 1.3844 Explore P: 0.7032\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 15.0 Average reward fake: 0.3802146017551422 Average reward real: 0.6242033839225769 Training q_loss: 1.0448 Training g_loss: 3.2853 Training d_loss: 1.0089 Explore P: 0.7022\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 21.0 Average reward fake: 0.5125498175621033 Average reward real: 0.5101760029792786 Training q_loss: 0.6862 Training g_loss: 2.7593 Training d_loss: 1.4806 Explore P: 0.7007\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 29.0 Average reward fake: 0.4089947044849396 Average reward real: 0.4736410081386566 Training q_loss: 0.6005 Training g_loss: 2.7251 Training d_loss: 1.3836 Explore P: 0.6987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 8.0 Average reward fake: 0.432871013879776 Average reward real: 0.4792068600654602 Training q_loss: 0.2381 Training g_loss: 2.8013 Training d_loss: 1.4539 Explore P: 0.6981\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 20.0 Average reward fake: 0.3578466773033142 Average reward real: 0.591045081615448 Training q_loss: 0.8346 Training g_loss: 3.1392 Training d_loss: 1.0606 Explore P: 0.6968\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 13.0 Average reward fake: 0.3402538001537323 Average reward real: 0.5655583739280701 Training q_loss: 1.1310 Training g_loss: 3.0457 Training d_loss: 1.1349 Explore P: 0.6959\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 9.0 Average reward fake: 0.45606106519699097 Average reward real: 0.5358110070228577 Training q_loss: 0.2376 Training g_loss: 2.9860 Training d_loss: 1.4040 Explore P: 0.6953\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 8.0 Average reward fake: 0.47450995445251465 Average reward real: 0.5329594612121582 Training q_loss: 0.6710 Training g_loss: 2.4766 Training d_loss: 1.3723 Explore P: 0.6947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 17.0 Average reward fake: 0.38685718178749084 Average reward real: 0.6576513051986694 Training q_loss: 1.1262 Training g_loss: 3.1047 Training d_loss: 1.0165 Explore P: 0.6936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 22.0 Average reward fake: 0.49332964420318604 Average reward real: 0.4849025011062622 Training q_loss: 0.1398 Training g_loss: 2.6739 Training d_loss: 1.5169 Explore P: 0.6921\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 19.0 Average reward fake: 0.35749390721321106 Average reward real: 0.622056245803833 Training q_loss: 0.4063 Training g_loss: 3.4015 Training d_loss: 1.0081 Explore P: 0.6908\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 9.0 Average reward fake: 0.41170939803123474 Average reward real: 0.5510053038597107 Training q_loss: 0.2243 Training g_loss: 3.0122 Training d_loss: 1.2040 Explore P: 0.6901\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 16.0 Average reward fake: 0.4020999073982239 Average reward real: 0.52262282371521 Training q_loss: 0.4746 Training g_loss: 3.2366 Training d_loss: 1.2705 Explore P: 0.6891\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 15.0 Average reward fake: 0.4495772421360016 Average reward real: 0.7043304443359375 Training q_loss: 1.1427 Training g_loss: 3.5591 Training d_loss: 1.0200 Explore P: 0.6880\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 17.0 Average reward fake: 0.46769627928733826 Average reward real: 0.6048681139945984 Training q_loss: 0.3705 Training g_loss: 3.2265 Training d_loss: 1.2441 Explore P: 0.6869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 21.0 Average reward fake: 0.4559498131275177 Average reward real: 0.5939583778381348 Training q_loss: 0.3373 Training g_loss: 2.8907 Training d_loss: 1.2648 Explore P: 0.6855\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 30.0 Average reward fake: 0.4830459952354431 Average reward real: 0.4708716571331024 Training q_loss: 0.7315 Training g_loss: 2.5360 Training d_loss: 1.4699 Explore P: 0.6834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 9.0 Average reward fake: 0.3713647127151489 Average reward real: 0.5788273215293884 Training q_loss: 0.3937 Training g_loss: 3.0164 Training d_loss: 1.0621 Explore P: 0.6828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 22.0 Average reward fake: 0.49366050958633423 Average reward real: 0.5236868262290955 Training q_loss: 0.8028 Training g_loss: 2.7799 Training d_loss: 1.4276 Explore P: 0.6814\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 14.0 Average reward fake: 0.41463059186935425 Average reward real: 0.5819242000579834 Training q_loss: 0.6201 Training g_loss: 3.3992 Training d_loss: 1.1617 Explore P: 0.6804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 24.0 Average reward fake: 0.3677426278591156 Average reward real: 0.5334632396697998 Training q_loss: 0.2607 Training g_loss: 3.0796 Training d_loss: 1.1191 Explore P: 0.6788\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 18.0 Average reward fake: 0.29863280057907104 Average reward real: 0.547142744064331 Training q_loss: 1.3290 Training g_loss: 3.2555 Training d_loss: 1.1042 Explore P: 0.6776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 15.0 Average reward fake: 0.34642401337623596 Average reward real: 0.6014474630355835 Training q_loss: 0.1141 Training g_loss: 2.9255 Training d_loss: 1.0604 Explore P: 0.6766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 11.0 Average reward fake: 0.440654993057251 Average reward real: 0.5065370798110962 Training q_loss: 0.1792 Training g_loss: 2.8252 Training d_loss: 1.2946 Explore P: 0.6759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 26.0 Average reward fake: 0.368211567401886 Average reward real: 0.6025816202163696 Training q_loss: 0.2503 Training g_loss: 3.2446 Training d_loss: 1.1223 Explore P: 0.6741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 28.0 Average reward fake: 0.34100207686424255 Average reward real: 0.6175645589828491 Training q_loss: 0.2807 Training g_loss: 3.5280 Training d_loss: 1.0052 Explore P: 0.6723\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 10.0 Average reward fake: 0.46381163597106934 Average reward real: 0.6476325988769531 Training q_loss: 0.7503 Training g_loss: 3.2736 Training d_loss: 1.1688 Explore P: 0.6716\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 18.0 Average reward fake: 0.47535061836242676 Average reward real: 0.5393345355987549 Training q_loss: 1.1598 Training g_loss: 3.0967 Training d_loss: 1.3016 Explore P: 0.6704\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 15.0 Average reward fake: 0.3741205334663391 Average reward real: 0.44038885831832886 Training q_loss: 0.4822 Training g_loss: 2.8038 Training d_loss: 1.3826 Explore P: 0.6694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 34.0 Average reward fake: 0.4108961224555969 Average reward real: 0.6394820213317871 Training q_loss: 0.9626 Training g_loss: 3.3744 Training d_loss: 1.1255 Explore P: 0.6672\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 30.0 Average reward fake: 0.3804660737514496 Average reward real: 0.56988525390625 Training q_loss: 1.2150 Training g_loss: 3.1061 Training d_loss: 1.1853 Explore P: 0.6652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 40.0 Average reward fake: 0.3832664489746094 Average reward real: 0.5611419081687927 Training q_loss: 0.7025 Training g_loss: 2.8643 Training d_loss: 1.1994 Explore P: 0.6626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 15.0 Average reward fake: 0.4040328860282898 Average reward real: 0.5386203527450562 Training q_loss: 0.3797 Training g_loss: 3.0314 Training d_loss: 1.2144 Explore P: 0.6616\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 13.0 Average reward fake: 0.4308468997478485 Average reward real: 0.5906564593315125 Training q_loss: 1.6959 Training g_loss: 3.0333 Training d_loss: 1.3432 Explore P: 0.6608\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 34.0 Average reward fake: 0.42819175124168396 Average reward real: 0.6421132683753967 Training q_loss: 0.2241 Training g_loss: 3.0365 Training d_loss: 1.1275 Explore P: 0.6586\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 21.0 Average reward fake: 0.39959198236465454 Average reward real: 0.4929666817188263 Training q_loss: 0.7737 Training g_loss: 3.0948 Training d_loss: 1.3780 Explore P: 0.6572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 10.0 Average reward fake: 0.2963751554489136 Average reward real: 0.6491920948028564 Training q_loss: 1.3567 Training g_loss: 3.6113 Training d_loss: 0.9096 Explore P: 0.6566\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 37.0 Average reward fake: 0.34930840134620667 Average reward real: 0.5226500630378723 Training q_loss: 0.4579 Training g_loss: 3.0538 Training d_loss: 1.1661 Explore P: 0.6542\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 19.0 Average reward fake: 0.3340374827384949 Average reward real: 0.7351486682891846 Training q_loss: 1.1807 Training g_loss: 4.2858 Training d_loss: 0.9462 Explore P: 0.6530\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 14.0 Average reward fake: 0.3544212281703949 Average reward real: 0.46717751026153564 Training q_loss: 0.3806 Training g_loss: 3.1283 Training d_loss: 1.3480 Explore P: 0.6521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 13.0 Average reward fake: 0.41230323910713196 Average reward real: 0.6840994954109192 Training q_loss: 0.3627 Training g_loss: 3.4777 Training d_loss: 0.8962 Explore P: 0.6512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 14.0 Average reward fake: 0.4694117605686188 Average reward real: 0.6446467041969299 Training q_loss: 0.4824 Training g_loss: 3.0180 Training d_loss: 1.2529 Explore P: 0.6503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 30.0 Average reward fake: 0.39867743849754333 Average reward real: 0.7897014617919922 Training q_loss: 0.5369 Training g_loss: 3.6734 Training d_loss: 0.9305 Explore P: 0.6484\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 24.0 Average reward fake: 0.41908499598503113 Average reward real: 0.5844457149505615 Training q_loss: 0.8144 Training g_loss: 3.3151 Training d_loss: 1.1845 Explore P: 0.6469\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 10.0 Average reward fake: 0.5167739987373352 Average reward real: 0.7361505031585693 Training q_loss: 0.7835 Training g_loss: 3.3410 Training d_loss: 1.1806 Explore P: 0.6463\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 15.0 Average reward fake: 0.32887816429138184 Average reward real: 0.5917524695396423 Training q_loss: 0.4056 Training g_loss: 3.2572 Training d_loss: 0.9627 Explore P: 0.6453\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 9.0 Average reward fake: 0.38035961985588074 Average reward real: 0.5170137882232666 Training q_loss: 0.7836 Training g_loss: 3.1762 Training d_loss: 1.3282 Explore P: 0.6447\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 12.0 Average reward fake: 0.2809331715106964 Average reward real: 0.6039592027664185 Training q_loss: 0.2670 Training g_loss: 3.7816 Training d_loss: 0.9188 Explore P: 0.6440\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 17.0 Average reward fake: 0.34991616010665894 Average reward real: 0.5745844841003418 Training q_loss: 1.8599 Training g_loss: 3.2404 Training d_loss: 1.1447 Explore P: 0.6429\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 9.0 Average reward fake: 0.3362007141113281 Average reward real: 0.6787356734275818 Training q_loss: 0.6921 Training g_loss: 4.1097 Training d_loss: 0.9978 Explore P: 0.6423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 26.0 Average reward fake: 0.4064238667488098 Average reward real: 0.6171643137931824 Training q_loss: 2.1814 Training g_loss: 3.4559 Training d_loss: 1.0154 Explore P: 0.6407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 15.0 Average reward fake: 0.34844455122947693 Average reward real: 0.6201423406600952 Training q_loss: 0.9513 Training g_loss: 4.0256 Training d_loss: 1.1610 Explore P: 0.6397\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 10.0 Average reward fake: 0.3098706007003784 Average reward real: 0.5516687035560608 Training q_loss: 0.5441 Training g_loss: 3.6510 Training d_loss: 1.0957 Explore P: 0.6391\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 18.0 Average reward fake: 0.3642030954360962 Average reward real: 0.6558201313018799 Training q_loss: 2.2393 Training g_loss: 3.4050 Training d_loss: 0.9293 Explore P: 0.6380\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 8.0 Average reward fake: 0.4126271903514862 Average reward real: 0.6413964033126831 Training q_loss: 2.3674 Training g_loss: 3.3106 Training d_loss: 1.1557 Explore P: 0.6375\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 12.0 Average reward fake: 0.36278149485588074 Average reward real: 0.5340908169746399 Training q_loss: 1.2837 Training g_loss: 3.4022 Training d_loss: 1.0990 Explore P: 0.6367\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 14.0 Average reward fake: 0.44647079706192017 Average reward real: 0.7161332964897156 Training q_loss: 0.7281 Training g_loss: 3.6222 Training d_loss: 0.9824 Explore P: 0.6358\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 28.0 Average reward fake: 0.24442321062088013 Average reward real: 0.5591885447502136 Training q_loss: 0.5727 Training g_loss: 3.9738 Training d_loss: 0.8835 Explore P: 0.6341\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 20.0 Average reward fake: 0.38190704584121704 Average reward real: 0.6954843401908875 Training q_loss: 6.8533 Training g_loss: 4.0681 Training d_loss: 1.0645 Explore P: 0.6328\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 14.0 Average reward fake: 0.3368789553642273 Average reward real: 0.4753660559654236 Training q_loss: 1.5700 Training g_loss: 3.2184 Training d_loss: 1.3408 Explore P: 0.6320\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 37.0 Average reward fake: 0.40047237277030945 Average reward real: 0.5125329494476318 Training q_loss: 0.2910 Training g_loss: 2.9499 Training d_loss: 1.2788 Explore P: 0.6297\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 27.0 Average reward fake: 0.4570225179195404 Average reward real: 0.5951389074325562 Training q_loss: 0.7180 Training g_loss: 3.2539 Training d_loss: 1.1691 Explore P: 0.6280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 19.0 Average reward fake: 0.2852628231048584 Average reward real: 0.659440279006958 Training q_loss: 1.5020 Training g_loss: 4.0346 Training d_loss: 0.8251 Explore P: 0.6268\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 35.0 Average reward fake: 0.45462149381637573 Average reward real: 0.6060965657234192 Training q_loss: 1.7780 Training g_loss: 3.6608 Training d_loss: 1.4327 Explore P: 0.6247\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 9.0 Average reward fake: 0.24688711762428284 Average reward real: 0.724172830581665 Training q_loss: 0.9504 Training g_loss: 4.8667 Training d_loss: 0.7065 Explore P: 0.6241\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 10.0 Average reward fake: 0.35897278785705566 Average reward real: 0.5007742047309875 Training q_loss: 0.7593 Training g_loss: 3.1653 Training d_loss: 1.1756 Explore P: 0.6235\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 16.0 Average reward fake: 0.38269394636154175 Average reward real: 0.5413801074028015 Training q_loss: 0.4626 Training g_loss: 3.7195 Training d_loss: 1.3547 Explore P: 0.6225\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 32.0 Average reward fake: 0.3591594696044922 Average reward real: 0.6888318061828613 Training q_loss: 0.4521 Training g_loss: 3.7043 Training d_loss: 0.9394 Explore P: 0.6206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 13.0 Average reward fake: 0.2294859141111374 Average reward real: 0.6167955994606018 Training q_loss: 0.4341 Training g_loss: 4.3324 Training d_loss: 0.8439 Explore P: 0.6198\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 11.0 Average reward fake: 0.28682321310043335 Average reward real: 0.7581340670585632 Training q_loss: 2.9289 Training g_loss: 4.6700 Training d_loss: 0.8225 Explore P: 0.6191\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 21.0 Average reward fake: 0.4278433322906494 Average reward real: 0.5654083490371704 Training q_loss: 0.8766 Training g_loss: 3.1914 Training d_loss: 1.2975 Explore P: 0.6178\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 10.0 Average reward fake: 0.3270293176174164 Average reward real: 0.6825958490371704 Training q_loss: 0.8541 Training g_loss: 4.1669 Training d_loss: 0.9088 Explore P: 0.6172\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 18.0 Average reward fake: 0.4073941111564636 Average reward real: 0.6499683856964111 Training q_loss: 1.5133 Training g_loss: 3.9810 Training d_loss: 1.2217 Explore P: 0.6161\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 21.0 Average reward fake: 0.21999935805797577 Average reward real: 0.6619853973388672 Training q_loss: 0.4666 Training g_loss: 4.2081 Training d_loss: 0.7908 Explore P: 0.6149\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 13.0 Average reward fake: 0.2991412878036499 Average reward real: 0.5555317997932434 Training q_loss: 0.4709 Training g_loss: 3.5965 Training d_loss: 1.0491 Explore P: 0.6141\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 29.0 Average reward fake: 0.3475061058998108 Average reward real: 0.7148804664611816 Training q_loss: 0.9385 Training g_loss: 3.4423 Training d_loss: 0.9047 Explore P: 0.6123\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 32.0 Average reward fake: 0.2810647487640381 Average reward real: 0.596750795841217 Training q_loss: 0.4873 Training g_loss: 4.9346 Training d_loss: 0.9779 Explore P: 0.6104\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 57.0 Average reward fake: 0.2897251546382904 Average reward real: 0.6795210242271423 Training q_loss: 1.1227 Training g_loss: 4.0978 Training d_loss: 0.9340 Explore P: 0.6070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 33.0 Average reward fake: 0.4243818521499634 Average reward real: 0.7364041209220886 Training q_loss: 0.1445 Training g_loss: 3.9864 Training d_loss: 1.1303 Explore P: 0.6050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 16.0 Average reward fake: 0.38560596108436584 Average reward real: 0.6829056739807129 Training q_loss: 0.5766 Training g_loss: 3.9350 Training d_loss: 1.0368 Explore P: 0.6041\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 15.0 Average reward fake: 0.3117580711841583 Average reward real: 0.5143173336982727 Training q_loss: 1.0116 Training g_loss: 3.6582 Training d_loss: 1.1611 Explore P: 0.6032\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 19.0 Average reward fake: 0.31224048137664795 Average reward real: 0.6230698823928833 Training q_loss: 0.4520 Training g_loss: 3.8446 Training d_loss: 0.9861 Explore P: 0.6021\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 18.0 Average reward fake: 0.2395782619714737 Average reward real: 0.5667087435722351 Training q_loss: 0.1446 Training g_loss: 3.4298 Training d_loss: 0.9768 Explore P: 0.6010\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 13.0 Average reward fake: 0.2696174383163452 Average reward real: 0.6701307892799377 Training q_loss: 1.2475 Training g_loss: 4.2366 Training d_loss: 0.9227 Explore P: 0.6002\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 12.0 Average reward fake: 0.27546197175979614 Average reward real: 0.5944307446479797 Training q_loss: 0.3271 Training g_loss: 3.8768 Training d_loss: 0.9908 Explore P: 0.5995\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 10.0 Average reward fake: 0.23505517840385437 Average reward real: 0.5204663276672363 Training q_loss: 0.2476 Training g_loss: 3.7186 Training d_loss: 1.0521 Explore P: 0.5989\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 13.0 Average reward fake: 0.36295947432518005 Average reward real: 0.7857757806777954 Training q_loss: 0.4758 Training g_loss: 4.4796 Training d_loss: 0.8317 Explore P: 0.5982\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 34.0 Average reward fake: 0.3344055116176605 Average reward real: 0.7392442226409912 Training q_loss: 1.1047 Training g_loss: 3.8960 Training d_loss: 0.8664 Explore P: 0.5962\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 35.0 Average reward fake: 0.2073661834001541 Average reward real: 0.5893149375915527 Training q_loss: 1.6785 Training g_loss: 4.1905 Training d_loss: 0.9106 Explore P: 0.5941\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 20.0 Average reward fake: 0.29703015089035034 Average reward real: 0.5834069848060608 Training q_loss: 0.6501 Training g_loss: 4.0291 Training d_loss: 1.1160 Explore P: 0.5930\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 19.0 Average reward fake: 0.23153232038021088 Average reward real: 0.6505404710769653 Training q_loss: 0.2843 Training g_loss: 4.4382 Training d_loss: 0.8117 Explore P: 0.5918\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 14.0 Average reward fake: 0.2507762014865875 Average reward real: 0.59983891248703 Training q_loss: 0.2380 Training g_loss: 4.2067 Training d_loss: 0.9945 Explore P: 0.5910\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 17.0 Average reward fake: 0.34223297238349915 Average reward real: 0.6620774865150452 Training q_loss: 2.3506 Training g_loss: 4.4845 Training d_loss: 0.8654 Explore P: 0.5900\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 23.0 Average reward fake: 0.21788692474365234 Average reward real: 0.577564537525177 Training q_loss: 0.2854 Training g_loss: 4.4795 Training d_loss: 0.9700 Explore P: 0.5887\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 18.0 Average reward fake: 0.3678041100502014 Average reward real: 0.7474796175956726 Training q_loss: 0.6336 Training g_loss: 4.3613 Training d_loss: 0.9722 Explore P: 0.5877\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 9.0 Average reward fake: 0.3238244950771332 Average reward real: 0.6845533847808838 Training q_loss: 1.7084 Training g_loss: 4.0897 Training d_loss: 0.9389 Explore P: 0.5872\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 11.0 Average reward fake: 0.29520705342292786 Average reward real: 0.6157940030097961 Training q_loss: 1.5379 Training g_loss: 3.8141 Training d_loss: 0.9767 Explore P: 0.5865\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 16.0 Average reward fake: 0.3209547698497772 Average reward real: 0.6188215017318726 Training q_loss: 0.6323 Training g_loss: 4.3413 Training d_loss: 0.8812 Explore P: 0.5856\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 22.0 Average reward fake: 0.3310531675815582 Average reward real: 0.6718415021896362 Training q_loss: 0.1754 Training g_loss: 3.6752 Training d_loss: 0.9903 Explore P: 0.5843\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 14.0 Average reward fake: 0.30586162209510803 Average reward real: 0.6243027448654175 Training q_loss: 3.8693 Training g_loss: 3.9149 Training d_loss: 0.8799 Explore P: 0.5835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 15.0 Average reward fake: 0.2211940735578537 Average reward real: 0.6374995708465576 Training q_loss: 2.7506 Training g_loss: 4.6300 Training d_loss: 0.9125 Explore P: 0.5827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 15.0 Average reward fake: 0.29042547941207886 Average reward real: 0.7842265367507935 Training q_loss: 2.0376 Training g_loss: 4.4592 Training d_loss: 0.8869 Explore P: 0.5818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 66.0 Average reward fake: 0.2825421094894409 Average reward real: 0.6600001454353333 Training q_loss: 0.4486 Training g_loss: 3.8361 Training d_loss: 0.9312 Explore P: 0.5780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 12.0 Average reward fake: 0.3161255717277527 Average reward real: 0.706072986125946 Training q_loss: 0.1594 Training g_loss: 4.1244 Training d_loss: 0.8452 Explore P: 0.5774\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 23.0 Average reward fake: 0.3676236867904663 Average reward real: 0.8021929264068604 Training q_loss: 0.1910 Training g_loss: 5.4512 Training d_loss: 0.9785 Explore P: 0.5761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 16.0 Average reward fake: 0.2552525997161865 Average reward real: 0.5337340831756592 Training q_loss: 0.3050 Training g_loss: 3.8186 Training d_loss: 1.0616 Explore P: 0.5752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 32.0 Average reward fake: 0.17764124274253845 Average reward real: 0.6462173461914062 Training q_loss: 1.5960 Training g_loss: 4.8766 Training d_loss: 0.8159 Explore P: 0.5734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 10.0 Average reward fake: 0.2748216390609741 Average reward real: 0.68815016746521 Training q_loss: 0.4079 Training g_loss: 4.4062 Training d_loss: 0.7993 Explore P: 0.5728\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 19.0 Average reward fake: 0.2590939402580261 Average reward real: 0.7763208150863647 Training q_loss: 2.0206 Training g_loss: 6.5730 Training d_loss: 0.5819 Explore P: 0.5717\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 35.0 Average reward fake: 0.2442096322774887 Average reward real: 0.7689222693443298 Training q_loss: 0.1466 Training g_loss: 4.7524 Training d_loss: 0.7020 Explore P: 0.5698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 41.0 Average reward fake: 0.24481883645057678 Average reward real: 0.7688827514648438 Training q_loss: 0.7461 Training g_loss: 5.5097 Training d_loss: 0.6678 Explore P: 0.5675\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 10.0 Average reward fake: 0.22138351202011108 Average reward real: 0.6443478465080261 Training q_loss: 0.1530 Training g_loss: 4.5052 Training d_loss: 0.7122 Explore P: 0.5669\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 16.0 Average reward fake: 0.20090645551681519 Average reward real: 0.706585705280304 Training q_loss: 0.8284 Training g_loss: 5.8836 Training d_loss: 0.7129 Explore P: 0.5660\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 12.0 Average reward fake: 0.36174920201301575 Average reward real: 0.8224525451660156 Training q_loss: 0.3063 Training g_loss: 5.1674 Training d_loss: 0.9169 Explore P: 0.5654\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 11.0 Average reward fake: 0.21416312456130981 Average reward real: 0.7577896118164062 Training q_loss: 2.8235 Training g_loss: 5.8908 Training d_loss: 0.5880 Explore P: 0.5647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 11.0 Average reward fake: 0.21708944439888 Average reward real: 0.6664451360702515 Training q_loss: 0.4545 Training g_loss: 5.2882 Training d_loss: 0.6741 Explore P: 0.5641\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 16.0 Average reward fake: 0.23827318847179413 Average reward real: 0.6147610545158386 Training q_loss: 0.8479 Training g_loss: 4.0528 Training d_loss: 0.8874 Explore P: 0.5632\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 9.0 Average reward fake: 0.28248271346092224 Average reward real: 0.6878402829170227 Training q_loss: 0.8014 Training g_loss: 5.1958 Training d_loss: 0.9882 Explore P: 0.5627\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 13.0 Average reward fake: 0.32238534092903137 Average reward real: 0.5515738725662231 Training q_loss: 5.0307 Training g_loss: 4.6073 Training d_loss: 1.2805 Explore P: 0.5620\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 16.0 Average reward fake: 0.17244865000247955 Average reward real: 0.8078880310058594 Training q_loss: 0.3064 Training g_loss: 5.8119 Training d_loss: 0.4605 Explore P: 0.5611\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 13.0 Average reward fake: 0.16277307271957397 Average reward real: 0.7150619626045227 Training q_loss: 0.8039 Training g_loss: 5.9828 Training d_loss: 0.6196 Explore P: 0.5604\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 27.0 Average reward fake: 0.15438082814216614 Average reward real: 0.6518728137016296 Training q_loss: 0.2113 Training g_loss: 6.1754 Training d_loss: 0.5908 Explore P: 0.5589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 15.0 Average reward fake: 0.18704569339752197 Average reward real: 0.5257964134216309 Training q_loss: 0.3809 Training g_loss: 4.8374 Training d_loss: 0.9668 Explore P: 0.5581\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 46.0 Average reward fake: 0.17769309878349304 Average reward real: 0.6397391557693481 Training q_loss: 0.5685 Training g_loss: 6.0451 Training d_loss: 0.9524 Explore P: 0.5556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 14.0 Average reward fake: 0.26498082280158997 Average reward real: 0.7143126130104065 Training q_loss: 0.2802 Training g_loss: 5.4186 Training d_loss: 0.8382 Explore P: 0.5548\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 11.0 Average reward fake: 0.23636578023433685 Average reward real: 0.6869140863418579 Training q_loss: 1.0680 Training g_loss: 5.0744 Training d_loss: 0.7739 Explore P: 0.5542\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 17.0 Average reward fake: 0.13026832044124603 Average reward real: 0.5595470666885376 Training q_loss: 0.3690 Training g_loss: 6.6381 Training d_loss: 0.8842 Explore P: 0.5533\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 27.0 Average reward fake: 0.2739594280719757 Average reward real: 0.7179737687110901 Training q_loss: 0.7820 Training g_loss: 5.4415 Training d_loss: 0.8628 Explore P: 0.5519\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 15.0 Average reward fake: 0.2653416395187378 Average reward real: 0.8146688938140869 Training q_loss: 0.2846 Training g_loss: 5.3708 Training d_loss: 0.6572 Explore P: 0.5510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 38.0 Average reward fake: 0.1562635749578476 Average reward real: 0.6030396819114685 Training q_loss: 2.5586 Training g_loss: 5.6595 Training d_loss: 0.7656 Explore P: 0.5490\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 18.0 Average reward fake: 0.20248530805110931 Average reward real: 0.6178750395774841 Training q_loss: 0.2794 Training g_loss: 5.5569 Training d_loss: 0.9034 Explore P: 0.5480\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 22.0 Average reward fake: 0.24652856588363647 Average reward real: 0.7995234727859497 Training q_loss: 0.3394 Training g_loss: 5.8451 Training d_loss: 0.6117 Explore P: 0.5468\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 27.0 Average reward fake: 0.20765551924705505 Average reward real: 0.8522995114326477 Training q_loss: 0.6328 Training g_loss: 5.7271 Training d_loss: 0.5173 Explore P: 0.5454\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 14.0 Average reward fake: 0.18837079405784607 Average reward real: 0.7844236493110657 Training q_loss: 0.4154 Training g_loss: 6.0478 Training d_loss: 0.6562 Explore P: 0.5446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 16.0 Average reward fake: 0.26415249705314636 Average reward real: 0.6771763563156128 Training q_loss: 0.7980 Training g_loss: 5.0262 Training d_loss: 0.7132 Explore P: 0.5438\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 22.0 Average reward fake: 0.23945999145507812 Average reward real: 0.7421507835388184 Training q_loss: 1.3513 Training g_loss: 6.1986 Training d_loss: 0.6727 Explore P: 0.5426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 28.0 Average reward fake: 0.25830891728401184 Average reward real: 0.8182031512260437 Training q_loss: 0.0861 Training g_loss: 5.6999 Training d_loss: 0.6641 Explore P: 0.5411\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 30.0 Average reward fake: 0.18372146785259247 Average reward real: 0.7593762278556824 Training q_loss: 0.6956 Training g_loss: 6.1522 Training d_loss: 0.5237 Explore P: 0.5395\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 23.0 Average reward fake: 0.2654367685317993 Average reward real: 0.7508549690246582 Training q_loss: 1.3280 Training g_loss: 5.5225 Training d_loss: 0.7009 Explore P: 0.5383\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 16.0 Average reward fake: 0.34460777044296265 Average reward real: 0.7634963989257812 Training q_loss: 0.4222 Training g_loss: 7.1801 Training d_loss: 0.7780 Explore P: 0.5375\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 16.0 Average reward fake: 0.1670359969139099 Average reward real: 0.6986324787139893 Training q_loss: 0.4539 Training g_loss: 5.9458 Training d_loss: 0.6361 Explore P: 0.5366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 11.0 Average reward fake: 0.2329384982585907 Average reward real: 0.7980712652206421 Training q_loss: 1.1876 Training g_loss: 6.9418 Training d_loss: 0.6712 Explore P: 0.5361\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 24.0 Average reward fake: 0.2959248423576355 Average reward real: 0.7119526863098145 Training q_loss: 0.6578 Training g_loss: 6.1924 Training d_loss: 0.8845 Explore P: 0.5348\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 43.0 Average reward fake: 0.17431816458702087 Average reward real: 0.7524656653404236 Training q_loss: 1.2164 Training g_loss: 6.9539 Training d_loss: 0.7569 Explore P: 0.5325\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 18.0 Average reward fake: 0.20679309964179993 Average reward real: 0.7516291737556458 Training q_loss: 1.2526 Training g_loss: 4.9829 Training d_loss: 0.5586 Explore P: 0.5316\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 22.0 Average reward fake: 0.12391811609268188 Average reward real: 0.6433296203613281 Training q_loss: 0.4033 Training g_loss: 6.5585 Training d_loss: 0.6244 Explore P: 0.5305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 13.0 Average reward fake: 0.3697400689125061 Average reward real: 0.6903032660484314 Training q_loss: 0.2084 Training g_loss: 5.1814 Training d_loss: 1.0125 Explore P: 0.5298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 22.0 Average reward fake: 0.28699660301208496 Average reward real: 0.6132639050483704 Training q_loss: 0.7717 Training g_loss: 5.8124 Training d_loss: 1.1802 Explore P: 0.5286\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 28.0 Average reward fake: 0.27501747012138367 Average reward real: 0.603401243686676 Training q_loss: 0.4888 Training g_loss: 4.4844 Training d_loss: 1.1097 Explore P: 0.5272\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 8.0 Average reward fake: 0.2526017427444458 Average reward real: 0.9062835574150085 Training q_loss: 0.7855 Training g_loss: 8.6579 Training d_loss: 0.5725 Explore P: 0.5268\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 18.0 Average reward fake: 0.2323930561542511 Average reward real: 0.5902852416038513 Training q_loss: 2.1004 Training g_loss: 7.8389 Training d_loss: 0.9587 Explore P: 0.5258\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 44.0 Average reward fake: 0.2240932285785675 Average reward real: 0.757114052772522 Training q_loss: 0.5535 Training g_loss: 5.4250 Training d_loss: 0.7269 Explore P: 0.5236\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 12.0 Average reward fake: 0.2100939154624939 Average reward real: 0.7969250679016113 Training q_loss: 0.2814 Training g_loss: 7.1484 Training d_loss: 0.7054 Explore P: 0.5230\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 12.0 Average reward fake: 0.1991337239742279 Average reward real: 0.7568826079368591 Training q_loss: 0.2774 Training g_loss: 6.1817 Training d_loss: 0.6430 Explore P: 0.5223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 10.0 Average reward fake: 0.15578025579452515 Average reward real: 0.7457492351531982 Training q_loss: 1.2985 Training g_loss: 7.9881 Training d_loss: 0.5460 Explore P: 0.5218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 38.0 Average reward fake: 0.20560933649539948 Average reward real: 0.5052931904792786 Training q_loss: 0.3112 Training g_loss: 5.5255 Training d_loss: 1.1036 Explore P: 0.5199\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 19.0 Average reward fake: 0.4215662479400635 Average reward real: 0.8836165070533752 Training q_loss: 0.6171 Training g_loss: 7.0273 Training d_loss: 0.8916 Explore P: 0.5189\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 24.0 Average reward fake: 0.11817428469657898 Average reward real: 0.33487099409103394 Training q_loss: 3.0515 Training g_loss: 6.8461 Training d_loss: 1.7666 Explore P: 0.5177\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 25.0 Average reward fake: 0.15284988284111023 Average reward real: 0.6038197875022888 Training q_loss: 0.7650 Training g_loss: 6.1682 Training d_loss: 0.8871 Explore P: 0.5164\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 323 Total reward: 46.0 Average reward fake: 0.17568419873714447 Average reward real: 0.5885888934135437 Training q_loss: 0.8649 Training g_loss: 6.8156 Training d_loss: 0.8443 Explore P: 0.5141\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 324 Total reward: 9.0 Average reward fake: 0.13194820284843445 Average reward real: 0.33758410811424255 Training q_loss: 0.4519 Training g_loss: 5.6876 Training d_loss: 1.6006 Explore P: 0.5137\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 325 Total reward: 35.0 Average reward fake: 0.21106933057308197 Average reward real: 0.6646164059638977 Training q_loss: 1.3375 Training g_loss: 6.4450 Training d_loss: 0.6887 Explore P: 0.5119\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 326 Total reward: 13.0 Average reward fake: 0.1943068951368332 Average reward real: 0.781719446182251 Training q_loss: 0.2837 Training g_loss: 7.5340 Training d_loss: 0.5003 Explore P: 0.5112\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 327 Total reward: 38.0 Average reward fake: 0.22249092161655426 Average reward real: 0.5852614641189575 Training q_loss: 0.2935 Training g_loss: 5.1564 Training d_loss: 0.9295 Explore P: 0.5093\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 328 Total reward: 36.0 Average reward fake: 0.34696415066719055 Average reward real: 0.5792971849441528 Training q_loss: 0.3675 Training g_loss: 5.1571 Training d_loss: 1.3131 Explore P: 0.5076\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 329 Total reward: 17.0 Average reward fake: 0.23252956569194794 Average reward real: 0.8115008473396301 Training q_loss: 0.2603 Training g_loss: 6.7662 Training d_loss: 0.6729 Explore P: 0.5067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 330 Total reward: 24.0 Average reward fake: 0.36892369389533997 Average reward real: 0.8282816410064697 Training q_loss: 0.3406 Training g_loss: 7.5682 Training d_loss: 0.9002 Explore P: 0.5055\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 331 Total reward: 9.0 Average reward fake: 0.3104236423969269 Average reward real: 0.7943623065948486 Training q_loss: 0.7107 Training g_loss: 6.4604 Training d_loss: 0.7987 Explore P: 0.5051\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 332 Total reward: 9.0 Average reward fake: 0.2917342483997345 Average reward real: 0.6274585723876953 Training q_loss: 0.1201 Training g_loss: 4.8635 Training d_loss: 0.9670 Explore P: 0.5046\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 333 Total reward: 39.0 Average reward fake: 0.19636081159114838 Average reward real: 0.6062264442443848 Training q_loss: 0.2721 Training g_loss: 5.1308 Training d_loss: 0.9201 Explore P: 0.5027\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 334 Total reward: 23.0 Average reward fake: 0.27094343304634094 Average reward real: 0.5770502090454102 Training q_loss: 0.2242 Training g_loss: 4.7199 Training d_loss: 0.9502 Explore P: 0.5016\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 335 Total reward: 13.0 Average reward fake: 0.22455264627933502 Average reward real: 0.6672155261039734 Training q_loss: 4.3545 Training g_loss: 6.9059 Training d_loss: 0.7396 Explore P: 0.5009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 336 Total reward: 12.0 Average reward fake: 0.3118644654750824 Average reward real: 0.7455549836158752 Training q_loss: 0.1787 Training g_loss: 4.6691 Training d_loss: 0.7630 Explore P: 0.5003\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 337 Total reward: 32.0 Average reward fake: 0.3391517996788025 Average reward real: 0.5968201756477356 Training q_loss: 0.2634 Training g_loss: 4.4302 Training d_loss: 1.1920 Explore P: 0.4988\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 338 Total reward: 53.0 Average reward fake: 0.29237669706344604 Average reward real: 0.7862047553062439 Training q_loss: 1.4550 Training g_loss: 6.2916 Training d_loss: 1.0300 Explore P: 0.4962\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 339 Total reward: 22.0 Average reward fake: 0.16950511932373047 Average reward real: 0.6096610426902771 Training q_loss: 0.3807 Training g_loss: 7.2479 Training d_loss: 0.9335 Explore P: 0.4951\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 340 Total reward: 18.0 Average reward fake: 0.18624228239059448 Average reward real: 0.6370193362236023 Training q_loss: 1.3825 Training g_loss: 5.6844 Training d_loss: 0.7090 Explore P: 0.4943\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 341 Total reward: 14.0 Average reward fake: 0.2956877648830414 Average reward real: 0.7419464588165283 Training q_loss: 2.0545 Training g_loss: 6.6629 Training d_loss: 0.6787 Explore P: 0.4936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 342 Total reward: 9.0 Average reward fake: 0.21191687881946564 Average reward real: 0.4481087923049927 Training q_loss: 0.3161 Training g_loss: 4.3549 Training d_loss: 1.2675 Explore P: 0.4931\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 343 Total reward: 19.0 Average reward fake: 0.2722538113594055 Average reward real: 0.8539429306983948 Training q_loss: 1.4103 Training g_loss: 7.3508 Training d_loss: 0.6309 Explore P: 0.4922\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 344 Total reward: 22.0 Average reward fake: 0.10078481584787369 Average reward real: 0.5626412630081177 Training q_loss: 0.4270 Training g_loss: 8.0594 Training d_loss: 0.7713 Explore P: 0.4912\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 345 Total reward: 16.0 Average reward fake: 0.23209601640701294 Average reward real: 0.6922900676727295 Training q_loss: 0.6103 Training g_loss: 5.9816 Training d_loss: 0.8157 Explore P: 0.4904\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 346 Total reward: 21.0 Average reward fake: 0.23452670872211456 Average reward real: 0.6427369713783264 Training q_loss: 0.3802 Training g_loss: 5.6671 Training d_loss: 0.8002 Explore P: 0.4894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 347 Total reward: 11.0 Average reward fake: 0.39339715242385864 Average reward real: 0.5455766916275024 Training q_loss: 0.4530 Training g_loss: 4.5102 Training d_loss: 1.0339 Explore P: 0.4889\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 348 Total reward: 13.0 Average reward fake: 0.1972547173500061 Average reward real: 0.6711685657501221 Training q_loss: 0.6331 Training g_loss: 6.1327 Training d_loss: 0.8629 Explore P: 0.4882\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 349 Total reward: 9.0 Average reward fake: 0.22317802906036377 Average reward real: 0.6707869172096252 Training q_loss: 0.3340 Training g_loss: 6.7683 Training d_loss: 0.7019 Explore P: 0.4878\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 350 Total reward: 8.0 Average reward fake: 0.1993943154811859 Average reward real: 0.5506959557533264 Training q_loss: 1.1563 Training g_loss: 6.2591 Training d_loss: 0.9331 Explore P: 0.4874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 351 Total reward: 12.0 Average reward fake: 0.2616831064224243 Average reward real: 0.8582804799079895 Training q_loss: 0.6684 Training g_loss: 7.1807 Training d_loss: 0.7827 Explore P: 0.4869\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 352 Total reward: 16.0 Average reward fake: 0.44565483927726746 Average reward real: 0.8762458562850952 Training q_loss: 0.3161 Training g_loss: 5.6447 Training d_loss: 1.3089 Explore P: 0.4861\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 353 Total reward: 11.0 Average reward fake: 0.1557438224554062 Average reward real: 0.49867114424705505 Training q_loss: 0.8155 Training g_loss: 7.5055 Training d_loss: 1.0299 Explore P: 0.4856\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 354 Total reward: 23.0 Average reward fake: 0.2694574296474457 Average reward real: 0.8635980486869812 Training q_loss: 1.4504 Training g_loss: 7.6581 Training d_loss: 0.6356 Explore P: 0.4845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 355 Total reward: 26.0 Average reward fake: 0.23517876863479614 Average reward real: 0.6384391784667969 Training q_loss: 2.0135 Training g_loss: 6.2890 Training d_loss: 0.8095 Explore P: 0.4832\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 356 Total reward: 21.0 Average reward fake: 0.20150478184223175 Average reward real: 0.6166036128997803 Training q_loss: 0.3601 Training g_loss: 6.2501 Training d_loss: 0.7714 Explore P: 0.4822\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 357 Total reward: 10.0 Average reward fake: 0.3482288122177124 Average reward real: 0.3635607957839966 Training q_loss: 1.3086 Training g_loss: 4.5023 Training d_loss: 1.4909 Explore P: 0.4818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 358 Total reward: 18.0 Average reward fake: 0.306417316198349 Average reward real: 0.7133025527000427 Training q_loss: 0.8226 Training g_loss: 6.9069 Training d_loss: 0.9755 Explore P: 0.4809\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 359 Total reward: 28.0 Average reward fake: 0.21695315837860107 Average reward real: 0.8204628229141235 Training q_loss: 0.9098 Training g_loss: 8.1761 Training d_loss: 0.5136 Explore P: 0.4796\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 360 Total reward: 15.0 Average reward fake: 0.26615819334983826 Average reward real: 0.8703378438949585 Training q_loss: 0.3874 Training g_loss: 6.6596 Training d_loss: 0.6862 Explore P: 0.4789\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 361 Total reward: 13.0 Average reward fake: 0.2067503035068512 Average reward real: 0.6565410494804382 Training q_loss: 0.6817 Training g_loss: 5.8365 Training d_loss: 0.8321 Explore P: 0.4783\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 362 Total reward: 9.0 Average reward fake: 0.17046594619750977 Average reward real: 0.6354315876960754 Training q_loss: 2.1569 Training g_loss: 7.3390 Training d_loss: 0.8220 Explore P: 0.4779\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 363 Total reward: 19.0 Average reward fake: 0.20832230150699615 Average reward real: 0.7044245004653931 Training q_loss: 0.9098 Training g_loss: 5.6535 Training d_loss: 0.6890 Explore P: 0.4770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 364 Total reward: 30.0 Average reward fake: 0.29124701023101807 Average reward real: 0.43779292702674866 Training q_loss: 0.4786 Training g_loss: 5.5700 Training d_loss: 1.3809 Explore P: 0.4756\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 365 Total reward: 17.0 Average reward fake: 0.1875520497560501 Average reward real: 0.47794899344444275 Training q_loss: 0.4303 Training g_loss: 5.7953 Training d_loss: 1.2348 Explore P: 0.4748\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 366 Total reward: 14.0 Average reward fake: 0.1842210441827774 Average reward real: 0.6439663767814636 Training q_loss: 0.6955 Training g_loss: 6.7653 Training d_loss: 0.8051 Explore P: 0.4741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 367 Total reward: 9.0 Average reward fake: 0.3302845060825348 Average reward real: 0.6422972679138184 Training q_loss: 0.3126 Training g_loss: 6.7568 Training d_loss: 1.2330 Explore P: 0.4737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 368 Total reward: 23.0 Average reward fake: 0.34460440278053284 Average reward real: 0.7749905586242676 Training q_loss: 0.2559 Training g_loss: 5.0875 Training d_loss: 0.8012 Explore P: 0.4727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 369 Total reward: 30.0 Average reward fake: 0.3424944579601288 Average reward real: 0.7495574951171875 Training q_loss: 0.5171 Training g_loss: 4.7926 Training d_loss: 0.8519 Explore P: 0.4713\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 370 Total reward: 25.0 Average reward fake: 0.18689562380313873 Average reward real: 0.7599537372589111 Training q_loss: 0.9389 Training g_loss: 8.0071 Training d_loss: 0.6014 Explore P: 0.4701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 371 Total reward: 13.0 Average reward fake: 0.1882573664188385 Average reward real: 0.7049600481987 Training q_loss: 0.1993 Training g_loss: 6.4687 Training d_loss: 0.7028 Explore P: 0.4695\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 372 Total reward: 15.0 Average reward fake: 0.29443880915641785 Average reward real: 0.8278791904449463 Training q_loss: 0.1316 Training g_loss: 5.9082 Training d_loss: 1.1401 Explore P: 0.4688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 373 Total reward: 9.0 Average reward fake: 0.28549015522003174 Average reward real: 0.7635551691055298 Training q_loss: 0.4552 Training g_loss: 7.6375 Training d_loss: 0.9633 Explore P: 0.4684\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 374 Total reward: 8.0 Average reward fake: 0.17520290613174438 Average reward real: 0.6513402462005615 Training q_loss: 1.0815 Training g_loss: 8.2490 Training d_loss: 0.7702 Explore P: 0.4681\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 375 Total reward: 22.0 Average reward fake: 0.2572738528251648 Average reward real: 0.6110408306121826 Training q_loss: 0.4324 Training g_loss: 5.3375 Training d_loss: 0.9792 Explore P: 0.4671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 376 Total reward: 25.0 Average reward fake: 0.19619129598140717 Average reward real: 0.7308862209320068 Training q_loss: 1.3213 Training g_loss: 5.6479 Training d_loss: 0.6840 Explore P: 0.4659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 377 Total reward: 16.0 Average reward fake: 0.1934283822774887 Average reward real: 0.6762853860855103 Training q_loss: 0.4635 Training g_loss: 7.2098 Training d_loss: 0.7834 Explore P: 0.4652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 378 Total reward: 11.0 Average reward fake: 0.422705739736557 Average reward real: 0.7896159291267395 Training q_loss: 0.3649 Training g_loss: 6.2486 Training d_loss: 1.2442 Explore P: 0.4647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 379 Total reward: 10.0 Average reward fake: 0.1545281857252121 Average reward real: 0.5008583068847656 Training q_loss: 0.3353 Training g_loss: 6.3029 Training d_loss: 1.1401 Explore P: 0.4642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 380 Total reward: 27.0 Average reward fake: 0.287447065114975 Average reward real: 0.738629937171936 Training q_loss: 0.6335 Training g_loss: 6.2048 Training d_loss: 0.8950 Explore P: 0.4630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 381 Total reward: 21.0 Average reward fake: 0.29062479734420776 Average reward real: 0.6833352446556091 Training q_loss: 0.4213 Training g_loss: 5.6685 Training d_loss: 0.8336 Explore P: 0.4621\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 382 Total reward: 19.0 Average reward fake: 0.2216830849647522 Average reward real: 0.605302631855011 Training q_loss: 0.2571 Training g_loss: 5.0118 Training d_loss: 1.2143 Explore P: 0.4612\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 383 Total reward: 16.0 Average reward fake: 0.30098283290863037 Average reward real: 0.7219012379646301 Training q_loss: 0.5312 Training g_loss: 7.4693 Training d_loss: 0.9828 Explore P: 0.4605\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 384 Total reward: 35.0 Average reward fake: 0.288697212934494 Average reward real: 0.5031682848930359 Training q_loss: 0.4768 Training g_loss: 4.9587 Training d_loss: 1.2254 Explore P: 0.4589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 385 Total reward: 13.0 Average reward fake: 0.3351733088493347 Average reward real: 0.7078793048858643 Training q_loss: 0.4185 Training g_loss: 5.3789 Training d_loss: 0.7697 Explore P: 0.4583\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 386 Total reward: 12.0 Average reward fake: 0.23683440685272217 Average reward real: 0.5571344494819641 Training q_loss: 1.2500 Training g_loss: 5.5578 Training d_loss: 1.1460 Explore P: 0.4578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 387 Total reward: 9.0 Average reward fake: 0.164291650056839 Average reward real: 0.6345863938331604 Training q_loss: 0.3954 Training g_loss: 5.9924 Training d_loss: 0.7090 Explore P: 0.4574\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 388 Total reward: 11.0 Average reward fake: 0.4366813600063324 Average reward real: 0.8638709187507629 Training q_loss: 0.2844 Training g_loss: 6.0355 Training d_loss: 0.9660 Explore P: 0.4569\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 389 Total reward: 14.0 Average reward fake: 0.16574600338935852 Average reward real: 0.4575633704662323 Training q_loss: 1.1970 Training g_loss: 6.4645 Training d_loss: 1.1891 Explore P: 0.4563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 390 Total reward: 9.0 Average reward fake: 0.2539407014846802 Average reward real: 0.7273655533790588 Training q_loss: 0.3202 Training g_loss: 5.3065 Training d_loss: 0.6311 Explore P: 0.4559\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 391 Total reward: 11.0 Average reward fake: 0.368943452835083 Average reward real: 0.493814617395401 Training q_loss: 0.4222 Training g_loss: 3.9339 Training d_loss: 1.3800 Explore P: 0.4554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 392 Total reward: 24.0 Average reward fake: 0.17525026202201843 Average reward real: 0.5639387369155884 Training q_loss: 0.2471 Training g_loss: 5.2080 Training d_loss: 1.0529 Explore P: 0.4543\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 393 Total reward: 13.0 Average reward fake: 0.1976909190416336 Average reward real: 0.7424518465995789 Training q_loss: 0.1114 Training g_loss: 5.6002 Training d_loss: 0.6867 Explore P: 0.4537\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 394 Total reward: 13.0 Average reward fake: 0.19072437286376953 Average reward real: 0.8212098479270935 Training q_loss: 0.4428 Training g_loss: 6.8603 Training d_loss: 0.5949 Explore P: 0.4531\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 395 Total reward: 10.0 Average reward fake: 0.42906057834625244 Average reward real: 0.7693221569061279 Training q_loss: 1.7959 Training g_loss: 6.2821 Training d_loss: 1.2356 Explore P: 0.4527\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 396 Total reward: 14.0 Average reward fake: 0.22859889268875122 Average reward real: 0.738537609577179 Training q_loss: 0.7628 Training g_loss: 7.2644 Training d_loss: 0.6490 Explore P: 0.4521\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 397 Total reward: 35.0 Average reward fake: 0.25849276781082153 Average reward real: 0.7665274739265442 Training q_loss: 0.4970 Training g_loss: 5.2885 Training d_loss: 0.7133 Explore P: 0.4505\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 398 Total reward: 12.0 Average reward fake: 0.309444785118103 Average reward real: 0.7839697003364563 Training q_loss: 0.4802 Training g_loss: 8.0559 Training d_loss: 0.8135 Explore P: 0.4500\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 399 Total reward: 18.0 Average reward fake: 0.333089143037796 Average reward real: 0.5661079287528992 Training q_loss: 0.5944 Training g_loss: 5.4059 Training d_loss: 1.0224 Explore P: 0.4492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 400 Total reward: 14.0 Average reward fake: 0.18559573590755463 Average reward real: 0.5828460454940796 Training q_loss: 0.8732 Training g_loss: 7.5440 Training d_loss: 0.9819 Explore P: 0.4486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 401 Total reward: 9.0 Average reward fake: 0.24614131450653076 Average reward real: 0.7340171337127686 Training q_loss: 0.3320 Training g_loss: 7.4283 Training d_loss: 0.6457 Explore P: 0.4482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 402 Total reward: 9.0 Average reward fake: 0.4049534499645233 Average reward real: 0.8884782195091248 Training q_loss: 0.4252 Training g_loss: 7.4680 Training d_loss: 1.2429 Explore P: 0.4478\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 403 Total reward: 10.0 Average reward fake: 0.29081183671951294 Average reward real: 0.7833676934242249 Training q_loss: 0.2650 Training g_loss: 6.1241 Training d_loss: 0.7704 Explore P: 0.4474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 404 Total reward: 12.0 Average reward fake: 0.188899427652359 Average reward real: 0.6812677979469299 Training q_loss: 0.2626 Training g_loss: 5.8736 Training d_loss: 0.7785 Explore P: 0.4469\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 405 Total reward: 10.0 Average reward fake: 0.23898392915725708 Average reward real: 0.7753871083259583 Training q_loss: 0.2520 Training g_loss: 6.2744 Training d_loss: 0.8688 Explore P: 0.4464\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 406 Total reward: 37.0 Average reward fake: 0.2959555685520172 Average reward real: 0.6646325588226318 Training q_loss: 0.4289 Training g_loss: 4.9680 Training d_loss: 1.3785 Explore P: 0.4448\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 407 Total reward: 17.0 Average reward fake: 0.16649788618087769 Average reward real: 0.6732044219970703 Training q_loss: 0.7076 Training g_loss: 5.9272 Training d_loss: 0.7374 Explore P: 0.4441\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 408 Total reward: 13.0 Average reward fake: 0.18821151554584503 Average reward real: 0.850491464138031 Training q_loss: 0.8052 Training g_loss: 7.7873 Training d_loss: 0.6918 Explore P: 0.4435\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 409 Total reward: 13.0 Average reward fake: 0.17632122337818146 Average reward real: 0.4633466601371765 Training q_loss: 2.1096 Training g_loss: 5.9354 Training d_loss: 1.1498 Explore P: 0.4429\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 410 Total reward: 16.0 Average reward fake: 0.26712659001350403 Average reward real: 0.8509101271629333 Training q_loss: 0.9085 Training g_loss: 9.2187 Training d_loss: 0.6125 Explore P: 0.4423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 411 Total reward: 28.0 Average reward fake: 0.3952586352825165 Average reward real: 0.6563501954078674 Training q_loss: 0.5352 Training g_loss: 7.7664 Training d_loss: 1.0565 Explore P: 0.4410\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 412 Total reward: 20.0 Average reward fake: 0.18652023375034332 Average reward real: 0.7534945607185364 Training q_loss: 0.1979 Training g_loss: 6.8938 Training d_loss: 0.7743 Explore P: 0.4402\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 413 Total reward: 10.0 Average reward fake: 0.2522493302822113 Average reward real: 0.848682701587677 Training q_loss: 0.3353 Training g_loss: 7.5919 Training d_loss: 0.8613 Explore P: 0.4398\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 414 Total reward: 8.0 Average reward fake: 0.24465133249759674 Average reward real: 0.8535342216491699 Training q_loss: 1.6702 Training g_loss: 7.2862 Training d_loss: 0.5278 Explore P: 0.4394\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 415 Total reward: 12.0 Average reward fake: 0.15925320982933044 Average reward real: 0.7319183349609375 Training q_loss: 0.4243 Training g_loss: 7.9150 Training d_loss: 0.6217 Explore P: 0.4389\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 416 Total reward: 24.0 Average reward fake: 0.2113083004951477 Average reward real: 0.6522624492645264 Training q_loss: 1.9751 Training g_loss: 8.3496 Training d_loss: 0.7883 Explore P: 0.4379\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 417 Total reward: 16.0 Average reward fake: 0.3092661201953888 Average reward real: 0.7474294900894165 Training q_loss: 0.3192 Training g_loss: 6.9950 Training d_loss: 0.7936 Explore P: 0.4372\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 418 Total reward: 13.0 Average reward fake: 0.18192395567893982 Average reward real: 0.7160632610321045 Training q_loss: 0.2362 Training g_loss: 6.2425 Training d_loss: 0.6353 Explore P: 0.4366\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 419 Total reward: 16.0 Average reward fake: 0.15021662414073944 Average reward real: 0.7285012602806091 Training q_loss: 0.7771 Training g_loss: 7.9194 Training d_loss: 0.5190 Explore P: 0.4359\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 420 Total reward: 33.0 Average reward fake: 0.17928628623485565 Average reward real: 0.598011314868927 Training q_loss: 0.5300 Training g_loss: 7.6282 Training d_loss: 1.4008 Explore P: 0.4345\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 421 Total reward: 27.0 Average reward fake: 0.3458106219768524 Average reward real: 0.746140718460083 Training q_loss: 0.5006 Training g_loss: 9.7959 Training d_loss: 1.3203 Explore P: 0.4334\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 422 Total reward: 14.0 Average reward fake: 0.48665037751197815 Average reward real: 0.567298948764801 Training q_loss: 1.5546 Training g_loss: 7.3222 Training d_loss: 1.6579 Explore P: 0.4328\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 423 Total reward: 10.0 Average reward fake: 0.17220042645931244 Average reward real: 0.7983003258705139 Training q_loss: 1.5434 Training g_loss: 8.1561 Training d_loss: 0.4896 Explore P: 0.4324\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 424 Total reward: 27.0 Average reward fake: 0.2706378102302551 Average reward real: 0.7952695488929749 Training q_loss: 0.5899 Training g_loss: 7.5835 Training d_loss: 0.6258 Explore P: 0.4312\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 425 Total reward: 18.0 Average reward fake: 0.2235465943813324 Average reward real: 0.6880897283554077 Training q_loss: 1.7762 Training g_loss: 6.7415 Training d_loss: 0.7776 Explore P: 0.4305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 426 Total reward: 16.0 Average reward fake: 0.17480537295341492 Average reward real: 0.476896733045578 Training q_loss: 0.8801 Training g_loss: 7.0308 Training d_loss: 1.0911 Explore P: 0.4298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 427 Total reward: 23.0 Average reward fake: 0.263511061668396 Average reward real: 0.7023002505302429 Training q_loss: 0.4954 Training g_loss: 6.2551 Training d_loss: 0.7743 Explore P: 0.4288\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 428 Total reward: 15.0 Average reward fake: 0.4907722771167755 Average reward real: 0.7199476361274719 Training q_loss: 5.7384 Training g_loss: 5.9607 Training d_loss: 1.6461 Explore P: 0.4282\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 429 Total reward: 14.0 Average reward fake: 0.35802626609802246 Average reward real: 0.6506306529045105 Training q_loss: 0.2129 Training g_loss: 5.1240 Training d_loss: 1.1005 Explore P: 0.4276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 430 Total reward: 33.0 Average reward fake: 0.21179473400115967 Average reward real: 0.5986884832382202 Training q_loss: 0.7751 Training g_loss: 6.2512 Training d_loss: 0.9831 Explore P: 0.4263\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 431 Total reward: 12.0 Average reward fake: 0.2629087567329407 Average reward real: 0.5435206890106201 Training q_loss: 0.6142 Training g_loss: 4.5946 Training d_loss: 1.1312 Explore P: 0.4258\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 432 Total reward: 16.0 Average reward fake: 0.13598790764808655 Average reward real: 0.6168766021728516 Training q_loss: 1.3054 Training g_loss: 10.3358 Training d_loss: 0.6952 Explore P: 0.4251\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 433 Total reward: 25.0 Average reward fake: 0.14258195459842682 Average reward real: 0.7510345578193665 Training q_loss: 1.6251 Training g_loss: 9.0585 Training d_loss: 0.4376 Explore P: 0.4241\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 434 Total reward: 28.0 Average reward fake: 0.1670217514038086 Average reward real: 0.6548799872398376 Training q_loss: 0.1537 Training g_loss: 6.2850 Training d_loss: 0.6758 Explore P: 0.4229\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 435 Total reward: 10.0 Average reward fake: 0.4640194773674011 Average reward real: 0.7380397915840149 Training q_loss: 0.6141 Training g_loss: 7.3584 Training d_loss: 1.1633 Explore P: 0.4225\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 436 Total reward: 10.0 Average reward fake: 0.13737744092941284 Average reward real: 0.6533258557319641 Training q_loss: 0.7335 Training g_loss: 8.8713 Training d_loss: 0.6902 Explore P: 0.4221\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 437 Total reward: 9.0 Average reward fake: 0.2499818652868271 Average reward real: 0.5756277441978455 Training q_loss: 0.5289 Training g_loss: 6.9350 Training d_loss: 0.9046 Explore P: 0.4217\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 438 Total reward: 15.0 Average reward fake: 0.3103020489215851 Average reward real: 0.6570103168487549 Training q_loss: 1.0295 Training g_loss: 6.7193 Training d_loss: 0.9320 Explore P: 0.4211\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 439 Total reward: 12.0 Average reward fake: 0.3184216022491455 Average reward real: 0.8343144059181213 Training q_loss: 0.9787 Training g_loss: 7.7251 Training d_loss: 0.6228 Explore P: 0.4206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 440 Total reward: 11.0 Average reward fake: 0.3468930125236511 Average reward real: 0.7568787932395935 Training q_loss: 0.6109 Training g_loss: 8.2944 Training d_loss: 0.8020 Explore P: 0.4201\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 441 Total reward: 12.0 Average reward fake: 0.14095701277256012 Average reward real: 0.6224528551101685 Training q_loss: 0.3862 Training g_loss: 6.6361 Training d_loss: 0.8691 Explore P: 0.4197\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 442 Total reward: 15.0 Average reward fake: 0.2634631097316742 Average reward real: 0.6071289777755737 Training q_loss: 0.4324 Training g_loss: 6.8023 Training d_loss: 1.1525 Explore P: 0.4190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 443 Total reward: 11.0 Average reward fake: 0.08491090685129166 Average reward real: 0.7319391369819641 Training q_loss: 0.8249 Training g_loss: 8.0677 Training d_loss: 0.5962 Explore P: 0.4186\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 444 Total reward: 7.0 Average reward fake: 0.23094631731510162 Average reward real: 0.6901432275772095 Training q_loss: 0.6519 Training g_loss: 9.1009 Training d_loss: 0.7416 Explore P: 0.4183\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 445 Total reward: 19.0 Average reward fake: 0.25024405121803284 Average reward real: 0.5036270618438721 Training q_loss: 0.9373 Training g_loss: 6.7908 Training d_loss: 1.5082 Explore P: 0.4175\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 446 Total reward: 20.0 Average reward fake: 0.2331334352493286 Average reward real: 0.7789623141288757 Training q_loss: 0.2975 Training g_loss: 7.8668 Training d_loss: 0.6841 Explore P: 0.4167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 447 Total reward: 56.0 Average reward fake: 0.4221607446670532 Average reward real: 0.565623939037323 Training q_loss: 2.7459 Training g_loss: 6.1973 Training d_loss: 1.1357 Explore P: 0.4144\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 448 Total reward: 12.0 Average reward fake: 0.14507783949375153 Average reward real: 0.5584974884986877 Training q_loss: 0.6255 Training g_loss: 7.2307 Training d_loss: 0.9811 Explore P: 0.4140\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 449 Total reward: 15.0 Average reward fake: 0.26786699891090393 Average reward real: 0.5265086889266968 Training q_loss: 0.4493 Training g_loss: 5.4489 Training d_loss: 1.0382 Explore P: 0.4134\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 450 Total reward: 16.0 Average reward fake: 0.11047950387001038 Average reward real: 0.5835067629814148 Training q_loss: 0.3024 Training g_loss: 7.6137 Training d_loss: 0.8627 Explore P: 0.4127\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 451 Total reward: 16.0 Average reward fake: 0.17391660809516907 Average reward real: 0.7726907134056091 Training q_loss: 0.2158 Training g_loss: 7.1175 Training d_loss: 0.6865 Explore P: 0.4121\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 452 Total reward: 23.0 Average reward fake: 0.23237141966819763 Average reward real: 0.8376849293708801 Training q_loss: 0.3101 Training g_loss: 7.6968 Training d_loss: 0.6507 Explore P: 0.4111\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 453 Total reward: 33.0 Average reward fake: 0.39800235629081726 Average reward real: 0.9113022089004517 Training q_loss: 0.3450 Training g_loss: 10.4163 Training d_loss: 0.9574 Explore P: 0.4098\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 454 Total reward: 9.0 Average reward fake: 0.4384724795818329 Average reward real: 0.7035185098648071 Training q_loss: 0.4633 Training g_loss: 6.4728 Training d_loss: 1.6853 Explore P: 0.4095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 455 Total reward: 12.0 Average reward fake: 0.1890917420387268 Average reward real: 0.6292243003845215 Training q_loss: 0.6638 Training g_loss: 8.0433 Training d_loss: 0.9409 Explore P: 0.4090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 456 Total reward: 22.0 Average reward fake: 0.6122776865959167 Average reward real: 0.8771826028823853 Training q_loss: 0.1855 Training g_loss: 5.7781 Training d_loss: 1.6467 Explore P: 0.4081\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 457 Total reward: 27.0 Average reward fake: 0.28512486815452576 Average reward real: 0.8245401978492737 Training q_loss: 0.8843 Training g_loss: 10.3739 Training d_loss: 0.6173 Explore P: 0.4070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 458 Total reward: 38.0 Average reward fake: 0.252820760011673 Average reward real: 0.573147177696228 Training q_loss: 0.2981 Training g_loss: 4.8600 Training d_loss: 1.0051 Explore P: 0.4055\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 459 Total reward: 12.0 Average reward fake: 0.19488456845283508 Average reward real: 0.7100761532783508 Training q_loss: 0.1270 Training g_loss: 6.5246 Training d_loss: 0.7177 Explore P: 0.4050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 460 Total reward: 28.0 Average reward fake: 0.1462097316980362 Average reward real: 0.5915861129760742 Training q_loss: 0.4584 Training g_loss: 9.3302 Training d_loss: 0.9762 Explore P: 0.4039\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 461 Total reward: 16.0 Average reward fake: 0.2946876585483551 Average reward real: 0.7164028286933899 Training q_loss: 1.2310 Training g_loss: 5.6912 Training d_loss: 0.8269 Explore P: 0.4033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 462 Total reward: 25.0 Average reward fake: 0.12924964725971222 Average reward real: 0.39232194423675537 Training q_loss: 0.2877 Training g_loss: 4.8015 Training d_loss: 1.2171 Explore P: 0.4023\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 463 Total reward: 16.0 Average reward fake: 0.15969237685203552 Average reward real: 0.6828019618988037 Training q_loss: 0.6226 Training g_loss: 7.8247 Training d_loss: 0.7376 Explore P: 0.4017\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 464 Total reward: 18.0 Average reward fake: 0.14978550374507904 Average reward real: 0.5251458287239075 Training q_loss: 0.9993 Training g_loss: 5.3772 Training d_loss: 1.0839 Explore P: 0.4010\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 465 Total reward: 33.0 Average reward fake: 0.28030818700790405 Average reward real: 0.6066710352897644 Training q_loss: 0.8786 Training g_loss: 7.5381 Training d_loss: 1.0909 Explore P: 0.3997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 466 Total reward: 44.0 Average reward fake: 0.3088075518608093 Average reward real: 0.7261044979095459 Training q_loss: 0.1497 Training g_loss: 5.4047 Training d_loss: 0.8556 Explore P: 0.3980\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 467 Total reward: 10.0 Average reward fake: 0.32270291447639465 Average reward real: 0.5778774619102478 Training q_loss: 0.4494 Training g_loss: 7.6257 Training d_loss: 1.0397 Explore P: 0.3976\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 468 Total reward: 9.0 Average reward fake: 0.2049536556005478 Average reward real: 0.6100051999092102 Training q_loss: 0.4278 Training g_loss: 7.6640 Training d_loss: 0.9327 Explore P: 0.3973\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 469 Total reward: 14.0 Average reward fake: 0.31038203835487366 Average reward real: 0.5618863105773926 Training q_loss: 0.6117 Training g_loss: 6.8046 Training d_loss: 1.0094 Explore P: 0.3967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 470 Total reward: 13.0 Average reward fake: 0.3583931624889374 Average reward real: 0.735291600227356 Training q_loss: 0.5497 Training g_loss: 9.4825 Training d_loss: 1.0151 Explore P: 0.3962\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 471 Total reward: 8.0 Average reward fake: 0.21408577263355255 Average reward real: 0.7046427726745605 Training q_loss: 0.6762 Training g_loss: 9.2914 Training d_loss: 0.8236 Explore P: 0.3959\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "q_loss_list, g_loss_list, d_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     # Restore/load the trained model \n",
    "    #     #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        q_loss, g_loss, d_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                rewards_real_list.append((ep, rewards_real_mean))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Calculating real current reward and next action\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states}\n",
    "            next_actions_logits, rewards_fake, rewards_real = sess.run([model.actions_logits, \n",
    "                                                                        model.rewards_fake, model.rewards_real], \n",
    "                                                                       feed_dict)\n",
    "            #             feed_dict={model.states: next_states}\n",
    "            #             next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0) # NOTE: action size\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            #targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "            targetQs = rewards_real.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating/training/optimizing the model\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states,\n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_fake_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Fake rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_real_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Real rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/QGAN-cartpole.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the env\n",
    "# WARNING: If you close, you can NOT restart again!!!!!!\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
