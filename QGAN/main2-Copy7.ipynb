{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q learning (Q-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    #     print('state, action, reward, done, info')\n",
    "    #     print(state, action, reward, done, info)\n",
    "    if done:\n",
    "    #         print('state, action, reward, done, info')\n",
    "    #         print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current and next states given\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # Previous and current actions given\n",
    "    prev_actions = tf.placeholder(tf.int32, [None], name='prev_actions')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "\n",
    "    # Qs = qs+ (gamma * nextQs)/values/logits: using next_states and dones/end-of-episodes\n",
    "    nextQs = tf.placeholder(tf.float32, [None], name='nextQs') # masked\n",
    "    #nextQs_G = tf.placeholder(tf.float32, [None], name='nextQs_G') # masked\n",
    "    dones = tf.placeholder(tf.bool, [None], name='dones') # masked\n",
    "    nextQs_D = tf.placeholder(tf.float32, [None], name='nextQs_D') # masked\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, next_states, prev_actions, actions, nextQs, dones, nextQs_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generater: Generating/predicting action and next states\n",
    "def qfunction(prev_actions, states, action_size, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=(action_size + state_size))\n",
    "        actions_logits, next_states_logits = tf.split(axis=1, num_or_size_splits=[action_size, state_size], \n",
    "                                                      value=logits)\n",
    "        #predictions = tf.nn.softmax(actions_logits)\n",
    "        #predictions = tf.sigmoid(next_states_logits)\n",
    "\n",
    "        # return actions and states logits\n",
    "        return actions_logits, next_states_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(prev_actions, states, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fusing states and actions\n",
    "        x_fused = tf.concat(axis=1, values=[prev_actions, states])\n",
    "        #print(x_fused.shape)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        #print(h1.shape)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        #print(h2.shape)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return reward logits/Qs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original model loss\n",
    "def model_loss(prev_actions, states, actions, # model input data for Qs/qs/rs \n",
    "               nextQs, gamma, # model input data for targetQs\n",
    "               state_size, action_size, hidden_size): # model init for Qs\n",
    "    # Calculating Qs total rewards\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    actions_logits, _ = qfunction(prev_actions=prev_actions_onehot, states=states, \n",
    "                                  hidden_size=hidden_size, state_size=state_size, action_size=action_size)\n",
    "    \n",
    "    # Masking actions_logits unmasked\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs_masked = tf.multiply(actions_logits, actions_onehot)\n",
    "    Qs = tf.reduce_max(Qs_masked, axis=1)\n",
    "    \n",
    "    # Bellman equaion for calculating total rewards using current reward + total future rewards/nextQs\n",
    "    qs = tf.sigmoid(Qs) # qt\n",
    "    targetQs = qs + (gamma * nextQs)\n",
    "    \n",
    "    # Calculating the loss: logits/predictions vs labels\n",
    "    q_loss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "\n",
    "    return actions_logits, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new idea of G\n",
    "def model_output(actions, next_states,\n",
    "                 action_size, hidden_size):\n",
    "    # Discriminator for nextQs_D\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size)\n",
    "    nextQs_D_unmasked = discriminator(prev_actions=actions_onehot, states=next_states, hidden_size=hidden_size)\n",
    "    \n",
    "    # Returning unmasked nextQs_D to masked using dones/ends of episodes\n",
    "    return nextQs_D_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model loss for the NEW idea G & D\n",
    "def model_loss2(nextQs_D, gamma, \n",
    "                prev_actions, states, \n",
    "                action_size, hidden_size):\n",
    "    \n",
    "    # Calculating Qs total rewards using Discriminator\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    Qs = discriminator(prev_actions=prev_actions_onehot, states=states, hidden_size=hidden_size, reuse=True)\n",
    "        \n",
    "    # Bellman equaion: Qs = rt/qt + nextQs_G/D\n",
    "    qs = tf.sigmoid(Qs) # qt\n",
    "    targetQs_D = qs + (gamma * nextQs_D)\n",
    "    \n",
    "    # Calculating the loss: logits/predictions vs labels\n",
    "    d_loss = tf.reduce_mean(tf.square(Qs - targetQs_D))\n",
    "\n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss of generator based on the generated/predicted states and actions\n",
    "def model_loss3(nextQs_D, gamma,\n",
    "                prev_actions, states, dones, \n",
    "                state_size, action_size, hidden_size):\n",
    "    # Generator for nextQs_G\n",
    "    prev_actions_onehot = tf.one_hot(indices=prev_actions, depth=action_size)\n",
    "    actions_logits, next_states_logits = qfunction(prev_actions=prev_actions_onehot, states=states,\n",
    "                                                   hidden_size=hidden_size, state_size=state_size, \n",
    "                                                   action_size=action_size, reuse=True)\n",
    "    \n",
    "    # Discriminator for nextQs_G\n",
    "    nextQs_G_unmasked = discriminator(prev_actions=actions_logits, states=next_states_logits, \n",
    "                                      hidden_size=hidden_size, reuse=True)\n",
    "    \n",
    "    # Masking the unmasked nextQs_G using dones/end of episodes/goal\n",
    "    dones_mask = tf.reshape(tensor=(1 - tf.cast(dtype=nextQs_G_unmasked.dtype, x=dones)), shape=[-1, 1])\n",
    "    nextQs_G_masked = tf.multiply(nextQs_G_unmasked, dones_mask)\n",
    "    nextQs_G = tf.reduce_max(axis=1, input_tensor=nextQs_G_masked)\n",
    "    \n",
    "    # # Bellman equaion: Qs = rt/qt + nextQs_G/D\n",
    "    # qs = tf.sigmoid(Qs) # qt\n",
    "    # targetQs_G = qs + (gamma * nextQs_G)\n",
    "    # targetQs_D = qs + (gamma * nextQs_D)\n",
    "    # targetQs_G = targetQs_D\n",
    "    # nextQs_G = nextQs_D \n",
    "    \n",
    "    # Calculating the loss: logits/predictions vs labels\n",
    "    g_loss = tf.reduce_mean(tf.square(nextQs_G - nextQs_D))\n",
    "    \n",
    "    # Returning g_loss which should impact Generator\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction loss Tensor for action and next state prediction\n",
    "    :param g_loss: Qfunction loss Tensor for action and next state prediction\n",
    "    :param d_loss: Discriminator loss Tensor for reward prob/logits prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars) # action prediction\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=q_vars) # state prediction\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars) # state & action judge\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.next_states, self.prev_actions, self.actions, self.nextQs, self.dones, self.nextQs_D = model_input(\n",
    "            state_size=state_size)\n",
    "\n",
    "        # Loss of the Model: action prediction/generation\n",
    "        self.actions_logits, self.q_loss = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, gamma=gamma, # model init parameters\n",
    "            prev_actions=self.prev_actions, states=self.states, actions=self.actions, nextQs=self.nextQs) # model input data\n",
    "\n",
    "        # Output of the Model\n",
    "        self.nextQs_D_unmasked = model_output(actions=self.actions, next_states=self.next_states,\n",
    "                                              action_size=action_size, hidden_size=hidden_size)\n",
    "        \n",
    "        # Loss of the model: reward prob/logits prediction\n",
    "        self.d_loss = model_loss2(nextQs_D=self.nextQs_D, gamma=gamma,\n",
    "                                  action_size=action_size, hidden_size=hidden_size,\n",
    "                                  prev_actions=self.prev_actions, states=self.states)\n",
    "        \n",
    "        # Loss of the model: states prediction/generation\n",
    "        self.g_loss = model_loss3(nextQs_D=self.nextQs_D, gamma=gamma, dones=self.dones,\n",
    "                                  state_size=state_size, action_size=action_size, hidden_size=hidden_size,\n",
    "                                  prev_actions=self.prev_actions, states=self.states)\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss, \n",
    "                                                       g_loss=self.g_loss, \n",
    "                                                       d_loss=self.d_loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate, \n",
    "             gamma=gamma)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "prev_action = env.action_space.sample() # At-1\n",
    "state, reward, done, info = env.step(prev_action) # St, Rt/Et (Epiosde)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()# At\n",
    "    next_state, reward, done, info = env.step(action) #St+1\n",
    "\n",
    "    # End of the episodes which defines the goal of the episode/mission\n",
    "    if done is True:\n",
    "        \n",
    "        # Print out reward and done and check if they are the same: They are NOT.\n",
    "        #print('if done is true:', reward, done)\n",
    "        \n",
    "        # # the episode ends so no next state\n",
    "        # next_state = np.zeros(state.shape)\n",
    "                \n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        prev_action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(prev_action)\n",
    "    else:\n",
    "        # Print out reward and done and check if they are the same!\n",
    "        #print('else done is false:', reward, done)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((prev_action, state, action, next_state, done))\n",
    "        \n",
    "        # Prepare for the next round\n",
    "        prev_action = action\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 6.0 Training q_loss: 0.4341 Training g_loss: 0.0774 Training d_loss: 0.5206 Explore P: 0.9994\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 36.0 Training q_loss: 0.9165 Training g_loss: 2.6567 Training d_loss: 1.1492 Explore P: 0.9959\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 24.0 Training q_loss: 0.7980 Training g_loss: 0.4744 Training d_loss: 6.2689 Explore P: 0.9935\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 15.0 Training q_loss: 0.7689 Training g_loss: 0.3915 Training d_loss: 10.4577 Explore P: 0.9920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 54.0 Training q_loss: 0.4983 Training g_loss: 0.4641 Training d_loss: 16.4433 Explore P: 0.9867\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 41.0 Training q_loss: 0.4903 Training g_loss: 0.0958 Training d_loss: 18.4791 Explore P: 0.9827\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 12.0 Training q_loss: 0.4797 Training g_loss: 0.0346 Training d_loss: 16.8593 Explore P: 0.9816\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 11.0 Training q_loss: 0.4912 Training g_loss: 0.2293 Training d_loss: 20.6599 Explore P: 0.9805\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 13.0 Training q_loss: 0.4646 Training g_loss: 0.1754 Training d_loss: 16.5488 Explore P: 0.9792\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 32.0 Training q_loss: 0.4966 Training g_loss: 0.1934 Training d_loss: 13.8256 Explore P: 0.9761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 13.0 Training q_loss: 0.5043 Training g_loss: 0.5092 Training d_loss: 18.3786 Explore P: 0.9749\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 21.0 Training q_loss: 0.5454 Training g_loss: 0.8736 Training d_loss: 8.0440 Explore P: 0.9729\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 22.0 Training q_loss: 0.5582 Training g_loss: 0.0590 Training d_loss: 13.3173 Explore P: 0.9707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 11.0 Training q_loss: 0.5525 Training g_loss: 0.7207 Training d_loss: 17.0069 Explore P: 0.9697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 18.0 Training q_loss: 0.5303 Training g_loss: 0.2844 Training d_loss: 15.8548 Explore P: 0.9680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 22.0 Training q_loss: 0.5479 Training g_loss: 0.6481 Training d_loss: 15.1933 Explore P: 0.9659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 14.0 Training q_loss: 0.5176 Training g_loss: 0.1602 Training d_loss: 14.9388 Explore P: 0.9645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 17.0 Training q_loss: 0.5271 Training g_loss: 0.0799 Training d_loss: 10.9146 Explore P: 0.9629\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 36.0 Training q_loss: 0.5511 Training g_loss: 0.5623 Training d_loss: 19.4647 Explore P: 0.9595\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 31.0 Training q_loss: 0.5356 Training g_loss: 0.4478 Training d_loss: 17.2271 Explore P: 0.9565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 23.0 Training q_loss: 0.5493 Training g_loss: 0.0302 Training d_loss: 11.4221 Explore P: 0.9544\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 42.0 Training q_loss: 0.5495 Training g_loss: 0.3283 Training d_loss: 11.1931 Explore P: 0.9504\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 23.0 Training q_loss: 0.5227 Training g_loss: 0.5788 Training d_loss: 9.6049 Explore P: 0.9482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 13.0 Training q_loss: 0.5308 Training g_loss: 1.5301 Training d_loss: 15.2912 Explore P: 0.9470\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 10.0 Training q_loss: 0.5392 Training g_loss: 0.1979 Training d_loss: 14.0799 Explore P: 0.9461\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 14.0 Training q_loss: 0.5162 Training g_loss: 1.2542 Training d_loss: 8.6259 Explore P: 0.9448\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 16.0 Training q_loss: 0.5250 Training g_loss: 0.0752 Training d_loss: 16.3388 Explore P: 0.9433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 22.0 Training q_loss: 0.5205 Training g_loss: 0.5536 Training d_loss: 8.7811 Explore P: 0.9412\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 21.0 Training q_loss: 0.5070 Training g_loss: 0.9424 Training d_loss: 15.6070 Explore P: 0.9393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 27.0 Training q_loss: 0.5360 Training g_loss: 0.8907 Training d_loss: 14.1472 Explore P: 0.9368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 10.0 Training q_loss: 0.4844 Training g_loss: 0.2071 Training d_loss: 19.0572 Explore P: 0.9358\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 24.0 Training q_loss: 0.4862 Training g_loss: 0.6172 Training d_loss: 10.7093 Explore P: 0.9336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 25.0 Training q_loss: 0.4400 Training g_loss: 0.4556 Training d_loss: 11.6706 Explore P: 0.9313\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 18.0 Training q_loss: 0.4223 Training g_loss: 0.2460 Training d_loss: 22.4966 Explore P: 0.9297\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 17.0 Training q_loss: 0.4394 Training g_loss: 0.4448 Training d_loss: 14.9083 Explore P: 0.9281\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 15.0 Training q_loss: 0.4186 Training g_loss: 0.3686 Training d_loss: 18.9333 Explore P: 0.9267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 19.0 Training q_loss: 0.4257 Training g_loss: 0.4426 Training d_loss: 18.1578 Explore P: 0.9250\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 13.0 Training q_loss: 0.4242 Training g_loss: 0.3723 Training d_loss: 13.4362 Explore P: 0.9238\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 14.0 Training q_loss: 0.4268 Training g_loss: 0.5971 Training d_loss: 16.4160 Explore P: 0.9225\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 24.0 Training q_loss: 0.4362 Training g_loss: 0.5341 Training d_loss: 12.8395 Explore P: 0.9203\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 11.0 Training q_loss: 0.4561 Training g_loss: 0.3433 Training d_loss: 5.1277 Explore P: 0.9193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 8.0 Training q_loss: 0.4378 Training g_loss: 0.5482 Training d_loss: 16.5775 Explore P: 0.9186\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 16.0 Training q_loss: 0.4335 Training g_loss: 0.3982 Training d_loss: 19.3033 Explore P: 0.9171\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 36.0 Training q_loss: 0.4368 Training g_loss: 1.0044 Training d_loss: 7.4758 Explore P: 0.9139\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 23.0 Training q_loss: 0.4318 Training g_loss: 0.5629 Training d_loss: 15.7254 Explore P: 0.9118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 15.0 Training q_loss: 0.4321 Training g_loss: 0.5640 Training d_loss: 12.2663 Explore P: 0.9105\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 43.0 Training q_loss: 0.4121 Training g_loss: 0.6545 Training d_loss: 14.1849 Explore P: 0.9066\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 11.0 Training q_loss: 0.4217 Training g_loss: 0.2151 Training d_loss: 18.3697 Explore P: 0.9056\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 10.0 Training q_loss: 0.4113 Training g_loss: 0.5177 Training d_loss: 19.9422 Explore P: 0.9047\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 40.0 Training q_loss: 0.4255 Training g_loss: 0.2480 Training d_loss: 9.9795 Explore P: 0.9011\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 9.0 Training q_loss: 0.4303 Training g_loss: 0.7274 Training d_loss: 8.4875 Explore P: 0.9003\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 14.0 Training q_loss: 0.4331 Training g_loss: 1.0366 Training d_loss: 10.7531 Explore P: 0.8991\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 28.0 Training q_loss: 0.4237 Training g_loss: 0.5900 Training d_loss: 14.2400 Explore P: 0.8966\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 38.0 Training q_loss: 0.4259 Training g_loss: 0.6469 Training d_loss: 12.0156 Explore P: 0.8932\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 8.0 Training q_loss: 0.4293 Training g_loss: 0.2022 Training d_loss: 12.1214 Explore P: 0.8925\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 12.0 Training q_loss: 0.4560 Training g_loss: 0.4264 Training d_loss: 10.7541 Explore P: 0.8915\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 14.0 Training q_loss: 0.4422 Training g_loss: 0.3954 Training d_loss: 15.3302 Explore P: 0.8902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 16.0 Training q_loss: 0.4541 Training g_loss: 0.8333 Training d_loss: 7.2759 Explore P: 0.8888\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 10.0 Training q_loss: 0.4262 Training g_loss: 0.3038 Training d_loss: 7.5048 Explore P: 0.8880\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 14.0 Training q_loss: 0.4139 Training g_loss: 0.1624 Training d_loss: 21.9567 Explore P: 0.8867\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 13.0 Training q_loss: 0.4475 Training g_loss: 0.7331 Training d_loss: 13.2488 Explore P: 0.8856\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 15.0 Training q_loss: 0.4400 Training g_loss: 0.6219 Training d_loss: 16.3435 Explore P: 0.8843\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 16.0 Training q_loss: 0.4562 Training g_loss: 0.7987 Training d_loss: 9.0470 Explore P: 0.8829\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 13.0 Training q_loss: 0.4434 Training g_loss: 0.2421 Training d_loss: 24.3339 Explore P: 0.8818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 29.0 Training q_loss: 0.4617 Training g_loss: 0.9979 Training d_loss: 8.8839 Explore P: 0.8792\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 14.0 Training q_loss: 0.4422 Training g_loss: 0.2847 Training d_loss: 10.1960 Explore P: 0.8780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 33.0 Training q_loss: 0.4159 Training g_loss: 0.4070 Training d_loss: 15.5790 Explore P: 0.8752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 12.0 Training q_loss: 0.4135 Training g_loss: 0.8725 Training d_loss: 12.0393 Explore P: 0.8741\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 29.0 Training q_loss: 0.4293 Training g_loss: 0.1155 Training d_loss: 12.6616 Explore P: 0.8716\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 19.0 Training q_loss: 0.4486 Training g_loss: 0.8055 Training d_loss: 15.6626 Explore P: 0.8700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 20.0 Training q_loss: 0.4635 Training g_loss: 0.6676 Training d_loss: 11.5657 Explore P: 0.8683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 49.0 Training q_loss: 0.4291 Training g_loss: 0.4755 Training d_loss: 11.6913 Explore P: 0.8641\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 22.0 Training q_loss: 0.4382 Training g_loss: 0.4322 Training d_loss: 14.7303 Explore P: 0.8622\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 20.0 Training q_loss: 0.4449 Training g_loss: 0.0358 Training d_loss: 19.9816 Explore P: 0.8605\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 13.0 Training q_loss: 0.4534 Training g_loss: 0.9380 Training d_loss: 13.8563 Explore P: 0.8594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 10.0 Training q_loss: 0.4437 Training g_loss: 0.4703 Training d_loss: 12.4001 Explore P: 0.8585\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 16.0 Training q_loss: 0.4274 Training g_loss: 0.1738 Training d_loss: 13.0996 Explore P: 0.8572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 15.0 Training q_loss: 0.4490 Training g_loss: 0.3224 Training d_loss: 13.1390 Explore P: 0.8559\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 33.0 Training q_loss: 0.4522 Training g_loss: 0.7800 Training d_loss: 12.9391 Explore P: 0.8531\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 31.0 Training q_loss: 0.4144 Training g_loss: 0.2566 Training d_loss: 16.2374 Explore P: 0.8505\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 16.0 Training q_loss: 0.4495 Training g_loss: 0.7082 Training d_loss: 8.4127 Explore P: 0.8492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 14.0 Training q_loss: 0.4434 Training g_loss: 0.6150 Training d_loss: 13.8584 Explore P: 0.8480\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 19.0 Training q_loss: 0.4421 Training g_loss: 0.3265 Training d_loss: 10.5020 Explore P: 0.8464\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 40.0 Training q_loss: 0.4023 Training g_loss: 0.0890 Training d_loss: 13.5092 Explore P: 0.8431\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 12.0 Training q_loss: 0.4537 Training g_loss: 0.6402 Training d_loss: 17.6415 Explore P: 0.8421\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 10.0 Training q_loss: 0.4239 Training g_loss: 0.4476 Training d_loss: 11.3194 Explore P: 0.8412\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 23.0 Training q_loss: 0.4291 Training g_loss: 0.7982 Training d_loss: 13.5734 Explore P: 0.8393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 14.0 Training q_loss: 0.4203 Training g_loss: 0.5471 Training d_loss: 14.2815 Explore P: 0.8382\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 26.0 Training q_loss: 0.4675 Training g_loss: 0.8719 Training d_loss: 8.6547 Explore P: 0.8360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 13.0 Training q_loss: 0.4382 Training g_loss: 0.6494 Training d_loss: 9.5986 Explore P: 0.8349\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 11.0 Training q_loss: 0.4332 Training g_loss: 0.3295 Training d_loss: 16.5619 Explore P: 0.8340\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 12.0 Training q_loss: 0.4247 Training g_loss: 0.1844 Training d_loss: 14.4674 Explore P: 0.8330\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 17.0 Training q_loss: 0.4197 Training g_loss: 0.3137 Training d_loss: 19.0319 Explore P: 0.8316\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 28.0 Training q_loss: 0.4121 Training g_loss: 0.6782 Training d_loss: 13.0008 Explore P: 0.8293\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 22.0 Training q_loss: 0.4347 Training g_loss: 0.1859 Training d_loss: 14.3525 Explore P: 0.8275\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 9.0 Training q_loss: 0.4366 Training g_loss: 1.7285 Training d_loss: 14.8850 Explore P: 0.8268\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 13.0 Training q_loss: 0.4478 Training g_loss: 0.6674 Training d_loss: 16.3446 Explore P: 0.8257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 9.0 Training q_loss: 0.4560 Training g_loss: 1.5135 Training d_loss: 13.4141 Explore P: 0.8250\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 41.0 Training q_loss: 0.4281 Training g_loss: 0.3010 Training d_loss: 15.8981 Explore P: 0.8217\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 17.0 Training q_loss: 0.4336 Training g_loss: 0.4315 Training d_loss: 15.1580 Explore P: 0.8203\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 26.0 Training q_loss: 0.4554 Training g_loss: 0.4271 Training d_loss: 13.3695 Explore P: 0.8182\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 10.0 Training q_loss: 0.4426 Training g_loss: 0.7061 Training d_loss: 16.0125 Explore P: 0.8174\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 20.0 Training q_loss: 0.4780 Training g_loss: 1.0081 Training d_loss: 9.3187 Explore P: 0.8158\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 32.0 Training q_loss: 0.4558 Training g_loss: 0.7036 Training d_loss: 11.7454 Explore P: 0.8132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 28.0 Training q_loss: 0.4409 Training g_loss: 0.2718 Training d_loss: 24.1559 Explore P: 0.8110\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 9.0 Training q_loss: 0.4689 Training g_loss: 1.7518 Training d_loss: 13.1289 Explore P: 0.8102\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 16.0 Training q_loss: 0.4395 Training g_loss: 1.5730 Training d_loss: 10.4832 Explore P: 0.8090\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 17.0 Training q_loss: 0.4417 Training g_loss: 0.9639 Training d_loss: 8.1741 Explore P: 0.8076\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 40.0 Training q_loss: 0.4181 Training g_loss: 0.1351 Training d_loss: 21.0943 Explore P: 0.8044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 57.0 Training q_loss: 0.4516 Training g_loss: 0.3417 Training d_loss: 14.7489 Explore P: 0.7999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 25.0 Training q_loss: 0.4231 Training g_loss: 0.4801 Training d_loss: 8.5717 Explore P: 0.7979\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 12.0 Training q_loss: 0.4005 Training g_loss: 0.5551 Training d_loss: 19.0783 Explore P: 0.7970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 10.0 Training q_loss: 0.4177 Training g_loss: 0.3232 Training d_loss: 15.5591 Explore P: 0.7962\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 35.0 Training q_loss: 0.4305 Training g_loss: 0.2517 Training d_loss: 11.6997 Explore P: 0.7934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 30.0 Training q_loss: 0.4461 Training g_loss: 0.7293 Training d_loss: 12.4551 Explore P: 0.7911\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 12.0 Training q_loss: 0.4237 Training g_loss: 0.2857 Training d_loss: 13.4471 Explore P: 0.7902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 12.0 Training q_loss: 0.4438 Training g_loss: 0.3544 Training d_loss: 12.8258 Explore P: 0.7892\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 10.0 Training q_loss: 0.4531 Training g_loss: 1.0456 Training d_loss: 13.9194 Explore P: 0.7885\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 19.0 Training q_loss: 0.4260 Training g_loss: 0.8047 Training d_loss: 11.4041 Explore P: 0.7870\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 18.0 Training q_loss: 0.4415 Training g_loss: 0.9985 Training d_loss: 16.6833 Explore P: 0.7856\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 18.0 Training q_loss: 0.4847 Training g_loss: 0.9159 Training d_loss: 11.4299 Explore P: 0.7842\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 14.0 Training q_loss: 0.4607 Training g_loss: 0.6427 Training d_loss: 17.4103 Explore P: 0.7831\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 31.0 Training q_loss: 0.4336 Training g_loss: 0.8185 Training d_loss: 11.3984 Explore P: 0.7807\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 11.0 Training q_loss: 0.4199 Training g_loss: 0.5471 Training d_loss: 15.1599 Explore P: 0.7799\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 15.0 Training q_loss: 0.4116 Training g_loss: 0.0911 Training d_loss: 20.5043 Explore P: 0.7787\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 9.0 Training q_loss: 0.4473 Training g_loss: 1.1718 Training d_loss: 16.5809 Explore P: 0.7780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 18.0 Training q_loss: 0.4768 Training g_loss: 1.3355 Training d_loss: 8.2712 Explore P: 0.7766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 19.0 Training q_loss: 0.4233 Training g_loss: 0.8117 Training d_loss: 11.7772 Explore P: 0.7752\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 23.0 Training q_loss: 0.4024 Training g_loss: 0.0377 Training d_loss: 13.0254 Explore P: 0.7734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 36.0 Training q_loss: 0.4350 Training g_loss: 0.1995 Training d_loss: 14.4214 Explore P: 0.7707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 12.0 Training q_loss: 0.4368 Training g_loss: 1.1670 Training d_loss: 13.3843 Explore P: 0.7698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 11.0 Training q_loss: 0.4256 Training g_loss: 0.8156 Training d_loss: 10.8088 Explore P: 0.7689\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 16.0 Training q_loss: 0.4256 Training g_loss: 0.9045 Training d_loss: 11.5999 Explore P: 0.7677\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 29.0 Training q_loss: 0.4415 Training g_loss: 0.5754 Training d_loss: 6.1180 Explore P: 0.7655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 16.0 Training q_loss: 0.4136 Training g_loss: 0.3847 Training d_loss: 16.9457 Explore P: 0.7643\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 17.0 Training q_loss: 0.4126 Training g_loss: 0.2310 Training d_loss: 17.3530 Explore P: 0.7630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 48.0 Training q_loss: 0.4503 Training g_loss: 1.2792 Training d_loss: 9.0991 Explore P: 0.7594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 11.0 Training q_loss: 0.4209 Training g_loss: 0.4462 Training d_loss: 13.9197 Explore P: 0.7586\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 13.0 Training q_loss: 0.4418 Training g_loss: 1.0054 Training d_loss: 10.1217 Explore P: 0.7576\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 11.0 Training q_loss: 0.4346 Training g_loss: 0.1067 Training d_loss: 30.2795 Explore P: 0.7568\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 23.0 Training q_loss: 0.4638 Training g_loss: 0.7081 Training d_loss: 16.6232 Explore P: 0.7551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 11.0 Training q_loss: 0.4627 Training g_loss: 0.7797 Training d_loss: 4.9613 Explore P: 0.7543\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 11.0 Training q_loss: 0.4278 Training g_loss: 0.6451 Training d_loss: 8.1490 Explore P: 0.7535\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 12.0 Training q_loss: 0.4407 Training g_loss: 1.0406 Training d_loss: 13.2715 Explore P: 0.7526\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 10.0 Training q_loss: 0.4435 Training g_loss: 0.1654 Training d_loss: 14.6395 Explore P: 0.7518\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 12.0 Training q_loss: 0.4603 Training g_loss: 0.9458 Training d_loss: 25.3830 Explore P: 0.7509\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 10.0 Training q_loss: 0.5220 Training g_loss: 1.6465 Training d_loss: 8.9132 Explore P: 0.7502\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 9.0 Training q_loss: 0.4631 Training g_loss: 0.2507 Training d_loss: 14.2450 Explore P: 0.7495\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 18.0 Training q_loss: 0.4253 Training g_loss: 0.2504 Training d_loss: 14.0279 Explore P: 0.7482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 12.0 Training q_loss: 0.4675 Training g_loss: 2.1646 Training d_loss: 9.4652 Explore P: 0.7473\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 49.0 Training q_loss: 0.4124 Training g_loss: 0.4375 Training d_loss: 19.4190 Explore P: 0.7437\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 19.0 Training q_loss: 0.4304 Training g_loss: 0.4208 Training d_loss: 17.7476 Explore P: 0.7423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 12.0 Training q_loss: 0.4483 Training g_loss: 1.1679 Training d_loss: 14.1276 Explore P: 0.7414\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 16.0 Training q_loss: 0.4487 Training g_loss: 0.6924 Training d_loss: 11.6422 Explore P: 0.7403\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 16.0 Training q_loss: 0.4480 Training g_loss: 1.0897 Training d_loss: 6.9963 Explore P: 0.7391\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 11.0 Training q_loss: 0.4325 Training g_loss: 0.3948 Training d_loss: 12.7356 Explore P: 0.7383\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 19.0 Training q_loss: 0.4842 Training g_loss: 0.3025 Training d_loss: 16.2056 Explore P: 0.7369\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 17.0 Training q_loss: 0.4516 Training g_loss: 0.5788 Training d_loss: 13.6830 Explore P: 0.7357\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 9.0 Training q_loss: 0.4731 Training g_loss: 0.9993 Training d_loss: 7.0898 Explore P: 0.7350\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 24.0 Training q_loss: 0.4290 Training g_loss: 0.4227 Training d_loss: 21.7390 Explore P: 0.7333\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 9.0 Training q_loss: 0.4406 Training g_loss: 0.7028 Training d_loss: 20.8805 Explore P: 0.7326\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 14.0 Training q_loss: 0.4677 Training g_loss: 1.0567 Training d_loss: 5.1071 Explore P: 0.7316\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 13.0 Training q_loss: 0.4461 Training g_loss: 0.2416 Training d_loss: 9.0530 Explore P: 0.7307\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 27.0 Training q_loss: 0.4530 Training g_loss: 0.6178 Training d_loss: 14.2382 Explore P: 0.7287\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 11.0 Training q_loss: 0.4453 Training g_loss: 0.3524 Training d_loss: 12.2546 Explore P: 0.7280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 9.0 Training q_loss: 0.4939 Training g_loss: 1.3609 Training d_loss: 10.0898 Explore P: 0.7273\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 22.0 Training q_loss: 0.4652 Training g_loss: 0.5499 Training d_loss: 12.4135 Explore P: 0.7257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 17.0 Training q_loss: 0.4411 Training g_loss: 0.4940 Training d_loss: 14.6108 Explore P: 0.7245\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 34.0 Training q_loss: 0.4485 Training g_loss: 0.9665 Training d_loss: 18.0463 Explore P: 0.7221\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 13.0 Training q_loss: 0.4444 Training g_loss: 0.7686 Training d_loss: 11.9133 Explore P: 0.7212\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 10.0 Training q_loss: 0.4348 Training g_loss: 0.4420 Training d_loss: 11.7093 Explore P: 0.7205\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 10.0 Training q_loss: 0.4815 Training g_loss: 0.7166 Training d_loss: 10.3362 Explore P: 0.7197\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 10.0 Training q_loss: 0.4973 Training g_loss: 0.7133 Training d_loss: 14.6139 Explore P: 0.7190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 18.0 Training q_loss: 0.4292 Training g_loss: 0.5203 Training d_loss: 16.3592 Explore P: 0.7178\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 11.0 Training q_loss: 0.4161 Training g_loss: 0.6370 Training d_loss: 14.0979 Explore P: 0.7170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 14.0 Training q_loss: 0.4696 Training g_loss: 0.8717 Training d_loss: 14.0886 Explore P: 0.7160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 10.0 Training q_loss: 0.5072 Training g_loss: 0.8121 Training d_loss: 7.1958 Explore P: 0.7153\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 21.0 Training q_loss: 0.4464 Training g_loss: 0.7707 Training d_loss: 12.1547 Explore P: 0.7138\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 14.0 Training q_loss: 0.4715 Training g_loss: 1.1288 Training d_loss: 12.1489 Explore P: 0.7128\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 11.0 Training q_loss: 0.4780 Training g_loss: 0.4539 Training d_loss: 15.7583 Explore P: 0.7121\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 45.0 Training q_loss: 0.5106 Training g_loss: 1.0023 Training d_loss: 8.3624 Explore P: 0.7089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 19.0 Training q_loss: 0.4450 Training g_loss: 0.2994 Training d_loss: 16.0055 Explore P: 0.7076\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 12.0 Training q_loss: 0.4732 Training g_loss: 1.5348 Training d_loss: 10.9157 Explore P: 0.7067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 14.0 Training q_loss: 0.4659 Training g_loss: 0.4795 Training d_loss: 6.2915 Explore P: 0.7058\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 11.0 Training q_loss: 0.4620 Training g_loss: 0.5883 Training d_loss: 16.2306 Explore P: 0.7050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 30.0 Training q_loss: 0.4391 Training g_loss: 1.3571 Training d_loss: 10.2312 Explore P: 0.7029\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 7.0 Training q_loss: 0.4103 Training g_loss: 0.2233 Training d_loss: 25.6752 Explore P: 0.7024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 9.0 Training q_loss: 0.4479 Training g_loss: 1.6057 Training d_loss: 18.7283 Explore P: 0.7018\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 18.0 Training q_loss: 0.4727 Training g_loss: 1.3680 Training d_loss: 10.3701 Explore P: 0.7006\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 18.0 Training q_loss: 0.4953 Training g_loss: 0.8584 Training d_loss: 16.6782 Explore P: 0.6993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 13.0 Training q_loss: 0.5179 Training g_loss: 0.7654 Training d_loss: 14.7010 Explore P: 0.6984\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 16.0 Training q_loss: 0.4803 Training g_loss: 0.7817 Training d_loss: 10.0473 Explore P: 0.6973\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 25.0 Training q_loss: 0.4886 Training g_loss: 0.8900 Training d_loss: 14.7757 Explore P: 0.6956\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 10.0 Training q_loss: 0.5019 Training g_loss: 1.0087 Training d_loss: 9.6641 Explore P: 0.6949\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 29.0 Training q_loss: 0.5022 Training g_loss: 1.1797 Training d_loss: 18.8667 Explore P: 0.6929\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 10.0 Training q_loss: 0.4762 Training g_loss: 0.5111 Training d_loss: 19.0255 Explore P: 0.6923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 12.0 Training q_loss: 0.4759 Training g_loss: 1.3429 Training d_loss: 9.1342 Explore P: 0.6914\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 11.0 Training q_loss: 0.5194 Training g_loss: 1.0462 Training d_loss: 7.7857 Explore P: 0.6907\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 11.0 Training q_loss: 0.4777 Training g_loss: 0.9340 Training d_loss: 14.4117 Explore P: 0.6899\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 35.0 Training q_loss: 0.4809 Training g_loss: 0.8067 Training d_loss: 18.2990 Explore P: 0.6876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 12.0 Training q_loss: 0.5164 Training g_loss: 1.2205 Training d_loss: 7.0685 Explore P: 0.6868\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 12.0 Training q_loss: 0.5243 Training g_loss: 0.5876 Training d_loss: 12.6660 Explore P: 0.6859\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 15.0 Training q_loss: 0.4783 Training g_loss: 0.9051 Training d_loss: 12.1971 Explore P: 0.6849\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 16.0 Training q_loss: 0.4779 Training g_loss: 1.2185 Training d_loss: 11.4865 Explore P: 0.6838\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 12.0 Training q_loss: 0.5251 Training g_loss: 0.8885 Training d_loss: 10.4182 Explore P: 0.6830\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 15.0 Training q_loss: 0.5038 Training g_loss: 1.0625 Training d_loss: 17.3939 Explore P: 0.6820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 11.0 Training q_loss: 0.5087 Training g_loss: 0.5424 Training d_loss: 13.3302 Explore P: 0.6813\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 12.0 Training q_loss: 0.4736 Training g_loss: 0.8346 Training d_loss: 12.5582 Explore P: 0.6805\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 11.0 Training q_loss: 0.4665 Training g_loss: 0.9637 Training d_loss: 12.7307 Explore P: 0.6798\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 20.0 Training q_loss: 0.5164 Training g_loss: 1.0860 Training d_loss: 5.3335 Explore P: 0.6784\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 10.0 Training q_loss: 0.4891 Training g_loss: 1.2507 Training d_loss: 9.9157 Explore P: 0.6777\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 11.0 Training q_loss: 0.4917 Training g_loss: 0.9911 Training d_loss: 6.4807 Explore P: 0.6770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 13.0 Training q_loss: 0.5124 Training g_loss: 0.5548 Training d_loss: 21.8287 Explore P: 0.6761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 23.0 Training q_loss: 0.4776 Training g_loss: 0.7505 Training d_loss: 13.2463 Explore P: 0.6746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 12.0 Training q_loss: 0.5165 Training g_loss: 0.9005 Training d_loss: 10.5606 Explore P: 0.6738\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 50.0 Training q_loss: 0.5160 Training g_loss: 1.0871 Training d_loss: 12.5000 Explore P: 0.6705\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 57.0 Training q_loss: 0.4862 Training g_loss: 0.6260 Training d_loss: 15.2593 Explore P: 0.6668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 13.0 Training q_loss: 0.5528 Training g_loss: 0.9189 Training d_loss: 13.7732 Explore P: 0.6659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 22.0 Training q_loss: 0.5131 Training g_loss: 1.0251 Training d_loss: 15.2015 Explore P: 0.6645\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 22.0 Training q_loss: 0.5155 Training g_loss: 1.6144 Training d_loss: 9.9753 Explore P: 0.6630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 14.0 Training q_loss: 0.4658 Training g_loss: 1.0217 Training d_loss: 18.0163 Explore P: 0.6621\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 21.0 Training q_loss: 0.5546 Training g_loss: 1.2002 Training d_loss: 16.3429 Explore P: 0.6607\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 21.0 Training q_loss: 0.4971 Training g_loss: 1.2746 Training d_loss: 13.8491 Explore P: 0.6594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 15.0 Training q_loss: 0.5095 Training g_loss: 0.6880 Training d_loss: 16.9606 Explore P: 0.6584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 51.0 Training q_loss: 0.5241 Training g_loss: 0.6525 Training d_loss: 13.9260 Explore P: 0.6551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 23.0 Training q_loss: 0.5035 Training g_loss: 1.0591 Training d_loss: 13.0277 Explore P: 0.6536\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 34.0 Training q_loss: 0.5658 Training g_loss: 2.0695 Training d_loss: 11.5790 Explore P: 0.6514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 24.0 Training q_loss: 0.5167 Training g_loss: 1.4773 Training d_loss: 9.6653 Explore P: 0.6499\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 11.0 Training q_loss: 0.5388 Training g_loss: 0.2889 Training d_loss: 13.4381 Explore P: 0.6492\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 20.0 Training q_loss: 0.4656 Training g_loss: 0.7131 Training d_loss: 13.8166 Explore P: 0.6479\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 14.0 Training q_loss: 0.4828 Training g_loss: 1.3235 Training d_loss: 11.2956 Explore P: 0.6470\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 14.0 Training q_loss: 0.5317 Training g_loss: 0.5358 Training d_loss: 9.5700 Explore P: 0.6461\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 10.0 Training q_loss: 0.5290 Training g_loss: 1.7786 Training d_loss: 9.0265 Explore P: 0.6455\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 14.0 Training q_loss: 0.5388 Training g_loss: 0.8111 Training d_loss: 20.8701 Explore P: 0.6446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 12.0 Training q_loss: 0.5217 Training g_loss: 1.4381 Training d_loss: 16.4961 Explore P: 0.6438\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 9.0 Training q_loss: 0.4926 Training g_loss: 0.3620 Training d_loss: 16.1745 Explore P: 0.6433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 13.0 Training q_loss: 0.5246 Training g_loss: 0.6126 Training d_loss: 11.5054 Explore P: 0.6425\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 11.0 Training q_loss: 0.5577 Training g_loss: 1.2641 Training d_loss: 7.2435 Explore P: 0.6418\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 19.0 Training q_loss: 0.5383 Training g_loss: 1.4679 Training d_loss: 13.1087 Explore P: 0.6406\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 22.0 Training q_loss: 0.4917 Training g_loss: 0.9958 Training d_loss: 12.4940 Explore P: 0.6392\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 12.0 Training q_loss: 0.4886 Training g_loss: 1.0140 Training d_loss: 12.7166 Explore P: 0.6384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 12.0 Training q_loss: 0.5217 Training g_loss: 1.1511 Training d_loss: 9.2080 Explore P: 0.6377\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 21.0 Training q_loss: 0.4979 Training g_loss: 0.8768 Training d_loss: 12.0908 Explore P: 0.6363\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 12.0 Training q_loss: 0.5365 Training g_loss: 0.7752 Training d_loss: 11.8464 Explore P: 0.6356\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 10.0 Training q_loss: 0.5946 Training g_loss: 1.2816 Training d_loss: 10.3827 Explore P: 0.6350\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 11.0 Training q_loss: 0.5579 Training g_loss: 1.1756 Training d_loss: 5.0812 Explore P: 0.6343\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 14.0 Training q_loss: 0.5068 Training g_loss: 1.0091 Training d_loss: 12.2232 Explore P: 0.6334\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 14.0 Training q_loss: 0.5369 Training g_loss: 0.6890 Training d_loss: 8.5262 Explore P: 0.6325\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 11.0 Training q_loss: 0.5759 Training g_loss: 1.2421 Training d_loss: 8.9660 Explore P: 0.6319\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 15.0 Training q_loss: 0.5388 Training g_loss: 0.6282 Training d_loss: 16.9172 Explore P: 0.6309\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 15.0 Training q_loss: 0.5036 Training g_loss: 0.2536 Training d_loss: 18.8066 Explore P: 0.6300\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 14.0 Training q_loss: 0.5295 Training g_loss: 1.2397 Training d_loss: 11.6231 Explore P: 0.6291\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 18.0 Training q_loss: 0.4872 Training g_loss: 1.4377 Training d_loss: 13.6423 Explore P: 0.6280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 14.0 Training q_loss: 0.5586 Training g_loss: 0.6459 Training d_loss: 17.5485 Explore P: 0.6271\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 19.0 Training q_loss: 0.5110 Training g_loss: 0.6505 Training d_loss: 9.0062 Explore P: 0.6260\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 9.0 Training q_loss: 0.4979 Training g_loss: 0.5843 Training d_loss: 12.0788 Explore P: 0.6254\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 15.0 Training q_loss: 0.5580 Training g_loss: 0.5272 Training d_loss: 12.7441 Explore P: 0.6245\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 8.0 Training q_loss: 0.5333 Training g_loss: 0.9178 Training d_loss: 15.7822 Explore P: 0.6240\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 13.0 Training q_loss: 0.5261 Training g_loss: 0.8112 Training d_loss: 15.0668 Explore P: 0.6232\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 23.0 Training q_loss: 0.5806 Training g_loss: 1.2372 Training d_loss: 13.7740 Explore P: 0.6218\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 13.0 Training q_loss: 0.5107 Training g_loss: 1.6467 Training d_loss: 11.8811 Explore P: 0.6210\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 22.0 Training q_loss: 0.5376 Training g_loss: 0.6665 Training d_loss: 14.4259 Explore P: 0.6197\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 8.0 Training q_loss: 0.5803 Training g_loss: 0.9935 Training d_loss: 10.6883 Explore P: 0.6192\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 8.0 Training q_loss: 0.5634 Training g_loss: 1.4261 Training d_loss: 8.9088 Explore P: 0.6187\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 14.0 Training q_loss: 0.5884 Training g_loss: 1.6912 Training d_loss: 10.3816 Explore P: 0.6178\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 14.0 Training q_loss: 0.5819 Training g_loss: 0.5676 Training d_loss: 6.1457 Explore P: 0.6170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 14.0 Training q_loss: 0.5446 Training g_loss: 0.4205 Training d_loss: 18.1683 Explore P: 0.6161\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 10.0 Training q_loss: 0.5603 Training g_loss: 1.7670 Training d_loss: 8.8274 Explore P: 0.6155\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 13.0 Training q_loss: 0.6088 Training g_loss: 1.0455 Training d_loss: 9.3567 Explore P: 0.6147\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 32.0 Training q_loss: 0.5185 Training g_loss: 0.6051 Training d_loss: 12.9221 Explore P: 0.6128\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 16.0 Training q_loss: 0.5120 Training g_loss: 1.5251 Training d_loss: 7.5695 Explore P: 0.6118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 14.0 Training q_loss: 0.5870 Training g_loss: 0.9023 Training d_loss: 11.9664 Explore P: 0.6110\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 15.0 Training q_loss: 0.5614 Training g_loss: 0.6732 Training d_loss: 12.7973 Explore P: 0.6101\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 14.0 Training q_loss: 0.4987 Training g_loss: 1.3392 Training d_loss: 11.1925 Explore P: 0.6093\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 38.0 Training q_loss: 0.5204 Training g_loss: 1.4177 Training d_loss: 11.0623 Explore P: 0.6070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 10.0 Training q_loss: 0.5731 Training g_loss: 1.0997 Training d_loss: 14.5539 Explore P: 0.6064\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 13.0 Training q_loss: 0.5620 Training g_loss: 1.2782 Training d_loss: 12.6250 Explore P: 0.6056\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 13.0 Training q_loss: 0.5447 Training g_loss: 1.2987 Training d_loss: 10.2932 Explore P: 0.6048\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 33.0 Training q_loss: 0.5575 Training g_loss: 0.6092 Training d_loss: 15.4440 Explore P: 0.6029\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 11.0 Training q_loss: 0.5828 Training g_loss: 1.8607 Training d_loss: 11.6739 Explore P: 0.6022\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 25.0 Training q_loss: 0.5768 Training g_loss: 1.0695 Training d_loss: 10.6155 Explore P: 0.6008\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 34.0 Training q_loss: 0.5852 Training g_loss: 1.0471 Training d_loss: 11.9990 Explore P: 0.5988\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 16.0 Training q_loss: 0.5332 Training g_loss: 0.5633 Training d_loss: 18.1902 Explore P: 0.5978\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 21.0 Training q_loss: 0.6047 Training g_loss: 1.4543 Training d_loss: 13.2378 Explore P: 0.5966\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 13.0 Training q_loss: 0.5052 Training g_loss: 0.7280 Training d_loss: 17.6657 Explore P: 0.5958\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 10.0 Training q_loss: 0.5618 Training g_loss: 1.6555 Training d_loss: 12.6709 Explore P: 0.5952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 23.0 Training q_loss: 0.5347 Training g_loss: 0.9141 Training d_loss: 9.3620 Explore P: 0.5939\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 9.0 Training q_loss: 0.6107 Training g_loss: 2.0402 Training d_loss: 11.1571 Explore P: 0.5934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 37.0 Training q_loss: 0.6287 Training g_loss: 1.7500 Training d_loss: 15.1123 Explore P: 0.5912\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 18.0 Training q_loss: 0.5443 Training g_loss: 1.0446 Training d_loss: 10.5182 Explore P: 0.5902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 11.0 Training q_loss: 0.5524 Training g_loss: 1.5884 Training d_loss: 6.5779 Explore P: 0.5895\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 20.0 Training q_loss: 0.6081 Training g_loss: 1.1458 Training d_loss: 8.6955 Explore P: 0.5884\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 17.0 Training q_loss: 0.5732 Training g_loss: 1.1911 Training d_loss: 13.6019 Explore P: 0.5874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 18.0 Training q_loss: 0.6427 Training g_loss: 1.0191 Training d_loss: 10.0696 Explore P: 0.5863\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 27.0 Training q_loss: 0.5492 Training g_loss: 0.8935 Training d_loss: 10.3754 Explore P: 0.5848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 15.0 Training q_loss: 0.5989 Training g_loss: 0.4592 Training d_loss: 15.3310 Explore P: 0.5839\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 23.0 Training q_loss: 0.6114 Training g_loss: 1.1733 Training d_loss: 8.7180 Explore P: 0.5826\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 15.0 Training q_loss: 0.5810 Training g_loss: 1.0219 Training d_loss: 11.2291 Explore P: 0.5818\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 12.0 Training q_loss: 0.5742 Training g_loss: 0.5646 Training d_loss: 13.9528 Explore P: 0.5811\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 11.0 Training q_loss: 0.6135 Training g_loss: 1.3687 Training d_loss: 16.0295 Explore P: 0.5804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 21.0 Training q_loss: 0.5533 Training g_loss: 1.4873 Training d_loss: 13.7361 Explore P: 0.5792\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 11.0 Training q_loss: 0.6038 Training g_loss: 1.1162 Training d_loss: 11.9502 Explore P: 0.5786\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 9.0 Training q_loss: 0.6170 Training g_loss: 1.4572 Training d_loss: 9.8372 Explore P: 0.5781\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 9.0 Training q_loss: 0.6572 Training g_loss: 1.0312 Training d_loss: 10.5051 Explore P: 0.5776\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 11.0 Training q_loss: 0.5986 Training g_loss: 1.4639 Training d_loss: 10.5504 Explore P: 0.5770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 19.0 Training q_loss: 0.6262 Training g_loss: 1.4099 Training d_loss: 9.4489 Explore P: 0.5759\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 25.0 Training q_loss: 0.5846 Training g_loss: 0.3256 Training d_loss: 14.3682 Explore P: 0.5745\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 10.0 Training q_loss: 0.5681 Training g_loss: 1.4980 Training d_loss: 16.8105 Explore P: 0.5739\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 9.0 Training q_loss: 0.5764 Training g_loss: 0.8816 Training d_loss: 9.2640 Explore P: 0.5734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 14.0 Training q_loss: 0.6108 Training g_loss: 1.8454 Training d_loss: 10.0244 Explore P: 0.5726\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 12.0 Training q_loss: 0.5817 Training g_loss: 0.9216 Training d_loss: 6.4023 Explore P: 0.5719\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 9.0 Training q_loss: 0.5655 Training g_loss: 1.7687 Training d_loss: 12.3143 Explore P: 0.5714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 31.0 Training q_loss: 0.5911 Training g_loss: 1.1254 Training d_loss: 14.7814 Explore P: 0.5697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 12.0 Training q_loss: 0.6014 Training g_loss: 0.8847 Training d_loss: 9.0904 Explore P: 0.5690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 13.0 Training q_loss: 0.5785 Training g_loss: 1.3565 Training d_loss: 10.5978 Explore P: 0.5683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 40.0 Training q_loss: 0.5790 Training g_loss: 0.3822 Training d_loss: 17.9330 Explore P: 0.5661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 16.0 Training q_loss: 0.5971 Training g_loss: 0.4436 Training d_loss: 10.7753 Explore P: 0.5652\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 14.0 Training q_loss: 0.5964 Training g_loss: 1.0340 Training d_loss: 15.2687 Explore P: 0.5644\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 9.0 Training q_loss: 0.5846 Training g_loss: 1.2202 Training d_loss: 9.8310 Explore P: 0.5639\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 10.0 Training q_loss: 0.5842 Training g_loss: 1.4861 Training d_loss: 14.3815 Explore P: 0.5634\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 14.0 Training q_loss: 0.6091 Training g_loss: 1.6638 Training d_loss: 12.6271 Explore P: 0.5626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 33.0 Training q_loss: 0.5703 Training g_loss: 1.5269 Training d_loss: 8.1355 Explore P: 0.5608\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 13.0 Training q_loss: 0.5871 Training g_loss: 1.3799 Training d_loss: 11.0856 Explore P: 0.5600\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 323 Total reward: 30.0 Training q_loss: 0.5794 Training g_loss: 0.9500 Training d_loss: 8.8679 Explore P: 0.5584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 324 Total reward: 19.0 Training q_loss: 0.6917 Training g_loss: 3.4791 Training d_loss: 13.8849 Explore P: 0.5574\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 325 Total reward: 21.0 Training q_loss: 0.5469 Training g_loss: 1.8442 Training d_loss: 12.9507 Explore P: 0.5562\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 326 Total reward: 11.0 Training q_loss: 0.6818 Training g_loss: 0.9124 Training d_loss: 15.1504 Explore P: 0.5556\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 327 Total reward: 12.0 Training q_loss: 0.5886 Training g_loss: 0.9410 Training d_loss: 13.9985 Explore P: 0.5550\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 328 Total reward: 13.0 Training q_loss: 0.5494 Training g_loss: 1.0390 Training d_loss: 13.2789 Explore P: 0.5542\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 329 Total reward: 19.0 Training q_loss: 0.5888 Training g_loss: 1.6124 Training d_loss: 8.5197 Explore P: 0.5532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 330 Total reward: 8.0 Training q_loss: 0.5927 Training g_loss: 0.4231 Training d_loss: 15.6290 Explore P: 0.5528\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 331 Total reward: 11.0 Training q_loss: 0.6533 Training g_loss: 1.4845 Training d_loss: 12.6454 Explore P: 0.5522\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 332 Total reward: 17.0 Training q_loss: 0.5526 Training g_loss: 0.9920 Training d_loss: 9.8435 Explore P: 0.5513\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 333 Total reward: 8.0 Training q_loss: 0.5632 Training g_loss: 2.0831 Training d_loss: 11.6449 Explore P: 0.5508\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 334 Total reward: 9.0 Training q_loss: 0.6059 Training g_loss: 1.3032 Training d_loss: 14.2021 Explore P: 0.5503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 335 Total reward: 11.0 Training q_loss: 0.6318 Training g_loss: 1.1325 Training d_loss: 9.4288 Explore P: 0.5497\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 336 Total reward: 18.0 Training q_loss: 0.5794 Training g_loss: 0.8879 Training d_loss: 10.9817 Explore P: 0.5488\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 337 Total reward: 10.0 Training q_loss: 0.6101 Training g_loss: 1.6303 Training d_loss: 10.3727 Explore P: 0.5482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 338 Total reward: 28.0 Training q_loss: 0.5906 Training g_loss: 0.4305 Training d_loss: 12.4206 Explore P: 0.5467\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 339 Total reward: 36.0 Training q_loss: 0.6828 Training g_loss: 0.8979 Training d_loss: 14.0502 Explore P: 0.5448\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 340 Total reward: 18.0 Training q_loss: 0.6020 Training g_loss: 0.7105 Training d_loss: 13.7050 Explore P: 0.5438\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 341 Total reward: 8.0 Training q_loss: 0.6209 Training g_loss: 1.5202 Training d_loss: 13.7791 Explore P: 0.5434\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 342 Total reward: 25.0 Training q_loss: 0.6575 Training g_loss: 1.6431 Training d_loss: 14.0055 Explore P: 0.5421\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 343 Total reward: 10.0 Training q_loss: 0.6378 Training g_loss: 1.3294 Training d_loss: 8.4986 Explore P: 0.5416\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 344 Total reward: 17.0 Training q_loss: 0.5946 Training g_loss: 0.9694 Training d_loss: 9.8741 Explore P: 0.5407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 345 Total reward: 12.0 Training q_loss: 0.5966 Training g_loss: 0.8001 Training d_loss: 14.7209 Explore P: 0.5400\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 346 Total reward: 12.0 Training q_loss: 0.6243 Training g_loss: 1.8319 Training d_loss: 10.5407 Explore P: 0.5394\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 347 Total reward: 24.0 Training q_loss: 0.5896 Training g_loss: 1.2472 Training d_loss: 6.7614 Explore P: 0.5381\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 348 Total reward: 11.0 Training q_loss: 0.5732 Training g_loss: 1.1848 Training d_loss: 13.5203 Explore P: 0.5375\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 349 Total reward: 13.0 Training q_loss: 0.6200 Training g_loss: 1.3152 Training d_loss: 9.9969 Explore P: 0.5368\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 350 Total reward: 18.0 Training q_loss: 0.5862 Training g_loss: 1.4063 Training d_loss: 10.3941 Explore P: 0.5359\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 351 Total reward: 19.0 Training q_loss: 0.6960 Training g_loss: 1.9472 Training d_loss: 14.6765 Explore P: 0.5349\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 352 Total reward: 29.0 Training q_loss: 0.6996 Training g_loss: 0.8384 Training d_loss: 10.7199 Explore P: 0.5334\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 353 Total reward: 10.0 Training q_loss: 0.6911 Training g_loss: 1.5109 Training d_loss: 10.5249 Explore P: 0.5329\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 354 Total reward: 14.0 Training q_loss: 0.6161 Training g_loss: 0.3161 Training d_loss: 19.0750 Explore P: 0.5321\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 355 Total reward: 27.0 Training q_loss: 0.7167 Training g_loss: 1.2911 Training d_loss: 16.7978 Explore P: 0.5307\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 356 Total reward: 17.0 Training q_loss: 0.5489 Training g_loss: 0.9533 Training d_loss: 7.1977 Explore P: 0.5298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 357 Total reward: 8.0 Training q_loss: 0.5457 Training g_loss: 1.5643 Training d_loss: 17.4484 Explore P: 0.5294\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 358 Total reward: 10.0 Training q_loss: 0.6199 Training g_loss: 1.2374 Training d_loss: 12.9300 Explore P: 0.5289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 359 Total reward: 11.0 Training q_loss: 0.6327 Training g_loss: 1.4325 Training d_loss: 10.1107 Explore P: 0.5283\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 360 Total reward: 17.0 Training q_loss: 0.6957 Training g_loss: 1.9237 Training d_loss: 14.9379 Explore P: 0.5274\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 361 Total reward: 9.0 Training q_loss: 0.6750 Training g_loss: 1.5259 Training d_loss: 8.8375 Explore P: 0.5270\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 362 Total reward: 11.0 Training q_loss: 0.5946 Training g_loss: 1.2164 Training d_loss: 16.0817 Explore P: 0.5264\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 363 Total reward: 14.0 Training q_loss: 0.6633 Training g_loss: 1.3128 Training d_loss: 13.1416 Explore P: 0.5257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 364 Total reward: 11.0 Training q_loss: 0.6004 Training g_loss: 0.8006 Training d_loss: 12.1249 Explore P: 0.5251\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 365 Total reward: 15.0 Training q_loss: 0.6134 Training g_loss: 0.9912 Training d_loss: 8.5274 Explore P: 0.5244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 366 Total reward: 18.0 Training q_loss: 0.5960 Training g_loss: 0.6505 Training d_loss: 17.6383 Explore P: 0.5234\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 367 Total reward: 11.0 Training q_loss: 0.6017 Training g_loss: 1.9460 Training d_loss: 16.8941 Explore P: 0.5229\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 368 Total reward: 11.0 Training q_loss: 0.5948 Training g_loss: 1.9091 Training d_loss: 7.6918 Explore P: 0.5223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 369 Total reward: 20.0 Training q_loss: 0.6132 Training g_loss: 1.0331 Training d_loss: 16.2818 Explore P: 0.5213\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 370 Total reward: 16.0 Training q_loss: 0.5644 Training g_loss: 1.0508 Training d_loss: 10.3301 Explore P: 0.5205\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 371 Total reward: 15.0 Training q_loss: 0.5999 Training g_loss: 1.8515 Training d_loss: 9.2355 Explore P: 0.5197\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 372 Total reward: 8.0 Training q_loss: 0.6196 Training g_loss: 1.1877 Training d_loss: 15.1282 Explore P: 0.5193\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 373 Total reward: 10.0 Training q_loss: 0.6244 Training g_loss: 1.3182 Training d_loss: 10.9461 Explore P: 0.5188\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 374 Total reward: 12.0 Training q_loss: 0.6576 Training g_loss: 1.5157 Training d_loss: 10.3649 Explore P: 0.5182\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 375 Total reward: 32.0 Training q_loss: 0.6061 Training g_loss: 2.1711 Training d_loss: 6.4829 Explore P: 0.5165\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 376 Total reward: 12.0 Training q_loss: 0.5779 Training g_loss: 1.9110 Training d_loss: 6.9659 Explore P: 0.5159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 377 Total reward: 15.0 Training q_loss: 0.7047 Training g_loss: 1.9119 Training d_loss: 11.7396 Explore P: 0.5152\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 378 Total reward: 19.0 Training q_loss: 0.6822 Training g_loss: 2.0520 Training d_loss: 11.7841 Explore P: 0.5142\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 379 Total reward: 14.0 Training q_loss: 0.5873 Training g_loss: 2.2235 Training d_loss: 7.3817 Explore P: 0.5135\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 380 Total reward: 11.0 Training q_loss: 0.6310 Training g_loss: 0.8575 Training d_loss: 13.4079 Explore P: 0.5130\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 381 Total reward: 17.0 Training q_loss: 0.6709 Training g_loss: 0.7001 Training d_loss: 10.0805 Explore P: 0.5121\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 382 Total reward: 10.0 Training q_loss: 0.6940 Training g_loss: 2.9165 Training d_loss: 11.5078 Explore P: 0.5116\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 383 Total reward: 10.0 Training q_loss: 0.7020 Training g_loss: 1.0732 Training d_loss: 5.6301 Explore P: 0.5111\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 384 Total reward: 22.0 Training q_loss: 0.6204 Training g_loss: 0.8707 Training d_loss: 12.7493 Explore P: 0.5100\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 385 Total reward: 13.0 Training q_loss: 0.6170 Training g_loss: 1.4461 Training d_loss: 5.4733 Explore P: 0.5093\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 386 Total reward: 9.0 Training q_loss: 0.6499 Training g_loss: 0.9074 Training d_loss: 22.7500 Explore P: 0.5089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 387 Total reward: 8.0 Training q_loss: 0.6473 Training g_loss: 1.5379 Training d_loss: 13.5123 Explore P: 0.5085\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 388 Total reward: 24.0 Training q_loss: 0.6472 Training g_loss: 1.8673 Training d_loss: 11.5577 Explore P: 0.5073\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 389 Total reward: 28.0 Training q_loss: 0.6413 Training g_loss: 2.2766 Training d_loss: 14.5280 Explore P: 0.5059\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 390 Total reward: 15.0 Training q_loss: 0.6575 Training g_loss: 1.9198 Training d_loss: 8.1002 Explore P: 0.5052\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 391 Total reward: 10.0 Training q_loss: 0.6024 Training g_loss: 1.1692 Training d_loss: 12.7783 Explore P: 0.5047\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 392 Total reward: 13.0 Training q_loss: 0.6403 Training g_loss: 1.5305 Training d_loss: 10.9820 Explore P: 0.5040\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 393 Total reward: 14.0 Training q_loss: 0.6490 Training g_loss: 1.0104 Training d_loss: 10.6644 Explore P: 0.5033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 394 Total reward: 10.0 Training q_loss: 0.5815 Training g_loss: 1.2942 Training d_loss: 9.4516 Explore P: 0.5029\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 395 Total reward: 14.0 Training q_loss: 0.6509 Training g_loss: 1.2035 Training d_loss: 12.9291 Explore P: 0.5022\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 396 Total reward: 9.0 Training q_loss: 0.5746 Training g_loss: 1.6004 Training d_loss: 15.8656 Explore P: 0.5017\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 397 Total reward: 15.0 Training q_loss: 0.6368 Training g_loss: 1.4444 Training d_loss: 8.2716 Explore P: 0.5010\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 398 Total reward: 9.0 Training q_loss: 0.6677 Training g_loss: 1.5515 Training d_loss: 12.4864 Explore P: 0.5005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 399 Total reward: 19.0 Training q_loss: 0.6400 Training g_loss: 1.1900 Training d_loss: 15.3789 Explore P: 0.4996\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 400 Total reward: 12.0 Training q_loss: 0.5946 Training g_loss: 1.4690 Training d_loss: 12.5922 Explore P: 0.4990\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 401 Total reward: 14.0 Training q_loss: 0.6097 Training g_loss: 0.8560 Training d_loss: 16.4759 Explore P: 0.4983\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 402 Total reward: 13.0 Training q_loss: 0.6445 Training g_loss: 2.2808 Training d_loss: 10.7871 Explore P: 0.4977\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 403 Total reward: 14.0 Training q_loss: 0.6007 Training g_loss: 1.1208 Training d_loss: 16.9517 Explore P: 0.4970\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 404 Total reward: 7.0 Training q_loss: 0.6578 Training g_loss: 1.5941 Training d_loss: 18.4911 Explore P: 0.4967\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 405 Total reward: 8.0 Training q_loss: 0.6648 Training g_loss: 1.6021 Training d_loss: 10.7062 Explore P: 0.4963\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 406 Total reward: 9.0 Training q_loss: 0.5921 Training g_loss: 1.5688 Training d_loss: 6.7056 Explore P: 0.4959\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 407 Total reward: 13.0 Training q_loss: 0.6174 Training g_loss: 1.3545 Training d_loss: 13.1688 Explore P: 0.4952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 408 Total reward: 17.0 Training q_loss: 0.6573 Training g_loss: 1.1515 Training d_loss: 12.3676 Explore P: 0.4944\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 409 Total reward: 10.0 Training q_loss: 0.6274 Training g_loss: 1.1095 Training d_loss: 15.7653 Explore P: 0.4939\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 410 Total reward: 18.0 Training q_loss: 0.6417 Training g_loss: 1.6792 Training d_loss: 10.2976 Explore P: 0.4930\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 411 Total reward: 14.0 Training q_loss: 0.6297 Training g_loss: 1.6572 Training d_loss: 10.0457 Explore P: 0.4924\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 412 Total reward: 12.0 Training q_loss: 0.8142 Training g_loss: 1.4643 Training d_loss: 10.6811 Explore P: 0.4918\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 413 Total reward: 16.0 Training q_loss: 0.7029 Training g_loss: 1.2404 Training d_loss: 10.0552 Explore P: 0.4910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 414 Total reward: 10.0 Training q_loss: 0.8213 Training g_loss: 2.1703 Training d_loss: 7.9712 Explore P: 0.4905\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 415 Total reward: 10.0 Training q_loss: 0.7361 Training g_loss: 2.1042 Training d_loss: 7.9580 Explore P: 0.4901\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 416 Total reward: 24.0 Training q_loss: 0.7455 Training g_loss: 1.6655 Training d_loss: 13.4065 Explore P: 0.4889\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 417 Total reward: 12.0 Training q_loss: 0.7272 Training g_loss: 1.1393 Training d_loss: 13.6237 Explore P: 0.4883\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 418 Total reward: 7.0 Training q_loss: 0.6883 Training g_loss: 1.8950 Training d_loss: 11.0316 Explore P: 0.4880\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 419 Total reward: 38.0 Training q_loss: 0.6218 Training g_loss: 0.8224 Training d_loss: 13.2641 Explore P: 0.4862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 420 Total reward: 8.0 Training q_loss: 0.7315 Training g_loss: 1.1815 Training d_loss: 13.8447 Explore P: 0.4858\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 421 Total reward: 13.0 Training q_loss: 0.6268 Training g_loss: 0.8731 Training d_loss: 18.0272 Explore P: 0.4852\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 422 Total reward: 8.0 Training q_loss: 0.7028 Training g_loss: 1.7355 Training d_loss: 9.0558 Explore P: 0.4848\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 423 Total reward: 13.0 Training q_loss: 0.6111 Training g_loss: 1.0509 Training d_loss: 10.6700 Explore P: 0.4842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 424 Total reward: 16.0 Training q_loss: 0.6782 Training g_loss: 2.0477 Training d_loss: 13.6287 Explore P: 0.4834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 425 Total reward: 12.0 Training q_loss: 0.7037 Training g_loss: 0.8573 Training d_loss: 14.1797 Explore P: 0.4829\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 426 Total reward: 12.0 Training q_loss: 0.6916 Training g_loss: 3.2510 Training d_loss: 10.2329 Explore P: 0.4823\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 427 Total reward: 16.0 Training q_loss: 0.6296 Training g_loss: 1.7757 Training d_loss: 9.4185 Explore P: 0.4815\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 428 Total reward: 11.0 Training q_loss: 0.5801 Training g_loss: 0.7884 Training d_loss: 17.5166 Explore P: 0.4810\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 429 Total reward: 14.0 Training q_loss: 0.7472 Training g_loss: 1.5415 Training d_loss: 13.5512 Explore P: 0.4804\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 430 Total reward: 14.0 Training q_loss: 0.7722 Training g_loss: 1.2215 Training d_loss: 11.8666 Explore P: 0.4797\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 431 Total reward: 20.0 Training q_loss: 0.6592 Training g_loss: 1.5764 Training d_loss: 8.7755 Explore P: 0.4788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 432 Total reward: 16.0 Training q_loss: 0.5892 Training g_loss: 0.8205 Training d_loss: 16.4544 Explore P: 0.4780\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 433 Total reward: 15.0 Training q_loss: 0.7180 Training g_loss: 2.1451 Training d_loss: 6.6153 Explore P: 0.4773\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 434 Total reward: 7.0 Training q_loss: 0.5849 Training g_loss: 0.8281 Training d_loss: 14.4820 Explore P: 0.4770\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 435 Total reward: 11.0 Training q_loss: 0.6653 Training g_loss: 2.0932 Training d_loss: 9.7662 Explore P: 0.4765\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 436 Total reward: 9.0 Training q_loss: 0.6647 Training g_loss: 1.1667 Training d_loss: 9.6738 Explore P: 0.4761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 437 Total reward: 11.0 Training q_loss: 0.6336 Training g_loss: 1.7867 Training d_loss: 14.1325 Explore P: 0.4755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 438 Total reward: 10.0 Training q_loss: 0.6987 Training g_loss: 1.8249 Training d_loss: 12.7991 Explore P: 0.4751\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 439 Total reward: 34.0 Training q_loss: 0.8030 Training g_loss: 1.8238 Training d_loss: 6.1661 Explore P: 0.4735\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 440 Total reward: 12.0 Training q_loss: 0.6815 Training g_loss: 1.4401 Training d_loss: 11.0596 Explore P: 0.4729\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 441 Total reward: 17.0 Training q_loss: 0.7552 Training g_loss: 2.0173 Training d_loss: 4.2906 Explore P: 0.4722\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 442 Total reward: 11.0 Training q_loss: 0.5917 Training g_loss: 0.9180 Training d_loss: 13.2284 Explore P: 0.4716\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 443 Total reward: 9.0 Training q_loss: 0.7521 Training g_loss: 2.0310 Training d_loss: 10.6769 Explore P: 0.4712\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 444 Total reward: 17.0 Training q_loss: 0.7508 Training g_loss: 1.9479 Training d_loss: 9.8596 Explore P: 0.4705\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 445 Total reward: 10.0 Training q_loss: 0.6754 Training g_loss: 1.4929 Training d_loss: 12.3143 Explore P: 0.4700\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 446 Total reward: 14.0 Training q_loss: 0.7838 Training g_loss: 1.8368 Training d_loss: 8.5767 Explore P: 0.4693\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 447 Total reward: 11.0 Training q_loss: 0.6976 Training g_loss: 1.5531 Training d_loss: 8.1988 Explore P: 0.4688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 448 Total reward: 8.0 Training q_loss: 0.6806 Training g_loss: 1.1653 Training d_loss: 11.5301 Explore P: 0.4685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 449 Total reward: 10.0 Training q_loss: 0.7970 Training g_loss: 1.8215 Training d_loss: 4.6581 Explore P: 0.4680\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 450 Total reward: 16.0 Training q_loss: 0.6539 Training g_loss: 1.5038 Training d_loss: 12.8879 Explore P: 0.4673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 451 Total reward: 10.0 Training q_loss: 0.7439 Training g_loss: 2.0980 Training d_loss: 8.5946 Explore P: 0.4668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 452 Total reward: 10.0 Training q_loss: 0.6032 Training g_loss: 1.1480 Training d_loss: 14.1534 Explore P: 0.4664\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 453 Total reward: 13.0 Training q_loss: 0.6498 Training g_loss: 1.5730 Training d_loss: 16.4918 Explore P: 0.4658\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 454 Total reward: 11.0 Training q_loss: 0.7183 Training g_loss: 2.0376 Training d_loss: 5.3010 Explore P: 0.4653\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 455 Total reward: 12.0 Training q_loss: 0.6982 Training g_loss: 1.2326 Training d_loss: 16.9059 Explore P: 0.4647\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 456 Total reward: 12.0 Training q_loss: 0.7694 Training g_loss: 2.1807 Training d_loss: 9.1529 Explore P: 0.4642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 457 Total reward: 9.0 Training q_loss: 0.6677 Training g_loss: 0.9509 Training d_loss: 8.6891 Explore P: 0.4638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 458 Total reward: 18.0 Training q_loss: 0.7563 Training g_loss: 0.9220 Training d_loss: 11.1421 Explore P: 0.4630\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 459 Total reward: 14.0 Training q_loss: 0.6952 Training g_loss: 1.7349 Training d_loss: 10.8454 Explore P: 0.4623\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 460 Total reward: 10.0 Training q_loss: 0.7130 Training g_loss: 1.2629 Training d_loss: 11.5338 Explore P: 0.4619\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 461 Total reward: 11.0 Training q_loss: 0.7676 Training g_loss: 3.1842 Training d_loss: 9.0980 Explore P: 0.4614\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 462 Total reward: 15.0 Training q_loss: 0.6926 Training g_loss: 1.6756 Training d_loss: 9.1236 Explore P: 0.4607\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 463 Total reward: 9.0 Training q_loss: 0.6448 Training g_loss: 2.5298 Training d_loss: 6.1530 Explore P: 0.4603\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 464 Total reward: 9.0 Training q_loss: 0.6925 Training g_loss: 0.7710 Training d_loss: 8.3391 Explore P: 0.4599\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 465 Total reward: 18.0 Training q_loss: 0.7868 Training g_loss: 1.7169 Training d_loss: 12.0559 Explore P: 0.4591\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 466 Total reward: 13.0 Training q_loss: 0.8698 Training g_loss: 2.0883 Training d_loss: 12.6501 Explore P: 0.4585\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 467 Total reward: 10.0 Training q_loss: 0.6410 Training g_loss: 0.6304 Training d_loss: 14.6023 Explore P: 0.4581\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 468 Total reward: 11.0 Training q_loss: 0.6587 Training g_loss: 2.9876 Training d_loss: 8.5531 Explore P: 0.4576\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 469 Total reward: 10.0 Training q_loss: 0.8040 Training g_loss: 1.1454 Training d_loss: 13.4220 Explore P: 0.4571\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 470 Total reward: 9.0 Training q_loss: 0.7739 Training g_loss: 1.7516 Training d_loss: 7.9830 Explore P: 0.4567\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 471 Total reward: 12.0 Training q_loss: 0.7780 Training g_loss: 2.0741 Training d_loss: 8.4314 Explore P: 0.4562\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 472 Total reward: 10.0 Training q_loss: 0.6373 Training g_loss: 1.5733 Training d_loss: 10.7461 Explore P: 0.4557\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 473 Total reward: 13.0 Training q_loss: 0.7594 Training g_loss: 1.6441 Training d_loss: 10.9571 Explore P: 0.4551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 474 Total reward: 12.0 Training q_loss: 0.6558 Training g_loss: 1.0491 Training d_loss: 14.5711 Explore P: 0.4546\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 475 Total reward: 13.0 Training q_loss: 0.8429 Training g_loss: 1.7751 Training d_loss: 9.0914 Explore P: 0.4540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 476 Total reward: 21.0 Training q_loss: 0.6482 Training g_loss: 1.0175 Training d_loss: 7.5153 Explore P: 0.4531\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 477 Total reward: 9.0 Training q_loss: 0.6053 Training g_loss: 3.5043 Training d_loss: 9.1799 Explore P: 0.4527\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 478 Total reward: 8.0 Training q_loss: 0.5635 Training g_loss: 0.8039 Training d_loss: 13.1649 Explore P: 0.4524\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 479 Total reward: 21.0 Training q_loss: 0.7692 Training g_loss: 2.1319 Training d_loss: 9.3321 Explore P: 0.4514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 480 Total reward: 10.0 Training q_loss: 0.7837 Training g_loss: 1.8334 Training d_loss: 13.6593 Explore P: 0.4510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 481 Total reward: 8.0 Training q_loss: 0.8054 Training g_loss: 0.6303 Training d_loss: 9.5218 Explore P: 0.4506\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 482 Total reward: 19.0 Training q_loss: 0.6314 Training g_loss: 0.4228 Training d_loss: 9.2215 Explore P: 0.4498\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 483 Total reward: 15.0 Training q_loss: 0.7999 Training g_loss: 2.0103 Training d_loss: 13.9849 Explore P: 0.4491\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 484 Total reward: 12.0 Training q_loss: 0.8404 Training g_loss: 1.5578 Training d_loss: 10.6802 Explore P: 0.4486\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 485 Total reward: 16.0 Training q_loss: 0.6446 Training g_loss: 1.5842 Training d_loss: 10.1728 Explore P: 0.4479\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 486 Total reward: 40.0 Training q_loss: 0.8196 Training g_loss: 3.4986 Training d_loss: 14.6864 Explore P: 0.4462\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 487 Total reward: 15.0 Training q_loss: 0.6829 Training g_loss: 0.3502 Training d_loss: 14.4863 Explore P: 0.4455\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 488 Total reward: 8.0 Training q_loss: 0.8310 Training g_loss: 2.4591 Training d_loss: 14.3582 Explore P: 0.4452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 489 Total reward: 12.0 Training q_loss: 0.8020 Training g_loss: 0.9704 Training d_loss: 10.0940 Explore P: 0.4446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 490 Total reward: 16.0 Training q_loss: 0.7385 Training g_loss: 1.6200 Training d_loss: 17.1123 Explore P: 0.4439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 491 Total reward: 16.0 Training q_loss: 0.7041 Training g_loss: 2.8624 Training d_loss: 12.4428 Explore P: 0.4432\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 492 Total reward: 22.0 Training q_loss: 0.9179 Training g_loss: 2.5589 Training d_loss: 12.0574 Explore P: 0.4423\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 493 Total reward: 18.0 Training q_loss: 0.6788 Training g_loss: 1.2983 Training d_loss: 10.5649 Explore P: 0.4415\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 494 Total reward: 11.0 Training q_loss: 0.7654 Training g_loss: 0.7754 Training d_loss: 10.5327 Explore P: 0.4410\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 495 Total reward: 8.0 Training q_loss: 0.8190 Training g_loss: 3.5307 Training d_loss: 10.9971 Explore P: 0.4407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 496 Total reward: 13.0 Training q_loss: 0.8222 Training g_loss: 2.9614 Training d_loss: 9.6342 Explore P: 0.4401\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 497 Total reward: 19.0 Training q_loss: 0.7516 Training g_loss: 1.7510 Training d_loss: 11.8552 Explore P: 0.4393\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 498 Total reward: 11.0 Training q_loss: 0.7510 Training g_loss: 3.6152 Training d_loss: 11.1743 Explore P: 0.4388\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 499 Total reward: 13.0 Training q_loss: 0.8356 Training g_loss: 2.6534 Training d_loss: 9.5714 Explore P: 0.4383\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 500 Total reward: 11.0 Training q_loss: 0.9593 Training g_loss: 1.0586 Training d_loss: 15.9478 Explore P: 0.4378\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 501 Total reward: 12.0 Training q_loss: 0.9737 Training g_loss: 1.2619 Training d_loss: 13.1164 Explore P: 0.4373\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 502 Total reward: 11.0 Training q_loss: 0.8628 Training g_loss: 1.9946 Training d_loss: 12.7441 Explore P: 0.4368\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 503 Total reward: 11.0 Training q_loss: 0.8032 Training g_loss: 2.6881 Training d_loss: 7.4180 Explore P: 0.4364\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 504 Total reward: 10.0 Training q_loss: 0.8029 Training g_loss: 0.7157 Training d_loss: 11.8275 Explore P: 0.4359\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 505 Total reward: 11.0 Training q_loss: 0.7452 Training g_loss: 3.6390 Training d_loss: 11.7879 Explore P: 0.4355\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 506 Total reward: 20.0 Training q_loss: 0.7188 Training g_loss: 8.9956 Training d_loss: 11.9834 Explore P: 0.4346\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 507 Total reward: 10.0 Training q_loss: 0.7220 Training g_loss: 0.3700 Training d_loss: 10.0863 Explore P: 0.4342\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 508 Total reward: 14.0 Training q_loss: 0.7144 Training g_loss: 5.9066 Training d_loss: 8.9523 Explore P: 0.4336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 509 Total reward: 24.0 Training q_loss: 0.8733 Training g_loss: 2.8727 Training d_loss: 8.1465 Explore P: 0.4326\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 510 Total reward: 10.0 Training q_loss: 0.7471 Training g_loss: 3.2297 Training d_loss: 9.4794 Explore P: 0.4322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 511 Total reward: 19.0 Training q_loss: 0.9158 Training g_loss: 0.4735 Training d_loss: 11.0199 Explore P: 0.4314\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 512 Total reward: 13.0 Training q_loss: 0.9693 Training g_loss: 4.3758 Training d_loss: 17.1878 Explore P: 0.4308\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 513 Total reward: 10.0 Training q_loss: 0.8175 Training g_loss: 1.3929 Training d_loss: 15.9543 Explore P: 0.4304\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 514 Total reward: 8.0 Training q_loss: 0.8644 Training g_loss: 3.6224 Training d_loss: 7.5145 Explore P: 0.4301\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 515 Total reward: 12.0 Training q_loss: 0.8654 Training g_loss: 1.6623 Training d_loss: 4.0606 Explore P: 0.4296\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 516 Total reward: 16.0 Training q_loss: 2.0226 Training g_loss: 2.1869 Training d_loss: 14.0569 Explore P: 0.4289\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 517 Total reward: 12.0 Training q_loss: 4.1390 Training g_loss: 0.4085 Training d_loss: 17.1544 Explore P: 0.4284\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 518 Total reward: 11.0 Training q_loss: 6.9648 Training g_loss: 12.7646 Training d_loss: 10.2809 Explore P: 0.4279\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 519 Total reward: 13.0 Training q_loss: 9.2043 Training g_loss: 3.1766 Training d_loss: 10.4119 Explore P: 0.4274\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 520 Total reward: 18.0 Training q_loss: 17.3245 Training g_loss: 11.1787 Training d_loss: 8.3332 Explore P: 0.4266\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 521 Total reward: 11.0 Training q_loss: 21.9389 Training g_loss: 29.0647 Training d_loss: 8.5277 Explore P: 0.4262\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 522 Total reward: 11.0 Training q_loss: 51.5452 Training g_loss: 22.4729 Training d_loss: 9.8621 Explore P: 0.4257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 523 Total reward: 11.0 Training q_loss: 30.1264 Training g_loss: 24.1224 Training d_loss: 10.1408 Explore P: 0.4253\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 524 Total reward: 20.0 Training q_loss: 20.7044 Training g_loss: 10.0126 Training d_loss: 15.7406 Explore P: 0.4244\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 525 Total reward: 7.0 Training q_loss: 12.5413 Training g_loss: 19.1809 Training d_loss: 9.6112 Explore P: 0.4241\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 526 Total reward: 69.0 Training q_loss: 2.2134 Training g_loss: 0.6981 Training d_loss: 6.8440 Explore P: 0.4213\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 527 Total reward: 17.0 Training q_loss: 1.4433 Training g_loss: 0.7493 Training d_loss: 14.9995 Explore P: 0.4206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 528 Total reward: 19.0 Training q_loss: 1.0347 Training g_loss: 0.3857 Training d_loss: 6.3898 Explore P: 0.4198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 529 Total reward: 31.0 Training q_loss: 1.1739 Training g_loss: 2.1829 Training d_loss: 8.8023 Explore P: 0.4185\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 530 Total reward: 17.0 Training q_loss: 1.9969 Training g_loss: 0.1440 Training d_loss: 10.6922 Explore P: 0.4179\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 531 Total reward: 19.0 Training q_loss: 1.7240 Training g_loss: 0.2501 Training d_loss: 12.8129 Explore P: 0.4171\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 532 Total reward: 15.0 Training q_loss: 1.8564 Training g_loss: 0.1766 Training d_loss: 15.4367 Explore P: 0.4165\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 533 Total reward: 15.0 Training q_loss: 2.3061 Training g_loss: 0.3665 Training d_loss: 11.0501 Explore P: 0.4159\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 534 Total reward: 10.0 Training q_loss: 1.6459 Training g_loss: 0.1266 Training d_loss: 12.7561 Explore P: 0.4155\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 535 Total reward: 18.0 Training q_loss: 3.0630 Training g_loss: 0.1316 Training d_loss: 16.1452 Explore P: 0.4147\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 536 Total reward: 21.0 Training q_loss: 2.5413 Training g_loss: 0.1028 Training d_loss: 10.8923 Explore P: 0.4139\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 537 Total reward: 11.0 Training q_loss: 2.5230 Training g_loss: 0.3070 Training d_loss: 7.7114 Explore P: 0.4134\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 538 Total reward: 23.0 Training q_loss: 5.1571 Training g_loss: 0.1319 Training d_loss: 15.6522 Explore P: 0.4125\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 539 Total reward: 16.0 Training q_loss: 6.8428 Training g_loss: 0.3184 Training d_loss: 7.4670 Explore P: 0.4119\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 540 Total reward: 19.0 Training q_loss: 8.6559 Training g_loss: 0.2366 Training d_loss: 13.6382 Explore P: 0.4111\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 541 Total reward: 12.0 Training q_loss: 16.6036 Training g_loss: 0.5744 Training d_loss: 15.6525 Explore P: 0.4106\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 542 Total reward: 11.0 Training q_loss: 10.8094 Training g_loss: 0.3931 Training d_loss: 9.7157 Explore P: 0.4102\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 543 Total reward: 8.0 Training q_loss: 19.4435 Training g_loss: 0.7484 Training d_loss: 12.9139 Explore P: 0.4099\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 544 Total reward: 10.0 Training q_loss: 23.5829 Training g_loss: 0.5031 Training d_loss: 9.3821 Explore P: 0.4095\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 545 Total reward: 14.0 Training q_loss: 9.8493 Training g_loss: 0.6198 Training d_loss: 9.9999 Explore P: 0.4089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 546 Total reward: 14.0 Training q_loss: 18.6206 Training g_loss: 0.7581 Training d_loss: 10.0158 Explore P: 0.4083\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 547 Total reward: 16.0 Training q_loss: 9.8543 Training g_loss: 0.6812 Training d_loss: 18.3529 Explore P: 0.4077\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 548 Total reward: 68.0 Training q_loss: 6.7068 Training g_loss: 0.3671 Training d_loss: 9.1803 Explore P: 0.4050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 549 Total reward: 15.0 Training q_loss: 4.4658 Training g_loss: 0.3718 Training d_loss: 13.6770 Explore P: 0.4044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 550 Total reward: 25.0 Training q_loss: 6.8367 Training g_loss: 0.9861 Training d_loss: 10.4505 Explore P: 0.4034\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 551 Total reward: 28.0 Training q_loss: 4.9994 Training g_loss: 0.5904 Training d_loss: 14.4778 Explore P: 0.4023\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 552 Total reward: 19.0 Training q_loss: 5.7123 Training g_loss: 0.4618 Training d_loss: 16.8905 Explore P: 0.4016\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 553 Total reward: 19.0 Training q_loss: 4.4289 Training g_loss: 1.4401 Training d_loss: 12.5662 Explore P: 0.4008\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 554 Total reward: 19.0 Training q_loss: 4.4125 Training g_loss: 0.9188 Training d_loss: 10.0456 Explore P: 0.4001\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 555 Total reward: 21.0 Training q_loss: 5.1931 Training g_loss: 0.2235 Training d_loss: 12.4940 Explore P: 0.3993\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 556 Total reward: 14.0 Training q_loss: 3.8867 Training g_loss: 0.1526 Training d_loss: 15.5324 Explore P: 0.3987\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 557 Total reward: 15.0 Training q_loss: 5.8276 Training g_loss: 0.1117 Training d_loss: 10.6385 Explore P: 0.3982\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 558 Total reward: 23.0 Training q_loss: 4.6199 Training g_loss: 0.3036 Training d_loss: 12.2335 Explore P: 0.3973\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 559 Total reward: 16.0 Training q_loss: 6.3298 Training g_loss: 0.4467 Training d_loss: 8.0539 Explore P: 0.3966\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 560 Total reward: 19.0 Training q_loss: 5.3827 Training g_loss: 0.1526 Training d_loss: 15.1642 Explore P: 0.3959\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 561 Total reward: 19.0 Training q_loss: 4.5699 Training g_loss: 0.1070 Training d_loss: 13.5942 Explore P: 0.3952\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 562 Total reward: 14.0 Training q_loss: 6.1252 Training g_loss: 0.0861 Training d_loss: 15.2150 Explore P: 0.3946\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 563 Total reward: 17.0 Training q_loss: 3.5965 Training g_loss: 0.6608 Training d_loss: 15.5617 Explore P: 0.3940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 564 Total reward: 14.0 Training q_loss: 6.3604 Training g_loss: 0.2814 Training d_loss: 10.6517 Explore P: 0.3934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 565 Total reward: 21.0 Training q_loss: 3.9195 Training g_loss: 0.1617 Training d_loss: 12.9414 Explore P: 0.3926\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 566 Total reward: 17.0 Training q_loss: 5.7250 Training g_loss: 0.3282 Training d_loss: 11.0459 Explore P: 0.3920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 567 Total reward: 25.0 Training q_loss: 5.6786 Training g_loss: 0.3834 Training d_loss: 11.3120 Explore P: 0.3910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 568 Total reward: 28.0 Training q_loss: 5.1673 Training g_loss: 0.3521 Training d_loss: 17.1173 Explore P: 0.3900\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 569 Total reward: 15.0 Training q_loss: 3.6817 Training g_loss: 1.0020 Training d_loss: 11.6344 Explore P: 0.3894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 570 Total reward: 22.0 Training q_loss: 3.3990 Training g_loss: 1.9108 Training d_loss: 12.7907 Explore P: 0.3886\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 571 Total reward: 17.0 Training q_loss: 6.0133 Training g_loss: 1.0939 Training d_loss: 9.4532 Explore P: 0.3879\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 572 Total reward: 23.0 Training q_loss: 7.1984 Training g_loss: 0.1858 Training d_loss: 16.0476 Explore P: 0.3871\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 573 Total reward: 21.0 Training q_loss: 4.5108 Training g_loss: 0.1844 Training d_loss: 11.0904 Explore P: 0.3863\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 574 Total reward: 23.0 Training q_loss: 5.5584 Training g_loss: 0.0630 Training d_loss: 10.8979 Explore P: 0.3854\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 575 Total reward: 16.0 Training q_loss: 6.0310 Training g_loss: 0.0629 Training d_loss: 12.6333 Explore P: 0.3848\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list = []\n",
    "q_loss_list = []\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward = 0\n",
    "        q_loss = 0\n",
    "        g_loss = 0\n",
    "        d_loss = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                             model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits) # arg with max value/Q is the class of action\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done is True:\n",
    "                # the episode ends so no next state\n",
    "                #next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, reward, done, info = env.step(prev_action)\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((prev_action, state, action, next_state, done))\n",
    "                \n",
    "                # One step forward: At-1=At and St=St+1\n",
    "                prev_action = action\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            prev_actions = np.array([each[0] for each in batch])\n",
    "            states = np.array([each[1] for each in batch])\n",
    "            actions = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            #print(prev_actions.shape, states.shape, actions.shape, next_states.shape, dones.shape, dones.dtype)\n",
    "            #print(dones[:3])\n",
    "            \n",
    "            # Calculating nextQs and setting them to 0 for states where episode ends/fails\n",
    "            feed_dict={model.prev_actions: actions, \n",
    "                       model.states: next_states}\n",
    "            next_actions_logits = sess.run(model.actions_logits, feed_dict)            \n",
    "            next_actions_mask = (1 - dones.astype(next_actions_logits.dtype)).reshape(-1, 1) \n",
    "            nextQs_masked = np.multiply(next_actions_logits, next_actions_mask)\n",
    "            nextQs = np.max(nextQs_masked, axis=1)\n",
    "            \n",
    "            # Calculating nextQs for Discriminator using D(At-1, St)= Qt: NOT this one\n",
    "            # Calculating nextQs for Discriminator using D(At, St+1)= Qt+1/nextQs_D/nextQs\n",
    "            # Calculating nextQs for Discriminator using D(~At, ~St+1)= ~Qt+1/nextQs_G/nextQs2\n",
    "            feed_dict={model.prev_actions: prev_actions, model.states: states,\n",
    "                       model.actions: actions, model.next_states: next_states}\n",
    "            nextQs_D_unmasked = sess.run(model.nextQs_D_unmasked, feed_dict)\n",
    "            \n",
    "            # Masking for the end of episodes/ goals\n",
    "            dones_mask = (1 - dones.astype(nextQs_D_unmasked[0].dtype)).reshape(-1, 1)\n",
    "            nextQs_D_masked = np.multiply(nextQs_D_unmasked[0], dones_mask)\n",
    "            nextQs_D = np.max(nextQs_D_masked, axis=1)\n",
    "            \n",
    "            # Calculating nextQs for Discriminator using D(At-1, St)= Qt: NOT this one\n",
    "            # D(At-1, St)= Qs and qs = tf.sigmoid(Qs)\n",
    "            # NextQs/Qt+1 are given both:\n",
    "            # targetQs = qs + gamma * nextQs_G\n",
    "            # targetQs = qs + gamma * nextQs_D\n",
    "            feed_dict = {model.prev_actions: prev_actions, \n",
    "                         model.states: states, \n",
    "                         model.actions: actions, \n",
    "                         model.next_states: next_states, \n",
    "                         model.dones: dones,\n",
    "                         model.nextQs: nextQs,\n",
    "                         model.nextQs_D: nextQs_D}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            ################################################################################\n",
    "            ################################################################################\n",
    "                        \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from the model\n",
    "            feed_dict = {model.prev_actions: np.array([prev_action]), \n",
    "                         model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                prev_action = env.action_space.sample()\n",
    "                state, reward, done, _ = env.step(prev_action)\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Closing the env\n",
    "# # WARNING: If you close, you can NOT restart again!!!!!!\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
