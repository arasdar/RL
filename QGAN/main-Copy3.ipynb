{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QGAN: (Q-Net) + GAN (G-Net and D-Net)\n",
    "\n",
    "More specifically, we'll use Q-GAN to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command 'pip install -e gym/[all]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards, states, actions, dones = [], [], [], []\n",
    "for _ in range(10):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    states.append(state)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    dones.append(done)\n",
    "    #     print('state, action, reward, done, info')\n",
    "    #     print(state, action, reward, done, info)\n",
    "    if done:\n",
    "    #         print('state, action, reward, done, info')\n",
    "    #         print(state, action, reward, done, info)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(10,) (10, 4) (10,) (10,)\n",
      "float64 float64 int64 bool\n",
      "actions: 1 0\n",
      "rewards min and max: 1.0 1.0\n",
      "state size: (10, 4) action size: 2\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print('actions:', np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print('rewards min and max:', np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print('state size:', np.array(states).shape, \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of the model\n",
    "def model_input(state_size):\n",
    "    # Current states given: input data\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    \n",
    "    # Current actions given: indices\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    \n",
    "    # Next states given: next input data\n",
    "    next_states = tf.placeholder(tf.float32, [None, state_size], name='next_states')\n",
    "    \n",
    "    # TargetQs/values\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    \n",
    "    # returning the given data to the model\n",
    "    return states, actions, next_states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Qfunction/Encoder/Classifier\n",
    "def qfunction(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('qfunction', reuse=reuse):        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits: Sqeezed/compressed/represented states into actions size\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G: Generator/Decoder: actions can be given actions, generated actions\n",
    "def generator(states, actions, state_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Fuse compressed states (actions fake) with actions (actions real)\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions]) # NxD: axis1=N, and axis2=D\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=state_size)        \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return next_states_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: Descriminator/Reward function\n",
    "def discriminator(states, actions, next_states, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Fuse states, actions, and next states (St, at, St+1) and (St, ~at, ~St+1)\n",
    "        x_fused = tf.concat(axis=1, values=[states, actions, next_states]) # NxD: axis1=N, and axis2=D\n",
    "\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=x_fused, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        #         # Fused compressed states, actions, and compressed next_states (all three in action size)\n",
    "        #         #h3 = tf.layers.dense(inputs=nl2, units=action_size)\n",
    "        #         h3_fused = tf.concat(axis=1, values=[states, actions, nl2])\n",
    "        #         bn3 = tf.layers.batch_normalization(h3_fused, training=training)        \n",
    "        #         nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)   \n",
    "        #predictions = tf.sigmoid(logits)\n",
    "\n",
    "        # return reward logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(states, actions, next_states, targetQs, # model_input\n",
    "               state_size, action_size, hidden_size): # model_init\n",
    "    # DQN: Q-learning - Bellman equations: loss (targetQ - Q)^2\n",
    "    actions_logits = qfunction(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_real = tf.one_hot(indices=actions, depth=action_size)\n",
    "    Qs = tf.reduce_sum(tf.multiply(actions_logits, actions_real), axis=1)\n",
    "    q_loss = tf.reduce_mean(tf.square(targetQs - Qs))\n",
    "\n",
    "    # GAN: Generate next states\n",
    "    next_states_logits = generator(states=states, actions=actions_real, \n",
    "                                   state_size=state_size, hidden_size=hidden_size)\n",
    "        \n",
    "    # GAN: Discriminate between fake and real\n",
    "    actions_fake = tf.nn.softmax(actions_logits)\n",
    "    d_logits_fake = discriminator(states=states, actions=actions_fake, next_states=next_states_logits, \n",
    "                                  hidden_size=hidden_size, reuse=False)\n",
    "    d_logits_real = discriminator(states=states, actions=actions_real, next_states=next_states, \n",
    "                                  hidden_size=hidden_size, reuse=True)    \n",
    "\n",
    "    # GAN: Adverserial training - G-learning -  Relavistic GAN\n",
    "    g_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "    g_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.zeros_like(d_logits_real)))\n",
    "    g_loss = g_loss_real + g_loss_fake \n",
    "    \n",
    "    # GAN: Adverserial training - G-learning -  Variational AE\n",
    "    g_loss_reconst = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=next_states_logits, labels=tf.sigmoid(x=next_states)))\n",
    "    q_loss += g_loss\n",
    "    g_loss += g_loss_reconst\n",
    "    \n",
    "    # GAN: Adverserial training - D-learning-  Standard GAN\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Rewards fake/real\n",
    "    rewards_fake = tf.sigmoid(d_logits_fake)\n",
    "    rewards_real = tf.sigmoid(d_logits_real)\n",
    "\n",
    "    return actions_logits, q_loss, g_loss, d_loss, rewards_fake, rewards_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(q_loss, g_loss, d_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param q_loss: Qfunction/Value loss Tensor for next action prediction\n",
    "    :param g_loss: Generator/Decoder loss Tensor for next state prediction\n",
    "    :param d_loss: Discriminator/Reward loss Tensor for current reward function\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    return q_opt, g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAN:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.next_states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.q_loss, self.g_loss, self.d_loss, self.rewards_fake, self.rewards_real = model_loss(\n",
    "            state_size=state_size, action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, next_states=self.next_states, actions=self.actions, targetQs=self.targetQs) # model input data\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.q_opt, self.g_opt, self.d_opt = model_opt(q_loss=self.q_loss, g_loss=self.g_loss, d_loss=self.d_loss, \n",
    "                                                       learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4 action size: 2\n"
     ]
    }
   ],
   "source": [
    "print('state size:', np.array(states).shape[1], \n",
    "      'action size: {}'.format((np.max(np.array(actions)) - np.min(np.array(actions)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 2000          # max number of episodes to learn from\n",
    "max_steps = 2000000000000000   # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000           # memory capacity\n",
    "batch_size = 200               # experience mini-batch size\n",
    "learning_rate = 0.001          # learning rate for adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = QGAN(state_size=state_size, action_size=action_size, hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# init memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        \n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 0 Total reward: 3.0 Average reward fake: 0.49238717555999756 Average reward real: 0.5137783885002136 Training q_loss: 1.7447 Training g_loss: 2.1280 Training d_loss: 1.3535 Explore P: 0.9997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 1 Total reward: 20.0 Average reward fake: 0.45753809809684753 Average reward real: 0.581372082233429 Training q_loss: 2.1482 Training g_loss: 2.3678 Training d_loss: 1.1787 Explore P: 0.9977\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 2 Total reward: 11.0 Average reward fake: 0.48067089915275574 Average reward real: 0.5992328524589539 Training q_loss: 2.3570 Training g_loss: 2.4151 Training d_loss: 1.2444 Explore P: 0.9966\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 3 Total reward: 17.0 Average reward fake: 0.5521181225776672 Average reward real: 0.4613312780857086 Training q_loss: 1.9933 Training g_loss: 1.9612 Training d_loss: 1.6954 Explore P: 0.9950\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 4 Total reward: 14.0 Average reward fake: 0.470398873090744 Average reward real: 0.4334763288497925 Training q_loss: 3.8232 Training g_loss: 2.1354 Training d_loss: 1.5318 Explore P: 0.9936\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 5 Total reward: 16.0 Average reward fake: 0.29287996888160706 Average reward real: 0.6054462790489197 Training q_loss: 5.9347 Training g_loss: 3.0756 Training d_loss: 0.8938 Explore P: 0.9920\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 6 Total reward: 12.0 Average reward fake: 0.27665212750434875 Average reward real: 0.7145492434501648 Training q_loss: 7.4693 Training g_loss: 3.4350 Training d_loss: 0.7216 Explore P: 0.9908\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 7 Total reward: 8.0 Average reward fake: 0.309905469417572 Average reward real: 0.7246622443199158 Training q_loss: 10.3817 Training g_loss: 3.3347 Training d_loss: 0.7864 Explore P: 0.9901\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 8 Total reward: 11.0 Average reward fake: 0.3770218789577484 Average reward real: 0.6535304188728333 Training q_loss: 14.3972 Training g_loss: 2.9169 Training d_loss: 1.0512 Explore P: 0.9890\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 9 Total reward: 14.0 Average reward fake: 0.47994500398635864 Average reward real: 0.5483671426773071 Training q_loss: 9.0856 Training g_loss: 2.3427 Training d_loss: 1.4739 Explore P: 0.9876\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 10 Total reward: 32.0 Average reward fake: 0.3475988805294037 Average reward real: 0.6019149422645569 Training q_loss: 8.7676 Training g_loss: 3.0130 Training d_loss: 1.0426 Explore P: 0.9845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 11 Total reward: 16.0 Average reward fake: 0.1811196208000183 Average reward real: 0.6278859376907349 Training q_loss: 17.0489 Training g_loss: 3.7966 Training d_loss: 0.7512 Explore P: 0.9829\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 12 Total reward: 13.0 Average reward fake: 0.24740886688232422 Average reward real: 0.7493435144424438 Training q_loss: 16.5825 Training g_loss: 3.8356 Training d_loss: 0.6626 Explore P: 0.9817\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 13 Total reward: 26.0 Average reward fake: 0.3677917420864105 Average reward real: 0.7287782430648804 Training q_loss: 21.7319 Training g_loss: 3.3646 Training d_loss: 0.9421 Explore P: 0.9791\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 14 Total reward: 29.0 Average reward fake: 0.472739577293396 Average reward real: 0.7441900372505188 Training q_loss: 10.8009 Training g_loss: 3.2331 Training d_loss: 1.1190 Explore P: 0.9763\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 15 Total reward: 14.0 Average reward fake: 0.14414460957050323 Average reward real: 0.6288217306137085 Training q_loss: 32.2416 Training g_loss: 4.4498 Training d_loss: 0.7605 Explore P: 0.9750\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 16 Total reward: 16.0 Average reward fake: 0.24908030033111572 Average reward real: 0.7155290842056274 Training q_loss: 27.3431 Training g_loss: 3.8086 Training d_loss: 0.6560 Explore P: 0.9734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 17 Total reward: 15.0 Average reward fake: 0.2709237337112427 Average reward real: 0.6818622350692749 Training q_loss: 22.8440 Training g_loss: 3.3970 Training d_loss: 0.7487 Explore P: 0.9720\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 18 Total reward: 51.0 Average reward fake: 0.2439994066953659 Average reward real: 0.7147714495658875 Training q_loss: 23.5229 Training g_loss: 4.3357 Training d_loss: 0.7720 Explore P: 0.9671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 19 Total reward: 12.0 Average reward fake: 0.30795660614967346 Average reward real: 0.7833858728408813 Training q_loss: 29.0473 Training g_loss: 4.1400 Training d_loss: 0.7583 Explore P: 0.9659\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 20 Total reward: 22.0 Average reward fake: 0.18567287921905518 Average reward real: 0.7538695335388184 Training q_loss: 30.8248 Training g_loss: 4.2110 Training d_loss: 0.5393 Explore P: 0.9638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 21 Total reward: 22.0 Average reward fake: 0.36890408396720886 Average reward real: 0.6656884551048279 Training q_loss: 42.8695 Training g_loss: 3.3687 Training d_loss: 1.1221 Explore P: 0.9618\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 22 Total reward: 22.0 Average reward fake: 0.24561282992362976 Average reward real: 0.7791685461997986 Training q_loss: 33.2808 Training g_loss: 4.2347 Training d_loss: 0.6265 Explore P: 0.9597\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 23 Total reward: 12.0 Average reward fake: 0.2853304445743561 Average reward real: 0.7617213726043701 Training q_loss: 41.2186 Training g_loss: 4.0602 Training d_loss: 0.6950 Explore P: 0.9585\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 24 Total reward: 17.0 Average reward fake: 0.2708226442337036 Average reward real: 0.763134241104126 Training q_loss: 33.8181 Training g_loss: 3.9210 Training d_loss: 0.6887 Explore P: 0.9569\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 25 Total reward: 15.0 Average reward fake: 0.21555614471435547 Average reward real: 0.7679376006126404 Training q_loss: 52.8620 Training g_loss: 4.3614 Training d_loss: 0.6076 Explore P: 0.9555\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 26 Total reward: 29.0 Average reward fake: 0.19944924116134644 Average reward real: 0.7682585716247559 Training q_loss: 41.2108 Training g_loss: 5.2048 Training d_loss: 0.5509 Explore P: 0.9528\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 27 Total reward: 85.0 Average reward fake: 0.2676621675491333 Average reward real: 0.7671348452568054 Training q_loss: 75.4989 Training g_loss: 4.8873 Training d_loss: 0.6227 Explore P: 0.9448\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 28 Total reward: 18.0 Average reward fake: 0.19425421953201294 Average reward real: 0.7149896025657654 Training q_loss: 41.3101 Training g_loss: 4.9540 Training d_loss: 0.6259 Explore P: 0.9431\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 29 Total reward: 10.0 Average reward fake: 0.24491125345230103 Average reward real: 0.7917832136154175 Training q_loss: 26.5425 Training g_loss: 5.5979 Training d_loss: 0.5423 Explore P: 0.9422\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 30 Total reward: 17.0 Average reward fake: 0.19387511909008026 Average reward real: 0.7435203790664673 Training q_loss: 18.2281 Training g_loss: 5.1409 Training d_loss: 0.5950 Explore P: 0.9406\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 31 Total reward: 44.0 Average reward fake: 0.26919475197792053 Average reward real: 0.7881271243095398 Training q_loss: 35.9116 Training g_loss: 5.4384 Training d_loss: 0.6057 Explore P: 0.9365\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 32 Total reward: 15.0 Average reward fake: 0.15655162930488586 Average reward real: 0.8179475665092468 Training q_loss: 62.4918 Training g_loss: 6.3515 Training d_loss: 0.4081 Explore P: 0.9351\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 33 Total reward: 14.0 Average reward fake: 0.16740207374095917 Average reward real: 0.794585108757019 Training q_loss: 61.4014 Training g_loss: 6.1189 Training d_loss: 0.4421 Explore P: 0.9338\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 34 Total reward: 17.0 Average reward fake: 0.22781500220298767 Average reward real: 0.8185073733329773 Training q_loss: 31.6978 Training g_loss: 6.1042 Training d_loss: 0.5057 Explore P: 0.9322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 35 Total reward: 46.0 Average reward fake: 0.2208690643310547 Average reward real: 0.762704074382782 Training q_loss: 63.0046 Training g_loss: 6.0371 Training d_loss: 0.6004 Explore P: 0.9280\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 36 Total reward: 14.0 Average reward fake: 0.13877014815807343 Average reward real: 0.8029468655586243 Training q_loss: 47.4395 Training g_loss: 6.7965 Training d_loss: 0.4252 Explore P: 0.9267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 37 Total reward: 52.0 Average reward fake: 0.16266299784183502 Average reward real: 0.8256852030754089 Training q_loss: 36.7159 Training g_loss: 6.4625 Training d_loss: 0.4306 Explore P: 0.9220\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 38 Total reward: 15.0 Average reward fake: 0.11477340757846832 Average reward real: 0.7734518647193909 Training q_loss: 39.2604 Training g_loss: 6.6557 Training d_loss: 0.4371 Explore P: 0.9206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 39 Total reward: 15.0 Average reward fake: 0.18116575479507446 Average reward real: 0.808911919593811 Training q_loss: 42.8647 Training g_loss: 6.2839 Training d_loss: 0.4528 Explore P: 0.9192\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 40 Total reward: 35.0 Average reward fake: 0.15172620117664337 Average reward real: 0.8430579900741577 Training q_loss: 16.9650 Training g_loss: 6.8860 Training d_loss: 0.3942 Explore P: 0.9161\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 41 Total reward: 14.0 Average reward fake: 0.15297657251358032 Average reward real: 0.7909655570983887 Training q_loss: 46.7538 Training g_loss: 6.4933 Training d_loss: 0.4991 Explore P: 0.9148\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 42 Total reward: 33.0 Average reward fake: 0.1791304349899292 Average reward real: 0.8708997368812561 Training q_loss: 22.9442 Training g_loss: 7.3458 Training d_loss: 0.3730 Explore P: 0.9118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 43 Total reward: 16.0 Average reward fake: 0.15363913774490356 Average reward real: 0.8073626756668091 Training q_loss: 44.8967 Training g_loss: 6.8533 Training d_loss: 0.4609 Explore P: 0.9104\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 44 Total reward: 14.0 Average reward fake: 0.2397063821554184 Average reward real: 0.8317424058914185 Training q_loss: 16.8599 Training g_loss: 6.3162 Training d_loss: 0.5383 Explore P: 0.9091\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 45 Total reward: 11.0 Average reward fake: 0.13422302901744843 Average reward real: 0.9052736163139343 Training q_loss: 101.7994 Training g_loss: 7.7923 Training d_loss: 0.2862 Explore P: 0.9081\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 46 Total reward: 13.0 Average reward fake: 0.13974522054195404 Average reward real: 0.7270910143852234 Training q_loss: 54.0708 Training g_loss: 7.9285 Training d_loss: 0.6521 Explore P: 0.9070\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 47 Total reward: 15.0 Average reward fake: 0.22498540580272675 Average reward real: 0.8636030554771423 Training q_loss: 44.0944 Training g_loss: 7.0909 Training d_loss: 0.4533 Explore P: 0.9056\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 48 Total reward: 16.0 Average reward fake: 0.1732444316148758 Average reward real: 0.8634973168373108 Training q_loss: 19.4807 Training g_loss: 6.8697 Training d_loss: 0.4687 Explore P: 0.9042\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 49 Total reward: 37.0 Average reward fake: 0.12725526094436646 Average reward real: 0.8194766044616699 Training q_loss: 29.1156 Training g_loss: 7.3264 Training d_loss: 0.4430 Explore P: 0.9009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 50 Total reward: 15.0 Average reward fake: 0.11719231307506561 Average reward real: 0.8206673264503479 Training q_loss: 42.4894 Training g_loss: 7.1148 Training d_loss: 0.3793 Explore P: 0.8995\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 51 Total reward: 16.0 Average reward fake: 0.16309309005737305 Average reward real: 0.8596148490905762 Training q_loss: 36.2994 Training g_loss: 7.4666 Training d_loss: 0.4387 Explore P: 0.8981\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 52 Total reward: 12.0 Average reward fake: 0.1710301786661148 Average reward real: 0.7575700283050537 Training q_loss: 54.7264 Training g_loss: 7.3220 Training d_loss: 0.5762 Explore P: 0.8971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 53 Total reward: 17.0 Average reward fake: 0.1313362717628479 Average reward real: 0.8825803399085999 Training q_loss: 22.2317 Training g_loss: 7.7816 Training d_loss: 0.3357 Explore P: 0.8955\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 54 Total reward: 18.0 Average reward fake: 0.1429816335439682 Average reward real: 0.8292167782783508 Training q_loss: 17.1709 Training g_loss: 7.2252 Training d_loss: 0.4023 Explore P: 0.8940\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 55 Total reward: 19.0 Average reward fake: 0.21281860768795013 Average reward real: 0.8484092950820923 Training q_loss: 22.7017 Training g_loss: 7.4141 Training d_loss: 0.4686 Explore P: 0.8923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 56 Total reward: 21.0 Average reward fake: 0.13078325986862183 Average reward real: 0.8417649865150452 Training q_loss: 49.3160 Training g_loss: 7.3950 Training d_loss: 0.3759 Explore P: 0.8904\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 57 Total reward: 12.0 Average reward fake: 0.18724381923675537 Average reward real: 0.7749107480049133 Training q_loss: 15.0015 Training g_loss: 7.0401 Training d_loss: 0.5334 Explore P: 0.8894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 58 Total reward: 32.0 Average reward fake: 0.22280319035053253 Average reward real: 0.831125020980835 Training q_loss: 39.7476 Training g_loss: 6.8325 Training d_loss: 0.4732 Explore P: 0.8866\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 59 Total reward: 25.0 Average reward fake: 0.1269553303718567 Average reward real: 0.8176043629646301 Training q_loss: 32.8282 Training g_loss: 7.1059 Training d_loss: 0.3784 Explore P: 0.8844\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 60 Total reward: 10.0 Average reward fake: 0.06448113918304443 Average reward real: 0.8052218556404114 Training q_loss: 39.8710 Training g_loss: 7.8743 Training d_loss: 0.3587 Explore P: 0.8835\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 61 Total reward: 23.0 Average reward fake: 0.1817248910665512 Average reward real: 0.8500573039054871 Training q_loss: 70.3478 Training g_loss: 7.0979 Training d_loss: 0.3771 Explore P: 0.8815\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 62 Total reward: 11.0 Average reward fake: 0.07620807737112045 Average reward real: 0.8563827276229858 Training q_loss: 43.8260 Training g_loss: 7.8446 Training d_loss: 0.2916 Explore P: 0.8805\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 63 Total reward: 19.0 Average reward fake: 0.2814443111419678 Average reward real: 0.8896955847740173 Training q_loss: 20.8823 Training g_loss: 7.3211 Training d_loss: 0.5290 Explore P: 0.8789\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 64 Total reward: 35.0 Average reward fake: 0.09602124243974686 Average reward real: 0.8851020932197571 Training q_loss: 28.9620 Training g_loss: 7.7582 Training d_loss: 0.2608 Explore P: 0.8758\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 65 Total reward: 17.0 Average reward fake: 0.2047313153743744 Average reward real: 0.8484470248222351 Training q_loss: 13.9404 Training g_loss: 6.8744 Training d_loss: 0.4568 Explore P: 0.8744\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 66 Total reward: 9.0 Average reward fake: 0.11886993050575256 Average reward real: 0.8988240957260132 Training q_loss: 44.9672 Training g_loss: 7.7242 Training d_loss: 0.2467 Explore P: 0.8736\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 67 Total reward: 10.0 Average reward fake: 0.09453283995389938 Average reward real: 0.9013413190841675 Training q_loss: 61.0407 Training g_loss: 7.7039 Training d_loss: 0.2939 Explore P: 0.8727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 68 Total reward: 10.0 Average reward fake: 0.2306841015815735 Average reward real: 0.8256704807281494 Training q_loss: 13.7694 Training g_loss: 6.7893 Training d_loss: 0.5369 Explore P: 0.8719\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 69 Total reward: 21.0 Average reward fake: 0.08576841652393341 Average reward real: 0.8094969987869263 Training q_loss: 16.5456 Training g_loss: 7.7211 Training d_loss: 0.3653 Explore P: 0.8701\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 70 Total reward: 21.0 Average reward fake: 0.1236233338713646 Average reward real: 0.8911586999893188 Training q_loss: 26.8123 Training g_loss: 7.7958 Training d_loss: 0.2881 Explore P: 0.8683\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 71 Total reward: 25.0 Average reward fake: 0.1155589297413826 Average reward real: 0.8698179721832275 Training q_loss: 40.1660 Training g_loss: 8.3179 Training d_loss: 0.2737 Explore P: 0.8661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 72 Total reward: 27.0 Average reward fake: 0.19690631330013275 Average reward real: 0.8564813137054443 Training q_loss: 53.1718 Training g_loss: 7.4015 Training d_loss: 0.3892 Explore P: 0.8638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 73 Total reward: 21.0 Average reward fake: 0.13280634582042694 Average reward real: 0.8844192624092102 Training q_loss: 16.9308 Training g_loss: 7.9086 Training d_loss: 0.2901 Explore P: 0.8620\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 74 Total reward: 26.0 Average reward fake: 0.17080368101596832 Average reward real: 0.8969589471817017 Training q_loss: 48.5765 Training g_loss: 7.5624 Training d_loss: 0.3689 Explore P: 0.8598\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 75 Total reward: 11.0 Average reward fake: 0.0955299437046051 Average reward real: 0.7757298946380615 Training q_loss: 37.4186 Training g_loss: 7.9909 Training d_loss: 0.4034 Explore P: 0.8589\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 76 Total reward: 19.0 Average reward fake: 0.17578327655792236 Average reward real: 0.8750468492507935 Training q_loss: 14.1550 Training g_loss: 7.4419 Training d_loss: 0.3680 Explore P: 0.8573\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 77 Total reward: 14.0 Average reward fake: 0.14768189191818237 Average reward real: 0.8725738525390625 Training q_loss: 40.9224 Training g_loss: 7.7980 Training d_loss: 0.3224 Explore P: 0.8561\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 78 Total reward: 37.0 Average reward fake: 0.2462213635444641 Average reward real: 0.9101737141609192 Training q_loss: 25.3419 Training g_loss: 7.1009 Training d_loss: 0.4086 Explore P: 0.8529\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 79 Total reward: 34.0 Average reward fake: 0.16586290299892426 Average reward real: 0.8810662627220154 Training q_loss: 45.0953 Training g_loss: 7.6745 Training d_loss: 0.3005 Explore P: 0.8501\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 80 Total reward: 15.0 Average reward fake: 0.1350782811641693 Average reward real: 0.8642861843109131 Training q_loss: 42.8697 Training g_loss: 7.7275 Training d_loss: 0.3225 Explore P: 0.8488\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 81 Total reward: 17.0 Average reward fake: 0.13156157732009888 Average reward real: 0.8623005747795105 Training q_loss: 23.0929 Training g_loss: 7.7190 Training d_loss: 0.3144 Explore P: 0.8474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 82 Total reward: 8.0 Average reward fake: 0.15364491939544678 Average reward real: 0.8607660531997681 Training q_loss: 17.4960 Training g_loss: 7.3265 Training d_loss: 0.3265 Explore P: 0.8467\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 83 Total reward: 12.0 Average reward fake: 0.08321305364370346 Average reward real: 0.8296332359313965 Training q_loss: 19.9639 Training g_loss: 8.7051 Training d_loss: 0.3069 Explore P: 0.8457\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 84 Total reward: 11.0 Average reward fake: 0.12106861919164658 Average reward real: 0.8845805525779724 Training q_loss: 21.5027 Training g_loss: 7.7041 Training d_loss: 0.2911 Explore P: 0.8448\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 85 Total reward: 25.0 Average reward fake: 0.06635107845067978 Average reward real: 0.7995755672454834 Training q_loss: 50.1289 Training g_loss: 7.8535 Training d_loss: 0.3366 Explore P: 0.8427\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 86 Total reward: 10.0 Average reward fake: 0.1306575983762741 Average reward real: 0.8410490155220032 Training q_loss: 34.5821 Training g_loss: 7.8058 Training d_loss: 0.3074 Explore P: 0.8419\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 87 Total reward: 44.0 Average reward fake: 0.10961000621318817 Average reward real: 0.9022384881973267 Training q_loss: 29.4758 Training g_loss: 8.4537 Training d_loss: 0.2209 Explore P: 0.8382\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 88 Total reward: 11.0 Average reward fake: 0.14901411533355713 Average reward real: 0.8614973425865173 Training q_loss: 17.2098 Training g_loss: 7.8292 Training d_loss: 0.3494 Explore P: 0.8373\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 89 Total reward: 12.0 Average reward fake: 0.08318597823381424 Average reward real: 0.8528456091880798 Training q_loss: 51.9765 Training g_loss: 8.8262 Training d_loss: 0.2755 Explore P: 0.8363\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 90 Total reward: 30.0 Average reward fake: 0.12136812508106232 Average reward real: 0.857310950756073 Training q_loss: 162.1597 Training g_loss: 8.1020 Training d_loss: 0.3006 Explore P: 0.8339\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 91 Total reward: 8.0 Average reward fake: 0.11849472671747208 Average reward real: 0.8687384128570557 Training q_loss: 39.0502 Training g_loss: 8.1119 Training d_loss: 0.3221 Explore P: 0.8332\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 92 Total reward: 12.0 Average reward fake: 0.16417770087718964 Average reward real: 0.8255947232246399 Training q_loss: 20.3453 Training g_loss: 6.6805 Training d_loss: 0.4158 Explore P: 0.8322\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 93 Total reward: 21.0 Average reward fake: 0.09896309673786163 Average reward real: 0.889380931854248 Training q_loss: 50.5444 Training g_loss: 8.4406 Training d_loss: 0.2561 Explore P: 0.8305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 94 Total reward: 10.0 Average reward fake: 0.12189680337905884 Average reward real: 0.7895339727401733 Training q_loss: 66.9664 Training g_loss: 7.6847 Training d_loss: 0.4674 Explore P: 0.8297\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 95 Total reward: 27.0 Average reward fake: 0.1672075241804123 Average reward real: 0.8874985575675964 Training q_loss: 19.5692 Training g_loss: 7.8984 Training d_loss: 0.3591 Explore P: 0.8275\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 96 Total reward: 51.0 Average reward fake: 0.07770179957151413 Average reward real: 0.7488148212432861 Training q_loss: 89.7908 Training g_loss: 8.2504 Training d_loss: 0.4493 Explore P: 0.8233\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 97 Total reward: 16.0 Average reward fake: 0.11720432341098785 Average reward real: 0.7691136002540588 Training q_loss: 24.6932 Training g_loss: 8.1379 Training d_loss: 0.4707 Explore P: 0.8220\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 98 Total reward: 55.0 Average reward fake: 0.14268158376216888 Average reward real: 0.9235488772392273 Training q_loss: 19.6099 Training g_loss: 8.2319 Training d_loss: 0.4013 Explore P: 0.8175\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 99 Total reward: 18.0 Average reward fake: 0.13394203782081604 Average reward real: 0.8824988007545471 Training q_loss: 37.6957 Training g_loss: 8.6930 Training d_loss: 0.3384 Explore P: 0.8161\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 100 Total reward: 13.0 Average reward fake: 0.20427405834197998 Average reward real: 0.9267999529838562 Training q_loss: 53.6173 Training g_loss: 9.4229 Training d_loss: 0.2766 Explore P: 0.8151\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 101 Total reward: 14.0 Average reward fake: 0.0491030216217041 Average reward real: 0.718528151512146 Training q_loss: 39.0409 Training g_loss: 9.5519 Training d_loss: 0.5010 Explore P: 0.8139\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 102 Total reward: 27.0 Average reward fake: 0.09685344994068146 Average reward real: 0.7594091892242432 Training q_loss: 96.0477 Training g_loss: 7.9065 Training d_loss: 0.4145 Explore P: 0.8118\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 103 Total reward: 17.0 Average reward fake: 0.09947791695594788 Average reward real: 0.7604604959487915 Training q_loss: 50.1075 Training g_loss: 8.3281 Training d_loss: 0.4331 Explore P: 0.8104\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 104 Total reward: 8.0 Average reward fake: 0.08458241820335388 Average reward real: 0.7562292218208313 Training q_loss: 25.1393 Training g_loss: 8.8343 Training d_loss: 0.4737 Explore P: 0.8098\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 105 Total reward: 11.0 Average reward fake: 0.12460071593523026 Average reward real: 0.9189115166664124 Training q_loss: 40.4438 Training g_loss: 9.0428 Training d_loss: 0.2116 Explore P: 0.8089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 106 Total reward: 20.0 Average reward fake: 0.13977965712547302 Average reward real: 0.9034562706947327 Training q_loss: 38.3082 Training g_loss: 7.7437 Training d_loss: 0.3385 Explore P: 0.8073\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 107 Total reward: 36.0 Average reward fake: 0.11160657554864883 Average reward real: 0.8731937408447266 Training q_loss: 19.0866 Training g_loss: 8.5702 Training d_loss: 0.3087 Explore P: 0.8044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 108 Total reward: 19.0 Average reward fake: 0.08980821818113327 Average reward real: 0.7947671413421631 Training q_loss: 34.3060 Training g_loss: 8.0015 Training d_loss: 0.3561 Explore P: 0.8029\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 109 Total reward: 10.0 Average reward fake: 0.18907280266284943 Average reward real: 0.9073017835617065 Training q_loss: 20.6717 Training g_loss: 7.8584 Training d_loss: 0.3184 Explore P: 0.8021\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 110 Total reward: 13.0 Average reward fake: 0.126348614692688 Average reward real: 0.8791490197181702 Training q_loss: 28.3045 Training g_loss: 8.6744 Training d_loss: 0.2988 Explore P: 0.8011\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 111 Total reward: 18.0 Average reward fake: 0.1232122927904129 Average reward real: 0.9231957793235779 Training q_loss: 77.3205 Training g_loss: 8.7208 Training d_loss: 0.2460 Explore P: 0.7997\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 112 Total reward: 17.0 Average reward fake: 0.12115537375211716 Average reward real: 0.8867807984352112 Training q_loss: 22.1495 Training g_loss: 8.1065 Training d_loss: 0.2944 Explore P: 0.7983\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 113 Total reward: 14.0 Average reward fake: 0.13666968047618866 Average reward real: 0.9122204780578613 Training q_loss: 22.3101 Training g_loss: 8.7710 Training d_loss: 0.2398 Explore P: 0.7972\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 114 Total reward: 38.0 Average reward fake: 0.15250428020954132 Average reward real: 0.9143319725990295 Training q_loss: 43.4739 Training g_loss: 8.4192 Training d_loss: 0.2961 Explore P: 0.7942\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 115 Total reward: 13.0 Average reward fake: 0.16008073091506958 Average reward real: 0.8993701338768005 Training q_loss: 21.9930 Training g_loss: 9.0285 Training d_loss: 0.2372 Explore P: 0.7932\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 116 Total reward: 12.0 Average reward fake: 0.11483463644981384 Average reward real: 0.8764762878417969 Training q_loss: 22.5850 Training g_loss: 9.3570 Training d_loss: 0.3566 Explore P: 0.7923\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 117 Total reward: 14.0 Average reward fake: 0.10147202759981155 Average reward real: 0.8284211754798889 Training q_loss: 51.4809 Training g_loss: 8.4966 Training d_loss: 0.3096 Explore P: 0.7912\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 118 Total reward: 16.0 Average reward fake: 0.11741048842668533 Average reward real: 0.85209059715271 Training q_loss: 108.4076 Training g_loss: 8.6960 Training d_loss: 0.2874 Explore P: 0.7899\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 119 Total reward: 20.0 Average reward fake: 0.11765249073505402 Average reward real: 0.8094406127929688 Training q_loss: 40.2102 Training g_loss: 9.3388 Training d_loss: 0.3288 Explore P: 0.7884\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 120 Total reward: 39.0 Average reward fake: 0.11845733970403671 Average reward real: 0.8146448731422424 Training q_loss: 42.8468 Training g_loss: 9.2013 Training d_loss: 0.3088 Explore P: 0.7853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 121 Total reward: 18.0 Average reward fake: 0.14401155710220337 Average reward real: 0.9206944108009338 Training q_loss: 83.3696 Training g_loss: 9.3834 Training d_loss: 0.2940 Explore P: 0.7839\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 122 Total reward: 25.0 Average reward fake: 0.11692377924919128 Average reward real: 0.9455746412277222 Training q_loss: 31.9383 Training g_loss: 9.0865 Training d_loss: 0.3943 Explore P: 0.7820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 123 Total reward: 10.0 Average reward fake: 0.08828625828027725 Average reward real: 0.6826747059822083 Training q_loss: 28.6278 Training g_loss: 9.9998 Training d_loss: 0.7404 Explore P: 0.7812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 124 Total reward: 9.0 Average reward fake: 0.04589241370558739 Average reward real: 0.6024951934814453 Training q_loss: 79.7032 Training g_loss: 10.1799 Training d_loss: 1.0579 Explore P: 0.7806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 125 Total reward: 17.0 Average reward fake: 0.1121533215045929 Average reward real: 0.9146655797958374 Training q_loss: 39.7496 Training g_loss: 9.6065 Training d_loss: 0.3658 Explore P: 0.7792\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 126 Total reward: 49.0 Average reward fake: 0.11830093711614609 Average reward real: 0.9166474938392639 Training q_loss: 39.3555 Training g_loss: 10.7646 Training d_loss: 0.2832 Explore P: 0.7755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 127 Total reward: 14.0 Average reward fake: 0.1563912332057953 Average reward real: 0.9092944264411926 Training q_loss: 36.0643 Training g_loss: 9.8275 Training d_loss: 0.2492 Explore P: 0.7744\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 128 Total reward: 16.0 Average reward fake: 0.07860206812620163 Average reward real: 0.7825166583061218 Training q_loss: 89.7712 Training g_loss: 10.3040 Training d_loss: 0.3664 Explore P: 0.7732\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 129 Total reward: 14.0 Average reward fake: 0.11813440173864365 Average reward real: 0.9006202816963196 Training q_loss: 26.7325 Training g_loss: 9.5702 Training d_loss: 0.2414 Explore P: 0.7721\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 130 Total reward: 21.0 Average reward fake: 0.08751961588859558 Average reward real: 0.8919045925140381 Training q_loss: 29.0771 Training g_loss: 9.7479 Training d_loss: 0.2304 Explore P: 0.7705\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 131 Total reward: 11.0 Average reward fake: 0.09665707498788834 Average reward real: 0.8838376402854919 Training q_loss: 43.2405 Training g_loss: 9.7139 Training d_loss: 0.2377 Explore P: 0.7697\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 132 Total reward: 15.0 Average reward fake: 0.09854014217853546 Average reward real: 0.9013170599937439 Training q_loss: 105.3168 Training g_loss: 9.7769 Training d_loss: 0.2430 Explore P: 0.7685\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 133 Total reward: 16.0 Average reward fake: 0.11521825939416885 Average reward real: 0.8787417411804199 Training q_loss: 100.8784 Training g_loss: 9.4428 Training d_loss: 0.2839 Explore P: 0.7673\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 134 Total reward: 24.0 Average reward fake: 0.12207307666540146 Average reward real: 0.929398775100708 Training q_loss: 20.5155 Training g_loss: 9.3495 Training d_loss: 0.2919 Explore P: 0.7655\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 135 Total reward: 23.0 Average reward fake: 0.13117899000644684 Average reward real: 0.9032854437828064 Training q_loss: 52.3525 Training g_loss: 9.9324 Training d_loss: 0.2317 Explore P: 0.7638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 136 Total reward: 13.0 Average reward fake: 0.11458151042461395 Average reward real: 0.8933995962142944 Training q_loss: 62.1501 Training g_loss: 9.7650 Training d_loss: 0.2526 Explore P: 0.7628\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 137 Total reward: 19.0 Average reward fake: 0.13026605546474457 Average reward real: 0.8957933783531189 Training q_loss: 22.6111 Training g_loss: 9.2837 Training d_loss: 0.2557 Explore P: 0.7614\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 138 Total reward: 23.0 Average reward fake: 0.08448352664709091 Average reward real: 0.9364770650863647 Training q_loss: 59.6148 Training g_loss: 11.1949 Training d_loss: 0.2058 Explore P: 0.7596\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 139 Total reward: 13.0 Average reward fake: 0.18528667092323303 Average reward real: 0.8642404079437256 Training q_loss: 120.5441 Training g_loss: 9.6702 Training d_loss: 0.2471 Explore P: 0.7587\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 140 Total reward: 16.0 Average reward fake: 0.05586705729365349 Average reward real: 0.5924490690231323 Training q_loss: 79.1525 Training g_loss: 10.1023 Training d_loss: 0.8594 Explore P: 0.7575\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 141 Total reward: 24.0 Average reward fake: 0.19637855887413025 Average reward real: 0.9146993160247803 Training q_loss: 22.2190 Training g_loss: 10.3326 Training d_loss: 0.6010 Explore P: 0.7557\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 142 Total reward: 23.0 Average reward fake: 0.05006236955523491 Average reward real: 0.5946301817893982 Training q_loss: 22.2539 Training g_loss: 10.7029 Training d_loss: 0.8612 Explore P: 0.7540\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 143 Total reward: 16.0 Average reward fake: 0.43801072239875793 Average reward real: 0.9908823370933533 Training q_loss: 136.2950 Training g_loss: 11.5191 Training d_loss: 0.6166 Explore P: 0.7528\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 144 Total reward: 24.0 Average reward fake: 0.08809669315814972 Average reward real: 0.8302780389785767 Training q_loss: 73.3558 Training g_loss: 10.1793 Training d_loss: 0.3400 Explore P: 0.7510\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 145 Total reward: 12.0 Average reward fake: 0.09879551082849503 Average reward real: 0.9114079475402832 Training q_loss: 24.0981 Training g_loss: 9.7872 Training d_loss: 0.2143 Explore P: 0.7501\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 146 Total reward: 15.0 Average reward fake: 0.10827552527189255 Average reward real: 0.8975491523742676 Training q_loss: 22.6506 Training g_loss: 9.8499 Training d_loss: 0.2380 Explore P: 0.7490\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 147 Total reward: 13.0 Average reward fake: 0.1025705337524414 Average reward real: 0.8749510049819946 Training q_loss: 43.4134 Training g_loss: 9.8500 Training d_loss: 0.2671 Explore P: 0.7480\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 148 Total reward: 9.0 Average reward fake: 0.10017780214548111 Average reward real: 0.847236156463623 Training q_loss: 31.9453 Training g_loss: 9.8149 Training d_loss: 0.3006 Explore P: 0.7474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 149 Total reward: 19.0 Average reward fake: 0.09713825583457947 Average reward real: 0.9004887342453003 Training q_loss: 38.2565 Training g_loss: 10.4504 Training d_loss: 0.2393 Explore P: 0.7460\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 150 Total reward: 13.0 Average reward fake: 0.09552034735679626 Average reward real: 0.8487520813941956 Training q_loss: 21.5360 Training g_loss: 9.3087 Training d_loss: 0.2790 Explore P: 0.7450\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 151 Total reward: 9.0 Average reward fake: 0.1491774618625641 Average reward real: 0.8954460024833679 Training q_loss: 50.1878 Training g_loss: 10.1919 Training d_loss: 0.2463 Explore P: 0.7444\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 152 Total reward: 12.0 Average reward fake: 0.12404008209705353 Average reward real: 0.9149096608161926 Training q_loss: 76.2386 Training g_loss: 10.6925 Training d_loss: 0.2198 Explore P: 0.7435\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 153 Total reward: 13.0 Average reward fake: 0.09597007930278778 Average reward real: 0.8100444078445435 Training q_loss: 24.9821 Training g_loss: 10.0774 Training d_loss: 0.3356 Explore P: 0.7425\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 154 Total reward: 15.0 Average reward fake: 0.0904349833726883 Average reward real: 0.8315179347991943 Training q_loss: 29.8276 Training g_loss: 10.3884 Training d_loss: 0.3197 Explore P: 0.7414\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 155 Total reward: 10.0 Average reward fake: 0.13218620419502258 Average reward real: 0.8631799817085266 Training q_loss: 44.2487 Training g_loss: 9.1157 Training d_loss: 0.2742 Explore P: 0.7407\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 156 Total reward: 13.0 Average reward fake: 0.08236942440271378 Average reward real: 0.9153956770896912 Training q_loss: 104.1575 Training g_loss: 11.1121 Training d_loss: 0.1863 Explore P: 0.7398\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 157 Total reward: 18.0 Average reward fake: 0.11315393447875977 Average reward real: 0.8878110647201538 Training q_loss: 29.1703 Training g_loss: 10.8969 Training d_loss: 0.2967 Explore P: 0.7384\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 158 Total reward: 15.0 Average reward fake: 0.10096403956413269 Average reward real: 0.8681895732879639 Training q_loss: 23.5964 Training g_loss: 10.1360 Training d_loss: 0.3078 Explore P: 0.7373\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 159 Total reward: 23.0 Average reward fake: 0.12509390711784363 Average reward real: 0.889240562915802 Training q_loss: 67.4410 Training g_loss: 9.0038 Training d_loss: 0.2982 Explore P: 0.7357\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 160 Total reward: 24.0 Average reward fake: 0.08865220844745636 Average reward real: 0.8720614910125732 Training q_loss: 47.7962 Training g_loss: 10.8843 Training d_loss: 0.2595 Explore P: 0.7339\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 161 Total reward: 13.0 Average reward fake: 0.07780192047357559 Average reward real: 0.9042888879776001 Training q_loss: 44.2951 Training g_loss: 10.2622 Training d_loss: 0.1986 Explore P: 0.7330\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 162 Total reward: 13.0 Average reward fake: 0.10209199041128159 Average reward real: 0.8408126831054688 Training q_loss: 37.4648 Training g_loss: 10.0036 Training d_loss: 0.2750 Explore P: 0.7321\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 163 Total reward: 22.0 Average reward fake: 0.12451833486557007 Average reward real: 0.9172399640083313 Training q_loss: 54.9497 Training g_loss: 12.3827 Training d_loss: 0.2743 Explore P: 0.7305\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 164 Total reward: 9.0 Average reward fake: 0.14638854563236237 Average reward real: 0.9217253923416138 Training q_loss: 23.2100 Training g_loss: 10.9503 Training d_loss: 0.3017 Explore P: 0.7298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 165 Total reward: 30.0 Average reward fake: 0.13664957880973816 Average reward real: 0.9213333129882812 Training q_loss: 76.6308 Training g_loss: 10.8369 Training d_loss: 0.2047 Explore P: 0.7277\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 166 Total reward: 24.0 Average reward fake: 0.10535808652639389 Average reward real: 0.8815639019012451 Training q_loss: 32.9563 Training g_loss: 10.0651 Training d_loss: 0.2600 Explore P: 0.7259\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 167 Total reward: 32.0 Average reward fake: 0.11517190933227539 Average reward real: 0.9080279469490051 Training q_loss: 34.1563 Training g_loss: 11.0038 Training d_loss: 0.2324 Explore P: 0.7237\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 168 Total reward: 15.0 Average reward fake: 0.11270365864038467 Average reward real: 0.8353644013404846 Training q_loss: 58.0643 Training g_loss: 10.4581 Training d_loss: 0.2773 Explore P: 0.7226\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 169 Total reward: 12.0 Average reward fake: 0.10191229730844498 Average reward real: 0.91770339012146 Training q_loss: 27.8363 Training g_loss: 10.5806 Training d_loss: 0.2312 Explore P: 0.7217\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 170 Total reward: 17.0 Average reward fake: 0.11683429777622223 Average reward real: 0.9292410016059875 Training q_loss: 69.6794 Training g_loss: 11.4819 Training d_loss: 0.2015 Explore P: 0.7205\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 171 Total reward: 12.0 Average reward fake: 0.11972971260547638 Average reward real: 0.9029088020324707 Training q_loss: 22.1118 Training g_loss: 10.0833 Training d_loss: 0.2727 Explore P: 0.7197\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 172 Total reward: 9.0 Average reward fake: 0.09930473566055298 Average reward real: 0.8882295489311218 Training q_loss: 40.6380 Training g_loss: 11.1313 Training d_loss: 0.2381 Explore P: 0.7190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 173 Total reward: 11.0 Average reward fake: 0.10072494298219681 Average reward real: 0.9056158661842346 Training q_loss: 41.0236 Training g_loss: 11.3765 Training d_loss: 0.2176 Explore P: 0.7183\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 174 Total reward: 24.0 Average reward fake: 0.10994066298007965 Average reward real: 0.8882355690002441 Training q_loss: 61.3061 Training g_loss: 13.2969 Training d_loss: 0.2647 Explore P: 0.7166\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 175 Total reward: 11.0 Average reward fake: 0.09460368752479553 Average reward real: 0.8667147755622864 Training q_loss: 120.3036 Training g_loss: 11.8362 Training d_loss: 0.2511 Explore P: 0.7158\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 176 Total reward: 14.0 Average reward fake: 0.08888397365808487 Average reward real: 0.9054102301597595 Training q_loss: 62.9190 Training g_loss: 11.6524 Training d_loss: 0.2113 Explore P: 0.7148\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 177 Total reward: 20.0 Average reward fake: 0.09703810513019562 Average reward real: 0.830386221408844 Training q_loss: 87.2176 Training g_loss: 14.2916 Training d_loss: 0.2890 Explore P: 0.7134\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 178 Total reward: 21.0 Average reward fake: 0.12232455611228943 Average reward real: 0.8969140648841858 Training q_loss: 33.2351 Training g_loss: 12.5206 Training d_loss: 0.2396 Explore P: 0.7119\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 179 Total reward: 9.0 Average reward fake: 0.10325107723474503 Average reward real: 0.9039000868797302 Training q_loss: 35.1653 Training g_loss: 11.9423 Training d_loss: 0.2174 Explore P: 0.7113\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 180 Total reward: 8.0 Average reward fake: 0.1029108315706253 Average reward real: 0.8880000114440918 Training q_loss: 32.0395 Training g_loss: 11.6935 Training d_loss: 0.2716 Explore P: 0.7107\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 181 Total reward: 11.0 Average reward fake: 0.09061425924301147 Average reward real: 0.896862268447876 Training q_loss: 34.2158 Training g_loss: 15.9619 Training d_loss: 0.2222 Explore P: 0.7099\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 182 Total reward: 29.0 Average reward fake: 0.12024945020675659 Average reward real: 0.8759784698486328 Training q_loss: 60.3963 Training g_loss: 11.2343 Training d_loss: 0.2782 Explore P: 0.7079\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 183 Total reward: 43.0 Average reward fake: 0.10474473983049393 Average reward real: 0.906989574432373 Training q_loss: 50.9974 Training g_loss: 11.9003 Training d_loss: 0.1999 Explore P: 0.7049\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 184 Total reward: 22.0 Average reward fake: 0.09327058494091034 Average reward real: 0.8954849243164062 Training q_loss: 34.4507 Training g_loss: 12.4889 Training d_loss: 0.2077 Explore P: 0.7034\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 185 Total reward: 11.0 Average reward fake: 0.12510348856449127 Average reward real: 0.9209123849868774 Training q_loss: 54.3235 Training g_loss: 11.3022 Training d_loss: 0.2148 Explore P: 0.7026\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 186 Total reward: 15.0 Average reward fake: 0.08291999995708466 Average reward real: 0.9022257924079895 Training q_loss: 53.5668 Training g_loss: 11.2378 Training d_loss: 0.1920 Explore P: 0.7016\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 187 Total reward: 12.0 Average reward fake: 0.08533678948879242 Average reward real: 0.8163465261459351 Training q_loss: 68.8646 Training g_loss: 12.0875 Training d_loss: 0.3283 Explore P: 0.7008\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 188 Total reward: 12.0 Average reward fake: 0.15331187844276428 Average reward real: 0.9456350803375244 Training q_loss: 88.3086 Training g_loss: 12.6649 Training d_loss: 0.2636 Explore P: 0.6999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 189 Total reward: 8.0 Average reward fake: 0.1530991643667221 Average reward real: 0.9200361371040344 Training q_loss: 29.9700 Training g_loss: 11.3745 Training d_loss: 0.2893 Explore P: 0.6994\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 190 Total reward: 15.0 Average reward fake: 0.11309418827295303 Average reward real: 0.9084209203720093 Training q_loss: 39.6193 Training g_loss: 14.0283 Training d_loss: 0.2315 Explore P: 0.6984\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 191 Total reward: 18.0 Average reward fake: 0.10026835650205612 Average reward real: 0.8936805129051208 Training q_loss: 65.3539 Training g_loss: 15.0655 Training d_loss: 0.2259 Explore P: 0.6971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 192 Total reward: 13.0 Average reward fake: 0.11524881422519684 Average reward real: 0.9225254654884338 Training q_loss: 111.5026 Training g_loss: 12.7376 Training d_loss: 0.2244 Explore P: 0.6962\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 193 Total reward: 22.0 Average reward fake: 0.13469792902469635 Average reward real: 0.9531612992286682 Training q_loss: 47.9123 Training g_loss: 14.0190 Training d_loss: 0.1984 Explore P: 0.6947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 194 Total reward: 31.0 Average reward fake: 0.12174289673566818 Average reward real: 0.9019436836242676 Training q_loss: 46.1774 Training g_loss: 11.7754 Training d_loss: 0.2416 Explore P: 0.6926\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 195 Total reward: 32.0 Average reward fake: 0.07899870723485947 Average reward real: 0.8462537527084351 Training q_loss: 58.9092 Training g_loss: 13.7703 Training d_loss: 0.2931 Explore P: 0.6904\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 196 Total reward: 17.0 Average reward fake: 0.11406344175338745 Average reward real: 0.8509865403175354 Training q_loss: 52.4583 Training g_loss: 12.2395 Training d_loss: 0.2813 Explore P: 0.6893\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 197 Total reward: 23.0 Average reward fake: 0.09769362211227417 Average reward real: 0.880788266658783 Training q_loss: 41.2714 Training g_loss: 13.8134 Training d_loss: 0.2464 Explore P: 0.6877\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 198 Total reward: 14.0 Average reward fake: 0.10321987420320511 Average reward real: 0.8652555346488953 Training q_loss: 28.8336 Training g_loss: 12.3251 Training d_loss: 0.2596 Explore P: 0.6868\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 199 Total reward: 11.0 Average reward fake: 0.13863790035247803 Average reward real: 0.9356616139411926 Training q_loss: 31.8282 Training g_loss: 11.8943 Training d_loss: 0.2525 Explore P: 0.6860\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 200 Total reward: 19.0 Average reward fake: 0.10315683484077454 Average reward real: 0.843385636806488 Training q_loss: 41.9718 Training g_loss: 14.0822 Training d_loss: 0.2683 Explore P: 0.6847\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 201 Total reward: 9.0 Average reward fake: 0.11646594852209091 Average reward real: 0.8805280923843384 Training q_loss: 37.6338 Training g_loss: 13.6550 Training d_loss: 0.2016 Explore P: 0.6841\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 202 Total reward: 20.0 Average reward fake: 0.052336886525154114 Average reward real: 0.7437971234321594 Training q_loss: 49.4408 Training g_loss: 11.2652 Training d_loss: 0.4406 Explore P: 0.6828\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 203 Total reward: 22.0 Average reward fake: 0.09935509413480759 Average reward real: 0.9416330456733704 Training q_loss: 46.7917 Training g_loss: 13.7334 Training d_loss: 0.3124 Explore P: 0.6813\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 204 Total reward: 9.0 Average reward fake: 0.058403391391038895 Average reward real: 0.8512449860572815 Training q_loss: 35.6745 Training g_loss: 12.6085 Training d_loss: 0.3699 Explore P: 0.6807\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 205 Total reward: 12.0 Average reward fake: 0.10162143409252167 Average reward real: 0.7987797260284424 Training q_loss: 33.2942 Training g_loss: 15.7416 Training d_loss: 0.4052 Explore P: 0.6799\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 206 Total reward: 16.0 Average reward fake: 0.03369474038481712 Average reward real: 0.8163208961486816 Training q_loss: 61.4290 Training g_loss: 27.9392 Training d_loss: 0.2762 Explore P: 0.6788\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 207 Total reward: 19.0 Average reward fake: 0.11830787360668182 Average reward real: 0.9042677879333496 Training q_loss: 80.3967 Training g_loss: 22.0622 Training d_loss: 0.3746 Explore P: 0.6775\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 208 Total reward: 34.0 Average reward fake: 0.10454370826482773 Average reward real: 0.9102213978767395 Training q_loss: 49.8894 Training g_loss: 14.7975 Training d_loss: 0.2571 Explore P: 0.6753\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 209 Total reward: 10.0 Average reward fake: 0.12024883925914764 Average reward real: 0.8683951497077942 Training q_loss: 52.3890 Training g_loss: 11.9054 Training d_loss: 0.2687 Explore P: 0.6746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 210 Total reward: 13.0 Average reward fake: 0.10003624856472015 Average reward real: 0.9273934960365295 Training q_loss: 64.6848 Training g_loss: 14.0885 Training d_loss: 0.2313 Explore P: 0.6737\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 211 Total reward: 11.0 Average reward fake: 0.060833439230918884 Average reward real: 0.8704214692115784 Training q_loss: 127.0234 Training g_loss: 12.8686 Training d_loss: 0.2201 Explore P: 0.6730\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 212 Total reward: 14.0 Average reward fake: 0.09450838714838028 Average reward real: 0.8787292242050171 Training q_loss: 60.1297 Training g_loss: 13.7335 Training d_loss: 0.2827 Explore P: 0.6721\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 213 Total reward: 10.0 Average reward fake: 0.08552806824445724 Average reward real: 0.8612523674964905 Training q_loss: 86.6686 Training g_loss: 13.3021 Training d_loss: 0.2686 Explore P: 0.6714\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 214 Total reward: 13.0 Average reward fake: 0.09109508246183395 Average reward real: 0.9396561980247498 Training q_loss: 37.8289 Training g_loss: 18.7258 Training d_loss: 0.1824 Explore P: 0.6706\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 215 Total reward: 22.0 Average reward fake: 0.08371308445930481 Average reward real: 0.9138474464416504 Training q_loss: 68.3461 Training g_loss: 14.9186 Training d_loss: 0.1659 Explore P: 0.6691\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 216 Total reward: 23.0 Average reward fake: 0.16536180675029755 Average reward real: 0.9426888823509216 Training q_loss: 45.4688 Training g_loss: 21.3382 Training d_loss: 0.2029 Explore P: 0.6676\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 217 Total reward: 12.0 Average reward fake: 0.07924091070890427 Average reward real: 0.8164443969726562 Training q_loss: 41.1096 Training g_loss: 18.2595 Training d_loss: 0.3662 Explore P: 0.6668\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 218 Total reward: 10.0 Average reward fake: 0.18373052775859833 Average reward real: 0.8445079326629639 Training q_loss: 88.8814 Training g_loss: 17.5956 Training d_loss: 0.3839 Explore P: 0.6662\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 219 Total reward: 13.0 Average reward fake: 0.0499514676630497 Average reward real: 0.8748605251312256 Training q_loss: 113.8837 Training g_loss: 15.4527 Training d_loss: 0.3249 Explore P: 0.6653\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 220 Total reward: 23.0 Average reward fake: 0.11010622978210449 Average reward real: 0.9442455768585205 Training q_loss: 28.7564 Training g_loss: 13.5595 Training d_loss: 0.1258 Explore P: 0.6638\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 221 Total reward: 10.0 Average reward fake: 0.2139207273721695 Average reward real: 0.9382180571556091 Training q_loss: 86.6976 Training g_loss: 15.5056 Training d_loss: 0.4383 Explore P: 0.6631\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 222 Total reward: 17.0 Average reward fake: 0.06889425218105316 Average reward real: 0.9164113402366638 Training q_loss: 60.4444 Training g_loss: 13.1921 Training d_loss: 0.1696 Explore P: 0.6620\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 223 Total reward: 33.0 Average reward fake: 0.10508573800325394 Average reward real: 0.9330699443817139 Training q_loss: 32.3869 Training g_loss: 13.7988 Training d_loss: 0.1749 Explore P: 0.6599\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 224 Total reward: 14.0 Average reward fake: 0.11060977727174759 Average reward real: 0.8984990119934082 Training q_loss: 30.9622 Training g_loss: 13.9602 Training d_loss: 0.2062 Explore P: 0.6590\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 225 Total reward: 9.0 Average reward fake: 0.07697837799787521 Average reward real: 0.8795608282089233 Training q_loss: 66.6525 Training g_loss: 13.5037 Training d_loss: 0.2031 Explore P: 0.6584\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 226 Total reward: 20.0 Average reward fake: 0.09744863212108612 Average reward real: 0.8469871282577515 Training q_loss: 65.9853 Training g_loss: 13.9456 Training d_loss: 0.2838 Explore P: 0.6571\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 227 Total reward: 12.0 Average reward fake: 0.1184515580534935 Average reward real: 0.901289701461792 Training q_loss: 85.5397 Training g_loss: 15.0785 Training d_loss: 0.2667 Explore P: 0.6563\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 228 Total reward: 24.0 Average reward fake: 0.07634391635656357 Average reward real: 0.9035532474517822 Training q_loss: 30.1364 Training g_loss: 12.1179 Training d_loss: 0.2044 Explore P: 0.6548\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 229 Total reward: 14.0 Average reward fake: 0.12913647294044495 Average reward real: 0.9375951886177063 Training q_loss: 47.0201 Training g_loss: 12.8684 Training d_loss: 0.2070 Explore P: 0.6539\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 230 Total reward: 11.0 Average reward fake: 0.08188457787036896 Average reward real: 0.891049861907959 Training q_loss: 103.7363 Training g_loss: 13.1930 Training d_loss: 0.2568 Explore P: 0.6532\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 231 Total reward: 20.0 Average reward fake: 0.1034911721944809 Average reward real: 0.9215692281723022 Training q_loss: 97.7086 Training g_loss: 21.5357 Training d_loss: 0.1774 Explore P: 0.6519\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 232 Total reward: 12.0 Average reward fake: 0.08310386538505554 Average reward real: 0.9013051390647888 Training q_loss: 38.6748 Training g_loss: 17.8262 Training d_loss: 0.2095 Explore P: 0.6511\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 233 Total reward: 13.0 Average reward fake: 0.09760651737451553 Average reward real: 0.929413914680481 Training q_loss: 71.0693 Training g_loss: 13.7603 Training d_loss: 0.2040 Explore P: 0.6503\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 234 Total reward: 15.0 Average reward fake: 0.10509075224399567 Average reward real: 0.9274046421051025 Training q_loss: 91.7726 Training g_loss: 21.1688 Training d_loss: 0.1523 Explore P: 0.6493\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 235 Total reward: 15.0 Average reward fake: 0.08018169552087784 Average reward real: 0.8912695050239563 Training q_loss: 36.8789 Training g_loss: 22.0257 Training d_loss: 0.1739 Explore P: 0.6484\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 236 Total reward: 15.0 Average reward fake: 0.04014277830719948 Average reward real: 0.8222885131835938 Training q_loss: 55.1822 Training g_loss: 15.2365 Training d_loss: 0.2426 Explore P: 0.6474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 237 Total reward: 16.0 Average reward fake: 0.09751340746879578 Average reward real: 0.927217423915863 Training q_loss: 116.0536 Training g_loss: 13.1462 Training d_loss: 0.2322 Explore P: 0.6464\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 238 Total reward: 14.0 Average reward fake: 0.06950533390045166 Average reward real: 0.917330801486969 Training q_loss: 36.5708 Training g_loss: 15.5243 Training d_loss: 0.2269 Explore P: 0.6455\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 239 Total reward: 9.0 Average reward fake: 0.05844160169363022 Average reward real: 0.8538068532943726 Training q_loss: 30.7585 Training g_loss: 13.3821 Training d_loss: 0.2310 Explore P: 0.6449\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 240 Total reward: 8.0 Average reward fake: 0.08514738082885742 Average reward real: 0.8919265270233154 Training q_loss: 31.9872 Training g_loss: 13.8541 Training d_loss: 0.2149 Explore P: 0.6444\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 241 Total reward: 9.0 Average reward fake: 0.08655082434415817 Average reward real: 0.895646870136261 Training q_loss: 40.6119 Training g_loss: 14.6237 Training d_loss: 0.1960 Explore P: 0.6438\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 242 Total reward: 13.0 Average reward fake: 0.06569582968950272 Average reward real: 0.8467305898666382 Training q_loss: 40.6151 Training g_loss: 15.0257 Training d_loss: 0.2876 Explore P: 0.6430\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 243 Total reward: 49.0 Average reward fake: 0.07116895914077759 Average reward real: 0.8514308333396912 Training q_loss: 34.5378 Training g_loss: 14.4778 Training d_loss: 0.2787 Explore P: 0.6399\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 244 Total reward: 21.0 Average reward fake: 0.07987222820520401 Average reward real: 0.8559069633483887 Training q_loss: 38.4260 Training g_loss: 12.1676 Training d_loss: 0.2881 Explore P: 0.6386\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 245 Total reward: 20.0 Average reward fake: 0.0774652287364006 Average reward real: 0.9235333204269409 Training q_loss: 77.2200 Training g_loss: 19.3973 Training d_loss: 0.1510 Explore P: 0.6374\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 246 Total reward: 21.0 Average reward fake: 0.14560705423355103 Average reward real: 0.9150527715682983 Training q_loss: 32.1640 Training g_loss: 14.0459 Training d_loss: 0.2608 Explore P: 0.6360\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 247 Total reward: 22.0 Average reward fake: 0.05227638781070709 Average reward real: 0.8855823278427124 Training q_loss: 59.0050 Training g_loss: 15.5251 Training d_loss: 0.2141 Explore P: 0.6347\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 248 Total reward: 17.0 Average reward fake: 0.08535592257976532 Average reward real: 0.8804057240486145 Training q_loss: 86.5371 Training g_loss: 17.9152 Training d_loss: 0.2297 Explore P: 0.6336\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 249 Total reward: 62.0 Average reward fake: 0.0845952257514 Average reward real: 0.9109350442886353 Training q_loss: 72.8180 Training g_loss: 32.4202 Training d_loss: 0.1889 Explore P: 0.6297\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 250 Total reward: 22.0 Average reward fake: 0.13916592299938202 Average reward real: 0.9527223110198975 Training q_loss: 52.8744 Training g_loss: 26.3924 Training d_loss: 0.2500 Explore P: 0.6284\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 251 Total reward: 13.0 Average reward fake: 0.0769074410200119 Average reward real: 0.9110636711120605 Training q_loss: 34.3424 Training g_loss: 16.7876 Training d_loss: 0.1730 Explore P: 0.6276\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 252 Total reward: 17.0 Average reward fake: 0.07787290215492249 Average reward real: 0.9214512705802917 Training q_loss: 77.8412 Training g_loss: 25.6815 Training d_loss: 0.2499 Explore P: 0.6265\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 253 Total reward: 12.0 Average reward fake: 0.1016903892159462 Average reward real: 0.9690136909484863 Training q_loss: 87.4731 Training g_loss: 34.8100 Training d_loss: 0.0874 Explore P: 0.6258\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 254 Total reward: 14.0 Average reward fake: 0.030375191941857338 Average reward real: 0.7756427526473999 Training q_loss: 52.2043 Training g_loss: 32.6791 Training d_loss: 0.5180 Explore P: 0.6249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 255 Total reward: 10.0 Average reward fake: 0.09760329872369766 Average reward real: 0.9794689416885376 Training q_loss: 86.9761 Training g_loss: 28.4390 Training d_loss: 0.1184 Explore P: 0.6243\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 256 Total reward: 12.0 Average reward fake: 0.052359722554683685 Average reward real: 0.9143710136413574 Training q_loss: 77.6039 Training g_loss: 24.7491 Training d_loss: 0.2053 Explore P: 0.6236\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 257 Total reward: 11.0 Average reward fake: 0.10505472123622894 Average reward real: 0.8985592722892761 Training q_loss: 47.0274 Training g_loss: 18.4296 Training d_loss: 0.2414 Explore P: 0.6229\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 258 Total reward: 12.0 Average reward fake: 0.08293668925762177 Average reward real: 0.886080801486969 Training q_loss: 32.7731 Training g_loss: 15.7609 Training d_loss: 0.2762 Explore P: 0.6222\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 259 Total reward: 13.0 Average reward fake: 0.07297416776418686 Average reward real: 0.9183880686759949 Training q_loss: 95.1824 Training g_loss: 18.2180 Training d_loss: 0.1651 Explore P: 0.6214\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 260 Total reward: 11.0 Average reward fake: 0.03846139833331108 Average reward real: 0.9062010049819946 Training q_loss: 43.6255 Training g_loss: 21.8268 Training d_loss: 0.2326 Explore P: 0.6207\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 261 Total reward: 22.0 Average reward fake: 0.08413200080394745 Average reward real: 0.9544435143470764 Training q_loss: 32.4524 Training g_loss: 16.7885 Training d_loss: 0.1608 Explore P: 0.6194\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 262 Total reward: 11.0 Average reward fake: 0.14198124408721924 Average reward real: 0.9331097602844238 Training q_loss: 54.3772 Training g_loss: 15.4578 Training d_loss: 0.1899 Explore P: 0.6187\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 263 Total reward: 11.0 Average reward fake: 0.09576450288295746 Average reward real: 0.9513214230537415 Training q_loss: 38.2991 Training g_loss: 20.7689 Training d_loss: 0.1842 Explore P: 0.6180\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 264 Total reward: 10.0 Average reward fake: 0.08666139841079712 Average reward real: 0.9203750491142273 Training q_loss: 50.3146 Training g_loss: 25.5921 Training d_loss: 0.1629 Explore P: 0.6174\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 265 Total reward: 12.0 Average reward fake: 0.06138564273715019 Average reward real: 0.9072755575180054 Training q_loss: 58.1483 Training g_loss: 20.8006 Training d_loss: 0.1873 Explore P: 0.6167\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 266 Total reward: 11.0 Average reward fake: 0.08494103699922562 Average reward real: 0.9019688367843628 Training q_loss: 33.9877 Training g_loss: 17.2336 Training d_loss: 0.2062 Explore P: 0.6160\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 267 Total reward: 13.0 Average reward fake: 0.1309046596288681 Average reward real: 0.9186238050460815 Training q_loss: 122.7248 Training g_loss: 17.0972 Training d_loss: 0.2253 Explore P: 0.6152\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 268 Total reward: 16.0 Average reward fake: 0.06978165358304977 Average reward real: 0.8595029711723328 Training q_loss: 130.2639 Training g_loss: 17.0877 Training d_loss: 0.2422 Explore P: 0.6143\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 269 Total reward: 17.0 Average reward fake: 0.12154623866081238 Average reward real: 0.904585599899292 Training q_loss: 32.9760 Training g_loss: 15.2470 Training d_loss: 0.2166 Explore P: 0.6132\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 270 Total reward: 9.0 Average reward fake: 0.21005718410015106 Average reward real: 0.9611474871635437 Training q_loss: 47.8502 Training g_loss: 24.0524 Training d_loss: 0.2064 Explore P: 0.6127\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 271 Total reward: 11.0 Average reward fake: 0.04365089535713196 Average reward real: 0.9014898538589478 Training q_loss: 96.7000 Training g_loss: 46.1579 Training d_loss: 0.1736 Explore P: 0.6120\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 272 Total reward: 9.0 Average reward fake: 0.043160125613212585 Average reward real: 0.8601444959640503 Training q_loss: 66.7995 Training g_loss: 45.3600 Training d_loss: 0.2397 Explore P: 0.6115\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 273 Total reward: 10.0 Average reward fake: 0.10429161041975021 Average reward real: 0.890644371509552 Training q_loss: 148.7874 Training g_loss: 48.0870 Training d_loss: 0.2175 Explore P: 0.6109\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 274 Total reward: 14.0 Average reward fake: 0.06231009587645531 Average reward real: 0.8803754448890686 Training q_loss: 57.0185 Training g_loss: 36.2809 Training d_loss: 0.2225 Explore P: 0.6100\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 275 Total reward: 19.0 Average reward fake: 0.07983415573835373 Average reward real: 0.8955407738685608 Training q_loss: 46.0596 Training g_loss: 30.0235 Training d_loss: 0.1843 Explore P: 0.6089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 276 Total reward: 9.0 Average reward fake: 0.05728987231850624 Average reward real: 0.9155730605125427 Training q_loss: 51.5493 Training g_loss: 29.0816 Training d_loss: 0.1832 Explore P: 0.6084\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 277 Total reward: 16.0 Average reward fake: 0.05949859693646431 Average reward real: 0.8714891076087952 Training q_loss: 57.5561 Training g_loss: 20.0658 Training d_loss: 0.2223 Explore P: 0.6074\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 278 Total reward: 15.0 Average reward fake: 0.14490383863449097 Average reward real: 0.9556508660316467 Training q_loss: 338.6979 Training g_loss: 44.4638 Training d_loss: 0.2019 Explore P: 0.6065\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 279 Total reward: 19.0 Average reward fake: 0.048807233572006226 Average reward real: 0.9068893194198608 Training q_loss: 82.7221 Training g_loss: 59.0179 Training d_loss: 0.1928 Explore P: 0.6054\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 280 Total reward: 20.0 Average reward fake: 0.08305691927671432 Average reward real: 0.8725727796554565 Training q_loss: 75.0045 Training g_loss: 55.7270 Training d_loss: 0.1995 Explore P: 0.6042\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 281 Total reward: 13.0 Average reward fake: 0.05506887286901474 Average reward real: 0.8516309857368469 Training q_loss: 71.3866 Training g_loss: 45.8513 Training d_loss: 0.3434 Explore P: 0.6034\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 282 Total reward: 14.0 Average reward fake: 0.18717093765735626 Average reward real: 0.9190804958343506 Training q_loss: 85.4754 Training g_loss: 46.0375 Training d_loss: 0.4041 Explore P: 0.6026\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 283 Total reward: 14.0 Average reward fake: 0.006773348897695541 Average reward real: 0.6875698566436768 Training q_loss: 69.3697 Training g_loss: 38.9974 Training d_loss: 0.5143 Explore P: 0.6018\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 284 Total reward: 14.0 Average reward fake: 0.14541202783584595 Average reward real: 0.9542004466056824 Training q_loss: 75.1567 Training g_loss: 39.6637 Training d_loss: 0.4844 Explore P: 0.6009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 285 Total reward: 19.0 Average reward fake: 0.0967056155204773 Average reward real: 0.9445556402206421 Training q_loss: 52.1275 Training g_loss: 32.4207 Training d_loss: 0.2721 Explore P: 0.5998\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 286 Total reward: 21.0 Average reward fake: 0.17366811633110046 Average reward real: 0.9574009776115417 Training q_loss: 69.6055 Training g_loss: 24.6430 Training d_loss: 0.1944 Explore P: 0.5986\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 287 Total reward: 14.0 Average reward fake: 0.044627733528614044 Average reward real: 0.8880754113197327 Training q_loss: 38.2933 Training g_loss: 19.9377 Training d_loss: 0.1823 Explore P: 0.5978\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 288 Total reward: 11.0 Average reward fake: 0.07219640910625458 Average reward real: 0.8683916330337524 Training q_loss: 72.4065 Training g_loss: 15.9530 Training d_loss: 0.2212 Explore P: 0.5971\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 289 Total reward: 11.0 Average reward fake: 0.07539359480142593 Average reward real: 0.9106409549713135 Training q_loss: 54.0228 Training g_loss: 17.2601 Training d_loss: 0.2248 Explore P: 0.5965\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 290 Total reward: 22.0 Average reward fake: 0.028924738988280296 Average reward real: 0.8862217664718628 Training q_loss: 62.2298 Training g_loss: 16.7747 Training d_loss: 0.1776 Explore P: 0.5952\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 291 Total reward: 8.0 Average reward fake: 0.07282794266939163 Average reward real: 0.8732561469078064 Training q_loss: 87.7716 Training g_loss: 32.3973 Training d_loss: 0.2311 Explore P: 0.5947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 292 Total reward: 22.0 Average reward fake: 0.05699094757437706 Average reward real: 0.9000859260559082 Training q_loss: 66.8398 Training g_loss: 34.4382 Training d_loss: 0.1609 Explore P: 0.5934\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 293 Total reward: 10.0 Average reward fake: 0.047298818826675415 Average reward real: 0.9356958866119385 Training q_loss: 86.3591 Training g_loss: 33.6336 Training d_loss: 0.1403 Explore P: 0.5928\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 294 Total reward: 19.0 Average reward fake: 0.06925300508737564 Average reward real: 0.8490270972251892 Training q_loss: 72.3164 Training g_loss: 26.7370 Training d_loss: 0.2381 Explore P: 0.5917\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 295 Total reward: 12.0 Average reward fake: 0.10626086592674255 Average reward real: 0.9500219225883484 Training q_loss: 41.8598 Training g_loss: 22.6847 Training d_loss: 0.1642 Explore P: 0.5910\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 296 Total reward: 25.0 Average reward fake: 0.30082911252975464 Average reward real: 0.978823721408844 Training q_loss: 49.4258 Training g_loss: 34.3352 Training d_loss: 0.2095 Explore P: 0.5896\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 297 Total reward: 21.0 Average reward fake: 0.09536565095186234 Average reward real: 0.9151052832603455 Training q_loss: 193.5559 Training g_loss: 28.0022 Training d_loss: 0.2058 Explore P: 0.5884\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 298 Total reward: 22.0 Average reward fake: 0.05054603889584541 Average reward real: 0.9007939100265503 Training q_loss: 44.1240 Training g_loss: 22.9155 Training d_loss: 0.1524 Explore P: 0.5871\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 299 Total reward: 12.0 Average reward fake: 0.20824310183525085 Average reward real: 0.9579687714576721 Training q_loss: 40.7732 Training g_loss: 18.6981 Training d_loss: 0.2997 Explore P: 0.5864\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 300 Total reward: 12.0 Average reward fake: 0.039106953889131546 Average reward real: 0.931006908416748 Training q_loss: 87.5036 Training g_loss: 18.1184 Training d_loss: 0.1063 Explore P: 0.5857\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 301 Total reward: 8.0 Average reward fake: 0.0754343569278717 Average reward real: 0.9127566814422607 Training q_loss: 45.2610 Training g_loss: 19.4270 Training d_loss: 0.1597 Explore P: 0.5853\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 302 Total reward: 19.0 Average reward fake: 0.08245790749788284 Average reward real: 0.9571886658668518 Training q_loss: 47.9344 Training g_loss: 24.2777 Training d_loss: 0.1498 Explore P: 0.5842\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 303 Total reward: 13.0 Average reward fake: 0.09797743707895279 Average reward real: 0.8694207072257996 Training q_loss: 69.5624 Training g_loss: 24.2974 Training d_loss: 0.2583 Explore P: 0.5834\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 304 Total reward: 15.0 Average reward fake: 0.0401594340801239 Average reward real: 0.8839619159698486 Training q_loss: 48.6037 Training g_loss: 23.1353 Training d_loss: 0.1895 Explore P: 0.5826\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 305 Total reward: 9.0 Average reward fake: 0.12877918779850006 Average reward real: 0.9235273599624634 Training q_loss: 85.3097 Training g_loss: 16.5964 Training d_loss: 0.2678 Explore P: 0.5820\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 306 Total reward: 17.0 Average reward fake: 0.058092013001441956 Average reward real: 0.8512082099914551 Training q_loss: 62.8661 Training g_loss: 20.5144 Training d_loss: 0.2671 Explore P: 0.5811\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 307 Total reward: 14.0 Average reward fake: 0.09155508130788803 Average reward real: 0.9504148364067078 Training q_loss: 50.7939 Training g_loss: 31.0500 Training d_loss: 0.1200 Explore P: 0.5803\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 308 Total reward: 9.0 Average reward fake: 0.0862530916929245 Average reward real: 0.9500108361244202 Training q_loss: 61.1581 Training g_loss: 33.6514 Training d_loss: 0.1437 Explore P: 0.5798\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 309 Total reward: 17.0 Average reward fake: 0.09337850660085678 Average reward real: 0.9624812602996826 Training q_loss: 48.9084 Training g_loss: 28.3988 Training d_loss: 0.1957 Explore P: 0.5788\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 310 Total reward: 12.0 Average reward fake: 0.07064594328403473 Average reward real: 0.900767982006073 Training q_loss: 78.8597 Training g_loss: 20.8697 Training d_loss: 0.1783 Explore P: 0.5781\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 311 Total reward: 12.0 Average reward fake: 0.058311957865953445 Average reward real: 0.9665595293045044 Training q_loss: 47.7473 Training g_loss: 25.3456 Training d_loss: 0.1329 Explore P: 0.5774\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 312 Total reward: 14.0 Average reward fake: 0.07599902898073196 Average reward real: 0.8980072140693665 Training q_loss: 42.9284 Training g_loss: 26.9382 Training d_loss: 0.1824 Explore P: 0.5766\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 313 Total reward: 10.0 Average reward fake: 0.12575174868106842 Average reward real: 0.9609116911888123 Training q_loss: 76.4928 Training g_loss: 26.4044 Training d_loss: 0.1402 Explore P: 0.5761\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 314 Total reward: 10.0 Average reward fake: 0.08523575961589813 Average reward real: 0.9269466996192932 Training q_loss: 39.0920 Training g_loss: 23.7351 Training d_loss: 0.2703 Explore P: 0.5755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 315 Total reward: 38.0 Average reward fake: 0.04096032306551933 Average reward real: 0.866018533706665 Training q_loss: 104.6439 Training g_loss: 58.3313 Training d_loss: 0.2639 Explore P: 0.5734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 316 Total reward: 12.0 Average reward fake: 0.07692468911409378 Average reward real: 0.8997437953948975 Training q_loss: 86.6564 Training g_loss: 61.8456 Training d_loss: 0.2751 Explore P: 0.5727\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 317 Total reward: 30.0 Average reward fake: 0.05827796086668968 Average reward real: 0.8844255208969116 Training q_loss: 66.9898 Training g_loss: 51.5093 Training d_loss: 0.2399 Explore P: 0.5710\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 318 Total reward: 12.0 Average reward fake: 0.07078786939382553 Average reward real: 0.9086447358131409 Training q_loss: 69.4755 Training g_loss: 51.6123 Training d_loss: 0.1829 Explore P: 0.5703\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 319 Total reward: 16.0 Average reward fake: 0.06710568070411682 Average reward real: 0.9400588274002075 Training q_loss: 120.7795 Training g_loss: 53.1579 Training d_loss: 0.1298 Explore P: 0.5694\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 320 Total reward: 11.0 Average reward fake: 0.0520954430103302 Average reward real: 0.91727614402771 Training q_loss: 66.0342 Training g_loss: 45.4922 Training d_loss: 0.1663 Explore P: 0.5688\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 321 Total reward: 7.0 Average reward fake: 0.1296105831861496 Average reward real: 0.8889977931976318 Training q_loss: 58.1993 Training g_loss: 38.2869 Training d_loss: 0.2084 Explore P: 0.5684\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 322 Total reward: 12.0 Average reward fake: 0.08205859363079071 Average reward real: 0.9767904877662659 Training q_loss: 57.8695 Training g_loss: 39.8099 Training d_loss: 0.1424 Explore P: 0.5677\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 323 Total reward: 31.0 Average reward fake: 0.03882545605301857 Average reward real: 0.887051522731781 Training q_loss: 52.3983 Training g_loss: 29.7934 Training d_loss: 0.1740 Explore P: 0.5660\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 324 Total reward: 16.0 Average reward fake: 0.06428186595439911 Average reward real: 0.9518378973007202 Training q_loss: 72.4566 Training g_loss: 26.3127 Training d_loss: 0.1706 Explore P: 0.5651\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 325 Total reward: 9.0 Average reward fake: 0.06288467347621918 Average reward real: 0.9098309874534607 Training q_loss: 47.4164 Training g_loss: 24.1021 Training d_loss: 0.1713 Explore P: 0.5646\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 326 Total reward: 14.0 Average reward fake: 0.08150113373994827 Average reward real: 0.9106082320213318 Training q_loss: 41.7390 Training g_loss: 21.7056 Training d_loss: 0.1710 Explore P: 0.5639\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 327 Total reward: 11.0 Average reward fake: 0.08574748784303665 Average reward real: 0.9212548732757568 Training q_loss: 83.0543 Training g_loss: 23.6597 Training d_loss: 0.1963 Explore P: 0.5632\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 328 Total reward: 11.0 Average reward fake: 0.035129331052303314 Average reward real: 0.8993609547615051 Training q_loss: 48.1587 Training g_loss: 24.6871 Training d_loss: 0.1892 Explore P: 0.5626\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 329 Total reward: 13.0 Average reward fake: 0.13492201268672943 Average reward real: 0.9553738236427307 Training q_loss: 95.6969 Training g_loss: 21.9432 Training d_loss: 0.1651 Explore P: 0.5619\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 330 Total reward: 13.0 Average reward fake: 0.056469131261110306 Average reward real: 0.9365693926811218 Training q_loss: 49.8343 Training g_loss: 21.7020 Training d_loss: 0.1851 Explore P: 0.5612\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 331 Total reward: 14.0 Average reward fake: 0.09034087508916855 Average reward real: 0.9560096859931946 Training q_loss: 54.3754 Training g_loss: 25.0111 Training d_loss: 0.1947 Explore P: 0.5604\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 332 Total reward: 19.0 Average reward fake: 0.057358480989933014 Average reward real: 0.85909104347229 Training q_loss: 38.1782 Training g_loss: 16.1543 Training d_loss: 0.2607 Explore P: 0.5594\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 333 Total reward: 10.0 Average reward fake: 0.044597968459129333 Average reward real: 0.8658179640769958 Training q_loss: 130.3597 Training g_loss: 16.9824 Training d_loss: 0.2340 Explore P: 0.5588\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 334 Total reward: 11.0 Average reward fake: 0.08811686187982559 Average reward real: 0.9143292307853699 Training q_loss: 84.2548 Training g_loss: 47.7594 Training d_loss: 0.1973 Explore P: 0.5582\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 335 Total reward: 20.0 Average reward fake: 0.19100604951381683 Average reward real: 0.9653576612472534 Training q_loss: 62.0841 Training g_loss: 40.2518 Training d_loss: 0.1824 Explore P: 0.5571\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 336 Total reward: 12.0 Average reward fake: 0.05067811161279678 Average reward real: 0.886720597743988 Training q_loss: 76.8820 Training g_loss: 40.0678 Training d_loss: 0.2053 Explore P: 0.5565\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 337 Total reward: 10.0 Average reward fake: 0.03439253568649292 Average reward real: 0.9376599192619324 Training q_loss: 50.4491 Training g_loss: 31.3466 Training d_loss: 0.1447 Explore P: 0.5559\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 338 Total reward: 15.0 Average reward fake: 0.07791373133659363 Average reward real: 0.954414963722229 Training q_loss: 52.0519 Training g_loss: 30.0473 Training d_loss: 0.1181 Explore P: 0.5551\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 339 Total reward: 11.0 Average reward fake: 0.08414573967456818 Average reward real: 0.9247959852218628 Training q_loss: 74.3894 Training g_loss: 28.5748 Training d_loss: 0.1670 Explore P: 0.5545\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 340 Total reward: 13.0 Average reward fake: 0.05647621303796768 Average reward real: 0.9155819416046143 Training q_loss: 37.4131 Training g_loss: 18.6946 Training d_loss: 0.1877 Explore P: 0.5538\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 341 Total reward: 13.0 Average reward fake: 0.04945546016097069 Average reward real: 0.899802565574646 Training q_loss: 42.7923 Training g_loss: 17.9574 Training d_loss: 0.1970 Explore P: 0.5531\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 342 Total reward: 14.0 Average reward fake: 0.048364412039518356 Average reward real: 0.9361469745635986 Training q_loss: 151.7960 Training g_loss: 99.7776 Training d_loss: 0.1142 Explore P: 0.5523\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 343 Total reward: 18.0 Average reward fake: 0.10999937355518341 Average reward real: 0.9547123908996582 Training q_loss: 148.1622 Training g_loss: 115.7106 Training d_loss: 0.1697 Explore P: 0.5514\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 344 Total reward: 10.0 Average reward fake: 0.05789606645703316 Average reward real: 0.926501989364624 Training q_loss: 116.0396 Training g_loss: 92.1105 Training d_loss: 0.1538 Explore P: 0.5508\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 345 Total reward: 35.0 Average reward fake: 0.022505076602101326 Average reward real: 0.9153407216072083 Training q_loss: 125.3726 Training g_loss: 95.1936 Training d_loss: 0.1166 Explore P: 0.5489\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 346 Total reward: 13.0 Average reward fake: 0.04268277809023857 Average reward real: 0.8383259773254395 Training q_loss: 127.1045 Training g_loss: 100.8390 Training d_loss: 0.4957 Explore P: 0.5482\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 347 Total reward: 10.0 Average reward fake: 0.016872771084308624 Average reward real: 0.8758450150489807 Training q_loss: 154.2970 Training g_loss: 113.0521 Training d_loss: 0.2114 Explore P: 0.5477\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 348 Total reward: 16.0 Average reward fake: 0.03325643017888069 Average reward real: 0.8462120294570923 Training q_loss: 119.8175 Training g_loss: 100.5207 Training d_loss: 0.3918 Explore P: 0.5468\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 349 Total reward: 14.0 Average reward fake: 0.03206043317914009 Average reward real: 0.9334547519683838 Training q_loss: 216.2125 Training g_loss: 105.7937 Training d_loss: 0.1164 Explore P: 0.5461\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 350 Total reward: 10.0 Average reward fake: 0.03030010685324669 Average reward real: 0.8491387367248535 Training q_loss: 116.9554 Training g_loss: 100.3410 Training d_loss: 0.2282 Explore P: 0.5456\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 351 Total reward: 8.0 Average reward fake: 0.05663634464144707 Average reward real: 0.8224130272865295 Training q_loss: 126.4156 Training g_loss: 103.4987 Training d_loss: 0.3240 Explore P: 0.5451\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 352 Total reward: 12.0 Average reward fake: 0.026106806471943855 Average reward real: 0.9204750061035156 Training q_loss: 109.0665 Training g_loss: 90.5321 Training d_loss: 0.1248 Explore P: 0.5445\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 353 Total reward: 8.0 Average reward fake: 0.03260389342904091 Average reward real: 0.9150557518005371 Training q_loss: 123.8790 Training g_loss: 82.8418 Training d_loss: 0.1394 Explore P: 0.5441\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 354 Total reward: 16.0 Average reward fake: 0.12309937179088593 Average reward real: 0.9297412633895874 Training q_loss: 114.4269 Training g_loss: 91.7730 Training d_loss: 0.1822 Explore P: 0.5432\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 355 Total reward: 14.0 Average reward fake: 0.027906043455004692 Average reward real: 0.8536767363548279 Training q_loss: 123.3208 Training g_loss: 88.6447 Training d_loss: 0.2257 Explore P: 0.5425\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 356 Total reward: 12.0 Average reward fake: 0.0779685229063034 Average reward real: 0.9374803304672241 Training q_loss: 95.8932 Training g_loss: 72.1778 Training d_loss: 0.1842 Explore P: 0.5418\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 357 Total reward: 12.0 Average reward fake: 0.1077812984585762 Average reward real: 0.8847154378890991 Training q_loss: 126.6622 Training g_loss: 71.0175 Training d_loss: 0.2007 Explore P: 0.5412\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 358 Total reward: 8.0 Average reward fake: 0.13350099325180054 Average reward real: 0.8795103430747986 Training q_loss: 98.8242 Training g_loss: 78.2268 Training d_loss: 0.2779 Explore P: 0.5408\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 359 Total reward: 15.0 Average reward fake: 0.03577910736203194 Average reward real: 0.9480857849121094 Training q_loss: 90.9541 Training g_loss: 70.0291 Training d_loss: 0.1153 Explore P: 0.5400\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 360 Total reward: 20.0 Average reward fake: 0.031674738973379135 Average reward real: 0.9197889566421509 Training q_loss: 91.3189 Training g_loss: 68.1606 Training d_loss: 0.1623 Explore P: 0.5389\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 361 Total reward: 14.0 Average reward fake: 0.0624442957341671 Average reward real: 0.9103211164474487 Training q_loss: 84.9536 Training g_loss: 58.5836 Training d_loss: 0.1393 Explore P: 0.5382\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 362 Total reward: 11.0 Average reward fake: 0.04868645966053009 Average reward real: 0.9775644540786743 Training q_loss: 84.7105 Training g_loss: 66.6918 Training d_loss: 0.1232 Explore P: 0.5376\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 363 Total reward: 16.0 Average reward fake: 0.07559772580862045 Average reward real: 0.9061408042907715 Training q_loss: 186.7198 Training g_loss: 60.4933 Training d_loss: 0.2373 Explore P: 0.5367\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 364 Total reward: 29.0 Average reward fake: 0.053251493722200394 Average reward real: 0.8600997924804688 Training q_loss: 72.7493 Training g_loss: 50.3809 Training d_loss: 0.2405 Explore P: 0.5352\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 365 Total reward: 25.0 Average reward fake: 0.08048534393310547 Average reward real: 0.9504830837249756 Training q_loss: 90.5160 Training g_loss: 42.5930 Training d_loss: 0.1728 Explore P: 0.5339\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 366 Total reward: 24.0 Average reward fake: 0.0908777117729187 Average reward real: 0.9127495288848877 Training q_loss: 72.7358 Training g_loss: 43.5857 Training d_loss: 0.2381 Explore P: 0.5326\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 367 Total reward: 12.0 Average reward fake: 0.09690997004508972 Average reward real: 0.9784336686134338 Training q_loss: 149.3845 Training g_loss: 37.4493 Training d_loss: 0.1166 Explore P: 0.5320\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 368 Total reward: 8.0 Average reward fake: 0.0591212660074234 Average reward real: 0.9270896911621094 Training q_loss: 64.4953 Training g_loss: 39.1891 Training d_loss: 0.1483 Explore P: 0.5316\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 369 Total reward: 16.0 Average reward fake: 0.12659943103790283 Average reward real: 0.930429220199585 Training q_loss: 58.5179 Training g_loss: 28.6115 Training d_loss: 0.1620 Explore P: 0.5308\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 370 Total reward: 18.0 Average reward fake: 0.11878576129674911 Average reward real: 0.9680931568145752 Training q_loss: 64.0888 Training g_loss: 29.2921 Training d_loss: 0.1310 Explore P: 0.5298\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 371 Total reward: 15.0 Average reward fake: 0.06643658876419067 Average reward real: 0.8157814145088196 Training q_loss: 54.6435 Training g_loss: 22.4839 Training d_loss: 0.3075 Explore P: 0.5291\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 372 Total reward: 24.0 Average reward fake: 0.35634636878967285 Average reward real: 0.9341791272163391 Training q_loss: 87.3804 Training g_loss: 30.0445 Training d_loss: 0.4781 Explore P: 0.5278\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 373 Total reward: 21.0 Average reward fake: 0.07546732574701309 Average reward real: 0.9139787554740906 Training q_loss: 88.6175 Training g_loss: 29.4730 Training d_loss: 0.2112 Explore P: 0.5267\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 374 Total reward: 19.0 Average reward fake: 0.03627980127930641 Average reward real: 0.7759383320808411 Training q_loss: 47.6830 Training g_loss: 26.2513 Training d_loss: 0.3538 Explore P: 0.5257\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 375 Total reward: 16.0 Average reward fake: 0.04459057375788689 Average reward real: 0.8315309882164001 Training q_loss: 39.2295 Training g_loss: 19.6403 Training d_loss: 0.2725 Explore P: 0.5249\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 376 Total reward: 12.0 Average reward fake: 0.03530038893222809 Average reward real: 0.846494734287262 Training q_loss: 63.5808 Training g_loss: 32.5196 Training d_loss: 0.2319 Explore P: 0.5243\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 377 Total reward: 14.0 Average reward fake: 0.03983919695019722 Average reward real: 0.8026731610298157 Training q_loss: 65.8409 Training g_loss: 31.4679 Training d_loss: 0.3235 Explore P: 0.5236\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 378 Total reward: 15.0 Average reward fake: 0.08656621724367142 Average reward real: 0.9181910157203674 Training q_loss: 61.6316 Training g_loss: 30.2284 Training d_loss: 0.2288 Explore P: 0.5228\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 379 Total reward: 9.0 Average reward fake: 0.3033466041088104 Average reward real: 0.9740695953369141 Training q_loss: 49.0849 Training g_loss: 25.6256 Training d_loss: 0.3190 Explore P: 0.5223\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 380 Total reward: 9.0 Average reward fake: 0.022980835288763046 Average reward real: 0.8069128394126892 Training q_loss: 49.5867 Training g_loss: 27.3598 Training d_loss: 0.3074 Explore P: 0.5219\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 381 Total reward: 12.0 Average reward fake: 0.03445608913898468 Average reward real: 0.9385880827903748 Training q_loss: 36.8322 Training g_loss: 20.1867 Training d_loss: 0.1100 Explore P: 0.5213\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 382 Total reward: 13.0 Average reward fake: 0.05443574860692024 Average reward real: 0.9019802212715149 Training q_loss: 61.6651 Training g_loss: 19.8362 Training d_loss: 0.2167 Explore P: 0.5206\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 383 Total reward: 15.0 Average reward fake: 0.09958446770906448 Average reward real: 0.9771733283996582 Training q_loss: 65.8581 Training g_loss: 26.6969 Training d_loss: 0.1326 Explore P: 0.5198\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 384 Total reward: 16.0 Average reward fake: 0.05797557532787323 Average reward real: 0.940125584602356 Training q_loss: 47.4245 Training g_loss: 25.0584 Training d_loss: 0.1392 Explore P: 0.5190\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 385 Total reward: 11.0 Average reward fake: 0.055775247514247894 Average reward real: 0.9124139547348022 Training q_loss: 39.3321 Training g_loss: 20.9269 Training d_loss: 0.1971 Explore P: 0.5185\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 386 Total reward: 13.0 Average reward fake: 0.2204587608575821 Average reward real: 0.9608219861984253 Training q_loss: 51.1008 Training g_loss: 30.5168 Training d_loss: 0.1526 Explore P: 0.5178\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 387 Total reward: 15.0 Average reward fake: 0.04532570391893387 Average reward real: 0.8211124539375305 Training q_loss: 132.6474 Training g_loss: 59.5270 Training d_loss: 0.3215 Explore P: 0.5170\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 388 Total reward: 19.0 Average reward fake: 0.0270808394998312 Average reward real: 0.9021990895271301 Training q_loss: 83.4854 Training g_loss: 63.9654 Training d_loss: 0.1465 Explore P: 0.5161\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 389 Total reward: 13.0 Average reward fake: 0.16059574484825134 Average reward real: 0.9681239128112793 Training q_loss: 74.2228 Training g_loss: 54.8906 Training d_loss: 0.2259 Explore P: 0.5154\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 390 Total reward: 11.0 Average reward fake: 0.09029880166053772 Average reward real: 0.9713526368141174 Training q_loss: 103.2313 Training g_loss: 57.9766 Training d_loss: 0.1918 Explore P: 0.5149\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 391 Total reward: 15.0 Average reward fake: 0.07330197095870972 Average reward real: 0.9118584990501404 Training q_loss: 90.5102 Training g_loss: 54.2338 Training d_loss: 0.1824 Explore P: 0.5141\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 392 Total reward: 9.0 Average reward fake: 0.03411976248025894 Average reward real: 0.845955491065979 Training q_loss: 68.3300 Training g_loss: 44.8189 Training d_loss: 0.2379 Explore P: 0.5137\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 393 Total reward: 8.0 Average reward fake: 0.026888974010944366 Average reward real: 0.8869056701660156 Training q_loss: 65.6704 Training g_loss: 46.4170 Training d_loss: 0.1893 Explore P: 0.5133\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 394 Total reward: 16.0 Average reward fake: 0.04528756067156792 Average reward real: 0.934663712978363 Training q_loss: 122.7788 Training g_loss: 42.5611 Training d_loss: 0.1171 Explore P: 0.5125\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 395 Total reward: 32.0 Average reward fake: 0.050971757620573044 Average reward real: 0.9284844994544983 Training q_loss: 45.9233 Training g_loss: 29.4323 Training d_loss: 0.1285 Explore P: 0.5108\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 396 Total reward: 15.0 Average reward fake: 0.21599914133548737 Average reward real: 0.8662737011909485 Training q_loss: 47.7869 Training g_loss: 26.6582 Training d_loss: 1.1491 Explore P: 0.5101\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 397 Total reward: 10.0 Average reward fake: 0.0008090419578365982 Average reward real: 0.8757003545761108 Training q_loss: 60.3591 Training g_loss: 35.9307 Training d_loss: 0.2406 Explore P: 0.5096\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 398 Total reward: 13.0 Average reward fake: 0.0022698126267641783 Average reward real: 0.8683725595474243 Training q_loss: 54.9549 Training g_loss: 34.3434 Training d_loss: 0.1828 Explore P: 0.5089\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 399 Total reward: 13.0 Average reward fake: 0.050129376351833344 Average reward real: 0.6159281730651855 Training q_loss: 55.7617 Training g_loss: 35.1836 Training d_loss: 0.9356 Explore P: 0.5083\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 400 Total reward: 17.0 Average reward fake: 0.06482651829719543 Average reward real: 0.9169355034828186 Training q_loss: 54.4590 Training g_loss: 35.7697 Training d_loss: 0.2084 Explore P: 0.5075\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 401 Total reward: 16.0 Average reward fake: 0.10104192793369293 Average reward real: 0.9818568229675293 Training q_loss: 54.9052 Training g_loss: 29.3165 Training d_loss: 0.1452 Explore P: 0.5067\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 402 Total reward: 15.0 Average reward fake: 0.09493400901556015 Average reward real: 0.9583854675292969 Training q_loss: 145.6233 Training g_loss: 90.4873 Training d_loss: 0.1503 Explore P: 0.5059\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 403 Total reward: 18.0 Average reward fake: 0.07433556020259857 Average reward real: 0.8931775093078613 Training q_loss: 217.5600 Training g_loss: 100.6759 Training d_loss: 0.1835 Explore P: 0.5050\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 404 Total reward: 12.0 Average reward fake: 0.060599975287914276 Average reward real: 0.9308537840843201 Training q_loss: 117.4249 Training g_loss: 94.6668 Training d_loss: 0.1708 Explore P: 0.5044\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 405 Total reward: 13.0 Average reward fake: 0.03671400994062424 Average reward real: 0.8572303652763367 Training q_loss: 119.2874 Training g_loss: 95.2217 Training d_loss: 0.2259 Explore P: 0.5038\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 406 Total reward: 10.0 Average reward fake: 0.07849164307117462 Average reward real: 0.9159315228462219 Training q_loss: 237.8065 Training g_loss: 87.6260 Training d_loss: 0.1550 Explore P: 0.5033\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 407 Total reward: 9.0 Average reward fake: 0.1402849406003952 Average reward real: 0.9506030082702637 Training q_loss: 111.7145 Training g_loss: 87.6430 Training d_loss: 0.1606 Explore P: 0.5029\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 408 Total reward: 10.0 Average reward fake: 0.05839967727661133 Average reward real: 0.9010217189788818 Training q_loss: 123.1751 Training g_loss: 94.9350 Training d_loss: 0.1732 Explore P: 0.5024\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 409 Total reward: 13.0 Average reward fake: 0.04498566687107086 Average reward real: 0.8631120324134827 Training q_loss: 136.4409 Training g_loss: 93.3096 Training d_loss: 0.2003 Explore P: 0.5017\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 410 Total reward: 17.0 Average reward fake: 0.12627626955509186 Average reward real: 0.9264354109764099 Training q_loss: 86.9374 Training g_loss: 69.1263 Training d_loss: 0.2007 Explore P: 0.5009\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 411 Total reward: 8.0 Average reward fake: 0.09669508039951324 Average reward real: 0.9251564145088196 Training q_loss: 92.8896 Training g_loss: 73.7307 Training d_loss: 0.1676 Explore P: 0.5005\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 412 Total reward: 12.0 Average reward fake: 0.13615010678768158 Average reward real: 0.9558840990066528 Training q_loss: 103.0661 Training g_loss: 81.5087 Training d_loss: 0.1639 Explore P: 0.4999\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 413 Total reward: 11.0 Average reward fake: 0.08415534347295761 Average reward real: 0.9428046941757202 Training q_loss: 124.2816 Training g_loss: 77.6623 Training d_loss: 0.1497 Explore P: 0.4994\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 414 Total reward: 11.0 Average reward fake: 0.07613489031791687 Average reward real: 0.8932655453681946 Training q_loss: 88.1331 Training g_loss: 67.9939 Training d_loss: 0.2233 Explore P: 0.4988\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 415 Total reward: 17.0 Average reward fake: 0.05952051281929016 Average reward real: 0.9406154751777649 Training q_loss: 88.8651 Training g_loss: 60.9698 Training d_loss: 0.1563 Explore P: 0.4980\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 416 Total reward: 16.0 Average reward fake: 0.049928031861782074 Average reward real: 0.9449729919433594 Training q_loss: 92.8088 Training g_loss: 59.0074 Training d_loss: 0.1610 Explore P: 0.4972\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 417 Total reward: 13.0 Average reward fake: 0.037074651569128036 Average reward real: 0.8745570182800293 Training q_loss: 100.4506 Training g_loss: 57.3834 Training d_loss: 0.2213 Explore P: 0.4966\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 418 Total reward: 22.0 Average reward fake: 0.06511794775724411 Average reward real: 0.9367846846580505 Training q_loss: 75.8938 Training g_loss: 56.3870 Training d_loss: 0.1636 Explore P: 0.4955\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 419 Total reward: 16.0 Average reward fake: 0.10406534373760223 Average reward real: 0.9712852239608765 Training q_loss: 79.7951 Training g_loss: 52.9745 Training d_loss: 0.1324 Explore P: 0.4947\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 420 Total reward: 8.0 Average reward fake: 0.04463525861501694 Average reward real: 0.6177629232406616 Training q_loss: 76.5830 Training g_loss: 48.5864 Training d_loss: 0.7255 Explore P: 0.4943\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 421 Total reward: 24.0 Average reward fake: 0.022371964529156685 Average reward real: 0.7174972295761108 Training q_loss: 70.7343 Training g_loss: 46.3995 Training d_loss: 0.4899 Explore P: 0.4932\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 422 Total reward: 22.0 Average reward fake: 0.5530843734741211 Average reward real: 0.977294385433197 Training q_loss: 92.1512 Training g_loss: 36.8976 Training d_loss: 0.7866 Explore P: 0.4921\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 423 Total reward: 12.0 Average reward fake: 0.07635551691055298 Average reward real: 0.9587008953094482 Training q_loss: 85.1753 Training g_loss: 39.2617 Training d_loss: 0.1401 Explore P: 0.4915\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 424 Total reward: 15.0 Average reward fake: 0.08741229772567749 Average reward real: 0.9342086315155029 Training q_loss: 99.8257 Training g_loss: 36.7817 Training d_loss: 0.1630 Explore P: 0.4908\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 425 Total reward: 12.0 Average reward fake: 0.07510098814964294 Average reward real: 0.896253764629364 Training q_loss: 177.0629 Training g_loss: 35.5868 Training d_loss: 0.1981 Explore P: 0.4902\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 426 Total reward: 9.0 Average reward fake: 0.0879925787448883 Average reward real: 0.8985966444015503 Training q_loss: 47.1695 Training g_loss: 27.7139 Training d_loss: 0.2794 Explore P: 0.4898\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 427 Total reward: 8.0 Average reward fake: 0.05287756770849228 Average reward real: 0.8888919353485107 Training q_loss: 202.3484 Training g_loss: 30.1316 Training d_loss: 0.2233 Explore P: 0.4894\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 428 Total reward: 12.0 Average reward fake: 0.07566633075475693 Average reward real: 0.8599357604980469 Training q_loss: 50.8939 Training g_loss: 24.7304 Training d_loss: 0.2444 Explore P: 0.4889\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 429 Total reward: 11.0 Average reward fake: 0.10116232186555862 Average reward real: 0.9406953454017639 Training q_loss: 173.3283 Training g_loss: 38.6591 Training d_loss: 0.1648 Explore P: 0.4883\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 430 Total reward: 9.0 Average reward fake: 0.044171132147312164 Average reward real: 0.89227694272995 Training q_loss: 47.6464 Training g_loss: 29.8703 Training d_loss: 0.1771 Explore P: 0.4879\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 431 Total reward: 11.0 Average reward fake: 0.056349724531173706 Average reward real: 0.9134863018989563 Training q_loss: 67.6160 Training g_loss: 38.7086 Training d_loss: 0.1882 Explore P: 0.4874\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 432 Total reward: 13.0 Average reward fake: 0.05017818510532379 Average reward real: 0.886271059513092 Training q_loss: 51.7581 Training g_loss: 26.7715 Training d_loss: 0.1739 Explore P: 0.4868\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 433 Total reward: 11.0 Average reward fake: 0.09643065184354782 Average reward real: 0.9193429350852966 Training q_loss: 54.3285 Training g_loss: 25.7864 Training d_loss: 0.1566 Explore P: 0.4862\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 434 Total reward: 24.0 Average reward fake: 0.14257508516311646 Average reward real: 0.9041251540184021 Training q_loss: 88.7914 Training g_loss: 69.0074 Training d_loss: 0.3223 Explore P: 0.4851\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 435 Total reward: 12.0 Average reward fake: 0.08538339287042618 Average reward real: 0.958251953125 Training q_loss: 112.5572 Training g_loss: 83.9153 Training d_loss: 0.1508 Explore P: 0.4845\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 436 Total reward: 18.0 Average reward fake: 0.15367472171783447 Average reward real: 0.9682660102844238 Training q_loss: 102.7426 Training g_loss: 74.5172 Training d_loss: 0.1698 Explore P: 0.4837\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 437 Total reward: 8.0 Average reward fake: 0.04816315695643425 Average reward real: 0.900892972946167 Training q_loss: 80.8985 Training g_loss: 61.3288 Training d_loss: 0.1712 Explore P: 0.4833\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 438 Total reward: 18.0 Average reward fake: 0.06436297297477722 Average reward real: 0.9143472909927368 Training q_loss: 72.9290 Training g_loss: 53.4926 Training d_loss: 0.1881 Explore P: 0.4824\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 439 Total reward: 11.0 Average reward fake: 0.09102150797843933 Average reward real: 0.9502384066581726 Training q_loss: 132.9667 Training g_loss: 61.5172 Training d_loss: 0.2176 Explore P: 0.4819\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 440 Total reward: 15.0 Average reward fake: 0.06969163566827774 Average reward real: 0.9806258678436279 Training q_loss: 78.3447 Training g_loss: 58.1484 Training d_loss: 0.1663 Explore P: 0.4812\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 441 Total reward: 12.0 Average reward fake: 0.36712124943733215 Average reward real: 0.9406410455703735 Training q_loss: 85.1915 Training g_loss: 53.4909 Training d_loss: 0.7241 Explore P: 0.4806\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 442 Total reward: 15.0 Average reward fake: 0.06684235483407974 Average reward real: 0.8706058263778687 Training q_loss: 99.9211 Training g_loss: 61.6122 Training d_loss: 0.4024 Explore P: 0.4799\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 443 Total reward: 13.0 Average reward fake: 0.03870219737291336 Average reward real: 0.8747321963310242 Training q_loss: 67.4446 Training g_loss: 43.3650 Training d_loss: 0.2124 Explore P: 0.4793\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 444 Total reward: 8.0 Average reward fake: 0.07687988132238388 Average reward real: 0.8938937187194824 Training q_loss: 81.1795 Training g_loss: 52.8522 Training d_loss: 0.2262 Explore P: 0.4790\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 445 Total reward: 11.0 Average reward fake: 0.14481528103351593 Average reward real: 0.9089205861091614 Training q_loss: 91.9732 Training g_loss: 44.0368 Training d_loss: 0.2326 Explore P: 0.4784\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 446 Total reward: 21.0 Average reward fake: 0.04499632865190506 Average reward real: 0.8542200326919556 Training q_loss: 125.8684 Training g_loss: 36.0984 Training d_loss: 0.2470 Explore P: 0.4775\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 447 Total reward: 13.0 Average reward fake: 0.08469310402870178 Average reward real: 0.9479348659515381 Training q_loss: 66.6806 Training g_loss: 37.6405 Training d_loss: 0.1249 Explore P: 0.4768\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 448 Total reward: 28.0 Average reward fake: 0.08611209690570831 Average reward real: 0.9499287605285645 Training q_loss: 136.2552 Training g_loss: 35.5213 Training d_loss: 0.1394 Explore P: 0.4755\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 449 Total reward: 11.0 Average reward fake: 0.055554796010255814 Average reward real: 0.8821181654930115 Training q_loss: 102.3914 Training g_loss: 80.7555 Training d_loss: 0.2823 Explore P: 0.4750\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 450 Total reward: 9.0 Average reward fake: 0.0811961367726326 Average reward real: 0.9270063638687134 Training q_loss: 148.0629 Training g_loss: 111.8123 Training d_loss: 0.2732 Explore P: 0.4746\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 451 Total reward: 14.0 Average reward fake: 0.09374498575925827 Average reward real: 0.9269195795059204 Training q_loss: 130.7630 Training g_loss: 100.3339 Training d_loss: 0.1594 Explore P: 0.4740\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 452 Total reward: 13.0 Average reward fake: 0.043018121272325516 Average reward real: 0.903749406337738 Training q_loss: 181.8351 Training g_loss: 108.2519 Training d_loss: 0.1845 Explore P: 0.4734\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 453 Total reward: 18.0 Average reward fake: 0.039929911494255066 Average reward real: 0.9194888472557068 Training q_loss: 120.0436 Training g_loss: 90.1787 Training d_loss: 0.1699 Explore P: 0.4725\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 454 Total reward: 16.0 Average reward fake: 0.10974645614624023 Average reward real: 0.920865535736084 Training q_loss: 104.1292 Training g_loss: 75.4924 Training d_loss: 0.1862 Explore P: 0.4718\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 455 Total reward: 11.0 Average reward fake: 0.0963987335562706 Average reward real: 0.9515370726585388 Training q_loss: 161.7687 Training g_loss: 92.0706 Training d_loss: 0.2227 Explore P: 0.4713\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 456 Total reward: 12.0 Average reward fake: 0.061026740819215775 Average reward real: 0.9243618845939636 Training q_loss: 100.2055 Training g_loss: 80.7766 Training d_loss: 0.2370 Explore P: 0.4707\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 457 Total reward: 12.0 Average reward fake: 0.04490875080227852 Average reward real: 0.8699326515197754 Training q_loss: 210.9099 Training g_loss: 91.5341 Training d_loss: 0.2003 Explore P: 0.4702\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 458 Total reward: 9.0 Average reward fake: 0.09369561821222305 Average reward real: 0.8749954104423523 Training q_loss: 101.2759 Training g_loss: 78.1597 Training d_loss: 0.2517 Explore P: 0.4698\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 459 Total reward: 16.0 Average reward fake: 0.1281471997499466 Average reward real: 0.9515405297279358 Training q_loss: 109.9181 Training g_loss: 82.4828 Training d_loss: 0.2193 Explore P: 0.4690\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 460 Total reward: 18.0 Average reward fake: 0.06172093003988266 Average reward real: 0.9493549466133118 Training q_loss: 162.3267 Training g_loss: 74.2164 Training d_loss: 0.1534 Explore P: 0.4682\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 461 Total reward: 14.0 Average reward fake: 0.06859531998634338 Average reward real: 0.9255155920982361 Training q_loss: 85.9871 Training g_loss: 66.3153 Training d_loss: 0.1503 Explore P: 0.4676\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 462 Total reward: 11.0 Average reward fake: 0.10231783241033554 Average reward real: 0.9692592024803162 Training q_loss: 96.9803 Training g_loss: 67.1898 Training d_loss: 0.1883 Explore P: 0.4671\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 463 Total reward: 20.0 Average reward fake: 0.084307000041008 Average reward real: 0.9437669515609741 Training q_loss: 73.0474 Training g_loss: 52.4669 Training d_loss: 0.1689 Explore P: 0.4661\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 464 Total reward: 22.0 Average reward fake: 0.07618222385644913 Average reward real: 0.9342875480651855 Training q_loss: 79.0292 Training g_loss: 49.6432 Training d_loss: 0.1389 Explore P: 0.4651\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 465 Total reward: 8.0 Average reward fake: 0.07001364231109619 Average reward real: 0.918500542640686 Training q_loss: 107.0838 Training g_loss: 45.6646 Training d_loss: 0.1364 Explore P: 0.4648\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 466 Total reward: 12.0 Average reward fake: 0.060904573649168015 Average reward real: 0.9538161754608154 Training q_loss: 78.8242 Training g_loss: 46.1902 Training d_loss: 0.2185 Explore P: 0.4642\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 467 Total reward: 14.0 Average reward fake: 0.10396583378314972 Average reward real: 0.9005791544914246 Training q_loss: 73.2269 Training g_loss: 38.2956 Training d_loss: 0.2095 Explore P: 0.4636\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 468 Total reward: 11.0 Average reward fake: 0.21324680745601654 Average reward real: 0.9743902683258057 Training q_loss: 67.2123 Training g_loss: 38.7651 Training d_loss: 0.3071 Explore P: 0.4631\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 469 Total reward: 17.0 Average reward fake: 0.013770506717264652 Average reward real: 0.8638623952865601 Training q_loss: 72.6681 Training g_loss: 40.4643 Training d_loss: 0.1893 Explore P: 0.4623\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 470 Total reward: 13.0 Average reward fake: 0.006971077993512154 Average reward real: 0.9188442230224609 Training q_loss: 66.8663 Training g_loss: 44.2492 Training d_loss: 0.1278 Explore P: 0.4617\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 471 Total reward: 18.0 Average reward fake: 0.013607511296868324 Average reward real: 0.6516682505607605 Training q_loss: 62.7085 Training g_loss: 43.4485 Training d_loss: 0.7034 Explore P: 0.4609\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 472 Total reward: 14.0 Average reward fake: 0.14116854965686798 Average reward real: 0.9500680565834045 Training q_loss: 157.1314 Training g_loss: 43.3314 Training d_loss: 0.3093 Explore P: 0.4603\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 473 Total reward: 23.0 Average reward fake: 0.040769241750240326 Average reward real: 0.9145747423171997 Training q_loss: 104.9110 Training g_loss: 38.6228 Training d_loss: 0.1424 Explore P: 0.4593\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 474 Total reward: 21.0 Average reward fake: 0.14357952773571014 Average reward real: 0.9586977362632751 Training q_loss: 105.7392 Training g_loss: 69.5747 Training d_loss: 0.1821 Explore P: 0.4583\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 475 Total reward: 11.0 Average reward fake: 0.04218374192714691 Average reward real: 0.9159963726997375 Training q_loss: 113.2527 Training g_loss: 91.0116 Training d_loss: 0.1493 Explore P: 0.4578\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 476 Total reward: 13.0 Average reward fake: 0.054583873599767685 Average reward real: 0.9484151601791382 Training q_loss: 164.3569 Training g_loss: 94.8832 Training d_loss: 0.0950 Explore P: 0.4572\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 477 Total reward: 10.0 Average reward fake: 0.055282581597566605 Average reward real: 0.9368113875389099 Training q_loss: 99.6563 Training g_loss: 81.4974 Training d_loss: 0.1348 Explore P: 0.4568\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 478 Total reward: 8.0 Average reward fake: 0.07513496279716492 Average reward real: 0.940897524356842 Training q_loss: 100.7485 Training g_loss: 82.1821 Training d_loss: 0.1434 Explore P: 0.4564\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 479 Total reward: 9.0 Average reward fake: 0.07722146809101105 Average reward real: 0.9523555040359497 Training q_loss: 134.3219 Training g_loss: 85.6665 Training d_loss: 0.1323 Explore P: 0.4560\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 480 Total reward: 15.0 Average reward fake: 0.06568890810012817 Average reward real: 0.9243040680885315 Training q_loss: 113.7247 Training g_loss: 81.7251 Training d_loss: 0.1529 Explore P: 0.4554\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 481 Total reward: 12.0 Average reward fake: 0.06660046428442001 Average reward real: 0.9275839924812317 Training q_loss: 104.0738 Training g_loss: 74.1740 Training d_loss: 0.1562 Explore P: 0.4548\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 482 Total reward: 8.0 Average reward fake: 0.0635882169008255 Average reward real: 0.9329016208648682 Training q_loss: 99.0772 Training g_loss: 74.6575 Training d_loss: 0.1769 Explore P: 0.4545\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 483 Total reward: 13.0 Average reward fake: 0.09384655207395554 Average reward real: 0.9539092779159546 Training q_loss: 110.5075 Training g_loss: 81.0104 Training d_loss: 0.1195 Explore P: 0.4539\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 484 Total reward: 23.0 Average reward fake: 0.07175477594137192 Average reward real: 0.9068546891212463 Training q_loss: 77.0317 Training g_loss: 60.2815 Training d_loss: 0.1887 Explore P: 0.4529\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Episode: 485 Total reward: 18.0 Average reward fake: 0.05863068997859955 Average reward real: 0.9450190663337708 Training q_loss: 99.9043 Training g_loss: 64.0149 Training d_loss: 0.1467 Explore P: 0.4521\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 486 Total reward: 20.0 Average reward fake: 0.030766822397708893 Average reward real: 0.8674182295799255 Training q_loss: 91.2550 Training g_loss: 48.1296 Training d_loss: 0.1934 Explore P: 0.4512\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 487 Total reward: 11.0 Average reward fake: 0.06325861066579819 Average reward real: 0.9366245865821838 Training q_loss: 79.9186 Training g_loss: 57.1687 Training d_loss: 0.1561 Explore P: 0.4507\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 488 Total reward: 15.0 Average reward fake: 0.12326568365097046 Average reward real: 0.9543029069900513 Training q_loss: 115.5726 Training g_loss: 48.4527 Training d_loss: 0.2777 Explore P: 0.4501\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 489 Total reward: 11.0 Average reward fake: 0.06842882186174393 Average reward real: 0.908751904964447 Training q_loss: 154.6850 Training g_loss: 44.3331 Training d_loss: 0.1850 Explore P: 0.4496\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 490 Total reward: 17.0 Average reward fake: 0.01803893968462944 Average reward real: 0.811988353729248 Training q_loss: 152.5817 Training g_loss: 38.7958 Training d_loss: 0.2538 Explore P: 0.4488\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 491 Total reward: 10.0 Average reward fake: 0.045795127749443054 Average reward real: 0.8650252819061279 Training q_loss: 64.2719 Training g_loss: 35.9053 Training d_loss: 0.2203 Explore P: 0.4484\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 492 Total reward: 9.0 Average reward fake: 0.05566233769059181 Average reward real: 0.8836549520492554 Training q_loss: 63.7060 Training g_loss: 40.1059 Training d_loss: 0.1833 Explore P: 0.4480\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 493 Total reward: 14.0 Average reward fake: 0.014213455840945244 Average reward real: 0.6776980757713318 Training q_loss: 76.6732 Training g_loss: 35.8057 Training d_loss: 0.6355 Explore P: 0.4474\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 494 Total reward: 7.0 Average reward fake: 0.07506787776947021 Average reward real: 0.9626208543777466 Training q_loss: 65.0755 Training g_loss: 27.6867 Training d_loss: 0.1581 Explore P: 0.4471\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 495 Total reward: 8.0 Average reward fake: 0.008564595133066177 Average reward real: 0.5295071601867676 Training q_loss: 75.5561 Training g_loss: 28.2782 Training d_loss: 1.7319 Explore P: 0.4467\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 496 Total reward: 24.0 Average reward fake: 0.03370339050889015 Average reward real: 0.842598557472229 Training q_loss: 73.3291 Training g_loss: 29.6839 Training d_loss: 0.2239 Explore P: 0.4457\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 497 Total reward: 11.0 Average reward fake: 0.034286972135305405 Average reward real: 0.8287150859832764 Training q_loss: 129.4951 Training g_loss: 32.9703 Training d_loss: 0.2913 Explore P: 0.4452\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 498 Total reward: 14.0 Average reward fake: 0.1129806712269783 Average reward real: 0.9903544783592224 Training q_loss: 119.4933 Training g_loss: 99.6365 Training d_loss: 0.2150 Explore P: 0.4446\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 499 Total reward: 15.0 Average reward fake: 0.1617886871099472 Average reward real: 0.9650856256484985 Training q_loss: 150.4832 Training g_loss: 114.7889 Training d_loss: 0.2958 Explore P: 0.4439\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 500 Total reward: 14.0 Average reward fake: 0.040397901087999344 Average reward real: 0.9649549722671509 Training q_loss: 125.9761 Training g_loss: 95.9176 Training d_loss: 0.1230 Explore P: 0.4433\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 501 Total reward: 16.0 Average reward fake: 0.0254683680832386 Average reward real: 0.9551454186439514 Training q_loss: 131.7033 Training g_loss: 111.8338 Training d_loss: 0.1116 Explore P: 0.4426\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Episode: 502 Total reward: 18.0 Average reward fake: 0.08201547712087631 Average reward real: 0.9466277360916138 Training q_loss: 145.3399 Training g_loss: 105.4962 Training d_loss: 0.2311 Explore P: 0.4419\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Total rewards and losses list for plotting\n",
    "rewards_list, rewards_fake_list, rewards_real_list = [], [], []\n",
    "q_loss_list, g_loss_list, d_loss_list = [], [], [] \n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     # Restore/load the trained model \n",
    "    #     #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #     saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    step = 0\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Env/agent steps/batches/minibatches\n",
    "        total_reward, rewards_fake_mean, rewards_real_mean = 0, 0, 0\n",
    "        q_loss, g_loss, d_loss = 0, 0, 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from model\n",
    "                feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "                actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "                action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            # Cumulative reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Episode/epoch training is done/failed!\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Average reward fake: {}'.format(rewards_fake_mean),\n",
    "                      'Average reward real: {}'.format(rewards_real_mean),\n",
    "                      'Training q_loss: {:.4f}'.format(q_loss),\n",
    "                      'Training g_loss: {:.4f}'.format(g_loss),\n",
    "                      'Training d_loss: {:.4f}'.format(d_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                # total rewards and losses for plotting\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                rewards_fake_list.append((ep, rewards_fake_mean))\n",
    "                rewards_real_list.append((ep, rewards_real_mean))\n",
    "                q_loss_list.append((ep, q_loss))\n",
    "                g_loss_list.append((ep, g_loss))\n",
    "                d_loss_list.append((ep, d_loss))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            #rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Calculating real current reward and next action\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states}\n",
    "            next_actions_logits, rewards_fake, rewards_real = sess.run([model.actions_logits, \n",
    "                                                                        model.rewards_fake, model.rewards_real], \n",
    "                                                                       feed_dict)\n",
    "            #             feed_dict={model.states: next_states}\n",
    "            #             next_actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "\n",
    "            # Mean/average fake and real rewards or rewarded generated/given actions\n",
    "            rewards_fake_mean = np.mean(rewards_fake.reshape(-1))\n",
    "            rewards_real_mean = np.mean(rewards_real.reshape(-1))\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            next_actions_logits[episode_ends] = (0, 0) # NOTE: action size\n",
    "\n",
    "            # Bellman equation: Qt = Rt + max(Qt+1)\n",
    "            #targetQs = rewards_fake.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "            targetQs = rewards_real.reshape(-1) + (gamma * np.max(next_actions_logits, axis=1))\n",
    "\n",
    "            # Updating/training/optimizing the model\n",
    "            feed_dict = {model.states: states, model.actions: actions, model.next_states: next_states,\n",
    "                         model.targetQs: targetQs}\n",
    "            q_loss, _ = sess.run([model.q_loss, model.q_opt], feed_dict)\n",
    "            g_loss, _ = sess.run([model.g_loss, model.g_opt], feed_dict)\n",
    "            d_loss, _ = sess.run([model.d_loss, model.d_opt], feed_dict)\n",
    "            \n",
    "    # Save the trained model\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_fake_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Fake rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_real_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Real rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(q_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(g_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('G losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(d_loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('D losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "test_max_steps = 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "# # # Create the env after closing it.\n",
    "# env = gym.make('CartPole-v0')\n",
    "# # env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore/load the trained model \n",
    "    #saver.restore(sess, 'checkpoints/QGAN-cartpole.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # iterations\n",
    "    for ep in range(test_episodes):\n",
    "        \n",
    "        # number of env/rob steps\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            \n",
    "            # Rendering the env graphics\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from DQAN\n",
    "            feed_dict = {model.states: state.reshape((1, *state.shape))}\n",
    "            actions_logits = sess.run(model.actions_logits, feed_dict)\n",
    "            action = np.argmax(actions_logits)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # The task is done or not;\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                \n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the env\n",
    "# WARNING: If you close, you can NOT restart again!!!!!!\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this to Deep Convolutional QAN\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
