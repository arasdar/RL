{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_v1/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_OneAgent/Reacher_Linux/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_OneAgent/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='/home/arasdar/unity-envs/Reacher_Linux_NoVis_MultiAgents/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  7.90150833e+00 -1.00000000e+00\n",
      "  1.25147629e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.22214413e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the train mode\n",
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "# state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# #scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# num_steps = 0\n",
    "# while True:\n",
    "#     num_steps += 1\n",
    "#     action = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     #print(action)\n",
    "#     action = np.clip(action, -1, 1)                  # all actions between -1 and 1\n",
    "#     #print(action)\n",
    "#     env_info = env.step(action)[brain_name]           # send all actions to tne environment\n",
    "#     next_state = env_info.vector_observations[0]         # get next state (for each agent)\n",
    "#     reward = env_info.rewards[0]                         # get reward (for each agent)\n",
    "#     done = env_info.local_done[0]                        # see if episode finished\n",
    "#     #scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     state = next_state                               # roll over states to next time step\n",
    "#     if done is True:                                  # exit loop if episode finished\n",
    "#         #print(action.shape, reward)\n",
    "#         #print(done)\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "# num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, action_size):\n",
    "    #states = tf.placeholder(tf.float32, [None, *state_size], name='states')\n",
    "    states = tf.placeholder(tf.float64, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.float64, [None, action_size], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float64, [None], name='targetQs')\n",
    "    isTraining = tf.placeholder(tf.bool, [], name='isTraining')\n",
    "    return states, actions, targetQs, isTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('actor', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic(states, actions, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('critic', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=action_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        nl1_fused = tf.concat(axis=1, values=[nl1, actions])\n",
    "        h2 = tf.layers.dense(inputs=nl1_fused, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "                \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, action_size, hidden_size, isTraining):\n",
    "    ###################################################################\n",
    "    actions_logits = actor(states=states, hidden_size=hidden_size, action_size=action_size, \n",
    "                           training=isTraining)\n",
    "    gQlogits = critic(states=states, actions=actions_logits, hidden_size=hidden_size, action_size=action_size,\n",
    "                      training=isTraining)\n",
    "    ###########################################################################\n",
    "    Qlogits = critic(states=states, actions=actions, hidden_size=hidden_size, action_size=action_size, \n",
    "                     training=isTraining, reuse=True)\n",
    "    ###########################################################################\n",
    "    Qs = tf.reshape(Qlogits, shape=[-1])\n",
    "    gQs = tf.reshape(gQlogits, shape=[-1])\n",
    "    dloss = tf.reduce_mean(tf.square(Qs - targetQs))\n",
    "    gloss = -tf.reduce_mean(gQs)\n",
    "    return actions_logits, gQlogits, gloss, dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(gloss, dloss, g_learning_rate, d_learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('actor')]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('critic')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        g_opt = tf.train.AdamOptimizer(g_learning_rate).minimize(gloss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(d_learning_rate).minimize(dloss, var_list=d_vars)\n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, g_learning_rate, d_learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.states, self.actions, self.targetQs, self.isTraining = model_input(state_size=state_size, \n",
    "                                                                                action_size=action_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.gQlogits, self.gloss, self.dloss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, isTraining=self.isTraining) # model input\n",
    "        \n",
    "        # Update the model: backward pass and backprop\n",
    "        self.g_opt, self.d_opt = model_opt(gloss=self.gloss, dloss=self.dloss,\n",
    "                                           g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 33), (20, 4), 4, 0, 4, 33)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info.vector_observations.shape, env_info.previous_vector_actions.shape, \\\n",
    "brain.vector_action_space_size, brain.number_visual_observations, \\\n",
    "brain.vector_action_space_size, brain.vector_observation_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "hidden_size = 33*2             # number of units in each Q-network hidden layer\n",
    "g_learning_rate = 1e-3         # Q-network learning rate\n",
    "d_learning_rate = 1e-3         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = int(1e6)            # memory capacity\n",
    "batch_size = 1024             # experience mini-batch size\n",
    "gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, state_size=state_size, hidden_size=hidden_size, gamma=gamma,\n",
    "              g_learning_rate=g_learning_rate, d_learning_rate=d_learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# for _ in range(memory_size):\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "#     for state, action, next_state, reward, done in zip(states, actions, next_states, rewards, dones):\n",
    "#         #agent.step(state, action, reward, next_state, done) # send actions to the agent\n",
    "#         memory.buffer.append([state, action, next_state, reward, done])\n",
    "        \n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "    \n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         print('Average scores: {}'.format(np.mean(scores)))\n",
    "#         env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "#         states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#         scores = np.zeros(num_agents)                          # initialize the score (for each agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "# states = env_info.vector_observations   # get the state\n",
    "# for _ in range(memory_size):\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "#     state = next_state\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]   # get the state\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:0.1980 R:0.1980 gloss:-3.9094 dloss:70.9249 exploreP:0.9058\n",
      "Episode:1 meanR:0.1690 R:0.1400 gloss:-35.1160 dloss:148.9027 exploreP:0.8205\n",
      "Episode:2 meanR:0.1323 R:0.0590 gloss:-53.2826 dloss:37.7698 exploreP:0.7434\n",
      "Episode:3 meanR:0.1245 R:0.1010 gloss:-113.9594 dloss:2857.2634 exploreP:0.6736\n",
      "Episode:4 meanR:0.1273 R:0.1385 gloss:-223.6206 dloss:1229.3695 exploreP:0.6105\n",
      "Episode:5 meanR:0.1487 R:0.2555 gloss:-330.2719 dloss:1397.7375 exploreP:0.5533\n",
      "Episode:6 meanR:0.1540 R:0.1860 gloss:-518.6990 dloss:1702.5973 exploreP:0.5016\n",
      "Episode:7 meanR:0.1659 R:0.2495 gloss:-761.5189 dloss:3698.9479 exploreP:0.4548\n",
      "Episode:8 meanR:0.1903 R:0.3855 gloss:-1029.6779 dloss:34510.5311 exploreP:0.4125\n",
      "Episode:9 meanR:0.2345 R:0.6320 gloss:-1163.9317 dloss:16449.2597 exploreP:0.3742\n",
      "Episode:10 meanR:0.2774 R:0.7065 gloss:-1346.3811 dloss:12830.7884 exploreP:0.3395\n",
      "Episode:11 meanR:0.3383 R:1.0085 gloss:-1408.8666 dloss:11176.2516 exploreP:0.3082\n",
      "Episode:12 meanR:0.4301 R:1.5310 gloss:-1675.6377 dloss:23965.3560 exploreP:0.2798\n",
      "Episode:13 meanR:0.5173 R:1.6510 gloss:-2015.4664 dloss:14771.0307 exploreP:0.2541\n",
      "Episode:14 meanR:0.5734 R:1.3590 gloss:-1917.5338 dloss:34652.3845 exploreP:0.2309\n",
      "Episode:15 meanR:0.5978 R:0.9635 gloss:-2150.8171 dloss:17227.4852 exploreP:0.2099\n",
      "Episode:16 meanR:0.6237 R:1.0380 gloss:-2362.8950 dloss:16966.3882 exploreP:0.1909\n",
      "Episode:17 meanR:0.6315 R:0.7645 gloss:-3019.0494 dloss:29314.3906 exploreP:0.1736\n",
      "Episode:18 meanR:0.6368 R:0.7315 gloss:-3491.5595 dloss:25149.1070 exploreP:0.1581\n",
      "Episode:19 meanR:0.6636 R:1.1735 gloss:-3435.3088 dloss:25878.3556 exploreP:0.1440\n",
      "Episode:20 meanR:0.6579 R:0.5440 gloss:-3609.8111 dloss:23519.9131 exploreP:0.1312\n",
      "Episode:21 meanR:0.6581 R:0.6625 gloss:-4159.0633 dloss:31865.8912 exploreP:0.1197\n",
      "Episode:22 meanR:0.6483 R:0.4335 gloss:-4723.7532 dloss:88084.1891 exploreP:0.1093\n",
      "Episode:23 meanR:0.6470 R:0.6150 gloss:-5309.4375 dloss:630360.2992 exploreP:0.0998\n",
      "Episode:24 meanR:0.6536 R:0.8140 gloss:-5781.1649 dloss:460480.8682 exploreP:0.0913\n",
      "Episode:25 meanR:0.6479 R:0.5050 gloss:-6290.8144 dloss:131440.2035 exploreP:0.0835\n",
      "Episode:26 meanR:0.6378 R:0.3740 gloss:-6683.3679 dloss:256090.2133 exploreP:0.0765\n",
      "Episode:27 meanR:0.6329 R:0.5005 gloss:-7091.7288 dloss:192184.4638 exploreP:0.0702\n",
      "Episode:28 meanR:0.6248 R:0.3995 gloss:-7512.7518 dloss:303746.9772 exploreP:0.0645\n",
      "Episode:29 meanR:0.6232 R:0.5765 gloss:-7794.8326 dloss:276117.0198 exploreP:0.0593\n",
      "Episode:30 meanR:0.6191 R:0.4945 gloss:-8044.6869 dloss:191127.0568 exploreP:0.0546\n",
      "Episode:31 meanR:0.6240 R:0.7755 gloss:-8290.5548 dloss:154955.4442 exploreP:0.0504\n",
      "Episode:32 meanR:0.6239 R:0.6235 gloss:-8438.1165 dloss:200363.1705 exploreP:0.0465\n",
      "Episode:33 meanR:0.6246 R:0.6460 gloss:-8594.6128 dloss:204115.5242 exploreP:0.0430\n",
      "Episode:34 meanR:0.6164 R:0.3395 gloss:-8835.3518 dloss:191604.9170 exploreP:0.0399\n",
      "Episode:35 meanR:0.6048 R:0.1985 gloss:-9068.0352 dloss:267923.9624 exploreP:0.0371\n",
      "Episode:36 meanR:0.6044 R:0.5880 gloss:-9339.4619 dloss:127237.3234 exploreP:0.0345\n",
      "Episode:37 meanR:0.5966 R:0.3075 gloss:-9738.1357 dloss:191012.3229 exploreP:0.0321\n",
      "Episode:38 meanR:0.5980 R:0.6535 gloss:-10230.2495 dloss:152541.4272 exploreP:0.0300\n",
      "Episode:39 meanR:0.5932 R:0.4040 gloss:-10761.5015 dloss:185572.3088 exploreP:0.0281\n",
      "Episode:40 meanR:0.5898 R:0.4545 gloss:-11137.1440 dloss:204026.9808 exploreP:0.0264\n",
      "Episode:41 meanR:0.5834 R:0.3220 gloss:-11095.2776 dloss:335298.7342 exploreP:0.0248\n",
      "Episode:42 meanR:0.5772 R:0.3160 gloss:-11546.8446 dloss:577235.1240 exploreP:0.0234\n",
      "Episode:43 meanR:0.5706 R:0.2875 gloss:-12744.3017 dloss:856051.3827 exploreP:0.0222\n",
      "Episode:44 meanR:0.5703 R:0.5570 gloss:-13099.1696 dloss:3007963.4959 exploreP:0.0210\n",
      "Episode:45 meanR:0.5719 R:0.6425 gloss:-13295.1902 dloss:387525.0194 exploreP:0.0200\n",
      "Episode:46 meanR:0.5682 R:0.3970 gloss:-12993.3316 dloss:601826.5903 exploreP:0.0190\n",
      "Episode:47 meanR:0.5642 R:0.3760 gloss:-13187.8438 dloss:620194.8746 exploreP:0.0181\n",
      "Episode:48 meanR:0.5601 R:0.3630 gloss:-13583.7492 dloss:421077.5037 exploreP:0.0174\n",
      "Episode:49 meanR:0.5568 R:0.3980 gloss:-13451.8766 dloss:713689.9425 exploreP:0.0167\n",
      "Episode:50 meanR:0.5566 R:0.5460 gloss:-13604.0792 dloss:762473.3472 exploreP:0.0160\n",
      "Episode:51 meanR:0.5558 R:0.5135 gloss:-13457.7035 dloss:485698.7567 exploreP:0.0155\n",
      "Episode:52 meanR:0.5625 R:0.9125 gloss:-13377.2275 dloss:1057250.1911 exploreP:0.0149\n",
      "Episode:53 meanR:0.5677 R:0.8455 gloss:-13741.1620 dloss:760445.5351 exploreP:0.0145\n",
      "Episode:54 meanR:0.5727 R:0.8385 gloss:-14039.3875 dloss:816479.3271 exploreP:0.0140\n",
      "Episode:55 meanR:0.5758 R:0.7495 gloss:-14435.5947 dloss:1844105.6484 exploreP:0.0137\n",
      "Episode:56 meanR:0.5799 R:0.8060 gloss:-15054.3790 dloss:1811014.0107 exploreP:0.0133\n",
      "Episode:57 meanR:0.5804 R:0.6095 gloss:-16304.5643 dloss:1113438.3581 exploreP:0.0130\n",
      "Episode:58 meanR:0.5826 R:0.7120 gloss:-17448.2663 dloss:780099.8106 exploreP:0.0127\n",
      "Episode:59 meanR:0.5842 R:0.6790 gloss:-18386.6590 dloss:2849360.0561 exploreP:0.0125\n",
      "Episode:60 meanR:0.5822 R:0.4630 gloss:-19065.9249 dloss:907899.1604 exploreP:0.0122\n",
      "Episode:61 meanR:0.5839 R:0.6875 gloss:-19909.2513 dloss:3360056.6967 exploreP:0.0120\n",
      "Episode:62 meanR:0.5821 R:0.4700 gloss:-20799.6829 dloss:1156082.8039 exploreP:0.0118\n",
      "Episode:63 meanR:0.5851 R:0.7720 gloss:-21864.9587 dloss:1420414.1431 exploreP:0.0116\n",
      "Episode:64 meanR:0.5823 R:0.4055 gloss:-23002.6177 dloss:2242589.2913 exploreP:0.0115\n",
      "Episode:65 meanR:0.5801 R:0.4330 gloss:-24158.1699 dloss:3159741.6846 exploreP:0.0113\n",
      "Episode:66 meanR:0.5803 R:0.5975 gloss:-25472.1486 dloss:1754022.7091 exploreP:0.0112\n",
      "Episode:67 meanR:0.5789 R:0.4850 gloss:-26882.7623 dloss:3554358.0968 exploreP:0.0111\n",
      "Episode:68 meanR:0.5783 R:0.5345 gloss:-28292.0738 dloss:1758504.4429 exploreP:0.0110\n",
      "Episode:69 meanR:0.5760 R:0.4165 gloss:-29771.0019 dloss:2934517.1963 exploreP:0.0109\n",
      "Episode:70 meanR:0.5743 R:0.4595 gloss:-31299.3359 dloss:2116491.6701 exploreP:0.0108\n",
      "Episode:71 meanR:0.5727 R:0.4590 gloss:-32722.0673 dloss:2329912.7783 exploreP:0.0107\n",
      "Episode:72 meanR:0.5697 R:0.3525 gloss:-34025.4341 dloss:2575434.4126 exploreP:0.0107\n",
      "Episode:73 meanR:0.5674 R:0.3970 gloss:-35571.1060 dloss:3311629.3802 exploreP:0.0106\n",
      "Episode:74 meanR:0.5642 R:0.3280 gloss:-37319.7519 dloss:1572028.2525 exploreP:0.0105\n",
      "Episode:75 meanR:0.5665 R:0.7415 gloss:-39337.5035 dloss:2937816.7456 exploreP:0.0105\n",
      "Episode:76 meanR:0.5661 R:0.5315 gloss:-41565.6178 dloss:6304525.3428 exploreP:0.0104\n",
      "Episode:77 meanR:0.5657 R:0.5410 gloss:-42929.6376 dloss:2020774.4695 exploreP:0.0104\n",
      "Episode:78 meanR:0.5637 R:0.4075 gloss:-44834.0587 dloss:4248238.2358 exploreP:0.0104\n",
      "Episode:79 meanR:0.5585 R:0.1420 gloss:-47306.4816 dloss:5501357.9497 exploreP:0.0103\n",
      "Episode:80 meanR:0.5533 R:0.1425 gloss:-49607.0221 dloss:4896277.8771 exploreP:0.0103\n",
      "Episode:81 meanR:0.5472 R:0.0510 gloss:-51371.7651 dloss:4469560.9844 exploreP:0.0103\n",
      "Episode:82 meanR:0.5410 R:0.0345 gloss:-53547.5632 dloss:5888719.3398 exploreP:0.0102\n",
      "Episode:83 meanR:0.5349 R:0.0250 gloss:-55656.0674 dloss:8160229.9649 exploreP:0.0102\n",
      "Episode:84 meanR:0.5291 R:0.0445 gloss:-57983.6343 dloss:7120325.8722 exploreP:0.0102\n",
      "Episode:85 meanR:0.5236 R:0.0535 gloss:-60275.9994 dloss:10812566.3263 exploreP:0.0102\n",
      "Episode:86 meanR:0.5182 R:0.0590 gloss:-63072.8845 dloss:13575612.2089 exploreP:0.0102\n",
      "Episode:87 meanR:0.5133 R:0.0835 gloss:-65289.0526 dloss:11813838.0975 exploreP:0.0101\n",
      "Episode:88 meanR:0.5087 R:0.1000 gloss:-67445.7842 dloss:12249338.6600 exploreP:0.0101\n",
      "Episode:89 meanR:0.5037 R:0.0670 gloss:-68774.3275 dloss:15061025.6592 exploreP:0.0101\n",
      "Episode:90 meanR:0.5003 R:0.1915 gloss:-69248.1593 dloss:6209263.2536 exploreP:0.0101\n",
      "Episode:91 meanR:0.4966 R:0.1555 gloss:-70180.2400 dloss:7864274.2046 exploreP:0.0101\n",
      "Episode:92 meanR:0.4924 R:0.1110 gloss:-73087.9928 dloss:13023462.0330 exploreP:0.0101\n",
      "Episode:93 meanR:0.4878 R:0.0590 gloss:-75907.0877 dloss:31472032.5506 exploreP:0.0101\n",
      "Episode:94 meanR:0.4855 R:0.2705 gloss:-78025.0599 dloss:57239743.2194 exploreP:0.0101\n",
      "Episode:95 meanR:0.4846 R:0.3965 gloss:-79380.6480 dloss:13097646.0593 exploreP:0.0101\n",
      "Episode:96 meanR:0.4861 R:0.6315 gloss:-81208.3279 dloss:19987820.0773 exploreP:0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:97 meanR:0.4855 R:0.4265 gloss:-83177.3785 dloss:20915977.8933 exploreP:0.0101\n",
      "Episode:98 meanR:0.4838 R:0.3165 gloss:-84731.4485 dloss:25871013.6040 exploreP:0.0100\n",
      "Episode:99 meanR:0.4809 R:0.1995 gloss:-85947.2341 dloss:35912204.2247 exploreP:0.0100\n",
      "Episode:100 meanR:0.4837 R:0.4715 gloss:-87337.5501 dloss:10557582.2537 exploreP:0.0100\n",
      "Episode:101 meanR:0.4865 R:0.4200 gloss:-88583.5641 dloss:16744735.1187 exploreP:0.0100\n",
      "Episode:102 meanR:0.4896 R:0.3745 gloss:-90787.3892 dloss:24569384.2632 exploreP:0.0100\n",
      "Episode:103 meanR:0.4922 R:0.3555 gloss:-91339.6390 dloss:17868248.5470 exploreP:0.0100\n",
      "Episode:104 meanR:0.4974 R:0.6630 gloss:-91187.5514 dloss:27756878.3392 exploreP:0.0100\n",
      "Episode:105 meanR:0.5014 R:0.6565 gloss:-93995.1693 dloss:17893127.5745 exploreP:0.0100\n",
      "Episode:106 meanR:0.5071 R:0.7490 gloss:-95990.0137 dloss:8365997.4112 exploreP:0.0100\n",
      "Episode:107 meanR:0.5090 R:0.4380 gloss:-98349.3069 dloss:14891072.7482 exploreP:0.0100\n",
      "Episode:108 meanR:0.5141 R:0.8965 gloss:-101109.8309 dloss:11945557.9669 exploreP:0.0100\n",
      "Episode:109 meanR:0.5143 R:0.6535 gloss:-103749.7329 dloss:16217563.4320 exploreP:0.0100\n",
      "Episode:110 meanR:0.5151 R:0.7855 gloss:-106290.4498 dloss:98646209.2288 exploreP:0.0100\n",
      "Episode:111 meanR:0.5116 R:0.6655 gloss:-104760.1146 dloss:56315860.4616 exploreP:0.0100\n",
      "Episode:112 meanR:0.5025 R:0.6180 gloss:-107378.7543 dloss:25073912.3151 exploreP:0.0100\n",
      "Episode:113 meanR:0.4937 R:0.7710 gloss:-110050.5429 dloss:50747067.6232 exploreP:0.0100\n",
      "Episode:114 meanR:0.4862 R:0.6110 gloss:-113452.2878 dloss:45884667.9899 exploreP:0.0100\n",
      "Episode:115 meanR:0.4837 R:0.7100 gloss:-116466.8251 dloss:53127815.5218 exploreP:0.0100\n",
      "Episode:116 meanR:0.4809 R:0.7590 gloss:-119747.0275 dloss:61924192.5476 exploreP:0.0100\n",
      "Episode:117 meanR:0.4814 R:0.8190 gloss:-122357.5475 dloss:68023031.3548 exploreP:0.0100\n",
      "Episode:118 meanR:0.4807 R:0.6580 gloss:-124864.6997 dloss:37019777.3487 exploreP:0.0100\n",
      "Episode:119 meanR:0.4753 R:0.6355 gloss:-127022.2565 dloss:44484895.7752 exploreP:0.0100\n",
      "Episode:120 meanR:0.4766 R:0.6675 gloss:-129611.3203 dloss:152586839.7459 exploreP:0.0100\n",
      "Episode:121 meanR:0.4758 R:0.5840 gloss:-132384.8280 dloss:81140319.0932 exploreP:0.0100\n",
      "Episode:122 meanR:0.4796 R:0.8180 gloss:-134846.3383 dloss:57277790.9337 exploreP:0.0100\n",
      "Episode:123 meanR:0.4816 R:0.8145 gloss:-137253.4936 dloss:54780032.8361 exploreP:0.0100\n",
      "Episode:124 meanR:0.4820 R:0.8480 gloss:-139834.5555 dloss:65415047.0866 exploreP:0.0100\n",
      "Episode:125 meanR:0.4861 R:0.9150 gloss:-142542.9854 dloss:74371952.8591 exploreP:0.0100\n",
      "Episode:126 meanR:0.4911 R:0.8825 gloss:-145628.4863 dloss:146236364.3878 exploreP:0.0100\n",
      "Episode:127 meanR:0.4925 R:0.6315 gloss:-148959.8233 dloss:75776488.5482 exploreP:0.0100\n",
      "Episode:128 meanR:0.4958 R:0.7305 gloss:-151964.3373 dloss:92806619.6612 exploreP:0.0100\n",
      "Episode:129 meanR:0.4991 R:0.9080 gloss:-155441.4744 dloss:158398685.0165 exploreP:0.0100\n",
      "Episode:130 meanR:0.5034 R:0.9280 gloss:-158822.2045 dloss:119096670.7279 exploreP:0.0100\n",
      "Episode:131 meanR:0.5048 R:0.9185 gloss:-161735.8274 dloss:100351478.0903 exploreP:0.0100\n",
      "Episode:132 meanR:0.5089 R:1.0245 gloss:-164533.9136 dloss:130184193.8915 exploreP:0.0100\n",
      "Episode:133 meanR:0.5114 R:0.9045 gloss:-167446.4490 dloss:110015714.4037 exploreP:0.0100\n",
      "Episode:134 meanR:0.5180 R:0.9995 gloss:-170828.4277 dloss:151417403.5718 exploreP:0.0100\n",
      "Episode:135 meanR:0.5245 R:0.8450 gloss:-174077.8765 dloss:214012416.4003 exploreP:0.0100\n",
      "Episode:136 meanR:0.5259 R:0.7250 gloss:-176133.7526 dloss:170437155.5973 exploreP:0.0100\n",
      "Episode:137 meanR:0.5292 R:0.6430 gloss:-178473.6488 dloss:87110587.6067 exploreP:0.0100\n",
      "Episode:138 meanR:0.5287 R:0.6035 gloss:-180375.7343 dloss:127443679.7038 exploreP:0.0100\n",
      "Episode:139 meanR:0.5298 R:0.5070 gloss:-182323.2143 dloss:126556574.5027 exploreP:0.0100\n",
      "Episode:140 meanR:0.5289 R:0.3730 gloss:-184302.7682 dloss:135430274.9628 exploreP:0.0100\n",
      "Episode:141 meanR:0.5309 R:0.5180 gloss:-186047.9916 dloss:126968076.8802 exploreP:0.0100\n",
      "Episode:142 meanR:0.5335 R:0.5775 gloss:-184653.0649 dloss:174230125.1112 exploreP:0.0100\n",
      "Episode:143 meanR:0.5391 R:0.8430 gloss:-187196.8897 dloss:54311136.9341 exploreP:0.0100\n",
      "Episode:144 meanR:0.5456 R:1.2080 gloss:-189705.8371 dloss:51514411.0082 exploreP:0.0100\n",
      "Episode:145 meanR:0.5468 R:0.7590 gloss:-191928.5012 dloss:96429719.0139 exploreP:0.0100\n",
      "Episode:146 meanR:0.5488 R:0.5990 gloss:-195048.5940 dloss:90831738.0083 exploreP:0.0100\n",
      "Episode:147 meanR:0.5512 R:0.6140 gloss:-198989.9723 dloss:102378836.3577 exploreP:0.0100\n",
      "Episode:148 meanR:0.5512 R:0.3685 gloss:-202650.1291 dloss:151324472.2672 exploreP:0.0100\n",
      "Episode:149 meanR:0.5510 R:0.3740 gloss:-205352.8652 dloss:125675911.3100 exploreP:0.0100\n",
      "Episode:150 meanR:0.5492 R:0.3690 gloss:-205755.4864 dloss:129697181.5582 exploreP:0.0100\n",
      "Episode:151 meanR:0.5446 R:0.0565 gloss:-203229.6962 dloss:64525896.4856 exploreP:0.0100\n",
      "Episode:152 meanR:0.5361 R:0.0595 gloss:-205158.7234 dloss:44736332.3965 exploreP:0.0100\n",
      "Episode:153 meanR:0.5295 R:0.1865 gloss:-207875.6108 dloss:57871490.8261 exploreP:0.0100\n",
      "Episode:154 meanR:0.5227 R:0.1595 gloss:-210115.3889 dloss:50076419.9073 exploreP:0.0100\n",
      "Episode:155 meanR:0.5194 R:0.4215 gloss:-210726.1893 dloss:48308734.6098 exploreP:0.0100\n",
      "Episode:156 meanR:0.5132 R:0.1845 gloss:-209328.5736 dloss:120638563.6595 exploreP:0.0100\n",
      "Episode:157 meanR:0.5087 R:0.1595 gloss:-214003.7123 dloss:53902924.0070 exploreP:0.0100\n",
      "Episode:158 meanR:0.5040 R:0.2395 gloss:-217805.3408 dloss:39238762.4594 exploreP:0.0100\n",
      "Episode:159 meanR:0.4982 R:0.1030 gloss:-221060.2301 dloss:30813265.2640 exploreP:0.0100\n",
      "Episode:160 meanR:0.4958 R:0.2175 gloss:-223725.5290 dloss:27013315.0233 exploreP:0.0100\n",
      "Episode:161 meanR:0.4899 R:0.1015 gloss:-226083.8756 dloss:26434472.6000 exploreP:0.0100\n",
      "Episode:162 meanR:0.4861 R:0.0875 gloss:-229744.3165 dloss:37740101.5858 exploreP:0.0100\n",
      "Episode:163 meanR:0.4789 R:0.0525 gloss:-235326.5763 dloss:71018904.5917 exploreP:0.0100\n",
      "Episode:164 meanR:0.4761 R:0.1260 gloss:-239286.1411 dloss:193131667.7037 exploreP:0.0100\n",
      "Episode:165 meanR:0.4725 R:0.0710 gloss:-241514.0617 dloss:96117505.3105 exploreP:0.0100\n",
      "Episode:166 meanR:0.4672 R:0.0650 gloss:-243031.3894 dloss:102049793.7050 exploreP:0.0100\n",
      "Episode:167 meanR:0.4626 R:0.0260 gloss:-244402.5738 dloss:120563978.4122 exploreP:0.0100\n",
      "Episode:168 meanR:0.4574 R:0.0175 gloss:-245657.2102 dloss:128457751.1871 exploreP:0.0100\n",
      "Episode:169 meanR:0.4548 R:0.1550 gloss:-246431.2489 dloss:137898148.6547 exploreP:0.0100\n",
      "Episode:170 meanR:0.4506 R:0.0455 gloss:-248933.7011 dloss:131325071.3601 exploreP:0.0100\n",
      "Episode:171 meanR:0.4463 R:0.0205 gloss:-249879.7679 dloss:41558114.8125 exploreP:0.0100\n",
      "Episode:172 meanR:0.4438 R:0.1080 gloss:-251673.6016 dloss:60987101.1824 exploreP:0.0100\n",
      "Episode:173 meanR:0.4404 R:0.0515 gloss:-254438.4907 dloss:115880991.3414 exploreP:0.0100\n",
      "Episode:174 meanR:0.4413 R:0.4210 gloss:-257642.2691 dloss:136483532.8309 exploreP:0.0100\n",
      "Episode:175 meanR:0.4390 R:0.5115 gloss:-259271.0386 dloss:362608943.5762 exploreP:0.0100\n",
      "Episode:176 meanR:0.4377 R:0.4045 gloss:-262051.1604 dloss:104269295.7512 exploreP:0.0100\n",
      "Episode:177 meanR:0.4336 R:0.1250 gloss:-264324.0194 dloss:82486308.3808 exploreP:0.0100\n",
      "Episode:178 meanR:0.4337 R:0.4235 gloss:-266490.0147 dloss:97415078.2708 exploreP:0.0100\n",
      "Episode:179 meanR:0.4376 R:0.5345 gloss:-269167.7278 dloss:70540937.0037 exploreP:0.0100\n",
      "Episode:180 meanR:0.4410 R:0.4775 gloss:-271668.1945 dloss:60867714.9947 exploreP:0.0100\n",
      "Episode:181 meanR:0.4459 R:0.5380 gloss:-275164.3665 dloss:77224338.5301 exploreP:0.0100\n",
      "Episode:182 meanR:0.4505 R:0.4960 gloss:-280268.5396 dloss:106708720.1639 exploreP:0.0100\n",
      "Episode:183 meanR:0.4582 R:0.7975 gloss:-285572.7881 dloss:81385929.6344 exploreP:0.0100\n",
      "Episode:184 meanR:0.4650 R:0.7275 gloss:-289926.6811 dloss:83717546.0716 exploreP:0.0100\n",
      "Episode:185 meanR:0.4715 R:0.7020 gloss:-294937.5451 dloss:119176262.9419 exploreP:0.0100\n",
      "Episode:186 meanR:0.4774 R:0.6430 gloss:-300890.8106 dloss:188096929.6914 exploreP:0.0100\n",
      "Episode:187 meanR:0.4818 R:0.5295 gloss:-307949.0965 dloss:338046142.4936 exploreP:0.0100\n",
      "Episode:188 meanR:0.4883 R:0.7455 gloss:-313803.0631 dloss:311792436.6221 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:189 meanR:0.4934 R:0.5795 gloss:-319327.3869 dloss:340669259.5086 exploreP:0.0100\n",
      "Episode:190 meanR:0.4989 R:0.7370 gloss:-324283.4034 dloss:306638317.9220 exploreP:0.0100\n",
      "Episode:191 meanR:0.5036 R:0.6265 gloss:-329364.3074 dloss:338646465.6450 exploreP:0.0100\n",
      "Episode:192 meanR:0.5107 R:0.8270 gloss:-333521.1792 dloss:627757543.1406 exploreP:0.0100\n",
      "Episode:193 meanR:0.5102 R:0.0025 gloss:-337378.2938 dloss:412147406.4175 exploreP:0.0100\n",
      "Episode:194 meanR:0.5078 R:0.0300 gloss:-341967.0606 dloss:311665632.9567 exploreP:0.0100\n",
      "Episode:195 meanR:0.5044 R:0.0625 gloss:-346475.4691 dloss:284799681.7153 exploreP:0.0100\n",
      "Episode:196 meanR:0.5008 R:0.2670 gloss:-351838.4327 dloss:392888746.2624 exploreP:0.0100\n",
      "Episode:197 meanR:0.4988 R:0.2305 gloss:-357311.5198 dloss:328395480.8080 exploreP:0.0100\n",
      "Episode:198 meanR:0.5002 R:0.4575 gloss:-361867.5928 dloss:289337875.6333 exploreP:0.0100\n",
      "Episode:199 meanR:0.4999 R:0.1670 gloss:-367680.6648 dloss:416682480.5720 exploreP:0.0100\n",
      "Episode:200 meanR:0.4977 R:0.2560 gloss:-373268.5411 dloss:396936583.4348 exploreP:0.0100\n",
      "Episode:201 meanR:0.4987 R:0.5145 gloss:-378977.6048 dloss:417149137.3635 exploreP:0.0100\n",
      "Episode:202 meanR:0.5019 R:0.6960 gloss:-384358.5725 dloss:406164396.2551 exploreP:0.0100\n",
      "Episode:203 meanR:0.5013 R:0.2965 gloss:-389379.9010 dloss:636693902.5163 exploreP:0.0100\n",
      "Episode:204 meanR:0.4967 R:0.1995 gloss:-395763.5036 dloss:573107838.3216 exploreP:0.0100\n",
      "Episode:205 meanR:0.4947 R:0.4580 gloss:-401439.2828 dloss:560184848.4979 exploreP:0.0100\n",
      "Episode:206 meanR:0.4924 R:0.5220 gloss:-406973.9983 dloss:625821830.4825 exploreP:0.0100\n",
      "Episode:207 meanR:0.4960 R:0.7985 gloss:-412564.5114 dloss:640704541.8923 exploreP:0.0100\n",
      "Episode:208 meanR:0.4944 R:0.7320 gloss:-418097.1078 dloss:612871080.3821 exploreP:0.0100\n",
      "Episode:209 meanR:0.4991 R:1.1205 gloss:-423444.5421 dloss:636096480.8047 exploreP:0.0100\n",
      "Episode:210 meanR:0.5003 R:0.9060 gloss:-428661.8357 dloss:635489449.9237 exploreP:0.0100\n",
      "Episode:211 meanR:0.5072 R:1.3600 gloss:-433744.5279 dloss:559237262.1702 exploreP:0.0100\n",
      "Episode:212 meanR:0.5141 R:1.3080 gloss:-438663.5501 dloss:697820791.1188 exploreP:0.0100\n",
      "Episode:213 meanR:0.5193 R:1.2910 gloss:-444067.2160 dloss:637549481.7908 exploreP:0.0100\n",
      "Episode:214 meanR:0.5276 R:1.4400 gloss:-449327.2295 dloss:686965230.3458 exploreP:0.0100\n",
      "Episode:215 meanR:0.5337 R:1.3165 gloss:-454549.5331 dloss:704763725.5736 exploreP:0.0100\n",
      "Episode:216 meanR:0.5332 R:0.7115 gloss:-458616.3051 dloss:965722221.5505 exploreP:0.0100\n",
      "Episode:217 meanR:0.5292 R:0.4210 gloss:-463494.0166 dloss:981675436.8221 exploreP:0.0100\n",
      "Episode:218 meanR:0.5290 R:0.6350 gloss:-468142.7221 dloss:658474336.0374 exploreP:0.0100\n",
      "Episode:219 meanR:0.5279 R:0.5295 gloss:-472375.1159 dloss:580396022.0488 exploreP:0.0100\n",
      "Episode:220 meanR:0.5295 R:0.8305 gloss:-477888.5283 dloss:759194591.2313 exploreP:0.0100\n",
      "Episode:221 meanR:0.5312 R:0.7545 gloss:-483110.6525 dloss:1015720640.3273 exploreP:0.0100\n",
      "Episode:222 meanR:0.5298 R:0.6705 gloss:-488323.9051 dloss:826148096.5845 exploreP:0.0100\n",
      "Episode:223 meanR:0.5239 R:0.2270 gloss:-491512.9926 dloss:794851828.7678 exploreP:0.0100\n",
      "Episode:224 meanR:0.5215 R:0.6105 gloss:-494276.5042 dloss:1416075147.1866 exploreP:0.0100\n",
      "Episode:225 meanR:0.5134 R:0.1025 gloss:-499779.1693 dloss:864111118.7972 exploreP:0.0100\n",
      "Episode:226 meanR:0.5099 R:0.5285 gloss:-505300.4580 dloss:1377866269.0077 exploreP:0.0100\n",
      "Episode:227 meanR:0.5105 R:0.6990 gloss:-511532.9234 dloss:1104236483.2303 exploreP:0.0100\n",
      "Episode:228 meanR:0.5088 R:0.5620 gloss:-519153.2420 dloss:1685294900.0761 exploreP:0.0100\n",
      "Episode:229 meanR:0.5076 R:0.7875 gloss:-527461.6357 dloss:2107050283.3391 exploreP:0.0100\n",
      "Episode:230 meanR:0.5036 R:0.5260 gloss:-535169.9922 dloss:2100420472.3677 exploreP:0.0100\n",
      "Episode:231 meanR:0.4992 R:0.4735 gloss:-541873.3852 dloss:2073758329.1255 exploreP:0.0100\n",
      "Episode:232 meanR:0.4951 R:0.6160 gloss:-548400.9759 dloss:1964514094.0174 exploreP:0.0100\n",
      "Episode:233 meanR:0.4919 R:0.5865 gloss:-554613.9704 dloss:1888143462.9442 exploreP:0.0100\n",
      "Episode:234 meanR:0.4886 R:0.6675 gloss:-560673.6821 dloss:2196976556.6673 exploreP:0.0100\n",
      "Episode:235 meanR:0.4856 R:0.5435 gloss:-566891.5941 dloss:1890722168.7931 exploreP:0.0100\n",
      "Episode:236 meanR:0.4821 R:0.3820 gloss:-573241.4754 dloss:1980744868.4853 exploreP:0.0100\n",
      "Episode:237 meanR:0.4802 R:0.4510 gloss:-579887.7943 dloss:3324865204.5211 exploreP:0.0100\n",
      "Episode:238 meanR:0.4773 R:0.3075 gloss:-587151.6974 dloss:2334051163.9637 exploreP:0.0100\n",
      "Episode:239 meanR:0.4759 R:0.3665 gloss:-592896.7548 dloss:3288360513.4383 exploreP:0.0100\n",
      "Episode:240 meanR:0.4754 R:0.3240 gloss:-599926.0014 dloss:2762620586.9457 exploreP:0.0100\n",
      "Episode:241 meanR:0.4729 R:0.2675 gloss:-606094.9228 dloss:2855428703.2010 exploreP:0.0100\n",
      "Episode:242 meanR:0.4705 R:0.3365 gloss:-598663.9737 dloss:9729536281.3543 exploreP:0.0100\n",
      "Episode:243 meanR:0.4677 R:0.5675 gloss:-596061.0662 dloss:2106572685.5427 exploreP:0.0100\n",
      "Episode:244 meanR:0.4609 R:0.5260 gloss:-599472.7842 dloss:1287291518.6972 exploreP:0.0100\n",
      "Episode:245 meanR:0.4596 R:0.6345 gloss:-609564.2145 dloss:1816038959.8100 exploreP:0.0100\n",
      "Episode:246 meanR:0.4590 R:0.5350 gloss:-622167.8468 dloss:3891199640.7035 exploreP:0.0100\n",
      "Episode:247 meanR:0.4595 R:0.6695 gloss:-633455.5425 dloss:3740692069.3322 exploreP:0.0100\n",
      "Episode:248 meanR:0.4614 R:0.5565 gloss:-640707.1939 dloss:4066165409.7798 exploreP:0.0100\n",
      "Episode:249 meanR:0.4650 R:0.7350 gloss:-646705.3051 dloss:7158418162.5619 exploreP:0.0100\n",
      "Episode:250 meanR:0.4673 R:0.5960 gloss:-654051.3250 dloss:3121778051.2167 exploreP:0.0100\n",
      "Episode:251 meanR:0.4699 R:0.3140 gloss:-660602.6094 dloss:3840019313.8461 exploreP:0.0100\n",
      "Episode:252 meanR:0.4729 R:0.3650 gloss:-668774.7118 dloss:4309003162.0671 exploreP:0.0100\n",
      "Episode:253 meanR:0.4744 R:0.3285 gloss:-676156.9416 dloss:4354803097.5148 exploreP:0.0100\n",
      "Episode:254 meanR:0.4753 R:0.2540 gloss:-683490.4883 dloss:4471197080.1191 exploreP:0.0100\n",
      "Episode:255 meanR:0.4733 R:0.2255 gloss:-675308.8917 dloss:8392302636.4154 exploreP:0.0100\n",
      "Episode:256 meanR:0.4758 R:0.4325 gloss:-670514.8998 dloss:3048018394.2024 exploreP:0.0100\n",
      "Episode:257 meanR:0.4789 R:0.4630 gloss:-668632.7411 dloss:3487932310.7910 exploreP:0.0100\n",
      "Episode:258 meanR:0.4829 R:0.6390 gloss:-665852.5779 dloss:2952549632.7660 exploreP:0.0100\n",
      "Episode:259 meanR:0.4879 R:0.6090 gloss:-669633.8785 dloss:2795957674.4357 exploreP:0.0100\n",
      "Episode:260 meanR:0.4918 R:0.6020 gloss:-683554.1985 dloss:2283006202.0292 exploreP:0.0100\n",
      "Episode:261 meanR:0.4964 R:0.5700 gloss:-694780.9106 dloss:2803780394.5445 exploreP:0.0100\n",
      "Episode:262 meanR:0.5029 R:0.7295 gloss:-703933.4126 dloss:3415866656.4904 exploreP:0.0100\n",
      "Episode:263 meanR:0.5076 R:0.5240 gloss:-716066.4174 dloss:6811395298.4609 exploreP:0.0100\n",
      "Episode:264 meanR:0.5108 R:0.4505 gloss:-726253.3238 dloss:8135747210.4554 exploreP:0.0100\n",
      "Episode:265 meanR:0.5143 R:0.4215 gloss:-735817.6571 dloss:6812594754.7140 exploreP:0.0100\n",
      "Episode:266 meanR:0.5202 R:0.6480 gloss:-743592.8318 dloss:7190816244.5931 exploreP:0.0100\n",
      "Episode:267 meanR:0.5260 R:0.6085 gloss:-739723.3618 dloss:6109380790.2178 exploreP:0.0100\n",
      "Episode:268 meanR:0.5342 R:0.8365 gloss:-744906.7950 dloss:5389684728.2051 exploreP:0.0100\n",
      "Episode:269 meanR:0.5403 R:0.7715 gloss:-758376.3002 dloss:7080269921.5903 exploreP:0.0100\n",
      "Episode:270 meanR:0.5478 R:0.7895 gloss:-768329.8959 dloss:5083913836.9610 exploreP:0.0100\n",
      "Episode:271 meanR:0.5554 R:0.7825 gloss:-772521.9271 dloss:2878846981.9131 exploreP:0.0100\n",
      "Episode:272 meanR:0.5613 R:0.7030 gloss:-777525.3259 dloss:3561216183.7947 exploreP:0.0100\n",
      "Episode:273 meanR:0.5683 R:0.7490 gloss:-782444.9243 dloss:2947538370.2212 exploreP:0.0100\n",
      "Episode:274 meanR:0.5720 R:0.7900 gloss:-787173.5088 dloss:2871792827.6776 exploreP:0.0100\n",
      "Episode:275 meanR:0.5735 R:0.6570 gloss:-792044.0075 dloss:2506726689.7509 exploreP:0.0100\n",
      "Episode:276 meanR:0.5766 R:0.7210 gloss:-798834.9790 dloss:3215960626.6794 exploreP:0.0100\n",
      "Episode:277 meanR:0.5817 R:0.6365 gloss:-804421.0229 dloss:2944398119.3096 exploreP:0.0100\n",
      "Episode:278 meanR:0.5849 R:0.7420 gloss:-810501.8866 dloss:2785664573.0977 exploreP:0.0100\n",
      "Episode:279 meanR:0.5870 R:0.7410 gloss:-815710.2929 dloss:3793027988.3721 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:280 meanR:0.5899 R:0.7685 gloss:-821391.9729 dloss:3967475146.8976 exploreP:0.0100\n",
      "Episode:281 meanR:0.5914 R:0.6900 gloss:-828508.3870 dloss:14702679108.6426 exploreP:0.0100\n",
      "Episode:282 meanR:0.5919 R:0.5445 gloss:-838630.3023 dloss:11516211999.7733 exploreP:0.0100\n",
      "Episode:283 meanR:0.5896 R:0.5680 gloss:-850592.3162 dloss:8305679899.3622 exploreP:0.0100\n",
      "Episode:284 meanR:0.5880 R:0.5655 gloss:-856588.7137 dloss:4740198529.7051 exploreP:0.0100\n",
      "Episode:285 meanR:0.5872 R:0.6260 gloss:-860579.5778 dloss:3458903552.0471 exploreP:0.0100\n",
      "Episode:286 meanR:0.5872 R:0.6405 gloss:-864991.6078 dloss:3308780543.6877 exploreP:0.0100\n",
      "Episode:287 meanR:0.5881 R:0.6140 gloss:-871792.5292 dloss:5535894492.4381 exploreP:0.0100\n",
      "Episode:288 meanR:0.5869 R:0.6300 gloss:-879683.1675 dloss:5537805928.2806 exploreP:0.0100\n",
      "Episode:289 meanR:0.5856 R:0.4540 gloss:-887294.6776 dloss:5244962344.3842 exploreP:0.0100\n",
      "Episode:290 meanR:0.5834 R:0.5120 gloss:-894245.4271 dloss:7014926296.4185 exploreP:0.0100\n",
      "Episode:291 meanR:0.5828 R:0.5670 gloss:-900809.0557 dloss:5259655223.6522 exploreP:0.0100\n",
      "Episode:292 meanR:0.5811 R:0.6595 gloss:-906920.4657 dloss:5239984133.2025 exploreP:0.0100\n",
      "Episode:293 meanR:0.5872 R:0.6115 gloss:-913672.1933 dloss:5492566064.0798 exploreP:0.0100\n",
      "Episode:294 meanR:0.5929 R:0.5950 gloss:-923318.6162 dloss:8908823849.1985 exploreP:0.0100\n",
      "Episode:295 meanR:0.5981 R:0.5880 gloss:-932749.6433 dloss:7292986122.4074 exploreP:0.0100\n",
      "Episode:296 meanR:0.6006 R:0.5155 gloss:-939241.6581 dloss:8817260699.7422 exploreP:0.0100\n",
      "Episode:297 meanR:0.6032 R:0.4925 gloss:-940899.7430 dloss:10983021837.4376 exploreP:0.0100\n",
      "Episode:298 meanR:0.6065 R:0.7835 gloss:-955059.4329 dloss:9798224153.0660 exploreP:0.0100\n",
      "Episode:299 meanR:0.6116 R:0.6755 gloss:-960953.3173 dloss:7498413972.4252 exploreP:0.0100\n",
      "Episode:300 meanR:0.6164 R:0.7415 gloss:-966336.0536 dloss:5897803035.4068 exploreP:0.0100\n",
      "Episode:301 meanR:0.6193 R:0.8050 gloss:-971826.6962 dloss:5195249334.4600 exploreP:0.0100\n",
      "Episode:302 meanR:0.6201 R:0.7765 gloss:-976906.6491 dloss:5060665837.2047 exploreP:0.0100\n",
      "Episode:303 meanR:0.6249 R:0.7755 gloss:-982720.7310 dloss:4715373434.1390 exploreP:0.0100\n",
      "Episode:304 meanR:0.6288 R:0.5905 gloss:-988572.5315 dloss:4424950548.9861 exploreP:0.0100\n",
      "Episode:305 meanR:0.6315 R:0.7270 gloss:-995015.2071 dloss:5498229073.2709 exploreP:0.0100\n",
      "Episode:306 meanR:0.6341 R:0.7750 gloss:-1002062.0094 dloss:4592690473.3398 exploreP:0.0100\n",
      "Episode:307 meanR:0.6329 R:0.6860 gloss:-1009389.6308 dloss:5074680032.4175 exploreP:0.0100\n",
      "Episode:308 meanR:0.6311 R:0.5490 gloss:-1017480.5321 dloss:5714130928.0551 exploreP:0.0100\n",
      "Episode:309 meanR:0.6259 R:0.6050 gloss:-1025268.8355 dloss:4665966985.9248 exploreP:0.0100\n",
      "Episode:310 meanR:0.6245 R:0.7570 gloss:-1031810.1650 dloss:5426093790.0947 exploreP:0.0100\n",
      "Episode:311 meanR:0.6173 R:0.6475 gloss:-1038647.2354 dloss:5559245679.4670 exploreP:0.0100\n",
      "Episode:312 meanR:0.6130 R:0.8790 gloss:-1046641.3477 dloss:11618689951.6705 exploreP:0.0100\n",
      "Episode:313 meanR:0.6083 R:0.8180 gloss:-1055470.4101 dloss:7888168871.0887 exploreP:0.0100\n",
      "Episode:314 meanR:0.6006 R:0.6665 gloss:-1064302.3152 dloss:12146991151.7099 exploreP:0.0100\n",
      "Episode:315 meanR:0.5955 R:0.8040 gloss:-1074199.8851 dloss:12567376575.7755 exploreP:0.0100\n",
      "Episode:316 meanR:0.5957 R:0.7380 gloss:-1084287.5315 dloss:10366419402.5247 exploreP:0.0100\n",
      "Episode:317 meanR:0.5986 R:0.7110 gloss:-1090731.3003 dloss:19165846351.1447 exploreP:0.0100\n",
      "Episode:318 meanR:0.6008 R:0.8520 gloss:-1097328.9467 dloss:13001869042.7904 exploreP:0.0100\n",
      "Episode:319 meanR:0.6022 R:0.6750 gloss:-1103620.1168 dloss:7666057661.2445 exploreP:0.0100\n",
      "Episode:320 meanR:0.6005 R:0.6585 gloss:-1111058.5996 dloss:9145941463.0693 exploreP:0.0100\n",
      "Episode:321 meanR:0.5999 R:0.6970 gloss:-1117176.5324 dloss:7547136698.4031 exploreP:0.0100\n",
      "Episode:322 meanR:0.6013 R:0.8045 gloss:-1123926.5820 dloss:10631331834.9898 exploreP:0.0100\n",
      "Episode:323 meanR:0.6065 R:0.7460 gloss:-1131766.2113 dloss:8262882035.9870 exploreP:0.0100\n",
      "Episode:324 meanR:0.6063 R:0.5930 gloss:-1139292.8058 dloss:9122839059.6102 exploreP:0.0100\n",
      "Episode:325 meanR:0.6128 R:0.7485 gloss:-1138430.4278 dloss:26084793228.7477 exploreP:0.0100\n",
      "Episode:326 meanR:0.6147 R:0.7200 gloss:-1153027.0990 dloss:15368862743.5135 exploreP:0.0100\n",
      "Episode:327 meanR:0.6157 R:0.7975 gloss:-1163475.7081 dloss:6323079943.4747 exploreP:0.0100\n",
      "Episode:328 meanR:0.6177 R:0.7665 gloss:-1170582.3445 dloss:5048802596.7391 exploreP:0.0100\n",
      "Episode:329 meanR:0.6147 R:0.4905 gloss:-1177436.5167 dloss:5627000320.3379 exploreP:0.0100\n",
      "Episode:330 meanR:0.6131 R:0.3625 gloss:-1184723.3633 dloss:5373631161.2706 exploreP:0.0100\n",
      "Episode:331 meanR:0.6131 R:0.4700 gloss:-1192387.2848 dloss:5091513535.2307 exploreP:0.0100\n",
      "Episode:332 meanR:0.6111 R:0.4215 gloss:-1200128.5012 dloss:4549857520.4636 exploreP:0.0100\n",
      "Episode:333 meanR:0.6095 R:0.4220 gloss:-1207771.5798 dloss:4266561194.2967 exploreP:0.0100\n",
      "Episode:334 meanR:0.6068 R:0.4035 gloss:-1215809.2083 dloss:4470892239.9384 exploreP:0.0100\n",
      "Episode:335 meanR:0.6060 R:0.4555 gloss:-1224139.9167 dloss:4914924359.6897 exploreP:0.0100\n",
      "Episode:336 meanR:0.6072 R:0.5080 gloss:-1232585.7822 dloss:4738541583.2387 exploreP:0.0100\n",
      "Episode:337 meanR:0.6066 R:0.3920 gloss:-1240183.2188 dloss:4873854317.0972 exploreP:0.0100\n",
      "Episode:338 meanR:0.6078 R:0.4240 gloss:-1235164.0217 dloss:14000774624.5651 exploreP:0.0100\n",
      "Episode:339 meanR:0.6093 R:0.5220 gloss:-1238819.2843 dloss:6032605780.8807 exploreP:0.0100\n",
      "Episode:340 meanR:0.6108 R:0.4695 gloss:-1255756.6955 dloss:7038135485.6959 exploreP:0.0100\n",
      "Episode:341 meanR:0.6148 R:0.6650 gloss:-1272938.9729 dloss:11946505403.2705 exploreP:0.0100\n",
      "Episode:342 meanR:0.6162 R:0.4760 gloss:-1288435.1467 dloss:8851267369.9815 exploreP:0.0100\n",
      "Episode:343 meanR:0.6144 R:0.3855 gloss:-1298287.9259 dloss:10831897766.1278 exploreP:0.0100\n",
      "Episode:344 meanR:0.6130 R:0.3900 gloss:-1307655.5002 dloss:10576867719.3565 exploreP:0.0100\n",
      "Episode:345 meanR:0.6128 R:0.6180 gloss:-1317176.2750 dloss:10222197922.9333 exploreP:0.0100\n",
      "Episode:346 meanR:0.6105 R:0.3035 gloss:-1326778.6201 dloss:10239751241.6427 exploreP:0.0100\n",
      "Episode:347 meanR:0.6090 R:0.5140 gloss:-1336208.9872 dloss:10247848259.3305 exploreP:0.0100\n",
      "Episode:348 meanR:0.6090 R:0.5625 gloss:-1345644.3772 dloss:10171053303.3527 exploreP:0.0100\n",
      "Episode:349 meanR:0.6078 R:0.6160 gloss:-1355714.2465 dloss:11313965800.7360 exploreP:0.0100\n",
      "Episode:350 meanR:0.6067 R:0.4830 gloss:-1366136.2828 dloss:12878984373.5361 exploreP:0.0100\n",
      "Episode:351 meanR:0.6097 R:0.6165 gloss:-1376768.4382 dloss:16343030758.9453 exploreP:0.0100\n",
      "Episode:352 meanR:0.6111 R:0.5005 gloss:-1385272.5836 dloss:13595208763.6089 exploreP:0.0100\n",
      "Episode:353 meanR:0.6131 R:0.5315 gloss:-1391495.1596 dloss:15094740801.6985 exploreP:0.0100\n",
      "Episode:354 meanR:0.6157 R:0.5150 gloss:-1399309.5389 dloss:16481967902.0525 exploreP:0.0100\n",
      "Episode:355 meanR:0.6179 R:0.4425 gloss:-1405854.0392 dloss:13186715345.8616 exploreP:0.0100\n",
      "Episode:356 meanR:0.6200 R:0.6430 gloss:-1413451.6000 dloss:22310699410.8306 exploreP:0.0100\n",
      "Episode:357 meanR:0.6198 R:0.4410 gloss:-1420505.6037 dloss:28616277085.4637 exploreP:0.0100\n",
      "Episode:358 meanR:0.6186 R:0.5200 gloss:-1433132.5815 dloss:17459684888.8412 exploreP:0.0100\n",
      "Episode:359 meanR:0.6200 R:0.7555 gloss:-1443202.4448 dloss:20383440380.2219 exploreP:0.0100\n",
      "Episode:360 meanR:0.6197 R:0.5680 gloss:-1451593.8189 dloss:18623900968.9383 exploreP:0.0100\n",
      "Episode:361 meanR:0.6183 R:0.4340 gloss:-1457766.4139 dloss:13841313633.7901 exploreP:0.0100\n",
      "Episode:362 meanR:0.6170 R:0.5995 gloss:-1463068.9039 dloss:16162504479.2015 exploreP:0.0100\n",
      "Episode:363 meanR:0.6167 R:0.4920 gloss:-1470382.0727 dloss:13124708526.5874 exploreP:0.0100\n",
      "Episode:364 meanR:0.6202 R:0.7965 gloss:-1481272.6836 dloss:13477939948.5044 exploreP:0.0100\n",
      "Episode:365 meanR:0.6246 R:0.8595 gloss:-1490866.5508 dloss:14587598596.3041 exploreP:0.0100\n",
      "Episode:366 meanR:0.6253 R:0.7225 gloss:-1500054.5444 dloss:17116827156.7474 exploreP:0.0100\n",
      "Episode:367 meanR:0.6265 R:0.7310 gloss:-1508956.2014 dloss:12956335702.4557 exploreP:0.0100\n",
      "Episode:368 meanR:0.6245 R:0.6325 gloss:-1516978.1291 dloss:16807533529.2178 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:369 meanR:0.6232 R:0.6465 gloss:-1525649.9679 dloss:16230826574.8062 exploreP:0.0100\n",
      "Episode:370 meanR:0.6233 R:0.7945 gloss:-1532006.0006 dloss:7803451116.8195 exploreP:0.0100\n",
      "Episode:371 meanR:0.6215 R:0.6035 gloss:-1544339.8702 dloss:26405680879.7779 exploreP:0.0100\n",
      "Episode:372 meanR:0.6235 R:0.9020 gloss:-1546512.8597 dloss:25466724190.7760 exploreP:0.0100\n",
      "Episode:373 meanR:0.6254 R:0.9435 gloss:-1548416.3019 dloss:49966825502.0476 exploreP:0.0100\n",
      "Episode:374 meanR:0.6250 R:0.7410 gloss:-1567777.8128 dloss:29946006620.8842 exploreP:0.0100\n",
      "Episode:375 meanR:0.6282 R:0.9865 gloss:-1571163.8853 dloss:13194457748.7920 exploreP:0.0100\n",
      "Episode:376 meanR:0.6283 R:0.7220 gloss:-1583237.8199 dloss:38739446166.2399 exploreP:0.0100\n",
      "Episode:377 meanR:0.6319 R:1.0025 gloss:-1586270.4758 dloss:30523554685.7764 exploreP:0.0100\n",
      "Episode:378 meanR:0.6326 R:0.8055 gloss:-1602385.9448 dloss:29979030983.3085 exploreP:0.0100\n",
      "Episode:379 meanR:0.6336 R:0.8420 gloss:-1608621.8181 dloss:40688453970.3546 exploreP:0.0100\n",
      "Episode:380 meanR:0.6329 R:0.7050 gloss:-1619183.8414 dloss:26313537309.4659 exploreP:0.0100\n",
      "Episode:381 meanR:0.6332 R:0.7150 gloss:-1628301.9490 dloss:20357705068.1342 exploreP:0.0100\n",
      "Episode:382 meanR:0.6354 R:0.7695 gloss:-1636129.2905 dloss:17842924923.8494 exploreP:0.0100\n",
      "Episode:383 meanR:0.6374 R:0.7650 gloss:-1644992.6361 dloss:18308764925.3062 exploreP:0.0100\n",
      "Episode:384 meanR:0.6378 R:0.6105 gloss:-1655538.9691 dloss:22416842681.1874 exploreP:0.0100\n",
      "Episode:385 meanR:0.6376 R:0.6020 gloss:-1665426.6117 dloss:19529448162.4305 exploreP:0.0100\n",
      "Episode:386 meanR:0.6367 R:0.5535 gloss:-1675748.5079 dloss:30495129752.9893 exploreP:0.0100\n",
      "Episode:387 meanR:0.6384 R:0.7815 gloss:-1685736.9283 dloss:27664503348.6395 exploreP:0.0100\n",
      "Episode:388 meanR:0.6383 R:0.6230 gloss:-1695599.0771 dloss:22740603995.7358 exploreP:0.0100\n",
      "Episode:389 meanR:0.6441 R:1.0285 gloss:-1705090.3940 dloss:20368547923.2621 exploreP:0.0100\n",
      "Episode:390 meanR:0.6485 R:0.9485 gloss:-1715143.5840 dloss:19592525181.5180 exploreP:0.0100\n",
      "Episode:391 meanR:0.6497 R:0.6870 gloss:-1724566.5619 dloss:18881309747.7705 exploreP:0.0100\n",
      "Episode:392 meanR:0.6513 R:0.8285 gloss:-1734090.0154 dloss:17844206941.1185 exploreP:0.0100\n",
      "Episode:393 meanR:0.6536 R:0.8385 gloss:-1742692.3690 dloss:17936639581.5030 exploreP:0.0100\n",
      "Episode:394 meanR:0.6562 R:0.8585 gloss:-1750794.2180 dloss:17717294639.5226 exploreP:0.0100\n",
      "Episode:395 meanR:0.6572 R:0.6800 gloss:-1758268.9721 dloss:15654273619.7895 exploreP:0.0100\n",
      "Episode:396 meanR:0.6587 R:0.6655 gloss:-1765523.6922 dloss:14724434687.5715 exploreP:0.0100\n",
      "Episode:397 meanR:0.6604 R:0.6655 gloss:-1777134.3077 dloss:15091849898.7810 exploreP:0.0100\n",
      "Episode:398 meanR:0.6617 R:0.9135 gloss:-1783481.8999 dloss:15998037695.3674 exploreP:0.0100\n",
      "Episode:399 meanR:0.6638 R:0.8815 gloss:-1801426.8777 dloss:22936430483.2209 exploreP:0.0100\n",
      "Episode:400 meanR:0.6677 R:1.1360 gloss:-1814937.1300 dloss:14188840127.7990 exploreP:0.0100\n",
      "Episode:401 meanR:0.6706 R:1.0975 gloss:-1828896.6732 dloss:24661230289.2971 exploreP:0.0100\n",
      "Episode:402 meanR:0.6720 R:0.9105 gloss:-1842547.9994 dloss:26282564163.6907 exploreP:0.0100\n",
      "Episode:403 meanR:0.6721 R:0.7860 gloss:-1857326.5716 dloss:26067960109.3056 exploreP:0.0100\n",
      "Episode:404 meanR:0.6740 R:0.7825 gloss:-1868201.9018 dloss:24563328065.2642 exploreP:0.0100\n",
      "Episode:405 meanR:0.6725 R:0.5800 gloss:-1878312.6681 dloss:26278667895.2640 exploreP:0.0100\n",
      "Episode:406 meanR:0.6730 R:0.8240 gloss:-1886515.4372 dloss:22612695570.4585 exploreP:0.0100\n",
      "Episode:407 meanR:0.6728 R:0.6690 gloss:-1895677.7868 dloss:23490648625.8149 exploreP:0.0100\n",
      "Episode:408 meanR:0.6738 R:0.6480 gloss:-1905724.2507 dloss:23349975281.7320 exploreP:0.0100\n",
      "Episode:409 meanR:0.6728 R:0.4990 gloss:-1916921.0622 dloss:24260147103.4497 exploreP:0.0100\n",
      "Episode:410 meanR:0.6704 R:0.5225 gloss:-1928542.3996 dloss:24659044860.5839 exploreP:0.0100\n",
      "Episode:411 meanR:0.6694 R:0.5435 gloss:-1940344.6758 dloss:24700608792.8107 exploreP:0.0100\n",
      "Episode:412 meanR:0.6669 R:0.6295 gloss:-1952009.9281 dloss:23985426086.3939 exploreP:0.0100\n",
      "Episode:413 meanR:0.6637 R:0.4955 gloss:-1963512.4671 dloss:24604484565.2730 exploreP:0.0100\n",
      "Episode:414 meanR:0.6640 R:0.7045 gloss:-1975066.6510 dloss:26742416681.9780 exploreP:0.0100\n",
      "Episode:415 meanR:0.6621 R:0.6110 gloss:-1986455.9954 dloss:25154347367.7369 exploreP:0.0100\n",
      "Episode:416 meanR:0.6609 R:0.6175 gloss:-1998369.7640 dloss:26532074011.6073 exploreP:0.0100\n",
      "Episode:417 meanR:0.6607 R:0.6900 gloss:-2009182.2380 dloss:24173618754.8844 exploreP:0.0100\n",
      "Episode:418 meanR:0.6563 R:0.4070 gloss:-2019897.9679 dloss:24042121760.7649 exploreP:0.0100\n",
      "Episode:419 meanR:0.6554 R:0.5930 gloss:-2032275.6571 dloss:24954748564.4414 exploreP:0.0100\n",
      "Episode:420 meanR:0.6564 R:0.7535 gloss:-2044107.9562 dloss:26622928412.8648 exploreP:0.0100\n",
      "Episode:421 meanR:0.6555 R:0.6095 gloss:-2056999.0002 dloss:28848202438.0370 exploreP:0.0100\n",
      "Episode:422 meanR:0.6536 R:0.6095 gloss:-2068228.8327 dloss:23425297793.3078 exploreP:0.0100\n",
      "Episode:423 meanR:0.6515 R:0.5360 gloss:-2079635.3388 dloss:21387066889.2872 exploreP:0.0100\n",
      "Episode:424 meanR:0.6516 R:0.6105 gloss:-2090757.7548 dloss:22229030106.9028 exploreP:0.0100\n",
      "Episode:425 meanR:0.6489 R:0.4730 gloss:-2102400.7264 dloss:20751808598.7767 exploreP:0.0100\n",
      "Episode:426 meanR:0.6489 R:0.7230 gloss:-2109180.8959 dloss:20846625590.9287 exploreP:0.0100\n",
      "Episode:427 meanR:0.6509 R:0.9985 gloss:-2119466.2204 dloss:27203589053.1852 exploreP:0.0100\n",
      "Episode:428 meanR:0.6531 R:0.9810 gloss:-2130853.7565 dloss:23984130703.7434 exploreP:0.0100\n",
      "Episode:429 meanR:0.6565 R:0.8355 gloss:-2147651.4987 dloss:32851933253.5596 exploreP:0.0100\n",
      "Episode:430 meanR:0.6616 R:0.8675 gloss:-2164067.9676 dloss:33800753821.2305 exploreP:0.0100\n",
      "Episode:431 meanR:0.6656 R:0.8730 gloss:-2178476.2984 dloss:30206029736.2794 exploreP:0.0100\n",
      "Episode:432 meanR:0.6699 R:0.8490 gloss:-2192199.6434 dloss:35325265514.6970 exploreP:0.0100\n",
      "Episode:433 meanR:0.6766 R:1.0940 gloss:-2204482.5263 dloss:29655828874.6791 exploreP:0.0100\n",
      "Episode:434 meanR:0.6824 R:0.9890 gloss:-2217621.0181 dloss:32300095832.9321 exploreP:0.0100\n",
      "Episode:435 meanR:0.6883 R:1.0385 gloss:-2231191.0786 dloss:32249304497.8652 exploreP:0.0100\n",
      "Episode:436 meanR:0.6917 R:0.8550 gloss:-2243766.7565 dloss:28052286228.4096 exploreP:0.0100\n",
      "Episode:437 meanR:0.6976 R:0.9775 gloss:-2256264.7520 dloss:26172479458.3653 exploreP:0.0100\n",
      "Episode:438 meanR:0.7028 R:0.9465 gloss:-2268439.7383 dloss:29243613586.4575 exploreP:0.0100\n",
      "Episode:439 meanR:0.7055 R:0.7945 gloss:-2282188.7358 dloss:35502322379.3765 exploreP:0.0100\n",
      "Episode:440 meanR:0.7086 R:0.7735 gloss:-2295923.3885 dloss:30528955759.1470 exploreP:0.0100\n",
      "Episode:441 meanR:0.7081 R:0.6175 gloss:-2308262.4858 dloss:32740772594.2901 exploreP:0.0100\n",
      "Episode:442 meanR:0.7093 R:0.5945 gloss:-2322038.9305 dloss:34519583959.5649 exploreP:0.0100\n",
      "Episode:443 meanR:0.7110 R:0.5590 gloss:-2335337.4161 dloss:33852946034.8491 exploreP:0.0100\n",
      "Episode:444 meanR:0.7148 R:0.7695 gloss:-2348666.4130 dloss:33479727066.2408 exploreP:0.0100\n",
      "Episode:445 meanR:0.7153 R:0.6625 gloss:-2361847.1441 dloss:33330046362.5038 exploreP:0.0100\n",
      "Episode:446 meanR:0.7186 R:0.6385 gloss:-2375162.0984 dloss:35272038704.9765 exploreP:0.0100\n",
      "Episode:447 meanR:0.7211 R:0.7585 gloss:-2388389.0397 dloss:38249945418.0031 exploreP:0.0100\n",
      "Episode:448 meanR:0.7231 R:0.7665 gloss:-2402157.7987 dloss:36712911415.7948 exploreP:0.0100\n",
      "Episode:449 meanR:0.7235 R:0.6590 gloss:-2415981.3168 dloss:38292159826.7351 exploreP:0.0100\n",
      "Episode:450 meanR:0.7246 R:0.5895 gloss:-2429415.2444 dloss:38412640830.8271 exploreP:0.0100\n",
      "Episode:451 meanR:0.7254 R:0.6995 gloss:-2443072.2132 dloss:39581374903.9365 exploreP:0.0100\n",
      "Episode:452 meanR:0.7255 R:0.5070 gloss:-2456027.6914 dloss:43544145617.0087 exploreP:0.0100\n",
      "Episode:453 meanR:0.7278 R:0.7610 gloss:-2470806.4371 dloss:44410808488.2836 exploreP:0.0100\n",
      "Episode:454 meanR:0.7317 R:0.9070 gloss:-2484249.5970 dloss:48471186648.6989 exploreP:0.0100\n",
      "Episode:455 meanR:0.7357 R:0.8375 gloss:-2498480.6382 dloss:48528526820.2066 exploreP:0.0100\n",
      "Episode:456 meanR:0.7354 R:0.6180 gloss:-2513160.5633 dloss:51519543849.8934 exploreP:0.0100\n",
      "Episode:457 meanR:0.7365 R:0.5470 gloss:-2526247.2270 dloss:56941440733.6889 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:458 meanR:0.7377 R:0.6400 gloss:-2536047.3049 dloss:50365419514.4309 exploreP:0.0100\n",
      "Episode:459 meanR:0.7359 R:0.5820 gloss:-2552347.7616 dloss:54088634484.2701 exploreP:0.0100\n",
      "Episode:460 meanR:0.7378 R:0.7560 gloss:-2565044.9501 dloss:56591880646.5340 exploreP:0.0100\n",
      "Episode:461 meanR:0.7415 R:0.8070 gloss:-2579897.0709 dloss:53350951052.2300 exploreP:0.0100\n",
      "Episode:462 meanR:0.7450 R:0.9450 gloss:-2592882.6392 dloss:59438227123.8391 exploreP:0.0100\n",
      "Episode:463 meanR:0.7480 R:0.7960 gloss:-2604817.9438 dloss:63225206248.6238 exploreP:0.0100\n",
      "Episode:464 meanR:0.7480 R:0.7920 gloss:-2619030.9401 dloss:63447941482.1953 exploreP:0.0100\n",
      "Episode:465 meanR:0.7477 R:0.8285 gloss:-2631186.0258 dloss:78066401154.7236 exploreP:0.0100\n",
      "Episode:466 meanR:0.7492 R:0.8775 gloss:-2646100.9929 dloss:79212376312.1178 exploreP:0.0100\n",
      "Episode:467 meanR:0.7502 R:0.8225 gloss:-2659634.6557 dloss:59590699961.8780 exploreP:0.0100\n",
      "Episode:468 meanR:0.7513 R:0.7505 gloss:-2669670.6678 dloss:70047059940.1959 exploreP:0.0100\n",
      "Episode:469 meanR:0.7500 R:0.5160 gloss:-2683253.0830 dloss:73858443841.3881 exploreP:0.0100\n",
      "Episode:470 meanR:0.7478 R:0.5670 gloss:-2698490.3092 dloss:70835667743.3302 exploreP:0.0100\n",
      "Episode:471 meanR:0.7514 R:0.9660 gloss:-2708007.6359 dloss:70925625921.6509 exploreP:0.0100\n",
      "Episode:472 meanR:0.7497 R:0.7385 gloss:-2720916.5931 dloss:74868523818.6706 exploreP:0.0100\n",
      "Episode:473 meanR:0.7478 R:0.7480 gloss:-2724908.7993 dloss:72490208106.9239 exploreP:0.0100\n",
      "Episode:474 meanR:0.7451 R:0.4760 gloss:-2747069.0124 dloss:73942629983.4822 exploreP:0.0100\n",
      "Episode:475 meanR:0.7405 R:0.5210 gloss:-2762636.7330 dloss:61020133115.1250 exploreP:0.0100\n",
      "Episode:476 meanR:0.7389 R:0.5630 gloss:-2754820.2239 dloss:93475835658.6646 exploreP:0.0100\n",
      "Episode:477 meanR:0.7357 R:0.6790 gloss:-2765268.6988 dloss:120551249783.8485 exploreP:0.0100\n",
      "Episode:478 meanR:0.7332 R:0.5635 gloss:-2788039.4052 dloss:32216207971.1550 exploreP:0.0100\n",
      "Episode:479 meanR:0.7302 R:0.5375 gloss:-2798756.9521 dloss:34615340016.5689 exploreP:0.0100\n",
      "Episode:480 meanR:0.7272 R:0.4085 gloss:-2812131.7167 dloss:49549995631.8507 exploreP:0.0100\n",
      "Episode:481 meanR:0.7261 R:0.6035 gloss:-2827936.1840 dloss:42535682778.4444 exploreP:0.0100\n",
      "Episode:482 meanR:0.7263 R:0.7870 gloss:-2839577.4200 dloss:51749396083.5811 exploreP:0.0100\n",
      "Episode:483 meanR:0.7278 R:0.9155 gloss:-2847238.9059 dloss:68037752303.6455 exploreP:0.0100\n",
      "Episode:484 meanR:0.7300 R:0.8360 gloss:-2865261.3315 dloss:38113838976.1025 exploreP:0.0100\n",
      "Episode:485 meanR:0.7320 R:0.8005 gloss:-2882686.5159 dloss:43068825400.8325 exploreP:0.0100\n",
      "Episode:486 meanR:0.7318 R:0.5345 gloss:-2896433.6617 dloss:64153747025.8377 exploreP:0.0100\n",
      "Episode:487 meanR:0.7323 R:0.8300 gloss:-2908937.0713 dloss:74238903006.8723 exploreP:0.0100\n",
      "Episode:488 meanR:0.7329 R:0.6765 gloss:-2931919.7688 dloss:65884856514.5867 exploreP:0.0100\n",
      "Episode:489 meanR:0.7313 R:0.8710 gloss:-2943398.1537 dloss:58930000181.0094 exploreP:0.0100\n",
      "Episode:490 meanR:0.7319 R:1.0115 gloss:-2938668.3202 dloss:128463065104.4921 exploreP:0.0100\n",
      "Episode:491 meanR:0.7343 R:0.9245 gloss:-2964892.1191 dloss:67950492105.6302 exploreP:0.0100\n",
      "Episode:492 meanR:0.7354 R:0.9365 gloss:-2985803.0023 dloss:60668291545.0084 exploreP:0.0100\n",
      "Episode:493 meanR:0.7362 R:0.9200 gloss:-2993906.7949 dloss:48042600415.4681 exploreP:0.0100\n",
      "Episode:494 meanR:0.7361 R:0.8450 gloss:-2996145.9864 dloss:99988140544.0758 exploreP:0.0100\n",
      "Episode:495 meanR:0.7397 R:1.0445 gloss:-3021211.7258 dloss:59218268907.9540 exploreP:0.0100\n",
      "Episode:496 meanR:0.7413 R:0.8265 gloss:-3013110.5379 dloss:85409221450.9129 exploreP:0.0100\n",
      "Episode:497 meanR:0.7434 R:0.8790 gloss:-2964124.3330 dloss:49475343897.1691 exploreP:0.0100\n",
      "Episode:498 meanR:0.7450 R:1.0715 gloss:-2982660.9049 dloss:73089013941.6278 exploreP:0.0100\n",
      "Episode:499 meanR:0.7428 R:0.6580 gloss:-3009101.5544 dloss:125405938252.4992 exploreP:0.0100\n",
      "Episode:500 meanR:0.7413 R:0.9825 gloss:-3026648.8731 dloss:79877286778.3798 exploreP:0.0100\n",
      "Episode:501 meanR:0.7403 R:1.0020 gloss:-3039395.1239 dloss:58852541075.8087 exploreP:0.0100\n",
      "Episode:502 meanR:0.7416 R:1.0425 gloss:-3053799.0971 dloss:60493126515.6570 exploreP:0.0100\n",
      "Episode:503 meanR:0.7428 R:0.9070 gloss:-3072679.6184 dloss:56140133131.6753 exploreP:0.0100\n",
      "Episode:504 meanR:0.7445 R:0.9495 gloss:-3102936.6254 dloss:106122663790.4564 exploreP:0.0100\n",
      "Episode:505 meanR:0.7479 R:0.9225 gloss:-3125947.1397 dloss:83725307272.9577 exploreP:0.0100\n",
      "Episode:506 meanR:0.7444 R:0.4725 gloss:-3146934.9752 dloss:108572899406.2483 exploreP:0.0100\n",
      "Episode:507 meanR:0.7430 R:0.5315 gloss:-3170059.0232 dloss:132615653240.1261 exploreP:0.0100\n",
      "Episode:508 meanR:0.7431 R:0.6595 gloss:-3184174.5796 dloss:87654686504.6684 exploreP:0.0100\n",
      "Episode:509 meanR:0.7452 R:0.7070 gloss:-3192866.4090 dloss:92751875241.3403 exploreP:0.0100\n",
      "Episode:510 meanR:0.7482 R:0.8215 gloss:-3208268.4975 dloss:102226293232.2482 exploreP:0.0100\n",
      "Episode:511 meanR:0.7523 R:0.9520 gloss:-3219203.9791 dloss:119716841419.0161 exploreP:0.0100\n",
      "Episode:512 meanR:0.7550 R:0.9035 gloss:-3234698.9110 dloss:106027003629.2793 exploreP:0.0100\n",
      "Episode:513 meanR:0.7611 R:1.0995 gloss:-3246528.4244 dloss:129788750879.9879 exploreP:0.0100\n",
      "Episode:514 meanR:0.7625 R:0.8475 gloss:-3260218.1213 dloss:135293612346.7567 exploreP:0.0100\n",
      "Episode:515 meanR:0.7633 R:0.6935 gloss:-3275292.7014 dloss:140549373605.2657 exploreP:0.0100\n",
      "Episode:516 meanR:0.7665 R:0.9315 gloss:-3289785.3569 dloss:156132583334.3280 exploreP:0.0100\n",
      "Episode:517 meanR:0.7697 R:1.0115 gloss:-3302999.3535 dloss:237873494138.2098 exploreP:0.0100\n",
      "Episode:518 meanR:0.7740 R:0.8395 gloss:-3312873.6272 dloss:147762499087.6624 exploreP:0.0100\n",
      "Episode:519 meanR:0.7799 R:1.1820 gloss:-3315473.4841 dloss:102489885756.1849 exploreP:0.0100\n",
      "Episode:520 meanR:0.7844 R:1.1980 gloss:-3335456.3565 dloss:121756517588.6900 exploreP:0.0100\n",
      "Episode:521 meanR:0.7892 R:1.0985 gloss:-3342620.1220 dloss:152058753408.2072 exploreP:0.0100\n",
      "Episode:522 meanR:0.7895 R:0.6335 gloss:-3358428.0465 dloss:158305866440.7570 exploreP:0.0100\n",
      "Episode:523 meanR:0.7923 R:0.8185 gloss:-3375709.6123 dloss:92749295576.7038 exploreP:0.0100\n",
      "Episode:524 meanR:0.7946 R:0.8395 gloss:-3388476.4758 dloss:99167112061.0834 exploreP:0.0100\n",
      "Episode:525 meanR:0.7980 R:0.8130 gloss:-3401340.2908 dloss:111690866826.5816 exploreP:0.0100\n",
      "Episode:526 meanR:0.8001 R:0.9285 gloss:-3411094.5214 dloss:144985690415.6445 exploreP:0.0100\n",
      "Episode:527 meanR:0.8002 R:1.0130 gloss:-3426636.6242 dloss:101929734006.9701 exploreP:0.0100\n",
      "Episode:528 meanR:0.8004 R:0.9965 gloss:-3439899.7573 dloss:90948994958.3848 exploreP:0.0100\n",
      "Episode:529 meanR:0.8024 R:1.0450 gloss:-3451492.2460 dloss:110532474242.6194 exploreP:0.0100\n",
      "Episode:530 meanR:0.8061 R:1.2375 gloss:-3466647.8501 dloss:115580403054.3882 exploreP:0.0100\n",
      "Episode:531 meanR:0.8063 R:0.8875 gloss:-3474299.7969 dloss:178285555216.8733 exploreP:0.0100\n",
      "Episode:532 meanR:0.8063 R:0.8545 gloss:-3495959.4106 dloss:126921699063.1200 exploreP:0.0100\n",
      "Episode:533 meanR:0.8028 R:0.7375 gloss:-3511356.7307 dloss:100207807156.5589 exploreP:0.0100\n",
      "Episode:534 meanR:0.8026 R:0.9745 gloss:-3524731.8821 dloss:181543297095.5063 exploreP:0.0100\n",
      "Episode:535 meanR:0.8042 R:1.1975 gloss:-3537320.3560 dloss:182675011136.2794 exploreP:0.0100\n",
      "Episode:536 meanR:0.8065 R:1.0850 gloss:-3547409.8134 dloss:282777871137.1823 exploreP:0.0100\n",
      "Episode:537 meanR:0.8064 R:0.9690 gloss:-3565136.9797 dloss:142937282470.6198 exploreP:0.0100\n",
      "Episode:538 meanR:0.8067 R:0.9685 gloss:-3577479.5179 dloss:147160970298.3951 exploreP:0.0100\n",
      "Episode:539 meanR:0.8095 R:1.0745 gloss:-3590998.0994 dloss:108212846807.2791 exploreP:0.0100\n",
      "Episode:540 meanR:0.8086 R:0.6870 gloss:-3602031.8139 dloss:145957774705.7277 exploreP:0.0100\n",
      "Episode:541 meanR:0.8118 R:0.9330 gloss:-3609878.4653 dloss:123199539532.9341 exploreP:0.0100\n",
      "Episode:542 meanR:0.8173 R:1.1525 gloss:-3615333.3816 dloss:166033452159.8801 exploreP:0.0100\n",
      "Episode:543 meanR:0.8195 R:0.7750 gloss:-3633099.6351 dloss:101938391298.8926 exploreP:0.0100\n",
      "Episode:544 meanR:0.8187 R:0.6860 gloss:-3638688.2893 dloss:88310751959.9849 exploreP:0.0100\n",
      "Episode:545 meanR:0.8202 R:0.8165 gloss:-3641729.2101 dloss:282456442188.4052 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:546 meanR:0.8237 R:0.9870 gloss:-3665216.0138 dloss:132065496293.0195 exploreP:0.0100\n",
      "Episode:547 meanR:0.8233 R:0.7190 gloss:-3675120.1177 dloss:95964190915.3275 exploreP:0.0100\n",
      "Episode:548 meanR:0.8217 R:0.6125 gloss:-3684590.1991 dloss:92292256838.3514 exploreP:0.0100\n",
      "Episode:549 meanR:0.8221 R:0.6985 gloss:-3696675.0110 dloss:151739891173.7192 exploreP:0.0100\n",
      "Episode:550 meanR:0.8218 R:0.5580 gloss:-3708748.1632 dloss:92981716067.6109 exploreP:0.0100\n",
      "Episode:551 meanR:0.8218 R:0.7000 gloss:-3719000.1225 dloss:207031315733.9788 exploreP:0.0100\n",
      "Episode:552 meanR:0.8211 R:0.4325 gloss:-3735143.0177 dloss:188593901667.8655 exploreP:0.0100\n",
      "Episode:553 meanR:0.8203 R:0.6775 gloss:-3744724.8470 dloss:84978296900.1844 exploreP:0.0100\n",
      "Episode:554 meanR:0.8180 R:0.6810 gloss:-3756180.6299 dloss:142860882391.3658 exploreP:0.0100\n",
      "Episode:555 meanR:0.8174 R:0.7765 gloss:-3763012.4982 dloss:261605152419.6177 exploreP:0.0100\n",
      "Episode:556 meanR:0.8198 R:0.8630 gloss:-3781631.4629 dloss:91784673686.0797 exploreP:0.0100\n",
      "Episode:557 meanR:0.8216 R:0.7250 gloss:-3792529.7132 dloss:90629323238.1918 exploreP:0.0100\n",
      "Episode:558 meanR:0.8222 R:0.6980 gloss:-3799241.6227 dloss:254589233590.5884 exploreP:0.0100\n",
      "Episode:559 meanR:0.8226 R:0.6185 gloss:-3799510.9757 dloss:270856578418.7605 exploreP:0.0100\n",
      "Episode:560 meanR:0.8237 R:0.8670 gloss:-3826285.9595 dloss:83449623952.0112 exploreP:0.0100\n",
      "Episode:561 meanR:0.8216 R:0.6020 gloss:-3835773.3032 dloss:151127402662.5725 exploreP:0.0100\n",
      "Episode:562 meanR:0.8172 R:0.4995 gloss:-3838462.2696 dloss:343743529794.6481 exploreP:0.0100\n",
      "Episode:563 meanR:0.8168 R:0.7595 gloss:-3853523.1654 dloss:193973756223.4247 exploreP:0.0100\n",
      "Episode:564 meanR:0.8151 R:0.6210 gloss:-3866532.7866 dloss:71685517156.5065 exploreP:0.0100\n",
      "Episode:565 meanR:0.8130 R:0.6155 gloss:-3874876.4822 dloss:80809299847.5258 exploreP:0.0100\n",
      "Episode:566 meanR:0.8115 R:0.7330 gloss:-3879308.4499 dloss:202936422852.8039 exploreP:0.0100\n",
      "Episode:567 meanR:0.8102 R:0.6885 gloss:-3886120.2181 dloss:429720066231.3681 exploreP:0.0100\n",
      "Episode:568 meanR:0.8103 R:0.7625 gloss:-3911167.6182 dloss:120308292430.2535 exploreP:0.0100\n",
      "Episode:569 meanR:0.8135 R:0.8330 gloss:-3919275.6698 dloss:134941362367.9846 exploreP:0.0100\n",
      "Episode:570 meanR:0.8143 R:0.6475 gloss:-3922066.2250 dloss:247206636546.3912 exploreP:0.0100\n",
      "Episode:571 meanR:0.8118 R:0.7145 gloss:-3935408.9975 dloss:236552802155.3693 exploreP:0.0100\n",
      "Episode:572 meanR:0.8116 R:0.7275 gloss:-3945841.9532 dloss:146280070064.7256 exploreP:0.0100\n",
      "Episode:573 meanR:0.8108 R:0.6685 gloss:-3956707.3328 dloss:130564545592.4874 exploreP:0.0100\n",
      "Episode:574 meanR:0.8122 R:0.6105 gloss:-3968808.0456 dloss:296263347748.5184 exploreP:0.0100\n",
      "Episode:575 meanR:0.8140 R:0.7050 gloss:-3970022.7423 dloss:280909506257.0618 exploreP:0.0100\n",
      "Episode:576 meanR:0.8143 R:0.5935 gloss:-3967120.8376 dloss:196591203158.9577 exploreP:0.0100\n",
      "Episode:577 meanR:0.8153 R:0.7800 gloss:-3984606.9912 dloss:104440649981.9603 exploreP:0.0100\n",
      "Episode:578 meanR:0.8151 R:0.5405 gloss:-3999708.8716 dloss:86670601321.4568 exploreP:0.0100\n",
      "Episode:579 meanR:0.8159 R:0.6135 gloss:-4021040.7957 dloss:84850835869.8728 exploreP:0.0100\n",
      "Episode:580 meanR:0.8174 R:0.5605 gloss:-4039644.8556 dloss:70210045075.7030 exploreP:0.0100\n",
      "Episode:581 meanR:0.8182 R:0.6795 gloss:-4051223.6815 dloss:67365290692.7062 exploreP:0.0100\n",
      "Episode:582 meanR:0.8166 R:0.6335 gloss:-4069449.9022 dloss:108579537657.1894 exploreP:0.0100\n",
      "Episode:583 meanR:0.8148 R:0.7305 gloss:-4084017.6443 dloss:80042063830.4623 exploreP:0.0100\n",
      "Episode:584 meanR:0.8137 R:0.7330 gloss:-4095905.3421 dloss:129747904495.7622 exploreP:0.0100\n",
      "Episode:585 meanR:0.8142 R:0.8490 gloss:-4112090.9056 dloss:145668803457.8475 exploreP:0.0100\n",
      "Episode:586 meanR:0.8154 R:0.6485 gloss:-4127987.1612 dloss:90452156959.3586 exploreP:0.0100\n",
      "Episode:587 meanR:0.8152 R:0.8165 gloss:-4141296.9005 dloss:141939498240.3512 exploreP:0.0100\n",
      "Episode:588 meanR:0.8142 R:0.5770 gloss:-4153490.9215 dloss:104789348307.1809 exploreP:0.0100\n",
      "Episode:589 meanR:0.8117 R:0.6140 gloss:-4165833.9892 dloss:124694406142.0697 exploreP:0.0100\n",
      "Episode:590 meanR:0.8066 R:0.5005 gloss:-4179851.2547 dloss:177890946687.6839 exploreP:0.0100\n",
      "Episode:591 meanR:0.8019 R:0.4635 gloss:-4192997.7383 dloss:138208230969.0949 exploreP:0.0100\n",
      "Episode:592 meanR:0.8009 R:0.8310 gloss:-4206373.0824 dloss:140467876252.4544 exploreP:0.0100\n",
      "Episode:593 meanR:0.8001 R:0.8360 gloss:-4215972.7577 dloss:191550645246.0179 exploreP:0.0100\n",
      "Episode:594 meanR:0.7982 R:0.6635 gloss:-4226611.2399 dloss:143436859892.0135 exploreP:0.0100\n",
      "Episode:595 meanR:0.7941 R:0.6325 gloss:-4240829.2377 dloss:122201501638.6129 exploreP:0.0100\n",
      "Episode:596 meanR:0.7912 R:0.5385 gloss:-4253969.2350 dloss:106245921196.7135 exploreP:0.0100\n",
      "Episode:597 meanR:0.7893 R:0.6810 gloss:-4267414.3500 dloss:146198025864.7166 exploreP:0.0100\n",
      "Episode:598 meanR:0.7864 R:0.7820 gloss:-4282036.0745 dloss:153809836956.7970 exploreP:0.0100\n",
      "Episode:599 meanR:0.7873 R:0.7560 gloss:-4297498.6558 dloss:107325838036.7478 exploreP:0.0100\n",
      "Episode:600 meanR:0.7855 R:0.7935 gloss:-4312755.8879 dloss:118058427243.7396 exploreP:0.0100\n",
      "Episode:601 meanR:0.7817 R:0.6280 gloss:-4328805.6422 dloss:120455259373.9496 exploreP:0.0100\n",
      "Episode:602 meanR:0.7769 R:0.5635 gloss:-4344479.8635 dloss:118658902119.1549 exploreP:0.0100\n",
      "Episode:603 meanR:0.7728 R:0.4905 gloss:-4360779.5483 dloss:137276493595.6994 exploreP:0.0100\n",
      "Episode:604 meanR:0.7685 R:0.5200 gloss:-4377639.7801 dloss:123253634326.7132 exploreP:0.0100\n",
      "Episode:605 meanR:0.7644 R:0.5120 gloss:-4392029.6525 dloss:142484752820.2280 exploreP:0.0100\n",
      "Episode:606 meanR:0.7637 R:0.4110 gloss:-4409366.0713 dloss:140927526234.8940 exploreP:0.0100\n",
      "Episode:607 meanR:0.7637 R:0.5310 gloss:-4427176.7240 dloss:157388233793.5459 exploreP:0.0100\n",
      "Episode:608 meanR:0.7611 R:0.3970 gloss:-4445308.0041 dloss:152006101157.4131 exploreP:0.0100\n",
      "Episode:609 meanR:0.7593 R:0.5270 gloss:-4462707.9360 dloss:133237936920.6342 exploreP:0.0100\n",
      "Episode:610 meanR:0.7556 R:0.4500 gloss:-4478654.6427 dloss:121932301850.6118 exploreP:0.0100\n",
      "Episode:611 meanR:0.7528 R:0.6735 gloss:-4494210.5901 dloss:114684133506.0885 exploreP:0.0100\n",
      "Episode:612 meanR:0.7502 R:0.6380 gloss:-4512821.0450 dloss:125301640497.1447 exploreP:0.0100\n",
      "Episode:613 meanR:0.7456 R:0.6430 gloss:-4531762.2033 dloss:122393557798.7107 exploreP:0.0100\n",
      "Episode:614 meanR:0.7435 R:0.6360 gloss:-4546194.5480 dloss:119979569304.4944 exploreP:0.0100\n",
      "Episode:615 meanR:0.7414 R:0.4820 gloss:-4561438.0624 dloss:104510790979.0862 exploreP:0.0100\n",
      "Episode:616 meanR:0.7367 R:0.4675 gloss:-4575481.8895 dloss:115649788970.7626 exploreP:0.0100\n",
      "Episode:617 meanR:0.7321 R:0.5510 gloss:-4590612.6472 dloss:119655867220.0565 exploreP:0.0100\n",
      "Episode:618 meanR:0.7298 R:0.6075 gloss:-4607980.0969 dloss:136797348778.5420 exploreP:0.0100\n",
      "Episode:619 meanR:0.7236 R:0.5625 gloss:-4625775.5922 dloss:137534796009.0412 exploreP:0.0100\n",
      "Episode:620 meanR:0.7176 R:0.5975 gloss:-4643143.6851 dloss:138165198539.1870 exploreP:0.0100\n",
      "Episode:621 meanR:0.7132 R:0.6575 gloss:-4661044.2049 dloss:128753127768.9341 exploreP:0.0100\n",
      "Episode:622 meanR:0.7121 R:0.5255 gloss:-4678709.2990 dloss:144160561628.1266 exploreP:0.0100\n",
      "Episode:623 meanR:0.7102 R:0.6245 gloss:-4696037.2677 dloss:138696604657.5504 exploreP:0.0100\n",
      "Episode:624 meanR:0.7091 R:0.7310 gloss:-4712976.3907 dloss:149721468187.3539 exploreP:0.0100\n",
      "Episode:625 meanR:0.7075 R:0.6595 gloss:-4730423.1321 dloss:160158705415.9508 exploreP:0.0100\n",
      "Episode:626 meanR:0.7029 R:0.4645 gloss:-4748509.6665 dloss:193175905002.4051 exploreP:0.0100\n",
      "Episode:627 meanR:0.6970 R:0.4260 gloss:-4765344.4279 dloss:195311475022.0103 exploreP:0.0100\n",
      "Episode:628 meanR:0.6920 R:0.4890 gloss:-4787521.2821 dloss:228476940447.1161 exploreP:0.0100\n",
      "Episode:629 meanR:0.6864 R:0.4885 gloss:-4807593.6888 dloss:231476956500.3878 exploreP:0.0100\n",
      "Episode:630 meanR:0.6796 R:0.5570 gloss:-4823253.6071 dloss:283347303899.6232 exploreP:0.0100\n",
      "Episode:631 meanR:0.6769 R:0.6135 gloss:-4842886.2563 dloss:216307948241.1303 exploreP:0.0100\n",
      "Episode:632 meanR:0.6739 R:0.5590 gloss:-4856311.1314 dloss:215346331031.5193 exploreP:0.0100\n",
      "Episode:633 meanR:0.6721 R:0.5550 gloss:-4873015.4382 dloss:241711933254.6519 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:634 meanR:0.6676 R:0.5255 gloss:-4888180.3312 dloss:241205230021.6314 exploreP:0.0100\n",
      "Episode:635 meanR:0.6614 R:0.5830 gloss:-4900554.6984 dloss:218197779234.8790 exploreP:0.0100\n",
      "Episode:636 meanR:0.6549 R:0.4285 gloss:-4921515.8736 dloss:211873453981.6480 exploreP:0.0100\n",
      "Episode:637 meanR:0.6512 R:0.6030 gloss:-4934112.3269 dloss:212433914594.1696 exploreP:0.0100\n",
      "Episode:638 meanR:0.6483 R:0.6750 gloss:-4949948.0827 dloss:252224758543.0456 exploreP:0.0100\n",
      "Episode:639 meanR:0.6437 R:0.6200 gloss:-4971019.7862 dloss:262881065094.2520 exploreP:0.0100\n",
      "Episode:640 meanR:0.6466 R:0.9770 gloss:-4990376.9770 dloss:228246591363.8145 exploreP:0.0100\n",
      "Episode:641 meanR:0.6449 R:0.7630 gloss:-4993554.9653 dloss:167000213637.3342 exploreP:0.0100\n",
      "Episode:642 meanR:0.6407 R:0.7325 gloss:-5013053.4834 dloss:174211771482.3965 exploreP:0.0100\n",
      "Episode:643 meanR:0.6382 R:0.5220 gloss:-5029421.6941 dloss:198477788056.2473 exploreP:0.0100\n",
      "Episode:644 meanR:0.6361 R:0.4760 gloss:-5048412.8786 dloss:197495880712.7564 exploreP:0.0100\n",
      "Episode:645 meanR:0.6335 R:0.5550 gloss:-5059872.8096 dloss:279114099872.1740 exploreP:0.0100\n",
      "Episode:646 meanR:0.6284 R:0.4755 gloss:-5072112.6813 dloss:280759634921.5773 exploreP:0.0100\n",
      "Episode:647 meanR:0.6238 R:0.2625 gloss:-5083207.9539 dloss:252087826227.7591 exploreP:0.0100\n",
      "Episode:648 meanR:0.6255 R:0.7785 gloss:-5096815.4852 dloss:172158913977.1177 exploreP:0.0100\n",
      "Episode:649 meanR:0.6284 R:0.9885 gloss:-5103175.9458 dloss:222819713681.5338 exploreP:0.0100\n",
      "Episode:650 meanR:0.6307 R:0.7930 gloss:-5118887.3458 dloss:132547436537.3413 exploreP:0.0100\n",
      "Episode:651 meanR:0.6294 R:0.5690 gloss:-5128597.5635 dloss:214971813715.9987 exploreP:0.0100\n",
      "Episode:652 meanR:0.6318 R:0.6765 gloss:-5079359.8462 dloss:383237174402.6852 exploreP:0.0100\n",
      "Episode:653 meanR:0.6330 R:0.7950 gloss:-5061146.8027 dloss:300340277746.4594 exploreP:0.0100\n",
      "Episode:654 meanR:0.6336 R:0.7345 gloss:-5112808.0372 dloss:133778746665.3073 exploreP:0.0100\n",
      "Episode:655 meanR:0.6328 R:0.7015 gloss:-5155548.0129 dloss:144242350720.9058 exploreP:0.0100\n",
      "Episode:656 meanR:0.6352 R:1.1060 gloss:-5208988.0049 dloss:219490640371.0942 exploreP:0.0100\n",
      "Episode:657 meanR:0.6362 R:0.8205 gloss:-5175019.5469 dloss:282446275654.6111 exploreP:0.0100\n",
      "Episode:658 meanR:0.6351 R:0.5910 gloss:-5155065.4958 dloss:188151562846.9170 exploreP:0.0100\n",
      "Episode:659 meanR:0.6341 R:0.5160 gloss:-5194794.6912 dloss:130892464024.8584 exploreP:0.0100\n",
      "Episode:660 meanR:0.6318 R:0.6410 gloss:-5220936.5882 dloss:176027874629.0926 exploreP:0.0100\n",
      "Episode:661 meanR:0.6293 R:0.3480 gloss:-5243185.6775 dloss:180995994567.5371 exploreP:0.0100\n",
      "Episode:662 meanR:0.6288 R:0.4480 gloss:-5272751.9252 dloss:158973991211.3259 exploreP:0.0100\n",
      "Episode:663 meanR:0.6277 R:0.6535 gloss:-5335028.2138 dloss:251868156864.0714 exploreP:0.0100\n",
      "Episode:664 meanR:0.6281 R:0.6550 gloss:-5369653.5231 dloss:201933469934.2279 exploreP:0.0100\n",
      "Episode:665 meanR:0.6303 R:0.8355 gloss:-5365912.6021 dloss:175660920445.5463 exploreP:0.0100\n",
      "Episode:666 meanR:0.6298 R:0.6835 gloss:-5389300.7492 dloss:197095034528.0460 exploreP:0.0100\n",
      "Episode:667 meanR:0.6317 R:0.8825 gloss:-5402512.5730 dloss:157797061787.7090 exploreP:0.0100\n",
      "Episode:668 meanR:0.6312 R:0.7100 gloss:-5420237.6496 dloss:158510495967.9886 exploreP:0.0100\n",
      "Episode:669 meanR:0.6302 R:0.7365 gloss:-5438373.4126 dloss:172484562569.2535 exploreP:0.0100\n",
      "Episode:670 meanR:0.6324 R:0.8620 gloss:-5457500.5178 dloss:172609446731.0305 exploreP:0.0100\n",
      "Episode:671 meanR:0.6317 R:0.6475 gloss:-5474991.8206 dloss:159729008050.0366 exploreP:0.0100\n",
      "Episode:672 meanR:0.6321 R:0.7725 gloss:-5493689.0620 dloss:166395523362.8109 exploreP:0.0100\n",
      "Episode:673 meanR:0.6312 R:0.5750 gloss:-5511123.0128 dloss:157538956598.6143 exploreP:0.0100\n",
      "Episode:674 meanR:0.6318 R:0.6710 gloss:-5528518.6276 dloss:168612391337.2492 exploreP:0.0100\n",
      "Episode:675 meanR:0.6307 R:0.5945 gloss:-5545547.0764 dloss:181260906224.2948 exploreP:0.0100\n",
      "Episode:676 meanR:0.6306 R:0.5865 gloss:-5562221.9177 dloss:158680437756.0696 exploreP:0.0100\n",
      "Episode:677 meanR:0.6303 R:0.7485 gloss:-5580574.9929 dloss:152167539812.5659 exploreP:0.0100\n",
      "Episode:678 meanR:0.6335 R:0.8555 gloss:-5596361.8634 dloss:170139885110.9484 exploreP:0.0100\n",
      "Episode:679 meanR:0.6347 R:0.7315 gloss:-5613182.2813 dloss:169671509758.9050 exploreP:0.0100\n",
      "Episode:680 meanR:0.6369 R:0.7840 gloss:-5628928.7512 dloss:180179022871.5241 exploreP:0.0100\n",
      "Episode:681 meanR:0.6385 R:0.8370 gloss:-5643213.9725 dloss:164393435809.9407 exploreP:0.0100\n",
      "Episode:682 meanR:0.6417 R:0.9585 gloss:-5658488.0728 dloss:166372151433.9328 exploreP:0.0100\n",
      "Episode:683 meanR:0.6409 R:0.6475 gloss:-5673307.9710 dloss:140414252806.1738 exploreP:0.0100\n",
      "Episode:684 meanR:0.6428 R:0.9220 gloss:-5687167.0433 dloss:153536713670.3766 exploreP:0.0100\n",
      "Episode:685 meanR:0.6426 R:0.8325 gloss:-5702297.7869 dloss:166448402779.7607 exploreP:0.0100\n",
      "Episode:686 meanR:0.6433 R:0.7145 gloss:-5719231.9295 dloss:145184515918.3019 exploreP:0.0100\n",
      "Episode:687 meanR:0.6416 R:0.6515 gloss:-5734640.0532 dloss:137041660364.8108 exploreP:0.0100\n",
      "Episode:688 meanR:0.6452 R:0.9385 gloss:-5751626.4801 dloss:145262755271.2198 exploreP:0.0100\n",
      "Episode:689 meanR:0.6468 R:0.7700 gloss:-5768682.3106 dloss:135360828745.7193 exploreP:0.0100\n",
      "Episode:690 meanR:0.6500 R:0.8230 gloss:-5785622.3975 dloss:143980578183.1551 exploreP:0.0100\n",
      "Episode:691 meanR:0.6513 R:0.5915 gloss:-5804755.7954 dloss:124290210278.0335 exploreP:0.0100\n",
      "Episode:692 meanR:0.6489 R:0.5875 gloss:-5823191.6372 dloss:127097757695.6706 exploreP:0.0100\n",
      "Episode:693 meanR:0.6473 R:0.6820 gloss:-5839543.8270 dloss:130252270496.2608 exploreP:0.0100\n",
      "Episode:694 meanR:0.6476 R:0.6905 gloss:-5859411.5272 dloss:133075806124.2623 exploreP:0.0100\n",
      "Episode:695 meanR:0.6482 R:0.6910 gloss:-5880408.1578 dloss:136789933018.5733 exploreP:0.0100\n",
      "Episode:696 meanR:0.6521 R:0.9265 gloss:-5898091.7206 dloss:132331496043.1709 exploreP:0.0100\n",
      "Episode:697 meanR:0.6539 R:0.8630 gloss:-5917861.1019 dloss:129562749285.2879 exploreP:0.0100\n",
      "Episode:698 meanR:0.6537 R:0.7675 gloss:-5939095.7927 dloss:119713208499.4159 exploreP:0.0100\n",
      "Episode:699 meanR:0.6537 R:0.7550 gloss:-5959368.3487 dloss:119188719079.7939 exploreP:0.0100\n",
      "Episode:700 meanR:0.6536 R:0.7785 gloss:-5979344.9981 dloss:116333721504.0139 exploreP:0.0100\n",
      "Episode:701 meanR:0.6542 R:0.6945 gloss:-5998228.2203 dloss:119025251985.8591 exploreP:0.0100\n",
      "Episode:702 meanR:0.6550 R:0.6385 gloss:-6017303.8614 dloss:122595573048.4222 exploreP:0.0100\n",
      "Episode:703 meanR:0.6587 R:0.8615 gloss:-6040052.2249 dloss:122487567394.2629 exploreP:0.0100\n",
      "Episode:704 meanR:0.6594 R:0.5865 gloss:-6060078.0169 dloss:130581338302.5132 exploreP:0.0100\n",
      "Episode:705 meanR:0.6612 R:0.6955 gloss:-6082090.6280 dloss:141163346582.1833 exploreP:0.0100\n",
      "Episode:706 meanR:0.6628 R:0.5675 gloss:-6095261.3772 dloss:128491585370.9466 exploreP:0.0100\n",
      "Episode:707 meanR:0.6659 R:0.8475 gloss:-6112361.3273 dloss:202522862684.3465 exploreP:0.0100\n",
      "Episode:708 meanR:0.6722 R:1.0235 gloss:-6105477.6352 dloss:534073595888.0426 exploreP:0.0100\n",
      "Episode:709 meanR:0.6729 R:0.5940 gloss:-6141543.6952 dloss:263439124838.0680 exploreP:0.0100\n",
      "Episode:710 meanR:0.6749 R:0.6540 gloss:-6159584.4739 dloss:332072073418.7264 exploreP:0.0100\n",
      "Episode:711 meanR:0.6733 R:0.5150 gloss:-6175138.5205 dloss:336697269618.4952 exploreP:0.0100\n",
      "Episode:712 meanR:0.6737 R:0.6735 gloss:-6198989.2442 dloss:432307785721.7390 exploreP:0.0100\n",
      "Episode:713 meanR:0.6777 R:1.0490 gloss:-6218302.6678 dloss:396213217102.1661 exploreP:0.0100\n",
      "Episode:714 meanR:0.6801 R:0.8750 gloss:-6237946.3577 dloss:345982565201.8615 exploreP:0.0100\n",
      "Episode:715 meanR:0.6865 R:1.1210 gloss:-6255561.3414 dloss:320941681373.5718 exploreP:0.0100\n",
      "Episode:716 meanR:0.6862 R:0.4320 gloss:-6205943.4510 dloss:1836814830832.1675 exploreP:0.0100\n",
      "Episode:717 meanR:0.6846 R:0.3945 gloss:-6219576.8062 dloss:326330233212.6804 exploreP:0.0100\n",
      "Episode:718 meanR:0.6825 R:0.4025 gloss:-6272656.1562 dloss:242225293201.1358 exploreP:0.0100\n",
      "Episode:719 meanR:0.6800 R:0.3115 gloss:-6289295.1840 dloss:215438340200.9781 exploreP:0.0100\n",
      "Episode:720 meanR:0.6776 R:0.3510 gloss:-6297357.3118 dloss:221918199091.1846 exploreP:0.0100\n",
      "Episode:721 meanR:0.6744 R:0.3385 gloss:-6305327.3255 dloss:261952445888.4474 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:722 meanR:0.6761 R:0.6945 gloss:-6335467.4426 dloss:354603268621.0072 exploreP:0.0100\n",
      "Episode:723 meanR:0.6776 R:0.7805 gloss:-6316613.1144 dloss:757264188818.2740 exploreP:0.0100\n",
      "Episode:724 meanR:0.6786 R:0.8245 gloss:-6366993.2342 dloss:293125897191.6288 exploreP:0.0100\n",
      "Episode:725 meanR:0.6780 R:0.6015 gloss:-6398311.4304 dloss:321870275163.1422 exploreP:0.0100\n",
      "Episode:726 meanR:0.6795 R:0.6170 gloss:-6430896.0860 dloss:389258851648.2724 exploreP:0.0100\n",
      "Episode:727 meanR:0.6814 R:0.6110 gloss:-6457122.1161 dloss:438733113016.1514 exploreP:0.0100\n",
      "Episode:728 meanR:0.6853 R:0.8840 gloss:-6482514.6093 dloss:442599100877.3058 exploreP:0.0100\n",
      "Episode:729 meanR:0.6888 R:0.8395 gloss:-6505039.5783 dloss:420534882408.1239 exploreP:0.0100\n",
      "Episode:730 meanR:0.6922 R:0.8985 gloss:-6524796.0909 dloss:456095811160.9547 exploreP:0.0100\n",
      "Episode:731 meanR:0.6941 R:0.8015 gloss:-6544605.8996 dloss:428916391921.7208 exploreP:0.0100\n",
      "Episode:732 meanR:0.6935 R:0.4960 gloss:-6562612.6144 dloss:397284018360.9154 exploreP:0.0100\n",
      "Episode:733 meanR:0.6949 R:0.7015 gloss:-6581403.4289 dloss:360960103042.7211 exploreP:0.0100\n",
      "Episode:734 meanR:0.6954 R:0.5755 gloss:-6598519.1896 dloss:318595037200.8019 exploreP:0.0100\n",
      "Episode:735 meanR:0.6981 R:0.8440 gloss:-6613665.4713 dloss:267671048315.8500 exploreP:0.0100\n",
      "Episode:736 meanR:0.7021 R:0.8375 gloss:-6563374.2032 dloss:779311794944.2140 exploreP:0.0100\n",
      "Episode:737 meanR:0.7016 R:0.5490 gloss:-6578058.9254 dloss:558752662071.1721 exploreP:0.0100\n",
      "Episode:738 meanR:0.6998 R:0.4940 gloss:-6653880.3787 dloss:252962084327.0264 exploreP:0.0100\n",
      "Episode:739 meanR:0.7000 R:0.6415 gloss:-6670126.1086 dloss:275232537952.8893 exploreP:0.0100\n",
      "Episode:740 meanR:0.6965 R:0.6245 gloss:-6693044.9324 dloss:323253837545.5540 exploreP:0.0100\n",
      "Episode:741 meanR:0.6933 R:0.4475 gloss:-6716775.4353 dloss:384888305848.4838 exploreP:0.0100\n",
      "Episode:742 meanR:0.6909 R:0.4885 gloss:-6743667.6256 dloss:399865976818.8615 exploreP:0.0100\n",
      "Episode:743 meanR:0.6901 R:0.4400 gloss:-6767765.6158 dloss:414857589017.3499 exploreP:0.0100\n",
      "Episode:744 meanR:0.6899 R:0.4570 gloss:-6793859.2605 dloss:421920254093.3526 exploreP:0.0100\n",
      "Episode:745 meanR:0.6891 R:0.4735 gloss:-6816803.5135 dloss:420786725505.1519 exploreP:0.0100\n",
      "Episode:746 meanR:0.6895 R:0.5140 gloss:-6839858.7039 dloss:441904129167.0651 exploreP:0.0100\n",
      "Episode:747 meanR:0.6915 R:0.4720 gloss:-6862657.2561 dloss:419279302345.6978 exploreP:0.0100\n",
      "Episode:748 meanR:0.6878 R:0.4055 gloss:-6884433.2084 dloss:438446732073.4847 exploreP:0.0100\n",
      "Episode:749 meanR:0.6828 R:0.4820 gloss:-6907676.8783 dloss:441626693769.7065 exploreP:0.0100\n",
      "Episode:750 meanR:0.6787 R:0.3845 gloss:-6931623.2422 dloss:455338416830.3318 exploreP:0.0100\n",
      "Episode:751 meanR:0.6777 R:0.4740 gloss:-6953878.7737 dloss:446537379244.4330 exploreP:0.0100\n",
      "Episode:752 meanR:0.6756 R:0.4665 gloss:-6975776.1440 dloss:449025359926.9102 exploreP:0.0100\n",
      "Episode:753 meanR:0.6720 R:0.4355 gloss:-6998673.2574 dloss:418262781508.0524 exploreP:0.0100\n",
      "Episode:754 meanR:0.6703 R:0.5580 gloss:-7019790.9227 dloss:430825826701.3740 exploreP:0.0100\n",
      "Episode:755 meanR:0.6681 R:0.4815 gloss:-7040766.0285 dloss:396280669664.2338 exploreP:0.0100\n",
      "Episode:756 meanR:0.6628 R:0.5770 gloss:-7060867.0601 dloss:389332146787.7089 exploreP:0.0100\n",
      "Episode:757 meanR:0.6596 R:0.4995 gloss:-7081453.5021 dloss:383128444754.7432 exploreP:0.0100\n",
      "Episode:758 meanR:0.6609 R:0.7295 gloss:-7102452.0892 dloss:361853412777.0674 exploreP:0.0100\n",
      "Episode:759 meanR:0.6607 R:0.4945 gloss:-7122412.7200 dloss:377938666532.3435 exploreP:0.0100\n",
      "Episode:760 meanR:0.6580 R:0.3715 gloss:-7144843.1699 dloss:376376318472.0797 exploreP:0.0100\n",
      "Episode:761 meanR:0.6600 R:0.5490 gloss:-7166963.4156 dloss:408609186360.1189 exploreP:0.0100\n",
      "Episode:762 meanR:0.6596 R:0.4075 gloss:-7188238.7568 dloss:343194149410.3095 exploreP:0.0100\n",
      "Episode:763 meanR:0.6583 R:0.5165 gloss:-7209172.6648 dloss:339977927437.3607 exploreP:0.0100\n",
      "Episode:764 meanR:0.6569 R:0.5145 gloss:-7230033.8962 dloss:348887122432.1019 exploreP:0.0100\n",
      "Episode:765 meanR:0.6533 R:0.4820 gloss:-7250370.4541 dloss:325748848773.1697 exploreP:0.0100\n",
      "Episode:766 meanR:0.6513 R:0.4835 gloss:-7272302.4940 dloss:347050877854.4108 exploreP:0.0100\n",
      "Episode:767 meanR:0.6472 R:0.4710 gloss:-7292575.2951 dloss:383779898536.6062 exploreP:0.0100\n",
      "Episode:768 meanR:0.6442 R:0.4110 gloss:-7316439.5065 dloss:362822409879.8461 exploreP:0.0100\n",
      "Episode:769 meanR:0.6428 R:0.5980 gloss:-7337868.6801 dloss:374806330547.7402 exploreP:0.0100\n",
      "Episode:770 meanR:0.6391 R:0.4890 gloss:-7359783.8211 dloss:385223771491.1531 exploreP:0.0100\n",
      "Episode:771 meanR:0.6382 R:0.5530 gloss:-7380319.0794 dloss:397135343234.5243 exploreP:0.0100\n",
      "Episode:772 meanR:0.6370 R:0.6595 gloss:-7404015.5038 dloss:399295798857.2222 exploreP:0.0100\n",
      "Episode:773 meanR:0.6379 R:0.6645 gloss:-7423402.5492 dloss:407730846890.8903 exploreP:0.0100\n",
      "Episode:774 meanR:0.6380 R:0.6830 gloss:-7447850.6233 dloss:399061750083.9660 exploreP:0.0100\n",
      "Episode:775 meanR:0.6400 R:0.7880 gloss:-7471443.8841 dloss:477427821129.9995 exploreP:0.0100\n",
      "Episode:776 meanR:0.6422 R:0.8060 gloss:-7494189.3054 dloss:561158544687.3605 exploreP:0.0100\n",
      "Episode:777 meanR:0.6420 R:0.7290 gloss:-7521014.8087 dloss:591990780557.7598 exploreP:0.0100\n",
      "Episode:778 meanR:0.6437 R:1.0275 gloss:-7543977.3526 dloss:411988694564.4456 exploreP:0.0100\n",
      "Episode:779 meanR:0.6443 R:0.7915 gloss:-7564893.4478 dloss:672799326556.8392 exploreP:0.0100\n",
      "Episode:780 meanR:0.6449 R:0.8435 gloss:-7586380.9548 dloss:489698856878.4499 exploreP:0.0100\n",
      "Episode:781 meanR:0.6428 R:0.6275 gloss:-7608515.5718 dloss:547516421039.3366 exploreP:0.0100\n",
      "Episode:782 meanR:0.6386 R:0.5425 gloss:-7628959.0394 dloss:492241094423.2329 exploreP:0.0100\n",
      "Episode:783 meanR:0.6383 R:0.6125 gloss:-7648076.4885 dloss:685602770232.1014 exploreP:0.0100\n",
      "Episode:784 meanR:0.6352 R:0.6095 gloss:-7672128.7341 dloss:830554901292.4082 exploreP:0.0100\n",
      "Episode:785 meanR:0.6325 R:0.5655 gloss:-7695652.2266 dloss:1029682109510.0055 exploreP:0.0100\n",
      "Episode:786 meanR:0.6301 R:0.4785 gloss:-7716005.0106 dloss:850737102702.3810 exploreP:0.0100\n",
      "Episode:787 meanR:0.6314 R:0.7810 gloss:-7741974.8131 dloss:874716214442.5203 exploreP:0.0100\n",
      "Episode:788 meanR:0.6303 R:0.8205 gloss:-7764032.7273 dloss:1228857910525.7268 exploreP:0.0100\n",
      "Episode:789 meanR:0.6303 R:0.7735 gloss:-7797454.4320 dloss:1421399300823.9746 exploreP:0.0100\n",
      "Episode:790 meanR:0.6323 R:1.0275 gloss:-7829007.1432 dloss:1430649727047.7673 exploreP:0.0100\n",
      "Episode:791 meanR:0.6326 R:0.6170 gloss:-7860736.1218 dloss:1332063930356.0464 exploreP:0.0100\n",
      "Episode:792 meanR:0.6322 R:0.5460 gloss:-7888962.8730 dloss:1649352658528.8320 exploreP:0.0100\n",
      "Episode:793 meanR:0.6326 R:0.7285 gloss:-7914048.0690 dloss:1592447642179.6135 exploreP:0.0100\n",
      "Episode:794 meanR:0.6333 R:0.7520 gloss:-7936269.3467 dloss:1444891192289.8274 exploreP:0.0100\n",
      "Episode:795 meanR:0.6309 R:0.4595 gloss:-7959764.7594 dloss:1394592100149.7070 exploreP:0.0100\n",
      "Episode:796 meanR:0.6276 R:0.5910 gloss:-7983955.7945 dloss:1424166684872.7803 exploreP:0.0100\n",
      "Episode:797 meanR:0.6254 R:0.6415 gloss:-8004912.4796 dloss:1172730493182.5701 exploreP:0.0100\n",
      "Episode:798 meanR:0.6240 R:0.6320 gloss:-8027777.9118 dloss:1277158467015.4746 exploreP:0.0100\n",
      "Episode:799 meanR:0.6232 R:0.6700 gloss:-8051561.2390 dloss:1409715444229.4641 exploreP:0.0100\n",
      "Episode:800 meanR:0.6230 R:0.7630 gloss:-8075469.7094 dloss:1540453291168.2607 exploreP:0.0100\n",
      "Episode:801 meanR:0.6237 R:0.7660 gloss:-8094878.4363 dloss:1246827386685.4395 exploreP:0.0100\n",
      "Episode:802 meanR:0.6247 R:0.7395 gloss:-8100824.2238 dloss:629258198253.7452 exploreP:0.0100\n",
      "Episode:803 meanR:0.6229 R:0.6745 gloss:-8116386.0640 dloss:1307890437175.5493 exploreP:0.0100\n",
      "Episode:804 meanR:0.6219 R:0.4940 gloss:-8126043.8646 dloss:1350182525280.3564 exploreP:0.0100\n",
      "Episode:805 meanR:0.6217 R:0.6680 gloss:-8148089.8586 dloss:1043014184013.8793 exploreP:0.0100\n",
      "Episode:806 meanR:0.6225 R:0.6490 gloss:-8172710.9488 dloss:665614754613.1932 exploreP:0.0100\n",
      "Episode:807 meanR:0.6200 R:0.5950 gloss:-8188059.2854 dloss:998383871465.2306 exploreP:0.0100\n",
      "Episode:808 meanR:0.6144 R:0.4670 gloss:-8216713.4084 dloss:812341516259.5532 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:809 meanR:0.6138 R:0.5375 gloss:-8236288.8603 dloss:714617515168.5876 exploreP:0.0100\n",
      "Episode:810 meanR:0.6126 R:0.5325 gloss:-8261478.6913 dloss:628237358944.8132 exploreP:0.0100\n",
      "Episode:811 meanR:0.6130 R:0.5505 gloss:-8277699.9328 dloss:804551983451.5613 exploreP:0.0100\n",
      "Episode:812 meanR:0.6104 R:0.4125 gloss:-8297981.2477 dloss:636898457158.6202 exploreP:0.0100\n",
      "Episode:813 meanR:0.6056 R:0.5765 gloss:-8323156.0241 dloss:678593400779.5126 exploreP:0.0100\n",
      "Episode:814 meanR:0.6010 R:0.4115 gloss:-8344637.1796 dloss:537430313916.8365 exploreP:0.0100\n",
      "Episode:815 meanR:0.5930 R:0.3175 gloss:-8368431.7256 dloss:610866941438.2528 exploreP:0.0100\n",
      "Episode:816 meanR:0.5943 R:0.5645 gloss:-8389783.9076 dloss:531336222872.3942 exploreP:0.0100\n",
      "Episode:817 meanR:0.5962 R:0.5900 gloss:-8414801.2677 dloss:635944152314.6639 exploreP:0.0100\n",
      "Episode:818 meanR:0.5974 R:0.5140 gloss:-8441748.2654 dloss:769817877947.2634 exploreP:0.0100\n",
      "Episode:819 meanR:0.5988 R:0.4535 gloss:-8463922.1098 dloss:939265307666.2241 exploreP:0.0100\n",
      "Episode:820 meanR:0.6012 R:0.5985 gloss:-8490193.2672 dloss:878627621761.1672 exploreP:0.0100\n",
      "Episode:821 meanR:0.6026 R:0.4785 gloss:-8515441.3884 dloss:1119410440657.2563 exploreP:0.0100\n",
      "Episode:822 meanR:0.6015 R:0.5845 gloss:-8542848.1689 dloss:1362556137300.3171 exploreP:0.0100\n",
      "Episode:823 meanR:0.5991 R:0.5385 gloss:-8570630.2918 dloss:1107228956540.9329 exploreP:0.0100\n",
      "Episode:824 meanR:0.5947 R:0.3825 gloss:-8599929.5358 dloss:1543840899710.8855 exploreP:0.0100\n",
      "Episode:825 meanR:0.5936 R:0.4920 gloss:-8627704.0113 dloss:1607670866739.4192 exploreP:0.0100\n",
      "Episode:826 meanR:0.5941 R:0.6650 gloss:-8655130.7662 dloss:1499939851752.3569 exploreP:0.0100\n",
      "Episode:827 meanR:0.5937 R:0.5705 gloss:-8678341.5698 dloss:1104364017651.3601 exploreP:0.0100\n",
      "Episode:828 meanR:0.5913 R:0.6460 gloss:-8531159.3090 dloss:875840647818.0063 exploreP:0.0100\n",
      "Episode:829 meanR:0.5905 R:0.7630 gloss:-8364276.6568 dloss:338507399550.1948 exploreP:0.0100\n",
      "Episode:830 meanR:0.5869 R:0.5355 gloss:-8363724.7868 dloss:247829391513.8636 exploreP:0.0100\n",
      "Episode:831 meanR:0.5840 R:0.5115 gloss:-8368210.0231 dloss:332111064630.3145 exploreP:0.0100\n",
      "Episode:832 meanR:0.5839 R:0.4800 gloss:-8589019.6468 dloss:1545340432630.3892 exploreP:0.0100\n",
      "Episode:833 meanR:0.5813 R:0.4495 gloss:-8662178.7057 dloss:2574945267209.8931 exploreP:0.0100\n",
      "Episode:834 meanR:0.5811 R:0.5530 gloss:-8699901.1686 dloss:2468810356846.5229 exploreP:0.0100\n",
      "Episode:835 meanR:0.5785 R:0.5830 gloss:-8751123.8008 dloss:1635533751205.0815 exploreP:0.0100\n",
      "Episode:836 meanR:0.5755 R:0.5390 gloss:-8780234.5349 dloss:1339938952496.3921 exploreP:0.0100\n",
      "Episode:837 meanR:0.5758 R:0.5810 gloss:-8807277.3528 dloss:1312762324298.3054 exploreP:0.0100\n",
      "Episode:838 meanR:0.5764 R:0.5480 gloss:-8830605.3665 dloss:1391937835132.1685 exploreP:0.0100\n",
      "Episode:839 meanR:0.5758 R:0.5850 gloss:-8855379.4866 dloss:1344173926168.3057 exploreP:0.0100\n",
      "Episode:840 meanR:0.5748 R:0.5230 gloss:-8878857.5529 dloss:1357728051342.1914 exploreP:0.0100\n",
      "Episode:841 meanR:0.5764 R:0.6125 gloss:-8904581.8595 dloss:1194058621581.4238 exploreP:0.0100\n",
      "Episode:842 meanR:0.5769 R:0.5375 gloss:-8927434.0114 dloss:1239813176867.6584 exploreP:0.0100\n",
      "Episode:843 meanR:0.5789 R:0.6330 gloss:-8950567.7768 dloss:1192701728572.0081 exploreP:0.0100\n",
      "Episode:844 meanR:0.5799 R:0.5600 gloss:-8977153.3495 dloss:1169423286451.6694 exploreP:0.0100\n",
      "Episode:845 meanR:0.5813 R:0.6190 gloss:-9000253.6865 dloss:991864959572.8597 exploreP:0.0100\n",
      "Episode:846 meanR:0.5823 R:0.6110 gloss:-9024417.3755 dloss:1071954758475.6681 exploreP:0.0100\n",
      "Episode:847 meanR:0.5830 R:0.5365 gloss:-9045654.1956 dloss:898337588019.5492 exploreP:0.0100\n",
      "Episode:848 meanR:0.5847 R:0.5765 gloss:-9064923.9239 dloss:902885662233.0060 exploreP:0.0100\n",
      "Episode:849 meanR:0.5864 R:0.6505 gloss:-9075799.4479 dloss:2650222116164.3882 exploreP:0.0100\n",
      "Episode:850 meanR:0.5880 R:0.5475 gloss:-9102451.5883 dloss:2588405179008.5562 exploreP:0.0100\n",
      "Episode:851 meanR:0.5882 R:0.4910 gloss:-9141915.2106 dloss:1986992378577.0232 exploreP:0.0100\n",
      "Episode:852 meanR:0.5882 R:0.4680 gloss:-9173197.7478 dloss:1558870615810.0732 exploreP:0.0100\n",
      "Episode:853 meanR:0.5885 R:0.4685 gloss:-9200223.5062 dloss:1114668760062.6797 exploreP:0.0100\n",
      "Episode:854 meanR:0.5872 R:0.4325 gloss:-9212924.5973 dloss:429125497164.1522 exploreP:0.0100\n",
      "Episode:855 meanR:0.5882 R:0.5750 gloss:-9220059.4523 dloss:375207841197.4512 exploreP:0.0100\n",
      "Episode:856 meanR:0.5868 R:0.4410 gloss:-9209817.0003 dloss:1975073173428.8013 exploreP:0.0100\n",
      "Episode:857 meanR:0.5886 R:0.6760 gloss:-9223835.9639 dloss:2360292200192.6543 exploreP:0.0100\n",
      "Episode:858 meanR:0.5872 R:0.5930 gloss:-9241616.2307 dloss:2922417678012.3638 exploreP:0.0100\n",
      "Episode:859 meanR:0.5890 R:0.6760 gloss:-9259674.2720 dloss:1749508086832.3428 exploreP:0.0100\n",
      "Episode:860 meanR:0.5914 R:0.6070 gloss:-9306416.3369 dloss:1788456523163.6040 exploreP:0.0100\n",
      "Episode:861 meanR:0.5929 R:0.6950 gloss:-9331269.6842 dloss:1232563210678.4375 exploreP:0.0100\n",
      "Episode:862 meanR:0.5946 R:0.5810 gloss:-9353347.6555 dloss:1150211864866.4475 exploreP:0.0100\n",
      "Episode:863 meanR:0.5946 R:0.5200 gloss:-9374197.6988 dloss:1014224886476.4077 exploreP:0.0100\n",
      "Episode:864 meanR:0.5972 R:0.7680 gloss:-9394313.4500 dloss:874162187653.3617 exploreP:0.0100\n",
      "Episode:865 meanR:0.5989 R:0.6580 gloss:-9413626.0045 dloss:806928388021.7836 exploreP:0.0100\n",
      "Episode:866 meanR:0.5998 R:0.5745 gloss:-9439397.0588 dloss:1037939413965.1498 exploreP:0.0100\n",
      "Episode:867 meanR:0.6014 R:0.6320 gloss:-9462625.1228 dloss:979998976053.4919 exploreP:0.0100\n",
      "Episode:868 meanR:0.6034 R:0.6060 gloss:-9488066.6375 dloss:1042516373390.1152 exploreP:0.0100\n",
      "Episode:869 meanR:0.6036 R:0.6195 gloss:-9509501.0478 dloss:1010092904858.5127 exploreP:0.0100\n",
      "Episode:870 meanR:0.6048 R:0.6075 gloss:-9536583.0105 dloss:1138980590732.0742 exploreP:0.0100\n",
      "Episode:871 meanR:0.6059 R:0.6675 gloss:-9566120.6763 dloss:1180150055523.0740 exploreP:0.0100\n",
      "Episode:872 meanR:0.6073 R:0.7930 gloss:-9590634.8603 dloss:1185488868356.3909 exploreP:0.0100\n",
      "Episode:873 meanR:0.6070 R:0.6395 gloss:-9614469.6836 dloss:1093155311778.2399 exploreP:0.0100\n",
      "Episode:874 meanR:0.6076 R:0.7425 gloss:-9637121.0396 dloss:1011176411544.5740 exploreP:0.0100\n",
      "Episode:875 meanR:0.6079 R:0.8175 gloss:-9659892.4474 dloss:1074157905049.2834 exploreP:0.0100\n",
      "Episode:876 meanR:0.6066 R:0.6795 gloss:-9680701.6808 dloss:995726564317.8553 exploreP:0.0100\n",
      "Episode:877 meanR:0.6056 R:0.6220 gloss:-9711952.3911 dloss:1189427878003.1470 exploreP:0.0100\n",
      "Episode:878 meanR:0.6015 R:0.6175 gloss:-9731910.0707 dloss:952686892684.4430 exploreP:0.0100\n",
      "Episode:879 meanR:0.5982 R:0.4655 gloss:-9759463.4917 dloss:1102792663317.0408 exploreP:0.0100\n",
      "Episode:880 meanR:0.5961 R:0.6280 gloss:-9777753.0652 dloss:1207617201385.5581 exploreP:0.0100\n",
      "Episode:881 meanR:0.5954 R:0.5630 gloss:-9808094.9010 dloss:1009884439329.4248 exploreP:0.0100\n",
      "Episode:882 meanR:0.5949 R:0.4925 gloss:-9836276.7213 dloss:1509310477955.3804 exploreP:0.0100\n",
      "Episode:883 meanR:0.5968 R:0.8035 gloss:-9857555.0176 dloss:1398247457249.1580 exploreP:0.0100\n",
      "Episode:884 meanR:0.5984 R:0.7670 gloss:-9900740.3387 dloss:1467614764957.0012 exploreP:0.0100\n",
      "Episode:885 meanR:0.6001 R:0.7380 gloss:-9919055.5358 dloss:1537052086822.8748 exploreP:0.0100\n",
      "Episode:886 meanR:0.6023 R:0.6930 gloss:-9952181.3064 dloss:1590088428377.8904 exploreP:0.0100\n",
      "Episode:887 meanR:0.6028 R:0.8385 gloss:-9972860.5801 dloss:1709537746712.6453 exploreP:0.0100\n",
      "Episode:888 meanR:0.6024 R:0.7745 gloss:-9950043.3521 dloss:1085651661831.0468 exploreP:0.0100\n",
      "Episode:889 meanR:0.6033 R:0.8665 gloss:-9977986.9051 dloss:1321961916168.5046 exploreP:0.0100\n",
      "Episode:890 meanR:0.5982 R:0.5135 gloss:-10013683.7820 dloss:1251750866100.6411 exploreP:0.0100\n",
      "Episode:891 meanR:0.5967 R:0.4675 gloss:-10073189.7154 dloss:1847029069173.0798 exploreP:0.0100\n",
      "Episode:892 meanR:0.5976 R:0.6430 gloss:-10092346.6364 dloss:1325819766196.6721 exploreP:0.0100\n",
      "Episode:893 meanR:0.5965 R:0.6185 gloss:-10140686.8827 dloss:2274015559752.6846 exploreP:0.0100\n",
      "Episode:894 meanR:0.5954 R:0.6335 gloss:-10164681.3173 dloss:1826328554526.6165 exploreP:0.0100\n",
      "Episode:895 meanR:0.5975 R:0.6685 gloss:-10192238.5331 dloss:1914456086943.6372 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:896 meanR:0.5994 R:0.7875 gloss:-10223025.7610 dloss:2751918887264.7334 exploreP:0.0100\n",
      "Episode:897 meanR:0.5988 R:0.5760 gloss:-10241734.9879 dloss:1980697010577.9136 exploreP:0.0100\n",
      "Episode:898 meanR:0.5982 R:0.5765 gloss:-10279804.6395 dloss:2405106907687.1851 exploreP:0.0100\n",
      "Episode:899 meanR:0.5962 R:0.4740 gloss:-10311499.6331 dloss:2677742583634.8960 exploreP:0.0100\n",
      "Episode:900 meanR:0.5949 R:0.6260 gloss:-10337160.9370 dloss:2733671722395.6250 exploreP:0.0100\n",
      "Episode:901 meanR:0.5928 R:0.5585 gloss:-10362399.0647 dloss:2524432255808.8940 exploreP:0.0100\n",
      "Episode:902 meanR:0.5914 R:0.5945 gloss:-10391441.0159 dloss:2613031205053.9951 exploreP:0.0100\n",
      "Episode:903 meanR:0.5923 R:0.7660 gloss:-10416887.2159 dloss:2445116516638.3950 exploreP:0.0100\n",
      "Episode:904 meanR:0.5972 R:0.9845 gloss:-10444553.8589 dloss:2468544527297.8691 exploreP:0.0100\n",
      "Episode:905 meanR:0.5969 R:0.6380 gloss:-10469570.1753 dloss:2523421712809.7285 exploreP:0.0100\n",
      "Episode:906 meanR:0.5993 R:0.8910 gloss:-10495600.6241 dloss:2285964479665.6816 exploreP:0.0100\n",
      "Episode:907 meanR:0.6016 R:0.8305 gloss:-10499071.2303 dloss:1757741873970.3445 exploreP:0.0100\n",
      "Episode:908 meanR:0.6046 R:0.7620 gloss:-10503358.8451 dloss:1512827025954.8220 exploreP:0.0100\n",
      "Episode:909 meanR:0.6056 R:0.6410 gloss:-10546811.4666 dloss:1739124504126.9241 exploreP:0.0100\n",
      "Episode:910 meanR:0.6043 R:0.3945 gloss:-10601066.4920 dloss:2146580538390.9705 exploreP:0.0100\n",
      "Episode:911 meanR:0.6046 R:0.5810 gloss:-10643654.9148 dloss:2875279581861.0420 exploreP:0.0100\n",
      "Episode:912 meanR:0.6045 R:0.4080 gloss:-10666300.0670 dloss:2259595121612.8345 exploreP:0.0100\n",
      "Episode:913 meanR:0.6028 R:0.4095 gloss:-10699365.7245 dloss:2636866234008.0171 exploreP:0.0100\n",
      "Episode:914 meanR:0.6036 R:0.4875 gloss:-10726791.1603 dloss:2714757095813.2720 exploreP:0.0100\n",
      "Episode:915 meanR:0.6086 R:0.8135 gloss:-10752861.7555 dloss:2162018410125.4036 exploreP:0.0100\n",
      "Episode:916 meanR:0.6144 R:1.1460 gloss:-10787038.9369 dloss:2549135025802.3750 exploreP:0.0100\n",
      "Episode:917 meanR:0.6185 R:0.9975 gloss:-10816021.0577 dloss:2770051901495.0635 exploreP:0.0100\n",
      "Episode:918 meanR:0.6241 R:1.0745 gloss:-10822409.8931 dloss:2095245292148.8274 exploreP:0.0100\n",
      "Episode:919 meanR:0.6299 R:1.0340 gloss:-10863102.5849 dloss:2963536348362.0366 exploreP:0.0100\n",
      "Episode:920 meanR:0.6355 R:1.1600 gloss:-10904027.2848 dloss:3120678518669.1938 exploreP:0.0100\n",
      "Episode:921 meanR:0.6394 R:0.8660 gloss:-10930835.2617 dloss:3365888349476.6211 exploreP:0.0100\n",
      "Episode:922 meanR:0.6443 R:1.0830 gloss:-10961494.5454 dloss:3551142317749.4917 exploreP:0.0100\n",
      "Episode:923 meanR:0.6495 R:1.0570 gloss:-10995176.1261 dloss:3895398634097.1709 exploreP:0.0100\n",
      "Episode:924 meanR:0.6545 R:0.8815 gloss:-11027517.2677 dloss:4489391981435.1367 exploreP:0.0100\n",
      "Episode:925 meanR:0.6573 R:0.7685 gloss:-11060022.1638 dloss:4339558623657.2598 exploreP:0.0100\n",
      "Episode:926 meanR:0.6594 R:0.8745 gloss:-11089660.3680 dloss:4671716288200.3477 exploreP:0.0100\n",
      "Episode:927 meanR:0.6633 R:0.9610 gloss:-11120398.7872 dloss:4986808767167.3535 exploreP:0.0100\n",
      "Episode:928 meanR:0.6637 R:0.6875 gloss:-11150304.5926 dloss:4463584970985.6904 exploreP:0.0100\n",
      "Episode:929 meanR:0.6617 R:0.5595 gloss:-11181755.7769 dloss:5292457831069.4062 exploreP:0.0100\n",
      "Episode:930 meanR:0.6614 R:0.5105 gloss:-11211001.3093 dloss:5089855644924.5850 exploreP:0.0100\n",
      "Episode:931 meanR:0.6613 R:0.5025 gloss:-11238201.2395 dloss:5127420616146.0762 exploreP:0.0100\n",
      "Episode:932 meanR:0.6632 R:0.6670 gloss:-11272048.8803 dloss:6041254733399.3877 exploreP:0.0100\n",
      "Episode:933 meanR:0.6646 R:0.5945 gloss:-11302712.0036 dloss:5461917493692.1328 exploreP:0.0100\n",
      "Episode:934 meanR:0.6653 R:0.6235 gloss:-11329069.4161 dloss:4975183210693.5459 exploreP:0.0100\n",
      "Episode:935 meanR:0.6652 R:0.5730 gloss:-11358314.0259 dloss:6070316178794.2119 exploreP:0.0100\n",
      "Episode:936 meanR:0.6655 R:0.5650 gloss:-11388096.8973 dloss:5648007504885.9639 exploreP:0.0100\n",
      "Episode:937 meanR:0.6649 R:0.5185 gloss:-11415931.7354 dloss:5987702524640.4043 exploreP:0.0100\n",
      "Episode:938 meanR:0.6637 R:0.4260 gloss:-11446040.0176 dloss:5723930841124.1777 exploreP:0.0100\n",
      "Episode:939 meanR:0.6629 R:0.5125 gloss:-11476019.0258 dloss:5760275314303.9131 exploreP:0.0100\n",
      "Episode:940 meanR:0.6610 R:0.3310 gloss:-11503498.4270 dloss:5830381867030.3936 exploreP:0.0100\n",
      "Episode:941 meanR:0.6598 R:0.4950 gloss:-11534538.9443 dloss:6674865178130.1084 exploreP:0.0100\n",
      "Episode:942 meanR:0.6592 R:0.4770 gloss:-11563953.3360 dloss:6235680730423.7324 exploreP:0.0100\n",
      "Episode:943 meanR:0.6566 R:0.3725 gloss:-11593765.1526 dloss:6170053026206.7773 exploreP:0.0100\n",
      "Episode:944 meanR:0.6549 R:0.3825 gloss:-11621388.2340 dloss:6506699164574.0898 exploreP:0.0100\n",
      "Episode:945 meanR:0.6526 R:0.3940 gloss:-11651542.2182 dloss:6609357442981.6641 exploreP:0.0100\n",
      "Episode:946 meanR:0.6512 R:0.4680 gloss:-11682171.5780 dloss:6990556295453.5830 exploreP:0.0100\n",
      "Episode:947 meanR:0.6510 R:0.5150 gloss:-11712537.3539 dloss:5750573545184.1035 exploreP:0.0100\n",
      "Episode:948 meanR:0.6503 R:0.5155 gloss:-11737432.7647 dloss:6223989944325.5498 exploreP:0.0100\n",
      "Episode:949 meanR:0.6494 R:0.5575 gloss:-11770481.4826 dloss:6472816672170.0762 exploreP:0.0100\n",
      "Episode:950 meanR:0.6479 R:0.3990 gloss:-11796805.7680 dloss:5716941460322.9941 exploreP:0.0100\n",
      "Episode:951 meanR:0.6495 R:0.6480 gloss:-11828122.5519 dloss:6075805841137.3682 exploreP:0.0100\n",
      "Episode:952 meanR:0.6527 R:0.7870 gloss:-11859433.0093 dloss:6161988628079.3350 exploreP:0.0100\n",
      "Episode:953 meanR:0.6543 R:0.6340 gloss:-11887252.6603 dloss:5771376772693.5342 exploreP:0.0100\n",
      "Episode:954 meanR:0.6547 R:0.4725 gloss:-11916039.5385 dloss:5396358846356.9346 exploreP:0.0100\n",
      "Episode:955 meanR:0.6532 R:0.4235 gloss:-11942683.2731 dloss:5032214219202.8662 exploreP:0.0100\n",
      "Episode:956 meanR:0.6546 R:0.5745 gloss:-11970337.5828 dloss:5212155281544.3877 exploreP:0.0100\n",
      "Episode:957 meanR:0.6541 R:0.6305 gloss:-11998072.0908 dloss:4763847681481.4248 exploreP:0.0100\n",
      "Episode:958 meanR:0.6568 R:0.8660 gloss:-12023119.0911 dloss:4824167743139.8193 exploreP:0.0100\n",
      "Episode:959 meanR:0.6579 R:0.7865 gloss:-12052101.9808 dloss:4542479847581.1348 exploreP:0.0100\n",
      "Episode:960 meanR:0.6606 R:0.8735 gloss:-12080346.6144 dloss:4047403311479.4058 exploreP:0.0100\n",
      "Episode:961 meanR:0.6622 R:0.8535 gloss:-12109103.3950 dloss:4186827258894.7158 exploreP:0.0100\n",
      "Episode:962 meanR:0.6666 R:1.0215 gloss:-12135759.7539 dloss:4054207119298.0942 exploreP:0.0100\n",
      "Episode:963 meanR:0.6709 R:0.9480 gloss:-12163793.4065 dloss:3838792196668.9028 exploreP:0.0100\n",
      "Episode:964 meanR:0.6725 R:0.9305 gloss:-12192079.9460 dloss:3399734161994.7656 exploreP:0.0100\n",
      "Episode:965 meanR:0.6750 R:0.9040 gloss:-12219876.3896 dloss:3596868671419.9712 exploreP:0.0100\n",
      "Episode:966 meanR:0.6785 R:0.9315 gloss:-12247673.9581 dloss:3324432839769.7358 exploreP:0.0100\n",
      "Episode:967 meanR:0.6817 R:0.9505 gloss:-12275983.4477 dloss:3288294918225.5742 exploreP:0.0100\n",
      "Episode:968 meanR:0.6882 R:1.2520 gloss:-12306064.4715 dloss:3364027792459.5488 exploreP:0.0100\n",
      "Episode:969 meanR:0.6921 R:1.0150 gloss:-12333810.0143 dloss:3518856165016.3232 exploreP:0.0100\n",
      "Episode:970 meanR:0.6954 R:0.9360 gloss:-12364555.4351 dloss:3604463673665.0010 exploreP:0.0100\n",
      "Episode:971 meanR:0.6977 R:0.8945 gloss:-12391999.1572 dloss:3325151117132.5288 exploreP:0.0100\n",
      "Episode:972 meanR:0.6988 R:0.9000 gloss:-12420369.1404 dloss:3191099984621.9839 exploreP:0.0100\n",
      "Episode:973 meanR:0.7044 R:1.2045 gloss:-12446973.5965 dloss:3117732212136.4268 exploreP:0.0100\n",
      "Episode:974 meanR:0.7059 R:0.8960 gloss:-12470662.1922 dloss:3179037708576.4473 exploreP:0.0100\n",
      "Episode:975 meanR:0.7085 R:1.0685 gloss:-12500289.1235 dloss:2901446420382.6851 exploreP:0.0100\n",
      "Episode:976 meanR:0.7109 R:0.9205 gloss:-12521663.6352 dloss:3108431263083.5430 exploreP:0.0100\n",
      "Episode:977 meanR:0.7126 R:0.7965 gloss:-12536877.0828 dloss:2984475360032.1157 exploreP:0.0100\n",
      "Episode:978 meanR:0.7155 R:0.9105 gloss:-12539928.1599 dloss:2231333351886.5864 exploreP:0.0100\n",
      "Episode:979 meanR:0.7165 R:0.5600 gloss:-12583249.7879 dloss:2077926981163.2295 exploreP:0.0100\n",
      "Episode:980 meanR:0.7178 R:0.7610 gloss:-12622572.5447 dloss:2513708806813.5601 exploreP:0.0100\n",
      "Episode:981 meanR:0.7185 R:0.6265 gloss:-12660592.2966 dloss:2709661368173.6621 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:982 meanR:0.7182 R:0.4635 gloss:-12690332.6520 dloss:2946505765373.1392 exploreP:0.0100\n",
      "Episode:983 meanR:0.7160 R:0.5835 gloss:-12716764.2982 dloss:2927123998004.2871 exploreP:0.0100\n",
      "Episode:984 meanR:0.7126 R:0.4325 gloss:-12739335.2865 dloss:2856049776741.0566 exploreP:0.0100\n",
      "Episode:985 meanR:0.7101 R:0.4860 gloss:-12764922.3918 dloss:2718348132754.7368 exploreP:0.0100\n",
      "Episode:986 meanR:0.7083 R:0.5105 gloss:-12794055.4119 dloss:2537774903018.6323 exploreP:0.0100\n",
      "Episode:987 meanR:0.7054 R:0.5475 gloss:-12816650.4797 dloss:2154069855598.8660 exploreP:0.0100\n",
      "Episode:988 meanR:0.7037 R:0.6130 gloss:-12839534.4254 dloss:1847842490379.3867 exploreP:0.0100\n",
      "Episode:989 meanR:0.7020 R:0.6920 gloss:-12870207.4608 dloss:2231459266754.0562 exploreP:0.0100\n",
      "Episode:990 meanR:0.7032 R:0.6295 gloss:-12896618.2485 dloss:2201424344523.3555 exploreP:0.0100\n",
      "Episode:991 meanR:0.7035 R:0.5045 gloss:-12923600.7066 dloss:2040164548339.7356 exploreP:0.0100\n",
      "Episode:992 meanR:0.7014 R:0.4320 gloss:-12946426.0552 dloss:1562327539360.8977 exploreP:0.0100\n",
      "Episode:993 meanR:0.7006 R:0.5380 gloss:-12976550.0447 dloss:2069174589322.6941 exploreP:0.0100\n",
      "Episode:994 meanR:0.6984 R:0.4155 gloss:-13002574.6533 dloss:1614270546490.7451 exploreP:0.0100\n",
      "Episode:995 meanR:0.6989 R:0.7170 gloss:-13027214.2551 dloss:1556022665373.2576 exploreP:0.0100\n",
      "Episode:996 meanR:0.6972 R:0.6170 gloss:-13061179.0858 dloss:1805836997637.5012 exploreP:0.0100\n",
      "Episode:997 meanR:0.6972 R:0.5770 gloss:-13068883.0845 dloss:1301037733500.8689 exploreP:0.0100\n",
      "Episode:998 meanR:0.6978 R:0.6385 gloss:-13065140.2924 dloss:900509617343.3082 exploreP:0.0100\n",
      "Episode:999 meanR:0.6994 R:0.6270 gloss:-13093970.4026 dloss:1159606378399.9451 exploreP:0.0100\n",
      "Episode:1000 meanR:0.6995 R:0.6345 gloss:-13111566.8751 dloss:1280010426612.9255 exploreP:0.0100\n",
      "Episode:1001 meanR:0.7010 R:0.7145 gloss:-13064935.3310 dloss:1406488343382.7964 exploreP:0.0100\n",
      "Episode:1002 meanR:0.7032 R:0.8115 gloss:-13130869.4324 dloss:1152381367039.3413 exploreP:0.0100\n",
      "Episode:1003 meanR:0.7022 R:0.6675 gloss:-13070136.0449 dloss:2273298819774.8101 exploreP:0.0100\n",
      "Episode:1004 meanR:0.7000 R:0.7660 gloss:-13128606.9728 dloss:1290906773868.6992 exploreP:0.0100\n",
      "Episode:1005 meanR:0.7013 R:0.7695 gloss:-13123169.6163 dloss:1366209072122.8525 exploreP:0.0100\n",
      "Episode:1006 meanR:0.7015 R:0.9045 gloss:-13179789.9879 dloss:1184566720045.0193 exploreP:0.0100\n",
      "Episode:1007 meanR:0.7005 R:0.7355 gloss:-13223221.0179 dloss:1041819519037.5393 exploreP:0.0100\n",
      "Episode:1008 meanR:0.7011 R:0.8160 gloss:-13238303.5236 dloss:1374982980520.2036 exploreP:0.0100\n",
      "Episode:1009 meanR:0.7016 R:0.6980 gloss:-13259517.3119 dloss:1431867573701.3237 exploreP:0.0100\n",
      "Episode:1010 meanR:0.7057 R:0.8035 gloss:-13321537.2028 dloss:1297260948309.0464 exploreP:0.0100\n",
      "Episode:1011 meanR:0.7083 R:0.8430 gloss:-13349864.5460 dloss:1332588146239.7021 exploreP:0.0100\n",
      "Episode:1012 meanR:0.7117 R:0.7460 gloss:-13384666.3873 dloss:1384663412660.6567 exploreP:0.0100\n",
      "Episode:1013 meanR:0.7176 R:1.0010 gloss:-13401832.9995 dloss:1483543263020.9104 exploreP:0.0100\n",
      "Episode:1014 meanR:0.7214 R:0.8615 gloss:-13438166.3059 dloss:1239961864589.1980 exploreP:0.0100\n",
      "Episode:1015 meanR:0.7210 R:0.7780 gloss:-13453635.5843 dloss:1322973807172.9607 exploreP:0.0100\n",
      "Episode:1016 meanR:0.7175 R:0.7970 gloss:-13487016.2852 dloss:825669393916.3219 exploreP:0.0100\n",
      "Episode:1017 meanR:0.7142 R:0.6620 gloss:-13497320.1743 dloss:1000310279731.2642 exploreP:0.0100\n",
      "Episode:1018 meanR:0.7152 R:1.1755 gloss:-13534993.3838 dloss:1362974103008.6130 exploreP:0.0100\n",
      "Episode:1019 meanR:0.7138 R:0.8995 gloss:-13566915.6231 dloss:894673292697.7218 exploreP:0.0100\n",
      "Episode:1020 meanR:0.7104 R:0.8195 gloss:-13576921.4447 dloss:1059181111946.3013 exploreP:0.0100\n",
      "Episode:1021 meanR:0.7104 R:0.8615 gloss:-13595090.6643 dloss:964843251976.2069 exploreP:0.0100\n",
      "Episode:1022 meanR:0.7087 R:0.9125 gloss:-13608022.4240 dloss:756674423699.6399 exploreP:0.0100\n",
      "Episode:1023 meanR:0.7066 R:0.8510 gloss:-13573906.3824 dloss:690708035761.8044 exploreP:0.0100\n",
      "Episode:1024 meanR:0.7045 R:0.6685 gloss:-13616620.8525 dloss:487589156248.5717 exploreP:0.0100\n",
      "Episode:1025 meanR:0.7056 R:0.8755 gloss:-13610566.9113 dloss:384859323730.7274 exploreP:0.0100\n",
      "Episode:1026 meanR:0.7053 R:0.8505 gloss:-13637732.0729 dloss:430991249240.8004 exploreP:0.0100\n",
      "Episode:1027 meanR:0.7037 R:0.7990 gloss:-13688448.3327 dloss:366455867377.1761 exploreP:0.0100\n",
      "Episode:1028 meanR:0.7035 R:0.6680 gloss:-13726036.8154 dloss:382056555685.4013 exploreP:0.0100\n",
      "Episode:1029 meanR:0.7045 R:0.6575 gloss:-13749900.3561 dloss:419258754007.1423 exploreP:0.0100\n",
      "Episode:1030 meanR:0.7077 R:0.8265 gloss:-13781019.6165 dloss:466222034062.5220 exploreP:0.0100\n",
      "Episode:1031 meanR:0.7083 R:0.5705 gloss:-13824829.0656 dloss:558015189058.8423 exploreP:0.0100\n",
      "Episode:1032 meanR:0.7090 R:0.7310 gloss:-13873942.4544 dloss:687047028503.8510 exploreP:0.0100\n",
      "Episode:1033 meanR:0.7107 R:0.7640 gloss:-13912134.2468 dloss:607744673799.4520 exploreP:0.0100\n",
      "Episode:1034 meanR:0.7099 R:0.5465 gloss:-13931149.0805 dloss:457035570225.7132 exploreP:0.0100\n",
      "Episode:1035 meanR:0.7074 R:0.3195 gloss:-13935812.1550 dloss:402495524054.9167 exploreP:0.0100\n",
      "Episode:1036 meanR:0.7039 R:0.2235 gloss:-13846076.4161 dloss:571618641266.2465 exploreP:0.0100\n",
      "Episode:1037 meanR:0.7051 R:0.6310 gloss:-13874601.3160 dloss:1083438901434.4935 exploreP:0.0100\n",
      "Episode:1038 meanR:0.7073 R:0.6455 gloss:-13993062.2654 dloss:984608602893.9219 exploreP:0.0100\n",
      "Episode:1039 meanR:0.7094 R:0.7215 gloss:-14033323.6120 dloss:1441338577682.9490 exploreP:0.0100\n",
      "Episode:1040 meanR:0.7128 R:0.6730 gloss:-14080207.9447 dloss:1436940986329.5725 exploreP:0.0100\n",
      "Episode:1041 meanR:0.7140 R:0.6205 gloss:-14135740.5165 dloss:1155125672925.3894 exploreP:0.0100\n",
      "Episode:1042 meanR:0.7175 R:0.8250 gloss:-14177543.2834 dloss:775020380915.5636 exploreP:0.0100\n",
      "Episode:1043 meanR:0.7205 R:0.6675 gloss:-14205107.5255 dloss:920308369170.7577 exploreP:0.0100\n",
      "Episode:1044 meanR:0.7252 R:0.8565 gloss:-14234056.7830 dloss:1020021155438.4714 exploreP:0.0100\n",
      "Episode:1045 meanR:0.7265 R:0.5240 gloss:-14263230.5447 dloss:1207720307373.6995 exploreP:0.0100\n",
      "Episode:1046 meanR:0.7283 R:0.6470 gloss:-14280833.2963 dloss:938165855807.7748 exploreP:0.0100\n",
      "Episode:1047 meanR:0.7298 R:0.6690 gloss:-14289058.3394 dloss:972772912360.4960 exploreP:0.0100\n",
      "Episode:1048 meanR:0.7319 R:0.7255 gloss:-14339493.6871 dloss:1413804475443.9929 exploreP:0.0100\n",
      "Episode:1049 meanR:0.7331 R:0.6775 gloss:-14388218.5395 dloss:1811163446946.9780 exploreP:0.0100\n",
      "Episode:1050 meanR:0.7350 R:0.5860 gloss:-14408009.7033 dloss:1661206159275.5786 exploreP:0.0100\n",
      "Episode:1051 meanR:0.7374 R:0.8875 gloss:-14359719.7081 dloss:2697426987304.4199 exploreP:0.0100\n",
      "Episode:1052 meanR:0.7373 R:0.7760 gloss:-14401998.1333 dloss:2410449130389.8110 exploreP:0.0100\n",
      "Episode:1053 meanR:0.7398 R:0.8820 gloss:-14448041.8629 dloss:1030694764957.4554 exploreP:0.0100\n",
      "Episode:1054 meanR:0.7425 R:0.7430 gloss:-14417022.7181 dloss:1717797887202.3506 exploreP:0.0100\n",
      "Episode:1055 meanR:0.7425 R:0.4260 gloss:-14499399.5025 dloss:1159811684507.9390 exploreP:0.0100\n",
      "Episode:1056 meanR:0.7411 R:0.4380 gloss:-14517389.6692 dloss:1198339200660.5925 exploreP:0.0100\n",
      "Episode:1057 meanR:0.7419 R:0.7025 gloss:-14548814.7162 dloss:1347917135599.5525 exploreP:0.0100\n",
      "Episode:1058 meanR:0.7390 R:0.5855 gloss:-14577816.5517 dloss:1231911105064.4834 exploreP:0.0100\n",
      "Episode:1059 meanR:0.7381 R:0.6965 gloss:-14635205.6725 dloss:1220460025131.2949 exploreP:0.0100\n",
      "Episode:1060 meanR:0.7347 R:0.5255 gloss:-14685162.5727 dloss:1384448041545.5117 exploreP:0.0100\n",
      "Episode:1061 meanR:0.7334 R:0.7230 gloss:-14725799.8679 dloss:1223874627707.2893 exploreP:0.0100\n",
      "Episode:1062 meanR:0.7287 R:0.5540 gloss:-14761033.3068 dloss:1334274344776.8044 exploreP:0.0100\n",
      "Episode:1063 meanR:0.7255 R:0.6310 gloss:-14794963.5875 dloss:1482111535454.8245 exploreP:0.0100\n",
      "Episode:1064 meanR:0.7210 R:0.4785 gloss:-14830164.3941 dloss:1426843030549.3779 exploreP:0.0100\n",
      "Episode:1065 meanR:0.7181 R:0.6175 gloss:-14854494.7202 dloss:1574703667931.3494 exploreP:0.0100\n",
      "Episode:1066 meanR:0.7137 R:0.4865 gloss:-14883808.4902 dloss:1516412761697.0439 exploreP:0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1067 meanR:0.7096 R:0.5390 gloss:-14912562.0791 dloss:1589144458769.0120 exploreP:0.0100\n",
      "Episode:1068 meanR:0.7024 R:0.5340 gloss:-14948025.1227 dloss:1675142802276.4336 exploreP:0.0100\n",
      "Episode:1069 meanR:0.7000 R:0.7790 gloss:-14952143.6869 dloss:1814592666961.0215 exploreP:0.0100\n",
      "Episode:1070 meanR:0.6976 R:0.6900 gloss:-14919477.7932 dloss:2460420916245.3081 exploreP:0.0100\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list = [], []\n",
    "gloss_list, dloss_list = [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes for running average/running mean/window\n",
    "    n_episodes=2000 \n",
    "    max_t=1000 \n",
    "    #     print_every=10, \n",
    "    learn_every=20 \n",
    "    num_learn=10\n",
    "    goal_score=30\n",
    "\n",
    "    # Training episodes/epochs\n",
    "    for ep in range(n_episodes):\n",
    "        gloss_batch, dloss_batch = [], []\n",
    "        \n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                 # get the current state (for each agent)\n",
    "        #print(states.shape)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        \n",
    "        # Training steps/batches\n",
    "        for t in range(max_t):\n",
    "            # Explore (env) or Exploit (model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                #action = env.action_space.sample()\n",
    "                actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "                #print('explore', actions.shape)\n",
    "            else:\n",
    "                #print(states.shape)\n",
    "                actions = sess.run(model.actions_logits, feed_dict={model.states: states, \n",
    "                                                                    model.isTraining: False})\n",
    "                #print('model', actions.shape)\n",
    "\n",
    "            actions = np.clip(actions, -1, 1) # [-1, +1]\n",
    "            #print(actions.shape)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                        # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            \n",
    "            for state, action, next_state, reward, done in zip(states, actions, next_states, rewards, dones):\n",
    "                #agent.step(state, action, reward, next_state, done) # send actions to the agent\n",
    "                memory.buffer.append([state, action, next_state, reward, done])\n",
    "\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            \n",
    "            # total reward\n",
    "            total_reward = np.mean(scores)\n",
    "            \n",
    "            # Training\n",
    "            if len(memory.buffer) >= batch_size:\n",
    "                if t%learn_every == 0:\n",
    "                    for each in range(num_learn):\n",
    "                        #agent.start_learn()\n",
    "                        #experiences = self.memory.sample()\n",
    "                        batch = memory.sample(batch_size)\n",
    "                        states_ = np.array([each[0] for each in batch])\n",
    "                        actions_ = np.array([each[1] for each in batch])\n",
    "                        next_states_ = np.array([each[2] for each in batch])\n",
    "                        rewards_ = np.array([each[3] for each in batch])\n",
    "                        dones_ = np.array([each[4] for each in batch])\n",
    "\n",
    "                        #self.learn(experiences, GAMMA)\n",
    "                        # TargetQs\n",
    "                        nextQlogits = sess.run(model.gQlogits, feed_dict = {model.states: next_states_, \n",
    "                                                                            model.isTraining: False})\n",
    "                        nextQs = nextQlogits.reshape(-1)\n",
    "                        targetQs = rewards_ + (gamma * nextQs * (1-dones_))\n",
    "\n",
    "                        feed_dict = {model.states: states_, model.actions: actions_, model.targetQs: targetQs,\n",
    "                                     model.isTraining: True}\n",
    "                        dloss, _= sess.run([model.dloss, model.d_opt], feed_dict)\n",
    "                        dloss_batch.append(dloss)\n",
    "\n",
    "                        # Learn actor only once compared to critic\n",
    "                        if each+1 == num_learn:\n",
    "                            gloss, _= sess.run([model.gloss, model.g_opt], feed_dict)\n",
    "                            gloss_batch.append(gloss)\n",
    "            \n",
    "            # End of episode\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # Print out\n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.4f}'.format(total_reward),\n",
    "              'gloss:{:.4f}'.format(np.mean(gloss_batch)),\n",
    "              'dloss:{:.4f}'.format(np.mean(dloss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        gloss_list.append([ep, np.mean(gloss_batch)])\n",
    "        dloss_list.append([ep, np.mean(dloss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        ## Option 1: Solve the First Version\n",
    "        #The task is episodic, and in order to solve the environment, \n",
    "        #your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "        if np.mean(episode_reward) >= goal_score:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "# plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Episode #')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(gloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(dloss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# while True:\n",
    "#     actions = agent.act(states)                        # select actions from loaded model agent\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score: {}'.format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
