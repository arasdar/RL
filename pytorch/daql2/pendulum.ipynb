{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(1,), Box(3,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size, state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -1., -8.], dtype=float32), array([1., 1., 8.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low, env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.], dtype=float32), array([2.], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low, env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10        # random seed for reproducibility of results\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.999            # discount factor\n",
    "LR_D = 1e-4         # learning rate of the actor/discriminator \n",
    "LR_G = 1e-3        # learning rate of the generator\n",
    "H_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(s_size=state_size, a_size=action_size, h_size=H_SIZE,\n",
    "              random_seed=RANDOM_SEED, buffer_size=BUFFER_SIZE, \n",
    "              batch_size=BATCH_SIZE, gamma=GAMMA, lr_d=LR_D, lr_g=LR_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=10000\n",
    "# n_steps=1000\n",
    "# n_learn=10\n",
    "# learn_every=20\n",
    "R_goal=0 # TOCHECK\n",
    "Rs_deque = deque(maxlen=100)\n",
    "list_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tTotal Average Score: -1215.19 Gloss:286.9578 Dloss:-0.0483 reward:-2.2420 reward_in:0.0000\n",
      "Episode 2\tTotal Average Score: -1520.30 Gloss:898.5074 Dloss:-0.1282 reward:-7.0232 reward_in:0.0001\n",
      "Episode 3\tTotal Average Score: -1376.32 Gloss:917.4470 Dloss:-0.1159 reward:-7.1827 reward_in:0.0001\n",
      "Episode 4\tTotal Average Score: -1296.80 Gloss:840.0034 Dloss:-0.0955 reward:-6.5956 reward_in:0.0001\n",
      "Episode 5\tTotal Average Score: -1366.29 Gloss:848.7045 Dloss:-0.0677 reward:-6.6869 reward_in:0.0001\n",
      "Episode 6\tTotal Average Score: -1420.19 Gloss:880.6981 Dloss:-0.0334 reward:-6.9635 reward_in:0.0001\n",
      "Episode 7\tTotal Average Score: -1448.52 Gloss:903.0233 Dloss:0.0097 reward:-7.1698 reward_in:0.0001\n",
      "Episode 8\tTotal Average Score: -1419.29 Gloss:892.2592 Dloss:0.0591 reward:-7.1194 reward_in:0.0001\n",
      "Episode 9\tTotal Average Score: -1405.33 Gloss:880.5452 Dloss:0.1142 reward:-7.0622 reward_in:0.0001\n",
      "Episode 10\tTotal Average Score: -1400.52 Gloss:865.1328 Dloss:0.1747 reward:-6.9757 reward_in:0.0001\n",
      "Episode 11\tTotal Average Score: -1403.40 Gloss:864.4330 Dloss:0.2407 reward:-7.0033 reward_in:0.0001\n",
      "Episode 12\tTotal Average Score: -1402.69 Gloss:855.9520 Dloss:0.3117 reward:-6.9691 reward_in:0.0001\n",
      "Episode 13\tTotal Average Score: -1416.10 Gloss:857.3911 Dloss:0.3879 reward:-7.0110 reward_in:0.0001\n",
      "Episode 14\tTotal Average Score: -1425.29 Gloss:862.0838 Dloss:0.4693 reward:-7.0780 reward_in:0.0001\n",
      "Episode 15\tTotal Average Score: -1438.91 Gloss:870.2082 Dloss:0.5557 reward:-7.1708 reward_in:0.0001\n",
      "Episode 16\tTotal Average Score: -1450.91 Gloss:868.2394 Dloss:0.6458 reward:-7.1841 reward_in:0.0001\n",
      "Episode 17\tTotal Average Score: -1444.35 Gloss:871.0104 Dloss:0.7408 reward:-7.2330 reward_in:0.0001\n",
      "Episode 18\tTotal Average Score: -1450.51 Gloss:866.9175 Dloss:0.8410 reward:-7.2273 reward_in:0.0001\n",
      "Episode 19\tTotal Average Score: -1459.53 Gloss:867.8494 Dloss:0.9453 reward:-7.2597 reward_in:0.0001\n",
      "Episode 20\tTotal Average Score: -1465.63 Gloss:869.3293 Dloss:1.0534 reward:-7.2959 reward_in:0.0001\n",
      "Episode 21\tTotal Average Score: -1467.07 Gloss:868.1359 Dloss:1.1649 reward:-7.3097 reward_in:0.0001\n",
      "Episode 22\tTotal Average Score: -1469.20 Gloss:865.8730 Dloss:1.2809 reward:-7.3140 reward_in:0.0001\n",
      "Episode 23\tTotal Average Score: -1469.05 Gloss:865.4522 Dloss:1.4010 reward:-7.3336 reward_in:0.0001\n",
      "Episode 24\tTotal Average Score: -1470.54 Gloss:865.1964 Dloss:1.5250 reward:-7.3537 reward_in:0.0001\n",
      "Episode 25\tTotal Average Score: -1474.21 Gloss:866.9270 Dloss:1.6525 reward:-7.3880 reward_in:0.0001\n",
      "Episode 26\tTotal Average Score: -1473.91 Gloss:857.2893 Dloss:1.7833 reward:-7.3341 reward_in:0.0001\n",
      "Episode 27\tTotal Average Score: -1472.77 Gloss:859.4250 Dloss:1.9184 reward:-7.3694 reward_in:0.0001\n",
      "Episode 28\tTotal Average Score: -1458.69 Gloss:846.2174 Dloss:2.0571 reward:-7.2867 reward_in:0.0001\n",
      "Episode 29\tTotal Average Score: -1462.33 Gloss:845.9045 Dloss:2.1986 reward:-7.3031 reward_in:0.0001\n",
      "Episode 30\tTotal Average Score: -1461.89 Gloss:843.9118 Dloss:2.3447 reward:-7.3078 reward_in:0.0001\n",
      "Episode 31\tTotal Average Score: -1451.29 Gloss:839.0400 Dloss:2.4937 reward:-7.2882 reward_in:0.0001\n",
      "Episode 32\tTotal Average Score: -1453.15 Gloss:834.6138 Dloss:2.6467 reward:-7.2731 reward_in:0.0001\n",
      "Episode 33\tTotal Average Score: -1441.20 Gloss:828.9890 Dloss:2.8032 reward:-7.2472 reward_in:0.0001\n",
      "Episode 34\tTotal Average Score: -1444.05 Gloss:822.4286 Dloss:2.9635 reward:-7.2131 reward_in:0.0001\n",
      "Episode 35\tTotal Average Score: -1447.10 Gloss:821.1902 Dloss:3.1277 reward:-7.2240 reward_in:0.0001\n",
      "Episode 36\tTotal Average Score: -1449.82 Gloss:820.4904 Dloss:3.2963 reward:-7.2370 reward_in:0.0001\n",
      "Episode 37\tTotal Average Score: -1451.19 Gloss:820.8000 Dloss:3.4671 reward:-7.2604 reward_in:0.0001\n",
      "Episode 38\tTotal Average Score: -1441.06 Gloss:814.6125 Dloss:3.6414 reward:-7.2267 reward_in:0.0001\n",
      "Episode 39\tTotal Average Score: -1438.86 Gloss:810.2083 Dloss:3.8191 reward:-7.2113 reward_in:0.0001\n",
      "Episode 40\tTotal Average Score: -1441.19 Gloss:805.8223 Dloss:4.0008 reward:-7.2002 reward_in:0.0001\n",
      "Episode 41\tTotal Average Score: -1437.22 Gloss:803.6144 Dloss:4.1868 reward:-7.1967 reward_in:0.0001\n",
      "Episode 42\tTotal Average Score: -1427.84 Gloss:794.8137 Dloss:4.3748 reward:-7.1469 reward_in:0.0001\n",
      "Episode 43\tTotal Average Score: -1430.78 Gloss:789.4943 Dloss:4.5667 reward:-7.1214 reward_in:0.0001\n",
      "Episode 44\tTotal Average Score: -1421.57 Gloss:790.7064 Dloss:4.7622 reward:-7.1472 reward_in:0.0001\n",
      "Episode 45\tTotal Average Score: -1423.93 Gloss:784.5187 Dloss:4.9581 reward:-7.1174 reward_in:0.0001\n",
      "Episode 46\tTotal Average Score: -1415.62 Gloss:775.4076 Dloss:5.1628 reward:-7.0644 reward_in:0.0001\n",
      "Episode 47\tTotal Average Score: -1414.06 Gloss:768.5158 Dloss:5.3704 reward:-7.0335 reward_in:0.0001\n",
      "Episode 48\tTotal Average Score: -1415.70 Gloss:772.7107 Dloss:5.5830 reward:-7.0831 reward_in:0.0001\n",
      "Episode 49\tTotal Average Score: -1418.65 Gloss:769.4716 Dloss:5.7983 reward:-7.0795 reward_in:0.0001\n",
      "Episode 50\tTotal Average Score: -1421.94 Gloss:770.7456 Dloss:6.0161 reward:-7.1096 reward_in:0.0001\n",
      "Episode 51\tTotal Average Score: -1425.25 Gloss:770.5419 Dloss:6.2384 reward:-7.1287 reward_in:0.0001\n",
      "Episode 52\tTotal Average Score: -1428.01 Gloss:767.9161 Dloss:6.4632 reward:-7.1215 reward_in:0.0001\n",
      "Episode 53\tTotal Average Score: -1432.04 Gloss:768.5054 Dloss:6.6908 reward:-7.1453 reward_in:0.0001\n",
      "Episode 54\tTotal Average Score: -1434.75 Gloss:771.4526 Dloss:6.9222 reward:-7.1824 reward_in:0.0001\n",
      "Episode 55\tTotal Average Score: -1437.57 Gloss:772.3808 Dloss:7.1574 reward:-7.2133 reward_in:0.0001\n",
      "Episode 56\tTotal Average Score: -1439.83 Gloss:767.7008 Dloss:7.3970 reward:-7.1929 reward_in:0.0001\n",
      "Episode 57\tTotal Average Score: -1442.27 Gloss:767.2706 Dloss:7.6361 reward:-7.1970 reward_in:0.0001\n",
      "Episode 58\tTotal Average Score: -1442.88 Gloss:763.8937 Dloss:7.8807 reward:-7.1898 reward_in:0.0001\n",
      "Episode 59\tTotal Average Score: -1443.55 Gloss:765.1208 Dloss:8.1345 reward:-7.2149 reward_in:0.0001\n",
      "Episode 60\tTotal Average Score: -1445.78 Gloss:766.0245 Dloss:8.3824 reward:-7.2339 reward_in:0.0001\n",
      "Episode 61\tTotal Average Score: -1447.38 Gloss:762.4341 Dloss:8.6396 reward:-7.2235 reward_in:0.0001\n",
      "Episode 62\tTotal Average Score: -1446.95 Gloss:760.3054 Dloss:8.8955 reward:-7.2204 reward_in:0.0001\n",
      "Episode 63\tTotal Average Score: -1446.81 Gloss:758.6666 Dloss:9.1605 reward:-7.2215 reward_in:0.0001\n",
      "Episode 64\tTotal Average Score: -1448.44 Gloss:758.9751 Dloss:9.4236 reward:-7.2420 reward_in:0.0001\n",
      "Episode 65\tTotal Average Score: -1450.59 Gloss:757.6873 Dloss:9.6945 reward:-7.2488 reward_in:0.0001\n",
      "Episode 66\tTotal Average Score: -1451.80 Gloss:757.8775 Dloss:9.9690 reward:-7.2642 reward_in:0.0001\n",
      "Episode 67\tTotal Average Score: -1453.64 Gloss:756.1234 Dloss:10.2454 reward:-7.2700 reward_in:0.0001\n",
      "Episode 68\tTotal Average Score: -1454.32 Gloss:754.7921 Dloss:10.5245 reward:-7.2699 reward_in:0.0001\n",
      "Episode 69\tTotal Average Score: -1455.15 Gloss:753.3762 Dloss:10.8061 reward:-7.2764 reward_in:0.0001\n",
      "Episode 70\tTotal Average Score: -1455.10 Gloss:752.0229 Dloss:11.0953 reward:-7.2819 reward_in:0.0001\n",
      "Episode 71\tTotal Average Score: -1455.64 Gloss:745.4295 Dloss:11.3846 reward:-7.2440 reward_in:0.0001\n",
      "Episode 72\tTotal Average Score: -1457.18 Gloss:753.2140 Dloss:11.6801 reward:-7.3187 reward_in:0.0001\n",
      "Episode 73\tTotal Average Score: -1458.22 Gloss:746.0792 Dloss:11.9742 reward:-7.2820 reward_in:0.0001\n",
      "Episode 74\tTotal Average Score: -1460.06 Gloss:744.8558 Dloss:12.2756 reward:-7.2889 reward_in:0.0001\n",
      "Episode 75\tTotal Average Score: -1462.84 Gloss:743.2466 Dloss:12.5825 reward:-7.2898 reward_in:0.0001\n",
      "Episode 76\tTotal Average Score: -1463.67 Gloss:744.7969 Dloss:12.8925 reward:-7.3190 reward_in:0.0001\n",
      "Episode 77\tTotal Average Score: -1467.33 Gloss:741.5698 Dloss:13.2014 reward:-7.3086 reward_in:0.0001\n",
      "Episode 78\tTotal Average Score: -1471.93 Gloss:743.9644 Dloss:13.5169 reward:-7.3430 reward_in:0.0001\n",
      "Episode 79\tTotal Average Score: -1472.24 Gloss:741.4442 Dloss:13.8317 reward:-7.3363 reward_in:0.0001\n",
      "Episode 80\tTotal Average Score: -1473.49 Gloss:740.7153 Dloss:14.1505 reward:-7.3431 reward_in:0.0001\n",
      "Episode 81\tTotal Average Score: -1474.87 Gloss:740.8622 Dloss:14.4692 reward:-7.3588 reward_in:0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82\tTotal Average Score: -1475.81 Gloss:740.9182 Dloss:14.7967 reward:-7.3669 reward_in:0.0001\n",
      "Episode 83\tTotal Average Score: -1476.94 Gloss:740.5934 Dloss:15.1299 reward:-7.3892 reward_in:0.0001\n",
      "Episode 84\tTotal Average Score: -1476.60 Gloss:736.3091 Dloss:15.4690 reward:-7.3684 reward_in:0.0001\n",
      "Episode 85\tTotal Average Score: -1476.95 Gloss:734.7644 Dloss:15.8027 reward:-7.3674 reward_in:0.0001\n",
      "Episode 86\tTotal Average Score: -1478.18 Gloss:739.1069 Dloss:16.1397 reward:-7.4119 reward_in:0.0001\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    s = env.reset() # s: state\n",
    "    #print(s.shape)\n",
    "\n",
    "    # initialize the score (for each agent)\n",
    "    R = 0 # R: total reward\n",
    "    list_steps = []\n",
    "    \n",
    "    #for i_step in range(n_steps):\n",
    "    while True:\n",
    "        \n",
    "        s = np.reshape(s, [1, -1])\n",
    "        a = agent.act(s) # a=[-2, 2]\n",
    "        a *= 2\n",
    "        #print(a.shape)\n",
    "        \n",
    "        a = np.reshape(a, [-1])\n",
    "        s = np.reshape(s, [-1])\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        #print(s2.shape, r, done)\n",
    "\n",
    "        agent.memory.add(s, a, r, s2, done)\n",
    "\n",
    "        R += r                           # update the score (for each agent)\n",
    "        s = s2                                # roll over states to next time step\n",
    "\n",
    "        gloss, dloss, reward, reward_in = agent.start_learn()\n",
    "        list_steps.append([gloss, dloss, reward, reward_in])\n",
    "\n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    Rs_deque.append(R) # not a list but deque\n",
    "    \n",
    "    print('\\rEpisode {}\\tTotal Average Score: {:.2f}'.format(i_episode+1, np.mean(Rs_deque)), \n",
    "          'Gloss:{:.4f}'.format(np.mean(list_steps, axis=0)[0]), \n",
    "          'Dloss:{:.4f}'.format(np.mean(list_steps, axis=0)[1]),\n",
    "          'reward:{:.4f}'.format(np.mean(list_steps, axis=0)[2]), \n",
    "          'reward_in:{:.4f}'.format(np.mean(list_steps, axis=0)[3]))\n",
    "\n",
    "    # Plotting\n",
    "    list_episodes.append([np.mean(Rs_deque), \n",
    "                          np.mean(list_steps, axis=0)[0], # gloss\n",
    "                          np.mean(list_steps, axis=0)[1], # dloss\n",
    "                          np.mean(list_steps, axis=0)[2], # rewards mean\n",
    "                          np.mean(list_steps, axis=0)[3]]) # reward_in mean\n",
    "\n",
    "    \n",
    "    if np.mean(Rs_deque) >= R_goal:\n",
    "        torch.save(agent.g.state_dict(), 'g-pendulum.pth')\n",
    "        torch.save(agent.d.state_dict(), 'd-pendulum.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = np.array(list_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(np.arange(1, len(list_)+1), list_[0])\n",
    "plt.ylabel('total_average_scores')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch the agent running with saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'g-pendulum.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-33e8401073d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the saved weights into Pytorch model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'g-pendulum.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd-pendulum.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'g-pendulum.pth'"
     ]
    }
   ],
   "source": [
    "# Load the saved weights into Pytorch model\n",
    "agent.g.load_state_dict(torch.load('g-pendulum.pth', map_location='cpu'))\n",
    "agent.d.load_state_dict(torch.load('d-pendulum.pth', map_location='cpu'))\n",
    "\n",
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# S = env_info.vector_observations                  # get the current states (S) (for each agent)\n",
    "s = env.reset() # s: state # get the current state (s) (for an agent)\n",
    "\n",
    "#Rs = np.zeros(num_agents)                          # initialize the total scores (Rs) (for each agent)\n",
    "R = 0 # R:total reward # initialize the total score (R) (for an agent)\n",
    "\n",
    "while True:\n",
    "    #A = agent.act(S)                        # select actions (A) from loaded model agents\n",
    "    a = agent.act(np.reshape(s, [1, -1])) # a: [-1, +1]    # select action (a) from loaded model agent\n",
    "    \n",
    "    # env_info = env.step(A)[brain_name]           # send all actions (A) to tne environment (env)\n",
    "    # S2 = env_info.vector_observations         # get next states (S2) (for each agent)\n",
    "    # rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "    # dones = env_info.local_done                        # see if the episode is done/finished (terminal)\n",
    "    s2, r, done, _ = env.step(np.reshape(a, [-1]))\n",
    "    \n",
    "    # Rs += env_info.rewards                         # update the total scores (Rs) (for each agent)\n",
    "    # S = S2                               # roll over current states (S) to next states (S2)\n",
    "    R += r # update the total score (R) (for an agent)\n",
    "    s = s2 # roll over current state (s) to next state (s2)\n",
    "    \n",
    "    #if np.any(dones):                                  # exit loop if episode is done/finished\n",
    "    if done: # exit loop if episode is done/finished (terminal)\n",
    "        break\n",
    "        \n",
    "print('Average of total scores: {}'.format(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
