{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(1,), Box(3,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size, state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -1., -8.], dtype=float32), array([1., 1., 8.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low, env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.], dtype=float32), array([2.], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low, env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10        # random seed for reproducibility of results\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.999            # discount factor\n",
    "LR_D = 1e-4         # learning rate of the actor/discriminator \n",
    "LR_G = 1e-3        # learning rate of the generator\n",
    "H_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(s_size=state_size, a_size=action_size, h_size=H_SIZE,\n",
    "              random_seed=RANDOM_SEED, buffer_size=BUFFER_SIZE, \n",
    "              batch_size=BATCH_SIZE, gamma=GAMMA, lr_d=LR_D, lr_g=LR_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=10000\n",
    "# n_steps=1000\n",
    "# n_learn=10\n",
    "# learn_every=20\n",
    "R_goal=0 # TOCHECK\n",
    "Rs_deque = deque(maxlen=100)\n",
    "list_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tTotal Average Score: -1215.13 Gloss:287.6008 Dloss:-0.0483 reward:-2.2470 reward_in:0.0000\n",
      "Episode 2\tTotal Average Score: -1520.60 Gloss:894.4889 Dloss:-0.1282 reward:-6.9919 reward_in:0.0001\n",
      "Episode 3\tTotal Average Score: -1341.53 Gloss:898.5991 Dloss:-0.1159 reward:-7.0353 reward_in:0.0001\n",
      "Episode 4\tTotal Average Score: -1233.84 Gloss:813.7974 Dloss:-0.0955 reward:-6.3909 reward_in:0.0001\n",
      "Episode 5\tTotal Average Score: -1314.84 Gloss:806.3221 Dloss:-0.0675 reward:-6.3560 reward_in:0.0001\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    s = env.reset() # s: state\n",
    "    #print(s.shape)\n",
    "\n",
    "    # initialize the score (for each agent)\n",
    "    R = 0 # R: total reward\n",
    "    list_steps = []\n",
    "    \n",
    "    #for i_step in range(n_steps):\n",
    "    while True:\n",
    "        \n",
    "        s = np.reshape(s, [1, -1])\n",
    "        a = agent.act(s) # a=[-2, 2]\n",
    "        a *= 2\n",
    "        #print(a.shape)\n",
    "        \n",
    "        a = np.reshape(a, [-1])\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        #print(s2.shape, r, done)\n",
    "\n",
    "        s = np.reshape(s, [-1])\n",
    "        agent.memory.add(s, a, r, s2, done)\n",
    "\n",
    "        R += r                           # update the score (for each agent)\n",
    "        s = s2                                # roll over states to next time step\n",
    "\n",
    "        gloss, dloss, reward, reward_in = agent.start_learn()\n",
    "        list_steps.append([gloss, dloss, reward, reward_in])\n",
    "\n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    Rs_deque.append(R) # not a list but deque\n",
    "    \n",
    "    print('\\rEpisode {}\\tTotal Average Score: {:.2f}'.format(i_episode+1, np.mean(Rs_deque)), \n",
    "          'Gloss:{:.4f}'.format(np.mean(list_steps, axis=0)[0]), \n",
    "          'Dloss:{:.4f}'.format(np.mean(list_steps, axis=0)[1]),\n",
    "          'reward:{:.4f}'.format(np.mean(list_steps, axis=0)[2]), \n",
    "          'reward_in:{:.4f}'.format(np.mean(list_steps, axis=0)[3]))\n",
    "\n",
    "    # Plotting\n",
    "    list_episodes.append([np.mean(Rs_deque), \n",
    "                          np.mean(list_steps, axis=0)[0], # gloss\n",
    "                          np.mean(list_steps, axis=0)[1], # dloss\n",
    "                          np.mean(list_steps, axis=0)[2], # rewards mean\n",
    "                          np.mean(list_steps, axis=0)[3]]) # reward_in mean\n",
    "\n",
    "    \n",
    "    if np.mean(Rs_deque) >= R_goal:\n",
    "        torch.save(agent.g.state_dict(), 'g-pendulum.pth')\n",
    "        torch.save(agent.d.state_dict(), 'd-pendulum.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(list_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(np.arange(1, len(arr)+1), arr[0])\n",
    "plt.ylabel('total_average_scores')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch the agent running with saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'g-pendulum.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-33e8401073d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the saved weights into Pytorch model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'g-pendulum.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd-pendulum.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'g-pendulum.pth'"
     ]
    }
   ],
   "source": [
    "# Load the saved weights into Pytorch model\n",
    "agent.g.load_state_dict(torch.load('g-pendulum.pth', map_location='cpu'))\n",
    "agent.d.load_state_dict(torch.load('d-pendulum.pth', map_location='cpu'))\n",
    "\n",
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# S = env_info.vector_observations                  # get the current states (S) (for each agent)\n",
    "s = env.reset() # s: state # get the current state (s) (for an agent)\n",
    "\n",
    "#Rs = np.zeros(num_agents)                          # initialize the total scores (Rs) (for each agent)\n",
    "R = 0 # R:total reward # initialize the total score (R) (for an agent)\n",
    "\n",
    "while True:\n",
    "    #A = agent.act(S)                        # select actions (A) from loaded model agents\n",
    "    a = agent.act(np.reshape(s, [1, -1])) # a: [-1, +1]    # select action (a) from loaded model agent\n",
    "    \n",
    "    # env_info = env.step(A)[brain_name]           # send all actions (A) to tne environment (env)\n",
    "    # S2 = env_info.vector_observations         # get next states (S2) (for each agent)\n",
    "    # rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "    # dones = env_info.local_done                        # see if the episode is done/finished (terminal)\n",
    "    s2, r, done, _ = env.step(np.reshape(a, [-1]))\n",
    "    \n",
    "    # Rs += env_info.rewards                         # update the total scores (Rs) (for each agent)\n",
    "    # S = S2                               # roll over current states (S) to next states (S2)\n",
    "    R += r # update the total score (R) (for an agent)\n",
    "    s = s2 # roll over current state (s) to next state (s2)\n",
    "    \n",
    "    #if np.any(dones):                                  # exit loop if episode is done/finished\n",
    "    if done: # exit loop if episode is done/finished (terminal)\n",
    "        break\n",
    "        \n",
    "print('Average of total scores: {}'.format(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
