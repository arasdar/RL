{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning or Q-network (DQN)\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.12.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0') # 200 total reward as goal\n",
    "env = gym.make('CartPole-v1') # 500 total reward as goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# batch = []\n",
    "# for _ in range(1000):\n",
    "#     # env.render()\n",
    "#     action = env.action_space.sample()\n",
    "#     state, reward, done, info = env.step(action) # take a random action\n",
    "#     batch.append([action, state, reward, done, info])\n",
    "#     #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "#     if done:\n",
    "#         env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = np.array([each[0] for each in batch])\n",
    "# states = np.array([each[1] for each in batch])\n",
    "# rewards = np.array([each[2] for each in batch])\n",
    "# dones = np.array([each[3] for each in batch])\n",
    "# infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards[-20:])\n",
    "# print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "# print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "# print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "# print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "# print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "# print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    return actions, states, targetQs, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(random_seed=1, dtype=tf.float32, uniform=False):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(\n",
    "        dtype=dtype,\n",
    "        seed=tf.set_random_seed(random_seed), \n",
    "        uniform=uniform) # False: normal\n",
    "    return xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(inputs, units, trainable=True):\n",
    "    outputs = tf.layers.dense(\n",
    "        inputs=inputs,\n",
    "        units=units,\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=init_xavier(), # Xavier with normal init\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        trainable=trainable,\n",
    "        name=None,\n",
    "        reuse=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.leaky_relu(\n",
    "#     features,\n",
    "#     alpha=0.2,\n",
    "#     name=None\n",
    "# )\n",
    "def nl(inputs, alpha=0.2):\n",
    "    outputs = tf.maximum(alpha * inputs, inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(inputs, training=True):\n",
    "    outputs = tf.layers.batch_normalization(\n",
    "        inputs=inputs,\n",
    "        axis=-1,\n",
    "        momentum=0.99,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        beta_initializer=tf.zeros_initializer(),\n",
    "        gamma_initializer=tf.ones_initializer(),\n",
    "        moving_mean_initializer=tf.zeros_initializer(),\n",
    "        moving_variance_initializer=tf.ones_initializer(),\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        training=training,\n",
    "        trainable=True,\n",
    "        name=None,\n",
    "        reuse=None,\n",
    "        renorm=False,\n",
    "        renorm_clipping=None,\n",
    "        renorm_momentum=0.99,\n",
    "        fused=None,\n",
    "        virtual_batch_size=None,\n",
    "        adjustment=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic/ D/Q\n",
    "def D(states, action_size, hidden_size, reuse=False, alpha=0.2, is_training=False):\n",
    "    with tf.variable_scope('D', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h = mlp(inputs=states, units=hidden_size)\n",
    "        h = bn(inputs=h, training=is_training)\n",
    "        h = nl(h)\n",
    "        print(states.shape, h.shape)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h = mlp(inputs=h, units=hidden_size)\n",
    "        h = bn(inputs=h, training=is_training)\n",
    "        h = nl(h)\n",
    "        print(h.shape)\n",
    "        \n",
    "        # Output layer\n",
    "        actions = mlp(inputs=h, units=action_size)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, action_size, hidden_size, is_training):\n",
    "    \n",
    "    actions_logits = D(states=states, hidden_size=hidden_size, action_size=action_size, \n",
    "                       is_training=is_training)\n",
    "    \n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    \n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    \n",
    "    loss = tf.reduce_mean((Qs - targetQs)**2)\n",
    "    \n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('D')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=d_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs, self.is_training = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size,\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs, \n",
    "            is_training=self.is_training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('state size:{}'.format(states.shape), \n",
    "#       'actions:{}'.format(actions.shape)) \n",
    "# print('action size:', np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "action_size = 2\n",
    "state_size = 4\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "gamma = 0.99                   # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4) (?, 64)\n",
      "(?, 64)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for _ in range(memory_size):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done is True:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(sess, memory, batch_size):\n",
    "    batch = memory.sample(batch_size)\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    actions = np.array([each[1] for each in batch])\n",
    "    next_states = np.array([each[2] for each in batch])\n",
    "    rewards = np.array([each[3] for each in batch])\n",
    "    dones = np.array([each[4] for each in batch])\n",
    "    \n",
    "    next_actions_logits = sess.run(model.actions_logits, feed_dict = {model.states: next_states, \n",
    "                                                                      model.is_training: False})\n",
    "    \n",
    "    nextQs = np.max(next_actions_logits, axis=1) * (1-dones)\n",
    "    \n",
    "    targetQs = rewards + (gamma * nextQs)\n",
    "    \n",
    "    loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.states: states, \n",
    "                                                             model.actions: actions,\n",
    "                                                             model.targetQs: targetQs, \n",
    "                                                             model.is_training: True})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(sess, state):\n",
    "    \n",
    "    action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1]), \n",
    "                                                              model.is_training: False})\n",
    "    \n",
    "    action = np.argmax(action_logits, axis=1)[0]\n",
    "    #print(action)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:13.0000 R:13.0 loss:0.7316 exploreP:0.9987\n",
      "Episode:1 meanR:19.5000 R:26.0 loss:0.7303 exploreP:0.9961\n",
      "Episode:2 meanR:25.6667 R:38.0 loss:0.6658 exploreP:0.9924\n",
      "Episode:3 meanR:26.7500 R:30.0 loss:0.6524 exploreP:0.9895\n",
      "Episode:4 meanR:24.6000 R:16.0 loss:0.6661 exploreP:0.9879\n",
      "Episode:5 meanR:26.5000 R:36.0 loss:0.6619 exploreP:0.9844\n",
      "Episode:6 meanR:24.8571 R:15.0 loss:0.6307 exploreP:0.9829\n",
      "Episode:7 meanR:25.1250 R:27.0 loss:0.7232 exploreP:0.9803\n",
      "Episode:8 meanR:24.4444 R:19.0 loss:0.6777 exploreP:0.9785\n",
      "Episode:9 meanR:23.5000 R:15.0 loss:0.7151 exploreP:0.9770\n",
      "Episode:10 meanR:23.1818 R:20.0 loss:0.8311 exploreP:0.9751\n",
      "Episode:11 meanR:23.0000 R:21.0 loss:0.7825 exploreP:0.9730\n",
      "Episode:12 meanR:22.9231 R:22.0 loss:0.9048 exploreP:0.9709\n",
      "Episode:13 meanR:22.3571 R:15.0 loss:0.8987 exploreP:0.9695\n",
      "Episode:14 meanR:24.1333 R:49.0 loss:1.0497 exploreP:0.9648\n",
      "Episode:15 meanR:23.5625 R:15.0 loss:1.5216 exploreP:0.9634\n",
      "Episode:16 meanR:23.1176 R:16.0 loss:1.6131 exploreP:0.9618\n",
      "Episode:17 meanR:22.7222 R:16.0 loss:2.3924 exploreP:0.9603\n",
      "Episode:18 meanR:23.5263 R:38.0 loss:2.4661 exploreP:0.9567\n",
      "Episode:19 meanR:22.9500 R:12.0 loss:2.3405 exploreP:0.9556\n",
      "Episode:20 meanR:23.1429 R:27.0 loss:3.8542 exploreP:0.9530\n",
      "Episode:21 meanR:23.8182 R:38.0 loss:4.5692 exploreP:0.9495\n",
      "Episode:22 meanR:23.5652 R:18.0 loss:4.1224 exploreP:0.9478\n",
      "Episode:23 meanR:23.0417 R:11.0 loss:5.0202 exploreP:0.9467\n",
      "Episode:24 meanR:22.8000 R:17.0 loss:5.0468 exploreP:0.9451\n",
      "Episode:25 meanR:22.4615 R:14.0 loss:4.8172 exploreP:0.9438\n",
      "Episode:26 meanR:22.0370 R:11.0 loss:5.4710 exploreP:0.9428\n",
      "Episode:27 meanR:22.7857 R:43.0 loss:5.0513 exploreP:0.9388\n",
      "Episode:28 meanR:23.5517 R:45.0 loss:6.8353 exploreP:0.9346\n",
      "Episode:29 meanR:23.2667 R:15.0 loss:7.4515 exploreP:0.9333\n",
      "Episode:30 meanR:23.0323 R:16.0 loss:6.4679 exploreP:0.9318\n",
      "Episode:31 meanR:23.3125 R:32.0 loss:8.0913 exploreP:0.9288\n",
      "Episode:32 meanR:23.6364 R:34.0 loss:7.7517 exploreP:0.9257\n",
      "Episode:33 meanR:23.9706 R:35.0 loss:9.6669 exploreP:0.9225\n",
      "Episode:34 meanR:23.9143 R:22.0 loss:9.8802 exploreP:0.9205\n",
      "Episode:35 meanR:24.1111 R:31.0 loss:10.0040 exploreP:0.9177\n",
      "Episode:36 meanR:23.7838 R:12.0 loss:10.1840 exploreP:0.9166\n",
      "Episode:37 meanR:23.4737 R:12.0 loss:11.3453 exploreP:0.9155\n",
      "Episode:38 meanR:23.2051 R:13.0 loss:11.6282 exploreP:0.9143\n",
      "Episode:39 meanR:23.0500 R:17.0 loss:9.7533 exploreP:0.9128\n",
      "Episode:40 meanR:22.7317 R:10.0 loss:6.7584 exploreP:0.9119\n",
      "Episode:41 meanR:22.8333 R:27.0 loss:10.1312 exploreP:0.9095\n",
      "Episode:42 meanR:22.5116 R:9.0 loss:9.6028 exploreP:0.9087\n",
      "Episode:43 meanR:22.2727 R:12.0 loss:8.6315 exploreP:0.9076\n",
      "Episode:44 meanR:22.4889 R:32.0 loss:12.4193 exploreP:0.9047\n",
      "Episode:45 meanR:22.3696 R:17.0 loss:13.7138 exploreP:0.9032\n",
      "Episode:46 meanR:22.3830 R:23.0 loss:11.1499 exploreP:0.9011\n",
      "Episode:47 meanR:22.1667 R:12.0 loss:11.8264 exploreP:0.9001\n",
      "Episode:48 meanR:22.2041 R:24.0 loss:12.7225 exploreP:0.8979\n",
      "Episode:49 meanR:22.1400 R:19.0 loss:11.1125 exploreP:0.8963\n",
      "Episode:50 meanR:22.2941 R:30.0 loss:12.5960 exploreP:0.8936\n",
      "Episode:51 meanR:22.2500 R:20.0 loss:15.9525 exploreP:0.8918\n",
      "Episode:52 meanR:22.5094 R:36.0 loss:11.3645 exploreP:0.8887\n",
      "Episode:53 meanR:22.2593 R:9.0 loss:22.2065 exploreP:0.8879\n",
      "Episode:54 meanR:22.3636 R:28.0 loss:16.4467 exploreP:0.8854\n",
      "Episode:55 meanR:22.5357 R:32.0 loss:15.5183 exploreP:0.8826\n",
      "Episode:56 meanR:22.4035 R:15.0 loss:16.1487 exploreP:0.8813\n",
      "Episode:57 meanR:22.7931 R:45.0 loss:15.4934 exploreP:0.8774\n",
      "Episode:58 meanR:22.8305 R:25.0 loss:14.3293 exploreP:0.8752\n",
      "Episode:59 meanR:22.6667 R:13.0 loss:14.5366 exploreP:0.8741\n",
      "Episode:60 meanR:22.5574 R:16.0 loss:12.9760 exploreP:0.8727\n",
      "Episode:61 meanR:22.5806 R:24.0 loss:14.3839 exploreP:0.8707\n",
      "Episode:62 meanR:22.4127 R:12.0 loss:13.7679 exploreP:0.8696\n",
      "Episode:63 meanR:22.2344 R:11.0 loss:14.2334 exploreP:0.8687\n",
      "Episode:64 meanR:22.2923 R:26.0 loss:9.9802 exploreP:0.8665\n",
      "Episode:65 meanR:22.3788 R:28.0 loss:9.8778 exploreP:0.8641\n",
      "Episode:66 meanR:22.2687 R:15.0 loss:6.7229 exploreP:0.8628\n",
      "Episode:67 meanR:22.1029 R:11.0 loss:7.6276 exploreP:0.8618\n",
      "Episode:68 meanR:22.0290 R:17.0 loss:7.1703 exploreP:0.8604\n",
      "Episode:69 meanR:22.3714 R:46.0 loss:7.3504 exploreP:0.8565\n",
      "Episode:70 meanR:22.2958 R:17.0 loss:11.3546 exploreP:0.8551\n",
      "Episode:71 meanR:22.4861 R:36.0 loss:18.0380 exploreP:0.8520\n",
      "Episode:72 meanR:22.4247 R:18.0 loss:23.8505 exploreP:0.8505\n",
      "Episode:73 meanR:22.4730 R:26.0 loss:28.4992 exploreP:0.8483\n",
      "Episode:74 meanR:22.4267 R:19.0 loss:28.6848 exploreP:0.8467\n",
      "Episode:75 meanR:22.3947 R:20.0 loss:28.2318 exploreP:0.8451\n",
      "Episode:76 meanR:22.3117 R:16.0 loss:25.5235 exploreP:0.8437\n",
      "Episode:77 meanR:22.2308 R:16.0 loss:23.1051 exploreP:0.8424\n",
      "Episode:78 meanR:22.1519 R:16.0 loss:28.8992 exploreP:0.8411\n",
      "Episode:79 meanR:22.0750 R:16.0 loss:23.4833 exploreP:0.8397\n",
      "Episode:80 meanR:22.1358 R:27.0 loss:21.0897 exploreP:0.8375\n",
      "Episode:81 meanR:22.0244 R:13.0 loss:20.2307 exploreP:0.8364\n",
      "Episode:82 meanR:21.9036 R:12.0 loss:16.0596 exploreP:0.8354\n",
      "Episode:83 meanR:22.0000 R:30.0 loss:14.0329 exploreP:0.8330\n",
      "Episode:84 meanR:22.4588 R:61.0 loss:13.4691 exploreP:0.8280\n",
      "Episode:85 meanR:22.4186 R:19.0 loss:15.5662 exploreP:0.8264\n",
      "Episode:86 meanR:22.2989 R:12.0 loss:10.3720 exploreP:0.8254\n",
      "Episode:87 meanR:22.2955 R:22.0 loss:10.5643 exploreP:0.8236\n",
      "Episode:88 meanR:22.1910 R:13.0 loss:11.0078 exploreP:0.8226\n",
      "Episode:89 meanR:22.0889 R:13.0 loss:10.8499 exploreP:0.8215\n",
      "Episode:90 meanR:22.0110 R:15.0 loss:9.4535 exploreP:0.8203\n",
      "Episode:91 meanR:22.4130 R:59.0 loss:8.2276 exploreP:0.8155\n",
      "Episode:92 meanR:22.3011 R:12.0 loss:10.1711 exploreP:0.8146\n",
      "Episode:93 meanR:22.2766 R:20.0 loss:8.6120 exploreP:0.8130\n",
      "Episode:94 meanR:22.1684 R:12.0 loss:9.2026 exploreP:0.8120\n",
      "Episode:95 meanR:22.0833 R:14.0 loss:9.7249 exploreP:0.8109\n",
      "Episode:96 meanR:22.1031 R:24.0 loss:8.9364 exploreP:0.8090\n",
      "Episode:97 meanR:22.1020 R:22.0 loss:11.4208 exploreP:0.8072\n",
      "Episode:98 meanR:21.9697 R:9.0 loss:10.1313 exploreP:0.8065\n",
      "Episode:99 meanR:21.9400 R:19.0 loss:9.4506 exploreP:0.8050\n",
      "Episode:100 meanR:21.9400 R:13.0 loss:10.2665 exploreP:0.8039\n",
      "Episode:101 meanR:22.0300 R:35.0 loss:10.3262 exploreP:0.8012\n",
      "Episode:102 meanR:21.7600 R:11.0 loss:15.2442 exploreP:0.8003\n",
      "Episode:103 meanR:21.5900 R:13.0 loss:9.2135 exploreP:0.7993\n",
      "Episode:104 meanR:21.5800 R:15.0 loss:10.6607 exploreP:0.7981\n",
      "Episode:105 meanR:21.3200 R:10.0 loss:12.4496 exploreP:0.7973\n",
      "Episode:106 meanR:21.3900 R:22.0 loss:12.3662 exploreP:0.7956\n",
      "Episode:107 meanR:21.2800 R:16.0 loss:11.9604 exploreP:0.7943\n",
      "Episode:108 meanR:21.2000 R:11.0 loss:15.2991 exploreP:0.7934\n",
      "Episode:109 meanR:21.2600 R:21.0 loss:11.7932 exploreP:0.7918\n",
      "Episode:110 meanR:21.2800 R:22.0 loss:10.7630 exploreP:0.7901\n",
      "Episode:111 meanR:21.2100 R:14.0 loss:12.0660 exploreP:0.7890\n",
      "Episode:112 meanR:21.2600 R:27.0 loss:9.2122 exploreP:0.7869\n",
      "Episode:113 meanR:21.2300 R:12.0 loss:11.4697 exploreP:0.7860\n",
      "Episode:114 meanR:20.9000 R:16.0 loss:10.0149 exploreP:0.7847\n",
      "Episode:115 meanR:21.0500 R:30.0 loss:9.2967 exploreP:0.7824\n",
      "Episode:116 meanR:21.5200 R:63.0 loss:8.8973 exploreP:0.7776\n",
      "Episode:117 meanR:21.9700 R:61.0 loss:7.5769 exploreP:0.7729\n",
      "Episode:118 meanR:22.1300 R:54.0 loss:15.3282 exploreP:0.7688\n",
      "Episode:119 meanR:22.2500 R:24.0 loss:28.6014 exploreP:0.7670\n",
      "Episode:120 meanR:22.4100 R:43.0 loss:28.0665 exploreP:0.7637\n",
      "Episode:121 meanR:22.2200 R:19.0 loss:33.6526 exploreP:0.7623\n",
      "Episode:122 meanR:22.1700 R:13.0 loss:30.4890 exploreP:0.7613\n",
      "Episode:123 meanR:22.4100 R:35.0 loss:21.6394 exploreP:0.7587\n",
      "Episode:124 meanR:22.4800 R:24.0 loss:16.7757 exploreP:0.7569\n",
      "Episode:125 meanR:22.5600 R:22.0 loss:13.8259 exploreP:0.7552\n",
      "Episode:126 meanR:22.7500 R:30.0 loss:13.4051 exploreP:0.7530\n",
      "Episode:127 meanR:22.5400 R:22.0 loss:11.6546 exploreP:0.7514\n",
      "Episode:128 meanR:22.2600 R:17.0 loss:11.8494 exploreP:0.7501\n",
      "Episode:129 meanR:22.7900 R:68.0 loss:9.5069 exploreP:0.7451\n",
      "Episode:130 meanR:23.5400 R:91.0 loss:14.9679 exploreP:0.7384\n",
      "Episode:131 meanR:23.8400 R:62.0 loss:43.3258 exploreP:0.7339\n",
      "Episode:132 meanR:23.8200 R:32.0 loss:43.7003 exploreP:0.7316\n",
      "Episode:133 meanR:23.8500 R:38.0 loss:39.8131 exploreP:0.7289\n",
      "Episode:134 meanR:23.8300 R:20.0 loss:33.2075 exploreP:0.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:135 meanR:23.8600 R:34.0 loss:26.9404 exploreP:0.7250\n",
      "Episode:136 meanR:23.9500 R:21.0 loss:20.3799 exploreP:0.7235\n",
      "Episode:137 meanR:24.1500 R:32.0 loss:17.7006 exploreP:0.7212\n",
      "Episode:138 meanR:24.5700 R:55.0 loss:14.7786 exploreP:0.7173\n",
      "Episode:139 meanR:25.0300 R:63.0 loss:10.1156 exploreP:0.7129\n",
      "Episode:140 meanR:25.1900 R:26.0 loss:7.6407 exploreP:0.7111\n",
      "Episode:141 meanR:25.1700 R:25.0 loss:9.1568 exploreP:0.7093\n",
      "Episode:142 meanR:25.5400 R:46.0 loss:7.4909 exploreP:0.7061\n",
      "Episode:143 meanR:25.8000 R:38.0 loss:7.5028 exploreP:0.7035\n",
      "Episode:144 meanR:25.6000 R:12.0 loss:6.1893 exploreP:0.7026\n",
      "Episode:145 meanR:25.6300 R:20.0 loss:6.9465 exploreP:0.7013\n",
      "Episode:146 meanR:26.1300 R:73.0 loss:8.7872 exploreP:0.6962\n",
      "Episode:147 meanR:26.2200 R:21.0 loss:24.1319 exploreP:0.6948\n",
      "Episode:148 meanR:26.1900 R:21.0 loss:22.0682 exploreP:0.6933\n",
      "Episode:149 meanR:26.7500 R:75.0 loss:21.4058 exploreP:0.6882\n",
      "Episode:150 meanR:26.8100 R:36.0 loss:17.6730 exploreP:0.6858\n",
      "Episode:151 meanR:26.7300 R:12.0 loss:13.5307 exploreP:0.6850\n",
      "Episode:152 meanR:26.6700 R:30.0 loss:11.9813 exploreP:0.6830\n",
      "Episode:153 meanR:27.1500 R:57.0 loss:11.4751 exploreP:0.6791\n",
      "Episode:154 meanR:27.1500 R:28.0 loss:9.4431 exploreP:0.6773\n",
      "Episode:155 meanR:26.9700 R:14.0 loss:10.0546 exploreP:0.6763\n",
      "Episode:156 meanR:27.0600 R:24.0 loss:10.2516 exploreP:0.6747\n",
      "Episode:157 meanR:26.7400 R:13.0 loss:12.6512 exploreP:0.6739\n",
      "Episode:158 meanR:26.6700 R:18.0 loss:21.6234 exploreP:0.6727\n",
      "Episode:159 meanR:27.6500 R:111.0 loss:42.0156 exploreP:0.6654\n",
      "Episode:160 meanR:27.6700 R:18.0 loss:37.6702 exploreP:0.6642\n",
      "Episode:161 meanR:27.7100 R:28.0 loss:31.9316 exploreP:0.6624\n",
      "Episode:162 meanR:28.0300 R:44.0 loss:22.1717 exploreP:0.6595\n",
      "Episode:163 meanR:28.3400 R:42.0 loss:17.0785 exploreP:0.6568\n",
      "Episode:164 meanR:28.2600 R:18.0 loss:16.4146 exploreP:0.6556\n",
      "Episode:165 meanR:28.5000 R:52.0 loss:12.9554 exploreP:0.6523\n",
      "Episode:166 meanR:28.7200 R:37.0 loss:9.6663 exploreP:0.6499\n",
      "Episode:167 meanR:29.0100 R:40.0 loss:11.1707 exploreP:0.6473\n",
      "Episode:168 meanR:29.3500 R:51.0 loss:10.1215 exploreP:0.6441\n",
      "Episode:169 meanR:29.1700 R:28.0 loss:9.9194 exploreP:0.6423\n",
      "Episode:170 meanR:29.4800 R:48.0 loss:13.4918 exploreP:0.6393\n",
      "Episode:171 meanR:29.4400 R:32.0 loss:25.5772 exploreP:0.6373\n",
      "Episode:172 meanR:29.4600 R:20.0 loss:29.1079 exploreP:0.6360\n",
      "Episode:173 meanR:29.6000 R:40.0 loss:26.8510 exploreP:0.6335\n",
      "Episode:174 meanR:29.9500 R:54.0 loss:24.2754 exploreP:0.6302\n",
      "Episode:175 meanR:29.9800 R:23.0 loss:28.4027 exploreP:0.6288\n",
      "Episode:176 meanR:30.6600 R:84.0 loss:12.2501 exploreP:0.6236\n",
      "Episode:177 meanR:30.7900 R:29.0 loss:9.3339 exploreP:0.6218\n",
      "Episode:178 meanR:31.2400 R:61.0 loss:13.7390 exploreP:0.6181\n",
      "Episode:179 meanR:31.6900 R:61.0 loss:34.9686 exploreP:0.6144\n",
      "Episode:180 meanR:31.5500 R:13.0 loss:56.5919 exploreP:0.6136\n",
      "Episode:181 meanR:31.6900 R:27.0 loss:38.9388 exploreP:0.6120\n",
      "Episode:182 meanR:31.9700 R:40.0 loss:37.1285 exploreP:0.6096\n",
      "Episode:183 meanR:31.9000 R:23.0 loss:42.1930 exploreP:0.6082\n",
      "Episode:184 meanR:31.4300 R:14.0 loss:33.6705 exploreP:0.6074\n",
      "Episode:185 meanR:31.6100 R:37.0 loss:28.7243 exploreP:0.6051\n",
      "Episode:186 meanR:32.0900 R:60.0 loss:21.6673 exploreP:0.6016\n",
      "Episode:187 meanR:32.4100 R:54.0 loss:16.0226 exploreP:0.5984\n",
      "Episode:188 meanR:32.5300 R:25.0 loss:13.2848 exploreP:0.5969\n",
      "Episode:189 meanR:32.5200 R:12.0 loss:11.4519 exploreP:0.5962\n",
      "Episode:190 meanR:32.6200 R:25.0 loss:13.4011 exploreP:0.5948\n",
      "Episode:191 meanR:32.2500 R:22.0 loss:14.0148 exploreP:0.5935\n",
      "Episode:192 meanR:32.7400 R:61.0 loss:23.7109 exploreP:0.5899\n",
      "Episode:193 meanR:33.1900 R:65.0 loss:40.2368 exploreP:0.5862\n",
      "Episode:194 meanR:33.8600 R:79.0 loss:28.9499 exploreP:0.5816\n",
      "Episode:195 meanR:34.2500 R:53.0 loss:15.8546 exploreP:0.5786\n",
      "Episode:196 meanR:34.6900 R:68.0 loss:13.7684 exploreP:0.5748\n",
      "Episode:197 meanR:35.1900 R:72.0 loss:18.6903 exploreP:0.5707\n",
      "Episode:198 meanR:35.6500 R:55.0 loss:34.5684 exploreP:0.5676\n",
      "Episode:199 meanR:36.3600 R:90.0 loss:29.7238 exploreP:0.5626\n",
      "Episode:200 meanR:36.3500 R:12.0 loss:30.3649 exploreP:0.5620\n",
      "Episode:201 meanR:36.2800 R:28.0 loss:26.2907 exploreP:0.5604\n",
      "Episode:202 meanR:36.8000 R:63.0 loss:21.7420 exploreP:0.5570\n",
      "Episode:203 meanR:37.2400 R:57.0 loss:17.8601 exploreP:0.5539\n",
      "Episode:204 meanR:37.7500 R:66.0 loss:16.2825 exploreP:0.5503\n",
      "Episode:205 meanR:38.0700 R:42.0 loss:23.0068 exploreP:0.5480\n",
      "Episode:206 meanR:38.2000 R:35.0 loss:37.3576 exploreP:0.5461\n",
      "Episode:207 meanR:38.6000 R:56.0 loss:54.0737 exploreP:0.5432\n",
      "Episode:208 meanR:38.7200 R:23.0 loss:50.7840 exploreP:0.5419\n",
      "Episode:209 meanR:38.6800 R:17.0 loss:46.0165 exploreP:0.5410\n",
      "Episode:210 meanR:38.6800 R:22.0 loss:40.0074 exploreP:0.5399\n",
      "Episode:211 meanR:38.6800 R:14.0 loss:49.2231 exploreP:0.5391\n",
      "Episode:212 meanR:38.9900 R:58.0 loss:22.4191 exploreP:0.5361\n",
      "Episode:213 meanR:39.2600 R:39.0 loss:16.4697 exploreP:0.5340\n",
      "Episode:214 meanR:39.9100 R:81.0 loss:16.4089 exploreP:0.5298\n",
      "Episode:215 meanR:40.5000 R:89.0 loss:36.5829 exploreP:0.5252\n",
      "Episode:216 meanR:40.1800 R:31.0 loss:34.1693 exploreP:0.5236\n",
      "Episode:217 meanR:39.8800 R:31.0 loss:38.2997 exploreP:0.5220\n",
      "Episode:218 meanR:39.7800 R:44.0 loss:25.0949 exploreP:0.5197\n",
      "Episode:219 meanR:40.0200 R:48.0 loss:19.6429 exploreP:0.5173\n",
      "Episode:220 meanR:39.8800 R:29.0 loss:20.7429 exploreP:0.5158\n",
      "Episode:221 meanR:40.4000 R:71.0 loss:15.5591 exploreP:0.5123\n",
      "Episode:222 meanR:41.2400 R:97.0 loss:33.6979 exploreP:0.5074\n",
      "Episode:223 meanR:41.3400 R:45.0 loss:55.5925 exploreP:0.5052\n",
      "Episode:224 meanR:41.3800 R:28.0 loss:52.5413 exploreP:0.5038\n",
      "Episode:225 meanR:41.4300 R:27.0 loss:45.0522 exploreP:0.5025\n",
      "Episode:226 meanR:41.7800 R:65.0 loss:34.9378 exploreP:0.4993\n",
      "Episode:227 meanR:42.2000 R:64.0 loss:21.2181 exploreP:0.4961\n",
      "Episode:228 meanR:42.4900 R:46.0 loss:14.4538 exploreP:0.4939\n",
      "Episode:229 meanR:42.5000 R:69.0 loss:27.6003 exploreP:0.4906\n",
      "Episode:230 meanR:42.4000 R:81.0 loss:40.9299 exploreP:0.4867\n",
      "Episode:231 meanR:42.4000 R:62.0 loss:28.0852 exploreP:0.4838\n",
      "Episode:232 meanR:42.9000 R:82.0 loss:23.1552 exploreP:0.4799\n",
      "Episode:233 meanR:42.8000 R:28.0 loss:17.0458 exploreP:0.4786\n",
      "Episode:234 meanR:43.3300 R:73.0 loss:22.3536 exploreP:0.4752\n",
      "Episode:235 meanR:43.4500 R:46.0 loss:41.6983 exploreP:0.4730\n",
      "Episode:236 meanR:44.0400 R:80.0 loss:55.2205 exploreP:0.4693\n",
      "Episode:237 meanR:43.8600 R:14.0 loss:48.4445 exploreP:0.4687\n",
      "Episode:238 meanR:43.5100 R:20.0 loss:52.9140 exploreP:0.4678\n",
      "Episode:239 meanR:43.0100 R:13.0 loss:33.0672 exploreP:0.4672\n",
      "Episode:240 meanR:43.0000 R:25.0 loss:25.5061 exploreP:0.4661\n",
      "Episode:241 meanR:43.2400 R:49.0 loss:19.2495 exploreP:0.4638\n",
      "Episode:242 meanR:43.8800 R:110.0 loss:23.4527 exploreP:0.4589\n",
      "Episode:243 meanR:44.0400 R:54.0 loss:29.3629 exploreP:0.4564\n",
      "Episode:244 meanR:44.6200 R:70.0 loss:31.1185 exploreP:0.4533\n",
      "Episode:245 meanR:44.8600 R:44.0 loss:24.7907 exploreP:0.4514\n",
      "Episode:246 meanR:45.1600 R:103.0 loss:24.8786 exploreP:0.4469\n",
      "Episode:247 meanR:45.8700 R:92.0 loss:36.9903 exploreP:0.4429\n",
      "Episode:248 meanR:46.1000 R:44.0 loss:29.5260 exploreP:0.4410\n",
      "Episode:249 meanR:46.2600 R:91.0 loss:28.6834 exploreP:0.4371\n",
      "Episode:250 meanR:46.2000 R:30.0 loss:19.5014 exploreP:0.4358\n",
      "Episode:251 meanR:47.7700 R:169.0 loss:28.2494 exploreP:0.4286\n",
      "Episode:252 meanR:48.9200 R:145.0 loss:27.1168 exploreP:0.4226\n",
      "Episode:253 meanR:49.5000 R:115.0 loss:43.7191 exploreP:0.4179\n",
      "Episode:254 meanR:49.7000 R:48.0 loss:41.4416 exploreP:0.4159\n",
      "Episode:255 meanR:51.8000 R:224.0 loss:29.3768 exploreP:0.4069\n",
      "Episode:256 meanR:52.9700 R:141.0 loss:29.6865 exploreP:0.4014\n",
      "Episode:257 meanR:53.2300 R:39.0 loss:29.5645 exploreP:0.3999\n",
      "Episode:258 meanR:54.1700 R:112.0 loss:28.1935 exploreP:0.3955\n",
      "Episode:259 meanR:54.6100 R:155.0 loss:30.9564 exploreP:0.3896\n",
      "Episode:260 meanR:54.6800 R:25.0 loss:32.3304 exploreP:0.3886\n",
      "Episode:261 meanR:54.9900 R:59.0 loss:31.6932 exploreP:0.3864\n",
      "Episode:262 meanR:55.5900 R:104.0 loss:33.5207 exploreP:0.3825\n",
      "Episode:263 meanR:55.6100 R:44.0 loss:27.6736 exploreP:0.3809\n",
      "Episode:264 meanR:56.3100 R:88.0 loss:29.9257 exploreP:0.3776\n",
      "Episode:265 meanR:56.5500 R:76.0 loss:31.7132 exploreP:0.3749\n",
      "Episode:266 meanR:57.1900 R:101.0 loss:33.4050 exploreP:0.3712\n",
      "Episode:267 meanR:57.7100 R:92.0 loss:31.1834 exploreP:0.3679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:268 meanR:59.4800 R:228.0 loss:33.2643 exploreP:0.3598\n",
      "Episode:269 meanR:60.8500 R:165.0 loss:38.7922 exploreP:0.3541\n",
      "Episode:270 meanR:61.9200 R:155.0 loss:40.7942 exploreP:0.3488\n",
      "Episode:271 meanR:63.1100 R:151.0 loss:43.6326 exploreP:0.3437\n",
      "Episode:272 meanR:64.8400 R:193.0 loss:45.2881 exploreP:0.3373\n",
      "Episode:273 meanR:65.9400 R:150.0 loss:40.1706 exploreP:0.3325\n",
      "Episode:274 meanR:67.3100 R:191.0 loss:46.0008 exploreP:0.3264\n",
      "Episode:275 meanR:68.3800 R:130.0 loss:47.7376 exploreP:0.3223\n",
      "Episode:276 meanR:70.5400 R:300.0 loss:49.1453 exploreP:0.3131\n",
      "Episode:277 meanR:72.8300 R:258.0 loss:55.5783 exploreP:0.3053\n",
      "Episode:278 meanR:73.4700 R:125.0 loss:59.4734 exploreP:0.3017\n",
      "Episode:279 meanR:73.9400 R:108.0 loss:59.6565 exploreP:0.2985\n",
      "Episode:280 meanR:76.6400 R:283.0 loss:56.1810 exploreP:0.2905\n",
      "Episode:281 meanR:78.4700 R:210.0 loss:55.4755 exploreP:0.2847\n",
      "Episode:282 meanR:80.6400 R:257.0 loss:59.1473 exploreP:0.2777\n",
      "Episode:283 meanR:82.2800 R:187.0 loss:62.9589 exploreP:0.2727\n",
      "Episode:284 meanR:87.0100 R:487.0 loss:64.0570 exploreP:0.2602\n",
      "Episode:285 meanR:88.4400 R:180.0 loss:66.5006 exploreP:0.2558\n",
      "Episode:286 meanR:91.1900 R:335.0 loss:71.2655 exploreP:0.2477\n",
      "Episode:287 meanR:93.9200 R:327.0 loss:65.2898 exploreP:0.2400\n",
      "Episode:288 meanR:95.3100 R:164.0 loss:76.9925 exploreP:0.2363\n",
      "Episode:289 meanR:97.3700 R:218.0 loss:65.4587 exploreP:0.2314\n",
      "Episode:290 meanR:97.5200 R:40.0 loss:70.7037 exploreP:0.2305\n",
      "Episode:291 meanR:99.8300 R:253.0 loss:67.7124 exploreP:0.2250\n",
      "Episode:292 meanR:101.7500 R:253.0 loss:68.5667 exploreP:0.2196\n",
      "Episode:293 meanR:103.4800 R:238.0 loss:72.3806 exploreP:0.2147\n",
      "Episode:294 meanR:106.8500 R:416.0 loss:75.2312 exploreP:0.2064\n",
      "Episode:295 meanR:108.0900 R:177.0 loss:78.5650 exploreP:0.2029\n",
      "Episode:296 meanR:109.6200 R:221.0 loss:87.6110 exploreP:0.1987\n",
      "Episode:297 meanR:110.9400 R:204.0 loss:80.3222 exploreP:0.1949\n",
      "Episode:298 meanR:112.6800 R:229.0 loss:89.1954 exploreP:0.1907\n",
      "Episode:299 meanR:114.4000 R:262.0 loss:84.6552 exploreP:0.1860\n",
      "Episode:300 meanR:116.5500 R:227.0 loss:89.4635 exploreP:0.1821\n",
      "Episode:301 meanR:118.3100 R:204.0 loss:104.7383 exploreP:0.1786\n",
      "Episode:302 meanR:119.9000 R:222.0 loss:106.6154 exploreP:0.1749\n",
      "Episode:303 meanR:121.4100 R:208.0 loss:120.4214 exploreP:0.1715\n",
      "Episode:304 meanR:123.5600 R:281.0 loss:110.7031 exploreP:0.1670\n"
     ]
    }
   ],
   "source": [
    "# Save/load the model and save for plotting\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    total_step = 0 # Explore or exploit parameter\n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(11111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            # Explore (Env) or Exploit (Model)\n",
    "            total_step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * total_step) \n",
    "            if explore_p > np.random.rand():\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = act(sess, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            memory.buffer.append([state, action, next_state, reward, float(done)])\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Training\n",
    "            loss = learn(sess, memory, batch_size)\n",
    "            loss_batch.append(loss)\n",
    "            \n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        episode_reward.append(total_reward)\n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.4f}'.format(np.mean(episode_reward)),\n",
    "              'R:{}'.format(total_reward),\n",
    "              'loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'exploreP:{:.4f}'.format(explore_p))\n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-qn.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-qn.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n",
      "total_reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# Creating a gym env\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# A training graph session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model-qn.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {}'.format(total_reward))\n",
    "# Close the env at the end\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
