{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0') # 200 steps/rewards total\n",
    "env = gym.make('CartPole-v1') # 500 steps/rewards total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "for _ in range(100):\n",
    "    #env.render()\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        rewards = []\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 10000          # max number of episodes to learn from\n",
    "max_steps = 2000                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Aeverage Total reward: 8.00 Total reward: 8.00 Training loss: 1.0883 Explore P: 0.9992\n",
      "Episode: 2 Aeverage Total reward: 15.50 Total reward: 23.00 Training loss: 1.0741 Explore P: 0.9969\n",
      "Episode: 3 Aeverage Total reward: 16.67 Total reward: 19.00 Training loss: 1.1762 Explore P: 0.9951\n",
      "Episode: 4 Aeverage Total reward: 15.75 Total reward: 13.00 Training loss: 1.1562 Explore P: 0.9938\n",
      "Episode: 5 Aeverage Total reward: 14.60 Total reward: 10.00 Training loss: 1.1445 Explore P: 0.9928\n",
      "Episode: 6 Aeverage Total reward: 14.83 Total reward: 16.00 Training loss: 1.0128 Explore P: 0.9912\n",
      "Episode: 7 Aeverage Total reward: 15.00 Total reward: 16.00 Training loss: 1.1579 Explore P: 0.9897\n",
      "Episode: 8 Aeverage Total reward: 15.50 Total reward: 19.00 Training loss: 1.1674 Explore P: 0.9878\n",
      "Episode: 9 Aeverage Total reward: 15.11 Total reward: 12.00 Training loss: 1.1358 Explore P: 0.9866\n",
      "Episode: 10 Aeverage Total reward: 14.80 Total reward: 12.00 Training loss: 1.1286 Explore P: 0.9855\n",
      "Episode: 11 Aeverage Total reward: 15.18 Total reward: 19.00 Training loss: 1.1716 Explore P: 0.9836\n",
      "Episode: 12 Aeverage Total reward: 15.08 Total reward: 14.00 Training loss: 1.0925 Explore P: 0.9822\n",
      "Episode: 13 Aeverage Total reward: 15.00 Total reward: 14.00 Training loss: 1.1135 Explore P: 0.9809\n",
      "Episode: 14 Aeverage Total reward: 15.57 Total reward: 23.00 Training loss: 1.1450 Explore P: 0.9787\n",
      "Episode: 15 Aeverage Total reward: 17.67 Total reward: 47.00 Training loss: 1.3963 Explore P: 0.9741\n",
      "Episode: 16 Aeverage Total reward: 17.81 Total reward: 20.00 Training loss: 1.4595 Explore P: 0.9722\n",
      "Episode: 17 Aeverage Total reward: 19.88 Total reward: 53.00 Training loss: 1.1567 Explore P: 0.9671\n",
      "Episode: 18 Aeverage Total reward: 19.50 Total reward: 13.00 Training loss: 1.6697 Explore P: 0.9659\n",
      "Episode: 19 Aeverage Total reward: 20.42 Total reward: 37.00 Training loss: 1.8998 Explore P: 0.9623\n",
      "Episode: 20 Aeverage Total reward: 21.40 Total reward: 40.00 Training loss: 1.5587 Explore P: 0.9585\n",
      "Episode: 21 Aeverage Total reward: 20.90 Total reward: 11.00 Training loss: 1.6637 Explore P: 0.9575\n",
      "Episode: 22 Aeverage Total reward: 20.82 Total reward: 19.00 Training loss: 2.0730 Explore P: 0.9557\n",
      "Episode: 23 Aeverage Total reward: 20.48 Total reward: 13.00 Training loss: 2.1852 Explore P: 0.9545\n",
      "Episode: 24 Aeverage Total reward: 20.25 Total reward: 15.00 Training loss: 3.2113 Explore P: 0.9530\n",
      "Episode: 25 Aeverage Total reward: 21.48 Total reward: 51.00 Training loss: 5.8163 Explore P: 0.9482\n",
      "Episode: 26 Aeverage Total reward: 20.96 Total reward: 8.00 Training loss: 2.6727 Explore P: 0.9475\n",
      "Episode: 27 Aeverage Total reward: 20.74 Total reward: 15.00 Training loss: 2.5922 Explore P: 0.9461\n",
      "Episode: 28 Aeverage Total reward: 20.36 Total reward: 10.00 Training loss: 2.6914 Explore P: 0.9451\n",
      "Episode: 29 Aeverage Total reward: 20.00 Total reward: 10.00 Training loss: 4.2965 Explore P: 0.9442\n",
      "Episode: 30 Aeverage Total reward: 19.93 Total reward: 18.00 Training loss: 2.4994 Explore P: 0.9425\n",
      "Episode: 31 Aeverage Total reward: 19.68 Total reward: 12.00 Training loss: 3.2229 Explore P: 0.9414\n",
      "Episode: 32 Aeverage Total reward: 19.44 Total reward: 12.00 Training loss: 3.1286 Explore P: 0.9403\n",
      "Episode: 33 Aeverage Total reward: 19.45 Total reward: 20.00 Training loss: 4.1039 Explore P: 0.9384\n",
      "Episode: 34 Aeverage Total reward: 19.53 Total reward: 22.00 Training loss: 10.4944 Explore P: 0.9364\n",
      "Episode: 35 Aeverage Total reward: 19.54 Total reward: 20.00 Training loss: 7.1569 Explore P: 0.9345\n",
      "Episode: 36 Aeverage Total reward: 19.47 Total reward: 17.00 Training loss: 11.8340 Explore P: 0.9330\n",
      "Episode: 37 Aeverage Total reward: 19.30 Total reward: 13.00 Training loss: 6.8885 Explore P: 0.9318\n",
      "Episode: 38 Aeverage Total reward: 19.13 Total reward: 13.00 Training loss: 2.6199 Explore P: 0.9306\n",
      "Episode: 39 Aeverage Total reward: 19.05 Total reward: 16.00 Training loss: 2.9906 Explore P: 0.9291\n",
      "Episode: 40 Aeverage Total reward: 19.27 Total reward: 28.00 Training loss: 5.7700 Explore P: 0.9265\n",
      "Episode: 41 Aeverage Total reward: 19.24 Total reward: 18.00 Training loss: 6.4468 Explore P: 0.9249\n",
      "Episode: 42 Aeverage Total reward: 19.71 Total reward: 39.00 Training loss: 6.4719 Explore P: 0.9213\n",
      "Episode: 43 Aeverage Total reward: 19.63 Total reward: 16.00 Training loss: 4.2767 Explore P: 0.9199\n",
      "Episode: 44 Aeverage Total reward: 20.34 Total reward: 51.00 Training loss: 18.2889 Explore P: 0.9152\n",
      "Episode: 45 Aeverage Total reward: 20.44 Total reward: 25.00 Training loss: 8.3807 Explore P: 0.9130\n",
      "Episode: 46 Aeverage Total reward: 20.50 Total reward: 23.00 Training loss: 22.0120 Explore P: 0.9109\n",
      "Episode: 47 Aeverage Total reward: 20.36 Total reward: 14.00 Training loss: 5.3689 Explore P: 0.9096\n",
      "Episode: 48 Aeverage Total reward: 20.23 Total reward: 14.00 Training loss: 23.4600 Explore P: 0.9084\n",
      "Episode: 49 Aeverage Total reward: 19.96 Total reward: 7.00 Training loss: 8.9890 Explore P: 0.9078\n",
      "Episode: 50 Aeverage Total reward: 19.72 Total reward: 8.00 Training loss: 13.7685 Explore P: 0.9070\n",
      "Episode: 51 Aeverage Total reward: 19.61 Total reward: 14.00 Training loss: 6.7956 Explore P: 0.9058\n",
      "Episode: 52 Aeverage Total reward: 19.54 Total reward: 16.00 Training loss: 4.8330 Explore P: 0.9044\n",
      "Episode: 53 Aeverage Total reward: 19.92 Total reward: 40.00 Training loss: 21.6482 Explore P: 0.9008\n",
      "Episode: 54 Aeverage Total reward: 19.83 Total reward: 15.00 Training loss: 4.9722 Explore P: 0.8995\n",
      "Episode: 55 Aeverage Total reward: 20.00 Total reward: 29.00 Training loss: 37.9983 Explore P: 0.8969\n",
      "Episode: 56 Aeverage Total reward: 19.79 Total reward: 8.00 Training loss: 54.0282 Explore P: 0.8962\n",
      "Episode: 57 Aeverage Total reward: 19.63 Total reward: 11.00 Training loss: 6.1315 Explore P: 0.8952\n",
      "Episode: 58 Aeverage Total reward: 19.59 Total reward: 17.00 Training loss: 14.6964 Explore P: 0.8937\n",
      "Episode: 59 Aeverage Total reward: 19.42 Total reward: 10.00 Training loss: 15.1594 Explore P: 0.8928\n",
      "Episode: 60 Aeverage Total reward: 19.28 Total reward: 11.00 Training loss: 13.2220 Explore P: 0.8918\n",
      "Episode: 61 Aeverage Total reward: 19.23 Total reward: 16.00 Training loss: 7.1104 Explore P: 0.8904\n",
      "Episode: 62 Aeverage Total reward: 19.23 Total reward: 19.00 Training loss: 19.9136 Explore P: 0.8888\n",
      "Episode: 63 Aeverage Total reward: 19.19 Total reward: 17.00 Training loss: 7.5880 Explore P: 0.8873\n",
      "Episode: 64 Aeverage Total reward: 19.09 Total reward: 13.00 Training loss: 63.1039 Explore P: 0.8861\n",
      "Episode: 65 Aeverage Total reward: 19.03 Total reward: 15.00 Training loss: 7.1321 Explore P: 0.8848\n",
      "Episode: 66 Aeverage Total reward: 19.02 Total reward: 18.00 Training loss: 22.4182 Explore P: 0.8832\n",
      "Episode: 67 Aeverage Total reward: 19.06 Total reward: 22.00 Training loss: 5.4441 Explore P: 0.8813\n",
      "Episode: 68 Aeverage Total reward: 19.03 Total reward: 17.00 Training loss: 5.4645 Explore P: 0.8798\n",
      "Episode: 69 Aeverage Total reward: 18.91 Total reward: 11.00 Training loss: 47.8720 Explore P: 0.8789\n",
      "Episode: 70 Aeverage Total reward: 18.81 Total reward: 12.00 Training loss: 45.7173 Explore P: 0.8778\n",
      "Episode: 71 Aeverage Total reward: 18.76 Total reward: 15.00 Training loss: 15.3938 Explore P: 0.8765\n",
      "Episode: 72 Aeverage Total reward: 18.78 Total reward: 20.00 Training loss: 4.6446 Explore P: 0.8748\n",
      "Episode: 73 Aeverage Total reward: 18.68 Total reward: 12.00 Training loss: 28.1556 Explore P: 0.8738\n",
      "Episode: 74 Aeverage Total reward: 18.62 Total reward: 14.00 Training loss: 7.2295 Explore P: 0.8726\n",
      "Episode: 75 Aeverage Total reward: 18.52 Total reward: 11.00 Training loss: 14.3299 Explore P: 0.8716\n",
      "Episode: 76 Aeverage Total reward: 18.50 Total reward: 17.00 Training loss: 5.8273 Explore P: 0.8701\n",
      "Episode: 77 Aeverage Total reward: 18.52 Total reward: 20.00 Training loss: 8.0088 Explore P: 0.8684\n",
      "Episode: 78 Aeverage Total reward: 18.41 Total reward: 10.00 Training loss: 6.3311 Explore P: 0.8676\n",
      "Episode: 79 Aeverage Total reward: 18.35 Total reward: 14.00 Training loss: 6.6585 Explore P: 0.8664\n",
      "Episode: 80 Aeverage Total reward: 18.45 Total reward: 26.00 Training loss: 40.0732 Explore P: 0.8641\n",
      "Episode: 81 Aeverage Total reward: 18.36 Total reward: 11.00 Training loss: 101.3083 Explore P: 0.8632\n",
      "Episode: 82 Aeverage Total reward: 18.26 Total reward: 10.00 Training loss: 37.8490 Explore P: 0.8624\n",
      "Episode: 83 Aeverage Total reward: 18.18 Total reward: 12.00 Training loss: 19.1905 Explore P: 0.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 84 Aeverage Total reward: 18.33 Total reward: 31.00 Training loss: 6.2961 Explore P: 0.8587\n",
      "Episode: 85 Aeverage Total reward: 18.88 Total reward: 65.00 Training loss: 45.6035 Explore P: 0.8532\n",
      "Episode: 86 Aeverage Total reward: 18.81 Total reward: 13.00 Training loss: 6.0026 Explore P: 0.8521\n",
      "Episode: 87 Aeverage Total reward: 19.03 Total reward: 38.00 Training loss: 36.5908 Explore P: 0.8489\n",
      "Episode: 88 Aeverage Total reward: 19.09 Total reward: 24.00 Training loss: 5.0715 Explore P: 0.8469\n",
      "Episode: 89 Aeverage Total reward: 19.11 Total reward: 21.00 Training loss: 53.0401 Explore P: 0.8451\n",
      "Episode: 90 Aeverage Total reward: 18.99 Total reward: 8.00 Training loss: 8.0667 Explore P: 0.8445\n",
      "Episode: 91 Aeverage Total reward: 18.88 Total reward: 9.00 Training loss: 6.0935 Explore P: 0.8437\n",
      "Episode: 92 Aeverage Total reward: 18.77 Total reward: 9.00 Training loss: 108.7181 Explore P: 0.8430\n",
      "Episode: 93 Aeverage Total reward: 18.72 Total reward: 14.00 Training loss: 5.7229 Explore P: 0.8418\n",
      "Episode: 94 Aeverage Total reward: 18.66 Total reward: 13.00 Training loss: 56.2426 Explore P: 0.8407\n",
      "Episode: 95 Aeverage Total reward: 18.61 Total reward: 14.00 Training loss: 52.7500 Explore P: 0.8396\n",
      "Episode: 96 Aeverage Total reward: 18.67 Total reward: 24.00 Training loss: 4.5891 Explore P: 0.8376\n",
      "Episode: 97 Aeverage Total reward: 18.57 Total reward: 9.00 Training loss: 54.8813 Explore P: 0.8368\n",
      "Episode: 98 Aeverage Total reward: 18.52 Total reward: 14.00 Training loss: 80.0126 Explore P: 0.8357\n",
      "Episode: 99 Aeverage Total reward: 18.48 Total reward: 15.00 Training loss: 76.6083 Explore P: 0.8344\n",
      "Episode: 100 Aeverage Total reward: 18.46 Total reward: 16.00 Training loss: 38.5367 Explore P: 0.8331\n",
      "Episode: 101 Aeverage Total reward: 18.59 Total reward: 21.00 Training loss: 29.6201 Explore P: 0.8314\n",
      "Episode: 102 Aeverage Total reward: 18.47 Total reward: 11.00 Training loss: 35.9591 Explore P: 0.8305\n",
      "Episode: 103 Aeverage Total reward: 18.41 Total reward: 13.00 Training loss: 142.6540 Explore P: 0.8294\n",
      "Episode: 104 Aeverage Total reward: 18.42 Total reward: 14.00 Training loss: 4.2185 Explore P: 0.8283\n",
      "Episode: 105 Aeverage Total reward: 18.41 Total reward: 9.00 Training loss: 131.4088 Explore P: 0.8275\n",
      "Episode: 106 Aeverage Total reward: 18.55 Total reward: 30.00 Training loss: 59.0728 Explore P: 0.8251\n",
      "Episode: 107 Aeverage Total reward: 18.60 Total reward: 21.00 Training loss: 44.3839 Explore P: 0.8234\n",
      "Episode: 108 Aeverage Total reward: 18.51 Total reward: 10.00 Training loss: 6.1148 Explore P: 0.8226\n",
      "Episode: 109 Aeverage Total reward: 18.61 Total reward: 22.00 Training loss: 7.5523 Explore P: 0.8208\n",
      "Episode: 110 Aeverage Total reward: 18.57 Total reward: 8.00 Training loss: 6.5859 Explore P: 0.8201\n",
      "Episode: 111 Aeverage Total reward: 18.48 Total reward: 10.00 Training loss: 5.8833 Explore P: 0.8193\n",
      "Episode: 112 Aeverage Total reward: 18.52 Total reward: 18.00 Training loss: 6.1181 Explore P: 0.8179\n",
      "Episode: 113 Aeverage Total reward: 18.53 Total reward: 15.00 Training loss: 5.0992 Explore P: 0.8167\n",
      "Episode: 114 Aeverage Total reward: 18.52 Total reward: 22.00 Training loss: 75.2606 Explore P: 0.8149\n",
      "Episode: 115 Aeverage Total reward: 18.24 Total reward: 19.00 Training loss: 55.2748 Explore P: 0.8134\n",
      "Episode: 116 Aeverage Total reward: 18.14 Total reward: 10.00 Training loss: 96.0820 Explore P: 0.8126\n",
      "Episode: 117 Aeverage Total reward: 17.76 Total reward: 15.00 Training loss: 5.7987 Explore P: 0.8114\n",
      "Episode: 118 Aeverage Total reward: 17.73 Total reward: 10.00 Training loss: 6.4738 Explore P: 0.8106\n",
      "Episode: 119 Aeverage Total reward: 17.58 Total reward: 22.00 Training loss: 40.8549 Explore P: 0.8088\n",
      "Episode: 120 Aeverage Total reward: 17.31 Total reward: 13.00 Training loss: 4.3048 Explore P: 0.8078\n",
      "Episode: 121 Aeverage Total reward: 17.28 Total reward: 8.00 Training loss: 6.1991 Explore P: 0.8071\n",
      "Episode: 122 Aeverage Total reward: 17.18 Total reward: 9.00 Training loss: 101.4338 Explore P: 0.8064\n",
      "Episode: 123 Aeverage Total reward: 17.20 Total reward: 15.00 Training loss: 5.5322 Explore P: 0.8052\n",
      "Episode: 124 Aeverage Total reward: 17.16 Total reward: 11.00 Training loss: 52.9173 Explore P: 0.8043\n",
      "Episode: 125 Aeverage Total reward: 16.77 Total reward: 12.00 Training loss: 5.4701 Explore P: 0.8034\n",
      "Episode: 126 Aeverage Total reward: 16.80 Total reward: 11.00 Training loss: 3.6423 Explore P: 0.8025\n",
      "Episode: 127 Aeverage Total reward: 16.84 Total reward: 19.00 Training loss: 90.7020 Explore P: 0.8010\n",
      "Episode: 128 Aeverage Total reward: 16.87 Total reward: 13.00 Training loss: 47.8502 Explore P: 0.8000\n",
      "Episode: 129 Aeverage Total reward: 16.89 Total reward: 12.00 Training loss: 34.7286 Explore P: 0.7990\n",
      "Episode: 130 Aeverage Total reward: 16.81 Total reward: 10.00 Training loss: 55.6096 Explore P: 0.7982\n",
      "Episode: 131 Aeverage Total reward: 16.82 Total reward: 13.00 Training loss: 6.6838 Explore P: 0.7972\n",
      "Episode: 132 Aeverage Total reward: 16.83 Total reward: 13.00 Training loss: 6.7070 Explore P: 0.7962\n",
      "Episode: 133 Aeverage Total reward: 16.80 Total reward: 17.00 Training loss: 73.8681 Explore P: 0.7949\n",
      "Episode: 134 Aeverage Total reward: 16.68 Total reward: 10.00 Training loss: 198.5762 Explore P: 0.7941\n",
      "Episode: 135 Aeverage Total reward: 16.59 Total reward: 11.00 Training loss: 153.6526 Explore P: 0.7932\n",
      "Episode: 136 Aeverage Total reward: 16.54 Total reward: 12.00 Training loss: 50.0684 Explore P: 0.7923\n",
      "Episode: 137 Aeverage Total reward: 16.49 Total reward: 8.00 Training loss: 57.1058 Explore P: 0.7916\n",
      "Episode: 138 Aeverage Total reward: 16.55 Total reward: 19.00 Training loss: 6.3002 Explore P: 0.7902\n",
      "Episode: 139 Aeverage Total reward: 16.50 Total reward: 11.00 Training loss: 5.3906 Explore P: 0.7893\n",
      "Episode: 140 Aeverage Total reward: 16.36 Total reward: 14.00 Training loss: 39.7648 Explore P: 0.7882\n",
      "Episode: 141 Aeverage Total reward: 16.28 Total reward: 10.00 Training loss: 75.7392 Explore P: 0.7874\n",
      "Episode: 142 Aeverage Total reward: 16.06 Total reward: 17.00 Training loss: 63.2550 Explore P: 0.7861\n",
      "Episode: 143 Aeverage Total reward: 16.01 Total reward: 11.00 Training loss: 38.2942 Explore P: 0.7853\n",
      "Episode: 144 Aeverage Total reward: 15.58 Total reward: 8.00 Training loss: 4.9376 Explore P: 0.7846\n",
      "Episode: 145 Aeverage Total reward: 15.55 Total reward: 22.00 Training loss: 72.2700 Explore P: 0.7829\n",
      "Episode: 146 Aeverage Total reward: 15.46 Total reward: 14.00 Training loss: 5.3286 Explore P: 0.7819\n",
      "Episode: 147 Aeverage Total reward: 15.49 Total reward: 17.00 Training loss: 6.2039 Explore P: 0.7806\n",
      "Episode: 148 Aeverage Total reward: 15.64 Total reward: 29.00 Training loss: 4.2407 Explore P: 0.7783\n",
      "Episode: 149 Aeverage Total reward: 15.69 Total reward: 12.00 Training loss: 5.3721 Explore P: 0.7774\n",
      "Episode: 150 Aeverage Total reward: 15.73 Total reward: 12.00 Training loss: 6.4126 Explore P: 0.7765\n",
      "Episode: 151 Aeverage Total reward: 15.68 Total reward: 9.00 Training loss: 84.4244 Explore P: 0.7758\n",
      "Episode: 152 Aeverage Total reward: 15.62 Total reward: 10.00 Training loss: 5.2681 Explore P: 0.7750\n",
      "Episode: 153 Aeverage Total reward: 15.46 Total reward: 24.00 Training loss: 5.2972 Explore P: 0.7732\n",
      "Episode: 154 Aeverage Total reward: 15.54 Total reward: 23.00 Training loss: 52.1766 Explore P: 0.7714\n",
      "Episode: 155 Aeverage Total reward: 15.38 Total reward: 13.00 Training loss: 248.1994 Explore P: 0.7704\n",
      "Episode: 156 Aeverage Total reward: 15.55 Total reward: 25.00 Training loss: 96.1853 Explore P: 0.7685\n",
      "Episode: 157 Aeverage Total reward: 15.55 Total reward: 11.00 Training loss: 2.8758 Explore P: 0.7677\n",
      "Episode: 158 Aeverage Total reward: 15.52 Total reward: 14.00 Training loss: 220.6586 Explore P: 0.7667\n",
      "Episode: 159 Aeverage Total reward: 15.54 Total reward: 12.00 Training loss: 97.2966 Explore P: 0.7657\n",
      "Episode: 160 Aeverage Total reward: 15.56 Total reward: 13.00 Training loss: 166.2140 Explore P: 0.7648\n",
      "Episode: 161 Aeverage Total reward: 15.52 Total reward: 12.00 Training loss: 60.3163 Explore P: 0.7639\n",
      "Episode: 162 Aeverage Total reward: 15.50 Total reward: 17.00 Training loss: 164.9697 Explore P: 0.7626\n",
      "Episode: 163 Aeverage Total reward: 15.59 Total reward: 26.00 Training loss: 83.9622 Explore P: 0.7606\n",
      "Episode: 164 Aeverage Total reward: 15.59 Total reward: 13.00 Training loss: 72.7803 Explore P: 0.7596\n",
      "Episode: 165 Aeverage Total reward: 15.53 Total reward: 9.00 Training loss: 79.3866 Explore P: 0.7590\n",
      "Episode: 166 Aeverage Total reward: 15.49 Total reward: 14.00 Training loss: 4.1467 Explore P: 0.7579\n",
      "Episode: 167 Aeverage Total reward: 15.45 Total reward: 18.00 Training loss: 57.1085 Explore P: 0.7566\n",
      "Episode: 168 Aeverage Total reward: 15.45 Total reward: 17.00 Training loss: 5.1214 Explore P: 0.7553\n",
      "Episode: 169 Aeverage Total reward: 15.47 Total reward: 13.00 Training loss: 6.3288 Explore P: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 170 Aeverage Total reward: 15.55 Total reward: 20.00 Training loss: 137.6290 Explore P: 0.7529\n",
      "Episode: 171 Aeverage Total reward: 15.55 Total reward: 15.00 Training loss: 35.9432 Explore P: 0.7517\n",
      "Episode: 172 Aeverage Total reward: 15.58 Total reward: 23.00 Training loss: 6.0441 Explore P: 0.7500\n",
      "Episode: 173 Aeverage Total reward: 15.62 Total reward: 16.00 Training loss: 55.0378 Explore P: 0.7489\n",
      "Episode: 174 Aeverage Total reward: 15.59 Total reward: 11.00 Training loss: 49.7355 Explore P: 0.7480\n",
      "Episode: 175 Aeverage Total reward: 15.59 Total reward: 11.00 Training loss: 37.4032 Explore P: 0.7472\n",
      "Episode: 176 Aeverage Total reward: 15.52 Total reward: 10.00 Training loss: 4.5394 Explore P: 0.7465\n",
      "Episode: 177 Aeverage Total reward: 15.48 Total reward: 16.00 Training loss: 4.1037 Explore P: 0.7453\n",
      "Episode: 178 Aeverage Total reward: 15.49 Total reward: 11.00 Training loss: 39.5486 Explore P: 0.7445\n",
      "Episode: 179 Aeverage Total reward: 15.48 Total reward: 13.00 Training loss: 3.6774 Explore P: 0.7436\n",
      "Episode: 180 Aeverage Total reward: 15.34 Total reward: 12.00 Training loss: 120.3331 Explore P: 0.7427\n",
      "Episode: 181 Aeverage Total reward: 15.33 Total reward: 10.00 Training loss: 3.7821 Explore P: 0.7419\n",
      "Episode: 182 Aeverage Total reward: 15.31 Total reward: 8.00 Training loss: 37.8996 Explore P: 0.7414\n",
      "Episode: 183 Aeverage Total reward: 15.35 Total reward: 16.00 Training loss: 3.8898 Explore P: 0.7402\n",
      "Episode: 184 Aeverage Total reward: 15.32 Total reward: 28.00 Training loss: 66.8868 Explore P: 0.7381\n",
      "Episode: 185 Aeverage Total reward: 14.84 Total reward: 17.00 Training loss: 81.0558 Explore P: 0.7369\n",
      "Episode: 186 Aeverage Total reward: 14.90 Total reward: 19.00 Training loss: 40.7106 Explore P: 0.7355\n",
      "Episode: 187 Aeverage Total reward: 14.67 Total reward: 15.00 Training loss: 35.5149 Explore P: 0.7344\n",
      "Episode: 188 Aeverage Total reward: 14.58 Total reward: 15.00 Training loss: 6.2078 Explore P: 0.7334\n",
      "Episode: 189 Aeverage Total reward: 14.50 Total reward: 13.00 Training loss: 89.8512 Explore P: 0.7324\n",
      "Episode: 190 Aeverage Total reward: 14.51 Total reward: 9.00 Training loss: 3.2338 Explore P: 0.7318\n",
      "Episode: 191 Aeverage Total reward: 14.63 Total reward: 21.00 Training loss: 144.9912 Explore P: 0.7303\n",
      "Episode: 192 Aeverage Total reward: 14.66 Total reward: 12.00 Training loss: 3.4381 Explore P: 0.7294\n",
      "Episode: 193 Aeverage Total reward: 14.64 Total reward: 12.00 Training loss: 112.1534 Explore P: 0.7285\n",
      "Episode: 194 Aeverage Total reward: 14.79 Total reward: 28.00 Training loss: 176.7408 Explore P: 0.7265\n",
      "Episode: 195 Aeverage Total reward: 14.88 Total reward: 23.00 Training loss: 98.2949 Explore P: 0.7249\n",
      "Episode: 196 Aeverage Total reward: 14.77 Total reward: 13.00 Training loss: 3.1411 Explore P: 0.7239\n",
      "Episode: 197 Aeverage Total reward: 14.83 Total reward: 15.00 Training loss: 4.3979 Explore P: 0.7229\n",
      "Episode: 198 Aeverage Total reward: 14.78 Total reward: 9.00 Training loss: 2.5538 Explore P: 0.7222\n",
      "Episode: 199 Aeverage Total reward: 14.73 Total reward: 10.00 Training loss: 125.2829 Explore P: 0.7215\n",
      "Episode: 200 Aeverage Total reward: 14.67 Total reward: 10.00 Training loss: 48.5456 Explore P: 0.7208\n",
      "Episode: 201 Aeverage Total reward: 14.69 Total reward: 23.00 Training loss: 104.7929 Explore P: 0.7192\n",
      "Episode: 202 Aeverage Total reward: 14.75 Total reward: 17.00 Training loss: 44.0931 Explore P: 0.7180\n",
      "Episode: 203 Aeverage Total reward: 14.83 Total reward: 21.00 Training loss: 51.1298 Explore P: 0.7165\n",
      "Episode: 204 Aeverage Total reward: 14.87 Total reward: 18.00 Training loss: 100.7022 Explore P: 0.7152\n",
      "Episode: 205 Aeverage Total reward: 14.98 Total reward: 20.00 Training loss: 64.5465 Explore P: 0.7138\n",
      "Episode: 206 Aeverage Total reward: 14.81 Total reward: 13.00 Training loss: 3.7710 Explore P: 0.7129\n",
      "Episode: 207 Aeverage Total reward: 14.76 Total reward: 16.00 Training loss: 47.4687 Explore P: 0.7118\n",
      "Episode: 208 Aeverage Total reward: 14.96 Total reward: 30.00 Training loss: 28.2996 Explore P: 0.7097\n",
      "Episode: 209 Aeverage Total reward: 14.84 Total reward: 10.00 Training loss: 166.4427 Explore P: 0.7090\n",
      "Episode: 210 Aeverage Total reward: 14.96 Total reward: 20.00 Training loss: 149.6597 Explore P: 0.7076\n",
      "Episode: 211 Aeverage Total reward: 15.05 Total reward: 19.00 Training loss: 30.7144 Explore P: 0.7062\n",
      "Episode: 212 Aeverage Total reward: 15.09 Total reward: 22.00 Training loss: 200.0695 Explore P: 0.7047\n",
      "Episode: 213 Aeverage Total reward: 15.10 Total reward: 16.00 Training loss: 2.3539 Explore P: 0.7036\n",
      "Episode: 214 Aeverage Total reward: 14.99 Total reward: 11.00 Training loss: 2.8525 Explore P: 0.7028\n",
      "Episode: 215 Aeverage Total reward: 15.11 Total reward: 31.00 Training loss: 35.3474 Explore P: 0.7007\n",
      "Episode: 216 Aeverage Total reward: 15.13 Total reward: 12.00 Training loss: 39.0287 Explore P: 0.6999\n",
      "Episode: 217 Aeverage Total reward: 15.07 Total reward: 9.00 Training loss: 27.2523 Explore P: 0.6993\n",
      "Episode: 218 Aeverage Total reward: 15.10 Total reward: 13.00 Training loss: 82.0962 Explore P: 0.6984\n",
      "Episode: 219 Aeverage Total reward: 15.02 Total reward: 14.00 Training loss: 47.8082 Explore P: 0.6974\n",
      "Episode: 220 Aeverage Total reward: 15.00 Total reward: 11.00 Training loss: 87.3371 Explore P: 0.6966\n",
      "Episode: 221 Aeverage Total reward: 15.02 Total reward: 10.00 Training loss: 33.7544 Explore P: 0.6960\n",
      "Episode: 222 Aeverage Total reward: 15.08 Total reward: 15.00 Training loss: 99.5231 Explore P: 0.6949\n",
      "Episode: 223 Aeverage Total reward: 15.05 Total reward: 12.00 Training loss: 132.3338 Explore P: 0.6941\n",
      "Episode: 224 Aeverage Total reward: 15.11 Total reward: 17.00 Training loss: 27.7257 Explore P: 0.6929\n",
      "Episode: 225 Aeverage Total reward: 15.09 Total reward: 10.00 Training loss: 39.1947 Explore P: 0.6923\n",
      "Episode: 226 Aeverage Total reward: 15.09 Total reward: 11.00 Training loss: 32.2883 Explore P: 0.6915\n",
      "Episode: 227 Aeverage Total reward: 15.00 Total reward: 10.00 Training loss: 31.6539 Explore P: 0.6908\n",
      "Episode: 228 Aeverage Total reward: 15.00 Total reward: 13.00 Training loss: 1.6294 Explore P: 0.6899\n",
      "Episode: 229 Aeverage Total reward: 15.11 Total reward: 23.00 Training loss: 72.6266 Explore P: 0.6884\n",
      "Episode: 230 Aeverage Total reward: 15.10 Total reward: 9.00 Training loss: 77.5018 Explore P: 0.6878\n",
      "Episode: 231 Aeverage Total reward: 15.07 Total reward: 10.00 Training loss: 56.7638 Explore P: 0.6871\n",
      "Episode: 232 Aeverage Total reward: 15.26 Total reward: 32.00 Training loss: 1.8686 Explore P: 0.6849\n",
      "Episode: 233 Aeverage Total reward: 15.25 Total reward: 16.00 Training loss: 1.5269 Explore P: 0.6838\n",
      "Episode: 234 Aeverage Total reward: 15.32 Total reward: 17.00 Training loss: 66.4952 Explore P: 0.6827\n",
      "Episode: 235 Aeverage Total reward: 15.33 Total reward: 12.00 Training loss: 27.1540 Explore P: 0.6819\n",
      "Episode: 236 Aeverage Total reward: 15.35 Total reward: 14.00 Training loss: 28.3082 Explore P: 0.6810\n",
      "Episode: 237 Aeverage Total reward: 15.41 Total reward: 14.00 Training loss: 60.4726 Explore P: 0.6800\n",
      "Episode: 238 Aeverage Total reward: 15.41 Total reward: 19.00 Training loss: 1.7924 Explore P: 0.6787\n",
      "Episode: 239 Aeverage Total reward: 15.39 Total reward: 9.00 Training loss: 1.5799 Explore P: 0.6781\n",
      "Episode: 240 Aeverage Total reward: 15.35 Total reward: 10.00 Training loss: 30.9997 Explore P: 0.6775\n",
      "Episode: 241 Aeverage Total reward: 15.35 Total reward: 10.00 Training loss: 86.2112 Explore P: 0.6768\n",
      "Episode: 242 Aeverage Total reward: 15.36 Total reward: 18.00 Training loss: 60.4136 Explore P: 0.6756\n",
      "Episode: 243 Aeverage Total reward: 15.36 Total reward: 11.00 Training loss: 76.2611 Explore P: 0.6749\n",
      "Episode: 244 Aeverage Total reward: 15.47 Total reward: 19.00 Training loss: 63.3664 Explore P: 0.6736\n",
      "Episode: 245 Aeverage Total reward: 15.38 Total reward: 13.00 Training loss: 28.7177 Explore P: 0.6728\n",
      "Episode: 246 Aeverage Total reward: 15.37 Total reward: 13.00 Training loss: 31.0723 Explore P: 0.6719\n",
      "Episode: 247 Aeverage Total reward: 15.41 Total reward: 21.00 Training loss: 29.6602 Explore P: 0.6705\n",
      "Episode: 248 Aeverage Total reward: 15.21 Total reward: 9.00 Training loss: 51.5185 Explore P: 0.6699\n",
      "Episode: 249 Aeverage Total reward: 15.20 Total reward: 11.00 Training loss: 29.9226 Explore P: 0.6692\n",
      "Episode: 250 Aeverage Total reward: 15.17 Total reward: 9.00 Training loss: 56.6427 Explore P: 0.6686\n",
      "Episode: 251 Aeverage Total reward: 15.17 Total reward: 9.00 Training loss: 1.1244 Explore P: 0.6680\n",
      "Episode: 252 Aeverage Total reward: 15.20 Total reward: 13.00 Training loss: 52.7123 Explore P: 0.6671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 253 Aeverage Total reward: 15.04 Total reward: 8.00 Training loss: 75.3359 Explore P: 0.6666\n",
      "Episode: 254 Aeverage Total reward: 14.92 Total reward: 11.00 Training loss: 1.1454 Explore P: 0.6659\n",
      "Episode: 255 Aeverage Total reward: 14.96 Total reward: 17.00 Training loss: 22.5462 Explore P: 0.6648\n",
      "Episode: 256 Aeverage Total reward: 14.81 Total reward: 10.00 Training loss: 31.9512 Explore P: 0.6641\n",
      "Episode: 257 Aeverage Total reward: 14.91 Total reward: 21.00 Training loss: 1.1991 Explore P: 0.6628\n",
      "Episode: 258 Aeverage Total reward: 14.90 Total reward: 13.00 Training loss: 25.8224 Explore P: 0.6619\n",
      "Episode: 259 Aeverage Total reward: 14.89 Total reward: 11.00 Training loss: 45.6379 Explore P: 0.6612\n",
      "Episode: 260 Aeverage Total reward: 14.92 Total reward: 16.00 Training loss: 119.8807 Explore P: 0.6602\n",
      "Episode: 261 Aeverage Total reward: 14.94 Total reward: 14.00 Training loss: 23.0528 Explore P: 0.6592\n",
      "Episode: 262 Aeverage Total reward: 14.89 Total reward: 12.00 Training loss: 22.2143 Explore P: 0.6585\n",
      "Episode: 263 Aeverage Total reward: 14.73 Total reward: 10.00 Training loss: 80.5206 Explore P: 0.6578\n",
      "Episode: 264 Aeverage Total reward: 14.69 Total reward: 9.00 Training loss: 46.8885 Explore P: 0.6572\n",
      "Episode: 265 Aeverage Total reward: 14.70 Total reward: 10.00 Training loss: 44.8624 Explore P: 0.6566\n",
      "Episode: 266 Aeverage Total reward: 14.65 Total reward: 9.00 Training loss: 26.3328 Explore P: 0.6560\n",
      "Episode: 267 Aeverage Total reward: 14.59 Total reward: 12.00 Training loss: 23.7463 Explore P: 0.6552\n",
      "Episode: 268 Aeverage Total reward: 14.53 Total reward: 11.00 Training loss: 0.8316 Explore P: 0.6545\n",
      "Episode: 269 Aeverage Total reward: 14.50 Total reward: 10.00 Training loss: 0.8449 Explore P: 0.6539\n",
      "Episode: 270 Aeverage Total reward: 14.44 Total reward: 14.00 Training loss: 47.1851 Explore P: 0.6530\n",
      "Episode: 271 Aeverage Total reward: 14.41 Total reward: 12.00 Training loss: 1.0414 Explore P: 0.6522\n",
      "Episode: 272 Aeverage Total reward: 14.26 Total reward: 8.00 Training loss: 0.8430 Explore P: 0.6517\n",
      "Episode: 273 Aeverage Total reward: 14.21 Total reward: 11.00 Training loss: 0.8513 Explore P: 0.6510\n",
      "Episode: 274 Aeverage Total reward: 14.19 Total reward: 9.00 Training loss: 42.5901 Explore P: 0.6504\n",
      "Episode: 275 Aeverage Total reward: 14.17 Total reward: 9.00 Training loss: 68.8157 Explore P: 0.6498\n",
      "Episode: 276 Aeverage Total reward: 14.18 Total reward: 11.00 Training loss: 24.6319 Explore P: 0.6491\n",
      "Episode: 277 Aeverage Total reward: 14.10 Total reward: 8.00 Training loss: 17.3514 Explore P: 0.6486\n",
      "Episode: 278 Aeverage Total reward: 14.08 Total reward: 9.00 Training loss: 62.8955 Explore P: 0.6480\n",
      "Episode: 279 Aeverage Total reward: 14.08 Total reward: 13.00 Training loss: 74.2658 Explore P: 0.6472\n",
      "Episode: 280 Aeverage Total reward: 14.07 Total reward: 11.00 Training loss: 0.9345 Explore P: 0.6465\n",
      "Episode: 281 Aeverage Total reward: 14.09 Total reward: 12.00 Training loss: 37.6664 Explore P: 0.6457\n",
      "Episode: 282 Aeverage Total reward: 14.11 Total reward: 10.00 Training loss: 19.3064 Explore P: 0.6451\n",
      "Episode: 283 Aeverage Total reward: 14.03 Total reward: 8.00 Training loss: 19.5830 Explore P: 0.6446\n",
      "Episode: 284 Aeverage Total reward: 13.84 Total reward: 9.00 Training loss: 40.9771 Explore P: 0.6440\n",
      "Episode: 285 Aeverage Total reward: 13.76 Total reward: 9.00 Training loss: 43.2613 Explore P: 0.6435\n",
      "Episode: 286 Aeverage Total reward: 13.75 Total reward: 18.00 Training loss: 19.8404 Explore P: 0.6423\n",
      "Episode: 287 Aeverage Total reward: 13.83 Total reward: 23.00 Training loss: 0.9007 Explore P: 0.6409\n",
      "Episode: 288 Aeverage Total reward: 13.89 Total reward: 21.00 Training loss: 19.9719 Explore P: 0.6395\n",
      "Episode: 289 Aeverage Total reward: 13.86 Total reward: 10.00 Training loss: 40.9183 Explore P: 0.6389\n",
      "Episode: 290 Aeverage Total reward: 13.95 Total reward: 18.00 Training loss: 35.9686 Explore P: 0.6378\n",
      "Episode: 291 Aeverage Total reward: 13.85 Total reward: 11.00 Training loss: 54.7506 Explore P: 0.6371\n",
      "Episode: 292 Aeverage Total reward: 13.89 Total reward: 16.00 Training loss: 0.6664 Explore P: 0.6361\n",
      "Episode: 293 Aeverage Total reward: 13.88 Total reward: 11.00 Training loss: 58.8230 Explore P: 0.6354\n",
      "Episode: 294 Aeverage Total reward: 13.69 Total reward: 9.00 Training loss: 18.0310 Explore P: 0.6348\n",
      "Episode: 295 Aeverage Total reward: 13.59 Total reward: 13.00 Training loss: 17.5046 Explore P: 0.6340\n",
      "Episode: 296 Aeverage Total reward: 13.69 Total reward: 23.00 Training loss: 0.8437 Explore P: 0.6326\n",
      "Episode: 297 Aeverage Total reward: 13.70 Total reward: 16.00 Training loss: 49.5558 Explore P: 0.6316\n",
      "Episode: 298 Aeverage Total reward: 13.72 Total reward: 11.00 Training loss: 18.0345 Explore P: 0.6309\n",
      "Episode: 299 Aeverage Total reward: 13.75 Total reward: 13.00 Training loss: 17.1871 Explore P: 0.6301\n",
      "Episode: 300 Aeverage Total reward: 13.74 Total reward: 9.00 Training loss: 16.3372 Explore P: 0.6296\n",
      "Episode: 301 Aeverage Total reward: 13.79 Total reward: 28.00 Training loss: 16.9059 Explore P: 0.6278\n",
      "Episode: 302 Aeverage Total reward: 13.78 Total reward: 16.00 Training loss: 34.3648 Explore P: 0.6268\n",
      "Episode: 303 Aeverage Total reward: 13.70 Total reward: 13.00 Training loss: 17.1183 Explore P: 0.6260\n",
      "Episode: 304 Aeverage Total reward: 13.63 Total reward: 11.00 Training loss: 33.7655 Explore P: 0.6254\n",
      "Episode: 305 Aeverage Total reward: 13.53 Total reward: 10.00 Training loss: 17.4677 Explore P: 0.6247\n",
      "Episode: 306 Aeverage Total reward: 13.57 Total reward: 17.00 Training loss: 0.8935 Explore P: 0.6237\n",
      "Episode: 307 Aeverage Total reward: 13.87 Total reward: 46.00 Training loss: 17.0670 Explore P: 0.6209\n",
      "Episode: 308 Aeverage Total reward: 13.77 Total reward: 20.00 Training loss: 14.5140 Explore P: 0.6197\n",
      "Episode: 309 Aeverage Total reward: 13.98 Total reward: 31.00 Training loss: 1.0462 Explore P: 0.6178\n",
      "Episode: 310 Aeverage Total reward: 14.12 Total reward: 34.00 Training loss: 60.8529 Explore P: 0.6157\n",
      "Episode: 311 Aeverage Total reward: 14.06 Total reward: 13.00 Training loss: 20.1095 Explore P: 0.6149\n",
      "Episode: 312 Aeverage Total reward: 13.94 Total reward: 10.00 Training loss: 28.6100 Explore P: 0.6143\n",
      "Episode: 313 Aeverage Total reward: 14.21 Total reward: 43.00 Training loss: 1.5009 Explore P: 0.6117\n",
      "Episode: 314 Aeverage Total reward: 14.20 Total reward: 10.00 Training loss: 1.1888 Explore P: 0.6111\n",
      "Episode: 315 Aeverage Total reward: 14.00 Total reward: 11.00 Training loss: 15.7938 Explore P: 0.6105\n",
      "Episode: 316 Aeverage Total reward: 14.06 Total reward: 18.00 Training loss: 31.9497 Explore P: 0.6094\n",
      "Episode: 317 Aeverage Total reward: 14.17 Total reward: 20.00 Training loss: 29.8834 Explore P: 0.6082\n",
      "Episode: 318 Aeverage Total reward: 14.45 Total reward: 41.00 Training loss: 18.1874 Explore P: 0.6057\n",
      "Episode: 319 Aeverage Total reward: 14.42 Total reward: 11.00 Training loss: 14.4841 Explore P: 0.6051\n",
      "Episode: 320 Aeverage Total reward: 14.45 Total reward: 14.00 Training loss: 30.8797 Explore P: 0.6043\n",
      "Episode: 321 Aeverage Total reward: 14.60 Total reward: 25.00 Training loss: 15.2975 Explore P: 0.6028\n",
      "Episode: 322 Aeverage Total reward: 14.75 Total reward: 30.00 Training loss: 85.3111 Explore P: 0.6010\n",
      "Episode: 323 Aeverage Total reward: 14.78 Total reward: 15.00 Training loss: 0.9336 Explore P: 0.6001\n",
      "Episode: 324 Aeverage Total reward: 14.81 Total reward: 20.00 Training loss: 18.0598 Explore P: 0.5989\n",
      "Episode: 325 Aeverage Total reward: 15.12 Total reward: 41.00 Training loss: 13.6717 Explore P: 0.5965\n",
      "Episode: 326 Aeverage Total reward: 15.31 Total reward: 30.00 Training loss: 28.6705 Explore P: 0.5948\n",
      "Episode: 327 Aeverage Total reward: 15.47 Total reward: 26.00 Training loss: 14.7242 Explore P: 0.5932\n",
      "Episode: 328 Aeverage Total reward: 15.54 Total reward: 20.00 Training loss: 29.7224 Explore P: 0.5921\n",
      "Episode: 329 Aeverage Total reward: 15.46 Total reward: 15.00 Training loss: 13.9583 Explore P: 0.5912\n",
      "Episode: 330 Aeverage Total reward: 15.77 Total reward: 40.00 Training loss: 12.4817 Explore P: 0.5889\n",
      "Episode: 331 Aeverage Total reward: 15.85 Total reward: 18.00 Training loss: 12.3138 Explore P: 0.5878\n",
      "Episode: 332 Aeverage Total reward: 15.64 Total reward: 11.00 Training loss: 1.0878 Explore P: 0.5872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 333 Aeverage Total reward: 15.88 Total reward: 40.00 Training loss: 1.1649 Explore P: 0.5849\n",
      "Episode: 334 Aeverage Total reward: 16.11 Total reward: 40.00 Training loss: 24.9050 Explore P: 0.5826\n",
      "Episode: 335 Aeverage Total reward: 16.16 Total reward: 17.00 Training loss: 14.5008 Explore P: 0.5816\n",
      "Episode: 336 Aeverage Total reward: 16.16 Total reward: 14.00 Training loss: 1.4969 Explore P: 0.5808\n",
      "Episode: 337 Aeverage Total reward: 16.38 Total reward: 36.00 Training loss: 13.4476 Explore P: 0.5788\n",
      "Episode: 338 Aeverage Total reward: 16.38 Total reward: 19.00 Training loss: 12.8081 Explore P: 0.5777\n",
      "Episode: 339 Aeverage Total reward: 16.44 Total reward: 15.00 Training loss: 13.2357 Explore P: 0.5769\n",
      "Episode: 340 Aeverage Total reward: 16.54 Total reward: 20.00 Training loss: 1.1931 Explore P: 0.5757\n",
      "Episode: 341 Aeverage Total reward: 16.60 Total reward: 16.00 Training loss: 1.0858 Explore P: 0.5748\n",
      "Episode: 342 Aeverage Total reward: 16.69 Total reward: 27.00 Training loss: 1.2297 Explore P: 0.5733\n",
      "Episode: 343 Aeverage Total reward: 16.77 Total reward: 19.00 Training loss: 1.1481 Explore P: 0.5722\n",
      "Episode: 344 Aeverage Total reward: 17.10 Total reward: 52.00 Training loss: 1.2583 Explore P: 0.5693\n",
      "Episode: 345 Aeverage Total reward: 17.27 Total reward: 30.00 Training loss: 36.6254 Explore P: 0.5676\n",
      "Episode: 346 Aeverage Total reward: 17.30 Total reward: 16.00 Training loss: 25.4733 Explore P: 0.5667\n",
      "Episode: 347 Aeverage Total reward: 17.24 Total reward: 15.00 Training loss: 13.8406 Explore P: 0.5659\n",
      "Episode: 348 Aeverage Total reward: 17.34 Total reward: 19.00 Training loss: 26.7457 Explore P: 0.5649\n",
      "Episode: 349 Aeverage Total reward: 17.51 Total reward: 28.00 Training loss: 1.3393 Explore P: 0.5633\n",
      "Episode: 350 Aeverage Total reward: 17.68 Total reward: 26.00 Training loss: 13.2457 Explore P: 0.5619\n",
      "Episode: 351 Aeverage Total reward: 17.79 Total reward: 20.00 Training loss: 0.9838 Explore P: 0.5608\n",
      "Episode: 352 Aeverage Total reward: 18.08 Total reward: 42.00 Training loss: 24.2870 Explore P: 0.5585\n",
      "Episode: 353 Aeverage Total reward: 18.19 Total reward: 19.00 Training loss: 11.9766 Explore P: 0.5574\n",
      "Episode: 354 Aeverage Total reward: 18.35 Total reward: 27.00 Training loss: 11.5892 Explore P: 0.5559\n",
      "Episode: 355 Aeverage Total reward: 18.60 Total reward: 42.00 Training loss: 23.3306 Explore P: 0.5536\n",
      "Episode: 356 Aeverage Total reward: 18.61 Total reward: 11.00 Training loss: 11.5623 Explore P: 0.5531\n",
      "Episode: 357 Aeverage Total reward: 18.51 Total reward: 11.00 Training loss: 0.7832 Explore P: 0.5525\n",
      "Episode: 358 Aeverage Total reward: 18.67 Total reward: 29.00 Training loss: 1.0185 Explore P: 0.5509\n",
      "Episode: 359 Aeverage Total reward: 19.56 Total reward: 100.00 Training loss: 12.6896 Explore P: 0.5455\n",
      "Episode: 360 Aeverage Total reward: 19.60 Total reward: 20.00 Training loss: 1.0664 Explore P: 0.5444\n",
      "Episode: 361 Aeverage Total reward: 19.67 Total reward: 21.00 Training loss: 1.7498 Explore P: 0.5433\n",
      "Episode: 362 Aeverage Total reward: 19.87 Total reward: 32.00 Training loss: 24.5098 Explore P: 0.5416\n",
      "Episode: 363 Aeverage Total reward: 20.15 Total reward: 38.00 Training loss: 58.7979 Explore P: 0.5396\n",
      "Episode: 364 Aeverage Total reward: 20.22 Total reward: 16.00 Training loss: 14.0234 Explore P: 0.5387\n",
      "Episode: 365 Aeverage Total reward: 20.32 Total reward: 20.00 Training loss: 1.3814 Explore P: 0.5377\n",
      "Episode: 366 Aeverage Total reward: 20.42 Total reward: 19.00 Training loss: 1.6520 Explore P: 0.5367\n",
      "Episode: 367 Aeverage Total reward: 20.63 Total reward: 33.00 Training loss: 1.0261 Explore P: 0.5350\n",
      "Episode: 368 Aeverage Total reward: 21.12 Total reward: 60.00 Training loss: 11.2187 Explore P: 0.5318\n",
      "Episode: 369 Aeverage Total reward: 21.32 Total reward: 30.00 Training loss: 1.0145 Explore P: 0.5302\n",
      "Episode: 370 Aeverage Total reward: 21.35 Total reward: 17.00 Training loss: 13.8151 Explore P: 0.5294\n",
      "Episode: 371 Aeverage Total reward: 21.54 Total reward: 31.00 Training loss: 15.2138 Explore P: 0.5278\n",
      "Episode: 372 Aeverage Total reward: 21.67 Total reward: 21.00 Training loss: 10.9286 Explore P: 0.5267\n",
      "Episode: 373 Aeverage Total reward: 21.74 Total reward: 18.00 Training loss: 30.4838 Explore P: 0.5257\n",
      "Episode: 374 Aeverage Total reward: 21.95 Total reward: 30.00 Training loss: 14.6562 Explore P: 0.5242\n",
      "Episode: 375 Aeverage Total reward: 22.13 Total reward: 27.00 Training loss: 12.9919 Explore P: 0.5228\n",
      "Episode: 376 Aeverage Total reward: 22.14 Total reward: 12.00 Training loss: 1.0166 Explore P: 0.5222\n",
      "Episode: 377 Aeverage Total reward: 22.25 Total reward: 19.00 Training loss: 28.0634 Explore P: 0.5212\n",
      "Episode: 378 Aeverage Total reward: 22.40 Total reward: 24.00 Training loss: 1.5561 Explore P: 0.5200\n",
      "Episode: 379 Aeverage Total reward: 22.57 Total reward: 30.00 Training loss: 1.0526 Explore P: 0.5185\n",
      "Episode: 380 Aeverage Total reward: 22.63 Total reward: 17.00 Training loss: 1.8915 Explore P: 0.5176\n",
      "Episode: 381 Aeverage Total reward: 22.95 Total reward: 44.00 Training loss: 25.2838 Explore P: 0.5154\n",
      "Episode: 382 Aeverage Total reward: 23.00 Total reward: 15.00 Training loss: 1.8612 Explore P: 0.5146\n",
      "Episode: 383 Aeverage Total reward: 23.28 Total reward: 36.00 Training loss: 1.5427 Explore P: 0.5128\n",
      "Episode: 384 Aeverage Total reward: 23.36 Total reward: 17.00 Training loss: 1.1776 Explore P: 0.5120\n",
      "Episode: 385 Aeverage Total reward: 23.53 Total reward: 26.00 Training loss: 1.8796 Explore P: 0.5106\n",
      "Episode: 386 Aeverage Total reward: 23.88 Total reward: 53.00 Training loss: 23.8332 Explore P: 0.5080\n",
      "Episode: 387 Aeverage Total reward: 23.81 Total reward: 16.00 Training loss: 1.0703 Explore P: 0.5072\n",
      "Episode: 388 Aeverage Total reward: 23.80 Total reward: 20.00 Training loss: 16.8096 Explore P: 0.5062\n",
      "Episode: 389 Aeverage Total reward: 24.07 Total reward: 37.00 Training loss: 39.9909 Explore P: 0.5044\n",
      "Episode: 390 Aeverage Total reward: 24.10 Total reward: 21.00 Training loss: 28.7749 Explore P: 0.5033\n",
      "Episode: 391 Aeverage Total reward: 24.33 Total reward: 34.00 Training loss: 1.7277 Explore P: 0.5017\n",
      "Episode: 392 Aeverage Total reward: 24.50 Total reward: 33.00 Training loss: 15.5358 Explore P: 0.5000\n",
      "Episode: 393 Aeverage Total reward: 24.59 Total reward: 20.00 Training loss: 17.8752 Explore P: 0.4991\n",
      "Episode: 394 Aeverage Total reward: 24.68 Total reward: 18.00 Training loss: 1.3757 Explore P: 0.4982\n",
      "Episode: 395 Aeverage Total reward: 24.79 Total reward: 24.00 Training loss: 1.3207 Explore P: 0.4970\n",
      "Episode: 396 Aeverage Total reward: 24.79 Total reward: 23.00 Training loss: 1.0783 Explore P: 0.4959\n",
      "Episode: 397 Aeverage Total reward: 24.77 Total reward: 14.00 Training loss: 1.2608 Explore P: 0.4952\n",
      "Episode: 398 Aeverage Total reward: 24.85 Total reward: 19.00 Training loss: 12.9999 Explore P: 0.4943\n",
      "Episode: 399 Aeverage Total reward: 24.89 Total reward: 17.00 Training loss: 29.2381 Explore P: 0.4935\n",
      "Episode: 400 Aeverage Total reward: 24.92 Total reward: 12.00 Training loss: 32.7601 Explore P: 0.4929\n",
      "Episode: 401 Aeverage Total reward: 24.89 Total reward: 25.00 Training loss: 14.3152 Explore P: 0.4917\n",
      "Episode: 402 Aeverage Total reward: 24.99 Total reward: 26.00 Training loss: 17.1907 Explore P: 0.4904\n",
      "Episode: 403 Aeverage Total reward: 25.10 Total reward: 24.00 Training loss: 33.8281 Explore P: 0.4893\n",
      "Episode: 404 Aeverage Total reward: 25.28 Total reward: 29.00 Training loss: 1.7076 Explore P: 0.4879\n",
      "Episode: 405 Aeverage Total reward: 25.49 Total reward: 31.00 Training loss: 16.4524 Explore P: 0.4864\n",
      "Episode: 406 Aeverage Total reward: 25.56 Total reward: 24.00 Training loss: 65.3894 Explore P: 0.4853\n",
      "Episode: 407 Aeverage Total reward: 25.32 Total reward: 22.00 Training loss: 30.5633 Explore P: 0.4842\n",
      "Episode: 408 Aeverage Total reward: 25.34 Total reward: 22.00 Training loss: 17.7613 Explore P: 0.4832\n",
      "Episode: 409 Aeverage Total reward: 25.13 Total reward: 10.00 Training loss: 50.6773 Explore P: 0.4827\n",
      "Episode: 410 Aeverage Total reward: 25.00 Total reward: 21.00 Training loss: 17.4805 Explore P: 0.4817\n",
      "Episode: 411 Aeverage Total reward: 25.18 Total reward: 31.00 Training loss: 14.9563 Explore P: 0.4803\n",
      "Episode: 412 Aeverage Total reward: 25.28 Total reward: 20.00 Training loss: 1.6573 Explore P: 0.4793\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "total_rewards_list = []\n",
    "average_rewards_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_rewards_deque = deque(maxlen=100)\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        \n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "            \n",
    "        total_rewards_deque.append(total_reward)\n",
    "        total_rewards_list.append((ep, total_reward))\n",
    "        average_rewards_list.append((ep, np.mean(total_rewards_deque)))\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'Aeverage Total reward: {:.2f}'.format(np.mean(total_rewards_deque)),\n",
    "              'Total reward: {:.2f}'.format(total_reward),\n",
    "              'Training loss: {:.4f}'.format(loss),\n",
    "              'Explore P: {:.4f}'.format(explore_p))\n",
    "        #print('\\rEpisode {}\\tTotal Average Score: {:.2f}'.format(ep, np.mean(total_rewards_deque)))  \n",
    "        if np.mean(total_rewards_deque) >= 500:\n",
    "            break\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(average_rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(total_rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x125c136d8>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120004dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for ep in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "            # Get action from Q-network\n",
    "            feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
