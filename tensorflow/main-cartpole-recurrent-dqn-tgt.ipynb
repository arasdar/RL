{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.12.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# state = env.reset()\n",
    "# for _ in range(10):\n",
    "#     # env.render()\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, info = env.step(action) # take a random action\n",
    "#     #print('state, action, next_state, reward, done, info:', state, action, next_state, reward, done, info)\n",
    "#     state = next_state\n",
    "#     if done:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, hidden_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell], state_is_tuple=False)\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    initial_state_tgt = cells.zero_state(batch_size, tf.float32)\n",
    "    is_training = tf.placeholder(dtype=tf.bool, shape=[], name='is_training')\n",
    "    print('initial_state, cells, cell', initial_state, cells, cell)\n",
    "    return states, actions, targetQs, cells, initial_state, initial_state_tgt, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(random_seed=1, dtype=tf.float32, uniform=False):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(\n",
    "        dtype=dtype,\n",
    "        seed=tf.set_random_seed(random_seed), \n",
    "        uniform=uniform) # False: normal\n",
    "    return xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(inputs, units, trainable=True):\n",
    "    outputs = tf.layers.dense(\n",
    "        inputs=inputs,\n",
    "        units=units,\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=init_xavier(), # Xavier with normal init\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        trainable=trainable,\n",
    "        name=None,\n",
    "        reuse=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.leaky_relu(\n",
    "#     features,\n",
    "#     alpha=0.2,\n",
    "#     name=None\n",
    "# )\n",
    "def nl(inputs, alpha=0.2):\n",
    "    outputs = tf.maximum(alpha * inputs, inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(inputs, training=False):\n",
    "    outputs = tf.layers.batch_normalization(\n",
    "        inputs=inputs,\n",
    "        axis=-1,\n",
    "        momentum=0.99,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        beta_initializer=tf.zeros_initializer(),\n",
    "        gamma_initializer=tf.ones_initializer(),\n",
    "        moving_mean_initializer=tf.zeros_initializer(),\n",
    "        moving_variance_initializer=tf.ones_initializer(),\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        training=training,\n",
    "        trainable=True,\n",
    "        name=None,\n",
    "        reuse=None,\n",
    "        renorm=False,\n",
    "        renorm_clipping=None,\n",
    "        renorm_momentum=0.99,\n",
    "        fused=None,\n",
    "        virtual_batch_size=None,\n",
    "        adjustment=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic or Discriminator/classifier\n",
    "def D(states, action_size, initial_state, cells, hidden_size, reuse=False, is_training=False): \n",
    "    with tf.variable_scope('D', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = mlp(inputs=states, units=hidden_size)\n",
    "        inputs = bn(inputs=inputs, training=is_training)\n",
    "        inputs = nl(inputs)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size and\n",
    "        # static means can NOT adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, hidden_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, \n",
    "                                                     initial_state=initial_state)\n",
    "        #outputs_rnn = tf.layers.batch_normalization(inputs=outputs_rnn, training=is_training)\n",
    "        #final_state = tf.layers.batch_normalization(inputs=final_state, training=is_training)\n",
    "        print(outputs_rnn.shape, final_state)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, hidden_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        actions_logits = mlp(inputs=outputs, units=action_size)\n",
    "        print(actions_logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return actions_logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic or Discriminator/classifier\n",
    "def D_tgt(states, action_size, initial_state, cells, hidden_size, reuse=False, is_training=False): \n",
    "    with tf.variable_scope('D_tgt', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = mlp(inputs=states, units=hidden_size)\n",
    "        inputs = bn(inputs=inputs, training=is_training)\n",
    "        inputs = nl(inputs)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size and\n",
    "        # static means can NOT adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, hidden_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, \n",
    "                                                     initial_state=initial_state)\n",
    "        #outputs_rnn = tf.layers.batch_normalization(inputs=outputs_rnn, training=is_training)\n",
    "        #final_state = tf.layers.batch_normalization(inputs=final_state, training=is_training)\n",
    "        print(outputs_rnn.shape, final_state)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, hidden_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        actions_logits = mlp(inputs=outputs, units=action_size)\n",
    "        print(actions_logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return actions_logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cells, initial_state, initial_state_tgt, \n",
    "               actions, targetQs, is_training):\n",
    "    \n",
    "    actions_logits, final_state = D(states=states, cells=cells, initial_state=initial_state, \n",
    "                                    hidden_size=hidden_size, action_size=action_size, \n",
    "                                    is_training=is_training)\n",
    "    \n",
    "    actions_target, final_state_target = D_tgt(states=states, cells=cells, \n",
    "                                               initial_state=initial_state_tgt, \n",
    "                                               hidden_size=hidden_size, action_size=action_size, \n",
    "                                               is_training=is_training)\n",
    "    \n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    \n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    \n",
    "    loss = tf.reduce_mean((Qs - targetQs)**2)\n",
    "    \n",
    "    return actions_logits, final_state, loss, actions_target, final_state_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate, gamma):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('D')]\n",
    "    d_vars_tgt = [var for var in t_vars if var.name.startswith('D_tgt')]\n",
    "\n",
    "    # Optimize\n",
    "    #with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=d_vars)\n",
    "\n",
    "    opt_tgt = [d_vars_tgt[i].assign((d_vars_tgt[i]*gamma) + (d_vars[i]*(1 - gamma))) \n",
    "               for i in range(len(d_vars))]\n",
    "\n",
    "        # Optimize RNN\n",
    "        #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "        #grads = tf.gradients(loss, g_vars)\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "    return opt, opt_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        (self.states, self.actions, self.targetQs, cells, self.initial_state, self.initial_state_tgt,\\\n",
    "         self.is_training) = model_input(state_size=state_size, hidden_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        (self.actions_logits, self.final_state, self.loss, \\\n",
    "         self.actions_target, self.final_state_target) = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cells=cells, \n",
    "            initial_state=self.initial_state, initial_state_tgt=self.initial_state_tgt, \n",
    "            is_training=self.is_training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt, self.opt_tgt = model_opt(loss=self.loss, learning_rate=learning_rate, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "action_size = 2\n",
    "state_size = 4\n",
    "hidden_size = 128               # number of units in each Q-network hidden layer\n",
    "learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "# memory_size = 128            # memory capacity - 1000 DQN\n",
    "batch_size = 128             # experience mini-batch size - 20 DQN\n",
    "gamma = 0.99                 # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_state, cells, cell Tensor(\"MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0\", shape=(1, 128), dtype=float32) <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f415eb22cc0> <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f415e7f44e0>\n",
      "(?, 4) (?, 128)\n",
      "(1, ?, 128) Tensor(\"MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0\", shape=(1, 128), dtype=float32)\n",
      "(1, ?, 128) Tensor(\"D/rnn/while/Exit_3:0\", shape=(1, 128), dtype=float32)\n",
      "(?, 128)\n",
      "(?, 2)\n",
      "(?, 4) (?, 128)\n",
      "(1, ?, 128) Tensor(\"MultiRNNCellZeroState_1/MultiRNNCellZeroState/zeros:0\", shape=(1, 128), dtype=float32)\n",
      "(1, ?, 128) Tensor(\"D_tgt/rnn/while/Exit_3:0\", shape=(1, 128), dtype=float32)\n",
      "(?, 128)\n",
      "(?, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 128 and 256 for 'add_4' (op: 'Add') with input shapes: [128,2], [256,256].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 128 and 256 for 'add_4' (op: 'Add') with input shapes: [128,2], [256,256].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-05b069131c89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Init the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n\u001b[0;32m----> 6\u001b[0;31m               learning_rate=learning_rate, gamma=gamma)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# # Init the memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-d2da137f91ad>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size, hidden_size, learning_rate, gamma)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Update the model: backward pass and backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-bbaa179f47f8>\u001b[0m in \u001b[0;36mmodel_opt\u001b[0;34m(loss, learning_rate, gamma)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     opt_tgt = [d_vars_tgt[i].assign((d_vars_tgt[i]*gamma) + (d_vars[i]*(1 - gamma))) \n\u001b[0;32m---> 12\u001b[0;31m                for i in range(len(d_vars))]\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Optimize RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-bbaa179f47f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     opt_tgt = [d_vars_tgt[i].assign((d_vars_tgt[i]*gamma) + (d_vars[i]*(1 - gamma))) \n\u001b[0;32m---> 12\u001b[0;31m                for i in range(len(d_vars))]\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Optimize RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 301\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 128 and 256 for 'add_4' (op: 'Add') with input shapes: [128,2], [256,256]."
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, \n",
    "              learning_rate=learning_rate, gamma=gamma)\n",
    "\n",
    "# # Init the memory\n",
    "# memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'strided_slice:0' shape=(128,) dtype=float32>,\n",
       " <tf.Tensor 'D/rnn/while/Exit_3:0' shape=(1, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0' shape=(1, 128) dtype=float32>,\n",
       " <tf.Tensor 'is_training:0' shape=() dtype=bool>,\n",
       " <tf.Tensor 'actions:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'D/dense_1/BiasAdd:0' shape=(?, 2) dtype=float32>,\n",
       " <tf.Tensor 'Mean:0' shape=() dtype=float32>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Tensor 'states:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'targetQs:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.initial_state[0], model.final_state, model.initial_state, model.is_training, model.actions, \\\n",
    "model.actions_logits, model.loss, model.opt, model.states, model.targetQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(batch, sess, gamma=0.99):\n",
    "    states = np.array([each[0] for each in batch if each is not None])\n",
    "    actions = np.array([each[1] for each in batch if each is not None])\n",
    "    next_states = np.array([each[2] for each in batch if each is not None])\n",
    "    rewards = np.array([each[3] for each in batch if each is not None])\n",
    "    dones = np.array([each[4] for each in batch if each is not None])\n",
    "    initial_states = np.array([each[5] for each in batch if each is not None])\n",
    "    #print('initial_states.shape', initial_states.shape)\n",
    "    #print('initial_states[0].shape', initial_states[0].shape)\n",
    "\n",
    "    next_actions_logits = sess.run(model.actions_logits, \n",
    "                                   feed_dict = {model.states: next_states,\n",
    "                                                model.initial_state: initial_states[1],\n",
    "                                                model.is_training: False})\n",
    "\n",
    "    nextQs = np.max(next_actions_logits, axis=1)\n",
    "    targetQs = rewards + (gamma * nextQs * (1-dones))\n",
    "\n",
    "    loss, _ = sess.run([model.loss, model.opt],\n",
    "                       feed_dict = {model.states: states, \n",
    "                                    model.actions: actions,\n",
    "                                    model.targetQs: targetQs,\n",
    "                                    model.initial_state: initial_states[0],    \n",
    "                                    model.is_training: True})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(sess, state, initial_state, is_training):\n",
    "    action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                          feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                       model.is_training: False,\n",
    "                                                       model.initial_state: initial_state})\n",
    "    action = np.argmax(action_logits, axis=1)[0]\n",
    "    #print('action_logits, np.argmax(action_logits, axis=1)')\n",
    "    #print(action_logits.shape, np.argmax(action_logits, axis=1).shape)\n",
    "    return action, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 meanR:19.00 R:19.00 loss:0.67\n",
      "Episode:1 meanR:14.50 R:10.00 loss:0.08\n",
      "Episode:2 meanR:13.00 R:10.00 loss:0.04\n",
      "Episode:3 meanR:12.00 R:9.00 loss:0.02\n",
      "Episode:4 meanR:11.60 R:10.00 loss:0.02\n",
      "Episode:5 meanR:11.33 R:10.00 loss:0.02\n",
      "Episode:6 meanR:10.86 R:8.00 loss:0.02\n",
      "Episode:7 meanR:10.75 R:10.00 loss:0.02\n",
      "Episode:8 meanR:10.56 R:9.00 loss:0.02\n",
      "Episode:9 meanR:10.40 R:9.00 loss:0.01\n",
      "Episode:10 meanR:10.36 R:10.00 loss:0.01\n",
      "Episode:11 meanR:10.33 R:10.00 loss:0.01\n",
      "Episode:12 meanR:10.31 R:10.00 loss:0.01\n",
      "Episode:13 meanR:10.14 R:8.00 loss:0.01\n",
      "Episode:14 meanR:10.13 R:10.00 loss:0.02\n",
      "Episode:15 meanR:10.06 R:9.00 loss:0.01\n",
      "Episode:16 meanR:9.94 R:8.00 loss:0.02\n",
      "Episode:17 meanR:10.00 R:11.00 loss:0.03\n",
      "Episode:18 meanR:9.95 R:9.00 loss:0.03\n",
      "Episode:19 meanR:9.95 R:10.00 loss:0.02\n",
      "Episode:20 meanR:9.90 R:9.00 loss:0.02\n",
      "Episode:21 meanR:9.86 R:9.00 loss:0.03\n",
      "Episode:22 meanR:9.91 R:11.00 loss:0.04\n",
      "Episode:23 meanR:9.92 R:10.00 loss:0.07\n",
      "Episode:24 meanR:9.88 R:9.00 loss:0.04\n",
      "Episode:25 meanR:9.88 R:10.00 loss:0.06\n",
      "Episode:26 meanR:9.85 R:9.00 loss:0.06\n",
      "Episode:27 meanR:9.82 R:9.00 loss:0.04\n",
      "Episode:28 meanR:9.83 R:10.00 loss:0.07\n",
      "Episode:29 meanR:9.80 R:9.00 loss:0.08\n",
      "Episode:30 meanR:9.81 R:10.00 loss:0.07\n",
      "Episode:31 meanR:9.78 R:9.00 loss:0.09\n",
      "Episode:32 meanR:9.73 R:8.00 loss:0.11\n",
      "Episode:33 meanR:9.71 R:9.00 loss:0.18\n",
      "Episode:34 meanR:9.66 R:8.00 loss:0.15\n",
      "Episode:35 meanR:9.67 R:10.00 loss:0.14\n",
      "Episode:36 meanR:9.70 R:11.00 loss:0.11\n",
      "Episode:37 meanR:9.79 R:13.00 loss:0.15\n",
      "Episode:38 meanR:9.79 R:10.00 loss:0.26\n",
      "Episode:39 meanR:9.78 R:9.00 loss:0.47\n",
      "Episode:40 meanR:9.76 R:9.00 loss:0.39\n",
      "Episode:41 meanR:9.79 R:11.00 loss:0.36\n",
      "Episode:42 meanR:9.88 R:14.00 loss:0.77\n",
      "Episode:43 meanR:10.00 R:15.00 loss:0.90\n",
      "Episode:44 meanR:10.07 R:13.00 loss:1.03\n",
      "Episode:45 meanR:10.11 R:12.00 loss:0.58\n",
      "Episode:46 meanR:10.19 R:14.00 loss:0.98\n",
      "Episode:47 meanR:10.29 R:15.00 loss:0.44\n",
      "Episode:48 meanR:10.33 R:12.00 loss:1.15\n",
      "Episode:49 meanR:10.40 R:14.00 loss:1.26\n",
      "Episode:50 meanR:10.55 R:18.00 loss:1.73\n",
      "Episode:51 meanR:10.69 R:18.00 loss:2.02\n",
      "Episode:52 meanR:10.83 R:18.00 loss:1.77\n",
      "Episode:53 meanR:11.00 R:20.00 loss:1.56\n",
      "Episode:54 meanR:11.05 R:14.00 loss:2.48\n",
      "Episode:55 meanR:11.11 R:14.00 loss:2.00\n",
      "Episode:56 meanR:11.42 R:29.00 loss:9.88\n",
      "Episode:57 meanR:11.74 R:30.00 loss:3.83\n",
      "Episode:58 meanR:11.90 R:21.00 loss:3.27\n",
      "Episode:59 meanR:12.05 R:21.00 loss:4.13\n",
      "Episode:60 meanR:12.23 R:23.00 loss:2.91\n",
      "Episode:61 meanR:12.44 R:25.00 loss:1.93\n",
      "Episode:62 meanR:12.62 R:24.00 loss:3.34\n",
      "Episode:63 meanR:15.34 R:187.00 loss:3.51\n",
      "Episode:64 meanR:15.68 R:37.00 loss:12.47\n",
      "Episode:65 meanR:16.06 R:41.00 loss:10.69\n",
      "Episode:66 meanR:16.52 R:47.00 loss:9.18\n",
      "Episode:67 meanR:16.94 R:45.00 loss:8.39\n",
      "Episode:68 meanR:17.14 R:31.00 loss:7.08\n",
      "Episode:69 meanR:17.33 R:30.00 loss:16.70\n",
      "Episode:70 meanR:18.23 R:81.00 loss:54.50\n",
      "Episode:71 meanR:19.29 R:95.00 loss:13.11\n",
      "Episode:72 meanR:20.10 R:78.00 loss:5.79\n",
      "Episode:73 meanR:20.81 R:73.00 loss:15.45\n",
      "Episode:74 meanR:21.29 R:57.00 loss:10.10\n",
      "Episode:75 meanR:23.17 R:164.00 loss:4.01\n",
      "Episode:76 meanR:24.18 R:101.00 loss:4.95\n",
      "Episode:77 meanR:24.50 R:49.00 loss:13.10\n",
      "Episode:78 meanR:25.15 R:76.00 loss:5.79\n",
      "Episode:79 meanR:25.54 R:56.00 loss:6.21\n",
      "Episode:80 meanR:25.94 R:58.00 loss:4.99\n",
      "Episode:81 meanR:26.37 R:61.00 loss:5.17\n",
      "Episode:82 meanR:26.54 R:41.00 loss:7.59\n",
      "Episode:83 meanR:26.68 R:38.00 loss:8.57\n",
      "Episode:84 meanR:26.79 R:36.00 loss:9.27\n",
      "Episode:85 meanR:27.36 R:76.00 loss:3.79\n",
      "Episode:86 meanR:27.78 R:64.00 loss:4.48\n",
      "Episode:87 meanR:28.65 R:104.00 loss:3.72\n",
      "Episode:88 meanR:28.84 R:46.00 loss:6.57\n",
      "Episode:89 meanR:29.28 R:68.00 loss:3.91\n",
      "Episode:90 meanR:29.57 R:56.00 loss:5.17\n",
      "Episode:91 meanR:29.65 R:37.00 loss:9.54\n",
      "Episode:92 meanR:29.87 R:50.00 loss:7.17\n",
      "Episode:93 meanR:30.09 R:50.00 loss:5.70\n",
      "Episode:94 meanR:30.29 R:50.00 loss:5.68\n",
      "Episode:95 meanR:30.89 R:87.00 loss:3.35\n",
      "Episode:96 meanR:32.44 R:182.00 loss:1.98\n",
      "Episode:97 meanR:32.79 R:66.00 loss:5.10\n",
      "Episode:98 meanR:33.38 R:92.00 loss:3.18\n",
      "Episode:99 meanR:34.21 R:116.00 loss:3.35\n",
      "Episode:100 meanR:34.86 R:84.00 loss:14.33\n",
      "Episode:101 meanR:35.63 R:87.00 loss:7.21\n",
      "Episode:102 meanR:36.69 R:116.00 loss:2.53\n",
      "Episode:103 meanR:38.09 R:149.00 loss:1.34\n",
      "Episode:104 meanR:40.23 R:224.00 loss:1.57\n",
      "Episode:105 meanR:41.47 R:134.00 loss:1.39\n",
      "Episode:106 meanR:43.28 R:189.00 loss:1.99\n",
      "Episode:107 meanR:45.08 R:190.00 loss:2.23\n",
      "Episode:108 meanR:46.96 R:197.00 loss:4.58\n",
      "Episode:109 meanR:48.52 R:165.00 loss:3.12\n",
      "Episode:110 meanR:50.38 R:196.00 loss:1.05\n",
      "Episode:111 meanR:52.37 R:209.00 loss:3.70\n",
      "Episode:112 meanR:53.93 R:166.00 loss:2.73\n",
      "Episode:113 meanR:56.14 R:229.00 loss:3.87\n",
      "Episode:114 meanR:58.84 R:280.00 loss:1.86\n",
      "Episode:115 meanR:63.75 R:500.00 loss:1.29\n",
      "Episode:116 meanR:68.67 R:500.00 loss:1.00\n",
      "Episode:117 meanR:73.56 R:500.00 loss:0.85\n",
      "Episode:118 meanR:78.47 R:500.00 loss:0.60\n",
      "Episode:119 meanR:82.53 R:416.00 loss:1.38\n",
      "Episode:120 meanR:85.41 R:297.00 loss:1.77\n",
      "Episode:121 meanR:87.74 R:242.00 loss:0.69\n",
      "Episode:122 meanR:90.13 R:250.00 loss:0.65\n",
      "Episode:123 meanR:92.32 R:229.00 loss:0.82\n",
      "Episode:124 meanR:94.86 R:263.00 loss:1.33\n",
      "Episode:125 meanR:97.19 R:243.00 loss:1.04\n",
      "Episode:126 meanR:99.64 R:254.00 loss:1.62\n",
      "Episode:127 meanR:103.87 R:432.00 loss:1.00\n",
      "Episode:128 meanR:108.54 R:477.00 loss:0.54\n",
      "Episode:129 meanR:113.45 R:500.00 loss:0.45\n",
      "Episode:130 meanR:118.35 R:500.00 loss:0.35\n",
      "Episode:131 meanR:123.26 R:500.00 loss:0.30\n",
      "Episode:132 meanR:128.18 R:500.00 loss:1.05\n",
      "Episode:133 meanR:133.09 R:500.00 loss:0.51\n",
      "Episode:134 meanR:138.01 R:500.00 loss:0.26\n",
      "Episode:135 meanR:142.91 R:500.00 loss:0.24\n",
      "Episode:136 meanR:147.80 R:500.00 loss:0.30\n",
      "Episode:137 meanR:152.67 R:500.00 loss:0.21\n",
      "Episode:138 meanR:157.57 R:500.00 loss:0.28\n",
      "Episode:139 meanR:162.48 R:500.00 loss:0.18\n",
      "Episode:140 meanR:162.61 R:22.00 loss:140.71\n",
      "Episode:141 meanR:162.73 R:23.00 loss:127.43\n",
      "Episode:142 meanR:162.80 R:21.00 loss:146.11\n",
      "Episode:143 meanR:162.89 R:24.00 loss:120.07\n",
      "Episode:144 meanR:162.97 R:21.00 loss:144.70\n",
      "Episode:145 meanR:163.07 R:22.00 loss:136.16\n",
      "Episode:146 meanR:163.16 R:23.00 loss:123.20\n",
      "Episode:147 meanR:163.27 R:26.00 loss:98.68\n",
      "Episode:148 meanR:163.31 R:16.00 loss:219.16\n",
      "Episode:149 meanR:163.42 R:25.00 loss:103.34\n",
      "Episode:150 meanR:163.52 R:28.00 loss:84.52\n",
      "Episode:151 meanR:163.65 R:31.00 loss:70.64\n",
      "Episode:152 meanR:163.61 R:14.00 loss:266.44\n",
      "Episode:153 meanR:163.51 R:10.00 loss:433.68\n",
      "Episode:154 meanR:163.47 R:10.00 loss:418.20\n",
      "Episode:155 meanR:163.43 R:10.00 loss:411.14\n",
      "Episode:156 meanR:163.23 R:9.00 loss:467.08\n",
      "Episode:157 meanR:163.03 R:10.00 loss:381.21\n",
      "Episode:158 meanR:162.91 R:9.00 loss:437.88\n",
      "Episode:159 meanR:162.80 R:10.00 loss:363.77\n",
      "Episode:160 meanR:162.65 R:8.00 loss:496.73\n",
      "Episode:161 meanR:162.50 R:10.00 loss:353.06\n",
      "Episode:162 meanR:162.34 R:8.00 loss:514.68\n",
      "Episode:163 meanR:160.56 R:9.00 loss:406.22\n",
      "Episode:164 meanR:160.29 R:10.00 loss:343.28\n",
      "Episode:165 meanR:159.98 R:10.00 loss:339.04\n",
      "Episode:166 meanR:159.61 R:10.00 loss:340.96\n",
      "Episode:167 meanR:159.25 R:9.00 loss:392.64\n",
      "Episode:168 meanR:159.02 R:8.00 loss:460.50\n",
      "Episode:169 meanR:158.82 R:10.00 loss:328.01\n",
      "Episode:170 meanR:158.12 R:11.00 loss:283.12\n",
      "Episode:171 meanR:157.30 R:13.00 loss:209.33\n",
      "Episode:172 meanR:156.65 R:13.00 loss:219.36\n",
      "Episode:173 meanR:157.00 R:108.00 loss:14.80\n",
      "Episode:174 meanR:156.56 R:13.00 loss:221.52\n",
      "Episode:175 meanR:155.07 R:15.00 loss:177.81\n",
      "Episode:176 meanR:154.21 R:15.00 loss:166.23\n",
      "Episode:177 meanR:153.86 R:14.00 loss:184.13\n",
      "Episode:178 meanR:153.23 R:13.00 loss:208.16\n",
      "Episode:179 meanR:152.79 R:12.00 loss:228.66\n",
      "Episode:180 meanR:152.33 R:12.00 loss:227.09\n",
      "Episode:181 meanR:151.85 R:13.00 loss:201.11\n",
      "Episode:182 meanR:151.54 R:10.00 loss:311.40\n",
      "Episode:183 meanR:151.28 R:12.00 loss:221.13\n",
      "Episode:184 meanR:151.02 R:10.00 loss:299.67\n",
      "Episode:185 meanR:151.30 R:104.00 loss:17.16\n",
      "Episode:186 meanR:150.75 R:9.00 loss:340.72\n",
      "Episode:187 meanR:149.82 R:11.00 loss:247.53\n",
      "Episode:188 meanR:149.46 R:10.00 loss:281.46\n",
      "Episode:189 meanR:148.87 R:9.00 loss:331.77\n",
      "Episode:190 meanR:148.41 R:10.00 loss:290.80\n",
      "Episode:191 meanR:148.14 R:10.00 loss:277.32\n",
      "Episode:192 meanR:147.73 R:9.00 loss:320.09\n",
      "Episode:193 meanR:147.33 R:10.00 loss:273.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:194 meanR:146.94 R:11.00 loss:234.27\n",
      "Episode:195 meanR:146.16 R:9.00 loss:314.50\n",
      "Episode:196 meanR:144.43 R:9.00 loss:308.40\n",
      "Episode:197 meanR:143.88 R:11.00 loss:221.65\n",
      "Episode:198 meanR:143.06 R:10.00 loss:255.00\n",
      "Episode:199 meanR:142.00 R:10.00 loss:239.80\n",
      "Episode:200 meanR:141.26 R:10.00 loss:276.80\n",
      "Episode:201 meanR:140.53 R:14.00 loss:159.76\n",
      "Episode:202 meanR:139.50 R:13.00 loss:156.72\n",
      "Episode:203 meanR:138.16 R:15.00 loss:127.88\n",
      "Episode:204 meanR:137.02 R:110.00 loss:33.45\n",
      "Episode:205 meanR:135.84 R:16.00 loss:161.79\n",
      "Episode:206 meanR:134.12 R:17.00 loss:116.01\n",
      "Episode:207 meanR:132.62 R:40.00 loss:41.83\n",
      "Episode:208 meanR:131.23 R:58.00 loss:20.75\n",
      "Episode:209 meanR:130.37 R:79.00 loss:12.60\n",
      "Episode:210 meanR:129.23 R:82.00 loss:11.49\n",
      "Episode:211 meanR:132.14 R:500.00 loss:0.85\n",
      "Episode:212 meanR:130.58 R:10.00 loss:325.23\n",
      "Episode:213 meanR:128.42 R:13.00 loss:208.12\n",
      "Episode:214 meanR:129.69 R:407.00 loss:0.55\n",
      "Episode:215 meanR:124.77 R:8.00 loss:433.86\n",
      "Episode:216 meanR:120.01 R:24.00 loss:79.58\n",
      "Episode:217 meanR:115.30 R:29.00 loss:54.75\n",
      "Episode:218 meanR:110.59 R:29.00 loss:52.71\n",
      "Episode:219 meanR:106.80 R:37.00 loss:34.38\n",
      "Episode:220 meanR:103.95 R:12.00 loss:224.11\n",
      "Episode:221 meanR:101.88 R:35.00 loss:37.66\n",
      "Episode:222 meanR:99.48 R:10.00 loss:283.91\n",
      "Episode:223 meanR:97.29 R:10.00 loss:267.95\n",
      "Episode:224 meanR:94.75 R:9.00 loss:278.75\n",
      "Episode:225 meanR:92.66 R:34.00 loss:47.99\n",
      "Episode:226 meanR:90.46 R:34.00 loss:42.09\n",
      "Episode:227 meanR:86.24 R:10.00 loss:203.49\n",
      "Episode:228 meanR:81.58 R:11.00 loss:205.13\n",
      "Episode:229 meanR:76.69 R:11.00 loss:147.37\n",
      "Episode:230 meanR:71.97 R:28.00 loss:66.95\n",
      "Episode:231 meanR:67.06 R:9.00 loss:186.30\n",
      "Episode:232 meanR:62.16 R:10.00 loss:144.44\n",
      "Episode:233 meanR:57.28 R:12.00 loss:81.83\n",
      "Episode:234 meanR:52.37 R:9.00 loss:158.02\n",
      "Episode:235 meanR:47.84 R:47.00 loss:38.87\n",
      "Episode:236 meanR:43.00 R:16.00 loss:115.23\n",
      "Episode:237 meanR:38.37 R:37.00 loss:35.57\n",
      "Episode:238 meanR:33.57 R:20.00 loss:75.32\n",
      "Episode:239 meanR:28.68 R:11.00 loss:88.39\n",
      "Episode:240 meanR:28.93 R:47.00 loss:35.37\n",
      "Episode:241 meanR:29.52 R:82.00 loss:9.90\n",
      "Episode:242 meanR:29.42 R:11.00 loss:234.25\n",
      "Episode:243 meanR:31.81 R:263.00 loss:0.87\n",
      "Episode:244 meanR:31.70 R:10.00 loss:273.65\n",
      "Episode:245 meanR:31.74 R:26.00 loss:50.26\n",
      "Episode:246 meanR:31.75 R:24.00 loss:40.79\n",
      "Episode:247 meanR:31.71 R:22.00 loss:48.16\n",
      "Episode:248 meanR:32.27 R:72.00 loss:64.15\n",
      "Episode:249 meanR:33.98 R:196.00 loss:7.47\n",
      "Episode:250 meanR:33.80 R:10.00 loss:283.43\n",
      "Episode:251 meanR:33.59 R:10.00 loss:268.50\n",
      "Episode:252 meanR:35.36 R:191.00 loss:38.36\n",
      "Episode:253 meanR:35.59 R:33.00 loss:96.86\n",
      "Episode:254 meanR:35.68 R:19.00 loss:81.54\n",
      "Episode:255 meanR:36.59 R:101.00 loss:15.73\n",
      "Episode:256 meanR:37.24 R:74.00 loss:10.11\n",
      "Episode:257 meanR:37.71 R:57.00 loss:14.88\n",
      "Episode:258 meanR:38.35 R:73.00 loss:9.34\n",
      "Episode:259 meanR:38.93 R:68.00 loss:11.02\n",
      "Episode:260 meanR:39.59 R:74.00 loss:9.04\n",
      "Episode:261 meanR:40.23 R:74.00 loss:9.26\n",
      "Episode:262 meanR:40.81 R:66.00 loss:10.92\n",
      "Episode:263 meanR:41.22 R:50.00 loss:20.12\n",
      "Episode:264 meanR:41.64 R:52.00 loss:18.70\n",
      "Episode:265 meanR:42.63 R:109.00 loss:8.09\n",
      "Episode:266 meanR:43.35 R:82.00 loss:8.19\n",
      "Episode:267 meanR:44.39 R:113.00 loss:5.18\n",
      "Episode:268 meanR:45.26 R:95.00 loss:5.75\n",
      "Episode:269 meanR:46.27 R:111.00 loss:6.27\n",
      "Episode:270 meanR:50.12 R:396.00 loss:239.03\n",
      "Episode:271 meanR:50.09 R:10.00 loss:359.44\n",
      "Episode:272 meanR:50.07 R:11.00 loss:231.60\n",
      "Episode:273 meanR:49.07 R:8.00 loss:322.39\n",
      "Episode:274 meanR:49.04 R:10.00 loss:216.44\n",
      "Episode:275 meanR:48.98 R:9.00 loss:266.56\n",
      "Episode:276 meanR:48.92 R:9.00 loss:264.71\n",
      "Episode:277 meanR:48.89 R:11.00 loss:195.73\n",
      "Episode:278 meanR:48.86 R:10.00 loss:223.86\n",
      "Episode:279 meanR:48.83 R:9.00 loss:259.03\n",
      "Episode:280 meanR:48.79 R:8.00 loss:305.37\n",
      "Episode:281 meanR:48.76 R:10.00 loss:218.13\n",
      "Episode:282 meanR:48.74 R:8.00 loss:300.35\n",
      "Episode:283 meanR:48.70 R:8.00 loss:297.82\n",
      "Episode:284 meanR:48.70 R:10.00 loss:212.70\n",
      "Episode:285 meanR:47.75 R:9.00 loss:246.41\n",
      "Episode:286 meanR:47.76 R:10.00 loss:206.56\n",
      "Episode:287 meanR:47.75 R:10.00 loss:208.67\n",
      "Episode:288 meanR:47.75 R:10.00 loss:207.89\n",
      "Episode:289 meanR:47.75 R:9.00 loss:240.54\n",
      "Episode:290 meanR:47.76 R:11.00 loss:177.56\n",
      "Episode:291 meanR:47.76 R:10.00 loss:201.57\n",
      "Episode:292 meanR:47.76 R:9.00 loss:240.25\n",
      "Episode:293 meanR:47.76 R:10.00 loss:201.26\n",
      "Episode:294 meanR:47.74 R:9.00 loss:233.74\n",
      "Episode:295 meanR:47.75 R:10.00 loss:197.00\n",
      "Episode:296 meanR:47.74 R:8.00 loss:274.42\n",
      "Episode:297 meanR:47.72 R:9.00 loss:228.98\n",
      "Episode:298 meanR:47.71 R:9.00 loss:229.92\n",
      "Episode:299 meanR:47.70 R:9.00 loss:229.22\n",
      "Episode:300 meanR:47.69 R:9.00 loss:225.40\n",
      "Episode:301 meanR:47.64 R:9.00 loss:223.94\n",
      "Episode:302 meanR:47.61 R:10.00 loss:188.93\n",
      "Episode:303 meanR:47.54 R:8.00 loss:262.44\n",
      "Episode:304 meanR:46.54 R:10.00 loss:189.54\n",
      "Episode:305 meanR:46.48 R:10.00 loss:187.47\n",
      "Episode:306 meanR:46.40 R:9.00 loss:217.89\n",
      "Episode:307 meanR:46.11 R:11.00 loss:189.79\n",
      "Episode:308 meanR:45.64 R:11.00 loss:176.00\n",
      "Episode:309 meanR:44.95 R:10.00 loss:183.65\n",
      "Episode:310 meanR:44.22 R:9.00 loss:215.28\n",
      "Episode:311 meanR:39.31 R:9.00 loss:215.80\n",
      "Episode:312 meanR:39.30 R:9.00 loss:211.85\n",
      "Episode:313 meanR:39.26 R:9.00 loss:210.02\n",
      "Episode:314 meanR:35.28 R:9.00 loss:208.71\n",
      "Episode:315 meanR:35.31 R:11.00 loss:155.37\n",
      "Episode:316 meanR:35.16 R:9.00 loss:201.90\n",
      "Episode:317 meanR:34.96 R:9.00 loss:210.21\n",
      "Episode:318 meanR:34.78 R:11.00 loss:153.40\n",
      "Episode:319 meanR:34.52 R:11.00 loss:149.71\n",
      "Episode:320 meanR:34.50 R:10.00 loss:169.84\n",
      "Episode:321 meanR:34.23 R:8.00 loss:232.90\n",
      "Episode:322 meanR:34.23 R:10.00 loss:177.74\n",
      "Episode:323 meanR:34.81 R:68.00 loss:79.18\n",
      "Episode:324 meanR:35.39 R:67.00 loss:35.45\n",
      "Episode:325 meanR:36.00 R:95.00 loss:14.79\n",
      "Episode:326 meanR:35.92 R:26.00 loss:37.78\n",
      "Episode:327 meanR:36.02 R:20.00 loss:56.40\n",
      "Episode:328 meanR:36.46 R:55.00 loss:10.64\n",
      "Episode:329 meanR:37.51 R:116.00 loss:4.05\n",
      "Episode:330 meanR:37.72 R:49.00 loss:13.93\n",
      "Episode:331 meanR:38.14 R:51.00 loss:12.06\n",
      "Episode:332 meanR:38.49 R:45.00 loss:16.02\n",
      "Episode:333 meanR:38.72 R:35.00 loss:22.45\n",
      "Episode:334 meanR:38.95 R:32.00 loss:25.73\n",
      "Episode:335 meanR:38.67 R:19.00 loss:58.49\n",
      "Episode:336 meanR:38.71 R:20.00 loss:55.29\n",
      "Episode:337 meanR:38.69 R:35.00 loss:21.72\n",
      "Episode:338 meanR:38.73 R:24.00 loss:39.86\n",
      "Episode:339 meanR:38.91 R:29.00 loss:30.44\n",
      "Episode:340 meanR:38.68 R:24.00 loss:37.85\n",
      "Episode:341 meanR:38.34 R:48.00 loss:13.31\n",
      "Episode:342 meanR:38.57 R:34.00 loss:21.53\n",
      "Episode:343 meanR:36.23 R:29.00 loss:27.36\n",
      "Episode:344 meanR:36.51 R:38.00 loss:17.49\n",
      "Episode:345 meanR:36.50 R:25.00 loss:33.78\n",
      "Episode:346 meanR:36.53 R:27.00 loss:30.63\n",
      "Episode:347 meanR:36.85 R:54.00 loss:11.11\n",
      "Episode:348 meanR:36.45 R:32.00 loss:23.18\n",
      "Episode:349 meanR:35.03 R:54.00 loss:11.48\n",
      "Episode:350 meanR:35.28 R:35.00 loss:20.02\n",
      "Episode:351 meanR:35.27 R:9.00 loss:167.17\n",
      "Episode:352 meanR:33.63 R:27.00 loss:28.29\n",
      "Episode:353 meanR:33.64 R:34.00 loss:19.40\n",
      "Episode:354 meanR:33.69 R:24.00 loss:36.15\n",
      "Episode:355 meanR:32.86 R:18.00 loss:54.00\n",
      "Episode:356 meanR:32.44 R:32.00 loss:22.40\n",
      "Episode:357 meanR:32.00 R:13.00 loss:83.84\n",
      "Episode:358 meanR:31.64 R:37.00 loss:17.76\n",
      "Episode:359 meanR:31.20 R:24.00 loss:32.80\n",
      "Episode:360 meanR:30.75 R:29.00 loss:24.39\n",
      "Episode:361 meanR:30.35 R:34.00 loss:20.64\n",
      "Episode:362 meanR:29.93 R:24.00 loss:33.99\n",
      "Episode:363 meanR:29.74 R:31.00 loss:20.79\n",
      "Episode:364 meanR:29.51 R:29.00 loss:23.51\n",
      "Episode:365 meanR:28.99 R:57.00 loss:11.02\n",
      "Episode:366 meanR:28.54 R:37.00 loss:16.70\n",
      "Episode:367 meanR:27.69 R:28.00 loss:24.22\n",
      "Episode:368 meanR:27.15 R:41.00 loss:13.03\n",
      "Episode:369 meanR:26.41 R:37.00 loss:15.51\n",
      "Episode:370 meanR:22.77 R:32.00 loss:18.51\n",
      "Episode:371 meanR:22.98 R:31.00 loss:19.70\n",
      "Episode:372 meanR:23.20 R:33.00 loss:18.48\n",
      "Episode:373 meanR:23.77 R:65.00 loss:8.43\n",
      "Episode:374 meanR:24.37 R:70.00 loss:7.85\n",
      "Episode:375 meanR:25.13 R:85.00 loss:5.87\n",
      "Episode:376 meanR:25.42 R:38.00 loss:16.59\n",
      "Episode:377 meanR:25.84 R:53.00 loss:8.99\n",
      "Episode:378 meanR:26.40 R:66.00 loss:6.69\n",
      "Episode:379 meanR:26.42 R:11.00 loss:138.38\n",
      "Episode:380 meanR:26.51 R:17.00 loss:65.79\n",
      "Episode:381 meanR:26.52 R:11.00 loss:125.19\n",
      "Episode:382 meanR:26.55 R:11.00 loss:132.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:383 meanR:26.56 R:9.00 loss:174.70\n",
      "Episode:384 meanR:26.55 R:9.00 loss:170.87\n",
      "Episode:385 meanR:26.55 R:9.00 loss:167.45\n",
      "Episode:386 meanR:26.53 R:8.00 loss:194.76\n",
      "Episode:387 meanR:26.52 R:9.00 loss:156.94\n",
      "Episode:388 meanR:26.50 R:8.00 loss:189.83\n",
      "Episode:389 meanR:26.51 R:10.00 loss:130.04\n",
      "Episode:390 meanR:26.49 R:9.00 loss:156.44\n",
      "Episode:391 meanR:26.48 R:9.00 loss:154.37\n",
      "Episode:392 meanR:26.47 R:8.00 loss:180.57\n",
      "Episode:393 meanR:26.47 R:10.00 loss:126.72\n",
      "Episode:394 meanR:26.49 R:11.00 loss:108.01\n",
      "Episode:395 meanR:26.48 R:9.00 loss:148.72\n",
      "Episode:396 meanR:26.49 R:9.00 loss:146.94\n",
      "Episode:397 meanR:26.48 R:8.00 loss:172.09\n",
      "Episode:398 meanR:26.48 R:9.00 loss:142.62\n",
      "Episode:399 meanR:26.50 R:11.00 loss:105.62\n",
      "Episode:400 meanR:26.50 R:9.00 loss:139.88\n",
      "Episode:401 meanR:26.51 R:10.00 loss:120.16\n",
      "Episode:402 meanR:26.52 R:11.00 loss:102.11\n",
      "Episode:403 meanR:26.52 R:8.00 loss:164.20\n",
      "Episode:404 meanR:26.50 R:8.00 loss:162.27\n",
      "Episode:405 meanR:26.50 R:10.00 loss:114.70\n",
      "Episode:406 meanR:26.51 R:10.00 loss:114.83\n",
      "Episode:407 meanR:26.49 R:9.00 loss:132.86\n",
      "Episode:408 meanR:26.48 R:10.00 loss:112.48\n",
      "Episode:409 meanR:26.47 R:9.00 loss:131.03\n",
      "Episode:410 meanR:26.47 R:9.00 loss:129.75\n",
      "Episode:411 meanR:26.48 R:10.00 loss:110.40\n",
      "Episode:412 meanR:26.48 R:9.00 loss:127.44\n",
      "Episode:413 meanR:26.49 R:10.00 loss:109.45\n",
      "Episode:414 meanR:26.49 R:9.00 loss:126.84\n",
      "Episode:415 meanR:26.48 R:10.00 loss:107.28\n",
      "Episode:416 meanR:26.50 R:11.00 loss:92.48\n",
      "Episode:417 meanR:26.49 R:8.00 loss:146.98\n",
      "Episode:418 meanR:26.47 R:9.00 loss:122.80\n",
      "Episode:419 meanR:26.45 R:9.00 loss:122.24\n",
      "Episode:420 meanR:26.45 R:10.00 loss:103.37\n",
      "Episode:421 meanR:26.47 R:10.00 loss:103.34\n",
      "Episode:422 meanR:26.47 R:10.00 loss:102.48\n",
      "Episode:423 meanR:25.88 R:9.00 loss:118.57\n",
      "Episode:424 meanR:25.31 R:10.00 loss:101.20\n",
      "Episode:425 meanR:24.46 R:10.00 loss:100.33\n",
      "Episode:426 meanR:24.31 R:11.00 loss:86.40\n",
      "Episode:427 meanR:24.21 R:10.00 loss:98.67\n",
      "Episode:428 meanR:23.76 R:10.00 loss:99.12\n",
      "Episode:429 meanR:22.69 R:9.00 loss:114.90\n",
      "Episode:430 meanR:22.30 R:10.00 loss:97.22\n",
      "Episode:431 meanR:21.90 R:11.00 loss:81.70\n",
      "Episode:432 meanR:21.53 R:8.00 loss:134.52\n",
      "Episode:433 meanR:21.30 R:12.00 loss:68.95\n",
      "Episode:434 meanR:21.07 R:9.00 loss:111.28\n",
      "Episode:435 meanR:20.98 R:10.00 loss:94.35\n",
      "Episode:436 meanR:20.88 R:10.00 loss:94.12\n",
      "Episode:437 meanR:20.64 R:11.00 loss:80.75\n",
      "Episode:438 meanR:20.50 R:10.00 loss:93.07\n",
      "Episode:439 meanR:20.31 R:10.00 loss:92.35\n",
      "Episode:440 meanR:20.15 R:8.00 loss:126.80\n",
      "Episode:441 meanR:19.77 R:10.00 loss:91.19\n",
      "Episode:442 meanR:19.53 R:10.00 loss:90.45\n",
      "Episode:443 meanR:19.33 R:9.00 loss:104.76\n",
      "Episode:444 meanR:19.04 R:9.00 loss:104.43\n",
      "Episode:445 meanR:18.88 R:9.00 loss:103.47\n",
      "Episode:446 meanR:18.72 R:11.00 loss:75.29\n",
      "Episode:447 meanR:18.28 R:10.00 loss:87.26\n",
      "Episode:448 meanR:18.06 R:10.00 loss:87.28\n",
      "Episode:449 meanR:17.60 R:8.00 loss:120.20\n",
      "Episode:450 meanR:17.35 R:10.00 loss:85.60\n",
      "Episode:451 meanR:17.35 R:9.00 loss:99.68\n",
      "Episode:452 meanR:17.18 R:10.00 loss:84.50\n",
      "Episode:453 meanR:16.94 R:10.00 loss:84.42\n",
      "Episode:454 meanR:16.79 R:9.00 loss:97.75\n",
      "Episode:455 meanR:16.72 R:11.00 loss:68.43\n",
      "Episode:456 meanR:16.49 R:9.00 loss:92.22\n",
      "Episode:457 meanR:16.44 R:8.00 loss:114.46\n",
      "Episode:458 meanR:16.17 R:10.00 loss:81.70\n",
      "Episode:459 meanR:16.04 R:11.00 loss:70.11\n",
      "Episode:460 meanR:15.84 R:9.00 loss:94.34\n",
      "Episode:461 meanR:15.59 R:9.00 loss:93.67\n",
      "Episode:462 meanR:15.45 R:10.00 loss:79.93\n",
      "Episode:463 meanR:15.22 R:8.00 loss:109.76\n",
      "Episode:464 meanR:15.02 R:9.00 loss:86.77\n",
      "Episode:465 meanR:14.54 R:9.00 loss:91.00\n",
      "Episode:466 meanR:14.26 R:9.00 loss:91.04\n",
      "Episode:467 meanR:14.08 R:10.00 loss:77.21\n",
      "Episode:468 meanR:13.75 R:8.00 loss:106.02\n",
      "Episode:469 meanR:13.48 R:10.00 loss:71.96\n",
      "Episode:470 meanR:13.27 R:11.00 loss:63.48\n",
      "Episode:471 meanR:13.06 R:10.00 loss:75.15\n",
      "Episode:472 meanR:12.81 R:8.00 loss:101.95\n",
      "Episode:473 meanR:12.28 R:12.00 loss:53.43\n",
      "Episode:474 meanR:11.70 R:12.00 loss:53.33\n",
      "Episode:475 meanR:10.93 R:8.00 loss:96.79\n",
      "Episode:476 meanR:10.64 R:9.00 loss:80.48\n",
      "Episode:477 meanR:10.20 R:9.00 loss:80.38\n",
      "Episode:478 meanR:9.63 R:9.00 loss:79.57\n",
      "Episode:479 meanR:9.64 R:12.00 loss:55.89\n",
      "Episode:480 meanR:9.58 R:11.00 loss:58.85\n",
      "Episode:481 meanR:9.58 R:11.00 loss:58.05\n",
      "Episode:482 meanR:9.56 R:9.00 loss:77.38\n",
      "Episode:483 meanR:9.56 R:9.00 loss:77.75\n",
      "Episode:484 meanR:9.55 R:8.00 loss:91.28\n",
      "Episode:485 meanR:9.57 R:11.00 loss:56.33\n",
      "Episode:486 meanR:9.57 R:8.00 loss:89.74\n",
      "Episode:487 meanR:9.59 R:11.00 loss:55.66\n",
      "Episode:488 meanR:9.61 R:10.00 loss:63.66\n",
      "Episode:489 meanR:9.60 R:9.00 loss:74.63\n",
      "Episode:490 meanR:9.59 R:8.00 loss:87.54\n",
      "Episode:491 meanR:9.60 R:10.00 loss:62.52\n",
      "Episode:492 meanR:9.61 R:9.00 loss:72.42\n",
      "Episode:493 meanR:9.60 R:9.00 loss:72.67\n",
      "Episode:494 meanR:9.59 R:10.00 loss:61.62\n",
      "Episode:495 meanR:9.60 R:10.00 loss:60.91\n",
      "Episode:496 meanR:9.61 R:10.00 loss:60.71\n",
      "Episode:497 meanR:9.62 R:9.00 loss:70.29\n",
      "Episode:498 meanR:9.62 R:9.00 loss:69.92\n",
      "Episode:499 meanR:9.60 R:9.00 loss:69.11\n",
      "Episode:500 meanR:9.60 R:9.00 loss:69.30\n",
      "Episode:501 meanR:9.59 R:9.00 loss:68.70\n",
      "Episode:502 meanR:9.58 R:10.00 loss:58.13\n",
      "Episode:503 meanR:9.58 R:8.00 loss:80.32\n",
      "Episode:504 meanR:9.59 R:9.00 loss:66.98\n",
      "Episode:505 meanR:9.61 R:12.00 loss:43.50\n",
      "Episode:506 meanR:9.60 R:9.00 loss:66.31\n",
      "Episode:507 meanR:9.60 R:9.00 loss:66.04\n",
      "Episode:508 meanR:9.59 R:9.00 loss:65.44\n",
      "Episode:509 meanR:9.60 R:10.00 loss:55.78\n",
      "Episode:510 meanR:9.62 R:11.00 loss:47.78\n",
      "Episode:511 meanR:9.60 R:8.00 loss:76.55\n",
      "Episode:512 meanR:9.62 R:11.00 loss:47.51\n",
      "Episode:513 meanR:9.61 R:9.00 loss:63.64\n",
      "Episode:514 meanR:9.61 R:9.00 loss:63.02\n",
      "Episode:515 meanR:9.60 R:9.00 loss:62.64\n",
      "Episode:516 meanR:9.57 R:8.00 loss:73.68\n",
      "Episode:517 meanR:9.58 R:9.00 loss:61.70\n",
      "Episode:518 meanR:9.59 R:10.00 loss:52.21\n",
      "Episode:519 meanR:9.58 R:8.00 loss:72.68\n",
      "Episode:520 meanR:9.57 R:9.00 loss:60.62\n",
      "Episode:521 meanR:9.57 R:10.00 loss:51.29\n",
      "Episode:522 meanR:9.57 R:10.00 loss:51.12\n",
      "Episode:523 meanR:9.58 R:10.00 loss:50.65\n",
      "Episode:524 meanR:9.56 R:8.00 loss:69.95\n",
      "Episode:525 meanR:9.55 R:9.00 loss:58.45\n",
      "Episode:526 meanR:9.55 R:11.00 loss:42.97\n",
      "Episode:527 meanR:9.55 R:10.00 loss:49.47\n",
      "Episode:528 meanR:9.55 R:10.00 loss:49.01\n",
      "Episode:529 meanR:9.54 R:8.00 loss:67.80\n",
      "Episode:530 meanR:9.54 R:10.00 loss:48.55\n",
      "Episode:531 meanR:9.53 R:10.00 loss:48.38\n",
      "Episode:532 meanR:9.56 R:11.00 loss:41.33\n",
      "Episode:533 meanR:9.53 R:9.00 loss:55.68\n",
      "Episode:534 meanR:9.54 R:10.00 loss:47.25\n",
      "Episode:535 meanR:9.54 R:10.00 loss:46.94\n",
      "Episode:536 meanR:9.53 R:9.00 loss:54.28\n",
      "Episode:537 meanR:9.52 R:10.00 loss:46.60\n",
      "Episode:538 meanR:9.50 R:8.00 loss:64.00\n",
      "Episode:539 meanR:9.50 R:10.00 loss:45.61\n",
      "Episode:540 meanR:9.52 R:10.00 loss:45.53\n",
      "Episode:541 meanR:9.52 R:10.00 loss:44.97\n",
      "Episode:542 meanR:9.51 R:9.00 loss:52.43\n",
      "Episode:543 meanR:9.54 R:12.00 loss:33.73\n",
      "Episode:544 meanR:9.55 R:10.00 loss:44.36\n",
      "Episode:545 meanR:9.55 R:9.00 loss:51.31\n",
      "Episode:546 meanR:9.54 R:10.00 loss:43.47\n",
      "Episode:547 meanR:9.55 R:11.00 loss:37.41\n",
      "Episode:548 meanR:9.54 R:9.00 loss:50.15\n",
      "Episode:549 meanR:9.55 R:9.00 loss:49.22\n",
      "Episode:550 meanR:9.55 R:10.00 loss:41.34\n",
      "Episode:551 meanR:9.54 R:8.00 loss:56.59\n",
      "Episode:552 meanR:9.54 R:10.00 loss:40.26\n",
      "Episode:553 meanR:9.53 R:9.00 loss:46.33\n",
      "Episode:554 meanR:9.53 R:9.00 loss:46.19\n",
      "Episode:555 meanR:9.51 R:9.00 loss:45.46\n",
      "Episode:556 meanR:9.50 R:8.00 loss:54.06\n",
      "Episode:557 meanR:9.53 R:11.00 loss:33.34\n",
      "Episode:558 meanR:9.53 R:10.00 loss:38.45\n",
      "Episode:559 meanR:9.51 R:9.00 loss:44.43\n",
      "Episode:560 meanR:9.51 R:9.00 loss:43.87\n",
      "Episode:561 meanR:9.51 R:9.00 loss:44.06\n",
      "Episode:562 meanR:9.52 R:11.00 loss:32.83\n",
      "Episode:563 meanR:9.53 R:9.00 loss:42.88\n",
      "Episode:564 meanR:9.54 R:10.00 loss:37.03\n",
      "Episode:565 meanR:9.55 R:10.00 loss:36.63\n",
      "Episode:566 meanR:9.56 R:10.00 loss:36.17\n",
      "Episode:567 meanR:9.54 R:8.00 loss:49.87\n",
      "Episode:568 meanR:9.55 R:9.00 loss:41.59\n",
      "Episode:569 meanR:9.55 R:10.00 loss:35.23\n",
      "Episode:570 meanR:9.54 R:10.00 loss:35.12\n",
      "Episode:571 meanR:9.52 R:8.00 loss:48.22\n",
      "Episode:572 meanR:9.53 R:9.00 loss:40.49\n",
      "Episode:573 meanR:9.50 R:9.00 loss:39.94\n",
      "Episode:574 meanR:9.48 R:10.00 loss:34.08\n",
      "Episode:575 meanR:9.48 R:8.00 loss:46.67\n",
      "Episode:576 meanR:9.49 R:10.00 loss:33.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:577 meanR:9.48 R:8.00 loss:46.34\n",
      "Episode:578 meanR:9.48 R:9.00 loss:38.52\n",
      "Episode:579 meanR:9.45 R:9.00 loss:38.29\n",
      "Episode:580 meanR:9.45 R:11.00 loss:28.09\n",
      "Episode:581 meanR:9.44 R:10.00 loss:32.40\n",
      "Episode:582 meanR:9.44 R:9.00 loss:37.41\n",
      "Episode:583 meanR:10.07 R:72.00 loss:5.34\n",
      "Episode:584 meanR:10.75 R:76.00 loss:7.59\n",
      "Episode:585 meanR:11.59 R:95.00 loss:4.17\n",
      "Episode:586 meanR:11.93 R:42.00 loss:6.58\n",
      "Episode:587 meanR:12.48 R:66.00 loss:3.81\n",
      "Episode:588 meanR:12.49 R:11.00 loss:42.61\n",
      "Episode:589 meanR:12.49 R:9.00 loss:47.36\n",
      "Episode:590 meanR:12.51 R:10.00 loss:40.18\n",
      "Episode:591 meanR:12.50 R:9.00 loss:49.64\n",
      "Episode:592 meanR:12.50 R:9.00 loss:44.54\n",
      "Episode:593 meanR:12.49 R:8.00 loss:44.52\n",
      "Episode:594 meanR:12.50 R:11.00 loss:24.26\n",
      "Episode:595 meanR:12.78 R:38.00 loss:8.47\n",
      "Episode:596 meanR:13.02 R:34.00 loss:8.79\n",
      "Episode:597 meanR:14.88 R:195.00 loss:16.86\n",
      "Episode:598 meanR:14.88 R:9.00 loss:74.84\n",
      "Episode:599 meanR:14.91 R:12.00 loss:54.21\n",
      "Episode:600 meanR:14.93 R:11.00 loss:36.50\n",
      "Episode:601 meanR:14.94 R:10.00 loss:39.83\n",
      "Episode:602 meanR:14.94 R:10.00 loss:37.72\n",
      "Episode:603 meanR:14.96 R:10.00 loss:36.44\n",
      "Episode:604 meanR:14.96 R:9.00 loss:37.39\n",
      "Episode:605 meanR:14.93 R:9.00 loss:39.69\n",
      "Episode:606 meanR:14.94 R:10.00 loss:32.15\n",
      "Episode:607 meanR:14.94 R:9.00 loss:36.95\n",
      "Episode:608 meanR:14.94 R:9.00 loss:36.14\n",
      "Episode:609 meanR:14.93 R:9.00 loss:35.29\n",
      "Episode:610 meanR:14.91 R:9.00 loss:34.25\n",
      "Episode:611 meanR:14.92 R:9.00 loss:34.47\n",
      "Episode:612 meanR:14.91 R:10.00 loss:29.17\n",
      "Episode:613 meanR:14.93 R:11.00 loss:24.99\n",
      "Episode:614 meanR:14.93 R:9.00 loss:32.57\n",
      "Episode:615 meanR:14.94 R:10.00 loss:28.16\n",
      "Episode:616 meanR:14.95 R:9.00 loss:31.93\n",
      "Episode:617 meanR:14.96 R:10.00 loss:27.15\n",
      "Episode:618 meanR:14.96 R:10.00 loss:26.67\n",
      "Episode:619 meanR:14.98 R:10.00 loss:26.31\n",
      "Episode:620 meanR:14.99 R:10.00 loss:25.85\n",
      "Episode:621 meanR:15.00 R:11.00 loss:22.38\n",
      "Episode:622 meanR:14.99 R:9.00 loss:29.63\n",
      "Episode:623 meanR:15.00 R:11.00 loss:22.56\n",
      "Episode:624 meanR:15.01 R:9.00 loss:29.06\n",
      "Episode:625 meanR:15.03 R:11.00 loss:21.43\n",
      "Episode:626 meanR:15.02 R:10.00 loss:24.95\n",
      "Episode:627 meanR:15.02 R:10.00 loss:24.54\n",
      "Episode:628 meanR:15.01 R:9.00 loss:27.70\n",
      "Episode:629 meanR:15.02 R:9.00 loss:27.81\n",
      "Episode:630 meanR:15.04 R:12.00 loss:17.99\n",
      "Episode:631 meanR:15.04 R:10.00 loss:23.38\n",
      "Episode:632 meanR:15.04 R:11.00 loss:19.96\n",
      "Episode:633 meanR:15.05 R:10.00 loss:22.77\n",
      "Episode:634 meanR:15.05 R:10.00 loss:22.50\n",
      "Episode:635 meanR:15.04 R:9.00 loss:25.76\n",
      "Episode:636 meanR:15.04 R:9.00 loss:25.47\n",
      "Episode:637 meanR:15.04 R:10.00 loss:21.99\n",
      "Episode:638 meanR:15.04 R:8.00 loss:29.45\n",
      "Episode:639 meanR:15.03 R:9.00 loss:24.75\n",
      "Episode:640 meanR:15.03 R:10.00 loss:21.01\n",
      "Episode:641 meanR:15.04 R:11.00 loss:18.14\n",
      "Episode:642 meanR:15.06 R:11.00 loss:17.85\n",
      "Episode:643 meanR:15.03 R:9.00 loss:23.67\n",
      "Episode:644 meanR:15.02 R:9.00 loss:23.39\n",
      "Episode:645 meanR:15.04 R:11.00 loss:17.45\n",
      "Episode:646 meanR:15.06 R:12.00 loss:15.14\n",
      "Episode:647 meanR:15.04 R:9.00 loss:22.78\n",
      "Episode:648 meanR:15.04 R:9.00 loss:22.50\n",
      "Episode:649 meanR:15.05 R:10.00 loss:19.30\n",
      "Episode:650 meanR:15.04 R:9.00 loss:21.99\n",
      "Episode:651 meanR:15.05 R:9.00 loss:21.78\n",
      "Episode:652 meanR:15.04 R:9.00 loss:21.38\n",
      "Episode:653 meanR:15.05 R:10.00 loss:18.65\n",
      "Episode:654 meanR:15.05 R:9.00 loss:20.98\n",
      "Episode:655 meanR:15.04 R:8.00 loss:24.79\n",
      "Episode:656 meanR:15.05 R:9.00 loss:20.59\n",
      "Episode:657 meanR:15.03 R:9.00 loss:20.44\n",
      "Episode:658 meanR:15.02 R:9.00 loss:19.95\n",
      "Episode:659 meanR:15.03 R:10.00 loss:17.37\n",
      "Episode:660 meanR:15.03 R:9.00 loss:19.80\n",
      "Episode:661 meanR:15.03 R:9.00 loss:19.45\n",
      "Episode:662 meanR:15.02 R:10.00 loss:16.42\n",
      "Episode:663 meanR:15.03 R:10.00 loss:16.38\n",
      "Episode:664 meanR:15.03 R:10.00 loss:16.58\n",
      "Episode:665 meanR:15.04 R:11.00 loss:13.86\n",
      "Episode:666 meanR:15.04 R:10.00 loss:15.92\n",
      "Episode:667 meanR:15.06 R:10.00 loss:15.57\n",
      "Episode:668 meanR:15.07 R:10.00 loss:15.75\n",
      "Episode:669 meanR:15.07 R:10.00 loss:15.27\n",
      "Episode:670 meanR:15.06 R:9.00 loss:18.10\n",
      "Episode:671 meanR:15.09 R:11.00 loss:13.06\n",
      "Episode:672 meanR:15.08 R:8.00 loss:20.35\n",
      "Episode:673 meanR:15.08 R:9.00 loss:17.37\n",
      "Episode:674 meanR:15.09 R:11.00 loss:12.95\n",
      "Episode:675 meanR:15.09 R:8.00 loss:19.94\n",
      "Episode:676 meanR:15.09 R:10.00 loss:14.17\n",
      "Episode:677 meanR:15.10 R:9.00 loss:16.40\n",
      "Episode:678 meanR:15.10 R:9.00 loss:16.12\n",
      "Episode:679 meanR:15.10 R:9.00 loss:16.06\n",
      "Episode:680 meanR:15.10 R:11.00 loss:11.92\n",
      "Episode:681 meanR:15.09 R:9.00 loss:15.89\n",
      "Episode:682 meanR:15.12 R:12.00 loss:10.47\n",
      "Episode:683 meanR:14.50 R:10.00 loss:13.35\n",
      "Episode:684 meanR:13.84 R:10.00 loss:13.09\n",
      "Episode:685 meanR:12.99 R:10.00 loss:13.04\n",
      "Episode:686 meanR:12.66 R:9.00 loss:14.96\n",
      "Episode:687 meanR:12.10 R:10.00 loss:12.68\n",
      "Episode:688 meanR:12.08 R:9.00 loss:14.53\n",
      "Episode:689 meanR:12.08 R:9.00 loss:14.42\n",
      "Episode:690 meanR:12.08 R:10.00 loss:12.25\n",
      "Episode:691 meanR:12.08 R:9.00 loss:14.01\n",
      "Episode:692 meanR:12.07 R:8.00 loss:16.65\n",
      "Episode:693 meanR:12.08 R:9.00 loss:13.66\n",
      "Episode:694 meanR:12.06 R:9.00 loss:13.86\n",
      "Episode:695 meanR:11.79 R:11.00 loss:10.51\n",
      "Episode:696 meanR:11.56 R:11.00 loss:9.93\n",
      "Episode:697 meanR:9.71 R:10.00 loss:11.29\n",
      "Episode:698 meanR:9.71 R:9.00 loss:13.09\n",
      "Episode:699 meanR:9.67 R:8.00 loss:15.63\n",
      "Episode:700 meanR:9.65 R:9.00 loss:12.74\n",
      "Episode:701 meanR:9.63 R:8.00 loss:15.06\n",
      "Episode:702 meanR:9.63 R:10.00 loss:11.04\n",
      "Episode:703 meanR:9.63 R:10.00 loss:10.57\n",
      "Episode:704 meanR:9.63 R:9.00 loss:12.17\n",
      "Episode:705 meanR:9.64 R:10.00 loss:10.79\n",
      "Episode:706 meanR:9.64 R:10.00 loss:10.19\n",
      "Episode:707 meanR:9.63 R:8.00 loss:13.82\n",
      "Episode:708 meanR:9.63 R:9.00 loss:11.55\n",
      "Episode:709 meanR:9.64 R:10.00 loss:10.12\n",
      "Episode:710 meanR:9.63 R:8.00 loss:13.25\n",
      "Episode:711 meanR:9.64 R:10.00 loss:9.55\n",
      "Episode:712 meanR:9.64 R:10.00 loss:9.34\n",
      "Episode:713 meanR:9.62 R:9.00 loss:10.95\n",
      "Episode:714 meanR:9.63 R:10.00 loss:9.23\n",
      "Episode:715 meanR:9.62 R:9.00 loss:10.59\n",
      "Episode:716 meanR:9.63 R:10.00 loss:9.01\n",
      "Episode:717 meanR:9.62 R:9.00 loss:10.33\n",
      "Episode:718 meanR:9.63 R:11.00 loss:7.85\n",
      "Episode:719 meanR:9.62 R:9.00 loss:10.07\n",
      "Episode:720 meanR:9.62 R:10.00 loss:8.66\n",
      "Episode:721 meanR:9.61 R:10.00 loss:8.50\n",
      "Episode:722 meanR:9.62 R:10.00 loss:8.38\n",
      "Episode:723 meanR:9.60 R:9.00 loss:9.75\n",
      "Episode:724 meanR:9.60 R:9.00 loss:9.56\n",
      "Episode:725 meanR:9.58 R:9.00 loss:9.53\n",
      "Episode:726 meanR:9.57 R:9.00 loss:9.28\n",
      "Episode:727 meanR:9.56 R:9.00 loss:9.26\n",
      "Episode:728 meanR:9.56 R:9.00 loss:9.13\n",
      "Episode:729 meanR:9.56 R:9.00 loss:8.90\n",
      "Episode:730 meanR:9.52 R:8.00 loss:10.56\n",
      "Episode:731 meanR:9.53 R:11.00 loss:6.59\n",
      "Episode:732 meanR:9.51 R:9.00 loss:8.66\n",
      "Episode:733 meanR:9.50 R:9.00 loss:8.64\n",
      "Episode:734 meanR:9.49 R:9.00 loss:8.63\n",
      "Episode:735 meanR:9.49 R:9.00 loss:8.27\n",
      "Episode:736 meanR:9.50 R:10.00 loss:6.93\n",
      "Episode:737 meanR:9.48 R:8.00 loss:9.40\n",
      "Episode:738 meanR:9.49 R:9.00 loss:7.91\n",
      "Episode:739 meanR:9.50 R:10.00 loss:7.02\n",
      "Episode:740 meanR:9.49 R:9.00 loss:7.79\n",
      "Episode:741 meanR:9.47 R:9.00 loss:7.75\n",
      "Episode:742 meanR:9.45 R:9.00 loss:7.55\n",
      "Episode:743 meanR:9.44 R:8.00 loss:8.74\n",
      "Episode:744 meanR:9.44 R:9.00 loss:7.38\n",
      "Episode:745 meanR:9.43 R:10.00 loss:6.33\n",
      "Episode:746 meanR:9.42 R:11.00 loss:5.42\n",
      "Episode:747 meanR:9.43 R:10.00 loss:6.14\n",
      "Episode:748 meanR:9.43 R:9.00 loss:7.00\n",
      "Episode:749 meanR:9.43 R:10.00 loss:6.04\n",
      "Episode:750 meanR:9.43 R:9.00 loss:6.84\n",
      "Episode:751 meanR:9.43 R:9.00 loss:6.83\n",
      "Episode:752 meanR:9.43 R:9.00 loss:6.68\n",
      "Episode:753 meanR:9.42 R:9.00 loss:6.67\n",
      "Episode:754 meanR:9.44 R:11.00 loss:5.29\n",
      "Episode:755 meanR:9.46 R:10.00 loss:5.70\n",
      "Episode:756 meanR:9.46 R:9.00 loss:6.25\n",
      "Episode:757 meanR:9.45 R:8.00 loss:7.82\n",
      "Episode:758 meanR:9.45 R:9.00 loss:6.37\n",
      "Episode:759 meanR:9.43 R:8.00 loss:7.54\n",
      "Episode:760 meanR:9.43 R:9.00 loss:5.96\n",
      "Episode:761 meanR:9.44 R:10.00 loss:5.17\n",
      "Episode:762 meanR:9.45 R:11.00 loss:4.48\n",
      "Episode:763 meanR:9.45 R:10.00 loss:4.96\n",
      "Episode:764 meanR:9.46 R:11.00 loss:4.36\n",
      "Episode:765 meanR:9.44 R:9.00 loss:5.61\n",
      "Episode:766 meanR:9.47 R:13.00 loss:3.89\n",
      "Episode:767 meanR:9.48 R:11.00 loss:4.33\n",
      "Episode:768 meanR:9.49 R:11.00 loss:4.23\n",
      "Episode:769 meanR:9.48 R:9.00 loss:5.44\n",
      "Episode:770 meanR:9.47 R:8.00 loss:6.47\n",
      "Episode:771 meanR:9.46 R:10.00 loss:4.98\n",
      "Episode:772 meanR:9.49 R:11.00 loss:4.06\n",
      "Episode:773 meanR:9.49 R:9.00 loss:5.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:774 meanR:9.47 R:9.00 loss:5.64\n",
      "Episode:775 meanR:9.49 R:10.00 loss:4.33\n",
      "Episode:776 meanR:9.49 R:10.00 loss:4.23\n",
      "Episode:777 meanR:9.48 R:8.00 loss:5.79\n",
      "Episode:778 meanR:9.49 R:10.00 loss:4.17\n",
      "Episode:779 meanR:9.49 R:9.00 loss:4.68\n",
      "Episode:780 meanR:9.47 R:9.00 loss:4.47\n",
      "Episode:781 meanR:9.48 R:10.00 loss:3.67\n",
      "Episode:782 meanR:9.46 R:10.00 loss:3.69\n",
      "Episode:783 meanR:9.45 R:9.00 loss:4.03\n",
      "Episode:784 meanR:9.44 R:9.00 loss:3.88\n",
      "Episode:785 meanR:9.44 R:10.00 loss:3.32\n",
      "Episode:786 meanR:9.45 R:10.00 loss:3.37\n",
      "Episode:787 meanR:9.45 R:10.00 loss:3.12\n",
      "Episode:788 meanR:9.44 R:8.00 loss:4.89\n",
      "Episode:789 meanR:9.44 R:9.00 loss:3.68\n",
      "Episode:790 meanR:9.43 R:9.00 loss:3.49\n",
      "Episode:791 meanR:9.44 R:10.00 loss:2.96\n",
      "Episode:792 meanR:9.45 R:9.00 loss:3.39\n",
      "Episode:793 meanR:9.46 R:10.00 loss:2.98\n",
      "Episode:794 meanR:9.47 R:10.00 loss:2.77\n",
      "Episode:795 meanR:9.46 R:10.00 loss:2.83\n",
      "Episode:796 meanR:9.44 R:9.00 loss:3.19\n",
      "Episode:797 meanR:9.45 R:11.00 loss:2.53\n",
      "Episode:798 meanR:9.46 R:10.00 loss:2.78\n",
      "Episode:799 meanR:9.47 R:9.00 loss:3.16\n",
      "Episode:800 meanR:9.48 R:10.00 loss:2.59\n",
      "Episode:801 meanR:9.49 R:9.00 loss:2.99\n",
      "Episode:802 meanR:9.49 R:10.00 loss:2.82\n",
      "Episode:803 meanR:9.49 R:10.00 loss:2.42\n",
      "Episode:804 meanR:9.49 R:9.00 loss:3.12\n",
      "Episode:805 meanR:9.49 R:10.00 loss:2.43\n",
      "Episode:806 meanR:9.49 R:10.00 loss:2.34\n",
      "Episode:807 meanR:9.51 R:10.00 loss:2.39\n",
      "Episode:808 meanR:9.52 R:10.00 loss:2.32\n",
      "Episode:809 meanR:9.51 R:9.00 loss:2.61\n",
      "Episode:810 meanR:9.52 R:9.00 loss:2.63\n",
      "Episode:811 meanR:9.52 R:10.00 loss:2.18\n",
      "Episode:812 meanR:9.52 R:10.00 loss:2.23\n",
      "Episode:813 meanR:9.54 R:11.00 loss:1.98\n",
      "Episode:814 meanR:9.54 R:10.00 loss:2.08\n",
      "Episode:815 meanR:9.54 R:9.00 loss:2.67\n",
      "Episode:816 meanR:9.54 R:10.00 loss:2.02\n",
      "Episode:817 meanR:9.55 R:10.00 loss:2.01\n",
      "Episode:818 meanR:9.54 R:10.00 loss:2.03\n",
      "Episode:819 meanR:9.55 R:10.00 loss:1.96\n",
      "Episode:820 meanR:9.56 R:11.00 loss:1.74\n",
      "Episode:821 meanR:9.56 R:10.00 loss:1.89\n",
      "Episode:822 meanR:9.56 R:10.00 loss:1.83\n",
      "Episode:823 meanR:9.56 R:9.00 loss:2.12\n",
      "Episode:824 meanR:9.56 R:9.00 loss:2.06\n",
      "Episode:825 meanR:9.57 R:10.00 loss:1.74\n",
      "Episode:826 meanR:9.58 R:10.00 loss:1.72\n",
      "Episode:827 meanR:9.58 R:9.00 loss:2.29\n",
      "Episode:828 meanR:9.58 R:9.00 loss:1.95\n",
      "Episode:829 meanR:9.58 R:9.00 loss:1.93\n",
      "Episode:830 meanR:9.59 R:9.00 loss:2.03\n",
      "Episode:831 meanR:9.57 R:9.00 loss:1.88\n",
      "Episode:832 meanR:9.57 R:9.00 loss:1.82\n",
      "Episode:833 meanR:9.58 R:10.00 loss:1.69\n",
      "Episode:834 meanR:9.58 R:9.00 loss:1.78\n",
      "Episode:835 meanR:9.59 R:10.00 loss:1.49\n",
      "Episode:836 meanR:9.58 R:9.00 loss:1.59\n",
      "Episode:837 meanR:9.60 R:10.00 loss:1.45\n",
      "Episode:838 meanR:9.59 R:8.00 loss:1.86\n",
      "Episode:839 meanR:9.59 R:10.00 loss:1.38\n",
      "Episode:840 meanR:9.58 R:8.00 loss:1.74\n",
      "Episode:841 meanR:9.58 R:9.00 loss:1.38\n",
      "Episode:842 meanR:9.59 R:10.00 loss:1.31\n",
      "Episode:843 meanR:9.62 R:11.00 loss:1.49\n",
      "Episode:844 meanR:9.63 R:10.00 loss:1.12\n",
      "Episode:845 meanR:9.63 R:10.00 loss:1.25\n",
      "Episode:846 meanR:9.62 R:10.00 loss:1.15\n",
      "Episode:847 meanR:9.61 R:9.00 loss:1.23\n",
      "Episode:848 meanR:9.62 R:10.00 loss:1.42\n",
      "Episode:849 meanR:9.61 R:9.00 loss:1.19\n",
      "Episode:850 meanR:9.61 R:9.00 loss:1.12\n",
      "Episode:851 meanR:9.61 R:9.00 loss:1.12\n",
      "Episode:852 meanR:9.62 R:10.00 loss:0.87\n",
      "Episode:853 meanR:9.63 R:10.00 loss:0.87\n",
      "Episode:854 meanR:9.61 R:9.00 loss:0.94\n",
      "Episode:855 meanR:9.59 R:8.00 loss:1.17\n",
      "Episode:856 meanR:9.59 R:9.00 loss:0.86\n",
      "Episode:857 meanR:9.61 R:10.00 loss:0.86\n",
      "Episode:858 meanR:9.62 R:10.00 loss:0.74\n",
      "Episode:859 meanR:9.66 R:12.00 loss:1.25\n",
      "Episode:860 meanR:9.66 R:9.00 loss:0.81\n",
      "Episode:861 meanR:9.66 R:10.00 loss:0.77\n",
      "Episode:862 meanR:9.64 R:9.00 loss:0.89\n",
      "Episode:863 meanR:9.63 R:9.00 loss:0.84\n",
      "Episode:864 meanR:9.61 R:9.00 loss:1.00\n",
      "Episode:865 meanR:9.61 R:9.00 loss:1.07\n",
      "Episode:866 meanR:9.58 R:10.00 loss:0.78\n",
      "Episode:867 meanR:9.57 R:10.00 loss:0.67\n",
      "Episode:868 meanR:9.56 R:10.00 loss:0.71\n",
      "Episode:869 meanR:9.58 R:11.00 loss:1.17\n",
      "Episode:870 meanR:9.60 R:10.00 loss:1.07\n",
      "Episode:871 meanR:10.32 R:82.00 loss:2.89\n",
      "Episode:872 meanR:11.07 R:86.00 loss:1.07\n",
      "Episode:873 meanR:11.85 R:87.00 loss:1.15\n",
      "Episode:874 meanR:12.80 R:104.00 loss:1.19\n",
      "Episode:875 meanR:13.17 R:47.00 loss:1.96\n",
      "Episode:876 meanR:13.58 R:51.00 loss:1.52\n",
      "Episode:877 meanR:13.91 R:41.00 loss:2.65\n",
      "Episode:878 meanR:14.16 R:35.00 loss:2.65\n",
      "Episode:879 meanR:14.42 R:35.00 loss:12.79\n",
      "Episode:880 meanR:14.43 R:10.00 loss:5.43\n",
      "Episode:881 meanR:14.42 R:9.00 loss:4.10\n",
      "Episode:882 meanR:14.41 R:9.00 loss:4.00\n",
      "Episode:883 meanR:14.66 R:34.00 loss:5.56\n",
      "Episode:884 meanR:14.65 R:8.00 loss:6.15\n",
      "Episode:885 meanR:14.64 R:9.00 loss:3.70\n",
      "Episode:886 meanR:14.63 R:9.00 loss:2.75\n",
      "Episode:887 meanR:14.63 R:10.00 loss:1.97\n",
      "Episode:888 meanR:14.63 R:8.00 loss:3.92\n",
      "Episode:889 meanR:14.63 R:9.00 loss:7.03\n",
      "Episode:890 meanR:14.64 R:10.00 loss:7.35\n",
      "Episode:891 meanR:14.64 R:10.00 loss:11.78\n",
      "Episode:892 meanR:14.74 R:19.00 loss:10.90\n",
      "Episode:893 meanR:14.86 R:22.00 loss:7.99\n",
      "Episode:894 meanR:14.97 R:21.00 loss:10.95\n",
      "Episode:895 meanR:15.15 R:28.00 loss:3.60\n",
      "Episode:896 meanR:15.32 R:26.00 loss:2.82\n",
      "Episode:897 meanR:15.42 R:21.00 loss:3.68\n",
      "Episode:898 meanR:15.48 R:16.00 loss:4.65\n",
      "Episode:899 meanR:15.58 R:19.00 loss:3.13\n",
      "Episode:900 meanR:15.60 R:12.00 loss:5.89\n",
      "Episode:901 meanR:15.67 R:16.00 loss:3.28\n",
      "Episode:902 meanR:15.75 R:18.00 loss:2.50\n",
      "Episode:903 meanR:15.78 R:13.00 loss:4.43\n",
      "Episode:904 meanR:15.83 R:14.00 loss:3.51\n",
      "Episode:905 meanR:16.00 R:27.00 loss:2.31\n",
      "Episode:906 meanR:16.13 R:23.00 loss:1.22\n",
      "Episode:907 meanR:16.20 R:17.00 loss:2.51\n",
      "Episode:908 meanR:16.27 R:17.00 loss:2.61\n",
      "Episode:909 meanR:16.37 R:19.00 loss:2.11\n",
      "Episode:910 meanR:16.47 R:19.00 loss:1.79\n",
      "Episode:911 meanR:16.53 R:16.00 loss:2.98\n",
      "Episode:912 meanR:16.63 R:20.00 loss:1.80\n",
      "Episode:913 meanR:16.69 R:17.00 loss:1.79\n",
      "Episode:914 meanR:16.78 R:19.00 loss:1.93\n",
      "Episode:915 meanR:16.82 R:13.00 loss:2.88\n",
      "Episode:916 meanR:16.87 R:15.00 loss:1.68\n",
      "Episode:917 meanR:16.93 R:16.00 loss:1.70\n",
      "Episode:918 meanR:17.00 R:17.00 loss:1.24\n",
      "Episode:919 meanR:17.04 R:14.00 loss:2.53\n",
      "Episode:920 meanR:17.05 R:12.00 loss:1.09\n",
      "Episode:921 meanR:17.08 R:13.00 loss:1.71\n",
      "Episode:922 meanR:17.25 R:27.00 loss:1.54\n",
      "Episode:923 meanR:17.34 R:18.00 loss:1.93\n",
      "Episode:924 meanR:17.45 R:20.00 loss:1.32\n",
      "Episode:925 meanR:17.49 R:14.00 loss:2.32\n",
      "Episode:926 meanR:17.69 R:30.00 loss:1.47\n",
      "Episode:927 meanR:17.82 R:22.00 loss:1.61\n",
      "Episode:928 meanR:17.87 R:14.00 loss:2.03\n",
      "Episode:929 meanR:18.09 R:31.00 loss:1.65\n",
      "Episode:930 meanR:18.25 R:25.00 loss:0.98\n",
      "Episode:931 meanR:18.39 R:23.00 loss:1.54\n",
      "Episode:932 meanR:18.53 R:23.00 loss:1.01\n",
      "Episode:933 meanR:18.59 R:16.00 loss:2.03\n",
      "Episode:934 meanR:18.63 R:13.00 loss:2.80\n",
      "Episode:935 meanR:18.79 R:26.00 loss:1.23\n",
      "Episode:936 meanR:18.86 R:16.00 loss:1.89\n",
      "Episode:937 meanR:18.93 R:17.00 loss:2.19\n",
      "Episode:938 meanR:19.04 R:19.00 loss:1.68\n",
      "Episode:939 meanR:19.15 R:21.00 loss:1.60\n",
      "Episode:940 meanR:19.23 R:16.00 loss:1.70\n",
      "Episode:941 meanR:19.47 R:33.00 loss:1.71\n",
      "Episode:942 meanR:19.54 R:17.00 loss:2.06\n",
      "Episode:943 meanR:19.62 R:19.00 loss:1.33\n",
      "Episode:944 meanR:19.71 R:19.00 loss:2.04\n",
      "Episode:945 meanR:19.77 R:16.00 loss:1.60\n",
      "Episode:946 meanR:19.84 R:17.00 loss:1.20\n",
      "Episode:947 meanR:19.96 R:21.00 loss:1.12\n",
      "Episode:948 meanR:20.13 R:27.00 loss:1.20\n",
      "Episode:949 meanR:20.21 R:17.00 loss:1.48\n",
      "Episode:950 meanR:20.36 R:24.00 loss:0.95\n",
      "Episode:951 meanR:20.53 R:26.00 loss:1.25\n",
      "Episode:952 meanR:20.64 R:21.00 loss:0.77\n",
      "Episode:953 meanR:20.82 R:28.00 loss:1.04\n",
      "Episode:954 meanR:21.00 R:27.00 loss:0.77\n",
      "Episode:955 meanR:21.07 R:15.00 loss:2.10\n",
      "Episode:956 meanR:21.21 R:23.00 loss:0.87\n",
      "Episode:957 meanR:21.34 R:23.00 loss:0.91\n",
      "Episode:958 meanR:21.45 R:21.00 loss:1.33\n",
      "Episode:959 meanR:21.52 R:19.00 loss:1.57\n",
      "Episode:960 meanR:21.60 R:17.00 loss:1.52\n",
      "Episode:961 meanR:21.66 R:16.00 loss:1.53\n",
      "Episode:962 meanR:21.80 R:23.00 loss:0.82\n",
      "Episode:963 meanR:21.92 R:21.00 loss:1.08\n",
      "Episode:964 meanR:22.06 R:23.00 loss:0.98\n",
      "Episode:965 meanR:22.16 R:19.00 loss:1.22\n",
      "Episode:966 meanR:22.27 R:21.00 loss:0.76\n",
      "Episode:967 meanR:22.35 R:18.00 loss:1.45\n",
      "Episode:968 meanR:22.49 R:24.00 loss:0.83\n",
      "Episode:969 meanR:22.66 R:28.00 loss:0.87\n",
      "Episode:970 meanR:22.80 R:24.00 loss:1.31\n",
      "Episode:971 meanR:22.28 R:30.00 loss:1.01\n",
      "Episode:972 meanR:21.81 R:39.00 loss:1.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:973 meanR:21.28 R:34.00 loss:1.27\n",
      "Episode:974 meanR:20.53 R:29.00 loss:1.31\n",
      "Episode:975 meanR:20.31 R:25.00 loss:1.86\n",
      "Episode:976 meanR:20.02 R:22.00 loss:1.61\n",
      "Episode:977 meanR:19.99 R:38.00 loss:2.60\n",
      "Episode:978 meanR:19.95 R:31.00 loss:1.55\n",
      "Episode:979 meanR:19.94 R:34.00 loss:1.29\n",
      "Episode:980 meanR:20.25 R:41.00 loss:1.29\n",
      "Episode:981 meanR:20.38 R:22.00 loss:2.36\n",
      "Episode:982 meanR:20.69 R:40.00 loss:1.76\n",
      "Episode:983 meanR:20.67 R:32.00 loss:1.47\n",
      "Episode:984 meanR:21.03 R:44.00 loss:1.33\n",
      "Episode:985 meanR:21.27 R:33.00 loss:1.35\n",
      "Episode:986 meanR:21.48 R:30.00 loss:1.43\n",
      "Episode:987 meanR:21.84 R:46.00 loss:1.45\n",
      "Episode:988 meanR:22.22 R:46.00 loss:1.34\n",
      "Episode:989 meanR:22.46 R:33.00 loss:1.60\n",
      "Episode:990 meanR:22.72 R:36.00 loss:1.48\n",
      "Episode:991 meanR:23.00 R:38.00 loss:1.78\n",
      "Episode:992 meanR:23.11 R:30.00 loss:1.98\n",
      "Episode:993 meanR:23.49 R:60.00 loss:1.84\n",
      "Episode:994 meanR:23.64 R:36.00 loss:1.39\n",
      "Episode:995 meanR:23.82 R:46.00 loss:1.44\n",
      "Episode:996 meanR:23.98 R:42.00 loss:1.45\n",
      "Episode:997 meanR:24.21 R:44.00 loss:1.24\n",
      "Episode:998 meanR:24.44 R:39.00 loss:1.68\n",
      "Episode:999 meanR:24.65 R:40.00 loss:1.35\n",
      "Episode:1000 meanR:24.96 R:43.00 loss:1.49\n",
      "Episode:1001 meanR:25.39 R:59.00 loss:1.13\n",
      "Episode:1002 meanR:25.64 R:43.00 loss:1.70\n",
      "Episode:1003 meanR:25.98 R:47.00 loss:1.17\n",
      "Episode:1004 meanR:26.23 R:39.00 loss:1.59\n",
      "Episode:1005 meanR:26.37 R:41.00 loss:1.33\n",
      "Episode:1006 meanR:26.54 R:40.00 loss:1.26\n",
      "Episode:1007 meanR:26.73 R:36.00 loss:1.33\n",
      "Episode:1008 meanR:27.07 R:51.00 loss:1.00\n",
      "Episode:1009 meanR:27.21 R:33.00 loss:1.52\n",
      "Episode:1010 meanR:28.20 R:118.00 loss:2.63\n",
      "Episode:1011 meanR:28.62 R:58.00 loss:3.08\n",
      "Episode:1012 meanR:29.30 R:88.00 loss:1.04\n",
      "Episode:1013 meanR:30.53 R:140.00 loss:11.77\n",
      "Episode:1014 meanR:31.51 R:117.00 loss:3.62\n",
      "Episode:1015 meanR:32.24 R:86.00 loss:9.28\n",
      "Episode:1016 meanR:33.63 R:154.00 loss:2.80\n",
      "Episode:1017 meanR:34.44 R:97.00 loss:3.77\n",
      "Episode:1018 meanR:34.61 R:34.00 loss:11.04\n",
      "Episode:1019 meanR:34.78 R:31.00 loss:17.27\n",
      "Episode:1020 meanR:35.00 R:34.00 loss:5.57\n",
      "Episode:1021 meanR:35.43 R:56.00 loss:3.10\n",
      "Episode:1022 meanR:35.50 R:34.00 loss:6.43\n",
      "Episode:1023 meanR:35.63 R:31.00 loss:6.80\n",
      "Episode:1024 meanR:35.85 R:42.00 loss:3.87\n",
      "Episode:1025 meanR:35.92 R:21.00 loss:13.10\n",
      "Episode:1026 meanR:35.89 R:27.00 loss:9.25\n",
      "Episode:1027 meanR:35.84 R:17.00 loss:16.69\n",
      "Episode:1028 meanR:35.92 R:22.00 loss:10.75\n",
      "Episode:1029 meanR:35.85 R:24.00 loss:7.77\n",
      "Episode:1030 meanR:35.80 R:20.00 loss:12.06\n",
      "Episode:1031 meanR:35.82 R:25.00 loss:7.47\n",
      "Episode:1032 meanR:35.79 R:20.00 loss:11.87\n",
      "Episode:1033 meanR:35.88 R:25.00 loss:8.73\n",
      "Episode:1034 meanR:35.98 R:23.00 loss:9.36\n",
      "Episode:1035 meanR:35.87 R:15.00 loss:20.20\n",
      "Episode:1036 meanR:35.92 R:21.00 loss:13.47\n",
      "Episode:1037 meanR:35.98 R:23.00 loss:9.52\n",
      "Episode:1038 meanR:35.95 R:16.00 loss:14.69\n",
      "Episode:1039 meanR:35.93 R:19.00 loss:10.78\n",
      "Episode:1040 meanR:35.98 R:21.00 loss:9.69\n",
      "Episode:1041 meanR:35.81 R:16.00 loss:14.17\n",
      "Episode:1042 meanR:35.80 R:16.00 loss:12.92\n",
      "Episode:1043 meanR:35.71 R:10.00 loss:27.92\n",
      "Episode:1044 meanR:35.65 R:13.00 loss:14.31\n",
      "Episode:1045 meanR:35.66 R:17.00 loss:9.46\n",
      "Episode:1046 meanR:35.64 R:15.00 loss:11.74\n",
      "Episode:1047 meanR:35.55 R:12.00 loss:15.43\n",
      "Episode:1048 meanR:35.40 R:12.00 loss:13.98\n",
      "Episode:1049 meanR:35.40 R:17.00 loss:7.95\n",
      "Episode:1050 meanR:35.35 R:19.00 loss:7.52\n",
      "Episode:1051 meanR:35.24 R:15.00 loss:8.78\n",
      "Episode:1052 meanR:35.15 R:12.00 loss:12.24\n",
      "Episode:1053 meanR:35.06 R:19.00 loss:6.07\n",
      "Episode:1054 meanR:34.96 R:17.00 loss:6.80\n",
      "Episode:1055 meanR:34.98 R:17.00 loss:6.65\n",
      "Episode:1056 meanR:34.90 R:15.00 loss:7.66\n",
      "Episode:1057 meanR:34.86 R:19.00 loss:6.40\n",
      "Episode:1058 meanR:34.84 R:19.00 loss:5.28\n",
      "Episode:1059 meanR:34.79 R:14.00 loss:8.67\n",
      "Episode:1060 meanR:34.77 R:15.00 loss:7.24\n",
      "Episode:1061 meanR:34.76 R:15.00 loss:7.38\n",
      "Episode:1062 meanR:34.74 R:21.00 loss:4.71\n",
      "Episode:1063 meanR:34.72 R:19.00 loss:6.17\n",
      "Episode:1064 meanR:34.67 R:18.00 loss:5.55\n",
      "Episode:1065 meanR:34.66 R:18.00 loss:6.78\n",
      "Episode:1066 meanR:34.61 R:16.00 loss:5.96\n",
      "Episode:1067 meanR:34.59 R:16.00 loss:6.22\n",
      "Episode:1068 meanR:34.51 R:16.00 loss:5.84\n",
      "Episode:1069 meanR:34.39 R:16.00 loss:6.15\n",
      "Episode:1070 meanR:34.32 R:17.00 loss:5.00\n",
      "Episode:1071 meanR:34.22 R:20.00 loss:5.55\n",
      "Episode:1072 meanR:34.17 R:34.00 loss:5.94\n",
      "Episode:1073 meanR:34.10 R:27.00 loss:3.78\n",
      "Episode:1074 meanR:34.08 R:27.00 loss:3.73\n",
      "Episode:1075 meanR:34.30 R:47.00 loss:3.26\n",
      "Episode:1076 meanR:34.51 R:43.00 loss:3.08\n",
      "Episode:1077 meanR:34.48 R:35.00 loss:3.11\n",
      "Episode:1078 meanR:34.78 R:61.00 loss:2.25\n",
      "Episode:1079 meanR:34.95 R:51.00 loss:1.78\n",
      "Episode:1080 meanR:35.11 R:57.00 loss:1.68\n",
      "Episode:1081 meanR:35.78 R:89.00 loss:1.33\n",
      "Episode:1082 meanR:36.81 R:143.00 loss:1.11\n",
      "Episode:1083 meanR:38.23 R:174.00 loss:1.41\n",
      "Episode:1084 meanR:38.51 R:72.00 loss:2.97\n",
      "Episode:1085 meanR:38.80 R:62.00 loss:3.72\n",
      "Episode:1086 meanR:39.44 R:94.00 loss:4.21\n",
      "Episode:1087 meanR:39.62 R:64.00 loss:4.32\n",
      "Episode:1088 meanR:40.43 R:127.00 loss:1.09\n",
      "Episode:1089 meanR:40.52 R:42.00 loss:3.89\n",
      "Episode:1090 meanR:40.70 R:54.00 loss:1.64\n",
      "Episode:1091 meanR:40.62 R:30.00 loss:4.44\n",
      "Episode:1092 meanR:40.78 R:46.00 loss:2.68\n",
      "Episode:1093 meanR:40.62 R:44.00 loss:2.98\n",
      "Episode:1094 meanR:40.61 R:35.00 loss:3.21\n",
      "Episode:1095 meanR:40.40 R:25.00 loss:3.96\n",
      "Episode:1096 meanR:40.13 R:15.00 loss:11.57\n",
      "Episode:1097 meanR:39.91 R:22.00 loss:6.45\n",
      "Episode:1098 meanR:39.65 R:13.00 loss:9.13\n",
      "Episode:1099 meanR:39.34 R:9.00 loss:14.62\n",
      "Episode:1100 meanR:39.02 R:11.00 loss:8.97\n",
      "Episode:1101 meanR:38.53 R:10.00 loss:8.28\n",
      "Episode:1102 meanR:38.53 R:43.00 loss:33.66\n",
      "Episode:1103 meanR:38.31 R:25.00 loss:42.70\n",
      "Episode:1104 meanR:38.09 R:17.00 loss:19.37\n",
      "Episode:1105 meanR:37.89 R:21.00 loss:11.26\n",
      "Episode:1106 meanR:37.68 R:19.00 loss:8.33\n",
      "Episode:1107 meanR:37.53 R:21.00 loss:6.47\n",
      "Episode:1108 meanR:37.23 R:21.00 loss:6.07\n",
      "Episode:1109 meanR:37.11 R:21.00 loss:4.98\n",
      "Episode:1110 meanR:36.18 R:25.00 loss:3.63\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "        batch = deque(maxlen=batch_size)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action, final_state =  act(sess=sess, state=state, initial_state=initial_state, is_training=False)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            batch.append([state, action, next_state, reward, float(done), initial_state])\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            initial_state = final_state\n",
    "\n",
    "            # Training\n",
    "            #print('len(batch), batch.maxlen', len(batch), batch.maxlen)\n",
    "            if len(batch) >= 5:\n",
    "                loss = learn(batch=batch, gamma=gamma, sess=sess)\n",
    "            else: loss = 0\n",
    "                \n",
    "            loss_batch.append(loss)\n",
    "                \n",
    "            if done is True:\n",
    "                break\n",
    "            \n",
    "        #print('len(batch), batch.maxlen', len(batch), batch.maxlen)\n",
    "        if len(batch) > 5:\n",
    "            if len(batch) < batch_size:\n",
    "                for _ in range(10):\n",
    "                    loss = learn(batch=batch, gamma=gamma, sess=sess)\n",
    "                    loss_batch.append(loss)\n",
    "\n",
    "        # Outputing: priting out/Potting\n",
    "        episode_reward.append(total_reward)\n",
    "        \n",
    "        print('Episode:{}'.format(ep),\n",
    "              'meanR:{:.2f}'.format(np.mean(episode_reward)),\n",
    "              'R:{:.2f}'.format(total_reward),\n",
    "              'loss:{:.2f}'.format(np.mean(loss_batch)))\n",
    "        \n",
    "        # Ploting out\n",
    "        episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "        rewards_list.append([ep, total_reward])\n",
    "        loss_list.append([ep, np.mean(loss_batch)])\n",
    "        \n",
    "        # Break episode/epoch loop\n",
    "        if np.mean(episode_reward) >= 500:\n",
    "            break\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total rewards')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average losses')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episode/epoch\n",
    "    for _ in range(10):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "        \n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                    feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                                 model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # At the end of each episode\n",
    "        print('total_reward:{}'.format(total_reward))\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
