{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent DQN\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.12.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# state = env.reset()\n",
    "# for _ in range(10):\n",
    "#     # env.render()\n",
    "#     action = env.action_space.sample()\n",
    "#     next_state, reward, done, info = env.step(action) # take a random action\n",
    "#     #print('state, action, next_state, reward, done, info:', state, action, next_state, reward, done, info)\n",
    "#     state = next_state\n",
    "#     if done:\n",
    "#         state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size, hidden_size, batch_size=1):\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell], state_is_tuple=False)\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    is_training = tf.placeholder(dtype=tf.bool, shape=[], name='is_training')\n",
    "    print('initial_state, cells, cell', initial_state, cells, cell)\n",
    "    return states, actions, targetQs, cells, initial_state, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(random_seed=1, dtype=tf.float32, uniform=False):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(\n",
    "        dtype=dtype,\n",
    "        seed=tf.set_random_seed(random_seed), \n",
    "        uniform=uniform) # False: normal\n",
    "    return xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(inputs, units, trainable=True):\n",
    "    outputs = tf.layers.dense(\n",
    "        inputs=inputs,\n",
    "        units=units,\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=init_xavier(), # Xavier with normal init\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        trainable=trainable,\n",
    "        name=None,\n",
    "        reuse=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.leaky_relu(\n",
    "#     features,\n",
    "#     alpha=0.2,\n",
    "#     name=None\n",
    "# )\n",
    "def nl(inputs, alpha=0.2):\n",
    "    outputs = tf.maximum(alpha * inputs, inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(inputs, is_training=False):\n",
    "    outputs = tf.layers.batch_normalization(\n",
    "        inputs=inputs,\n",
    "        axis=-1,\n",
    "        momentum=0.99,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        beta_initializer=tf.zeros_initializer(),\n",
    "        gamma_initializer=tf.ones_initializer(),\n",
    "        moving_mean_initializer=tf.zeros_initializer(),\n",
    "        moving_variance_initializer=tf.ones_initializer(),\n",
    "        beta_regularizer=None,\n",
    "        gamma_regularizer=None,\n",
    "        beta_constraint=None,\n",
    "        gamma_constraint=None,\n",
    "        training=is_training,\n",
    "        trainable=True,\n",
    "        name=None,\n",
    "        reuse=None,\n",
    "        renorm=False,\n",
    "        renorm_clipping=None,\n",
    "        renorm_momentum=0.99,\n",
    "        fused=None,\n",
    "        virtual_batch_size=None,\n",
    "        adjustment=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic or Discriminator/classifier or Q-Net/Q-function\n",
    "def D(states, action_size, initial_state, cells, hidden_size, reuse=False, is_training=False): \n",
    "    with tf.variable_scope('D', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        inputs = mlp(inputs=states, units=hidden_size)\n",
    "        inputs = bn(inputs=inputs, is_training=is_training)\n",
    "        inputs = nl(inputs)\n",
    "        print(states.shape, inputs.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        # dynamic means adapt to the batch_size and\n",
    "        # static means can NOT adapt to the batch_size\n",
    "        inputs_rnn = tf.reshape(inputs, [1, -1, hidden_size]) # NxH -> 1xNxH\n",
    "        print(inputs_rnn.shape, initial_state)\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, \n",
    "                                                     initial_state=initial_state)\n",
    "        #outputs_rnn = tf.layers.batch_normalization(inputs=outputs_rnn, training=is_training)\n",
    "        #final_state = tf.layers.batch_normalization(inputs=final_state, training=is_training)\n",
    "        print(outputs_rnn.shape, final_state)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, hidden_size]) # 1xNxH -> NxH\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        actions_logits = mlp(inputs=outputs, units=action_size)\n",
    "        print(actions_logits.shape)\n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # logits are the action logits\n",
    "        return actions_logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(action_size, hidden_size, states, cells, initial_state, actions, targetQs, is_training=False):\n",
    "    \n",
    "    actions_logits, final_state = D(states=states, cells=cells, initial_state=initial_state, \n",
    "                                    hidden_size=hidden_size, action_size=action_size, is_training=is_training)\n",
    "    \n",
    "    actions_labels = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    \n",
    "    Qs = tf.reduce_max(actions_logits*actions_labels, axis=1)\n",
    "    \n",
    "    loss = tf.reduce_mean((Qs - targetQs)**2)\n",
    "    \n",
    "    return actions_logits, final_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('D')]\n",
    "\n",
    "    # # Optimize MLP/CNN/RNN without clipping grads\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=d_vars)\n",
    "\n",
    "        # Optimize RNN\n",
    "        #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "        #grads = tf.gradients(loss, g_vars)\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, g_vars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        (self.states, self.actions, self.targetQs, cells, self.initial_state, \\\n",
    "         self.is_training) = model_input(state_size=state_size, hidden_size=hidden_size)\n",
    "        \n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.final_state, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, \n",
    "            states=self.states, actions=self.actions, \n",
    "            targetQs=self.targetQs, cells=cells, initial_state=self.initial_state, \n",
    "            is_training=self.is_training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "action_size = 2\n",
    "state_size = 4\n",
    "hidden_size = 128               # number of units in each Q-network hidden layer\n",
    "learning_rate = 1e-4         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "# memory_size = 128            # memory capacity - 1000 DQN\n",
    "batch_size = 128             # experience mini-batch size - 20 DQN\n",
    "gamma = 0.99                 # future reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_state, cells, cell Tensor(\"MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0\", shape=(1, 128), dtype=float32) <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fa4101f44a8> <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7fa4d796fe10>\n",
      "(?, 4) (?, 128)\n",
      "(1, ?, 128) Tensor(\"MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0\", shape=(1, 128), dtype=float32)\n",
      "(1, ?, 128) Tensor(\"D/rnn/while/Exit_3:0\", shape=(1, 128), dtype=float32)\n",
      "(?, 128)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "graph = tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# # Init the memory\n",
    "# memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'strided_slice:0' shape=(128,) dtype=float32>,\n",
       " <tf.Tensor 'D/rnn/while/Exit_3:0' shape=(1, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/MultiRNNCellZeroState/zeros:0' shape=(1, 128) dtype=float32>,\n",
       " <tf.Tensor 'is_training:0' shape=() dtype=bool>,\n",
       " <tf.Tensor 'actions:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'D/dense_1/BiasAdd:0' shape=(?, 2) dtype=float32>,\n",
       " <tf.Tensor 'Mean:0' shape=() dtype=float32>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Tensor 'states:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'targetQs:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.initial_state[0], model.final_state, model.initial_state, model.is_training, model.actions, \\\n",
    "model.actions_logits, model.loss, model.opt, model.states, model.targetQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(sess, batch, gamma=0.99):\n",
    "    \n",
    "    states = np.array([each[0] for each in batch if each is not None])\n",
    "    actions = np.array([each[1] for each in batch if each is not None])\n",
    "    next_states = np.array([each[2] for each in batch if each is not None])\n",
    "    rewards = np.array([each[3] for each in batch if each is not None])\n",
    "    dones = np.array([each[4] for each in batch if each is not None])\n",
    "    initial_states = np.array([each[5] for each in batch if each is not None])\n",
    "    #print('initial_states.shape', initial_states.shape)\n",
    "    #print('initial_states[0].shape', initial_states[0].shape)\n",
    "\n",
    "    next_actions_logits = sess.run(model.actions_logits, \n",
    "                                   feed_dict = {model.states: next_states,\n",
    "                                                model.initial_state: initial_states[1],\n",
    "                                                model.is_training: False})\n",
    "\n",
    "    nextQs = np.max(next_actions_logits, axis=1)\n",
    "    targetQs = rewards + (gamma * nextQs * (1-dones))\n",
    "\n",
    "    loss, _ = sess.run([model.loss, model.opt],\n",
    "                       feed_dict = {model.states: states, \n",
    "                                    model.actions: actions,\n",
    "                                    model.targetQs: targetQs,\n",
    "                                    model.initial_state: initial_states[0],    \n",
    "                                    model.is_training: False})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(sess, state, initial_state, is_training=False):\n",
    "    \n",
    "    action_logits, final_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                          feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                       model.is_training: is_training,\n",
    "                                                       model.initial_state: initial_state})\n",
    "    action = np.argmax(action_logits, axis=1)[0]\n",
    "    #print('action_logits, np.argmax(action_logits, axis=1)')\n",
    "    #print(action_logits.shape, np.argmax(action_logits, axis=1).shape)\n",
    "    return action, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(batch) < batch_size 26 128\n",
      "len(batch) < batch_size 73 128\n",
      "len(batch) < batch_size 93 128\n",
      "len(batch) < batch_size 113 128\n",
      "Episode:4 meanR:15.00 R:15.00 loss:0.99\n",
      "Episode:5 meanR:16.00 R:17.00 loss:1.07\n",
      "Episode:6 meanR:13.67 R:9.00 loss:1.17\n",
      "Episode:7 meanR:12.25 R:8.00 loss:1.23\n",
      "Episode:8 meanR:11.80 R:10.00 loss:1.40\n",
      "Episode:9 meanR:11.33 R:9.00 loss:1.82\n",
      "Episode:10 meanR:11.14 R:10.00 loss:2.80\n",
      "Episode:11 meanR:11.00 R:10.00 loss:4.47\n",
      "Episode:12 meanR:10.89 R:10.00 loss:5.76\n",
      "Episode:13 meanR:10.70 R:9.00 loss:6.18\n",
      "Episode:14 meanR:10.55 R:9.00 loss:5.65\n",
      "Episode:15 meanR:10.42 R:9.00 loss:5.96\n",
      "Episode:16 meanR:10.31 R:9.00 loss:6.24\n",
      "Episode:17 meanR:10.21 R:9.00 loss:6.46\n",
      "Episode:18 meanR:10.13 R:9.00 loss:6.62\n",
      "Episode:19 meanR:10.12 R:10.00 loss:6.61\n",
      "Episode:20 meanR:10.06 R:9.00 loss:6.43\n",
      "Episode:21 meanR:10.06 R:10.00 loss:6.35\n",
      "Episode:22 meanR:10.00 R:9.00 loss:6.27\n",
      "Episode:23 meanR:9.95 R:9.00 loss:6.22\n",
      "Episode:24 meanR:9.86 R:8.00 loss:6.23\n",
      "Episode:25 meanR:9.86 R:10.00 loss:6.13\n",
      "Episode:26 meanR:9.83 R:9.00 loss:5.93\n",
      "Episode:27 meanR:9.83 R:10.00 loss:5.60\n",
      "Episode:28 meanR:9.84 R:10.00 loss:5.29\n",
      "Episode:29 meanR:9.81 R:9.00 loss:5.03\n",
      "Episode:30 meanR:9.78 R:9.00 loss:4.79\n",
      "Episode:31 meanR:9.79 R:10.00 loss:4.47\n",
      "Episode:32 meanR:9.76 R:9.00 loss:4.15\n",
      "Episode:33 meanR:9.73 R:9.00 loss:3.83\n",
      "Episode:34 meanR:9.74 R:10.00 loss:3.34\n",
      "Episode:35 meanR:9.78 R:11.00 loss:2.94\n",
      "Episode:36 meanR:9.79 R:10.00 loss:2.59\n",
      "Episode:37 meanR:9.82 R:11.00 loss:2.34\n",
      "Episode:38 meanR:10.00 R:16.00 loss:2.16\n",
      "Episode:39 meanR:10.19 R:17.00 loss:2.45\n",
      "Episode:40 meanR:10.35 R:16.00 loss:2.87\n",
      "Episode:41 meanR:10.55 R:18.00 loss:3.22\n",
      "Episode:42 meanR:10.72 R:17.00 loss:3.52\n",
      "Episode:43 meanR:10.90 R:18.00 loss:3.82\n",
      "Episode:44 meanR:11.05 R:17.00 loss:4.02\n",
      "Episode:45 meanR:11.31 R:22.00 loss:4.14\n",
      "Episode:46 meanR:11.70 R:28.00 loss:3.86\n",
      "Episode:47 meanR:12.09 R:29.00 loss:3.71\n",
      "Episode:48 meanR:12.49 R:30.00 loss:4.00\n",
      "Episode:49 meanR:13.00 R:36.00 loss:4.54\n",
      "Episode:50 meanR:13.45 R:34.00 loss:5.07\n",
      "Episode:51 meanR:13.60 R:21.00 loss:5.94\n",
      "Episode:52 meanR:13.80 R:23.00 loss:7.37\n",
      "Episode:53 meanR:13.98 R:23.00 loss:8.90\n",
      "Episode:54 meanR:14.12 R:21.00 loss:9.91\n",
      "Episode:55 meanR:14.10 R:13.00 loss:11.46\n",
      "Episode:56 meanR:14.15 R:17.00 loss:12.08\n",
      "Episode:57 meanR:14.20 R:17.00 loss:13.30\n",
      "Episode:58 meanR:14.20 R:14.00 loss:13.75\n",
      "Episode:59 meanR:14.38 R:24.00 loss:9.73\n",
      "Episode:60 meanR:14.81 R:39.00 loss:8.27\n",
      "Episode:61 meanR:14.90 R:20.00 loss:8.83\n",
      "Episode:62 meanR:14.86 R:13.00 loss:9.17\n",
      "Episode:63 meanR:15.05 R:26.00 loss:8.33\n",
      "Episode:64 meanR:15.20 R:24.00 loss:9.07\n",
      "Episode:65 meanR:15.39 R:27.00 loss:9.58\n",
      "Episode:66 meanR:15.60 R:29.00 loss:10.89\n",
      "Episode:67 meanR:15.72 R:23.00 loss:11.28\n",
      "Episode:68 meanR:15.66 R:12.00 loss:11.49\n",
      "Episode:69 meanR:15.64 R:14.00 loss:13.35\n",
      "Episode:70 meanR:15.69 R:19.00 loss:12.36\n",
      "Episode:71 meanR:15.71 R:17.00 loss:11.81\n",
      "Episode:72 meanR:15.77 R:20.00 loss:11.66\n",
      "Episode:73 meanR:15.91 R:26.00 loss:10.41\n",
      "Episode:74 meanR:16.44 R:53.00 loss:8.14\n",
      "Episode:75 meanR:16.50 R:21.00 loss:7.07\n",
      "Episode:76 meanR:16.99 R:52.00 loss:5.70\n",
      "Episode:77 meanR:17.03 R:20.00 loss:4.12\n",
      "Episode:78 meanR:17.61 R:61.00 loss:6.12\n",
      "Episode:79 meanR:17.50 R:9.00 loss:6.45\n",
      "Episode:80 meanR:18.52 R:96.00 loss:5.15\n",
      "Episode:81 meanR:19.10 R:64.00 loss:3.03\n",
      "Episode:82 meanR:19.13 R:21.00 loss:6.15\n",
      "Episode:83 meanR:20.54 R:132.00 loss:8.72\n",
      "Episode:84 meanR:20.40 R:9.00 loss:4.98\n",
      "Episode:85 meanR:20.26 R:9.00 loss:8.39\n",
      "Episode:86 meanR:21.18 R:97.00 loss:5.28\n",
      "Episode:87 meanR:21.68 R:63.00 loss:7.59\n",
      "Episode:88 meanR:21.55 R:11.00 loss:11.31\n",
      "Episode:89 meanR:21.44 R:12.00 loss:12.71\n",
      "Episode:90 meanR:21.33 R:12.00 loss:14.04\n",
      "Episode:91 meanR:21.76 R:59.00 loss:11.74\n",
      "Episode:92 meanR:21.75 R:21.00 loss:13.93\n",
      "Episode:93 meanR:21.70 R:17.00 loss:22.35\n",
      "Episode:94 meanR:21.60 R:13.00 loss:22.09\n",
      "Episode:95 meanR:21.50 R:12.00 loss:25.99\n",
      "Episode:96 meanR:21.41 R:13.00 loss:29.73\n",
      "Episode:97 meanR:21.30 R:11.00 loss:33.47\n",
      "Episode:98 meanR:21.22 R:14.00 loss:37.51\n",
      "Episode:99 meanR:21.60 R:58.00 loss:36.02\n",
      "Episode:100 meanR:21.62 R:23.00 loss:27.65\n",
      "Episode:101 meanR:21.56 R:16.00 loss:25.12\n",
      "Episode:102 meanR:21.49 R:15.00 loss:24.60\n",
      "Episode:103 meanR:21.45 R:17.00 loss:23.60\n",
      "Episode:104 meanR:21.42 R:12.00 loss:23.58\n",
      "Episode:105 meanR:21.39 R:14.00 loss:27.29\n",
      "Episode:106 meanR:22.38 R:108.00 loss:8.94\n",
      "Episode:107 meanR:22.46 R:16.00 loss:7.28\n",
      "Episode:108 meanR:22.47 R:11.00 loss:13.13\n",
      "Episode:109 meanR:22.50 R:12.00 loss:15.65\n",
      "Episode:110 meanR:22.57 R:17.00 loss:19.86\n",
      "Episode:111 meanR:22.65 R:18.00 loss:22.16\n",
      "Episode:112 meanR:22.78 R:23.00 loss:23.88\n",
      "Episode:113 meanR:22.99 R:30.00 loss:25.13\n",
      "Episode:114 meanR:23.08 R:18.00 loss:21.28\n",
      "Episode:115 meanR:23.11 R:12.00 loss:19.52\n",
      "Episode:116 meanR:23.14 R:12.00 loss:18.38\n",
      "Episode:117 meanR:23.18 R:13.00 loss:17.01\n",
      "Episode:118 meanR:23.28 R:19.00 loss:13.69\n",
      "Episode:119 meanR:23.77 R:59.00 loss:9.79\n",
      "Episode:120 meanR:25.10 R:142.00 loss:54.06\n",
      "Episode:121 meanR:25.87 R:87.00 loss:1.07\n",
      "Episode:122 meanR:26.37 R:59.00 loss:5.91\n",
      "Episode:123 meanR:26.52 R:24.00 loss:11.21\n",
      "Episode:124 meanR:26.66 R:22.00 loss:16.54\n",
      "Episode:125 meanR:27.34 R:78.00 loss:11.28\n",
      "Episode:126 meanR:28.65 R:140.00 loss:4.77\n",
      "Episode:127 meanR:30.26 R:171.00 loss:5.89\n",
      "Episode:128 meanR:31.56 R:140.00 loss:4.48\n",
      "Episode:129 meanR:32.84 R:137.00 loss:9.57\n",
      "Episode:130 meanR:33.35 R:60.00 loss:7.28\n",
      "Episode:131 meanR:33.48 R:23.00 loss:16.96\n",
      "Episode:132 meanR:33.99 R:60.00 loss:92.03\n",
      "Episode:133 meanR:34.75 R:85.00 loss:270.86\n",
      "Episode:134 meanR:35.16 R:51.00 loss:18.44\n",
      "Episode:135 meanR:35.37 R:32.00 loss:22.52\n",
      "Episode:136 meanR:35.50 R:23.00 loss:34.85\n",
      "Episode:137 meanR:35.54 R:15.00 loss:38.88\n",
      "Episode:138 meanR:35.50 R:12.00 loss:42.60\n",
      "Episode:139 meanR:35.44 R:11.00 loss:50.52\n",
      "Episode:140 meanR:35.39 R:11.00 loss:57.39\n",
      "Episode:141 meanR:35.33 R:12.00 loss:60.39\n",
      "Episode:142 meanR:35.36 R:20.00 loss:37.73\n",
      "Episode:143 meanR:36.12 R:94.00 loss:11.84\n",
      "Episode:144 meanR:36.63 R:68.00 loss:11.85\n",
      "Episode:145 meanR:37.54 R:113.00 loss:14.02\n",
      "Episode:146 meanR:37.75 R:49.00 loss:6.74\n",
      "Episode:147 meanR:37.74 R:28.00 loss:6.21\n",
      "Episode:148 meanR:37.86 R:42.00 loss:8.89\n",
      "Episode:149 meanR:37.71 R:21.00 loss:7.75\n",
      "Episode:150 meanR:37.56 R:19.00 loss:16.02\n",
      "Episode:151 meanR:37.72 R:37.00 loss:19.34\n",
      "Episode:152 meanR:39.00 R:151.00 loss:8.41\n",
      "Episode:153 meanR:42.38 R:361.00 loss:1.24\n",
      "Episode:154 meanR:42.83 R:66.00 loss:6.79\n",
      "Episode:155 meanR:44.29 R:159.00 loss:10.77\n",
      "Episode:156 meanR:44.62 R:50.00 loss:18.11\n",
      "Episode:157 meanR:44.62 R:17.00 loss:35.74\n",
      "Episode:158 meanR:44.67 R:19.00 loss:51.21\n",
      "Episode:159 meanR:44.69 R:26.00 loss:88.74\n",
      "Episode:160 meanR:46.69 R:239.00 loss:232.17\n",
      "Episode:161 meanR:47.29 R:80.00 loss:5.48\n",
      "Episode:162 meanR:51.16 R:400.00 loss:5.14\n",
      "Episode:163 meanR:51.90 R:100.00 loss:22.15\n",
      "Episode:164 meanR:52.15 R:49.00 loss:33.89\n",
      "Episode:165 meanR:53.26 R:138.00 loss:388.23\n",
      "Episode:166 meanR:55.20 R:223.00 loss:75.38\n",
      "Episode:167 meanR:57.29 R:232.00 loss:12.30\n",
      "Episode:168 meanR:57.37 R:20.00 loss:21.52\n",
      "Episode:169 meanR:57.41 R:18.00 loss:40.86\n",
      "Episode:170 meanR:57.38 R:16.00 loss:61.32\n",
      "Episode:171 meanR:57.34 R:13.00 loss:79.79\n",
      "Episode:172 meanR:57.99 R:85.00 loss:77.85\n",
      "Episode:173 meanR:58.35 R:62.00 loss:37.93\n",
      "Episode:174 meanR:58.39 R:57.00 loss:37.93\n",
      "Episode:175 meanR:58.66 R:48.00 loss:43.72\n",
      "Episode:176 meanR:58.73 R:59.00 loss:46.68\n",
      "Episode:177 meanR:59.23 R:70.00 loss:43.52\n",
      "Episode:178 meanR:61.85 R:323.00 loss:21.47\n",
      "Episode:179 meanR:61.95 R:19.00 loss:19.76\n",
      "Episode:180 meanR:61.25 R:26.00 loss:38.21\n",
      "Episode:181 meanR:64.04 R:343.00 loss:13.43\n",
      "Episode:182 meanR:68.26 R:443.00 loss:5.56\n",
      "Episode:183 meanR:68.33 R:139.00 loss:21.51\n",
      "Episode:184 meanR:70.31 R:207.00 loss:16.28\n",
      "Episode:185 meanR:74.20 R:398.00 loss:8.68\n",
      "Episode:186 meanR:75.25 R:202.00 loss:18.83\n",
      "Episode:187 meanR:75.90 R:128.00 loss:27.45\n",
      "Episode:188 meanR:80.79 R:500.00 loss:7.66\n",
      "Episode:189 meanR:81.69 R:102.00 loss:34.41\n",
      "Episode:190 meanR:82.12 R:55.00 loss:48.41\n",
      "Episode:191 meanR:83.30 R:177.00 loss:36.35\n",
      "Episode:192 meanR:84.01 R:92.00 loss:32.00\n",
      "Episode:193 meanR:84.51 R:67.00 loss:44.64\n",
      "Episode:194 meanR:84.59 R:21.00 loss:46.57\n",
      "Episode:195 meanR:84.62 R:15.00 loss:76.18\n",
      "Episode:196 meanR:84.69 R:20.00 loss:103.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:197 meanR:84.77 R:19.00 loss:119.85\n",
      "Episode:198 meanR:84.80 R:17.00 loss:125.92\n",
      "Episode:199 meanR:84.44 R:22.00 loss:88.29\n",
      "Episode:200 meanR:85.51 R:130.00 loss:25.15\n",
      "Episode:201 meanR:85.83 R:48.00 loss:13.08\n",
      "Episode:202 meanR:86.08 R:40.00 loss:21.65\n",
      "Episode:203 meanR:87.06 R:115.00 loss:15.09\n",
      "Episode:204 meanR:88.11 R:117.00 loss:3.02\n",
      "Episode:205 meanR:89.19 R:122.00 loss:18.61\n",
      "Episode:206 meanR:88.53 R:42.00 loss:24.47\n",
      "Episode:207 meanR:88.75 R:38.00 loss:38.52\n",
      "Episode:208 meanR:90.22 R:158.00 loss:23.99\n",
      "Episode:209 meanR:90.47 R:37.00 loss:24.50\n",
      "Episode:210 meanR:91.70 R:140.00 loss:28.94\n",
      "Episode:211 meanR:92.96 R:144.00 loss:22.73\n",
      "Episode:212 meanR:94.36 R:163.00 loss:21.00\n",
      "Episode:213 meanR:95.12 R:106.00 loss:27.51\n",
      "Episode:214 meanR:97.80 R:286.00 loss:3.11\n",
      "Episode:215 meanR:101.71 R:403.00 loss:3.77\n",
      "Episode:216 meanR:106.59 R:500.00 loss:6.94\n",
      "Episode:217 meanR:111.46 R:500.00 loss:10.78\n",
      "Episode:218 meanR:116.27 R:500.00 loss:11.41\n",
      "Episode:219 meanR:116.78 R:110.00 loss:52.80\n",
      "Episode:220 meanR:120.36 R:500.00 loss:14.78\n",
      "Episode:221 meanR:124.49 R:500.00 loss:13.79\n",
      "Episode:222 meanR:124.52 R:62.00 loss:56.18\n",
      "Episode:223 meanR:125.70 R:142.00 loss:53.85\n",
      "Episode:224 meanR:128.71 R:323.00 loss:17.42\n",
      "Episode:225 meanR:129.31 R:138.00 loss:31.91\n",
      "Episode:226 meanR:129.59 R:168.00 loss:13.23\n",
      "Episode:227 meanR:129.27 R:139.00 loss:5.73\n",
      "Episode:228 meanR:128.63 R:76.00 loss:4.68\n",
      "Episode:229 meanR:127.84 R:58.00 loss:13.39\n",
      "Episode:230 meanR:128.01 R:77.00 loss:7.35\n",
      "Episode:231 meanR:129.14 R:136.00 loss:2.41\n",
      "Episode:232 meanR:129.96 R:142.00 loss:1.07\n",
      "Episode:233 meanR:131.58 R:247.00 loss:0.49\n",
      "Episode:234 meanR:132.87 R:180.00 loss:0.70\n",
      "Episode:235 meanR:134.90 R:235.00 loss:1.26\n",
      "Episode:236 meanR:137.11 R:244.00 loss:0.80\n",
      "Episode:237 meanR:141.96 R:500.00 loss:0.60\n",
      "Episode:238 meanR:142.88 R:104.00 loss:48.07\n",
      "Episode:239 meanR:147.77 R:500.00 loss:2.32\n",
      "Episode:240 meanR:148.36 R:70.00 loss:65.08\n",
      "Episode:241 meanR:151.88 R:364.00 loss:9.71\n",
      "Episode:242 meanR:152.87 R:119.00 loss:5.68\n",
      "Episode:243 meanR:156.93 R:500.00 loss:124.28\n",
      "Episode:244 meanR:160.06 R:381.00 loss:21.32\n",
      "Episode:245 meanR:163.93 R:500.00 loss:686.82\n",
      "Episode:246 meanR:168.44 R:500.00 loss:16.01\n",
      "Episode:247 meanR:173.16 R:500.00 loss:16.92\n",
      "Episode:248 meanR:177.74 R:500.00 loss:15.76\n",
      "Episode:249 meanR:182.53 R:500.00 loss:13.36\n",
      "Episode:250 meanR:187.34 R:500.00 loss:15.14\n",
      "Episode:251 meanR:187.75 R:78.00 loss:356.99\n",
      "Episode:252 meanR:191.24 R:500.00 loss:402.58\n",
      "Episode:253 meanR:192.63 R:500.00 loss:16.09\n",
      "Episode:254 meanR:196.97 R:500.00 loss:15.45\n",
      "Episode:255 meanR:200.38 R:500.00 loss:14.97\n",
      "Episode:256 meanR:204.88 R:500.00 loss:15.47\n",
      "Episode:257 meanR:209.71 R:500.00 loss:14.92\n",
      "Episode:258 meanR:214.52 R:500.00 loss:14.66\n",
      "Episode:259 meanR:219.26 R:500.00 loss:14.35\n",
      "Episode:260 meanR:221.87 R:500.00 loss:15.93\n",
      "Episode:261 meanR:226.07 R:500.00 loss:14.22\n",
      "Episode:262 meanR:227.07 R:500.00 loss:16.04\n",
      "Episode:263 meanR:231.07 R:500.00 loss:15.18\n",
      "Episode:264 meanR:235.58 R:500.00 loss:14.94\n",
      "Episode:265 meanR:239.20 R:500.00 loss:8.47\n",
      "Episode:266 meanR:241.97 R:500.00 loss:16.47\n",
      "Episode:267 meanR:244.65 R:500.00 loss:16.94\n",
      "Episode:268 meanR:244.69 R:24.00 loss:72.96\n",
      "Episode:269 meanR:249.51 R:500.00 loss:336.73\n",
      "Episode:270 meanR:254.35 R:500.00 loss:15.94\n",
      "Episode:271 meanR:259.22 R:500.00 loss:15.54\n",
      "Episode:272 meanR:263.37 R:500.00 loss:15.51\n",
      "Episode:273 meanR:267.75 R:500.00 loss:15.21\n",
      "Episode:274 meanR:272.18 R:500.00 loss:15.15\n",
      "Episode:275 meanR:276.70 R:500.00 loss:15.00\n",
      "Episode:276 meanR:281.11 R:500.00 loss:14.75\n",
      "Episode:277 meanR:285.41 R:500.00 loss:14.89\n",
      "Episode:278 meanR:287.18 R:500.00 loss:14.71\n",
      "Episode:279 meanR:291.99 R:500.00 loss:14.35\n",
      "Episode:280 meanR:296.73 R:500.00 loss:14.98\n",
      "Episode:281 meanR:298.30 R:500.00 loss:14.63\n",
      "Episode:282 meanR:298.87 R:500.00 loss:14.12\n",
      "Episode:283 meanR:302.48 R:500.00 loss:15.24\n",
      "Episode:284 meanR:305.41 R:500.00 loss:15.37\n",
      "Episode:285 meanR:306.43 R:500.00 loss:15.07\n",
      "Episode:286 meanR:309.41 R:500.00 loss:15.59\n",
      "Episode:287 meanR:310.91 R:278.00 loss:673.73\n",
      "Episode:288 meanR:310.91 R:500.00 loss:6.64\n",
      "Episode:289 meanR:314.89 R:500.00 loss:16.44\n",
      "Episode:290 meanR:319.34 R:500.00 loss:16.01\n",
      "Episode:291 meanR:321.92 R:435.00 loss:18.23\n",
      "Episode:292 meanR:326.00 R:500.00 loss:3.99\n",
      "Episode:293 meanR:329.22 R:389.00 loss:22.26\n",
      "Episode:294 meanR:333.27 R:426.00 loss:3.98\n",
      "Episode:295 meanR:338.12 R:500.00 loss:1.04\n",
      "Episode:296 meanR:342.92 R:500.00 loss:5.44\n",
      "Episode:297 meanR:347.73 R:500.00 loss:16.82\n",
      "Episode:298 meanR:352.56 R:500.00 loss:16.94\n",
      "Episode:299 meanR:357.34 R:500.00 loss:14.36\n",
      "Episode:300 meanR:361.04 R:500.00 loss:15.11\n",
      "Episode:301 meanR:365.56 R:500.00 loss:18.83\n",
      "Episode:302 meanR:370.16 R:500.00 loss:18.21\n",
      "Episode:303 meanR:374.01 R:500.00 loss:16.60\n",
      "Episode:304 meanR:377.84 R:500.00 loss:15.90\n",
      "Episode:305 meanR:381.62 R:500.00 loss:16.96\n",
      "Episode:306 meanR:386.20 R:500.00 loss:20.21\n",
      "Episode:307 meanR:390.82 R:500.00 loss:19.31\n",
      "Episode:308 meanR:394.24 R:500.00 loss:17.50\n",
      "Episode:309 meanR:398.87 R:500.00 loss:17.42\n",
      "Episode:310 meanR:402.47 R:500.00 loss:16.76\n",
      "Episode:311 meanR:406.03 R:500.00 loss:15.71\n",
      "Episode:312 meanR:409.40 R:500.00 loss:16.13\n",
      "Episode:313 meanR:413.34 R:500.00 loss:13.62\n",
      "Episode:314 meanR:415.48 R:500.00 loss:14.23\n",
      "Episode:315 meanR:416.45 R:500.00 loss:17.73\n",
      "Episode:316 meanR:416.45 R:500.00 loss:19.59\n",
      "Episode:317 meanR:413.15 R:170.00 loss:51.21\n",
      "Episode:318 meanR:413.15 R:500.00 loss:3.45\n",
      "Episode:319 meanR:417.05 R:500.00 loss:15.85\n",
      "Episode:320 meanR:414.48 R:243.00 loss:32.68\n",
      "Episode:321 meanR:413.48 R:400.00 loss:1.81\n",
      "Episode:322 meanR:417.86 R:500.00 loss:3.37\n",
      "Episode:323 meanR:421.44 R:500.00 loss:13.41\n",
      "Episode:324 meanR:423.21 R:500.00 loss:16.82\n",
      "Episode:325 meanR:426.83 R:500.00 loss:12.95\n",
      "Episode:326 meanR:430.15 R:500.00 loss:16.84\n",
      "Episode:327 meanR:433.76 R:500.00 loss:8.87\n",
      "Episode:328 meanR:438.00 R:500.00 loss:11.57\n",
      "Episode:329 meanR:442.42 R:500.00 loss:16.95\n",
      "Episode:330 meanR:446.65 R:500.00 loss:10.62\n",
      "Episode:331 meanR:446.49 R:120.00 loss:57.93\n",
      "Episode:332 meanR:447.36 R:229.00 loss:5.88\n",
      "Episode:333 meanR:446.48 R:159.00 loss:8.63\n",
      "Episode:334 meanR:446.17 R:149.00 loss:10.53\n",
      "Episode:335 meanR:445.06 R:124.00 loss:10.61\n",
      "Episode:336 meanR:443.93 R:131.00 loss:12.00\n",
      "Episode:337 meanR:439.95 R:102.00 loss:6.89\n",
      "Episode:338 meanR:440.16 R:125.00 loss:15.65\n",
      "Episode:339 meanR:436.39 R:123.00 loss:4.28\n",
      "Episode:340 meanR:436.88 R:119.00 loss:7.74\n",
      "Episode:341 meanR:434.46 R:122.00 loss:2.87\n",
      "Episode:342 meanR:434.35 R:108.00 loss:3.86\n",
      "Episode:343 meanR:430.79 R:144.00 loss:6.40\n",
      "Episode:344 meanR:428.21 R:123.00 loss:1.13\n",
      "Episode:345 meanR:424.78 R:157.00 loss:0.79\n",
      "Episode:346 meanR:421.16 R:138.00 loss:1.05\n",
      "Episode:347 meanR:417.46 R:130.00 loss:1.00\n",
      "Episode:348 meanR:413.66 R:120.00 loss:0.52\n",
      "Episode:349 meanR:409.74 R:108.00 loss:1.16\n",
      "Episode:350 meanR:405.91 R:117.00 loss:0.71\n",
      "Episode:351 meanR:406.20 R:107.00 loss:0.41\n",
      "Episode:352 meanR:402.37 R:117.00 loss:1.63\n",
      "Episode:353 meanR:398.47 R:110.00 loss:0.66\n",
      "Episode:354 meanR:394.67 R:120.00 loss:0.43\n",
      "Episode:355 meanR:390.70 R:103.00 loss:0.38\n",
      "Episode:356 meanR:386.83 R:113.00 loss:1.03\n",
      "Episode:357 meanR:382.93 R:110.00 loss:0.85\n",
      "Episode:358 meanR:378.91 R:98.00 loss:1.04\n",
      "Episode:359 meanR:374.60 R:69.00 loss:1.24\n",
      "Episode:360 meanR:370.02 R:42.00 loss:6.70\n",
      "Episode:361 meanR:366.23 R:121.00 loss:4.91\n",
      "Episode:362 meanR:362.35 R:112.00 loss:1.71\n",
      "Episode:363 meanR:358.63 R:128.00 loss:3.48\n",
      "Episode:364 meanR:354.88 R:125.00 loss:2.32\n",
      "Episode:365 meanR:351.35 R:147.00 loss:1.03\n",
      "Episode:366 meanR:347.70 R:135.00 loss:3.27\n",
      "Episode:367 meanR:344.28 R:158.00 loss:1.13\n",
      "Episode:368 meanR:345.36 R:132.00 loss:3.55\n",
      "Episode:369 meanR:341.57 R:121.00 loss:1.61\n",
      "Episode:370 meanR:338.45 R:188.00 loss:2.11\n",
      "Episode:371 meanR:337.57 R:412.00 loss:16.38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "episode_rewards_list, rewards_list, loss_list = [], [], []\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    episode_reward = deque(maxlen=100) # 100 episodes average/running average/running mean/window\n",
    "    batch = deque(maxlen=batch_size)\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(1111):\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "\n",
    "        # Training steps/batches\n",
    "        while True:\n",
    "            action, final_state =  act(sess=sess, state=state, initial_state=initial_state)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            batch.append([state, action, next_state, reward, float(done), initial_state])\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            initial_state = final_state\n",
    "\n",
    "            # Training/learning\n",
    "            #print('len(batch), batch.maxlen', len(batch), batch.maxlen)\n",
    "            if len(batch) >= batch_size:\n",
    "                loss = learn(sess=sess, batch=batch, gamma=gamma)\n",
    "                loss_batch.append(loss)\n",
    "                \n",
    "            if done is True:\n",
    "                break\n",
    "            \n",
    "        if len(batch) >= batch_size:\n",
    "            # Outputing: priting out/Potting\n",
    "            episode_reward.append(total_reward)\n",
    "\n",
    "            print('Episode:{}'.format(ep),\n",
    "                  'meanR:{:.2f}'.format(np.mean(episode_reward)),\n",
    "                  'R:{:.2f}'.format(total_reward),\n",
    "                  'loss:{:.2f}'.format(np.mean(loss_batch)))\n",
    "\n",
    "            # Ploting out\n",
    "            episode_rewards_list.append([ep, np.mean(episode_reward)])\n",
    "            rewards_list.append([ep, total_reward])\n",
    "            loss_list.append([ep, np.mean(loss_batch)])\n",
    "\n",
    "            # Break episode/epoch loop\n",
    "            if np.mean(episode_reward) >= 500:\n",
    "                break\n",
    "                \n",
    "        else: print('len(batch) < batch_size', len(batch), batch_size )\n",
    "            \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(episode_rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n",
      "total_reward:500.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episode/epoch\n",
    "    for _ in range(10):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        initial_state = sess.run(model.initial_state) # Qs or current batch or states[:-1]\n",
    "        \n",
    "        # Steps/batches\n",
    "        while True:\n",
    "            env.render()\n",
    "            action_logits, initial_state = sess.run([model.actions_logits, model.final_state],\n",
    "                                                    feed_dict = {model.states: state.reshape([1, -1]), \n",
    "                                                                 model.initial_state: initial_state})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # At the end of each episode\n",
    "        print('total_reward:{}'.format(total_reward))\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
