{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We start with our text encoded as integers in one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`batch_size` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. \n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\n",
    "        \n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(arr=encoded, batch_size=10, n_steps=50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) (10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data to/for/of the model\n",
    "def model_inputs(batch_size, num_steps, lstm_size):    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    #print(inputs.shape, targets.shape)\n",
    "    \n",
    "    # LSTM: Memory cells\n",
    "    # lstm1 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # lstm2 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # lstm3 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # cell = tf.nn.rnn_cell.MultiRNNCell([lstm1, lstm2, lstm3])\n",
    "    # lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # cell = tf.nn.rnn_cell.MultiRNNCell([lstm])\n",
    "\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return inputs, targets, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(lstm_size, batch_size, num_classes, inputs, initial_state, cell, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "        # Input to the RNN\n",
    "        inputs_onehot = tf.one_hot(indices=inputs, depth=num_classes)\n",
    "\n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_onehot, initial_state=initial_state)\n",
    "        #print(outputs.shape, final_state.shape)\n",
    "\n",
    "        seq_output = tf.reshape(outputs, [-1, lstm_size])\n",
    "        #print(seq_output.shape)\n",
    "\n",
    "        # with tf.variable_scope('softmax'):\n",
    "        logits = tf.layers.dense(inputs=seq_output, units=num_classes)\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        #print(predictions.shape)\n",
    "\n",
    "        # logits for loss calculation, predictions for accuracy/sample generation, and final_state for feedbackloop\n",
    "        return logits, predictions, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(num_classes, lstm_size, batch_size, inputs, targets, cell, initial_state):\n",
    "    # Build rnncell and initilize it\n",
    "    logits, predictions, final_state = generator(inputs=inputs, cell=cell, initial_state=initial_state,\n",
    "                                                 batch_size=batch_size, lstm_size=lstm_size, \n",
    "                                                 num_classes=num_classes)\n",
    "\n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    targets_one_hot = tf.one_hot(indices=targets, depth=num_classes)\n",
    "    labels = tf.reshape(targets_one_hot, logits.get_shape())\n",
    "    #print(targets.shape, targets_one_hot.shape, labels.shape)\n",
    "\n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    \n",
    "    # Loss for updating, predictions for accuracy and final state for the next bacth\n",
    "    return loss, predictions, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    #print(tvars)\n",
    "    \n",
    "    # grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, tvars), clip_norm=5) # usually around 1-5\n",
    "    grads=tf.gradients(loss, tvars)\n",
    "    #print(grads)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    opt = train_op.apply_gradients(grads_and_vars=zip(grads, tvars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, batch_size, num_steps, lstm_size, num_classes, learning_rate):\n",
    "        \n",
    "        # The data of the model and the RNNcell in the model and initialize it\n",
    "        #self.cell, self.initial_state = model_rnncell(batch_size=batch_size, lstm_size=lstm_size)\n",
    "        self.inputs, self.targets, self.cell, self.initial_state = model_inputs(batch_size=batch_size,\n",
    "                                                                                lstm_size=lstm_size, \n",
    "                                                                                num_steps=num_steps)\n",
    "        \n",
    "        # The loss of the model\n",
    "        self.loss, self.predictions, self.final_state = model_loss(\n",
    "            batch_size=batch_size, lstm_size=lstm_size, num_classes=num_classes,\n",
    "            inputs=self.inputs, targets=self.targets, cell=self.cell, initial_state=self.initial_state)\n",
    "        \n",
    "        # The update/training of the model\n",
    "        self.opt = model_opt(learning_rate=learning_rate, loss=self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "# num_layers = 2        # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "epochs = 100            # Epochs/episodes of training/update\n",
    "num_classes=len(vocab)  # number of classes of letter for input and output/target letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(batch_size=batch_size, learning_rate=learning_rate, lstm_size=lstm_size, num_classes=num_classes,\n",
    "              num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/100 Training loss:2.6269\n",
      "Epoch:2/100 Training loss:2.3145\n",
      "Epoch:3/100 Training loss:2.1288\n",
      "Epoch:4/100 Training loss:1.9972\n",
      "Epoch:5/100 Training loss:1.8977\n",
      "Epoch:6/100 Training loss:1.8192\n",
      "Epoch:7/100 Training loss:1.7543\n",
      "Epoch:8/100 Training loss:1.6990\n",
      "Epoch:9/100 Training loss:1.6524\n",
      "Epoch:10/100 Training loss:1.6113\n",
      "Epoch:11/100 Training loss:1.5751\n",
      "Epoch:12/100 Training loss:1.5429\n",
      "Epoch:13/100 Training loss:1.5139\n",
      "Epoch:14/100 Training loss:1.4876\n",
      "Epoch:15/100 Training loss:1.4637\n",
      "Epoch:16/100 Training loss:1.4417\n",
      "Epoch:17/100 Training loss:1.4215\n",
      "Epoch:18/100 Training loss:1.4028\n",
      "Epoch:19/100 Training loss:1.3853\n",
      "Epoch:20/100 Training loss:1.3690\n",
      "Epoch:21/100 Training loss:1.3537\n",
      "Epoch:22/100 Training loss:1.3394\n",
      "Epoch:23/100 Training loss:1.3260\n",
      "Epoch:24/100 Training loss:1.3132\n",
      "Epoch:25/100 Training loss:1.3012\n",
      "Epoch:26/100 Training loss:1.2897\n",
      "Epoch:27/100 Training loss:1.2789\n",
      "Epoch:28/100 Training loss:1.2687\n",
      "Epoch:29/100 Training loss:1.2590\n",
      "Epoch:30/100 Training loss:1.2497\n",
      "Epoch:31/100 Training loss:1.2408\n",
      "Epoch:32/100 Training loss:1.2322\n",
      "Epoch:33/100 Training loss:1.2240\n",
      "Epoch:34/100 Training loss:1.2162\n",
      "Epoch:35/100 Training loss:1.2086\n",
      "Epoch:36/100 Training loss:1.2014\n",
      "Epoch:37/100 Training loss:1.1945\n",
      "Epoch:38/100 Training loss:1.1878\n",
      "Epoch:39/100 Training loss:1.1813\n",
      "Epoch:40/100 Training loss:1.1751\n",
      "Epoch:41/100 Training loss:1.1691\n",
      "Epoch:42/100 Training loss:1.1633\n",
      "Epoch:43/100 Training loss:1.1576\n",
      "Epoch:44/100 Training loss:1.1520\n",
      "Epoch:45/100 Training loss:1.1466\n",
      "Epoch:46/100 Training loss:1.1414\n",
      "Epoch:47/100 Training loss:1.1363\n",
      "Epoch:48/100 Training loss:1.1315\n",
      "Epoch:49/100 Training loss:1.1268\n",
      "Epoch:50/100 Training loss:1.1222\n",
      "Epoch:51/100 Training loss:1.1176\n",
      "Epoch:52/100 Training loss:1.1132\n",
      "Epoch:53/100 Training loss:1.1089\n",
      "Epoch:54/100 Training loss:1.1048\n",
      "Epoch:55/100 Training loss:1.1007\n",
      "Epoch:56/100 Training loss:1.0967\n",
      "Epoch:57/100 Training loss:1.0929\n",
      "Epoch:58/100 Training loss:1.0891\n",
      "Epoch:59/100 Training loss:1.0854\n",
      "Epoch:60/100 Training loss:1.0819\n",
      "Epoch:61/100 Training loss:1.0785\n",
      "Epoch:62/100 Training loss:1.0751\n",
      "Epoch:63/100 Training loss:1.0719\n",
      "Epoch:64/100 Training loss:1.0687\n",
      "Epoch:65/100 Training loss:1.0655\n",
      "Epoch:66/100 Training loss:1.0624\n",
      "Epoch:67/100 Training loss:1.0594\n",
      "Epoch:68/100 Training loss:1.0564\n",
      "Epoch:69/100 Training loss:1.0534\n",
      "Epoch:70/100 Training loss:1.0506\n",
      "Epoch:71/100 Training loss:1.0477\n",
      "Epoch:72/100 Training loss:1.0450\n",
      "Epoch:73/100 Training loss:1.0423\n",
      "Epoch:74/100 Training loss:1.0397\n",
      "Epoch:75/100 Training loss:1.0371\n",
      "Epoch:76/100 Training loss:1.0346\n",
      "Epoch:77/100 Training loss:1.0321\n",
      "Epoch:78/100 Training loss:1.0297\n",
      "Epoch:79/100 Training loss:1.0273\n",
      "Epoch:80/100 Training loss:1.0249\n",
      "Epoch:81/100 Training loss:1.0226\n",
      "Epoch:82/100 Training loss:1.0202\n",
      "Epoch:83/100 Training loss:1.0179\n",
      "Epoch:84/100 Training loss:1.0157\n",
      "Epoch:85/100 Training loss:1.0134\n",
      "Epoch:86/100 Training loss:1.0113\n",
      "Epoch:87/100 Training loss:1.0091\n",
      "Epoch:88/100 Training loss:1.0070\n",
      "Epoch:89/100 Training loss:1.0049\n",
      "Epoch:90/100 Training loss:1.0029\n",
      "Epoch:91/100 Training loss:1.0009\n",
      "Epoch:92/100 Training loss:0.9989\n",
      "Epoch:93/100 Training loss:0.9970\n",
      "Epoch:94/100 Training loss:0.9950\n",
      "Epoch:95/100 Training loss:0.9931\n",
      "Epoch:96/100 Training loss:0.9912\n",
      "Epoch:97/100 Training loss:0.9893\n",
      "Epoch:98/100 Training loss:0.9874\n",
      "Epoch:99/100 Training loss:0.9856\n",
      "Epoch:100/100 Training loss:0.9838\n"
     ]
    }
   ],
   "source": [
    "# Save and restore the trained model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# training losses for plotting\n",
    "train_losses = []\n",
    "\n",
    "# A tf session for computational graph (has to be reset)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the graph variables/parameters in this session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # # Use the line below to load a checkpoint and resume training\n",
    "    # saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir='checkpoints/'))\n",
    "\n",
    "    # Training epochs/episodes\n",
    "    loss_batch = []\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Training batches/steps\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "        for inputs, targets in get_batches(encoded, batch_size, num_steps):\n",
    "            # Feeding the batches into the model\n",
    "            feed_dict = {model.inputs: inputs, \n",
    "                         model.targets: targets, \n",
    "                         model.initial_state: initial_state}\n",
    "            loss, final_state, _ = sess.run([model.loss, \n",
    "                                             model.final_state, \n",
    "                                             model.opt], feed_dict)\n",
    "            # Feedback loop or recurrence/recursion/lateral connections\n",
    "            initial_state = final_state\n",
    "            \n",
    "            # Save the batch losses\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Printing out the loss and the time of training for every epoch/episode\n",
    "        print('Epoch:{}/{}'.format(e+1, epochs),\n",
    "              'Training loss:{:.4f}'.format(np.mean(loss_batch)))\n",
    "        \n",
    "        # Saving the mean/average training loss batches for plotting\n",
    "        train_losses.append(np.mean(loss_batch))\n",
    "        \n",
    "    # Save the trained model\n",
    "    #saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    saver.save(sess, \"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/model.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for testing/ sequence prediction\n",
    "batch_size = 1          # Sequences per batch\n",
    "num_steps = 1           # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "# num_layers = 2        # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "# epochs = 1              # Epochs/episodes of training/update\n",
    "num_classes=len(vocab)  # number of classes of letter for input and output/target letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the sampling predictor\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "n_samples=100 \n",
    "lstm_size=lstm_size\n",
    "vocab_size=len(vocab)\n",
    "prime=\"Far\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(learning_rate=learning_rate, lstm_size=lstm_size, num_classes=num_classes,\n",
    "              batch_size=1, num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "Farrown folding\n",
      "sight, but he called to the still son. He drove out a will gellov with a\n",
      "clover eyes gro\n"
     ]
    }
   ],
   "source": [
    "# Initial sample input\n",
    "samples = [c for c in prime]\n",
    "\n",
    "# To restore/load the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# A session for computational graph\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Load/restore the trained model\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints/'))\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "    # Testing\n",
    "    initial_state = sess.run(model.initial_state)\n",
    "    for c in prime:\n",
    "        inputs = np.zeros((1, 1))\n",
    "        inputs[0,0] = vocab_to_int[c]\n",
    "\n",
    "        # Model feed and fetch\n",
    "        feed_dict = {model.inputs: inputs, \n",
    "                     model.initial_state: initial_state}\n",
    "        predictions, final_state = sess.run([model.predictions, \n",
    "                                             model.final_state], feed_dict)\n",
    "\n",
    "        # Feedback/recuurence/recursion/lateral connection\n",
    "        initial_state = final_state\n",
    "\n",
    "    c = pick_top_n(preds=predictions, vocab_size=len(vocab))\n",
    "    samples.append(int_to_vocab[c])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        inputs[0,0] = c\n",
    "\n",
    "        # Model feed and fetch\n",
    "        feed_dict = {model.inputs: inputs, \n",
    "                     model.initial_state: initial_state}\n",
    "        predictions, final_state = sess.run([model.predictions, \n",
    "                                             model.final_state], feed_dict)\n",
    "\n",
    "        # Feedback/recuurence/recursion/lateral connection\n",
    "        initial_state = final_state\n",
    "\n",
    "        c = pick_top_n(preds=predictions, vocab_size=len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "print(''.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
