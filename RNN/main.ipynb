{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We start with our text encoded as integers in one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`batch_size` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. \n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\n",
    "        \n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(arr=encoded, batch_size=10, n_steps=50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) (10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data to/for/of the model\n",
    "def model_inputs(batch_size, num_steps, lstm_size):    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    #print(inputs.shape, targets.shape)\n",
    "    \n",
    "    # LSTM: Memory cells\n",
    "    # lstm1 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # lstm2 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # lstm3 = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # cell = tf.nn.rnn_cell.MultiRNNCell([lstm1, lstm2, lstm3])\n",
    "    # lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # cell = tf.nn.rnn_cell.MultiRNNCell([lstm])\n",
    "\n",
    "    # GRU: Gated Recurrent Units\n",
    "    gru = tf.nn.rnn_cell.GRUCell(lstm_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([gru], state_is_tuple=False)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return inputs, targets, cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN generator or sequence generator\n",
    "def generator(lstm_size, num_classes, inputs, initial_state, cell, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "        # Input to the RNN\n",
    "        inputs_onehot = tf.one_hot(indices=inputs, depth=num_classes)\n",
    "        print(inputs.shape, inputs_onehot.shape)\n",
    "        \n",
    "        # with tf.variable_scope('dynamic_rnn_', reuse=tf.AUTO_REUSE):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs_onehot, initial_state=initial_state)\n",
    "        print(outputs.shape, final_state.shape)\n",
    "\n",
    "        seq_output = tf.reshape(outputs, [-1, lstm_size])\n",
    "        print(seq_output.shape)\n",
    "\n",
    "        # with tf.variable_scope('softmax'):\n",
    "        logits = tf.layers.dense(inputs=seq_output, units=num_classes)\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        print(predictions.shape)\n",
    "\n",
    "        # logits for loss calculation, predictions for accuracy/sample generation, and final_state for feedbackloop\n",
    "        return logits, predictions, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(num_classes, lstm_size, inputs, targets, cell, initial_state):\n",
    "    # Build rnncell and initilize it\n",
    "    logits, predictions, final_state = generator(inputs=inputs, cell=cell, initial_state=initial_state,\n",
    "                                                 lstm_size=lstm_size, num_classes=num_classes)\n",
    "\n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    targets_one_hot = tf.one_hot(indices=targets, depth=num_classes)\n",
    "    labels = tf.reshape(targets_one_hot, logits.get_shape())\n",
    "    print(targets.shape, targets_one_hot.shape, labels.shape)\n",
    "\n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    \n",
    "    # Loss for updating, predictions for accuracy and final state for the next bacth\n",
    "    return loss, predictions, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    # grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, tvars), clip_norm=5) # usually around 1-5\n",
    "    grads=tf.gradients(loss, tvars)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, tvars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, batch_size, num_steps, lstm_size, num_classes, learning_rate):\n",
    "        \n",
    "        # The data of the model and the RNNcell in the model and initialize it\n",
    "        #self.cell, self.initial_state = model_rnncell(batch_size=batch_size, lstm_size=lstm_size)\n",
    "        self.inputs, self.targets, self.cell, self.initial_state = model_inputs(batch_size=batch_size,\n",
    "                                                                                lstm_size=lstm_size, \n",
    "                                                                                num_steps=num_steps)\n",
    "        \n",
    "        # The loss of the model\n",
    "        self.loss, self.predictions, self.final_state = model_loss(\n",
    "            lstm_size=lstm_size, num_classes=num_classes, inputs=self.inputs, \n",
    "            targets=self.targets, cell=self.cell, initial_state=self.initial_state)\n",
    "        \n",
    "        # The update/training of the model\n",
    "        self.opt = model_opt(learning_rate=learning_rate, loss=self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "# num_layers = 2        # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "epochs = 100            # Epochs/episodes of training/update\n",
    "num_classes=len(vocab)  # number of classes of letter for input and output/target letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100) (100, 100, 83)\n",
      "(100, 100, 512) (100, 512)\n",
      "(10000, 512)\n",
      "(10000, 83)\n",
      "(100, 100) (100, 100, 83) (10000, 83)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(batch_size=batch_size, learning_rate=learning_rate, lstm_size=lstm_size, num_classes=num_classes,\n",
    "              num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/100 Mean Training loss:2.6850 Std Training loss:0.3736\n",
      "Epoch:2/100 Mean Training loss:2.3641 Std Training loss:0.4206\n",
      "Epoch:3/100 Mean Training loss:2.1670 Std Training loss:0.4438\n",
      "Epoch:4/100 Mean Training loss:2.0269 Std Training loss:0.4552\n",
      "Epoch:5/100 Mean Training loss:1.9201 Std Training loss:0.4601\n",
      "Epoch:6/100 Mean Training loss:1.8352 Std Training loss:0.4612\n",
      "Epoch:7/100 Mean Training loss:1.7655 Std Training loss:0.4599\n",
      "Epoch:8/100 Mean Training loss:1.7071 Std Training loss:0.4573\n",
      "Epoch:9/100 Mean Training loss:1.6572 Std Training loss:0.4537\n",
      "Epoch:10/100 Mean Training loss:1.6141 Std Training loss:0.4496\n",
      "Epoch:11/100 Mean Training loss:1.5762 Std Training loss:0.4452\n",
      "Epoch:12/100 Mean Training loss:1.5425 Std Training loss:0.4407\n",
      "Epoch:13/100 Mean Training loss:1.5123 Std Training loss:0.4362\n",
      "Epoch:14/100 Mean Training loss:1.4850 Std Training loss:0.4318\n",
      "Epoch:15/100 Mean Training loss:1.4601 Std Training loss:0.4275\n",
      "Epoch:16/100 Mean Training loss:1.4374 Std Training loss:0.4232\n",
      "Epoch:17/100 Mean Training loss:1.4165 Std Training loss:0.4190\n",
      "Epoch:18/100 Mean Training loss:1.3972 Std Training loss:0.4150\n",
      "Epoch:19/100 Mean Training loss:1.3792 Std Training loss:0.4111\n",
      "Epoch:20/100 Mean Training loss:1.3625 Std Training loss:0.4073\n",
      "Epoch:21/100 Mean Training loss:1.3468 Std Training loss:0.4037\n",
      "Epoch:22/100 Mean Training loss:1.3321 Std Training loss:0.4001\n",
      "Epoch:23/100 Mean Training loss:1.3183 Std Training loss:0.3967\n",
      "Epoch:24/100 Mean Training loss:1.3053 Std Training loss:0.3933\n",
      "Epoch:25/100 Mean Training loss:1.2931 Std Training loss:0.3900\n",
      "Epoch:26/100 Mean Training loss:1.2814 Std Training loss:0.3869\n",
      "Epoch:27/100 Mean Training loss:1.2703 Std Training loss:0.3839\n",
      "Epoch:28/100 Mean Training loss:1.2597 Std Training loss:0.3810\n",
      "Epoch:29/100 Mean Training loss:1.2496 Std Training loss:0.3782\n",
      "Epoch:30/100 Mean Training loss:1.2401 Std Training loss:0.3754\n",
      "Epoch:31/100 Mean Training loss:1.2311 Std Training loss:0.3726\n",
      "Epoch:32/100 Mean Training loss:1.2227 Std Training loss:0.3698\n",
      "Epoch:33/100 Mean Training loss:1.2144 Std Training loss:0.3671\n",
      "Epoch:34/100 Mean Training loss:1.2065 Std Training loss:0.3645\n",
      "Epoch:35/100 Mean Training loss:1.1989 Std Training loss:0.3620\n",
      "Epoch:36/100 Mean Training loss:1.1917 Std Training loss:0.3595\n",
      "Epoch:37/100 Mean Training loss:1.1847 Std Training loss:0.3572\n",
      "Epoch:38/100 Mean Training loss:1.1779 Std Training loss:0.3548\n",
      "Epoch:39/100 Mean Training loss:1.1713 Std Training loss:0.3526\n",
      "Epoch:40/100 Mean Training loss:1.1650 Std Training loss:0.3504\n",
      "Epoch:41/100 Mean Training loss:1.1590 Std Training loss:0.3482\n",
      "Epoch:42/100 Mean Training loss:1.1533 Std Training loss:0.3460\n",
      "Epoch:43/100 Mean Training loss:1.1478 Std Training loss:0.3438\n",
      "Epoch:44/100 Mean Training loss:1.1423 Std Training loss:0.3418\n",
      "Epoch:45/100 Mean Training loss:1.1369 Std Training loss:0.3399\n",
      "Epoch:46/100 Mean Training loss:1.1316 Std Training loss:0.3381\n",
      "Epoch:47/100 Mean Training loss:1.1265 Std Training loss:0.3362\n",
      "Epoch:48/100 Mean Training loss:1.1216 Std Training loss:0.3344\n",
      "Epoch:49/100 Mean Training loss:1.1169 Std Training loss:0.3326\n",
      "Epoch:50/100 Mean Training loss:1.1123 Std Training loss:0.3308\n",
      "Epoch:51/100 Mean Training loss:1.1079 Std Training loss:0.3291\n",
      "Epoch:52/100 Mean Training loss:1.1035 Std Training loss:0.3274\n",
      "Epoch:53/100 Mean Training loss:1.0993 Std Training loss:0.3257\n",
      "Epoch:54/100 Mean Training loss:1.0951 Std Training loss:0.3242\n",
      "Epoch:55/100 Mean Training loss:1.0910 Std Training loss:0.3226\n",
      "Epoch:56/100 Mean Training loss:1.0870 Std Training loss:0.3211\n",
      "Epoch:57/100 Mean Training loss:1.0831 Std Training loss:0.3197\n",
      "Epoch:58/100 Mean Training loss:1.0793 Std Training loss:0.3182\n",
      "Epoch:59/100 Mean Training loss:1.0756 Std Training loss:0.3168\n",
      "Epoch:60/100 Mean Training loss:1.0719 Std Training loss:0.3154\n",
      "Epoch:61/100 Mean Training loss:1.0684 Std Training loss:0.3140\n",
      "Epoch:62/100 Mean Training loss:1.0650 Std Training loss:0.3126\n",
      "Epoch:63/100 Mean Training loss:1.0617 Std Training loss:0.3112\n",
      "Epoch:64/100 Mean Training loss:1.0585 Std Training loss:0.3098\n",
      "Epoch:65/100 Mean Training loss:1.0552 Std Training loss:0.3085\n",
      "Epoch:66/100 Mean Training loss:1.0521 Std Training loss:0.3072\n",
      "Epoch:67/100 Mean Training loss:1.0490 Std Training loss:0.3060\n",
      "Epoch:68/100 Mean Training loss:1.0460 Std Training loss:0.3047\n",
      "Epoch:69/100 Mean Training loss:1.0431 Std Training loss:0.3035\n",
      "Epoch:70/100 Mean Training loss:1.0402 Std Training loss:0.3022\n",
      "Epoch:71/100 Mean Training loss:1.0373 Std Training loss:0.3011\n",
      "Epoch:72/100 Mean Training loss:1.0345 Std Training loss:0.2999\n",
      "Epoch:73/100 Mean Training loss:1.0317 Std Training loss:0.2988\n",
      "Epoch:74/100 Mean Training loss:1.0289 Std Training loss:0.2978\n",
      "Epoch:75/100 Mean Training loss:1.0262 Std Training loss:0.2967\n",
      "Epoch:76/100 Mean Training loss:1.0236 Std Training loss:0.2956\n",
      "Epoch:77/100 Mean Training loss:1.0209 Std Training loss:0.2946\n",
      "Epoch:78/100 Mean Training loss:1.0184 Std Training loss:0.2935\n",
      "Epoch:79/100 Mean Training loss:1.0159 Std Training loss:0.2925\n",
      "Epoch:80/100 Mean Training loss:1.0135 Std Training loss:0.2915\n",
      "Epoch:81/100 Mean Training loss:1.0111 Std Training loss:0.2905\n",
      "Epoch:82/100 Mean Training loss:1.0088 Std Training loss:0.2895\n",
      "Epoch:83/100 Mean Training loss:1.0065 Std Training loss:0.2885\n",
      "Epoch:84/100 Mean Training loss:1.0043 Std Training loss:0.2875\n",
      "Epoch:85/100 Mean Training loss:1.0020 Std Training loss:0.2865\n",
      "Epoch:86/100 Mean Training loss:0.9998 Std Training loss:0.2856\n",
      "Epoch:87/100 Mean Training loss:0.9976 Std Training loss:0.2847\n",
      "Epoch:88/100 Mean Training loss:0.9955 Std Training loss:0.2838\n",
      "Epoch:89/100 Mean Training loss:0.9934 Std Training loss:0.2829\n",
      "Epoch:90/100 Mean Training loss:0.9913 Std Training loss:0.2820\n",
      "Epoch:91/100 Mean Training loss:0.9893 Std Training loss:0.2811\n",
      "Epoch:92/100 Mean Training loss:0.9872 Std Training loss:0.2802\n",
      "Epoch:93/100 Mean Training loss:0.9853 Std Training loss:0.2794\n",
      "Epoch:94/100 Mean Training loss:0.9834 Std Training loss:0.2785\n",
      "Epoch:95/100 Mean Training loss:0.9815 Std Training loss:0.2777\n",
      "Epoch:96/100 Mean Training loss:0.9796 Std Training loss:0.2768\n",
      "Epoch:97/100 Mean Training loss:0.9777 Std Training loss:0.2760\n",
      "Epoch:98/100 Mean Training loss:0.9759 Std Training loss:0.2752\n",
      "Epoch:99/100 Mean Training loss:0.9741 Std Training loss:0.2744\n",
      "Epoch:100/100 Mean Training loss:0.9723 Std Training loss:0.2736\n"
     ]
    }
   ],
   "source": [
    "# Save and restore the trained model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# training losses for plotting\n",
    "train_losses = []\n",
    "\n",
    "# A tf session for computational graph (has to be reset)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the graph variables/parameters in this session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir='checkpoints/'))\n",
    "\n",
    "    # Training epochs/episodes\n",
    "    loss_batch = []\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Training batches/steps\n",
    "        initial_state = sess.run(model.initial_state)\n",
    "        for inputs, targets in get_batches(encoded, batch_size, num_steps):\n",
    "            # Feeding the batches into the model\n",
    "            loss, initial_state, _ = sess.run([model.loss, model.final_state, model.opt],\n",
    "                                              feed_dict = {model.inputs: inputs, \n",
    "                                                           model.targets: targets, \n",
    "                                                           model.initial_state: initial_state})\n",
    "            # Save the batch losses\n",
    "            loss_batch.append(loss)\n",
    "\n",
    "        # Printing out the loss and the time of training for every epoch/episode\n",
    "        print('Epoch:{}/{}'.format(e+1, epochs),\n",
    "              'Mean Training loss:{:.4f}'.format(np.mean(loss_batch)),\n",
    "              'Std Training loss:{:.4f}'.format(np.std(loss_batch)))\n",
    "        \n",
    "        # Saving the mean/average training loss batches for plotting\n",
    "        train_losses.append(np.mean(loss_batch))\n",
    "        \n",
    "    # Save the trained model\n",
    "    #saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    saver.save(sess, \"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/model.ckpt'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for testing/ sequence prediction\n",
    "batch_size = 1          # Sequences per batch\n",
    "num_steps = 1           # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "# num_layers = 2        # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "# epochs = 1              # Epochs/episodes of training/update\n",
    "num_classes=len(vocab)  # number of classes of letter for input and output/target letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the sampling predictor\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "n_samples=100 \n",
    "lstm_size=lstm_size\n",
    "vocab_size=len(vocab)\n",
    "prime=\"Far\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) (1, 1, 83)\n",
      "(1, 1, 512) (1, 512)\n",
      "(1, 512)\n",
      "(1, 83)\n",
      "(1, 1) (1, 1, 83) (1, 83)\n"
     ]
    }
   ],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(learning_rate=learning_rate, lstm_size=lstm_size, num_classes=num_classes,\n",
    "              batch_size=1, num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n",
      "Farmanary are of\n",
      "the whole theater. There was nothing out of the husband, the consciencial of\n",
      "electibili\n"
     ]
    }
   ],
   "source": [
    "# Initial sample input\n",
    "samples = [c for c in prime]\n",
    "\n",
    "# To restore/load the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# A session for computational graph\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Load/restore the trained model\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints/'))\n",
    "    #saver.restore(sess, 'checkpoints/model.ckpt')\n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "    # Testing\n",
    "    initial_state = sess.run(model.initial_state)\n",
    "    for c in prime:\n",
    "        inputs = np.zeros((1, 1))\n",
    "        inputs[0,0] = vocab_to_int[c]\n",
    "\n",
    "        # Model feed and fetch\n",
    "        predictions, initial_state = sess.run([model.predictions, model.final_state],\n",
    "                                              feed_dict = {model.inputs: inputs, \n",
    "                                                           model.initial_state: initial_state})\n",
    "\n",
    "    c = pick_top_n(preds=predictions, vocab_size=len(vocab))\n",
    "    samples.append(int_to_vocab[c])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        inputs[0,0] = c\n",
    "\n",
    "        # Model feed and fetch\n",
    "        predictions, initial_state = sess.run([model.predictions, model.final_state],\n",
    "                                            feed_dict = {model.inputs: inputs, \n",
    "                                                         model.initial_state: initial_state})\n",
    "\n",
    "        c = pick_top_n(preds=predictions, vocab_size=len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "print(''.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
