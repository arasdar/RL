{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning or Q-network (QN)\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use Q-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned into the same directory with this notebook. I've included `gym` as a submodule, so you can run `git submodule --init --recursive` to pull the contents into the `gym` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make sure you have OpenAI Gym cloned. Then run this command `pip install -e gym/[all]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "batch = []\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) # take a random action\n",
    "    batch.append([action, state, reward, done, info])\n",
    "    #print('state, action, reward, done, info:', state, action, reward, done, info)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use `env.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  array([-0.02735362, -0.20781045, -0.00759164,  0.29000792]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}],\n",
       " (4,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0], batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([each[0] for each in batch])\n",
    "states = np.array([each[1] for each in batch])\n",
    "rewards = np.array([each[2] for each in batch])\n",
    "dones = np.array([each[3] for each in batch])\n",
    "infos = np.array([each[4] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(1000,) (1000, 4) (1000,) (1000,)\n",
      "float64 float64 int64 bool\n",
      "1 0\n",
      "2\n",
      "1.0 1.0\n",
      "2.3325768624303636 -2.7797553630083534\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])\n",
    "print(np.array(rewards).shape, np.array(states).shape, np.array(actions).shape, np.array(dones).shape)\n",
    "print(np.array(rewards).dtype, np.array(states).dtype, np.array(actions).dtype, np.array(dones).dtype)\n",
    "print(np.max(np.array(actions)), np.min(np.array(actions)))\n",
    "print((np.max(np.array(actions)) - np.min(np.array(actions)))+1)\n",
    "print(np.max(np.array(rewards)), np.min(np.array(rewards)))\n",
    "print(np.max(np.array(states)), np.min(np.array(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-_table_. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(state_size):\n",
    "    actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "    states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "    targetQs = tf.placeholder(tf.float32, [None], name='targetQs')\n",
    "    return actions, states, targetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: Generating/prediting the actions\n",
    "def generator(states, action_size, hidden_size, reuse=False, alpha=0.1, training=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=states, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)        \n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)        \n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=action_size)        \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # return actions logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actions, states, targetQs, # model input\n",
    "               action_size, hidden_size): # model init\n",
    "    actions_logits = generator(states=states, hidden_size=hidden_size, action_size=action_size)\n",
    "    actions_onehot = tf.one_hot(indices=actions, depth=action_size, dtype=actions_logits.dtype)\n",
    "    Qs = tf.reduce_max(tf.multiply(actions_logits[:-1], actions_onehot[1:]), axis=1)\n",
    "    #loss = tf.reduce_mean(tf.square(Qs - targetQs[1:]))\n",
    "    #loss = tf.reduce_mean(tf.square(Qs[:-1] - targetQs[2:]))\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Qs, labels=tf.nn.sigmoid(targetQs[1:])))    \n",
    "    return actions_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Get optimization operations in order\n",
    "    :param loss: Generator loss Tensor for action prediction\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :return: A tuple of (qfunction training, generator training, discriminator training)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.actions, self.states, self.targetQs = model_input(state_size=state_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.actions_logits, self.loss = model_loss(\n",
    "            action_size=action_size, hidden_size=hidden_size, # model init parameters\n",
    "            states=self.states, actions=self.actions, targetQs=self.targetQs) # model input\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():    \n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon).  That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an **$\\epsilon$-greedy policy**.\n",
    "\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called _exploitation_. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning training algorithm\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in _episodes_. One *episode* is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "  * **For** $t$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcememt learning are the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:(1000, 4) actions:(1000,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('state size:{}'.format(states.shape), \n",
    "      'actions:{}'.format(actions.shape)) \n",
    "print(np.max(actions) - np.min(actions)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 3000000000         # max steps in an episode\n",
    "learning_rate = 0.001          # learning rate for adam\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "state_size = 4                 # number of units for the input state/observation -- simulation\n",
    "action_size = 2                # number of units for the output actions -- simulation\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer -- simulation\n",
    "batch_size = 500               # number of samples in the memory/ experience as mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset/init the graph/session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Init the model\n",
    "model = Model(action_size=action_size, hidden_size=hidden_size, state_size=state_size, learning_rate=learning_rate)\n",
    "\n",
    "# Init the memory\n",
    "memory = Memory(max_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "## Create the Cart-Pole game environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the memory (exprience memory)\n",
    "\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    memory.buffer.append([action, state, done])\n",
    "    if done is True:\n",
    "        # Start new episode\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the `env.render()` line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01743245, -0.36202042, -0.04714307,  0.45659561]), 1.0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model-qn.ckpt\n",
      "Episode: 0 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 1 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 2 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 3 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 4 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 5 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 6 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 7 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 8 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 9 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 10 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 11 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 12 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 13 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 14 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 15 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 16 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 17 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 18 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 19 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 20 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 21 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 22 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 23 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 24 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 25 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 26 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 27 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 28 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 29 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 30 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 31 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 32 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 33 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 34 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 35 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 36 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 37 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 38 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 39 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 40 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 41 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 42 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 43 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 44 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 45 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 46 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 47 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 48 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 49 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 50 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 51 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 52 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 53 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 54 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 55 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 56 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 57 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 58 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 59 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 60 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 61 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 62 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 63 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 64 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 65 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 66 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 67 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 68 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 69 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 70 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 71 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 72 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 73 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 74 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 75 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 76 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 77 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 78 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 79 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 80 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 81 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 82 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 83 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 84 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 85 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 86 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 87 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 88 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 89 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 90 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 91 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 92 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 93 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 94 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 95 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 96 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 97 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 98 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 99 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 100 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 101 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 102 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 103 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 104 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 105 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 106 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 107 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 108 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 109 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 110 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 111 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 112 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 113 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 114 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 115 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 116 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 117 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 118 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 119 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 120 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 121 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 122 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 123 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 124 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 125 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 126 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 127 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 128 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 129 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 130 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 131 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 132 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 133 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 134 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 135 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 136 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 137 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 138 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 139 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 141 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 142 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 143 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 144 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 145 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 146 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 147 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 148 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 149 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 150 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 151 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 152 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 153 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 154 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 155 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 156 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 157 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 158 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 159 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 160 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 161 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 162 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 163 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 164 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 165 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 166 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 167 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 168 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 169 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 170 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 171 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 172 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 173 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 174 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 175 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 176 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 177 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 178 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 179 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 180 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 181 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 182 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 183 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 184 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 185 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 186 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 187 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 188 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 189 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 190 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 191 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 192 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 193 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 194 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 195 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 196 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 197 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 198 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 199 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 200 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 201 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 202 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 203 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 204 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 205 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 206 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 207 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 208 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 209 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 210 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 211 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 212 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 213 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 214 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 215 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 216 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 217 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 218 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 219 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 220 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 221 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 222 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 223 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 224 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 225 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 226 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 227 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 228 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 229 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 230 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 231 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 232 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 233 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 234 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 235 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 236 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 237 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 238 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 239 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 240 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 241 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 242 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 243 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 244 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 245 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 246 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 247 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 248 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 249 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 250 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 251 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 252 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 253 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 254 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 255 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 256 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 257 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 258 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 259 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 260 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 261 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 262 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 263 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 264 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 265 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 266 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 267 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 268 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 269 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 270 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 271 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 272 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 273 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 274 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 275 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 276 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 277 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 278 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 279 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 280 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 281 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 282 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 283 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 284 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 285 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 286 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 287 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 288 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 289 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 290 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 291 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 292 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 293 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 294 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 295 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 296 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 297 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 298 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 299 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 300 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 301 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 302 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 303 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 304 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 305 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 306 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 307 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 308 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 309 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 310 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 311 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 312 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 313 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 314 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 315 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 316 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 317 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 318 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 319 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 320 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 321 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 322 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 323 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 324 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 325 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 326 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 327 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 328 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 329 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 330 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 331 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 332 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 333 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 334 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 335 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 336 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 337 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 338 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 339 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 340 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 341 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 342 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 343 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 344 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 345 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 346 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 347 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 348 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 349 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 350 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 351 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 352 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 353 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 354 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 355 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 356 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 357 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 358 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 359 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 360 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 361 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 362 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 363 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 364 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 365 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 366 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 367 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 368 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 369 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 370 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 371 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 372 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 373 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 374 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 375 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 376 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 377 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 378 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 379 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 380 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 381 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 382 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 383 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 384 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 385 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 386 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 387 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 388 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 389 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 390 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 391 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 392 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 393 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 394 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 395 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 396 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 397 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 398 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 399 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 400 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 401 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 402 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 403 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 404 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 405 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 406 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 407 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 408 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 409 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 410 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 411 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 412 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 413 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 414 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 415 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 416 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 417 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 418 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 419 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 420 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 421 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 422 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 423 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 424 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 425 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 426 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 427 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 428 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 429 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 430 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 431 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 432 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 433 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 434 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 435 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 436 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 437 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 438 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 439 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 440 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 441 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 442 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 443 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 444 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 445 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 446 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 447 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 448 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 449 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 450 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 451 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 452 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 453 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 454 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 455 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 456 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 457 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 458 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 459 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 460 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 461 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 462 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 463 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 464 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 465 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 466 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 467 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 468 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 469 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 470 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 471 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 472 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 473 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 474 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 475 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 476 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 477 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 478 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 479 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 480 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 481 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 482 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 483 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 484 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 485 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 486 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 487 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 488 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 489 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 490 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 491 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 492 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 493 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 494 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 495 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 496 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 497 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 498 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 499 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 500 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 501 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 502 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 503 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 504 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 505 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 506 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 507 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 508 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 509 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 510 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 511 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 512 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 513 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 514 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 515 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 516 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 517 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 518 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 519 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 520 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 521 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 522 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 523 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 524 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 525 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 526 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 527 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 528 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 529 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 530 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 531 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 532 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 533 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 534 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 535 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 536 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 537 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 538 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 539 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 540 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 541 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 542 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 543 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 544 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 545 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 546 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 547 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 548 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 549 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 550 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 551 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 552 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 553 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 554 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 555 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 556 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 557 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 558 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 559 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 560 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 561 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 562 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 563 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 564 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 565 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 566 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 567 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 568 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 569 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 570 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 571 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 572 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 573 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 574 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 575 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 576 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 577 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 578 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 579 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 580 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 581 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 582 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 583 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 584 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 585 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 586 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 587 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 588 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 589 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 590 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 591 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 592 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 593 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 594 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 595 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 596 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 597 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 598 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 599 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 600 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 601 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 602 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 603 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 604 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 605 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 606 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 607 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 608 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 609 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 610 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 611 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 612 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 613 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 614 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 615 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 616 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 617 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 618 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 619 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 620 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 621 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 622 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 623 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 624 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 625 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 626 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 627 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 628 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 629 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 630 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 631 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 632 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 633 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 634 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 635 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 636 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 637 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 638 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 639 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 640 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 641 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 642 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 643 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 644 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 645 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 646 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 647 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 648 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 649 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 650 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 651 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 652 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 653 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 654 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 655 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 656 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 657 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 658 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 659 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 660 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 661 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 662 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 663 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 664 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 665 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 666 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 667 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 668 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 669 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 670 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 671 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 672 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 673 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 674 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 675 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 676 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 677 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 678 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 679 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 680 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 681 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 682 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 683 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 684 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 685 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 686 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 687 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 688 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 689 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 690 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 691 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 692 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 693 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 694 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 695 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 696 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 697 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 698 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 699 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 700 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 701 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 702 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 703 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 704 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 705 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 706 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 707 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 708 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 709 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 710 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 711 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 712 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 713 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 714 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 715 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 716 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 717 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 718 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 719 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 720 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 721 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 722 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 723 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 724 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 725 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 726 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 727 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 728 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 729 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 730 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 731 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 732 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 733 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 734 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 735 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 736 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 737 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 738 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 739 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 740 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 741 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 742 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 743 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 744 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 745 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 746 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 747 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 748 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 749 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 750 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 751 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 752 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 753 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 754 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 755 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 756 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 757 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 758 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 759 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 760 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 761 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 762 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 763 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 764 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 765 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 766 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 767 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 768 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 769 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 770 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 771 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 772 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 773 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 774 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 775 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 776 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 777 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 778 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 779 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 780 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 781 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 782 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 783 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 784 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 785 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 786 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 787 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 788 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 789 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 790 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 791 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 792 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 793 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 794 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 795 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 796 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 797 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 798 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 799 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 800 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 801 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 802 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 803 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 804 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 805 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 806 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 807 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 808 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 809 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 810 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 811 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 812 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 813 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 814 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 815 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 816 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 817 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 818 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 819 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 820 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 821 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 822 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 823 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 824 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 825 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 826 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 827 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 828 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 829 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 830 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 831 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 832 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 833 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 834 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 835 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 836 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 837 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 838 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 839 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 840 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 841 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 842 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 843 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 844 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 845 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 846 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 847 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 848 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 849 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 850 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 851 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 852 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 853 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 854 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 855 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 856 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 857 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 858 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 859 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 860 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 861 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 862 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 863 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 864 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 865 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 866 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 867 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 868 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 869 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 870 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 871 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 872 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 873 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 874 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 875 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 876 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 877 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 878 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 879 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 880 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 881 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 882 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 883 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 884 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 885 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 886 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 887 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 888 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 889 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 890 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 891 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 892 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 893 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 894 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 895 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 896 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 897 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 898 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 899 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 900 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 901 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 902 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 903 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 904 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 905 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 906 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 907 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 908 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 909 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 910 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 911 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 912 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 913 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 914 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 915 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 916 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 917 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 918 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 919 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 920 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 921 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 922 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 923 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 924 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 925 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 926 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 927 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 928 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 929 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 930 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 931 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 932 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 933 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 934 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 935 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 936 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 937 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 938 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 939 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 940 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 941 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 942 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 943 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 944 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 945 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 946 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 947 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 948 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 949 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 950 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 951 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 952 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 953 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 954 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 955 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 956 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 957 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 958 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 959 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 960 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 961 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 962 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 963 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 964 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 965 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 966 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 967 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 968 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 969 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 970 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 971 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 972 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 973 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 974 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 975 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 976 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 977 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 978 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 979 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 980 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 981 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 982 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 983 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 984 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 985 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 986 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 987 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 988 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 989 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 990 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 991 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 992 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 993 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 994 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 995 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 996 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 997 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 998 Total reward: 500.0 Average loss: 0.693147123\n",
      "Episode: 999 Total reward: 500.0 Average loss: 0.693147123\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = [] # accuracy\n",
    "loss_list = [] # loss\n",
    "\n",
    "# TF session for training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize/restore variables\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model-qn.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Training episodes/epochs\n",
    "    for ep in range(train_episodes):\n",
    "        \n",
    "        # Start new episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        loss_batch = []\n",
    "\n",
    "        # Training steps/batches\n",
    "        for _ in range(max_steps): # start=0, step=1, stop=max_steps/done/reward            \n",
    "            # Get action from model\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: state.reshape([1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.buffer.append([action, state, done])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done is True:\n",
    "                break\n",
    "                \n",
    "        # At the end of each episode/epoch\n",
    "        # Batch from NEW memory\n",
    "        batch = memory.buffer\n",
    "        actions = np.array([each[0] for each in batch])\n",
    "        states = np.array([each[1] for each in batch])\n",
    "        dones = np.array([each[2] for each in batch])\n",
    "        # Calculate targetQs/nextQs\n",
    "        actions_logits = sess.run(model.actions_logits, feed_dict={model.states: states})\n",
    "        Qs = np.max(actions_logits, axis=1)\n",
    "        targetQs = Qs * (1 - dones.astype(float))\n",
    "        loss, _ = sess.run([model.loss, model.opt], feed_dict = {model.actions: actions,\n",
    "                                                                 model.states: states,\n",
    "                                                                 model.targetQs: targetQs})\n",
    "        # At the end of each episode/epoch\n",
    "        print('Episode: {}'.format(ep),\n",
    "              'Total reward: {}'.format(total_reward),\n",
    "              'Average loss: {:.9f}'.format(loss))\n",
    "        rewards_list.append((ep, total_reward))\n",
    "        loss_list.append((ep, loss))\n",
    "        \n",
    "    # At the end of all training episodes/epochs\n",
    "    saver.save(sess, 'checkpoints/model-qn2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing training\n",
    "\n",
    "Below I'll plot the total rewards for each episode. I'm plotting the rolling average too, in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total rewards')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(rewards_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average losses')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, arr = np.array(loss_list).T\n",
    "smoothed_arr = running_mean(arr, 10)\n",
    "plt.plot(eps[-len(smoothed_arr):], smoothed_arr)\n",
    "plt.plot(eps, arr, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's checkout how our trained agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/model-qn2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 500.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('Blackjack-v0')\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('AirRaid-ram-v0')\n",
    "# env = gym.make('AirRaid-v0')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('Copy-v0')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = gym.make('Ant-v2') #mujoco\n",
    "# env = gym.make('FetchPickAndPlace-v1') # mujoco required!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'checkpoints/model-qn2.ckpt')    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # Episodes/epochs\n",
    "    for _ in range(1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # Steps/batches\n",
    "        for _ in range(111111111111111111):\n",
    "            env.render()\n",
    "            action_logits = sess.run(model.actions_logits, feed_dict={model.states: np.reshape(state, [1, -1])})\n",
    "            action = np.argmax(action_logits)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Closing the env\n",
    "        print('total_reward: {:.2f}'.format(total_reward))\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending this\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
